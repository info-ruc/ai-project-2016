 
large case databases are numerous and packed with information. the largest of them are potentially rich sources of domain knowledge. the fgp machine is a software architecture that can make this knowledge explicit and bring it to bear on classification and prediction problems. the architecture provides much of the functionality of traditional expert systems without requiring the system builder to preprocess the database into rules  frames  or any other fixed abstraction. implementations of the f g p machine use similarity-based reminding and the cases themselves to drive the inference engine. by having the system calculate and incorporate a measure of feature salience into its distance calculations  the f g p architecture smoothly copes with incomplete data and is particularly well-suited to weak-theory domains. we explain the model  describe a particular implementation of it  and present testresults for a classification task in three application areas. 
1 	introduction 
the knowledge acquisition bottleneck is still one of al's major problems. it has engendered commentary from senior ai practitioners for years  sparked the current revival of interest in machine learning  and is the primary motivation behind a multi-million dollar project to assemble a massive knowledge base  on the order of 1 facts   guha and lenat  1 . in recent workshops  many researchers have come to the conclusion that the effective use of large databases is our best hope to break through this bottleneck  friesen and golshani  1 . our work is in this vein  and is an attempt to devise a domain-independent software architecture and related algorithms to extract knowledge from a large database of unstructured cases  a term we define below. 
   cases are similar to feature-vector representations of data  but more general. they may be incomplete; individual cases need to have values for only an arbitrary subset of the universe of features. cases are not assigned to fixed categories. rather any feature in the feature-
1 	learning and knowledge acquisition 
universe is a potential category; the program must be willing to direct its inference process at determining the probable value of any arbitrary feature for a particular case. we say probable because we make no assumption about consistency of cases. t w o cases may have identical values along every dimension but one. 
   we adopt this definition because these are the characteristics observed in cases drawn from real-world domains. the question we sought to address is whether the static feature information stored in cases provides enough material to do useful inferencing and learning. the approach we choose is e x e m p l a r - b a s e d  cf.  porter et a/.  1   and relies on the cases themselves to drive the inference engine. our goals are the following: the system should  reason  on the basis of specific cases and groups of cases  and should therefore be able to cite specific precedents  including precedents that are themselves incompletely understood   to modify its behavior on the basis of every new information-providing transaction  and to subsume the functions of a conventional information-retrieval system. it should be a viable learning system  equalling or surpassing the accuracy of other purely data-driven approaches to classification and prediction problems  and do this while maintaining its availability as a real-time system. unlike typical connectionist approaches  it should be designed to interact with a user as a case is described incrementally  or to generate line-by-line commentary on new cases. most important  it should attempt to display not only  quantitative  but  qualitative  expertise. in our experimental definition  qualitative expertise requires  particularly on hard cases  but these are the whole point  pursuing possibly premature hypotheses  offering  occasionally brash  guesses and citing interesting precedents on the basis of a small but evocative degree of feature overlap. we are developing  simulated speculation  in an effort to approximate this behavior. 
1 	the fgp machine 
the program we've built in an initial attempt to realize these properties is the f g p machine  after its basic operations-fetch  generalize and project. we imagine the f g p machine's database as a collection of regions in space  cf. the standard vector space text-retrieval model . each element of the database corresponds to 

some region. nearby regions correspond to nearby cases. when presented w i t h an inquiry  the machine's basic task is to add to the database a new region corresponding to the inquiry. stationing itself on top of this new region  so to speak   the machine then looks around and reports the identities of the nearby regions-these will correspond to elements of the database that are nearby to  in other words closely related to  the subject of the inquiry. we can then inspect this list of nearby regions and  generalize -determine which attributes tend to be shared in common by all or by most of them. we can guess that these common attributes are likely to hold true for the case being described in the inquiry as well. 
   having reached whatever conclusions seem reasonable  the machine simulates  speculation.  temporarily turning aside from the inquiry in hand  it focusses on any  evocative possiblities  that may have suggested themselves during the examination of nearby regions. an  evocative possibility  is a d a t u m that might be true  and that would be significant if it were. the machine's interaction with the user  see figure 1  represents a combination of fairly safe conclusions  speculation experiments and the subsequent investigation of resulting guesses. 
   the system can operate interactively  but here it is working in  commentary  mode: the user presents an entire case; the system scans it element-by-element  offering comments. this case initially seems malignant  note the early mention of related cases with diagnoses of infiltrating ductal carcinoma ; the fact that the mass has not changed in density and has no comet  contradicting the system's guesses  which in the nature of guesses will often be wrong  points in the other direction   cyst  and  fed  refer to benign diagnoses ; but further data  particularly the absence of a halo  tips the balance  and the system guesses that this is a malignant mass. this guess is correct  and the diagnosis was in fact infiltrating ductal carcinoma. this transcript is driven by a small collection of 1 cases  which is the only domain knowledge provided. 
   an f g p machine is defined in terms of a single kind of data-object and three primitive operators. these define a virtual machine in terms of which the system is programmed. we summarize the essential points in the remainder of this section; see  fertig and gelernter  1; fertig  1  for further discussion. 
1 	d a t a - o b j e c t s a n d databases 
fgp machines run off a database of a single type of dataobject  a feature tuple to which we refer generically as a r. a r consists basically of a list of attribute-value pairs; we give examples below. an f g p database consists of an unordered collection of r's. a new case for inclusion in the database is presented as a r  and a query is an incomplete r-a partial list of attribute-value pairs  with a request that the system fill in certain missing ones. 
   individual cases are stored in r's; paradigm cases  cases that represent not a single object or incident  but a 
   generalized or prototypical object or incident  are stored in formally-equivalent  heavy r's   as we discuss. the basic fgp-operations described below operate on r's individually and in groups. 
the feature tuple 
       name apple   type fruit   color red   is a r that represents a single apple; 
       name apple 1   type fruit 1   color  red 1   yellow 1    is a r that represents one hundred different apples; half were yellow and half were red  and beyond this fact no individual apple can be distinguished. when the count field is omitted    1   is the default. a relatively  weightier  r is a r that reflects a greater number of individual cases. we use m to represent an unordered collection of r's; an fgp database of stored cases and paradigms is an m.l is a list of r's ordered on their  closeness  to some other r: we explain below. 
1 	p r i m i t i v e o p e r a t o r s 
the three basic f g p operations are fetch  generalize and project. they work as follows. 
	fetch maps a r and an m to an 	l: 	given a feature-
tuple and a database of feature-tuples  it produces an ordered list of those feature-tuples in the database that are  closest to  the r mentioned in the query  fetch uses a two-step procedure. 
   first it calculates a  distance  from the new point to every point in memory1; all cases further away than some parameterized threshold are removed from further consideration. the distance of one case from another cannot be statically determined; two cases may be more closely related in the context of one goal than in another. therefore fetch's calculation not only takes into consideration the number of shared attributes and their types  but also  in the context of a request to fill in values for missing attributes  the  evocativeness  of each with respect to the current goal-a more evocative feature is one that recalls a group of cases with a more highly focussed set of values for the goal attribute.1 features are weighted in the distance calculation in proportion to their evocativeness. the final distance score is the weighted sum of the individual feature distances normalized by the size  number of features  of the cases. in standard terms  the similarity metric measures the weighted euclidean distance between cases  where the weighting is done automatically and reflects the evocativeness of individual features with respect to the goal. the system comes with default feature distance metrics defined for nominal  ordinal  and dimensional  real-number valued  data  although user-defined feature metrics can be used to reflect more specialized knowledge of the domain. 
   next fetch checks to see if there exists a well-defined group of very-close points among those remaining by performing a crude cluster analysis. details can be found in 
1
　　this calculation needn't require that every r in m be examined; as a first cut  we can ignore all cases that fail to intersect the query on at least one attribute. we use hashing to determine the candidate cases  to exclude the irrelevant ones . 
1
　　the evocativeness of an attribute-value pair with respect to a goal attribute is inversely proportional to the entropy  disorder  of the distribution of values for the goal represented in the group of cases returned by fetch. 
fertig and gelernter 

 age 1  
 mass-density iso-dense  
 mass border-cqmplete  no  
 mass type border irregular   mass border defined  mo  
 massjlocation uil  
 mass size changed  yes  
 mass-density-changed  ho   mass-comet  no  
 mass halo  no  
 background demsity moderate  
 skin-changes no  
 nipple-inversion  no  
 adenopathy  no  
 family-history-cancer sister  
 personal-historyjcancer 	no  speculating: 	mass-density-changed ... 
guessing 	increased - 	e.g. 
case 	  id 	1  	 age 1  	 diagnosis 	ca inf-ductal   case 	  id 	1  	 age 	1  	 diagnosis 	cajnf-ductal   
speculating: 	mass-comet ... 
guessing yes - e.g. case 1 case   id 1   age 1   diagnosis cajnf-ductal   
speculating: 	background-density... 
guessing dense - e.g. 
case   id 1   age 1   diagnosis cyst     id 1   age 1   diagnosis fcd   
concluding 
 architectural-distortion  	no  
speculating: 	malignant ... guessing yes - e.g. cases  1 1  
speculating: 	skin-changes... 
guessing retraction - e.g. 	cases  1 1  
closest known cases: 	 1  	 yes  	 cajnf-ductal  
	 1  	 yes  	 cajnf-ductal  
	 1  	 yes  	 ca-inf-ductal  
	 1  	 yes  	 cajnf-ductal  
	 1  	 yes  	 ca  
	yes has been 	concluded 	or guessed for malignant  
	speculating: 	diagnosis... 
ca  
cajnf-ductal  
figure 1: transcript of an f g p machine operating in the domain of mammography. the user's case description is in the left column  the system's commentary on the right. 
1 	learning and knowledge acquisition 

 fertig  1 . an ordered list of these very-close points is returned as fetch's value. 
　generalize maps an to a it takes an ordered list of feature-tuples and  compresses  them into a single new feature-tuple. the weighter a r and the closer it is to the top of the list  the larger the contribution its attribute-value pairs make to the combined r returned by generalize. suppose we query on the r  name apple   and suppose that m holds one hundred individual apples  half red and half yellow; a generalize operation over a list consisting of one hundred apples  half red and half yellow  yields a single r that might look like our second example above. 
   project maps a r to a r: given a feature-tuple it returns a new tuple constructed from a subset of the features in the original. while project is a purely syntactic operation  it is used by higher level operations  see the discussion of refocus below  to change contexts; the system focusses on those attributes and values that are deemed  interesting 1  temporarily ignoring other information on hand. 
1 	t h e basic cycle 
given this three-instruction virtual machine  how does the system operate  the basic cycle is two phase:  1  extend the current r;  1  choose a new current r  and repeat. step one is implemented by an extend r  function that is defined in terms of fetch and generalize. step two is implemented by refocus which is defined in terms of all three. 
   to extend a r - to discover new implications given our database of cases and paradigms - we begin by executing the operation generalize fetch m  r       where m is the database. if r  for example  describes a particular patient  will return a list of remembered r's that are close to  similar to or reminiscent of  this particular patient; executing generalize over this list will produce an amalgam of all these remembered cases. any highly-focussed and sufficiently-weighty values can be classified as conclusions: if the memories examined by generalize mainly have a value of  blonde  for attribute  hair-color   say  the system will conclude that   h a i r - c o l o r b l o n d e   is likely to characterize this case as well. it reports   h a i r - c o l o r b l o n d e   to the user as a conclusion and augments the current r with this new at tribute-value pair. the system attempts to conclude any value turned up by the fetch-generalize combination which hasn't yet been seen in the context of the current query. values which contradict1 are withdrawn; the user's input always takes precedence over system guesses. the extend operation is complete when all values that can be concluded have been and all contradictions removed. 
1
　　the system uses the evocativeness values calculated by fetch to determine which features are interesting. 
1
　　 as expected  two distinct values of a boolean-typed attribute always contradict. system-concluded values of other types of attributes contradict only if this information is specified in the attribute's distance metric. see  fertig  1  for more details. 
1.1 	s i m u l a t e d s p e c u l a t i o n 
   refocus is then invoked over the extended r. its role is to examine a . and refocus attention from this entire r to one  possibly small and conceivably unrepresentative  part of it. this element considered in isolation may serve as a seed for a new set of inferences. we call this process  simulated speculation.  refocus may choose no  one or many data points; each chosen data point becomes the current in t u r n . the more evocative a data point with respect to the goal-the more sharply-defined the cases nearby a  consisting only of that data point with respect to the goal attribute  in other words-the likelier target for refocus. the more sharply a data point stands out from the pack-by assumption it won't stand out clearly enough to qualify as a conclusion  but there are many intermediate shadings here-the likelier a refocus target. 
   typically  the system will examine each of a small set of values associated with a particular attribute whose value  if known  would focus the search space considerably. the system performs the basic fetch-generalize cycle on each of these seed-tuples and is left with a set of regions in vector-space. one may be much closer to the original query than the others and may therefore be mergeable with it. the reader can see the system's behavior during several refocus experiments by examining the transcript shown in figure 1. refocus first announces the attribute projected to  followed by any values tentatively guessed as the result of the speculation experiment. it then gives pointers to specific cases that both have this value and also resemble the rest of the user's input. 
   the system can operate in various modes; in the example given the basic cycle is repeated after considering each attribute of the presented case. after all input has been considered  the system will execute the basic cycle one final time. it is then in a position to present a list of the closest cases in the database given all the input and project to any undetermined goal attributes in one final attempt to determine values or at least narrow the space of possibilities. 
   w h y should simulated speculation lead to more accurate performance  as a user enters a case the system is automatically calculating a weight for individual features based on their evocativeness  or how closely they focus the search space with respect to the goal attribute. in statistical terms  we can think of the system as treating all features other than the goal as independent variables and the goal as the dependent variable. the system is automatically calculating a weight for each non-goal feature in isolation  in essence making the simplifying assumption that it is dealing with a simple bivariate distribution. of course this isn't correct; in fact each case is better thought of as a many-internal multivariable data point in which no feature can be labelled a priori as dependent. a n d as in any multivariable case  correlations between any two features may be artifacts that disappear when seen in the context of additional variables. the weighting of individual features according to a bivariate model is essentially a useful approximation but not guaranteed to be accurate. it will lead to inaccu-
	fertig 	and 	gelernter 
rate retrieval when the impact of one feature  variable  is enhanced or diminished considerably when it appears in the context of one or more others. 
　the heuristic approximation of feature salience by treating each feature as the independent variable in a bivariate model is a useful approximating device that fails to capture in its calculation important combinations of features. it is clear that we cannot calculate a salience score for each subset of all possible features. the cost is exponential and is clearly impractical for domains in which cases consist of more than a handful of features. however  we may be able to identify subsets of features that are important when considered together through the judicious use of simulated speculation. 
   for example  consider what happens when the system sets aside its standard processing and speculates on an undetermined goal feature. the system first collects all the values for that feature that appear among the cases currently judged to be close. then a fetch is performed on each of these values and yields a profile  i.e. a generalized t  in which some  other  features are strongly associated with that value. since now the mapping is one-to-many there is no danger of hidden interactions among the features masking or spuriously adding to the importance of any single feature  and those strongly associated with one goal value and not with others are good markers for the presence of that value. those that appear in all or most of the retrieved profiles are bad or completely ineffective markers. 
　as outlined  simulated speculation can help the system adjust the feature salience measures crucial to effective retrieval of relevant cases. in so doing it improves accuracy. simulated speculation also improves efficiency  that is how quickly it converges on a list of relevant cases and determines the values for all goal features  if possible . similated speculation may narrow the search space directly by eliminating from further consideration  at least temporarily  potential goal values that are incon-
sistent with the case as currently entered. the subsets of features which are discovered to be strongly associated with potential goal values provide an automatic standard by which to judge whether the current case is consistent with that goal value. 
　finally  simulated speculation provides a road map for further processing. among the features strongly associated with a goal may be some about which nothing has been entered. directing its attention to these  and using the same refocus operation  the system may be able to determine the probable values for one or more of these non-goal features that are nevertheless highly correlated with the goal. in so doing it will have moved a significant step closer to pinpointing the goal. even if unsuccessful on its own  the system can ask the user if he knows the value for any of these correlated features and thus engage in a reasonable directed dialogue. 
1 	higher-level operations 
an interesting and useful collection of higher-level operations can be based on these primitives. we have seen one  refocus  which is central to our attempt to simulate speculation. others are discussed in  fertig 
1 	learning and knowledge acquisition 
and gelernter  1; fertig  1 . the primitive operations have proven sufficient to allow the fgp machine to assemble intelligent sequences of database retrieval out of individual comparisons  performing if necessary a large number of individual analyses in an attempt to converge on a goal. the results discussed in the next section along with the example transcript illustrate the power of this structure-simple primitives  more complicated strategies-in achieving a measure of qualitative and quantitative expertise. 
1 	experiments 
this section discusses experiments performed with the implemented  serial  version of the fgp system. 
　the experiments were conducted on case databases in three domains: descriptions of mammograms collected at yale-new haven hospital  fisher's iris plant database  fisher  1   and a collection of international folk dance descriptions.1 these datasets are described below. 
　we wanted to validate the fgp system performance in three ways. first  we wanted to verify that the higher-level operations such as refocus worked as intended. recall that all fgp operations are built recursively from the three primitive operations fetch  generalize  and project. the behavior of fetch depends not 
only on the syntactic structure of the inputs to the operation but also on the domain information contained in the database. this semantic dependency makes testing any implementation of an fgp machine on synthetic data only partially effective. real data from real domains must be used to exercise the various component operations in the algebra. 
　we also wanted to quantify the performance of the finished system. although we do not intend for fgp systems to be used solely or even primarily as stand alone diagnostic systems  accuracy in a diagnostic task is an easily understood metric and probably the most commonly used to compare the performance of automatic classifiers and expert systems to that of human experts. for this reason  we've chosen diagnostic accuracy as the primary quantitative measure to be used in the three domains. 
　measures of qualitative behavior are our third and final means of measuring the system's performance. as we've stressed  an fgp system is intended to be used as an interactive tool that can suggest plausible hypotheses and appropiate comments as information about a case is entered. this kind of behavior is hard to assess quantitatively. nonetheless  we have made an attempt and have developed a metric to measure the system's rough 
qualitative behavior in the radiology domain. 
1 	folk dances 
the first context in which discuss the fgp machine's accuracy is the one in which a reasonable size database and domain expert first presented themselves. problem: determine the nationality of a previously unseen folk dance given a list of other attributes  such as whether it is done by couples or in fours  in a circle or a line  
1
 we thank jonathan young for providing this database. 

etc . the database has 1 cases spanning 1 different nationalities; the average number of features per case is 
1. 
   as reported in  fertig and gelernter  1   we presented 1 test cases for the f g p machine to classify. the domain expert  who happened to be the creator and maintainer of the database  answered correctly 1% of the time. the prototype determined the correct nationality 1% of the time overall. note that the classification task is not trivial given the number of possible classes and the sparseness of the training data. the performance of both the domain expert and the program is quite good although under 1% correct in an absolute sense. 
   we applied the current version of the f g p machine  which incorporates a working implementation of the refocus operation  to the same database and task to determine if  simulated speculation  could improve accuracy. we found that it did: the f g p machine was able to determine the correct nationality 1% of the time overall. the system was successful in 1 of 1 attempts to use refocus to determine the dance's nationality when it failed to draw a conclusion from the input directly. the f g p machine also used refocus to guess the values of other attributes much as seen in figure 1. these operations are harder to quantify  as many test cases are missing attributes. we were able to conclude  however  that the system guessed either a correct or consistent1 value in 1% of these refocus experiments. the guess was clearly wrong 1% of the time. 
1 	irises 
our second application is in the domain of plant identification. r.a. fisher's data on three types of iris  fisher  1  is often cited in the pattern recognition literature  and has become a de facto performance benchmark for new classification systems  gates  1; duda and hart  1; dasarathy  1 . following the example of the previous studies  we randomly divided the iris db into two partitions  of size 1 and 1   making sure to include equal numbers from each class in both the training and test partitions. 
   table 1 gives the basic result. the f g p machine correctly classifies 1 out of 1 of the test cases  1%   slightly improving on the performance of dasarathy's neighborhood census rule algorithm  table 1   the best-performing of the nearest neighbor approaches  freeman  1; gates  1; dasarathy  1 . 
dasarathy allowed his system to choose a simple 
 don't know  option if none of the known categories was closer than some parameterized threshold. the f g p system's ability to suggest multiple categories when unable to classify an item unambiguously is similar although often more informative. suggesting a few possible categories is equivalent to ruling many categories out. this behavior is meant to capture an expert's ability to reserve judgement when faced with an unusual combina-
1
　　 a value was judged consistent if it appeared in a majority of the cases in the database with the same nationality as the testcase. 

table 1: 	frequency 	of correct 	classification from dasarathy1 

table 1: frequency of correct classification by fgp system 
tion of facts  while still ordering past experiences in relation to the new situation. 
1 	r a d i o l o g y 
the current prototype has also been tested against a small database of patient records  descriptions of mammograms. there were originally 1 records in the database; 1 cases were reserved for testing and 1 were used to seed the system  spanning 1 possible diagnoses  one of these 1 possibilities was the diagnosis normal  meaning no disease present .1 the system was presented with the 1 test cases and asked to judge if a malignant lesion was indicated  and if so to determine a specific diagnosis. as discussed above  the system would present a short list  of possible diagnoses if unable to decide on one with certainty. 
   the domain expert was the radiologist who had compiled the database.1 working from the descriptions of the mammograms alone  he accurately judged the malignancy of the testcases at the 1% level. the system performed slightly worse at 1%. however the system outperformed the domain expert in producing a differential diagnosis  with the right answer being stated outright or appearing in a short list of possibilities 1% of the time to the clinician's 1% correct performance. 
1.1 	m e a s u r i n g q u a l i t a t i v e p e r f o r m a n c e 
   as we have discussed  we wanted the fgp system to exhibit qualitative expertise as well as quantitative accuracy. we developed the refocus operation to model the human propensity to set aside goal-directed thinking in the face of evocative events. we have labeled the f g p system's ability to behave similarly  simulated 
1
　　one record was thrown out because no diagnostic information was included. 
1
　　dr. paul fisher of the department of diagnostic radiology  yale university school of medicine. dr. fisher's clinical specialty is mammography. 
1
　　 we present no evidence for this propensity as it is tautologically beyond question: an event could not  by definition  be evocative if it did not cause the observer to stop and take notice. 
fertig and gelernter 

speculation.  we discussed it in the context of both the folk dance experiment and figure 1. still  we would like to quantify the system's qualitative performance  and in particular the usefulness of simulated speculation. 
   we took two approaches: first  measure the absolute improvement in classification accuracy when simulated speculation is enabled. as reported  the f g p system's accuracy improved from 1 to 1% correct in the folk dance experiment  for a 1% gain. the system's performance is identical with or without speculation when applied to the iris test  a fact we expected given the extremely small number of features per case. preliminary results indicate an 1% improvement in the radiology domain.1 note that the improvement in any domain is on precisely the most difficult cases. refocus is only invoked when the direct evidence for choosing one value over another is underwhelming or nonexistent. 
   second  we wanted to measure whether simulated speculation caused the system to make appropiate comments  in about the right places and at the right times.  to test this  we asked the domain expert to provide us with an audio tape of his analysis of five of the x-rays chosen at random. we presented the same five to the fgp machine and then placed the transcripts side-byside. 
   we then counted how often the f g p system and domain expert agreed that a particular feature indicated a malignant or benign condition in the patient. the system agreed on 1 of the 1 times the expert chose to mention that a particular feature indicated malignancy or its opposite. the system made this type of comment at a total of 1 junctures where the expert said nothing  but these comments were clearly inappropiate in 1 places only  in the opinion of the domain expert . while preliminary  we are encouraged by this concordance of opinion. 
1 	related work 
the idea of retrieval based upon similarity is crucial to our system's ability to select a few relevant cases from a dense and extensive database  an idea exploited by research in case-based reasoning  analogy and information retrieval.  fertig  1  provides an extensive literature review. 
1 	work in progress 
although it is built to be reasonably efficient  the current fgp machine is inadequate to handle case databases of more than 1 records assuming 1 features per case. the technique is intended specifically for use with large databases-the larger the better. generating transcripts like the ones in the figures requires repeated computations against many or all elements in the database. standard indexing strategies are useful to some extent  but in this problem  unlike the keyword-based text retrieval problem  for example   the database is likely to be fairly homogeneous  with most features present in most cases. 
1
　　the numbers reported above for the radiology experiment are with  speculation  enabled. 
1 	learning and knowledge acquisition 
it's clear that if this program is to perform well  we must be able to execute operations involving large portions of large databases quickly. parallel programming techniques explored by members of the linda group at yale are a promising approach  e.g.  carriero and gelernter  1   and we are currently applying these techniques to a new implementation of the f g p machine. 
