
we present an algorithm for organizing partiallyordered observations into multiple  threads   some of which may be concurrent.  the algorithm is applied to the problem of constructing career histories for individual scientists from the abstracts of published papers. because abstracts generally do not provide rich information about the contents of papers  we developed a novel relational method for judging the similarity of papers. we report four experiments that demonstrate the advantage of this method over the traditional dice and tanimoto coefficients  and that evaluate the quality of induced multi-thread career histories.
1 introduction
like most researchers  i work on several problems at once  so my publications represent several research  threads.  some threads produce several papers  others lie dormant for months or years  and some produce infrequent  irregular publications. some papers clearly represent new lines of research  for me  while others are continuations. although my research has changed over the years it is difficult to identify when these changes occurred. yet  in retrospect  it is usually clear to me that one paper is a direct descendent of another  while a third is unrelated to the other two.
¡¡how should we model the processes that produce research and publications  a single researcher can be viewed as a bundle of non-independent processes  each of which produces one kind of research. for example  some of my papers are about modeling cognitive development on robots and some are about finding structure in time series  and although these threads are distinct  they are not independent. occasionally two or more threads go through one paper  or  to say it differently  one paper can descend from two or more parents. within a thread  papers may be ordered roughly by their publication dates  although some appear months or years after they were written. threads sometimes appear with no apparent connection to previous work.
¡¡my publications can be arranged in a graph like the one in figure 1. later publications are represented as children of earlier ones or  when they are unrelated to earlier publications  as children of the root  e.g.  the node labeled d begins a new thread . to avoid false advertising  i must say immediately that the algorithm i present in this paper produces trees in which every publication has only one parent  e.g.  it cannot produce nodes like the one labeled a b . an algorithm for generating graphs like the one in figure 1 is under development.

figure 1: a publication tree. each node represents a publication. threads are identified by letters. the depth of a node represents its publication date.
¡¡trees or graphs of publications are not models of the processes that generate publications  but nor are they entirely descriptive. inference is required to generate them  in particular  research threads and parent-child relationships must be inferred. the inference problem is as follows: given an ordered or partially-ordered sequence of observations  identify the processes that generated each observation  and assign each observation to at least one process. some processes may not be active at the beginning of the sequence. i call this the problem of disentangling the threads.
1 the pubtree algorithm
the pubtree algorithm greedily adds papers to a growing tree of papers:
1. arrange the papers in chronological order  p1 p1 ...pn  least to most recent.
1. create a root node r. make p1 a child of r.
1. for each remaining paper pi  find the most similar paper in the tree and make pi its child. if pi is not similar enough to any paper in the tree  make it the child of r.
¡¡the last step is managed with the help of a threshold. for two papers  a and b  if s a b    tthread  treat them as similar enough for b to be a's child  otherwise make b the child of r.
1 relational similarity
clearly  the pubtree algorithm relies on a good similarity metric. this section describes a novel approach to judging the similarity of the abstracts of papers. two abstracts  a and
b  contain sets of words wa and wb  of sizes  respectively. for example  we might have wa =  tree search optimize constraint restart  and wb =  graph search vertex constraint . note that the abstracts have two common or shared words  search and constraint  and five words that are not common  or unshared. shared and unshared words each contribute a component to our measure of similarity of abstracts.
¡¡a prevalent way to compare abstracts is to count the number of shared words and divide by the total number of words in both abstracts:
		 1 
this is called the dice coefficient. its value will be one when wa = wb and zero when the abstracts share no words. several other coefficients have been proposed  e.g.  jaccard  cosine  see  grossman and frieder  1  . the differences between them were slight in the experiments that follow.
¡¡for unshared words  we define a simple relational measure of similarity. let occ wi wj  be the number of abstracts in the corpus in which words wi and wj co-occur. two words are said to be related if they co-occur in a super-threshold number of abstracts:
		 1 
for two abstracts  a and b  with words wa and wb  a relatedness coefficient is

¡¡r is the average value of rel for all pairs of unshared words. its value will be one when every pair of unshared words is related  that is  occurs in a super-threshold number of abstracts. note that r does not require two abstracts to share any words. that is  the measure works even if wa ¡É wb is empty.
putting d and r together we get:
	s a b  = ¦Ád a b  +  1   ¦Á r a b 	 1 
this measure  a weighted sum of similarity measures on shared and unshared words  also will range from zero to one. if ¦Á = 1 then s a b  is the dice coefficient and depends entirely on shared words. if ¦Á = 1 then s a b  is the relational similarity measure  r  and depends entirely on unshared words.
¡¡to illustrate the value of the relational similarity measure  r  consider two abstracts of papers about the same piece of research  a system for automating exploratory data analysis. after non-content words are removed  the first abstract is represented by: automate  capture  complexities  contribute  exploratory  eda  igor  knowledge-based  manage  phoenix  relies and script-based. the second abstract  by the same authors  represents another paper about a different aspect of the system  with these words: aide  assistant  arparome  begins  captured  contracts  data-driven  descriptive  extracted  examined  enormous  explored  eda  exploratory  informal  presentation  plays  prelude  researcher  subtle  understanding  word. the dice coefficient for these abstracts is just d = 1 because  although the abstracts are from very similar papers  they share only the words. exploratory and eda. the relational coefficient is r = 1  a much more satisfactory representation of the similarity of these papers.
1 related work
pubtree is neither a clustering algorithm nor a sequence analysis algorithm but has some attributes of both. like clustering  pubtree forms groups of objects  that is  it groups papers together into threads. yet clusters are generally unstructured  or hierarchically structured  while pubtree's threads are organized sequentially by publication dates. another difference is that  generally  an item may be added to a cluster if it is similar to all items in the cluster  usually via comparison to the cluster centroid  whereas an item may be added to a thread if it is similar to the last item on the thread.
¡¡sometimes  adding an item to a thread causes the thread to split into two or more threads  i.e.  threads are organized into trees . a thread a b c... splits under node b when a paper d is considered more similar to b than to c. recall  a thread is supposed to represent a process that produces research papers  and a researcher is thought of as a bundle of such processes  so splitting a thread is analogous to changepoint analysis  e.g.   lai  1    or finding a point in a sequence where a process changes. one can imagine a kind of pubtree algorithm that repeatedly samples sequences from the author's list of papers  builds markov chain models of some kind  and identifies threads as sequences that do not feature changepoints. the challenge  of course  is to model the production of papers as a stochastic process. we do not know of any way to do this  which is why we built the similarity-based pubtree algorithm. it would be nice to model the author as a hidden markov model  hmm   where the hidden state is the thread identifier  but  again  we do not have a stochastic framework for the production of research papers. nor do we know a priori how many threads  i.e.  hidden states  an author will produce  as fitting hmms requires.
¡¡genter and her colleagues introduced the term relational similarity to emphasize that situations may be similar by merit of the relationships that hold among their elements  even though the elements themselves are different  gentner and forbus  1; gentner and markman  1; goldstone et al.  1 . this notion of similarity requires situations to be structured; there are no relationships between elements in a bag. however  as this paper shows  elements wi and wj from unstructured bags of words wa and wb  respectively  can be related by their appearances in other bags wc wd .... clearly  this is a weak kind of relational similarity  but the term seems apt  nonetheless.
¡¡our kind of relational similarity is a bit like query expansion in information retrieval. the idea of query expansion is that words in queries are rarely sufficient to find the right documents  so the query is  expanded  or augmented with additional words. local query expansion  xu and croft  1  uses the query to retrieve some documents and the words in those documents to expand the query. so  in query expansion  wi ¡Ê wa causes wc wd ... to be retrieved  and wj ¡Ê wc wk ¡Ê wd ... expand the query  which might cause wb to be retrieved. whereas  in our technique wa and wb are judged to be similar if wi ¡Ê wa and wj ¡Ê wb and wi wj ¡Ê wc;wi wj ¡Ê wd .... clearly these aren't identical inferences  but they are related.
1 experiments
the experiments reported here were performed on a corpus of 1 papers from the citeseer database. the papers were selected by starting with my own citeseer entries  adding all those of my coauthors  and all those of their coauthors. each paper's abstract was processed to remove common and duplicate words so each paper was represented by a set of words.  unlike in most information retrieval applications  no special effort was taken to identify discriminating words  such as ranking words by their tf/idf scores.  the mean  median and standard deviations of the number of words in an abstract were 1 1  and 1  respectively.
1 experiment 1: same/different author
our first experiment tested the relative contributions of the d and r coefficients to performance on a classification task. the task was to decide whether two papers were written by the same author given their abstracts. the procedure is: select abstract a at random from the corpus; with probability p select another abstract  b  from the papers of the same author as abstract a  and with probability 1   p select b at random from the corpus; record whether a and b have the same author; record d a b  and r a b .  for this experiment  tocc = 1.  now  for a given value of ¦Á  compute the similarity scores a b   s ta bs decide that  and compare it to a threshold a and b have the same author.ts  and if
because we recorded whether the papers were truly written by one author or two  we know whether a  single-author  decision is a correct  a hit  or a incorrect  a false positive . repeating this process for many pairs of papers  1 in these experiments  yields a hit rate and a false positive rate for each value of ts  and by varying ts we get a roc curve.
¡¡the roc curve represents how good a classifier is. ideally  the curve should have a vertical line going from the origin to the top-left corner. such a curve would indicate that the falsepositive rate is zero for all levels of hit rate up to 1. more commonly  as ts decreases  one gets more hits but also more false positives. figure 1 shows curves for three levels of ¦Á  that is  for three degrees of mixing of the dice and relational similarity measurements. the curve labeled r is for relational similarity alone  ¦Á = 1  and the curve labeled d is for the dice coefficient alone  ¦Á = 1 . between them lies a curve for ¦Á = .1.

figure 1: roc curve for classifying whether two papers are written by one author or two  based on the decision criterion s a b    ts. points are for different values of ts.
¡¡the relational similarity measure r has a slightly better roc curve than the others because it is a more nuanced metric than the dice coefficient  d. in fact  d has a fairly high accuracy; for example  if one says two abstracts are written by different authors when d = 1 and by the same author when d   1  then one's overall accuracy  true negatives plus true positives  is 1%. however  roughly 1% of the pairs of abstracts that are written by one author have d = 1  and it is here that the r coefficient provides additional resolution.
¡¡a followup experiment compared not individual papers but the entire oeuvres of authors. the protocol was as follows: select two authors a and b at random from the corpus. select random pairs a b in which a and b are papers of a's and b's  respectively. calculate s a b  for each such pair and add the mean of these scores  i.e.  the mean pairwise similarity of papers by authors a and b  to the distribution of betweenauthor scores. select random pairs of papers ai aj and bi bj  calculate their similarity scores and add the mean similarity scores to the distribution of within-author scores. finally  repeatedly select a mean score from either the within-author or between-author distribution and  if the score exceeds a threshold t assert that it is a within-author mean score  as these are expected to be higher than between-author scores . calculate the classification accuracy of these assertions for different values of t to get an roc curve. the result is that the area under the roc curve is 1  which is nearly perfect classification. in other words  the mean similarity scores for pairs of papers by the same author are very much higher  in general  than the mean similarity scores of pairs of papers by different authors.
1 experiment 1: author trees and intrusions
the second experiment tests how the similarity measure in equation 1 performs in the pubtree algorithm. it is difficult to get a gold standard for performance for pubtree. the algorithm is intended to organize the work of a research into  threads   but we lack an objective way to assess the threads that characterize researchers' work  see the following section for a subjective assessment . however  if we build a graph from two researchers' abstracts  then we would hope the pubtree algorithm would not mix up the abstracts. each thread it discovers should belong to one author and should include no abstracts by the other. this is our proxy for pubtree's ability to disentangle research threads  and it can be evaluated objectively.
¡¡the procedure for experiment 1 is: select ten abstracts at random from the work of a random author. select another ten abstracts from a second author. run pubtree to produce a tree for the first author. now  have the algorithm add the papers of the second author to this tree. to score performance  recall that every abstract can be added as the child of one already in the tree  extending a thread; or it can be added as a child of the root  starting a new thread. good performance has two aspects: there should be no  intrusions  of the second author into threads of the first author  and the number of new threads should be small. if the number of new threads is large  then pubtree is saying  essentially   every abstract looks different to me  i can't find any threads.  thus  each replicate of the experiment returns two numbers: the proportion of all the abstracts  from both authors  that join extant threads; and the proportion of the second author's abstracts that intrude on the threads of the first author. fifty replicates were run at each of ten levels of the threshold tthread  which is the threshold for allowing an abstract to extend a thread. the entire experiment was repeated three times for different mixing proportions of d and r  namely  ¦Á =  1 1 1 
¡¡the results of this experiment are shown in figure 1. the axes are the levels of the two dependent measures described in the previous paragraph. on the horizontal axis is the proportion of all abstracts that join extant threads  and on the vertical axis is the proportion of abstracts of the second author that intrude into threads of the first author. each point on the graph represents the mean of fifty replicates associated with one level of ¦Á and one level of tthread. in general  higher values of tthread cause new threads to be formed because if an abstract is not similar enough to an abstract in an extant thread to exceed tthread  then it starts a new thread. if an abstract begins a new thread then it cannot intrude in an extant one  so there is a positive relationship between the tendency to join an extant thread and the number of intrusions.
¡¡although the relationship between these variables is not linear  we can interpret the slopes of the lines in figure 1 very roughly as the error rate  in terms of intrusions  of joining extant threads. this error rate is highest for the relational similarity measure  r  and appears to be lowest for the dice coefficient d. in fact  almost all of the data for ¦Á = 1 - which is equivalent to the dice coefficient - is grouped near the origin because d = 1 so often  as we noted in the previous section. said differently  when the similarity metric is d  intrusions are avoided by starting a lot of new threads. in contrast  when the similarity metric is r  the relational similarity coefficient  the error rate associated with joining extant threads is highest. an intermediate error rate is evident for ¦Á = .1.

figure 1: the relationship between the number of threads and the proportion of opportunities to include intrusions that were taken  for three levels of ¦Á and ten levels of tthread
¡¡experiment 1 shows that the pubtree algorithm does a good job in most conditions of keeping the abstracts of one author from intruding on the threads of another. this result provides some evidence that pubtree can separate abstracts into threads and keep the threads relatively  pure.  of course  this experiment actually tested whether pubtree would confuse the abstracts of two different authors  it did not test whether pubtree correctly finds the threads in the papers of one author.
1 experiment 1: one author with coauthor indicator
the third experiment uses coauthor information as a gold standard for the performance of pubtree. the idea is that papers within a thread often have the same coauthors  whereas papers in different threads are less likely to. we would have reason to doubt whether pubtree finds real threads if the degree of overlap between coauthors within a thread is no different than the overlap between coauthors across threads.
¡¡the procedure for experiment 1 is replicated many times  once for each author of ten or more abstracts. for each such author  build a publication tree with pubtree. then for every pair of abstracts in the tree  calculate the overlap between coauthor lists with our old friend  the dice coefficient  eq. 1   substituting lists of authors for wa and wb. next  the overlap between papers within each thread is calculated in the same way. thus we generate two lists of overlap scores  one for all pairs of papers  the other for papers in threads. finally  a two-sample t test is run on these lists and the t statistic is reported.  note we are not using the t test for any inferential purpose  merely using the t statistic as a descriptive measure of the difference between two samples. 

figure 1: the distribution of t statistics for 1 authors of at least 1 papers. a publication tree is built for each author and the degree of overlap between coauthors of pairs of papers within and between threads is calculated. the between-thread and within-thread samples are compared with a two-sample t test and this is the distribution of t statistics.
¡¡virtually all the authors showed the same pattern: coauthors of papers on a thread overlapped more than coauthors of papers on different threads. the distribution of t statistics is shown in figure 1. values greater than zero indicate more within-thread coauthor overlap  values less than zero indicate more between-thread overlap. values greater than 1 would be significant at the .1 level if we were testing the hypothesis that within-thread overlap is no different than between-thread overlap. roughly 1% of the authors had values less than zero and 1% had values greater than 1. on average  the overlap coefficients for papers within and between threads were .1 and .1  respectively. said differently  papers within threads had 1% more overlap among coauthors  calculated per primary author and then averaged  than papers between threads. we may safely conclude that pubtree's threads  which it constructs with no knowledge of coauthors  satisfy one intuitive feature of threads  namely  that papers on threads often share coauthors.
1 experiment 1: subjective judgments
the first three experiments report proxies for the subjective notion of a research thread. experiment 1 showed that the similarity measure s a b  does pretty well as a classifier of whether abstracts a and b are written by the same author.
experiment 1 showed that pubtree tends to segregate papers written by different authors into different threads of a publication tree. experiment 1 used exogenous information about coauthors to show that abstracts in pubtree's threads tend to share coauthors. none of these is a direct test of whether pubtree builds trees that represent threads in authors' research. research threads are subjective entities and the accuracy with which they are recovered by pubtree cannot be assessed objectively with the information available in citeseer. the best we can do is ask authors to assess subjectively the publication trees constructed for them by pubtree. three authors volunteered for this arduous job. each was instructed to examine every thread of his or her publication tree and count the number of inappropriate parent/child relationships. for example  consider a thread of three papers  a b and c. two papers  b and c might be misplaced: b might not be a child of a and c might not be a child of b. the latter question is settled independently of the former  so even if b is not appropriately placed under a  c might be appropriately placed under b. one ought to ask whether c is appropriately placed under a  but this question made the task of scoring publication trees considerably harder for the volunteers and was not asked.
¡¡the trees for the three authors contained 1  1  and 1 papers  respectively. the authors all have written considerably more papers  so pubtree was working with incomplete data. the first author reported that 1 of her papers  or 1% were placed correctly. of the remaining 1 papers  three should have started entirely new threads  i.e.  should have been children of the root node   and two had incorrect citeseer dates and might have been placed wrongly for this reason. the second author reports a slightly higher proportion of misplaced papers  1  or 1%  but notes that for more than half of these  the publication date was unknown and so was not used by the algorithm. the third author reported that 1 of the 1 papers had corrupted titles and were duplicates of others in the set. of the remaining 1 papers  1  or 1% were placed wrongly. three started new threads but should have been added to extant threads  four had valid dates and were placed under the wrong parents  and three had no dates and were placed under the wrong parents.
¡¡these results are not dramatically good  but nor are they dramatically bad  especially when one considers that every paper might have been placed below any of two dozen  for the first author  or three dozen  for the second and third author  papers  on average. one of the authors said    the  tree did surprisingly well  given the subtle differences between papers. ...the tree made most but not all of the right connections. i noticed that some papers could be thought of as having more than one parent in reality  which would complicate threads  but of course that can't be addressed in a single tree. 
1 discussion and future work
as the author noted correctly in the previous section  pub-
tree builds trees  not graphs  and so it cannot give a paper more than one parent. this is unfortunate because some papers truly are the children of two or more research threads. a version of the algorithm that produces graphs is under development. another likely way to improve the algorithm is to include additional information about papers  such as coauthor lists.
   pubtree and its successors might be applied to several current problems. one is to track the development of ideas in the blogosphere. blogs are like abstracts of research papers in many ways and it will be interesting to look for blog threads in the writings of individual authors  and perhaps even more enlightening to find threads in amalgamations of several authors' blogs. email threading is a related problem.
¡¡the relational similarity measure that underlies pubtree also has numerous applications  notably to problems that require some degree of semantic matching between documents that do not necessarily share terms. one example is a kind of entity resolution for authors. the citeseer corpus we worked with contains abstracts for paul r. cohen and a nearly disjoint set of abstracts for paul cohen. we might wonder whether these authors are the same person. one approach is to calculate the average between-author similarity for these authors and compare it to a null hypothesis distribution of betweenauthor similarities for pairs of authors who are known to be different. the between-author similarity for the two authors is 1  a number which is expected by chance fewer than six times in a thousand  so it is very likely that the two are the same person.
¡¡in conclusion  the contributions of this paper are three: we identified a class of problems that we call  disentangling the threads   in which several processes generate a single ordered or partially-ordered sequence of observations and the challenge is to assign each observation to its generating process. we developed a simple  greedy algorithm for solving the problem. and we introduced a new relational similarity metric that seems to work better than the dice coefficient and other common metrics that rely on overlapping sets of items. we presented four experiments that suggest the metric and the pubtree algorithm do a credible job of disentangling the threads in the publication records of researchers. many improvements to the algorithm are possible  and yet  even the simple version presented here performs very well.
1 acknowledgments
thanks to wesley kerr for help with the citeseer corpus and to wesley kerr  sinjini mitra and aram galstyan for preliminary work on the problem of constructing career histories. thanks to padhraic smyth for providing access to his topic model software. special thanks to the three authors who painstakingly analyzed their own publication trees: prof. adele howe  prof. tim oates and prof. robert st. amant. this work was supported by contract #1*s1 from ibm.
