
deciding what question to branch on at each node is a key element of search algorithms. we introduce the information-theoretic paradigm for branching question selection. the idea is to drive the search to reduce uncertainty  entropy  in the current subproblem. we present four families of methods that fall within this paradigm. in the first  a variable to branch on is selected based on lookahead. this method performs comparably to strong branching on miplib  and better than strong branching on hard real-world procurement optimization instances on which cplex's default strong branching outperforms cplex's default branching strategy. the second family combines this idea with strong branching. the third family does not use lookahead  but instead exploits the tie between indicator variables and the other variables they govern. this significantly outperforms the state-ofthe-art branching strategies. the fourth family is about branching using carefully constructed linear inequality constraints over sets of variables.
1 introduction
search is a fundamental technique for problem solving in ai and operations research  or . at a node of the search tree  the search algorithm poses a question  and then tries out the different answers  which correspond to the branches emanating from that node . many different ways of deciding which question to pose  branching strategies  have been studied.
﹛we introduce a new paradigm for developing branching strategies  employing information theory as the principle that guides the development. the strategies aim to reduce the uncertainty  entropy  in the current subtree. in the context of solving integer programs  we develop four high-level families of strategies that fall within this paradigm  and show that some of them significantly improvespeed overexisting strategies. the rest of this section covers the needed background.
1 integer programming
one of the most important computational problems in cs and
or is integerprogramming. applicationsof integerprogramming include scheduling  routing  vlsi circuit design  and facility location  nemhauser and wolsey  1 . integer programming is the problem of optimizing a linear function subject to linear constraints and integrality constraints on some of the variables. formally:
definition 1  1 integer programming 
given an n-tuple c of rationals  an m-tuple b of rationals  and an m ℅ n matrix a of rationals  the 1 integer programming problem is to find the n-tuple x such that ax ≒ b  x ﹋ {1}n  and c ﹞ x is minimized.
if some variables are constrained to be integers  not necessarily binary   then the problem is simply called integer programming. if not all variables are constrained to be integral  they can be real   then the problem is called mixed integer programming  mip . otherwise  the problem is called pure integer programming.
﹛while  the decision version of  mip is np-complete  karp  1   there are many sophisticated techniques that can solve very large instances in practice. we now review the existing techniques upon which we build our methods.
branch-and-bound
in branch-and-bound search  the best solution found so far  the incumbent  is kept in memory. once a node in the search tree is generated  a lower bound  aka. an f-estimate  on the solution value is computed by solving a relaxed version of the problem  while honoring the commitments made on the search path so far. the most common method for doing this is to solve the problemwhile relaxing only the integralityconstraints of all undecided variables; that linear program  lp  can be solved fast in practice  for example using the simplex algorithm  and in polynomial worst-case time using interiorpoint methods . a path terminates when the lower bound is at least the value of the incumbent. once all paths have terminated  the incumbent is a provably optimal solution.
﹛there are several ways to decide which leaf node of the search tree to expand next. for example  in depth-first branch-and-bound the most recent node is expanded next. in a* search  hart et al.  1   the leaf with the lowest lower bound is expanded next. a* is desirable in the sense that  for any given branch question ordering  no tree search algorithm that finds a provably optimal solution can guarantee expanding fewer nodes  dechter and pearl  1 .  an almost identical node-selection strategy  best-bound search  is often used in mip  wolsey  1 .1  therefore  in the experiments we will use best-bound search in all of the algorithms.
branch-and-cut
a more modern algorithm for solving mips is branch-andcut  which first achieved success in solving large instances of the traveling salesman problem  padberg and rinaldi  1; 1   and is now the core of the fastest commercial generalpurpose integer programmingpackages. it is like branch-andbound  except that in addition  the algorithm may generate cutting planes  nemhauser and wolsey  1 . they are constraints that  when added to the problem at a search node  result in a tighter lp polytope  while not cutting off the optimal integer solution  and thus a higher lower bound. the higher lower bound in turn can cause earlier termination of the search path  and thus yields smaller search trees.
﹛cplex  ilog inc  1  is the leading commercial software product for solving mips. it uses branch-and-cut. it can be configured to support many different branching algorithms  and it makes available low-levelinterfaces for controlling the search. we developed our branching methods in the framework of cplex  allowing us to take advantage of the many components already in cplex  such as the presolver  the cutting plane engine  the lp solver  etc.  while allowing us flexibility in developing our own methods.  we used cplex version 1  i.e.  the newest version at the time of the experiments.  we configured the search order to best-bound  and we vary the branching strategies as we will discuss.
selecting a question to branch on
at every node of a tree search  the search algorithm has to decide what question to branch on  and thus what the children of the node will be . the bulk of research has focused on branchingon individualvariables because that is intuitive  has a relatively small set of branching candidates  and tends to keep the lp sparse and thus relatively fast to solve. in other words  the question is:  what should the valueof this variable be  . the children correspond to different answers to this question.
﹛one commonlyused method in or is to branch on the most fractional variable  i.e.  variable whose lp value is furthest from being integral  wolsey  1 . finding the branching variable under this rule is fast and this method yields small search trees on many problem instances.
﹛a more sophisticated approach  which is better suited for certain hard problem instances  is strong branching  applegate et al.  1 . the algorithm performs a one-step lookahead for each variable that is non-integral in the lp at the node. the one-step lookahead computations solve the lp relaxation for each of the children.1
algorithm 1 strong branching  sb  1. candidates ↘ {i | xi fractional}
1. for each	:
 a  f ↘	i	i
 b  let zl be solution of current lp with.
 c  let zu be solution of current lp with.
 d  score xi  ↘ 1 ﹞ min{zl zu} + max{zl zu}
1. i  ↘ argmaxi﹋candidates score xi 
1. return i 
there are many different ways that step 1.d could be performed in algorithm1. the abovemethod is from  applegate et al.  1 . we experimented with the following variations in addition to the method above:
1. score xi  ↘ min{zl zu} + 1 ﹞ max{zl zu}
1. score xi  ↘ zl + zu
1. score xi  ↘ max{zl zu}
1. score xi  ↘ min{zl zu}
1. score xi  ↘  1   xi zl + xizu
in our preliminary experiments  there was no variation that dominated any of the others. we therefore decided to go with the variation in algorithm 1  which has been shown to perform well in practice  applegate et al.  1 .
1 the information-theoretic paradigm to branching in search
the simple key observation behind our paradigm is that in the beginning of a search  the nodes on the frontier of the search tree have large amounts of uncertainty about the variables' values  while at the end of a search there is none  a path ends once there is no uncertainty left in a node's variable assignments 1. motivated by this observation  our paradigm is to guide the search so as to remove uncertainty from the nodes on the frontier of the search tree. in this paper we apply this paradigm to decide what question should be branched on at each search node. while there has been much work on search  and specifically on developing branching heuristics   to our knowledge this is the first work that takes an information-theoreticapproach to guiding the search process. another view to this paradigm is that we use informationtheoretic measures to quantify how much propagation a candidate branching question would cause-not only counting how many of the unassigned variables would be affected  but also by how much.
﹛specifically in the context of mip  the idea behind our paradigm is to treat the fractional portion of integerconstrained variables in the lp solution as probabilities  indicating the probability with which we expect the variable to be greater than its current value in the optimal solution. clearly  interpreting lp variable values as independent probabilities

of lps that need to be solved. similarly  the child lps are not solved to optimality; the amount of work  such as the number of simplex pivots  to perform on each child is a parameter. we will vary both of these parameters in our experiments. 1
﹛﹛in addition to all variables getting fixed value assignments  a path can end by infeasibility  the assignments so far implying a contradiction  or by pruning by bound  the optimistic value of the node being no better than the value of the incumbent .
is an enormous inaccurate assumption  and it is one that we approach with caution. due to the constraints in the problem  the variables are indeed interdependent. however  because we are not attempting to derive theoretical results related to this assumption  and because we use the assumption only in deciding how to branch within a search framework  and thus we still guarantee optimality   this assumption does not negatively interfere with any of our results. as we demonstrate later in our experiments  this assumption works well in practice  so it is not without merit.  interpreting lp variables as probabilities is also used successfully in randomized approximation algorithms  vazirani  1 . 
﹛before we describe any specific families of branch question selection techniques underthis paradigm it will be useful to define how we can quantify the  uncertainty  of a partial solution. for this  we borrow some definitions from information theory  from which the primary contribution is the notion of entropy  shannon  1   which measures the amount of uncertainty in a random event. given an event with two outcomes  say 1 or 1   we can compute the entropy of the event from the probability of each outcome occurring.
definition 1  entropy of a binary variable 
consider an event with two outcomes  1 and 1. let x be the probability of outcome 1 occurring. then 1   x is the probability of outcome 1 occurring and we can compute the entropy of x as follows:
.
﹛it is possible to use other functions to measure the uncertainty in a binary variable. for example  and e x  = x   x1 could alternatively be used. in the context of the first family of branching question selection techniques  discussed below   we experimented using these other functions but found no major improvement compared to using e x  as in definition 1. thus  throughout the rest of the paper  we will use this standard way of calculating entropy.
﹛entropy is additive for independent variables so we compute the entropy of a group of variables as follows:
definition 1  entropy of a group of binary variables  given a set x of probabilities corresponding to independent binary events  we can compute the entropy of the set as:

where e x  is as in definition 1.
﹛while it is possible for there to be multiple optimal solutions to an optimization problem  all optimal solutions will have zero uncertainty according to this measure. we are now ready to present our fourfamilies of branchingquestion selection techniques. they all fall under the information-theoretic paradigm for branching.
1 family 1: entropic lookahead for variable selection
in the first family  we determine the variable to branch on using one-step lookahead as in strong branching. the difference is that instead of examining the objective values of potential child nodes  we examine the remaining uncertainty  entropy  in potential child nodes  choosing a variable to branch on that yields children with the least uncertainty.
algorithm 1 entropic branching  eb  1. candidates ↘ {i | xi fractional} 1. for each:
 a  xf ↘ xi	xi
 b  let x l be solution vector of lp with.
.
j
1. i  ↘ argmini﹋candidates entropy xi 
1. return i 
﹛eb is usable on any mip since it does not make any assumptions about the underlying model for the problem on which it is used.
﹛eb can be modified to perform more than one-step lookahead in the obvious way. this would likely lead to smaller search trees but more time would be spent per node. one way of mitigating this tradeoff would be to conduct deeper lookaheads only on candidates that look promising based on shallower lookaheads. one could even curtail the candidate set based on heuristics before any lookahead is conducted.
﹛for illustrative purposes  there is an interesting  though not exact  analogy between our entropic lookahead method for question selection at search nodes and algorithms for decision tree induction  quinlan  1 . in most recursive decision tree induction algorithms  a question is inserted at a leaf that results in the greatest information gain. similarly  in search  by choosing questions whose children have the least entropy  we are creating children that in a sense also result in the greatest information gain.
1 family 1: hybridizing sb and eb
as can be observed from the pseudocode given in algorithms 1 and 1  sb and eb are computed quite similarly. a natural question arises: can we develop a hybrid approach combining the strengths of both without using significantly more computational resources  in this section we answer this question in the affirmative by introducing a second family of variable selection strategies. in this family  sb and eb are hybridized in different ways. each of these methods requires only a small amount of additional computation compared to performing only sb or eb  because the same lookahead with the same child lp solves is used . we classify our hybrid approaches into two categories: tie-breaking and combinational.
1 tie-breaking methods
in this approach  we first perform the sb computations as in algorithm 1  but instead of simply branching on the variable with best score  we break ties using an entropy computation. since we have already computed the lp relaxations for each branch  computing the entropy is a negligible computational cost  relative to computing the lp relaxations . in addition to breaking exact ties  we also experimented with the approach of considering two variables having sb scores within x% of each other as tied. in the experiments  described below  we tested with x ﹋ {1 1}.
1 combinational methods
we present two methods of combining information from sb and eb in order to compute a single score for a variable. the variable with the best score will then be branched on.
﹛the first method  rank  performs the computation for sb and eb first.  again  these computations are performed simultaneously at little additional cost beyond doing either sb or eb alone.  define ranksb xi  to be the rank of variable xi in terms of its sb score  i.e.  the variable with the largest score would have rank 1  the variable with the second-largest score would have rank 1  and so on . similarly  define rankeb xi  to be the rank of variable xi in terms of its eb entropy. then  for each variable  we let rank xi  = ranksb xi +rankeb xi  and choose the variable xi with the smallest rank.
﹛the second method  comb 老  1   老   computes a convex combination of the sb score  with weight 老  and the current entropy minus the eb score  with weight 1 老 . it then selects the variable with the highest final score.
1 experiments on eb and the hybrid methods
we conducted a host of experiments with the search methods described above  both on miplib and real-world procurement optimization instances. in all of our experiments the algorithms ran in main memory  so paging was not an issue.
﹛in order to be able to carefully and fairly control the parameter settings of both sb and eb  we implemented both.  using cplex's default sb would not have allowed us to controlthe proprietaryand undocumentedcandidate selection method and scoring function  and cplex does not provide adequateapis to use those same methods for eb.  implementation of both algorithmsin the same codebasealso minimizes differences in implementation-related run-time overhead.
experiments on miplib 1
miplib  bixby et al.  1  is a library of mip instances that is commonly used for benchmarking mip algorithms. we experimented on all instances of the newest miplib  1 .
﹛eb and sb were comparable: they reached the 1-hour time limit on 1% and 1% of the instances  respectively. eb outperformed sb on 1 of the 1 instances. this is quite remarkable in the sense that eb does not use the objective function of the problem at all in making branching decisions. our experiments show that  on average  rank is the best among the hybrid methods. on 1 of the instances  at least one of the hybrid methods outperformed both sb and eb  showing that the combination is better than either of its parts. although eb performs comparably to sb  both of these strategies are dominated on miplib by cplex's default branching strategy  although it searches a larger number of nodes . for problems with lots of structure  we expect the reverse to be true. indeed  in the next subsection we use instances on which cplex's default sb outperforms cplex's default branching strategy  and we demonstrate that eb outperforms sb on that data set.
experiments on real-world procurement optimization
as we showed in the previous section  lookahead  such as in strong branching  does not pay off on all classes of problems. to obtain a pertinent evaluation of eb against the state-ofthe-art lookahead-based technique  sb  we wanted to test on a problem set on which sb is the algorithm of choice  compared to non-lookahead-based standard techniques . we ran experiments on combinenet  inc.'s repository of thousands of large-scale real-world industrial procurement optimization instances of varying sizes and structures  and selected the instances on which cplex's implementation of sb was faster than cplex's default branching strategy. on those 1 instances  cplex's implementation of sb was on average 1% faster than cplex's default branching strategy. thus we concluded that sb is a good algorithm for that data set  and performed a comparison of eb against sb on that data.
﹛tables 1 summarize the experimental results for eb. we varied the size of the candidate list and the number of simplex iterations performed in the dual lp of each child of the candidate.1 when limiting the size of the candidate list  we choose the candidates in order of most fractional.
candidates1 iters1 iters1 itersunlimited iters1.1.1.1.1unlimited1111table 1: average computation time over 1 instances for entropic branching with a 1-hour time limit.
candidates1 iters1 iters1 itersunlimited iters111unlimited1111table 1: median computation time over 1 instances for entropic branching with a 1-hour time limit.
candidates1 iters1 iters1 itersunlimited iters111unlimited11table 1: number of instances  of 1  taking over an hour.
﹛as the tables demonstrate  eb was fastest with the most detailed branch selection. the additional work in performing more simplex iterations pays off by giving a more accurate estimate of the entropy of the individual variables. similarly  by examining more candidate variables  eb is more likely to branch on a variable that decreases the total amount of entropy the most. this suggests that a better method for choosing the candidate list could lead to further speedup. when both methods were allowed unlimited candidates and iterations  eb was 1% faster than sb: the comparable number to eb's 1 average seconds was 1 for sb.
1 family 1: entropic lookahead-free variable selection
we introduce a third family of branching strategies  again within the entropy-based branching paradigm. this method is computationally less expensive than the methods we presented so far because it does not use lookahead. however  it does require an advanced knowledge of the structure of the problem. the problem with which we experiment is motivated by a real-world electronic commerce application: a combinatorial procurement auction  aka. reverse auction  where the buyer specifies the maximum number of winning suppliers  davenport and kalagnanam  1; sandholm and suri  1 .
definition 1  combinatorial procurement auction with maximum winners constraint 
let m = {1 ... m} be the m goods that the buyer wishes to procure  the buyer wants at least one unit of each good . let s = {1 ... s} be the s suppliers participating in the procurement and let b = {b1 ... bn} be the bids  where  indicates that supplier si can supply the bundle of goods gi   m at price pi. finally  the buyer indicates the maximum number of winners  k. the winner determination problem is to identify the winning bids so as to minimize the buyer's cost subject to the constraints that the buyer's demand is satisfied and that the maximum number of winners constraint is satisfied.
this problem is np-complete  even if the bids are on single items only  sandholm and suri  1 . integer programming methods have been successfully used previously in winner determination research  e.g.  andersson et al.  1; sandholm et al.  1; sandholm  1    and we can very naturally formulate the above generalized winner determination problem as a mip:
	minimize	
such thatj ﹋ {1 ...m} j ﹋ {1 ... s}
i ﹋ {1 ... n} j ﹋ {1 ... s}
the formulation is typical of a common class of problems in which binary  indicator  variables-the yj variables in the formulation above-are used to model logical connectives  nemhauser and wolsey  1 . constraints that state that at most  or exactly  k variables from a set of variables can be nonzero are an important special case.  the case k = 1 is called a special-orderedset of type i.  typically  the lp relaxation gives poor approximate values for indicator variables: due to big-m's in the constraints that include the indicators  and even with large m's that are as small as possible as in the above formulation   an indicator can in effect be on even while taking a tiny value  or conversely  be off while holding a value close to 1 . as we show  the branching method that we propose helps significantly to address this problem.
1 branching strategy
the hardness in this problem comes primarily from determining the winning set of suppliers. in terms of the above mip  we need to determine the yj values. the main idea in our branching strategy for this problem is to branch on yj values that correspond to suppliers about which we are most uncertain. but rather than deriving this uncertainty from the variable yj  for which the lp gives very inaccurate values   we derive it from the variables corresponding to supplier bids. the branching strategy works as follows. for each supplier j where yj ﹋ {/ 1}  compute  and branch on the variable
where = argminj entropy j . this strategy does not use lookahead: it only uses the lp values of the current search node. we call this branching strategy indicator entropic branching  ieb .
1 experimental results
although this problem is motivated by a real-world application  there are no publicly available real-world instances  the real-world data studied in the previous section does not exactly fit this model .1 instead we created an artificial instance distribution for this problem which closely approximates the real-world problem.
﹛given parameters s  number of suppliers   r  number of regions   m  goods per region   and b  bids per region  we create an instance of a procurement auction with a maximum winners constraint as follows: 1  each bidder bids on a region with probability 1; 1  for each region  generate the bidder's bids using the decay distribution  sandholm  1  with 汐 = 1.1 for this data distribution  we determined that cplex's default branching strategy was the best of the four qualitatively different branching strategies that cplex offers. in particular  it was faster than strong branching on this data. table 1 shows experimental results comparing ieb with cplex using its default branching strategy. the results indicate that ieb performs significantly better  and the relative difference increases with problem size.  we also found that part of the benefit can be achieved even without entropic branching by simply forcing branching on every path to occur on the fractional indicator variables first before branching on any other variables. other than that  we let cplex make the variable selection in that strategy  cplex-if  .
srmbkcplexcplex-ifieb111.1.1.111111111.1%1%1%table 1: the first two rows contain the solution time  in seconds  for finding the optimal solution and proving optimality averaged over 1 instances  there were no timeouts . the third row indicates the average integrality gap  how far from optimal  at worst  the best solution found so far is  after one hour.
1 family 1: entropic lookahead for multi-variable branches
in this section we introduce a fourth family of methods for determining a good question to branch on. in eb  we performed a one-step lookahead for each non-integral variable. here  we generalize entropic lookahead beyond branching on variables. in integer programming one can branch on the sum of the values of a set of variables. for example  if the lp relaxation at the current search node has xi = 1 and xj = 1  we could set one branch to be xi + xj ≒ 1 and the other branch to be xi + xj ≡ 1. in general  given a set x of variables and the current lp relaxation solution x   we can let and we can generate the branches
 then  instead of
branching on the variable with the smallest amount of entropy in its child nodes  we select the set x of variables for branching that results in the smallest amount of entropy in the two child nodes.1 in step 1.d of eb  we weighted the entropy of each child by the probability that we expect the optimal solution to occur in each child. in the multi-variable case  we still perform this weighting  but it is more complicated since the probability of each branch depends on several variables. the probability that the sum is less than k is the summation of a combinatorial number of products of probabilities; this number is exponential only in |x|  so it is not prohibitive for generating branches with small numbers of variables. 
﹛while branching on more than one variable at a time may seem unintuitive it does not cause any obviousloss in branching power:
proposition 1 assume that each integer variable xi has finite domain di. ignoring the effects of pruning  by infeasibility  bound  and lp integrality   propagation  by lp   and learning  e.g.  from conflicts in subproblems  achterberg  1; sandholm and shields  1    the number of leaves in the tree is the same regardless of how many  and which 

﹛﹛﹛1no other value of k is worth considering: any other integer value would cause one child's lp optimum to be exactly the same as the node's. thus that child's eb analysis  or sb analysis  or in fact any branching rule based solely on the node's local information  would be the same as that of the node's  leading to an infinitely deep search path and non-termination of the overall algorithm.
﹛﹛﹛1the special case k = 1 of this branching question has been shown to be effective in set partitioning  set packing  and set covering problems  etcheberry  1; ryan and foster  1; ryan  1 . for such 1 problems  theory has been developed that shows that each path consisting of carefully selected such branches will yield an lp with all vertices integral after a quadratic number of branches in the number of constraints  barnhart et al.  1 . future research includes exploring the use of techniques from our family 1 to select among such branching candidates in those problems. 1
﹛﹛more generally  one can branch on the disjunction pi﹋x aixi ≒ r versus pi﹋x aixi ≡ r + 1  where the constants ai can be positive or negative  as long as there are no integer solutions between those two hyperplanes  e.g.   owen and mehrotra  1  . our entropy reduction measure could then be used to select from among such pairs of hyperplanes. branching on high-level properties has also been shown efficient in constraint satisfaction problems  gomes and sellmann  1   and it would be interesting to try the information-theoretic paradigm for branch question selection in that context as well.
     1we never branch on a set of variables x if pi﹋x x i happens to be integral because one of the branches will not constrain the current lp solution. thus the corresponding child node will be identical to its parent  leading again to nontermination with any branching rule that is based on location information.  in fact  we do not even conduct the lookahead for such x. 
variables are used in different branches-as long as trivial branches where a child is identical to its parent are not used. if the branching factor is two  then this equality applies to the number of nodes in the tree as well.
proof. there are solutions. for any nontrivial branching  that is the number of leaves. binary trees with the same number of leaves have the same number of nodes.  we performed experiments on miplib 1 and on combinatorial auction winner determination problem instances  again from the decay distribution . we limited our algorithm to considering branches containing just 1 or 1 variables in order to keep the number of candidate branching questions small so as to not have to solve too many child lps. this also helps keep the lp sparse and thus relatively fast to solve.
﹛while we found that this strategy led to trees that are slightly smaller on average than when branching on individual variables only  the computationaleffort needed to perform the lookahead for pairs of variables was not worth it in terms of total search time. it thus seems that in orderfor this method to be effective  there needs to be some quick way of determining  before lookahead  what good candidate variable sets to branch on might be  and to only conduct the lookahead on them. we also tried randomly picking only a restricted number of variable pairs as candidates; even though that helped in overall run-time  it did not help enough to beat branching on individual variables only. hopefully future research will shed light on what might be considered good multi-variable branching candidates.
1 conclusions  discussion  and future research
we introduced a new paradigm for branch selection in search based on an information-theoreticapproach. in the beginning of a search  there is the most uncertainty about the optimal solution. when the search is complete  there is zero uncertainty. using this observation  we developed four families of methods for selecting what question to branch on at a search node so as to reduce the amount of uncertainty in the search process. all four of our families of methods are informationtheoretically motivated to reduce remaining entropy. in the first family  a good variable to branch on is selected based on lookahead. experiments show that this entropic branching method performs comparably to strong branching  a classic technique that uses lookahead and lp-bounds to guide the search  on miplib  and better than strong branching on hard real-world procurementoptimization instances on which cplex's default strong branching outperforms cplex's default branching strategy. the second family combines this idea with strong branching in different ways. the third family does not use lookahead  but instead exploits the tie between indicator variables and the other variables they govern. experiments show that this family significantly outperforms the state-of-the-art branching strategies. the fourth family is about branching using carefully constructed linear inequality constraints over sets of variables.
﹛one can view many existing search methods as quick approximations of entropy-based search. first  the classic or idea of branching on the variable that has the most fractional lp value at the node in best-bound search  wolsey  1  is a lookahead-freeapproximation to entropy-based variable selection: it also tries to branch on the variable that the lp is most uncertain about and thus that branch should reduce uncertainty the most. second  using entropy as the f-function in a* search  i.e.  always picking the node with least entropy to expand next  tends to make a* more like depth-firstsearch because deeper nodes tend to have less entropy.  however  entropy does not always decrease monotonically even along one search path.  third  the most common heuristic in constraint satisfaction problems  most-constrained-variablefirst  bitner and reingold  1   and its usual tie-breaker  the most-constraining-variable-first heuristic  bre∩laz  1   approximate entropy-based variable selection: they tend to assign a variable that affects the other unassigned variables the most  thus reducing the entropy of those variables.
﹛there are several promising directions for future research down this path. one could develop quick heuristics to curtail the set of candidate questions to branch on before lookahead. one could even narrow down the candidate set incrementally by deeper and deeper lookahead  the leaves being evaluated using our entropic measure .
﹛it would also be interesting to try the paradigm on additional problems  for example  on set covering  packing  and partitioning problems-perhapsusing the specialized branching constraints  discussed above  as the candidates.
﹛our methods were largely derived for a*  best-bound  search where it is best to branch on a question about which the algorithm is most uncertain; in contrast  in the depth-first node selection strategy it is often best to branch on a question for which the algorithm knows the right answer with high confidence  sandholm et al.  1; sandholm  1 . thus it is not clear that these branch question selection techniques would work as well under the depth-first strategy. that is a future question to pursue. one could also try our paradigm on constraint satisfaction problems; given that there are no lp values  one would need another way of measuring entropy.
﹛while we introduced four families of branch question selection techniques under the information-theoretic paradigm  we see no reason to believe that these are the only  or best  families. future research should explore the development of additional families of entropy-motivated branch question selection techniques. perhaps the information-theoretic paradigm can be helpful in search guidance beyond branch question selection as well.
