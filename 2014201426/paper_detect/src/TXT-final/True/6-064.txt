 
information extraction can be defined as the task of automatically extracting instances of specified classes or relations from text. we consider the case of using machine learning methods to induce models for extracting relation instances from biomedical articles. we propose and evaluate an approach that is based on using hierarchical hidden markov models to represent the grammatical structure of the sentences being processed. our approach first uses a shallow parser to construct a multi-level representation of each sentence being processed. then we train hierarchical hmms to capture the regularities of the parses for both positive and negative sentences. we evaluate our method by inducing models to extract binary relations in three biomedical domains. our experiments indicate that our approach results in more accurate models than several baseline h m m approaches. 
1 	introduction 
in many application domains  there is the potential to greatly increase the utility of on-line text sources by using automated methods for mapping selected parts of the unstructured text into a structured representation. for example  the curators of genome databases would like to have tools that could accurately extract information from the scientific literature about entities such as genes  proteins  cells  diseases  etc. for this reason  there has been much recent interest in developing methods for the task of information extraction  ie   which can be defined as automatically recognizing and extracting instances of specific classes of entities and relationships among entities from text sources. 
   machine learning methods often play a key role in ie systems because it is difficult and costly to manually encode the necessary extraction models. hidden markov models  hmms   leek  1; bikel et al  1; freitag and mccallum  1   and related probabilistic sequence models  mccallum et at  1; lafferty et al.y 1   have been among the most accurate methods for learning information extractors. most of the work in learning hmms for information extraction has focused on tasks with semi-structured and other text sources in which english grammar does not play a key here we report the identification of an integral membrane ubiquitin-conjugating enzyme. this enzyme  ubc1  localizes to the endoplasmic reticulum  with the catalytic domain facing the cytosol. 
subcellular-localization ubc1 endoplasmic reticulum  
figure 1: an example of the information extraction task. the top of the figure shows part of a document from which we wish to extract instances of the subcellular-localization relation. the bottom of 
the figure shows the extracted tuple. 
role. in contrast  the task we consider here is extracting information from abstracts of biological articles  hirschman et a/.  1 . in this domain  it is important that the learned models are able to represent regularities in the grammatical structure of sentences. 
   in this paper  we present an approach based on using hierarchical hidden markov models  hhmms   fine etal  1  to extract information from the scientific literature. hierarchical hidden markov models have multiple  levels  of states which describe input sequences at different levels of granularity. in our models  the top level of the hmms represent sentences at the level of phrases  and the lower level of the hmms represent sentences at the level of individual words. 
our approach involves computing a shallow parse of each sentence to be processed. during training and testing  the hierarchical hmms manipulate a two-level description of the sentence parse  instead of just processing the sentence words directly. we evaluate our approach by extracting instances of three binary relations from abstracts of scientific articles. our experiments show that our approach results in more accurate models than several baseline approaches using hmms. 
   an example of a binary relation that we consider in our experiments is the subcellular-localization relation  which represents the location of a particular protein within a cell. we refer to the domains of this relation as protein and location. we refer to an instance of a relation as a tuple. 
figure 1 provides an illustration of our extraction task. the top of the figure shows two sentences in an abstract  and the bottom of the figure shows the instance of the target relation 

information extraction 	1 

subcellular-localization that we would like to extract from the second sentence. this tuple asserts that the protein ubc1 is found in the subcellular compartment called the endoplasmic reticulum. in order to learn models to perform this task  we use training examples consisting of passages of text  annotated with the tuples that should be extracted from them. 
   in earlier work  ray and craven  1   we presented an approach that incorporates grammatical information into single-level hmms. the approach described in this paper extends the earlier work by using hierarchical hmms to provide a richer description of the information available from a sentence parse. 
   hierarchical hmms originally were developed by fine et al.  1   but the application of these models to information extraction is novel  and our approach incorporates several extensions to these models to tailor them to our task. bikel el al.  1  developed an approach to named enlily recognition that uses hmms with a multi-level representation similar to a hierarchical h m m . in their models  the top level represents the classes of interest  e.g. person name   and the bottom level represents the words in a sentence being processed. our approach differs from theirs in several key respects:  i  our input representation for all sentences being processed is hierarchical   ii  our models represent the shallow phrase structure of sentences   iii  we focus on learning to extract relations rather than entities   iv  we use null models to represent sentences that do not describe relations of interest  and  v  we use a discriminative training procedure. miller el al.  1  developed an information-extraction approach that uses a lexicalized  probabilistic context-free grammar  lpcfg  to simultaneously do syntactic parsing and semantic information extraction. the genre of text that we consider here  however  is quite different from the news story corpus on which available lpcfgs have been trained. thus it is not clear how well this intriguing approach would transfer to our task. 
1 	sentence representation 
in most previous work on hmms for natural language tasks  the passages of text being processed have been represented as sequences of tokens. a hypothesis underlying our work is that incorporating sentence structure into the learned models will provide better extraction accuracy. our approach is based on using syntactic parses of all sentences to be processed. in particular  we use the sundance system  riloff  1  to obtain a shallow parse of each given sentence. 
　the representation we use in this paper does not incorporate all of the information provided by the sundance parser. instead our representation provides a partially  flattened   two-level description of each sundance parse tree. the top level represents each sentence as a sequence of phrase segments. the lower level represents individual tokens  along with their part-of-speech  pos  tags. in positive training examples  if a segment contains a word or words that belong to a domain in a target tuple  the segment and the words of interest are annotated with the corresponding domain. we iefer to these annotations as labels. test instances do not contain labels - the labels are to be predicted by the learned ie model. figure 1 shows a sentence containing an instance of the figure 1: input representation for a sentence which contains a subcellular-localization tuple: the sentence is segmented into typed phrases and each phrase is segmented into words typed with part-of-speech tags. phrase types and labels arc shown in column  a . word part-of-speech tags and labels arc shown in column  b . the words of the sentence are shown in column  c . note the grouping of words in phrases. the labels  protein  location  are present only in the training sentences. 
subcellular-localization relation and its annotated segments. the sentence is segmented into typed phrases and each phrase is segmented into words typed with part-of-speech tags. for example  the second phrase segment is a noun phrase 
 np.segment  that contains the protein name ubc1  hence the protein label . note that the types are constants that are pre-defined by our representation of sundance parses  whereas the labels are defined by the domains of the particular relation we are trying to extract. 
1 	hierarchical hmms for information extraction 
a schematic of one of our hierarchical hmms is shown in 
1 	information extraction figure 1. the top of the figure shows the positive model  which is trained to represent sentences that contain instances of the target relation. the bottom of the figure shows the null model  which is trained to represent sentences that do not contain relation instances  e.g. off-topic sentences . at the  coarse  level  our hierarchical hmms represent sentences as sequences of phrases. thus  we can think of the top level as an h m m whose states emit phrases. we refer to this h m m as the phrase hmm  and its states phrase states. at the  fine  level  each phrase is represented as a sequence of words. this is achieved by embedding an h m m within each phrase state. we refer to these embedded hmms as word hmms and their states as word states. the phrase states in figure 1 are depicted with rounded rectangles and word states are depicted with ovals. to explain a sentence  the h m m would first follow a transition from the start state to some phrase state qi  use the word h m m of qi to emit the first phrase of the sentence  then transition to another phrase state qj  emit another 

figure 1: schematic of the architecture of* a hierarchical hmm for the subcellular-localization relation. the top part of the figure shows the positive model and the bottom part the null model. phrase states are depicted as rounded rectangles and word states as ovals. the types and labels of the phrase states are shown within rectangles at the bottom right of each state. labels are shown in bold and states associated with non-empty label sets are depicted with bold borders. the labels of word states are abbreviated for compactness. 
phrase using the word h m m of q1 and so on until it moves to the end state of the phrase h m m . note that only the word states have direct emissions. 
   like the phrases in our input representation  each phrase state in the h m m has a type and may have one or more labels. each phrase state is constrained to emit only phrases whose type agrees with the state's type. we refer to states that have labels associated with them as extraction states  since they are used to predict which test sentences should have tuples extracted from them. 
   the architectures of the word hmms are shown in figure 1. we use three different architectures depending on the labels associated with the phrase state in which the word h m m is embedded. the word hmms for the phrase states with empty label sets  figure 1 a   consist of a single emitting state with a self-transition. for the extraction states of the phrase h m m   the word hmms have a specialized architecture with different states for the domain instances  and for the words that come before  between and after the domain instances  figures 1 b  and 1 c  . all the states of the word hmms can emit words of any type  part-of-speech . that is  they are untyped  in contrast to the typed phrase states. the word states are annotated with label sets  and are trained to emit words with identical label sets. for example  the word 

figure 1: architectures of the word hmms for the subcellularlocalization relation. bold text within states denotes domain labels. for states with implicit empty labels  italicized text within parentheses denotes the position of the state's emissions relative to the domain words. the figure shows  a  the structure of the embedded hmms for phrase states without labels   b   phrase states with one label and  c  phrase states with two labels. 
h m m shown in figure 1 b  can explain the phrase  the endoplasmic reticulum   by following a transition from the start state to the  before  state  emitting the word  the   transitioning to the location state  emitting the words  endoplasmic  and  reticulum  with the location label and then transitioning to the end state. in order for a phrase state to emit a whole phrase  as given by the input representation  and not sequences of words that are shorter or longer than a phrase  we require that the embedded word h m m transition to the end state exactly when it has emitted all the words of a given phrase. thus word hmms will always emit sequences of words that constitute whole phrases and transitions between phrase states occur only at phrase boundaries. 
   the standard dynamic programming algorithms that are used for learning and inference in hmms - forward  backward and viterbi  rabiner  1  - need to be slightly modified for our hierarchical hmms. in particular  they need to  i  handle the multiple-levels of the input representation  enforcing the constraint that word hmms must emit sequences of words that constitute phrases  and  ii  support the use of typed phrase states by enforcing agreement between state and phrase types. 
   the forward algorithm for our hierarchical hmms is defined by the recurrence relationships shown in table 1. the first three equations of the recurrence relation provide a phrase-level description of the algorithm  and the last three equations provide a word-level description. notice that the third equation describes the linkage between the phrase level 

information extraction 	1 


tabic 1: the left side of the table shows the forward-algorithm recurrence relation for our hierarchical hmms. the right side of the table 

defines the notation used in the recurrence relation. 
and the word level. the backward and viterbi algorithms require similar modifications  but we do not show them due to space limitations. 
as illustrated in figure 1  each training instance for our 
hmms consists of a sequence of words  segmented into phrases  and an associated sequence of labels. for a test instance  we would like our trained model to accurately predict a sequence of labels given only the observable part of the sentence  i.e. the words and phrases . we use a discriminative training algorithm  krogh  1  that tries to find model parameters  1  to maximize the conditional likelihood of the labels given the observable part of the sentences: 
		 1  
here sl is the sequence of words/phrases for the zth instance  and c1 is the sequence of labels for the instance. this training algorithm will converge to a local maximum of the objective function. we initialize the parameters of our models by first doing standard generative training. we then apply krogh's algorithm which involves iterative updates to the hmm parameters. to avoid overfitting  we stop training when the accuracy on a held-aside tuning set is maximized. 
　in order for this algorithm to be able to adjust the parameters of the positive model in response to negative instances and vice-versa  we join our positive and null models as shown in figure 1. this combined model includes the positive and 

figure 1: architecture of the combined model. the positive and null models refer to the models in figure 1. 
the null models  shown in figure 1  as its two submodels  with shared start and end states. 
　once a model has been trained  we can use the viterbi algorithm to predict tuples in test sentences. we extract a tuple from a given sentence if the viterbi path goes through states with labels for all the domains of the relation. for example  for the subcellular-localization relation  the viterbi path for a sentence must pass through a state with the protein label and a state with the location label. this process is illustrated in figure 1. 
1 	hierarchical hmms with context features 
in this section we describe an extension to the hierarchical 
hmms presented in the previous section that enables them to represent additional information about the structure of sentences within phrases. we refer to these extended hmms as context hierarchical hmms  chhmms . whereas the hierarchical hmms presented earlier partition a sentence s into disjoint observations  where each  is a word  a chhmm represents s as a sequence of overlapping observations . each observation consists of a window of three words  centered around together with the partof-speech tags of these words. formally  is a vector 
	where 	is the 
part-of-speech tag of word 1ij. note that 	and 
share 	although these features 
are located in different positions in the two vectors. figure 1 shows the vectors emitted for the phrase  the endoplasmic reticulum  by a word hmm in the chhmm. 
　using features that represent the previous and next words allows the models to capture regularities about pairs or triplets of words. for instance  a chhmm is potentially able to learn that the word  membrane   is part of a subcellular location when found in  plasma membrane  while it is not when found in  a membrane . furthermore  by using features that represent the part-of-speech of words  the models are able to learn regularities about groups of words with the same part of speech in addition to regularities about individual words. 

1 	information extraction 


	extracted tuples: 	subcellular-localization mas1  mitochondria  
subcellular-localization mas1  mitochondria  
figure 1: example of the procedure for extracting tuples of the subcellular-localization relation from the sentence fragment  ...mas1 and mas1 are found in the mitochondria... . the top of the figure shows how the most likely path explains the sentence fragment. bold transitions between states denote the most likely path. dashed lines connect each state with the words that it emits. the table shows the label 
sets that are assigned to the phrases and the words of the sentence. the extracted tuples arc shown at the bottom of the figure. 
figure 1: generation of the phrase  the endoplasmic reticulum  by a word hmm in a chhmm. the bold arcs represent the path that 
generates the phrase. the vector observations oi j emitted by each state are shown in the rectangles above the model and are connected with dotted arcs with the emitting state. the word that would be emitted by each state of the equivalent hhmm is shown in boldface. 
the advantages of this representation are especially realized when dealing with an out-of-vocabulary word; in this case part-of-speech tags and neighboring words may be quite informative about the meaning and use of the out-of-vocabulary word. for example  an out-of-vocabulary adjective will rarely be a protein  since proteins are usually nouns. 
　because the number of possible observations for a given word state in a c h h m m is very large  all possible vectors representing sequences of three words and their pos tags   to model the probability of an observation  our chhmms assume that the features are conditionally independent given the state. under this assumption  the probability of the obser-1 	empirical evaluation 
in this section we present experiments testing the hypothesis that our hierarchical hmms are able to provide more accurate models than hmms that incorporate less grammatical infor-
mation. in particular we empirically compare two types of hierarchical hmms with three baseline hmms. 
  context h h m m s : hierarchical hmms with context features  as described in the previous section. 
  h h m m s : hierarchical hmms without context features. 
  phrase h m m s : single-level hmms in which states are typed  as in the phrase level of an hhmm  and emit whole phrases. these hmms were introduced by ray and craven  1 . unlike hierarchical hmms  the states of phrase hmms do not have embedded hmms which emit words. instead each state has a single multinomial distribution to represent its emissions  and each emitted phrase is treated as a bag of words. information extraction 1 
  pos hmms: single-level hmms in which states emit words  but are typed with part-of-speech tags so that a given state can emit words with only a single pos. 
  token hmms: single-level hmms in which untyped states emit words. 
　we evaluate our hypotheses on three data sets that we have assembled from the biomedical literature.1 the data sets are composed of abstracts gathered from the medline database  national library of medicine  1 . the first set contains instances of the subcellular-localization relation. it is composed of 1 positive and 1 negative sentences. the positive sentences contain 1 total tuple instances. the number of actual tuples is 1 since some tuples occur multiple times either in the same sentence or in multiple sentences. the second  which we refer to as the disorder-association data set  characterizes a binary relation between genes and disorders. it contains 1 positive and 1 negative sentences. the positive sentences represent 1 instances of 1 tuples. 
the third  which we refer to as the protein-interaction data set  characterizes physical interactions between pairs of proteins. it is composed of 1 positive and 1 negative sentences. it contains 1 instances of 1 tuples. 
　we use five-fold cross-validation to measure the accuracy of each approach. before processing all sentences  we obtain parses from sundance  and then stem words with porter's stemmer  porter  1 . we map all numbers to a special number token and all words that occur only once in a training set to an out-of-vocab token. also  we discard all punctuation. the same preprocessing is done on test sentences  with the exception that words that were not encountered in the training set are mapped to the out-of-vocab token. the vocabulary is the same for all emitting states in the models  and all parameters are smoothed using m-estimates  cestnik  1j. we train all models using the discriminative training procedure referred to in section 1  krogh  1 . 
　to evaluate our models we construct precision-recall graphs. precision is defined as the fraction of correct tuple instances among those instances that are extracted by the model. recall is defined as the fraction of correct tuple instances extracted by the model over the total number of tuple instances that exist in the data set. for each tuple extracted from sentence s  we calculate a confidence measure as: 

here qn refers to the end state of the combined model  
 1n |s|  is the probability of the most likely path  given by the viterbi algorithm  and an n  is the total probability of the sequence  calculated with the forward algorithm. we construct precision-recall curves by varying a threshold on these confidences. 
　figures 1  1 and 1 show the precision-recall curves for the three data sets. each figure shows curves for the five types of 
　　earlier versions of two of these data sets were used in our previous work  ray and craven  1 . various aspects of the data sets have been cleaned up  however  and thus the versions used here are 
somewhat different. all three data sets arc available from http://www.biostat.wise.edu/ craven/ie/. 


hmms described at the beginning of this section. we show error bars for the context hhmm precision values for the subcellular-localization and protein-interaction data sets. 
for these two data sets  the hierarchical hmm models clearly have superior precision-recall curves to the baseline models. at nearly every level of recall  the hierarchical hmms exhibit higher precision than the baselines. additionally  the hhmms achieve higher endpoint recall values. the results are not as definitive for the disorder-association data set. here  the pos hmms and the token hmms achieve precision levels that are comparable to  and in some cases slightly better than  the context hhmms. there is not a clear winner for this data set  but the context hhmms are competitive. 
　comparing the context hhmms to the ordinary hhmms  we see that the former results in superior precision-recall curves for all three data sets. this result demonstrates that clearly there is value in including the context features in hierarchical hmms for this type of task. in summary  our empirical results support the hypothesis that the ability our hierarchical hmm approach to capture grammatical information about sentences results in more accurate learned models. 

1 	information extraction 


1 	conclusion 
we have presented an approach to learning models for information extraction that is based on using hierarchical hmms to represent the grammatical structure of the sentences being processed. we employ a shallow parser to obtain parse trees for sentences and then use these trees to construct the input representation for the hierarchical hmms 
　our approach builds on previous work on hierarchical hmms and incorporating grammatical knowledge into information-extraction models. the application of hhmms to ie is novel and has required us to modify h h m m learning algorithms to operate on a hierarchical input representation. in particular our methods take into account that phrases and states must have matching types  and that phrase states must emit complete phrases. we have also introduced a novel modification of hhmms in which observations can be feature vectors. with respect to previous work on incorporating grammatical knowledge into ie models  our main contribution is an approach that takes advantage of grammatical information represented at multiple scales. an appealing property of our approach is that it generalizes to additional levels of description of the input text. 
we have evaluated our approach in the context of learning 
ie models to extract instances of three biomedical relations from the abstracts of scientific articles. these experiments demonstrate that incorporating a hierarchical representation of grammatical structure improves extraction accuracy in hidden markov models. 
acknowledgments 
this research was supported in part by nih grant 1 lm1  nsf grant iis-1  and a grant to the university of wisconsin medical school under the howard hughes medical institute research resources program for medical schools. 
