 
     this paper examines the various manifestations of input-expectation discrepancy that occurs in a broad spectrum of research on intelligent behavior. it makes the point that each of the different research activities highlights different aspects of an input-expectation reduction mechanism and neglects others. 
     a comprehensive view of this mechanism has been constructed and applied in the design of a cognitive industrial robot. the mechanism is explained as both a key for machine learning strategies  and a guide for the selection of appropriate memory structures to support intelligent behavior. 
a . 	introduction 
     this paper is an attempt to integrate and unify a spectrum of theories about intelligent behavior. i will make the claim that input-expectation discrepancy reduction is the crux of a generic strategy that underlies intelligent behavior. i then go one step further and argue that this mechanism may itself be a particular example of the even more general strategy of primary drive reduction or fitness enhancement. thus we arrive at a mechanism that derives support from the basic mechanism of life - organic evolution. 
     i attempt to draw together work from cognitive modelling  experimental psychology  ai  and brain modelling. the motivations behind ai work are seldom this broad  perhaps because ai researchers often have an overly parochial view of what is directly relevant and thus of interest. i hope to demonstrate that each of these viewpoints carries with it a set of biases - certain aspects of the phenomenon are emphasized and others are neglected. thus  it is by surveying such a range of approaches that we can hope to construct a reasonably complete and unbiased view of the mechanism of interest. 
b. scenes  scripts  plans  m o p s   t o p s   etc. 
taking a top-down approach  we find schank 
 1  advocating that high-level cognitive activity  such as engaging in day-to-day dialogue  is mediated by a complex set of interrelated memory structures: tops  thematic organization packets   mops 
 memory organization packets   scripts  scenes  and memories. a major part of schank's thesis is that learning is driven by expectation failures from predictions encoded in memory. 
     for schank   the dominant notion in building and altering memory structures is expectation-failure.  when this happens he offers three general possibilities: 
 a  modify specific expectation 
 b  alter script itself 
 c  index as expectation failure 
     these three classes of memory modification correspond to fine tuning  a generalization  and expecting an accepted anomaly in the generalized information  respectively. 
     a first time failure is indexed as an exceptional occurrence. repetition of similar failures suggests that what we believed was exceptional is perhaps quite normal and so a more drastic revision of memory structures is called for: either replacement of an entire structure  or reorganization of the placement of a structure. the decision between these two alternatives is  according to schank  based upon the degree of overall success that we have had with this structure in the past. if it has in general worked well then we keep it and reorganize; if it has not worked well then we will replace it. here we see a first appearance of a 'confidence' measure associated with the learning process  other studies elaborate on this aspect of the general mechanism. 
     notice also that expectation failure is the cue for modifying expectations  no mention is made of modifying the perceptual mechanisms. in the language of a study discussed below  the input and expectation disagree - perhaps we misperceived the input  schank doesn't seem to address this possibility. 
     it is true that in typical schankian contexts  i.e.  restaurants  it is difficult to believe that you could  perceive  that you paid before the food arrived when actually you paid as normal  after eating and just before leaving. but in other contexts  e.g. natural language communication  gross perceptual errors are quite possible. the possibility for error in the processing and interpretation of sensory information adds more complexity to the problem of expectation failure and subsequent learning. 
     schank's term  expectation failure  proclaims this asymmetry - when sensory information and expectation don't agree  it is the expectation that is deemed to have failed. in order to emphasize that there are two sides to this lack of agreement  as in all quarrels  i prefer the term  input-expectation  i - e  discrepancy.  
     to the extent that expectations may also drive top-down perceptual processing  misperception may also be expectation failure. there are two points here: first  by no means is all perception top-down. second  it may still be useful to maintain a separation between 
     
1 d. partridge 
the processing of sensory information and the subsequent comparison with memory structures. 
     the extent to which i - e discrepancy is due to the perceived input  i take i ''s to be interpretations of raw data - in the head or system rather than the world  rather than the expectation being wrong will depend  in general  upon both the unfamiliarity of the perceiver with a particular context  and the extent to which the context is assumed to be strange. thus perceptual errors will be most likely when the perceiver is confident that he is in a familiar context when in fact he isn't. 
     a literary work that always makes this point forcibly to me is the alexandria quartet by lawrence durrell. the first three volumes treat the same general sequence of events from the perspectives of three different people. each of the three perceives the sequence of events in a self-consistent  quite believable manner  given durrell's penchant for the fantastic   but each perceives certain critical events totally differently and therefore has a radically different explanation of what they witnessed. in fact what we are treated to in these books is expectation confirmation  there was no f - e discrepancy despite the fact that there were three very different e 's and only one i on a number of occasions. the point is of course  there were also three different i 's after perceptual processing  and what is more  each individual's expectations influenced the i 's that they perceived. 
     inputs and expectations are not  in general  independent of each other and the dependence works both ways. a last point is that because of this interdependence the occurrence of an expectation failure or an i - t discrepancy depends upon some higher level control: we can refuse to admit that discrepancies exist  or insist on their existence as dictated by some higher level goals. 
     thus to borrow schank's favorite situation  the restaurant: if i eat at a particularly expensive restaurant and the food is poor  i might well resist acknowledging the failure of my expectation that the food will be good because i wish to preserve my general belief that i always spend money wisely. this view of i - e discrepancy takes us into the realm of  dissonance theory   see aronson  1  and falls into the category of inconsistencies between one cognition and a more general  more encompassing cognition. dissonance theory suggests that individuals will strive to reduce such dissonance  for example  by refusing to acknowledge the poor quality of the expensive food. 
     rumelhart and ortony  1  do emphasize topdown processing which leads  from conceptual expectations towards the data in the input where satisfaction of these expectations might be found.  they suggest that finding a good fit between expectations and input  i.e.  minimizing i - e discrepancy  is a critical part of the strategy for selection of appropriate memory structures  schemata  from the enormous number of possible schemata - a context-directed selection process. 
     from this perspective  expectation failure  or more accurately  i - e discrepancy  is not the cue for learning  but for eliminating the schemata responsible for the failed expectations from the set of potentially appropriate schemata for comprehension of the current 
situation. this is  of course  not the function of expectation failure in schank's model at all. 
i am not suggesting that these two theories are contradictory  only that their advocates are emphasizing different aspects of a very complex process. human information processing is a non-trivial combination of top-down and bottom-up mechanisms  and recognition and understanding are not separate processes. 
     rumelhart and ortony summarize the processes:  information  including both the stimulus and the context  enters the system and directly suggests certain plausible candidate schemata to account for it. at the same time as this data driven processing is going on  such postulated schemata activate their dominating schemata  which in turn look for other as yet unsuspected aspects of the situation... a schema is said to provide a good account of  aspects of  the input situation when it can find good evidence for itself.  
the above theorizing falls into the class of the 
 theory development methodology  in cognitive science  miller  1 . a sufficient explanation of cognitive phenomena is being sought without undue concern for the existence of empirical consequences of the theory. 
c. lexical decisions  rt and mismatch detectors  etc. 
     the second class of investigation of an i - e discrepancy reduction mechanism favors the term 'mismatch detector' and epitomizes the alternative to the  theory development methodology  -- it is  theory demonstration methodology.  this approach to theorizing demands empirical testability of a proposed theory  and consequently  the theorizing is limited to highly controlled and thus somewhat artificial phenomena. 
     the evidence for mismatch detectors has been sought in experimental paradigms that involve word comprehension. lexical decision tasks  such as word or nonword discriminations applied to a target string of letters  constitute one source of empirical evidence for and against theories of language understanding. 
     the  verification model   becker  schvanevcldt  and gomez  1  and becker  1  attempts to account for the context effects observed in lexical decision tasks  word or nonword response to a target stimulus that follows a cue stimulus . this model postulates mechanisms that generate two sets of expectations: the sensory set  generated on the basis of sensory features extracted from the target stimulus - i.e.  structural or 'syntactic' similarity   and the semantic set  generated on the basis of a semantic similarity to the cue stimulus . the semantic set is searched first during the verification process which attempts to effect recognition of the target stimulus. 
     becker  1  postulates a  prediction  strategy  when there is a small semantic set size due to the cuetarget pairs being highly related   which we can view as involving a focused expectation  and an  expectancy  strategy  when semantic set size is large   which generates a broader  unfocused expectation. this class of research emphasizes the use of mismatch or i - e discrepancy as a guide to the selection of correct perceptions in the style of rumelhart and ortony  and in sharp contrast to schank - learning behavior is neglected. 
     becker  1  states that it is a common assumption  although not one that goes unchallenged  that the processes isolated in word recognition are the same as those involved in fluent reading skills. his final suggestion is that the types of strategies he describes are 
     
indeed general strategies and thus we should detect their operation in tasks involving  say  the perception of pictures. an event may be divided into a sequence of snapshots. we might then take a pair of snapshots and present them as a cue-target pair. thus in the context of a restaurant  a pair like  order meal/eat meal  might lead to the use of the  prediction  strategy  and  order meal/pay for meal  might yield  expectancy  strategy effects. 
     now the word recognition task paradigm has clearly been stretched up into schank's domain  in more than one sense. but we are  in general  on the way down seeking the more reductionists uses of i - c discrepancy. staying with approximately this level of phenomena  i.e.  word recognition and more generally  reading  we can leave the jungle of rts and %-correct  and examine empirical data that is directly tied to the observable mechanisms of the brain. 
d. 	a l o n g come b r a i n waves 
     the use of computer analysis has enabled the isolation of stimulus-locked segments of the electrical activity of the brain. the electrical activity recorded during and shortly after the presentation or expected presentation of a stimulus is called an event related potential  erp . extensive research has shown that certain components of the erp are sensitive to a person's expectations. in particular  unexpected or novel stimuli are typically followed after some 1 msec  by a positive erp component known as the p1  which has also been shown to be well-correlated with other indices of orienting  e.g. pupil dilation . 
     the subjective probability  or confidence in the expectation  of a stimulus and the value  utility  or relevance or a stimulus are two classes of variables that appear to affect the amplitude of the p1 component. in general  p1 amplitude increases with both the unexpectedness and the value of a stimulus  johnston  1 . p1 has been shown to be influenced by a number of other variables. 
     for my current purposes  p1 appears to be an electrophysiological indication of i - e discrepancy or expectation failure. despite the wealth of erp experiments it is only recently that linguistic material has been used in erp tasks. kutas and hillyard  1  have investigated erps in the context of a sentence reading task. they state that the language comprehension task has often been characterized as a continuous testing and updating of hypotheses about the words that are likely to occur next in a text or conversation. they found that semantically inappropriate words  i.e.  
 he spread the warm bread with socks.   elicited a late negative wave  n1 . this wave may be  they argue  an electrophysiological sign of the  reprocessing  of semantically anomalous information - the result of an i - e discrepancy  in this case  expectation failure. fainsilber  miller  and ortony  1  examined 
n1 to assess whether the detection of anomaly is an integral part of understanding metaphor  and concluded that  understanding a metaphorical comparison appears to involve the registration of a mismatch  whereas understanding of a literal comparison does not.  
     holcomb  1  notes that the n1 findings fit well with becker's verification model: the  n1 component in many ways appears to resemble a semantic mismatch detector. n1 is large when the probability 
d. partridge 	1 
of another word occurring is great  but only if the expectation was based on semantic information. in the present framework  an erp model of semantic context effects  n1 is proposed to represent the activity of an automatic semantic mismatch detector.  
e. d o w n to the neurons 
     moving on down to a more reductionist view of t - e discrepancy  partridge  johnston  and lopez  johnston et al.  1  partridge et al.  1   have theorized  modelled  and presented results of a detailed physiological mechanism for generating expectations and modifying memory structures as a result of expectation failure. 
     the theorizing was based on a number of sources: sokolov  1  suggested that neural 'models' of our expectations are constructed and modified as a result of the  impulses of discrepancy  encountered  an early parallel of schank's suggestions  one that is tied to relatively concrete representational structures but lacks the depth of schank theories ; cell assembly theory  originated by hebb  1  gives us physiologically-based units for distributing and maintaining 'activity' in a neural network; and the existence of empirical data relating the magnitude of the orienting response  or  to variables underlying unexpected stimuli. 
     in addition  the basic learning behaviors accounted for  described below  are sufficiently low-level and ubiquitous in the animal world that we can postulate why they might exist in terms of evolution theory and survival. expectation failure or i - e discrepancy from this biological perspective can be viewed as a specific class of more general mechanisms: a genetically determined motivation to satisfy primary drives  or goals'  - such as find food and avoid pain. stated somewhat simplistically  the importance of satisfying these goals is that they are a basis for survival  and survival is a key idea in the theory of evolution. hence we might reasonably expect the products of evolution to be genetically preprogrammed with efficient mechanisms for satisfying these goals. 
     the final major hypothesis is that the efficient selection and assimilation of useful information is also a 
     fundamental survival goal analogous to the more conventional ones. the key to the selection mechanism is the unexpectedness or novelty of the information. given the uncertainty of the empirical world and an organism that attempts to predict its future  the better predictors will be the survivors   the predictions will sometimes  and to some extent  fail - this mismatch  failure  or discrepancy is the key to the selection mechanism. thus we postulate a novelty drive mechanism  which is just another way of saying mismatch detector or expectation failure mechanism  except that it implies that the mechanism will be analogous to the other basic drive mechanisms such as hunger drive. hence  theories and data pertaining to these conventional drive mechanisms should provide insight into the mechanism of t - e discrepancy reduction and human learning. so we find yet another potential source of information for elucidating human learning mechanisms. 
         rescorla and holland  1  divided basic learning behaviors into three categories: 
 a  single stimulus presentations; 
 b  exposure to relations among stimuli; and 
 c  exposure to relations between responses and 
1 	d. partridge 
stimuli. 
     johnston  partridge  and lopez  1  modelled the novelty drive theory and demonstrated that it accounted for a wide range of empirical data on human learning in categories  a  and  b  above. 
     a cell assembly network generates an expectation of the next stimulus input that it will encounter. any i - e discrepancy  that exceeds certain thresholds exact matches are not a feature of reality  is interpreted as an expectation failure  the environment is scanned and an internal representation of the input stimulus is generated but there was no provision for questioning the t 'perceived' . 
it is argued  johnston et al.  1  partridge et al.  
1   that the novelty or unexpectedness of an input with respect to an expectation is: 
/  qualitative difference between i and c ; quantitative difference between i and e   
and that or is: 
             / '  novelty; i - e magnitude  where t - e magnitude is some  additive  function of magnitude of the expected stimulus and magnitude of the actual input stimulus. 
     apart from suggesting and allowing the exploration of such details of the mechanism  the study also demonstrated that the hypothesis that the i - t discrepancy reduction mechanism can be structured as a conventional drive mechanism  i.e.  novelty drive  is a viable one. an implication of this hypothesis is that the unexpectedness of a stimulus is a primary drive reducing quality just like several others  such as food  sex  and pain. whereas schank  for example  states that  when we see something that in no way surprises us  it is also likely that it in no way interests us either. ' however  the importance of an input stimulus is not just its unexpectedness  but more fundamentally  the importance is the potential for reducing a primary drive. in al terms  this importance is the potential for achieving some basic goal  such as a need for novelty when bored  a need for food when hungry  etc. as mentioned earlier some erp data is supportive of this view - the magnitude of the or appears to be a function of both unexpectedness and utility  i.e.  food is an arousing stimulus even if expected when we are hungry . 
     the basic learning behavior - habituation  learning not to respond   may sound fairly trivial to the al researcher  but it is in fact crucial at all levels of behavior. an organism must ignore most of the information that bombards its sensory organs - it is an apparent efficiency measure that the real world promotes to one of necessity. 
     consider the al paradigm of rule-learning but in the empirical world rather than an abstracted context characterized by drastically pruned descriptions. the act of describing removes most  perhaps all  of the potentially relevant  but actually irrelevant  attributes of each event before the learning algorithm sets to work - it is fed predigested reality  hence no need to learn when and what to ignore. the child that is always fed filleted fish is not very impressed by a technique for avoiding bones. 
     next i shall describe a mainstream al application that makes important use of the i - t discrepancy reduction mechanism and can be used to illustrate many of the specific biases inherent in each of the above-described approaches to this mechanism. 
f. 	a cognitive industrial robot 
     high level control mechanisms for industrial robotics applications have been designed  implemented  and partially tested  partridge  burleson  and lopez  1  - the hand-eve robot learns and reasons  hence a  cognitive  robot  about a general task plan with respect to a specific task setup and attempts to optimize its performance. the mechanism behind the attempted optimization is to learn the constancies in a flexibly fixtured environment  e.g. that a certain target object tends to be situated in a certain position . this learned knowledge is then used predictively to optimize task execution in the current environmental setup. 
     the context of industrial robotics provides a constrained and thus potentially tractable micro-world but one that also contains much potential for the effective application of fundamental ai techniques. there is potential for developing a rich knowledge structure  certainly richer than the cell assembly model but not as rich as that of schank's theories  within a set of constraints that both promise tractability  and yet still offer a realistic and thus potentially testable context  in contrast to both schank's theories which are not currently very testable  and the rt paradigms which are testable but highly artificial . a cognitive industrial robot can fill a real need and offers some hope of comparison with the human performance of similar tasks. 
     of major importance for the current discussion is the implementation of the i - c discrepancy reduction mechanism within this project. having learned that some significant object  a target object with respect to this sub-task   say object a   has tended to be situated at position  whenever it was required  the robot might predict the future occurrence of a at the expectation  in the context of this particular subtask  that a will be at  can be used to drive a top-down pattern recognition process - i.e.  the process just checks that a is at  rather than analyzing the input image bottom-up to find a . when a is at  the robot confirms its expectations quickly and proceeds to deal with object a as dictated by the current sub-task  e.g. it might pick it up . 
     but when the input and the expectation don't agree  object a does not appear to be at  then the rodot needs to learn something - the big question is: what  
　　according to schank's theory the expectation that object a would be at  failed  and it needs to change its memory structures. on first failure it just indexes the general expectations with the exceptional ossibility that object a may alternatively be at i - the position where it was eventually found 
as a result of bottom-up recognition. on subsequent failures of this expectation  the robot should learn that its general expectation of a at  no longer holds and should be abandoned. it should be replaced by the expectation of a at  if this hitherto exceptional possibility has been repeatedly encountered. alternatively  the general expectation that a will be situated in any particular position may be completely dropped if the failures were due to seemingly arbitrary sequences of positionings of object a - an environmental constancy has disappeared  or was erroneously learned in the first place  in either case it can no longer be 
     
exploited. 
     a second possibility once we have decided that expectation failure is the source of the i - e discrepancy is that the expectation itself may be correct but the context that it was generated from was wrong. this is perhaps just to say that the subsequent learning should be at a higher level  i.e.  the structure that selects the context of the current stimuli should be altered so that in the future it will select the correct context. schank does raise this type of question  that of level of learning  and he sketches out some answers. 
     but as mentioned earlier  rumelhart and ortony suggest that such expectation failure may be a crucial factor in the correct selection of an appropriate context and not a learning situation at all. they offer the view that minimizing i' - e discrepancy is the route to efficient selection of an appropriate context. the verification model also uses expectation failure as a selection mechanism. 
     similarly  the cognitive robot may use this best fit between input and expectation to efficiently select the appropriate context after a discontinuity in task execution due to the occurrence of an error condition. as part of the analysis of the error condition it might need to choose between  say  an expectation of a at  x  y  or b at  x 1 y1 . 
     but in this robotics context  an i - e discrepancy can be due to a problem with the i component. first  the input image may be of poor quality due to 'noisy' conditions  in which case it might be appropriate to question the raw input data and our interpretation of it that yielded the i that was discrepant. 
     for the cognitive robot expecting object a at position  x  y    a subsequent i - e discrepancy may be due to the fact that although a was indeed at  x  y  the input pattern was so degraded by 'noise' that the cursory top-down recognition algorithm failed to confirm its expected presence. a subsequent  more exhaustive  bottom-up analysis might well find a at  x  y  with a sufficiently high confidence  it might for instance have the information that a is definitely somewhere in the image  and it might determine that the patterns at all other positions resemble object a even less that the pattern at  x  y  . 
     the problem with the i component may be due to the interpretation of the raw data rather than the quality of the data itself. this possibility then leads us into the murky world of i - e interdependencies: interpretation of the raw data depends upon expectancies  and expectancies depend upon interpretations of the data. 
     the lexical decision tasks described earlier have been used to probe the complexities of this i - e interdependence. schvaneveldt and mcdonald  1  report on a series of six such experiments that were designed to investigate the role of semantic context in the perceptual process. their results support the view that there are two modes of processing sensory information. 
     one mode involves the initial analysis of sensory information  such as features of the stimulus or such holistic properties as word shape  and is not directly affected by semantic context. the second mode is characterized as a  second look  at the stimulus  remember the n1 component of the erp  . they view this secondary analysis as  basically a memorydriven process in which 'hypotheses' about the identity of the stimulus are tested by comparing actual stimulus characteristics with those predicted by the hypothesis. 
	d. partridge 	1 
the hypotheses are generated by a combination of sensory information  from the first mode of processing  and contextual information.  thus an initial  partial t is generated independent of any e   this t is then combined with contextual information to yield e 's that guide the final step in the recognition process. 
     in terms of the popular general theory that the analysis of sensory information is compounded of a succession of processes which depend to varying degrees on bottom-up and top-down modes of organization  schvaneveldt and mcdonald suggest that the initial process is independent of expectations while the later ones are directed by expectations. they also suggest that the top-down process may enhance perception of discrepancies rather than induce a perceptual or decision bias in favor of expected stimuli. 
	subsequently  	paap  	newsome  	mcdonald 	and 
schvaneveldt  1  described a development of the verification model  called the  activation-verification  model. their goal is to specify the nature and interaction of bottom-up and top-down information-processing activities in recognition. the solution provides an independent top-down process  verification  that involves comparing stimulus information to prototypes stored in memory. 
     the suggestion that a top-down process may enhance perception of discrepancies raises again the basic problem of whether or not an i - e discrepancy exists in any particular situation. discrepancies may be perceived or unperceived dependent upon both the general level and the detailed focus of an i - e comparison. a high-level comparison will eliminate lowlevel discrepancies which may well be appropriate in the empirical world where repetition is never exact. on the other hand  some details of a stimulus are likely to be important while others are not  hence the % - e comparison needs to be focused on the significant details only.  also a fundamental problem in machine learning: how are the significant features in a series of events selected   
     there are two general classes of misinterpretation of the input stimulus: 
 1  misinterpretation due to erroneous recognition of input data  e.g. an input pattern generated by object a may be erroneously recognized as object b; and 
 1  misinterpretation due to erroneous selection from the input data  e.g. a pattern generated by noise is recognized as object a whilst the pattern generated by object a is dismissed as noise. 
in the cognitive industrial robot such misinterpretations are more likely when recognition is top-down  hypothesis or expectation driven   and when the system is 'confident' in its expectation. 
     this raises the last feature of the i - e discrepancy reduction mechanism: confidence. confidences in both the i 's and the t 's obtained interact with general contextual confidences to influence the following: 
 a  whether or not we perceive an i - e 
discrepancy; and 
 b  how to analyze a perceived discrepancy. 
problem  b  has largely been dealt with above except to note that analysis of the cause of an i - e discrepancy can be guided by the confidence that the system has in 
     
1 d. partridge 
the correctness of the structures and processes that are applied. the obvious strategy is to suspect that the element in which we have least confidence is the most likely cause of the discrepancy perceived. 
　　apart from the fact that the above rule is just a heuristic and as such carries no guarantees  in addition it has not  to my knowledge  been tested in this context although the cognitive robot uses it . there is  in addition  no correct way to compute these confidences - 
again we must explore heuristics until we find an adequate strategy. another common ai problem: confidence ratings  or some similarly named attribute  appear to be necessary but there is no obvious and correct way to compute them. 
     the cognitive industrial robot generates a confidence in its expectation that  say  object a is at  x  y  based upon the relative frequency of past occurrence of a at  x  y . it also generates a 
     confidence that a is actually at  x  y . this confidence is based upon both the degree to which the actual features of the pattern found at  x  y  have been matched against the characteristic features of a  the number of features and how well they matched   and the degree to which recognition has been top-down rather than bottom-up  a cursory top-down analysis is more efficient but less reliable . 
　　problem  a  concerns the conditions under which we acknowledge the presence of an i' - e discrepancy. very roughly  the more confident we are that we understand the situation  the less likely we are to admit that there is a discrepancy. 
　　but  you might object  either i and e match or they don't - there should be no question of confidence here. in an idealized situation this may be true  but the real world is far from ideal. two aspects of reality suggest the necessity for confidence ratings: 
 i  exactly the same event never occurs twice  it is only abstractions from the sensory data that exactly repeat   thus even the best i - e match will not be perfect; there will always be some discrepancy. 
 ii  stimuli are not just given  they must be selected from a rich and complex continuum; variation in the selection of appropriate attributes will result in varying discrepancies. 
　　as mentioned earlier  point  ii  is a well-known and difficult ai problem. one approach to this prob-1 
lem is through the goals of the system. thus the cognitive robot  for example  has a major goal of increasing task efficiency  or reducing task execution time; hence time is an important attribute of any subtask and one that it must always select. but  in general  selection of the significant aspects of its environment is subtask dependent. 
　　problem  i  suggests that the mechanism is not founded on an i - e discrepancy itself  but on a discrepancy that exceeds some threshold. compounded with this there is not one threshold but one for each significant attribute of the stimulus. finally these thresholds are dynamically adjustable and a major factor in this dynamic adjustment is confidence - if confidence is high then the thresholds  in general  are raised. high confidence suggests that we will ignore larger discrepancies. 
g. summary 
　　a range of projects has been surveyed in an attempt to demonstrate the potential utility to ai of work that lies outside the normal concerns of mainstream ai researchers. in particular  a generic mechanism that appears to underlie intelligent behavior was examined - the t - e discrepancy reduction mechanism. this mechanism appears to play a key role in human learning and in control of cognition. it is thus expected to be of importance in ai both as a key to machine learning  when to learn and what to learn two major unsolved ai problems   and in the control of complex context selection. 
　　it was shown that a comprehensive understanding of this mechanism is obtained from a consideration of this range of approaches to it; each approach embodies a different set of biases. first  there are two general applications of the i - e discrepancy reduction mechanism: 
 i  as the basis for a selection mechanism - selection of 'best-fit' contexts at one level  and of words at another level  it is a focusing mechanism ; and 
 ii  as the cue for a learning mechanism. 
within the latter application the presence of an i - e discrepancy signals the need for learning  but different research has emphasized different details of interpretation of this discrepancy to guide what needs to be learned. did the expectation fail  or the interpretation of the input  or both  or  from a higher level viewpoint  was some aspect of the i - e comparison itself misconceived  this last possibility leads us back to the first general application above - points  i  and  ii  are not independent. 
　　it was further argued that i - e discrepancy reduction might itself be a special case of the more general mechanism of basic goal achievement - the fundamental mechanisms of survival. 
h. acknowledgements 
     i wish to thank victor johnston  patty lopez  andrew ortony  and roger schvaneveldt for their assistance and guidance in my forays into domains that i am only just beginning to appreciate. 
1. 