
we present a fully connectionist system for the learning of first-order logic programs and the generation of corresponding models: given a program and a set of training examples  we embed the associated semantic operator into a feed-forward network and train the network using the examples. this results in the learning of first-order knowledge while damaged or noisy data is handled gracefully.
1 motivation
three long-standing open research problems in connectionism are the questions of how to instantiate the power of symbolic computation within a fully connectionist system  smolensky  1   how to represent and reason about structured objects and structure sensitive processes  fodor and pylyshyn  1   and how to overcome the propositional fixation  mccarthy  1   i.e. how to use connectionist systems for symbolic learning and reasoning beyond propositional logic. it has been shown that feed-forward networks are universal approximators and that artificial neural networks are turing complete. thus we know that symbolic computation is possible in principle  but at the same time the mentioned results are mainly theoretical.
¡¡here we are concerned with the model generation for firstorder logic programs  i.e. sets of rules which may contain variables ranging over infinite domains. our approach is based on the following ideas first expressed in  holldobler¡§ et al.  1 : various semantics of logic programs coincide with fixed points of associated semantic operators. given that the semantic operator is continuous on the reals  the operator can be approximated arbitrarily well by a feed-forward network. in addition  if the operator is a contraction  then its fixed point can be computed by a recurrent extension of the feed-forward network.
¡¡until now this approach was also purely theoretical for the first-order case. in this paper we show how feed-forward networks approximating the semantic operator of a given firstorder logic program can be constructed  we show how these networks can be trained using input-output examples  and we demonstrate that the obtained connectionist system is robust against damage and noise. in particular  and after stating necessary preliminaries in section 1  we make the following novel contributions in section 1: we define a new multidimensional embedding of semantic operators into the reals  we construct a feed-forward network to approximate these operators and we present a new learning method using domain knowledge. the resulting system is evaluated in section 1. finally  we draw some conclusions and point out what needs to be done in the future in section 1. for an overview of related work we refer to  d'avila garcez et al.  1  and  bader and hitzler  1 .
1 preliminaries
in this section  some preliminary notions from logic programming and connectionist systems are presented  along with the core method as one approach to integrate both paradigms.
1 first-order logic programs
a logic program over some first-order language l is a set of clauses of the form a ¡û l1 ¡Ä ¡¤¡¤¡¤ ¡Ä ln  a is an atom in l  and the li are literals in l  that is  atoms or negated atoms. a is called the head of the clause  the li are called body literals  and their conjunction l1 ¡Ä ¡¤¡¤¡¤ ¡Ä ln is called the body of the clause. if n = 1  a is called a fact. a clause is ground if it does not contain any variables. local variables are those variables occurring in some body but not in the corresponding head. a logic program is covered if none of the clauses contain local variables.
example 1. the following is a covered logic program which will serve as our running example. e 1 .	% 1 is even
e s x   ¡û o x . % the successor s x  of an odd x is even o x  ¡û  e x . % x is odd if it is not even the herbrand universe ul is the set of all ground terms of l  the herbrand base bl is the set of all ground atoms  which we assume to be infinite - indeed the case of a finite bl can be reduced to a propositional setting. a ground instance of a literal or a clause is obtained by replacing all variables by terms from ul. for a logic program p  g p  denotes the set of all ground instances of clauses from p.
¡¡a level mapping is a function assigning a natural number |a| ¡Ý 1 to each ground atom a. for negative ground literals we define | a| := |a|. a logic program p is called acyclic if there exists a level mapping | ¡¤ | such that for all clauses a ¡û l1¡Ä¡¤¡¤¡¤¡Äln ¡Ê g p  we have |a|   |li| for 1 ¡Ü i ¡Ü n.
example 1. consider the program from example 1 and let sn denote the n-fold application of s. with |e sn 1  | := 1n+1 and |o sn 1  | := 1n + 1  we find that p is acyclic.
¡¡a  herbrand  interpretation i is a subset of bl. those atoms a with a ¡Ê i are said to be true under i  those with are said to be false under i. il denotes the set of all interpretations. an interpretation i is a  herbrand  model of a logic program p  in symbols: i |= p  if i is a model for each clause in g p  in the usual sense. example 1. for the program p from example 1 we have m := {e sn 1   | n even} ¡È {o sm 1   | m odd} |= p.
¡¡given a logic program p  the single-step operator tp : atomsil ¡ú ialfor which there is a clausemaps an interpretation i to the set of exactly thosea ¡û body ¡Ê g p 
such that the body is true under i. the operator tp captures the semantics of p as the herbrand models of the latter are exactly the pre-fixed points of the former  i.e. those interpretations i with tp i    i. for logic programming purposes it is usually preferable to consider fixed points of tp  instead of pre-fixed points  as the intended meaning of programs. these fixed points are called supported models of the program  apt et al.  1 . in example 1  the  obviously intended  model m is supported  while bl is a model but not supported.
¡¡logic programming is an established and mature paradigm for knowledge representation and reasoning  see e.g.  lloyd  1   with recent applications in areas like rational agents or semantic web technologies  e.g.  angele and lausen  1  .
1 connectionist systems
a connectionist system is a network of simple computational units  which accumulate real numbers from their inputs and send a real number to their output. each unit's output is connected to other units' inputs with a certain real-valued weight. those units without incoming connections are called input units  those without outgoing ones are called output units.
¡¡we will consider 1-layered feed-forward networks  i.e. networks without cycles where the outputs of units in one layer are only connected to the inputs of units in the next layer. the first and last layers contain the input and output units respectively  the intermediate layer is called the hidden layer.
¡¡each unit has an input function which uses the connections' weights to merge its inputs into one single value  and an output function. an example for a so-called radial basis input function is  where the xi are the inputs and the wi are the corresponding weights. possible output functions are the sigmoidal function    for the hidden layer  and the identity  x ¡ú x  usually used in the output layer . if only one unit of a layer is allowed to output a value = 1   the layer implements a winner-take-all behavior. connectionist systems are successfully used for the learning of complex functions from raw data called training samples. desirable properties include robustness with respect to damage and noise; see e.g.  rojas  1  for details.
1 the core method
in  holldobler and kalinke  1; hitzler¡§	et al.  1  a method was proposed to translate a propositional logic program p into a neural network  such that the network will settle down in a stable state corresponding to a model of the program. to achieve this goal  the single-step operator tp associated with p was implemented using a connectionist system. this general approach is nowadays called the core method  bader and hitzler  1 .
¡¡in  holldobler¡§ et al.  1   the idea was extended to firstorder logic programs: it was shown that the tp-operator of acyclic programs can be represented as a continuous function on the real numbers. exploiting the universal approximation capabilities of 1-layered feed-forward networks  it was shown that those networks can approximate tp up to any given accuracy. however  no algorithms for the generation of the networks from given programs were presented. this was finally done in  bader et al.  1  in a preliminary fashion.
1 the fineblend system
in this section we will first discuss a new embedding of interpretations into vectors of real numbers. this extends the approach presented in  holldobler¡§ et al.  1  by computing m-dimensional vectors instead of a single real number  thus allowing for a higher and scalable precision. afterwards  we will show how to construct a connectionist system approximating the tp-operator of a given program p up to a given accuracy ¦Å. as mentioned above  in  bader et al.  1  first algorithms were presented. however  the accuracy obtainable in practice was limited through the use of a single real number for the embedding. the approach presented here allows for arbitrarily precise approximations. additionally  we will present a novel training method  tailored for our specific setting. the system presented here is a fine blend of techniques from the supervised growing neural gas  sgng   fritzke  1  and the approach presented in  bader et al.  1 .
1 embedding
obviously  we need to link the space of interpretations and the space of real vectors in order to feed the former into a connectionist system. to this end  we will first extend level mappings to a multi-dimensional setting  and then use them to represent interpretations as real vectors.
definition 1. an m-dimensional level mapping is a bijective function . for a ¡Ê bl  if
  then l and d are called level and dimension of a  respectively. again  we define .
definition 1.rm and its extension. thelet b ¡Ým¦É-1:dimensional embeddingiand let¡ú ram ¡Ê bare defined asl be an atom with¦É :¦É bal  :=¡ú
 ¦É1 a  ... ¦Ém a   where l
	and	.
with cm we denote the set of all embedded interpretations  i.e. cm := {¦É i  | i ¡Ê il}   rm.1

figure 1: c1  left  and c1  right  for b = 1 and m from ex. 1.
	y	y	y	y
x
figure 1: the first steps while constructing the limit c1.
example 1. using the 1-dimensional level mapping from example 1  we obtain c1 as depicted in figure 1 on the left. using the 1-dimensional level mapping
 n + 1  and   we obtain c as depicted on the right and ¦É m  =  1b 1b  ¡Ö  1 1  for the embedding of m.
¡¡for readers familiar with fractal geometry  we note that c1 is the classical cantor set and c1 the 1-dimensional variant of it  barnsley  1 . obviously  ¦É is injective for a bijective level mapping and it is bijective on cm. using the mdimensional embedding  the tp-operator can be embedded into the real vectors to obtain a real-valued function fp.
definition 1. the m-dimensional embedding of tp  namely fp : cm ¡ú cm  is defined as.
¡¡the m-dimensional embedding of tp is preferable to the one introduced in  holldobler¡§ et al.  1  and used in  bader et al.  1   because it allows for scalable approximation precision on real computers. otherwise  only 1 atoms could be represented with 1 bits.
¡¡now we introduce hyper-squares which will play an important role in the sequel. without going into detail  figure 1 shows the first 1 steps in the construction of c1. the big square is first replaced by 1m shrunken copies of itself  the result is again replaced by 1m smaller copies and so on. the limit of this iterative replacement is c1. we will use cmi to denote the result of the i-th replacement  i.e. figure 1 depicts c1 c1 c1 and c1. again  for readers with background in fractal geometry we note  that these are the first 1 applications of an iterated function system  barnsley  1 . the squares occurring in the intermediate results of the constructions are referred to as hyper-squares in the sequel. hl denotes a hyper-square of level l  i.e. one of the squares occurring in cml . an approximation of tp up to some level l will yield a function constant on all hyper-squares of level l.
definition 1. the largest exclusive hyper-square of a vector  and a set of vectors  denoted by   either does not exist or is the hypersquare h of least level for which. the smallest inclusive hyper-square of a non-empty set of vectors
  denoted by hin u   is the hyper-
square h of greatest level for which u   h.

figure 1: fp for the program from example 1 and the embedding from example 1 is shown on the left. a piecewise constant approximation fq  level l = 1  is shown on the right.
1 construction
in this section  we will show how to construct a connectionist network n for a given covered program p and a given accuracy ¦Å  such that the dimension-wise maximum distance d fp fn  := maxx j |¦Ðj fp x     ¦Ðj fn x  |  between the embedded tp-operator fp and the function fn computed by n is at most ¦Å. we will use a 1-layered network with a winner-take-all hidden layer.
¡¡with  we obtain a level l such that whenever two interpretations i and j agree on all atoms up to level l in dimension j  we find that |¦Éj i    ¦Éj j | ¡Ü ¦Å. for a covered program p  we can construct a finite subset q   g p  such that for allup to level l in all dimensions  hencei ¡Ê il  tp i  and tq di  fpagree on all atoms fq  ¡Ü ¦Å. furthermore  we find that the embedding fq is constant on all hyper-squares of level l  bader et al.  1   i.e. we obtain a piecewise constant function fq such that d fp fq  ¡Ü ¦Å.
¡¡we can now construct the feed-forward network as follows: for each hyper-square h of level l  we add a unit to the hidden layer  such that the input weights encode the position of the center of h. the unit shall output 1 if it is selected as winner  and 1 otherwise. the weight associated with the output connections of this unit is the value of fq on that hypersquare. thus  we obtain a connectionist network approximating the semantic operator tp up to the given accuracy ¦Å. to determine the winner for a given input  we designed a locally receptive activation function such that its outcome is smallest for the closest  responsible  unit. responsible units here are defined as follows: given some hyper-square h  units which are positioned in h but not in any of its sub-hyper-squares are called default units of h  and they are responsible for inputs from h except for inputs from sub-hyper-squares containing other units. if h does not have any default units  the units positioned in its sub-hyper-squares are responsible for all inputs from h as well. when all units' activations have been  locally  computed  the unit with the smallest value is selected as the winner.
¡¡the following example is taken from  witzel  1  and used to convey the underlying intuitions. all constructions work for m-dimensional embeddings in general  but for clarity the graphs here result from a 1-dimensional level mapping.
example 1. using the program from example 1 and the 1dimensional level mapping from example 1 we obtain fp and fq for level l = 1 as depicted in figure 1. the corresponding network consists of 1 input  1 hidden and 1 output units.
1 training
in this section  we will describe the adaptation of the system during training  i.e. how the weights and the structure of a network are changed  given training samples with input and desired output  in such a way that the distribution underlying the training data is better represented by the network. this process can be used to refine a network resulting from an incorrect program  or to train a network from scratch. the training samples in our case come from the original  non approximated  program  but might also be observed in the real world or given by experts. first we discuss the adaptation of the weights and then the adaptation of the structure by adding and removing units. some of the methods used here are adaptations of ideas described in  fritzke  1 . for a more detailed discussion of the training algorithms and modifications we refer to  witzel  1 .
adapting the weights let x be the input  y be the desired output and u be the winner-unit from the hidden layer. to adapt the system  we change the output weights for u towards the desired output  i.e.
thermore  we move u towards the center  i.e.  where ¦Ç and ¦Ì are predefined learning rates. note that the winner unit is not moved towards the input but towards the center of the smallest hypersquare including the unit and the input. the intention is that units should be positioned in the center of the hyper-square for which they are responsible.
adding new units the adjustment described above enables a certain kind of expansion of the network by allowing units to move to positions where they are responsible for larger areas of the input space. a refinement now should take care of densifying the network in areas where a great error is caused. therefore  when a unit u is selected for refinement 1 we try to figure out the area it is responsible for and a suitable position to add a new unit.
¡¡if u occupies a hyper-square on its own  then the largest such hyper-square is considered to be u's responsibility area. otherwise  we take the smallest hyper-square containing u. now u is moved to the center of this area  and some information gathered by u is used to determine a sub-hyper-square into whose center a new unit is placed  and to set up the output weights for the new unit.
removing inutile units each unit maintains a utility value  initially set to 1  which decreases over time and increases only if the unit contributes to the network's output.1 if a unit's utility drops below a threshold  the unit will be removed.
1 robustness
the described system is able to handle noisy data and to cope with damage. indeed  the effects of damage to the system are

	 1	 1
	 1	 1	 1	 1	 1	 1
#examples
figure 1: fineblend 1 versus fineblend 1.
quite obvious: if a hidden unit u fails  the receptive area is taken over by other units  thus only the specific results learned for u's receptive area are lost. while a corruption of the input weights may cause no changes at all in the network function  generally it can alter the unit's receptive area. if the output weights are corrupted  only certain inputs are effected. if the damage to the system occurs during training  it will be repaired very quickly as indicated by the experiment reported in section 1. noise is generally handled gracefully  because wrong or unnecessary adjustments or refinements can be undone in the further training process.
1 evaluation
in this section we will discuss some preliminary experiments. in the diagrams  we use a logarithmic scale for the error axis  and the error values are relative to ¦Å  i.e. a value of 1 designates an absolute error of ¦Å. for incorrect network initialization  we used the following wrong program:
e s x   ¡û  o x . o x  ¡û e x .
training samples were created randomly using the semantic operator of the program from example 1.
1 variants of fine blend
to illustrate the effects of varying the parameters  we use two setups: one with softer utility criteria  fineblend 1  and one with stricter ones  fineblend 1 . figure 1 shows that  starting from the incorrect initialization  the former decreases the initial error  paying with an increasing number of units  while the latter significantly decreases the number of units  paying with an increasing error. hence  the performance of the network critically depends on the choice of the parameters. the optimal parameters obviously depend on the concrete setting 
e.g. the kind and amount of noise present in the training data  and methods for finding them will be investigated in the future. for our further experiments we will use the fineblend 1 parameters  which resulted from a mixture of intuition and  non-exhaustive  comparative simulations.

	 1	 1
	 1	 1	 1	 1	 1	 1	 1	 1
#examples
figure 1: fineblend 1 versus sgng.
1 fine blend versus sgng
figure 1 compares fineblend 1 with sgng  fritzke  1 . both start off similarly  but soon sgng fails to improve further. the increasing number of units is partly due to the fact that no error threshold is used to inhibit refinement  but this should not be the cause for the constantly high error level. the choice of sgng parameters is rather subjective  and even though some testing was done to find them  they might be far from optimal. finding the optimal parameters for sgng is beyond the scope of this paper; however  it should be clear that it is not perfectly suited for our specific application. this comparison to an established generic architecture shows that our specialized architecture actually works  i.e. it is able to learn  and that it achieves the goal of specialization  i.e. it outperforms the generic architecture in our specific setting.
1 unit failure
figure 1 shows the effects of unit failure. a fineblend 1 network is  correctly  initialized and refined through training with 1 samples  then one third of its hidden units are removed randomly  and then training is continued as if nothing had happened. the network proves to handle the damage gracefully and to recover quickly. the relative error exceeds 1 only slightly and drops back very soon; the number of units continues to increase to the previous level  recreating the redundancy necessary for robustness.
1 iterating random inputs
one of the original aims of the core method is to obtain connectionist systems for logic programs which  when iteratively feeding their output back as input  settle to a stable state corresponding to an approximation of a fixed point of the program's single-step operator. in our running example  a unique fixed point is known to exist. to check whether our system reflects this  we proceed as follows:
1. train a network from scratch until the relative errorcaused by the network is below 1  i.e. network outputs are in the ¦Å-neighborhood of the desired output.
1. transform the obtained network into a recurrent one byconnecting the outputs to the corresponding inputs.

	 1	 1
	 1	 1	 1	 1	 1	 1	 1	 1	 1
#examples
figure 1: the effects of unit failure.
y

figure 1: iterating random inputs. the two dimensions of the input vectors are plotted against each other. the ¦Åneighborhood of the fixed point m is shown as a small box.
1. choose a random input vector  which is not necessarily a valid embedded interpretation  and use it as initial input to the network.
1. iterate the network until it reaches a stable state  i.e. untilthe outputs stay inside an ¦Å-neighborhood.
¡¡for our example program  the unique fixed point of tp is m as given in example 1. figure 1 shows the input space and the ¦Å-neighborhood of m  along with all intermediate results of the iteration for 1 random initial inputs. the example computations converge  because the underlying program is acyclic  witzel  1; holldobler¡§ et al.  1 . after at most 1 steps  the network is stable in all cases  in fact it is completely stable in the sense that all outputs stay exactly the same and not only within an ¦Å-neighborhood. this corresponds roughly to the number of applications of our program's tp operator required to fix the significant atoms  which confirms that the training method really implements our intention of learning tp. the fact that even a network obtained through training from scratch converges in this sense further underlines the efficacy of our training method.
1 conclusions and further work
we have reported on new results for overcoming the propositional fixation of current neural-symbolic systems: to the best of our knowledge this is the first constructive approach of approximating the semantic operators of first-order logic programs as well as their least fixed points in a fully connectionist setting. we also showed how the semantic operators can be learned from given training examples using a modified neural gas method which exploits domain knowledge. the resulting system degrades gracefully under damage and noise  and recovers using training.
¡¡whereas we define the embedding ¦É externally  in  gust and kuhnberger  1¡§   such embeddings are learned using ideas from category theory. in  seda and lane  1   connectionist systems for a covered program p are constructed by generating finite subsets of g p  and employing the constructions presented in  holldobler and kalinke  1¡§  .
¡¡besides a thorough comparison of these approaches much remains to be done. the presented methods and procedures involve parameters  which are set manually; we would like to find  preferably optimal  parameters automatically. we would like to extract first-order logic programs after training  but all the extraction methods that we are aware of are propositional. this is a prerequisite not only to compare our method of learning semantic operators of logic programs with that of inductive logic programming  but also to complete the neural-symbolic learning cycle  bader and hitzler  1 . the investigation of realistic applications  e.g. to the learning of ontologies and other types of knowledge bases  hitzler et al.  1  will follow.
acknowledgments
we would like to thank three anonymous referees for their valuable comments on the preliminary version of this paper. sebastian bader is supported by the gk1 of the german research foundation  dfg . pascal hitzler is supported by the german federal ministry of education and research  bmbf  under the smartweb project  grant 1 imd1 b   and by the x-media project  www.x-media-project.org  sponsored by the european commission as part of the information society technologies  ist  programme under ec grant number ist-fp1. andreas witzel is supported by a marie curie early stage research fellowship in the project gloriclass  mest-ct-1 .
