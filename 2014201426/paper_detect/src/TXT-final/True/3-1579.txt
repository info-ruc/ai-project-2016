
we present a novel correspondence-based technique for efficient shape classification and retrieval. shape boundaries are described by a set of  ad hoc  equally spaced points - avoiding the need to extract  landmark points . by formulating the correspondence problem in terms of a simple generative model  we are able to efficiently compute matches that incorporate scale  translation  rotation and reflection invariance. a hierarchical scheme with likelihood cut-off provides additional speed-up. in contrast to many shape descriptors  the concept of a mean  prototype  shape follows naturally in this setting. this enables model based classification  greatly reducing the cost of the testing phase. equal spacing of points can be defined in terms of either perimeter distance or radial angle. it is shown that combining the two leads to improved classification/retrieval performance.
1 introduction
in many object recognition problems  the examples are most easily disambiguated on the basis of shape - as opposed to features such as color or texture. often the classification of objects extracted from an image database are most intuitively formulated as a shape classification task. here we present a fast  general algorithm for classification and retrieval of 1d objects.
¡¡much of the recent work in this area uses a finite set of points taken from the object's boundary as the shape representation. points can be selected on the basis of maximal curvature  super  1   distance from the centroid  zhang et al.  1  or any criteria deemed suitable to the class of shapes involved. more sophisticated approaches parameterize the boundary as a closed curve and slide points along the outline to minimize an objective function  e.g.  wang et al.  1  . these methods produce good results but generally use expensive optimization algorithms. an alternative to finding the 'correct points' is to simply place points at roughly equal intervals along the boundary. belongie et al.  belongie et al.  1  used this approach effectively in their work on shape contexts.
¡¡having chosen how to represent the objects  one must solve the correspondence problem in order to compare shapes. this is illustrated in fig.1. also  a valid set of transformations for shape matching is required. these transformations represent the chosen definition of shape  i.e. they are the operations that leave shape unchanged.
¡¡in this paper  we introduce a simple  generic technique for shape classification and retrieval. shapes are represented by a potentially large number of points from their boundaries. the points lie at fixed intervals in terms of distance along the boundary or radial angle. this gives an accurate and robust description of shape that avoids the somewhat arbitrary decision of what constitutes a good point. the high computational cost associated with using many points is reduced in three ways. firstly  a hierarchical approach avoids the need to consider all possible label assignments. secondly  potential correspondences are considered simultaneously and a clear winner often emerges early in the computation. finally  for classification problems  the algorithm produces class means with associated variances at each point. this allows model based classification  requiring fewer shape comparisons at the testing stage. by considering both the distance and angle based descriptors  we can accurately describe a wider range of shapes and increase the classification/retrieval accuracy. the effectiveness of the method is demonstrated on a benchmark data set and compared to other state-of-the-art methods in the field.
1 method
we wish to develop generic techniques for classifying or retrieving shapes. there are two important issues to consider: firstly  the ultimate goal is to work with large data sets and hence  computational expense is an important factor. secondly  the shapes involved may be very diverse  with large differences between some classes. for example  one class might consist of very jagged shapes and the other smoothly curved shapes. this implies that overly specific models of shape variation should be avoided.
1 shape correspondence
given two shapes  we wish to optimally align them in order to observe the genuine differences in shape. when shapes are represented by a set of points  it is necessary to find the best match over all possible label assignments  fig.1 . in this paper  we use a definition of shape from statistical shape analysis  e.g.  dryden and mardia  1  : the shape of a set of labelled points is all the information that is invariant to rotation  scaling and translation. in addition to this  we also consider reflection to be a valid transformation. allowing

figure 1: two identical shapes  left . large symbols indicate corresponding points under a random assignment of labels. to align the shapes  we must find the correct labels as well as the correct transformations  right .
only basic transformations reflects our lack of prior knowledge about the shape classes involved. we now formulate the correspondence problem in a probabilistic fashion. this will enable correspondences to be chosen at minimal computational cost.
correspondence algorithm
consider two shapes  each represented by k points from their respective boundaries: x =  x1 ...xk t and y =  y1 ...yk t ¡Ê rk¡Á1. both shapes are centered:
. we assume that the yj are generated independently from
	yj ¡« n s¦£xj+m|k + t ¦Ò1 	 1 
for j = 1 ... k  where s is a scaling parameter  t a translation vector and ¦£ a 1d rotation matrix - reflection is dealt with later. m ¡Ê {1 ... k   1} cycles the labels of the points in x. finding the maximum likelihood estimates  mles  of the parameters is a restricted version of the matching problem for two unlabelled sets investigated in  kent et al.  1 . for the general case  the large number of potential correspondences  k!  can make finding the mles prohibitively expensive. however  here we are dealing with boundaries and need only consider cycling of the labels. this means that there are only k potential label assignments and the computational cost of computing the mles ¦£  s    t and m  is more acceptable.
the likelihood of the parameters can be written as
l
where
.
 for a fixed m ¡Ê {1 ... k   1}  s    t and ¦£  take values that minimize d1 y x  - a consequence of assuming isotropic variance and independence between points. we now exploit a closed form solution for the minimum of d1 y x  which can be written compactly using complex notation. letting xj ¡Ô  xj1 xj1 t ¡ú xj1 + ixj1 ¡Ô zj ¡Ê c  the shapes become complex vectors: x ¡ú z y ¡ú w ¡Ê ck. following

figure 1: two shapes from the same class  each described by 1 points  left  - linear interpolation is used to aid visualization. the correspondence algorithm correctly cycles the labels and the shapes can be aligned  right .
 dryden and mardia  1  we have  for a fixed cycling m  and unit sized shapes  
mind1 x y  = 1   |w z|1
¦£ s t
where w  denotes the conjugate transpose of w. absorbing the size normalization into the distance expression gives the expression for the full procrustes distance:
1
	.	 1 
this is a standard metric in shape analysis. substituting d1f z w  into eq. 1  allows us to evaluate l m s    t ¦£   for all m ¡Ê {1 ... k   1} and hence  find m  . fig.1 demonstrates the correspondence algorithm applied to two identical shapes - alignment is achieved using the full procrustes fit  section 1 . as expected  the shapes can be perfectly matched. in fig.1  the shapes are different examples from the same class. a good match is possible after finding the best correspondence.
¡¡invariance with respect to reflection is not yet incorporated into the distance computation. it is worth mentioning that procrustes distances can directly incorporate reflection invariance  mardia et al.  1 . unfortunately  this idea is not compatible with our formulation of the correspondence problem. a simple example demonstrates this: imagine z is the reflection of w. when faced with the task of corresponding the two shapes  we are obviously unaware of the reflection and proceed to label both shapes in an anticlockwise direction. w will only be the same as its reflection  in the procrustes sense  if the labels are reflected along with the points. doing this would flip the direction of the labels to clockwise. z cannot be matched to the original w  due to reflection  nor to the reflected w  because the labels run in opposite directions . the reflection of z is given by z  ¡Ô   ¡¥z1 ... z¡¥k . this operation changes the direction of the labels to clockwise. reversing the order of the entries: zr ¡Ô   ¡¥zk ... z¡¥1 t  switches the direction back to anticlockwise. the optimal correspondence between zr and w can now be found as normal. this apparent doubling in computational expense can be significantly reduced by tracking the correspondence calculations for the non-reflected shapes.
reducing the computational cost

figure 1: example of how the speed-up techniques can affect the dimensions of the outer product matrix - see text for details.any classification or retrieval problem will require a large number of correspondences to be found. the cost of computing correspondences is dominated by k - the number of boundary points used. firstly  the number of cycles to consider increase linearly with k. secondly  the distance computation used to evaluate a correspondence is essentially a dot product of vectors in ck. from an accuracy perspective  it may be necessary to have a large k. fortunately  for any given k  there are options for reducing both the number of cycles that are considered and the cost of each distance computation.
¡¡to evaluate d1f z w  it is necessary to compute the dot products w w  z z and w z. the values of w w and z z do not change with the cycle value and need only be computed once. the cost of corresponding z and w is therefore dominated by calculating the dot product between w  and all cycled versions of z. this is equivalent to summing along the diagonals of the outer product matrix zw   for example  summing the leading diagonal gives the value of w z when the cycle value is zero - see fig.1 . the following procedures reduce the dimensions of zw  and hence  the computational cost of the algorithm.
¡¡the number of points used  k  is assumed to be large  say k = 1 . to avoid considering all k possible values of the cycling parameter m  we initially consider a coarse representation of the shapes using points  say and the points are found by selecting every fifth point of the 1 . the likelihoods for all 1 values of the cycling parameter m are then evaluated as normal. it seems reasonable to assume that the most likely m will correspond the shapes in a similar way to that which would be achieved using all 1 points. since the 1 points come from the original 1  it is easy to relate these approximate correspondences back to the true shapes  those described by 1 points . the final correspondences are found in the standard way  but only cycle values close to the approximated value are investigated. the outer product matrix is in c1¡Á1 rather than c1¡Á1 for the first round of correspondences  fig.1 . at the fine tuning stage  only a small number of the potential 1 correspondences are evaluated. thus  the overall cost is greatly reduced. regardless of the number of points used to describe a shape  it seems that the full matrix zw  must be evaluated and used to compute the procrustes distances. however  by computing the columns of zw  incrementally  the likelihoods over all k possible cycles can be monitored and a
¡¡correspondence chosen when one of them exceeds a given threshold  c.f. fig.1 . this approach captures the intuitive idea that some correspondences will be much easier to find than others. the likelihood threshold is reached after fewer iterations when 'new' points are selected at random  e.g. w1¡¥ z w¡¥1z ...  so that the full boundary is explored quickly.1 clearly if the threshold is never reached and all the points are needed  this incremental method will be more costly than the basic algorithm. the additional cost comes from updating the k different z z associated with each labelling of z and from evaluating the likelihood at each stage.1 it can be partly avoided by evaluating zw  in blocks  rather than single columns. often very few points are required to find the best correspondence  section 1 . it is worth noting that our algorithm could assign correspondences on the basis of minimum procrustes distance  df  - so why bother with likelihoods  fig.1 shows the evolution of the likelihoods compared to the squared full procrustes distances for a typical problem. the likelihood approach magnifies the best correspondences and kills-off the worst  leading to a quicker choice of cycle parameter m. recall from eq. 1  that there is isotropic variance  ¦Ò1  at each point. ¦Ò1 is a free parameter that affects how quickly the likelihoods peak to a single cycle value. a small variance encourages fast correspondence selection  fig.1  right   but assumes little within class variability. note that this thresholding technique is applicable whenever the correspondence algorithm is used  i.e. it is independent of the above method for reducing the number of cycle values considered.
shape descriptors

figure 1: squared full procrustes distances  left  and normalized likelihoods  middle  between two shapes from the mouse class. the different lines indicate the distance/likelihood after using a given number of points - or equivalently  computing a given number of columns of zw . decreasing ¦Ò1 exaggerates the peaked profile of the likelihoods  right .the correspondence algorithm is independent of how the k boundary points are selected in the first place. to keep our approach as generic as possible  very simple point selection techniques are used. we either choose points at roughly equal intervals along the boundary or points at equal increments of the radial angle  the centroid radii model  chang et al.  1  . we refer to the former method as the perimeter shape descriptor and the latter as the radial descriptor. in some cases  a line from the origin can intercept the boundary at more than one point. to make the radial descriptor consistent  all boundary points close to these intercepts are considered and the point most distant from the origin is selected. this can lead to parts of the shape being ignored. however  the results in section 1 demonstrate the advantage of using this sub-optimal descriptor.
1 shape model
the correspondence technique discussed above can be incorporated into a nearest neighbor classifier: given a test shape  find its optimal correspondence with every training shape and classify on the basis of minimum procrustes distance. this requires finding a large number of correspondences  a problem that can be avoided using prototypes. ideally  the full procrustes mean of each class would form the set of prototypes. the definition of this mean follows naturally from the definition of df. it is defined as ¦Ì¡¥ ¡Ê ck such that ||¦Ì¡¥|| = 1 and
n
¦Ì¡¥ = argmin 1 
¦Ì
l=1
where z1 ... zn are all the shapes in the class. bookstein  bookstein  1  has proposed an iterative method for finding ¦Ì¡¥. by incorporating the correspondence algorithm into this method  we can correspond shapes directly onto a running class mean.1 this procedure relies on the full procrustes fit for aligning two shapes. fitting z to w transforms z so that the sum of squared distances between corresponding points is equal to df z w . in fig.1  df z w  is zero and a perfect fit is achieved. in fig.1  df z w  is small because the shapes are similar and a reasonable fit is possible. the fit of z onto w is given by
	zfit = z wz/ z z .	 1 
the algorithm for finding the correspondences and the mean within a given class is shown in table 1. shapes are successively corresponded and fitted to a running mean.
¡¡given a mean  the sample covariance matrices can be calculated at each point. the final class model consists of a mean shape - k independent 1d points  each with an associated covariance matrix. fig.1 shows the algorithm applied to twenty objects from the same class.
1 shape classification
the most likely correspondence and fit between a test shape and each class mean is found as described in section 1. for classification  the model for finding correspondences  eq. 1   is modified. points are still assumed independent  but the table 1: algorithm for corresponding all shapes from a given class to a running class mean.
initialize: ¦Ì = final shape in class for i = 1:cycles	for j = 1:final shape in class
   1. get corresponding labels of shape j and ¦Ì
   1. fit shape j to ¦Ì
   1. recompute ¦Ì using arithmetic mean at each point end end fit all shapes to the final ¦Ì
figure 1: left: class mean  bold  and the twenty class examples fitted to the mean using 1 points  interpolated for visualization ; right: class mean and scatter of corresponding points at four positions.
isotropic covariance is replaced by the sample covariance matrix1 at each point. thinking of shapes as sets of points in r1  a test shape x =  x1 ... xk t is assigned to a class c x  using maximum likelihood  ml 
		 1 
where c is the number of classes   ¦Ìc1 ...¦Ìck t ¡Ê rk¡Á1 is the mean of class c  ¦²cj ¡Ê r1¡Á1 is the sample covariance at ¦Ìcj and f ¡¤;¦Ìcj ¦²cj  is the pdf of the bivariate normal distribution n ¦Ìcj ¦²cj .

figure 1: example boundaries from the mpeg-1 data set.
1 evaluation and results
the proposed technique is tested on the benchmark mpeg1 shape database  latecki et al.  1 . this consists of 1 classes with 1 observations in each class. pixels lying on a closed boundary are extracted in a clockwise direction using matlab's image processing toolbox. some of these boundaries are shown in fig.1  the same data was used for figs. 1  1 and 1 . the first principle component of these  1d  points is aligned with the x-axis. this helps reduce the problem of 'sub-interval shifts' producing an artificially high procrustes distance between similar shapes - particulary for small k. the shape is described by k points using either the perimeter or radial descriptor as described in section 1. initially we present results without using any speed-up techniques. the impact of these are explored in section 1.
1 leave-one-out shape classification
a thirty class subset of the mpeg-1 database was recently used by  kunttu et al.  1  in their work on multiscale fourier descriptors. their approach outperformed the standard contour fourier descriptor in various classification tasks. using leave-one-out classification with a nearest neighbor classifier  they achieved 1-1% accuracy  depending on the length of shape descriptor . we use the perimeter descriptor with the same data set and testing procedure  i.e. leaveone-out and nearest neighbor  to enable a direct comparison with their results. test shapes are fitted to training shapes using the correspondence method described in section 1. the nearest neighbor is the training shape closest to the test shape in terms of full procrustes distance. results are shown in the first row of table 1. classification accuracy is slightly better than that of  kunttu et al.  1  and the results are consistent over a wide range of k. the same tests were carried out using the ml method described in section 1. results are given in the bottom row of table 1. this prototype based technique is almost as accurate as the nearest neighbor classifier and requires that many fewer correspondences are found - 1 versus 1 per test shape.
¡¡the first column of table 1 shows the corresponding results for the full seventy class data set with k = 1 one table 1: classification performance %  for the thirty class data set  k = number of points  perimeter descriptor used .
k111n. neighbor11111ml11111perimeterradialcombinedn. neighbor111ml111table 1: classification performance %  for the seventy class data set with k = 1. reason for the slightly poorer results on the full data set is that there are now classes for which the perimeter descriptor is definitely not suitable. despite the generally inferior performance of the radial descriptor  table 1  middle column   it performs significantly better for some classes. this suggests that using both descriptors may improve classification accuracy. here  we demonstrate how a naive combination of the descriptors can enhance performance. the ml classifier is modified as follows: each descriptor  perimeter and radial  gives a vector of class-conditional likelihoods for the test shape. the two vectors are normalized so that they each sum to one. for a given class  the maximum of the two candidate likelihoods is used for classification. this reflects an assumption that the descriptor with the most confident prediction is correct. it is less clear how to normalize the distances for the nearest neighbor classifier. we assume that the majority of shapes bear no resemblance to the test shape and hence  give rise to meaningless correspondences and distances. normalization is therefore carried over the minimum q entries in each distance vector  total number of shapes . the distance used for classification is the minimum of the two candidate distances. here  and in section 1  we fix q = 1. the results in table 1 show a small rise in classification accuracy for the combined case over the perimeter only case. evidently  the increase in performance is limited by the already high performance of the perimeter descriptor and the simplicity of the normalization technique.
1 bullseye test for shape retrieval
this is a frequently used test in shape retrieval and enables us to compare our algorithm against many of the best performing shape retrieval techniques. a single shape is presented as a query and the top forty matches are retrieved  from the entire data set - the test shape is not removed . the task is repeated for each shape and the number of correct matches  out of a maximum possible 1  are noted. a perfect performance results in 1 ¡Á 1 = 1 matches. results are given as a percentage of this perfect score. the first two rows of table 1 show the result of the proposed technique using the perimeter and radial descriptors when no speed-ups were used. the bullseye score was also computed using the combined approach as described for nearest neighbor clas-
sification in the previous section. results are shown in the bottom row of table 1.  super  1  notes that the best performing algorithms in this test have scored between 1% and 1%. a naive combination of two simple descriptors table 1: scores for the bullseye test % .
k111perimeter11111radial11111combined11111table 1: classification performance using speed-ups.
standardlcc-fc-f/lcnn11  1 11  1 ml11  1 11  1 outperforms all of the previously tested algorithms. it should be noted that these results depend on the free parameter q  the number of shapes used in normalization. we are investigating methods for automatically selecting this value.
1 fast correspondence
we repeat the classification tests on the thirty class data set using the speed-ups described in section 1. the perimeter descriptor with k=1 is used throughout. the results are given in table 1 - results are averaged over 1 runs where likelihood cutoff is used. column one shows the results when no speed-ups are used. the second column shows the results using likelihood cut-off. a correspondence is chosen when the highest likelihood exceeds the second highest likelihood by 1/ 1 ¡Á k   i.e. twice the probability assigned to all correspondences under a uniform distribution - recall reflection must be considered . the average number of points needed to choose a correspondence is given in brackets. column three gives results when the coarse-to-fine correspondence technique is implemented with =1. an approximate shape correspondence is mapped back to the 1-point shapes and the best correspondence is chosen over three shifts of the labels in either direction. the final column also shows results using the coarse-to-fine technique but with likelihood cut-off implemented when corresponding the coarse shapes. implementing the speed-ups has little impact on the accuracy of nearest-neighbor classification. the ml classifier is significantly affected; the coarse-to-fine method produces a moderate reduction in accuracy  1 from 1  whilst likelihood cut-off leads to a dramatic decrease in performance. since a test-shape is only compared to its correct class once in the ml case  the class prototype   it is not robust to the increased probability of the correspondence algorithm failing when a
low threshold likelihood cut-off is used.
1 discussion
we have proposed an efficient  correspondence-based shape classification algorithm for 1d boundaries. shapes are represented by points placed at equal intervals along the boundary; hence  there is no heuristic procedure for choosing the points. a hierarchical matching  going from coarse to fine matches  and likelihood thresholding significantly reduce the cost of finding correspondences with minimal impact on classification accuracy. this ensures that easier to match shapes do not have to go through the complete correspondence computations. the algorithm was tested on the mpeg-1 shape database and was shown to outperform the popular contour fourier method. our approach leads to an intuitive class model that can be used for fast prototype-based classification. the results using this method were comparable to that of standard nearest-neighbor classification. the approach was also shown to perform well in the commonly used bullseye retrieval test. combining two simple shape descriptors further improved retrieval accuracy.
¡¡the ideas presented are most relevant to general shape classification and retrieval problems where there is insufficient knowledge to construct a priori models. combining/choosing between different shape descriptors is difficult in retrieval since there is no training data available. however  relevance feedback  common in document-based information retrieval  is a realistic mechanism for gaining additional information and can be used to guide dynamic selection and weighting of shape descriptors. future work will investigate this possibility.
