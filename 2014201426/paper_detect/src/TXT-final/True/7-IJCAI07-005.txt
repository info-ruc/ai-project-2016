
finding a constraint network that will be efficiently solved by a constraint solver requires a strong expertise in constraint programming. hence  there is an increasing interest in automatic reformulation. this paper presents a general framework for learning implied global constraints in a constraint network assumed to be provided by a non-expert user. the learned global constraints can then be added to the network to improve the solving process. we apply our technique to global cardinality constraints. experiments show the significance of the approach.
1 introduction
encoding a problem as a constraint network  called model  that will be solved efficiently by a constraint toolkit is a difficult task. this requires a strong expertise in constraint programming  cp   and this is known as one of the main bottlenecks to the widespread of cp technology  puget  1 . cp experts often start by building a first model that expresses correctly the problem without any other requirement. then  if the time to solve this model is too high  they refine it by adding new constraints that do not change the set of solutions but that increase the propagation capabilities of the solver. in ilog solver 1. user's manual  a gcc constraint is added to improve the solving time of a graph coloring problem  page 1 in  ilog  1  . in  re¡ägin  1   new constraints are added in a sports league scheduling problem. such constraints are called implied constraints  smith et al.  1 . the community has studied automatic ways of generating implied constraints from a model. colton and miguel  colton and miguel  1  exploit a theory formation program to provide concepts and theorems which can be translated into implied constraints. in hnich et al.  hnich et al.  1   some implied linear constraints are derived by using an extension of the prolog equation solving system  sterling et al.  1 . in all these frameworks the new generated constraints are derived from structural properties of the original constraints. they do not depend on domains of variables.
¡¡this paper presents a framework based on ideas presented in  bessiere et al.  1 . this framework contains two original contributions in automatic generation of implied constraints: first  it permits to learn global constraints with parameters; second  the implied constraints are learned according to the actual domains of the variables in the model.
¡¡learning global constraints is important because global constraints are a key feature of constraint programming. they permit to reduce the search space with efficient propagators. global constraints generally involve parameters. for instance  consider the nvalue k  x1 ...  xn   constraint. it holds iff k is equal to the number of different values taken by x1 ...  xn. k can be seen as the parameter  which leads to different sets of allowed tuples on the xi's depending on the value it takes. the increase of expressiveness provided by the parameters increases the chances to find implied constraints in a network. a learning algorithm will try  for example  to learn the smallest range of possible values for k s.t. the nvalue is an implied constraint. the tighter the range for k  the more the learned constraint can reduce the search space. many constraints from  beldiceanu et al.  1  may be used as implied constraints involving parameters.
¡¡learning implied constraints according to the actual domains is important because constraints learned that way take into account more than just the structure of the network and the constraints. they are closer to the real problem to be solved. for instance  if we know that x y and z are 1 variables  we can derive x = z from the set of constraints x  y y  z. without knowledge on the domains  we cannot derive anything. even an expert in cp has difficulties to find such constraints because they depend on complexinteractions between domains and constraints.
1 motivation example
let us focus on a simple problem where n tasks have to be scheduled. all tasks have the same duration d. each task i requires a given amount mi of resource. steps of time are represented by integers starting at 1. at any time  the amount of resource used by all tasks is bounded by maxr. the starting times of any two tasks must be separated by at least two steps of time. random precedence constraints between start times of tasks are added. the makespan  maximumtime  is equal to 1  n 1 +d  the best possible makespan w.r.t. the constraint on starting times . the question is to find a feasible schedule. we implemented a naive model with choco  choco  1 . the start time of task i is represented by a variable xi with domain  1..1    n   1    because of the makespan   and the resource capacity is represented at each unit of time t by an
naive modelimplied constraintn / d / n1 / n1 / maxr#nodes / time  sec. #nodes / time  sec. 1 / 1 / 1 / 1 / 1-- /   1 / 11 / 1 / 1 / 1 / 1 1 / 11 / 11 / 1 / 1 / 1 / 1 / 11 / 11 / 1 / 1 / 1 / 1 / 11 / 11 / 1 / 1 / 1 / 1 / 11 / 11 / 1 / 1 / 1 / 1  unsat  / 11  unsat  / 1table 1: results on the naive model  left  and when an implied gcc is added to the model  right .  n tasks of duration d among which nj tasks use j resources. 
array r t  1..n  of variables  where r t  i  = mi if task i is active at time t  r t  i  = 1 otherwise. constraints ensuring consistency between xi and r t  i  and constraints on separation of starting times are primitive ones. sum constraints are used for expressing resource capacity  ni=1 r t  i  ¡Ü maxr . we place us in the context of a non expert user  so we run the default search heuristics of choco  the variable with minimum domain first  assigned with its smallest available value . we put a cutoff at 1 seconds. table 1 left  reports results on instances where n = 1 d = 1 and nj is the number of tasks requiring j resources. only two precedence constraints are added. it shows that even on these small instances  our naive model can be hard to solve with standard solving techniques. if we want to solve this problem with more tasks  we definitely need to improve the model. an expert-user will see that constraints on the starting time of tasks induce limits on the number of tasks that can start at each point of time.1 the makespan is indeed equal to the minimal possible value for scheduling all the tasks if they are separated by at least two units. we therefore have to place tasks as soon as possible. this means that one task starts at time zero  one task at time 1  etc. this can be expressed explicitly with a global cardinality constraint  gcc . gcc  x1 ...  xn   pv1 ...  pvk   holds iff |{i | xi = vj}| = pvj for all j. so  we add to our model a gcc constraint that involves all starting times x1 ...  xn and guarantees that the n first even values of time are taken at least once: gcc  x1 ...  xn   p1 ...  p1n 1    where pv ¡Ê 1...n if v is even  pv ¡Ê 1...n otherwise. as shown in table 1 right   adding this simple gcc constraint to the naive model leads to significant improvements. this example shows that for a nonexpert user  even very small problems can lead to bad performance of the solver. adding simple implied constraints to the model can dramatically reduce the solving cost.
1 basic definitions
a constraint network n is defined by a set x = {x1 ...  xn} of variables  a set d = {d x1  ...  d xn } of domains of values for the variables in x  and a set c of constraints. a constraint c is defined on its scope x c    x. c specifies which combinations of values  or tuples  are allowed on the variables in x c . a solution to n is an assignment of values from their domain to all variables in x s.t. all constraints in c are satisfied. sol n  denotes the set of solutions of n.
¡¡an implied constraint for a constraint network n is a constraint that does not change the solutions of n. that is  given the constraint network n =  x  d  c   the constraint c with x c    x is an implied constraint for n if sol n  = sol n + c  where n + c denotes the network  x  d  c ¡È {c} . global constraints are defined on any number of variables. for instance  alldifferent is a global constraint that can be instantiated on scopes of variables of any size. for most of the global constraints  the signature is not completely fixed. some of their variables can be seen as parameters  that is  'external' variables that are not involved in any other constraint of the problem. for instance  the gcc constraint gcc  x1 ...  xn   pv1 ...  pvk    involves xi's and pv's. the pv's can be variables of the problem  like in  quimper et al.  1    or parameters that take a value in a range  like in  re¡ägin  1  . another example is the among constraint. among  x1 ...  xn   v1 ... vk   n  holds iff |{i |   j ¡Ê 1..k  xi = vj}| = n. the vj's and n can be parameters or variables  beldiceanu and contejean  1; bessiere et al.  1 . so  a global constraint c defined on a scheme scheme c  can be specified into different types of parametric constraints depending on which elements in scheme c  are variables or parameters.
definition 1  parametric constraint  given a global constraint c defined on scheme c   a parametric constraint derived from c is a global constraint c p  s.t. p = {p1 ...  pm} is the set of parameters of c p  with p   scheme c   and x c p   is the scope of c p  with x c p   = scheme c    p.
¡¡so  in addition to its variables  a constraint can be defined by a set p of parameters. the set of tuples that are allowed by the constraint depends on the values the parameters can take.
definition 1  instance of parametric constraint  given a constraint network n =  x  d  c   an instance of a parametric constraint c p  in n is a constraint c p ¡û s  s.t.:
  x c p ¡û s   = x c p     x 
  p ¡É x =   
  s   |p| is a set of tuples on p 
  c p ¡û s  is satisfied by an instantiation e on x c p   iff there exists t ¡Ê s s.t. the tuple  e + t  on scheme c  is a satisfying tuple for the global constraint c from which c p  is derived.
example 1 let c p  be the parametric constraint derived from
among  x1 ...  x1   v   n  with scope  x1 ...  x1  and parameters p =  v  n . let s = { 1   1  1 } be a set of combinations for
p. a tuple on  x1 ...  x1  is accepted by the instance of constraint
c p ¡û s  iff it has three occurrences of value 1 or one occurrence of value 1. tuples  1 1  1  and  1  1  1  satisfy the constraint. tuples  1  1  and  1  1  1  do not. we suppose that this parametric constraint allows us specifying combinations of parameters that are allowed. in practice  propagation algorithms often put restrictions on how parameters can be expressed.
given a parametric constraint c p   different sets s and
s  of tuples for the parameters lead to different constraints
c p ¡û s  and c p ¡û s  . hence  when adding an instance of parametric constraint c p  as implied constraint in a network n  it is worth using a set s of tuples for the parameters which is as tight as possible and c p ¡û s  is implied in n.
1 learning implied parametric constraints
the objective is to learn a 'target' set t   |p|  as small as possible s.t. c p  is still an implied constraint. any s ¡Ê |p| which is not necessary to accept some solutions of n can be discarded from t. the tighter the learned constraint is  the more promising its filtering power is.
definition 1  learning an implied parametric constraint  given a constraint network n =  x  d  c  and a parametric constraint c p  with p ¡É x =   and x c p     x  learning an instance of the parametric constraint c p  implied in n consists in finding a set s   |p| as tight as possible s.t. c p ¡û s  is implied in n.
¡¡a set of tuples t s.t. c p ¡û t  is implied in n and there does not exist any s   t with c p ¡û s  implied in n is called a target set for c p  in n.
¡¡we now give a general process to learn the parameters of implied global constraints. since our goal is to deal with any type of constraint network  we focus on global constraints c and sets of parameters p   scheme c  s.t. c p ¡û |p|  is the universal constraint. this means that if n contains the variables in scheme c    p  c p ¡û |p|  is an implied constraint in n. the learning task is to find a small superset of a target set t  see definition 1 . given a problem n =  x  d  c  and a parametric constraint c p   we denote by:
  ubt a subset of |p| s.t. c p ¡û ubt  is an implied constraint 
  lbt a subset of ubt s.t. for any t ¡Ê lbt  c p ¡û ubt   {t}  is not an implied constraint.
in other words  lbt represents those combinations of values for the parameters that are necessary in ubt to preserve the set of solutions of n  and ubt those for which we have not proved yet that we would not lose solutions without them. the following propositions give some general conditions on how to incrementally tighten the bounds lbt and ubt while keeping the invariant lbt   t   ubt  where t is a target.
proposition 1 let c p ¡û ubt  be an implied constraint in a network n. if there exists s   ubt s.t. sol n  = sol n + c p ¡û s   then c p ¡û s  is an implied constraint in n. so ubt can be replaced by s.
¡¡testing equivalenceof the sets of solutions of two networks is conp-complete. this cannot be handled by classical constraint solvers. hence  we have to relax the condition to a condition that can more easily be checked by a solver.
corollary 1 let c p ¡û ubt  be an implied constraint in a network n. if there exists s   ubt s.t. n + c p ¡û s  is inconsistent then c p ¡û ubt   s  is an implied constraint in n. so ubt can be replaced by ubt   s.
¡¡the weakness of this corollary is that we have no clue on which subsets s of ubt to test for inconsistency. but we know that an implied constraint should not remove solutions when we add it to the network. hence  solutions can help us.
proposition 1 let c p ¡û ubt  be an implied constraint in a network n and e be a solution of n. for any s   ubt s.t. c p ¡û s  is an implied constraint for n there exists t in s s.t. e satisfies c p ¡û {t} .
¡¡if a given solution is accepted by a unique combination of values for the parameters  then this combination is necessary for having an implied constraint  i.e.  it belongs to lbt .
corollary 1 let c p ¡û ubt  be an implied constraint in a network n and e be a solution of n. if there is a unique t in ubt s.t. e satisfies c p ¡û {t}  then t can be put in lbt.
¡¡by corollaries 1 and 1 we can shrink the bounds of an initial set of possibilities for the parameters. algorithm 1 performs a brute-force computation of a set ubt s.t. c p ¡û ubt  is an implied constraint for a network n. this algorithm works when no extra-information is known from the parameters. it uses corollaries 1 and 1. the input is a parametric constraint and a constraint network  and optionally an initial upper bound ub on the target set . the output is a set ubt of tuples for the parameters  s.t. no solution of the problem is lost if we add c p ¡û ubt  to the network.

algorithm 1: brute-force learning algorithm

input: c p  n  optionally ub ; output: ubt ; begin
lbt ¡û  ;
ubt ¡û |p| ; /* or the tighter ub */ while lbt  ubt and not 'time-out' do choose s   ubt   lbt ;
1 if sol n + c p ¡û s   =   then ubt ¡û ubt   s ; /* corollary 1 */ else
pick e in sol n + c p ¡û s  ;
1 if there is a unique t in ubt s.t. e satisfies
c p ¡û {t}  then
	lbt ¡û lbt ¡È{t} ;	/* corollary 1 */
end

¡¡it is highly predictable that algorithm 1 can be inefficient. the space of possible combinations of values to explore is exponential in the number of parameters. in addition  checking whether n + c p ¡û s  is inconsistent  line 1  is npcomplete. even if n in this learning phase should be only a subpart of the whole problem  it is necessary to use some heuristics to improve the process. fortunately  in practice  constraints have characteristics that can be used. the next section studies some of these properties.
1 using properties on the constraints
for a given parametric constraint c p   different representations of parameters may exist. the complexity of associated filtering algorithms generally differs according to the representation used. the more general case studied in section 1 consists in considering that allowed tuples of parameters for c p  are given in extension as a set s   |p|.
1 partitioning parameters
in practice  many parametric constraints satisfy the following property: any two different combinations of values in |p| for the parameters correspond to disjoint sets of solutions on x c p  . for instance  this is the case for the gcc  x1 ...  xn   pv1 ...  pvk    constraint: each combination of values for the parameters pv1 ...  pvk imposes a fixed number of occurrences for each value v1 ... vk in x1 ...  xn.
definition 1 a parametric constraint c p  is called parameter-partitioning iff for any t t in |p|  if t  t then c p ¡û {t}  ¡É c p ¡û {t}  =  .
¡¡given an instantiation e on x c p    the unique tuple t on p s.t. e satisfies c p ¡û {t}  is denoted by te.
¡¡when a constraint is parameter-partitioning  we have the nice property that there is a unique target set.
lemma 1 given a constraint network n  if a parametric constraint c p  is parameter-partitioning then there exists a unique target set t for c p  in n.
¡¡the next proposition tells us that when a constraint is parameter-partitioning  updating the lower bound on the target set is easier than in the general case.
proposition 1 let c p ¡û ubt  be an implied constraint in a network n and e be a solution of n. if c p  is a parameterpartitioning constraint then the unique tuple te on p s.t. e satisfies c p ¡û {t}  can be put in lbt.
¡¡this proposition is a first way to improve algorithm 1. it allows a faster construction of lbt because any solution of n contributes to the lower bound lbt in line 1.
1 parameters as sets of integers
as far as we know  existing parametric constraints are defined by sets of possible values for their parameters p1 ...  pm taken separately. this means that the target set t must be a cartesian product t p1  ¡Á ¡¤ ¡¤ ¡¤ ¡Á t pm   where t pi  is the target set of values for parameter pi. in other words  t is derived from the sets of possible values of each parameter. this is less expressive than directly considering any subset of |p|  but the learning process is easier to handle. let lbt pi  and ubt pi  be the required and possible values in t pi . corollaries 1 and 1 are specialized in corollaries 1 and 1.
corollary 1 let c p ¡û ubt  be an implied constraint in a constraint network n  with ubt = pi¡Êp ubt pi  . let pi ¡Ê p  s i   ubt pi    lbt pi  and s ji ubt pj  ¡Á s i. if n + c p ¡û s  has no solution then values in s i can be removed from ubt pi .
this corollary says that if a parametric constraint instance
c p ¡û s  is inconsistent with a network whereas all its parameters except one  pi  can take all the values in their upper bound  then all values in s pi  can be discarded from ubt pi .
corollary 1 let c p ¡û ubt  be an implied constraint in a network n and e be a solution of n. given a parameter pi ¡Ê p and a value v ¡Ê ubt pi   if we have t pi  = v for every tuple t in ubt s.t. e satisfies c p ¡û {t}   then v can be put in lbt pi .
¡¡as in section 1  algorithm 1 can be modified to deal with the specific case where parameters are represented as sets of integers. corollary 1 tells us when a value must go in lbt pi  for some parameter pi. corollary 1 tells us when a value can be removed from ubt pi  for some parameter pi.
1 parameters as ranges
the possible values for a parameter pi can be represented by a range of integers  that is  a special case of set where all values must be consecutive w.r.t. the total order on integers. the only possibility for modifying the sets lbt pi  and ubt pi  of a parameter pi is to shrink their bounds min lbt pi    max lbt pi    min ubt pi   and max ubt pi  . this case restricts even more the possibilities for the combinations of parameters allowed  and simplifies the learning process. corollaries 1 and 1 can be specialized as corollaries 1 and 1.
corollary 1 let c p ¡û ubt  be an implied constraint in a constraint network n  where parameters are represented as ranges. let pi ¡Ê p and v   min lbt pi    resp. v   max lbt pi    and s = ji ub pj  ¡Á   ¡Þ..v   resp. s = ji ub pj  ¡Á  v.. + ¡Þ  . if n + c p ¡û s  has no solution then min ubt pi     v  resp. max ubt pi     v .
corollary 1 let c p ¡û ubt  be an implied constraint in a network n and e be a solution of n. given a parameter pi ¡Ê p and a value v ¡Ê ubt pi   if we have t pi  = v for every tuple t in ubt s.t. e satisfies c p ¡û {t}   then min lbt pi   ¡Ü v ¡Ü max lbt pi  .
¡¡thanks to corollary 1  tightening the upper bounds for a given pi is simply done by checking if forcing it to take values smaller than the minimum  or greater than the maximum  value of its lower bound leads to an inconsistency in the network. if yes  we know that all these values smaller than the minimum of lbt pi   or greater than the maximum of lbt pi   can be removed from ubt pi .
1 making the learning process tractable
one of the operations performed by the learning technique described in sections 1 and 1 is a call to a solver to check if the network containing a given instance of parametric constraint is still consistent  line 1 in algorithm 1 . this is npcomplete. a key idea to tackle this problem is that implied constraints learned on a relaxation of the original network are still implied constraints on the original network.
proposition 1 given a network n =  x  d  c  and a network n =  x  d  c   if a constraint c is implied in n and sol then c is implied in n.
selecting a subset of the constraints.
thanks to proposition 1  we can select any subset of the constraints of the network on which we want to learn an implied constraint  and the constraint learned on that subnetwork will be implied in the original network. in the example of section 1  the gcc constraint is only posted on the variables representing tasks and is an implied constraint on the subnetwork where neither the resource constraints nor the precedence constraints are taken into account. thus  the underlying network was a simple one that could easily be solved.
optimization problems.
in an optimization problem  the set of solutions is the set of instantiations that satisfy all constraints and s.t. a cost function has minimal  or maximal  value. if n =  x  d  c  is the network and f is the cost function  let n =  x  d  c¡È{cubf } 
n / d / n1 / n1 / maxr#nodeslearning sec.  + solve  sec. 1 / 1 / 1 / 1 / 11 + 11 / 1 / 1 / 1 / 11 + 11 / 1 / 1 / 1 / 11 + 11 / 1 / 1 / 1 / 11 + 11 / 1 / 1 / 1 / 11 + 11 / 1 / 1 / 1 / 1  unsat 1 + 1table 1: learning an implied gcc on the naive model of section 1: time to learn and solving time.  n tasks of duration d among which nj tasks use j resources. 
where cubf is a constraint accepting all tuples on x with cost below a fixed upper bound ub  we concentrate on minimization . if ub is greater than the minimal cost  then any implied constraint in n accepts all solutions of n optimal wrt f. the idea is thus to run a branch and bound on n for a fixed  short  amount of time. ub is set to the value of the best solution found. then  the classical learning process is launched on the network n + cubf . note that if the learned implied constraint improves the solving phase  it can permit to quickly find a bound ub better than ub. using this new bound ub  we can continue the learning process on n +cubf instead of n + cubf . the learned implied constraint will be tighter because n + cubf is a relaxation of n + cubf . we observe a nice cooperation between the learning process and the solving process.
time limit.
it can be the case that finding a subnetworkeasy to solve is not possible. in this case  we have to find other ways to decrease the cost of the consistency test of line 1 in algorithm 1. a simple way is to put a time limit to the consistency test. if the solver has not finished its search before the limit  ubt is not updated and the algorithm goes to the next loop.
1 experiments
we evaluate our learning technique on the gcc constraint  see section 1 . the gcc global constraint is np-hard to propagate when all elements in its scheme are variables  quimper et al.  1 . however  when the cardinalities of the values are parameters that take values in a range  efficient algorithms exist to propagate it  re¡ägin  1; quimper et al.  1 . in addition  the gcc constraint can express many different features on the cardinalities of the values. this is thus a good candidate for being added as an implied constraint. in all these experiments we learn a gcc where cardinalities are ranges of integers. results in sections 1 and 1 apply directly. we used the choco constraint solver  choco  1 .
satisfaction problem.
the first experiment was performed on the problem of section 1. the learning algorithm was run on the subnetwork where resource constraints and precedence constraints are discarded. the learning is thus fast  1 to 1 milliseconds for n = 1 or n = 1 . we fixed to 1 the limit on the number of consistency tests of the subnetwork with the added gcc  line
n / d / n1 / n1 / maxr#nodessolve  sec. 1 / 1 / 1 / 1 / 1--  1 / 1 / 1 / 1 / 1 1  unsat 1 / 1 / 1 / 1 / 1--  1 / 1 / 1 / 1 / 1 1  unsat 11 / 1 / 1 / 1 / 1 1.1 / 1 / 1 / 1 / 1 1  unsat 1table 1: naive model with 1 tasks  n tasks of duration d among which nj tasks use j resources. 
n / d / n1 / n1 / maxr#nodeslearning  sec.  + solve  sec. 1 / 1 / 1 / 1 / 11 + 11 / 1 / 1 / 1 / 1  unsat 1 + 11 / 1 / 1 / 1 / 11 + 11 / 1 / 1 / 1 / 1  unsat 1 + 11 / 1 / 1 / 1 / 11 + 11 / 1 / 1 / 1 / 1  unsat 1 + 1table 1: learning an implied gcc on the problems with 1 tasks: time to learn and solving time.  n tasks of duration d among which nj tasks use j resources. 
1 in algorithm 1 . table 1 shows the results on a model containing the learned implied gcc. the first time number corresponds to the learning process and the second to the solving time. results in table 1 are almost the same as results in table 1  where the implied gcc was added by the expert user. this shows that a short run of the learning algorithm gives some robustness to the model wrt the solving process: the problem is consistently easy to solve on all types of instances whereas some of them could not be solved on the naive model of section 1. tables 1 and 1 show the results when we increase the number of tasks and their length  n = d = 1 instead of 1  and the number of random precedence constraints  1 instead of 1 . we stopped the search at 1 seconds. we were able to solve all problems with size n = 1 in a few seconds  including the learning time  while most of them are not solvable without the implied gcc even after several minutes. this first experiment shows that when taking a naive model with a naive search strategy  our learning technique can improve the robustness of the model in terms of solving time. the effort asked to the user was just to have the intuition that  there is maybe some hidden gcc related to the ordering of tasks . this is a much lower effort than studying by hand the possible implied constraints for each number of tasks and durations n and d.
optimization problem.
at the institute of technology of the university of montpellier  iut   n students provide a totally ordered list  u1 ... um  of the m projects they prefer: project ui is strictly preferredto project ui+1. the goal is to assign projects s.t. two students do not share a same project  while maximizing their satisfaction: a student is very satisfied if she obtains her first choice  less if she obtains the second one  etc. obtaining no project of her list is the worst possible satisfaction. additional constraints exist such as a limit of 1 projects selected in the set
ninitial modelimplied constraint#nodessolve  sec. #nodeslearning  sec.  + solve  sec. 111.1 + 11 1.1 1.1 + 11-  1 hours1 ¡Á 1.1 + 1min.table 1: learning an implied gcc on the 'projects' optimization problem. n is the number of students
of projects proposed by a teacher. a first model was provided by students from that institute who are novices in constraint programming: students are variables  domains are their set of preferred projects plus an extra-value  1   which means that no project in the preferred list was found. constraints   xi  xj  ¡Å  xi = xj = 1   between each pair  xi  xj  of students express mutual exclusion on projects. each student has a preference-variable measuring her satisfaction. preferencevariables are linked to student-variables by table constraints { u1  ...  um m   1   1 m }. the objective is to minimize the sum of preference-variables. this model is referred as the initial model. we wish to learn an implied gcc on student variables. the solver is first launched on the initial model for a short time  1 sec.  to get a first solution s1 and an upper bound on the quality of solutions  as described in section 1. we had only one real instance  with 1 students   so we derived from the real data some sub-instances with less students  preserving the ratio  numberof projects/numberof students  and the distribution of the choices .
¡¡left hand side of table 1 shows the time and number of nodes with the initial model according to the number of students n. right hand side of table 1 shows the results obtained when learning an implied gcc on the student variables. we fixed to 1 ¡Á n the limit on the number of consistency tests  line 1 in algorithm 1   with a cutoff of 1 seconds for each of them. thus  the maximum learning time grows linearly with the size of the problem. the learning time includes the generation of an initial positive example. the results show a tremendous speed up on the model refined with a learned implied gcc compared to the initial model. this shows that learning implied global constraints in an optimization problem can greatly pay off when the model has been designed by a non cp expert.
1 conclusion
global constraints are essential in improving the efficiency of solving constraint models. we proposed a generic framework for automatically learning implied global constraints with parameters. this is both the first approach that permits to derive implied global constraints and the first approach that learns implied constraints according to the actual domains  not just constraints derived from structural or syntactical properties. we applied the generic framework to special properties that global constraints often satisfy. our experiments show that a very small effort spent learning implied constraints with our technique can dramatically improve the solving time.
