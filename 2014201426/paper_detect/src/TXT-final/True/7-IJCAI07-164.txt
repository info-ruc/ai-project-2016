
we propose a novel variant of the conjugate gradient algorithm  kernel conjugate gradient  kcg   designed to speed up learning for kernel machines with differentiable loss functions. this approach leads to a better conditioned optimization problem during learning. we establish an upper bound on the number of iterations for kcg that indicates it should require less than the square root of the number of iterations that standard conjugate gradient requires. in practice  for various differentiable kernel learning problems  we find kcg consistently  and significantly  outperforms existing techniques. the algorithm is simple to implement  requires no more computation per iteration than standard approaches  and is well motivated by reproducing kernel hilbert space  rkhs  theory. we further show that data-structure techniques recently used to speed up kernel machine approaches are well matched to the algorithm by reducing the dominant costs of training: function evaluation and rkhs inner product computation.
1 introduction
kernel methods  in their various incarnations  e.g. gaussian
processes  gps   support vector machines  svms   kernel logistic regression  klr   have recently become a preferred approach to non-parametric machine learning and statistics. they enjoy this status because of their conceptual clarity  strong empirical performance  and theoretical foundations.
﹛the primary drawback to kernel methods is their computational complexity. gps require the inversion of an n ℅ n  covariance/kernel  matrix  implying a running time of o n1   where n is the size of the training set. svms require similar computation to solve the convex program  although intense research has gone into fast  specialized approximations  scholkopf & smola  1“	 .
﹛state-of-the-art approaches to kernel learning revolve largely around two techniques: iterative optimization algorithms  and learning by representing the solution with only a subset of the original data points. our algorithm applies most directly to the former line of research  although we address the latter in the conclusions.
﹛we propose a novel variant of the conjugate gradient algorithm  kernel conjugate gradient  kcg   designed to speed up learning for kernel machines with differentiable loss functions  e.g. gaussian process mean inference  kernel logistic regression . this algorithm is motivated by the understanding that all gradient-based methods rely  at least implicitly  on a particular metric or inner product  scholkopf & smola “ 1 . it is natural in kernel learning problems for the algorithm to inherit the metric on functions that the kernel provides. in section 1 we show that such an approach can be interpreted as a riemannian metric method similar to amari's natural gradient  amari & nagaoka  1   although the kernel metric is much less expensive to compute.
﹛in section 1  we establish an upper bound on the number of iterations for kcg that indicates it should require fewer than the square root of the number of iterations that standard conjugate gradient requires. the algorithm is simple to implement  requires no more computation per iteration than standard approaches  and is well motivated by reproducing kernel hilbert space  rkhs  theory. in practice  for various differentiable kernel learning problems  we find kcg consistently  and significantly  outperforms existing techniques.
﹛recent research has demonstrated the benefits of spacepartitioning data-structures to speed up certain kernel machines  shen et al.  1; gray & moore  1 . we show that these techniques work well with kcg as they reduce the dominant computational burdens in training: rkhs function evaluations and inner product computations.
1 preliminaries
below we briefly review the theory of kernel machines in terms of reproducing kernel hilbert spaces. we then describe the regularized risk functionals we are interested in optimizing during learning  and finish by reviewing the functional gradient  a generalization of the standard gradient to inner product spaces of functions.
1 reproducing kernel hilbert spaces
an rkhs of functions hk is a complete inner product space  known as a hilbert space  that arises from the completion of a set of basis functions  where k : x ℅x ↙ r is a symmetric  positive-definite kernel function  and x is a continuous domain sometimes known as an index set. a common kernel  and one used exclusively in our experiments  is the exponential radial basis function  rbf 
. the rkhs inner product between two functions is defined as
		 1 
central to the idea of an rkhs is the reproducing property which follows directly from the above definition. it states that the basis functions k x .  ﹋ bkx are representers of evaluation. formally  for all f ﹋ hk and x ﹋ x 
. when the basis functions are normalized  this means the evaluation of f at x is the scalar projection of f onto k x . . note that there exists a simple mapping  between the domain x and the rkhs basis . it follows that for any
.
a complete overview of these concepts can be found in
 aronszajn  1 .
﹛a fundamental result first proven in  kimeldorf & wahba  1  and then generalized in  scholkopf et al.  1“   is the representer theorem  which makes possible the direct minimization of a particular class of functionals. it states that given a subsetr  i.e. the dataset   the minimizer of a functional with the form
 
where c : x ℅ r1 ↙ r is arbitrary and g :  1 ﹢  ↙ r is strictly monotonically increasing  must have the form f  =
.
1 regularized risk functionals
an important class of functionals  common in machine learning  for which the representer theorem holds is the regularized risk functional  scholkopf & smola  1“  :
		 1 
these functionals combine a data-dependent risk term  with a  prior   or regularization  term that controls the complexity of the solution by penalizing the norm of the function f in the rkhs. our work focuses on the case where l is differentiable in its third argument; many important kernel machines have this property. for instance  we can write kernel logistic regression  zhu & hastie  1  in this form. let y ﹋ { 1}. then

similarly  we may write the popular gaussian process regression  for the mean function  and the regularized least squares classification algorithm in this form 1:
	 1  numerous other important examples occur in the literature  scholkopf & smola  1“  . we focus on these  two of the best known kernel algorithms other than the nondifferentiable support vector machine  as they are particularly important tools in machine learning for classification and regression.
1 the functional gradient
for large-scale problems and problems for which direct solution is inapplicable  the most popular approach to optimizing these functionals is through the use of iterative  gradientbased techniques. gradient-based algorithms  such as steepest descent  are traditionally defined with respect to the gradient arising from the euclidean inner product between parameter vectors. there are many ways in which a given regularized risk functional can be parameterized  though  and each gives rise to a different parameter gradient. it is intuitively natural to follow the gradient defined uniquely by the rkhs inner product. we review some basic concepts behind the functional gradient below.
﹛the functional gradient may be defined implicitly as the linear term of the change in a function due to a small perturbation  in its input  mason et al.  1 
.
following this definition using the rkhs inner product  allows us to define the kernel gradient of a regularized risk functional  scholkopf & smola  1“  . we use three basic formulas that can be easily derived from this definition:
1. gradient of the evaluation functional.	define fx :
. then
1. gradient of the square rkhs norm.  : . then
1. chain rule. let g : r ↙ r be a differentiable function  and let f : hk ↙ r be an arbitrary differentiable functional. then.
﹛a straight forward application of the above formulas brings us to the following result. the kernel gradient of a regularized risk functional  equation 1  is
  1 
where we again use 汐i to denote the parameters of the expansion of f in terms of the kernels k xi . .
﹛equation 1 allows us to easily find the kernel gradient of the functionals we described above and use in the remainder of this paper:
kernel logistic regression  klr :

regularized least squares  rls   gaussian process:
		 1 
1 the kernel gradient and riemannian metrics
above we described the kernel gradient as a function; that is as a linear combination of basis functions. the kernel gradient as in equation 1 demonstrates a property similar to the representer theorem: namely that  kf f  =  for appropriate 污i ﹋ r. in other words  the kernel gradient of f is represented in the finite-dimensional subspace sd = span{bkd} of hk. a gradient descent type method through sd then amounts to modifying the coefficients 汐i of the current function f by 污i. that is  we can understand the kernel gradient as modifying parameters  f ↘ f   竹 kf f    汐 i ↘ 汐i   竹污i 
just as a standard gradient descent algorithm would. the difference is that the coefficients 污i for the kernel gradient are not the same as those of the parameter gradient. one can verify that they differ by  汐f f  = k污 where k is the kernel matrix and 污 is the vector of coefficients.
﹛we can derive this relation in another way. starting from the parameter gradient  we define a riemannian metric  hassani  1  on the space of parameters. this defines our notion of size in the space of parameters. we then consider an alternate definition of the gradient  as the direction of steepest ascent for a  small  change in coefficients :
it can be shown that takinggives the vanilla parameter gradient  汐f  while defining the norm with respect to the rkhs inner product 
污tk污 gives the functional gradient coefficients  kf = k 1 汐f.  hassani  1 
﹛this interpretation of the kernel gradient makes connections with other metric methods more clear. for instance  amari  amari & nagaoka  1  considers the use of a metric derived from information geometry that leads to the  natural gradient . such algorithms are applicable here as well since we can compute the metric for the probabilistic models given by gaussian processes or klr. unfortunately  computing the natural gradient in these cases is very expensive: for instance  in gaussian processes it is as expensive as inverting the kernel matrix  the very computational difficulty we are striving to avoid. by contrast  computing the kernel gradient is very cheap: cheaper in fact then the standard parameter gradient.1
algorithm 1 kernel conjugate gradient
1:
1:
1:procedure
 do1:fi+1 ↘ fi + 竹ihi where 竹i = argmin竹 f fi + 竹h  1: 1:	灰i	= 1: 1:
end while1:	return fi 1: end procedure1 the kernel conjugate gradient algorithm
both in theory and in practice it is understood that conjugate gradient  cg  methods outperform standard steepest descent procedures  ashby et al.  1 . these techniques have been used profusely throughout machine learning  in particular  for regularized risk minimization and kernel matrix inversion  gibbs  1; scholkopf & smola  1“	 .
﹛in this section  we present an algorithm we term kernel conjugate gradient  kcg  that takes advantage of conjugate direction search while utilizing the rkhs inner product. algorithm 1 gives the general  nonlinear  kcg algorithm in polak-ribiere form`  ashby et al.  1 . in essence  algorithm 1 comes directly from conjugate gradient by replacing all gradients by their functional equivalents and replacing euclidean inner products with an rkhs inner product.
note that the computational complexity per iteration of
kcg is essentially identical to that of the more conventional parameter conjugate gradient  pcg  algorithm. intuitively  while the kernel inner product takes time o n1  compared to the o n  vanilla inner product used by pcg  kcg is correspondingly more efficient in the gradient computation since
 汐f f  = k污  where. it is possible in the case of rls to step through an iteration of each algorithm and show that the number of operations is equivalent.
﹛we emphasize that despite its somewhat involved derivation  the implementation of this algorithm is just a simple extension of pcg. the differences amount to only a change of inner product  and a different  though in some ways simpler  gradient computation. we also point out that the line optimization  step 1  can be solved in closed-form in the case of quadratic risk functionals  e.g. rls . for starting point and search direction we have
argmin
where a is the hessian of the quadratic functional when parameterized by 汐. note that this formula differs from that derived under the parameter gradient   汐t污/污ta污  only in the numerator's inner product  as is a common theme throughout this algorithm. the theoretical and experimental results given below suggest that there is little reason why one should prefer pcg to kcg in most  differentiable  kernel algorithms.
1 experimental results - kernel conjugate gradient
we bench-marked kcg against pcg for both classification and regression tasks. in all cases  kcg significantly out performed pcg.
﹛our first test was performed using klr on the usps dataset  with a training/test size of 1  for the common one-vs-all task of recognizing the digit 1. we used a length scale hyperparameter 考 = 1 as was used in  rifkin et al.  1  for rls classification  and a regularization constant 竹 = 1. figure 1 summarizes the results in log scale.
﹛second  we used rls for both regression and classification using the abalone and soil datasets in addition to the usps dataset. the abalone dataset 1 consisted of 1 training examples and 1 test examples in 1 attributes. our experimental setup was equivalent to that in  smola & scholkopf “ 1 . the soil dataset contained three-dimensional examples of soil ph levels in areas of honduras partitioned into a training set of size 1 and a test set of size 1. the latter dataset and corresponding hyperparameters  竹 = 1 and 考 = 1  were provided by  gonzalez  1 . again  the results are summarized in figure 1. for rls  there exists a quadratic

that can provide a lower bound to the regularized risk  gibbs  1; scholkopf & smola  1“  . as theory suggests  see below  the upper bound under kcg converges comparably with the lower bound  while the upper bound under pcg lags considerably behind. this implies faster convergence under a gap termination criterion  scholkopf & smola  1“  .
﹛the right-most plots of figure 1 contrast  in iterations  equivalent to multiples of wall-clock time  the speeds of pcg and kcg for both rls and klr. we plot of the number of iterations of each to reach the same level of performance in terms of loss on the data-set. plots are terminated when convergence is achieved as measured with the gap termination criterion scholkopf & smola  1“    and hyper-parameters were chosen on a hold-out set. these figures confirm what analysis in the next section suggests: it takes more than the square of the amount of time to achieve the same level of performance using pcg as it does with kcg. finally  in table 1 we directly compare the number of iterations needed to achieve convergence on a number of data sets  all taken again from the uci data repository  for both rls and klr. averaged over the datasets  kcg is 1 times faster than the standard conjugate gradient approach.
1 kcg analysis
we derived the kernel conjugate gradient algorithm from a normative point of view arguing that defined the natural notion of inner product in the rkhs and hence for the optimization procedure as well. the strong empirical performance of kcg noted in the previous section  while in some sense not surprising given we are using the  correct  inner product  deserves analysis. we examine here the linear case  as in rls  where the analysis is more transparent  although presumably similar results hold near the optima of non-linear risk functionals.
we note a classic bound on the error reduction of cg  see
 luenberger  1   
 
where i is the iteration number  a is the hessian of the quadratic form with condition number is a norm on x  and ei = xi   x . loosely speaking  this gives a﹟ running time complexity of o  百  for pcg.
﹛we start by analyzing the effective condition number of kcg. as with essentially all variants of cg  the algorithm's dynamics can be described in terms of a preconditioning to the spectrum of the hessian  ashby et al.  1 . it can be verified by inspection of the algorithm  that kcg is equivalent to an implicitly preconditioned conjugate gradient algorithm with preconditioner k ashby et al.  1 . the following theorem relates the running time of pcg to kcg in light of the bound given above.
﹛theorem. let 百pcg be the condition number of rrls  equation 1   and let 百k be the condition number of the kernel matrix k =  k xi xj  i j. then the condition number 百kcg resulting from preconditioning the rls risk functional by k has the relation 百pcg = 百k百kcg.
proof. let 考1 ≡ 考1 ≡ ... ≡ 考n be the eigenvalues of
k. the condition number of k is then 百k = 考1/考n. the hessian of rrls is a = ktk + 竹k and has eigenvalues 考i1 + 竹考i = 考i 考i + 竹   given in terms of the eigenvalues of
k. this implies
.
since k is symmetric  positive-definite  the preconditioned
hessian becomes k 1a = k 1 ktk + 竹k  = k + 竹i  with corresponding eigenvalues 考i + 竹. thus  百k百kcg.
﹛the condition number 百k of k is typically very large. in particular  as the regularization constant decreases  the asymptotic bound on the convergence of pcg approaches the square of the bound on kcg. alternatively  as the regularization constant increases  百kcg approaches 1 implying an o 1  convergence bound for kcg  while the convergence bound of pcg remains bounded below by o 百k . we would thus expect a number of iterations for kcg that is dramatically less than that of pcg.
﹛it is informative to note that the decrease in computational complexity from pcg to kcg  o 百1  to o 百1   is at
least the amount we see from steepest descent o 百  to pcg o 百1    luenberger  1 .

figure 1: upper left shows relative performances  in log scale  of kcg and pcg on the usps data set optimizing klr. the remaining three left-most plots show relative convergence under rls; green and red lines depict pcg performance on the upper and lower bound gap-convergence quadratic forms  and the light blue line gives the  significantly tighter  performance of kcg on the upper bound. the third column shows the benefit of using kd-trees for a single run  bottom  and by training set size  top  using kcg. the right-most two plots show the equivalent number of pcg iterations required to achieve the performance per iteration of kcg on the covtype data set from the uci data repository. top right and bottom right show the performances of rls and klr  respectively. an approximately quadratic relationship can be seen in both cases as the theory suggests.1 tree-augmented algorithms
for many stationary kernels  it is often the case that the majority of the basis functions inare nearly orthogonal. this often stems from a relationship between the degree of orthogonality of two basis functions and the
euclidean distance between. this is often the case when using the exponential rbf kernel given above  in which the orthogonality between two basis functions increases exponentially with the square euclidean distance between the two points.
﹛previous work has shown how the evaluation of rkhs functions can be made fast when this holds using n-body type algorithms  gray & moore  1 . intuitively  the idea is to store the training data in a spacepartitioning tree  such as a kd-tree  moore  1  as was used in our experiments below  and recursively descend the tree during evaluation  pruning negligible contributions.
﹛the maximum and minimum impact of each set of pruned points can be easily calculated resulting in upper and lower bounds on the evaluation error. we demonstrate how such data-structures and algorithms can be used to reduce the per iteration o n1  computational cost of both kcg and pcg during learning as well as evaluation.
﹛the inner loop computational bottleneck of kcg is in evaluating functions and calculating the kernel inner product. if we rewrite the rkhs inner product be as
  then re-
ducing the computational complexity of rkhs function evaluations will simultaneously encompass both of these bottlenecks. similarly  the iteration complexity of pcg is dominated by the computation of the parameter gradient. we can rewrite the parameter gradient as  汐f f  =   kf f  x1   kf f  x1  ...  kf f  xn  t  see 1   reducing the complexity to that of finding  kf f  ﹋ hk and evaluating it n times. as was the case with the kcg algorithm without trees  the tradeoff still balances out so that the per iteration complexity is essentially equivalent between kcg and pcg using tree-augmented function evaluation.
﹛noting  f x1  ... f xn  t = k汐 suggests that the closed form quadratic line minimization for both the upper and lower bounds of rls under either kcg or pcg can easily be augmented as well by expanding the hessians au = ktk+竹k in the case of the upper bound and al = k+竹i in the case of the lower bound. this was used in the treeaugmented rls experiments described below.
1 experimental results - tree-augmented algorithms
for these experiments  in addition to using the soil dataset described in section 1  we performed large scale rls regressions using a tree-augmented kcg on variable sized subsets of a pugetsound elevation map1 using hyperparameters 竹 = 1/n and 考1 = kn/ n羽   where n is the size of the training set  and n is the size of the entire height map. in this case  we chose n = 1 ℅ 1 = 1  and k = 1.
the largest resulting datasets were on the order of 1 points. it should be noted that the naive implementation in“ this case did not cache kernel evaluations in a kernel matrix as such matrices for datasets above o 1  points proved
rlscmccovtypeglassionosphereirispage-blockpimaspamwineexamples11111pcg iters11111kcg iters11111speedup111111111klrcmccovtypeglassionosphereirispage-blockpimaspamwineexamples111-11pcg iters111-11kcg iters111-11speedup11111-111figure 1: times  in iterations  to achieve convergence of rls  top  and klr  bottom  on a subset of uci data-sets. kcgdecreases the number of iterations on average by a factor of 1.
intractable for the machine on which the experiments were performed.
﹛figure 1 shows that tree-augmentation significantly outperforms the na“ ve algorithm. extrapolating from the plot on the right  trees make possible accurate kernel learning on very large datasets without requiring explicit subset selection techniques.
1 conclusions and future work
we have demonstrated that the novel gradient method  kernel conjugate gradient  can dramatically improve learning speed for differentiable kernel machines. furthermore  we have shown how this can be understood as a very efficient preconditioning that naturally derives from the inner product on functions defined by the kernel. in practice  for various differentiable kernel learning problems  we find kcg consistently  and significantly  outperforms existing techniques. we emphasize that the algorithm is simple to implement and requires no more computation per iteration than standard approaches.
﹛further  we demonstrated that space-partitioning datastructures  also developed by other authors  shen et al.  1   for optimizing gaussian processes extend naturally to other kernel methods. we find that this approach meshes well with the kcg algorithm  by significantly speeding up the inner loop computations of functions and inner products.
﹛while conjugate gradient is a powerful optimization algorithm  there are other approaches  like limited-memory quasi-newton methods  luenberger  1  that may also be derived in similar ways in terms of the kernel inner product. these algorithms have proved to be practical when using the euclidean inner product; we expect they would also gain the benefits of preconditioning that kcg enjoys.
﹛finally  very large scale kernel applications  it seems  will invariably need to rely on sparse representation techniques that do not use kernels at all of the data points. nearly all of these methods require the efficient solution of large kernel problems using an iterative approximation. it is natural to explore how kernel conjugate gradient can speed up the expensive inner loop of these approximation procedures.
acknowledgements
the authors gratefully acknowledge the partial support of this research by the darpa learning for locomotion contract.
