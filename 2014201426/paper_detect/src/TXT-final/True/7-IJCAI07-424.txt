
while pomdps  partially observable markov decision problems  are a popular computational model with wide-ranging applications  the computational cost for optimal policy generation is prohibitive. researchers are investigating ever-more efficient algorithms  yet many applications demand such algorithms bound any loss in policy quality when chasing efficiency. to address this challenge  we present two new techniques. the first approximates in the value space to obtain solutions efficiently for a pre-specified error bound. unlike existing techniques  our technique guarantees the resulting policy will meet this bound. furthermore  it does not require costly computations to determine the quality loss of the policy. our second technique prunes large tracts of belief space that are unreachable  allowing faster policy computation without any sacrifice in optimality. the combination of the two techniques  which are complementary to existing optimal policy generation algorithms  provides solutions with tight error bounds efficiently in domains where competing algorithms fail to provide such tight bounds.1
1 introduction
partially observable markov decision problems  pomdps  are a popular model with potential applications in health care  disaster rescue and space exploration  hauskrecht and fraser  1; pineau et al.  1; poupart and boutilier  1; nair et al.  1 . unfortunately  the computational cost for optimal policy generation in pomdps is prohibitive  requiring increasingly efficient algorithms to address problems in these large-scale domains. furthermore  as we apply pomdps in domains where property and human safety are of concern  e.g. disaster rescue  patient care and space exploration  error bounds  limiting pomdp policy quality loss  become critical.
¡¡previous work in pomdps has made encouraging progress using two approaches:  1  exact algorithms compute the optimal solution and thus avoid problems with error bounds  cassandra et al.  1; feng and zilberstein  1; 1   however  they do not scale.  1  approximate algorithms  a currently popular approach  address the significant computational complexity in pomdp policy generation by sacrificing solution quality for speed  pineau et al.  1; smith and simmons  1; poupart and boutilier  1 . while some approximate algorithms  point-based approaches  pineau et al.  1; smith and simmons  1   provide expressions for error bounds  they have many drawbacks:  a  the bounds in these point-based approximation algorithms are based on maximum and minimum possible single-stage reward  which can take extreme values  leading to a very loose bound which is not useful in many domains  especially in those that have penalties  e.g.   .  b  the computation of the bound for a particular policy is itself a potentially significant expense  e.g.  requiring a non-linear program .  c  the algorithms cannot guarantee that they will yield a policy that can achieve a pre-specified error bound.
¡¡to address these challenges  we present two new techniques that are complementary to existing optimal policy generation algorithms. in particular  both are presented as wrappers to generalized incremental pruning  gip   cassandra et al.  1  and region-based incremental pruning  rbip   feng and zilberstein  1; 1 . our first technique eva  expected value approximation  is an approximate one that sacrifices quality for speed  but operates within a pre-specified error bound. eva differs from existing approaches  primarily point-based approaches  pineau et al.  1; smith and simmons  1    by approximating directly in the value space as opposed to sampling in the belief space for an indirect value approximation. eva alleviates the three key problems mentioned earlier:  a  eva provides an error bound that does not depend on the maximum and minimum possible reward.  b  the computation of the error bound in itself imposes no overhead when computing policies in eva.  c  given any pre-specified error bound  eva will generate a policy whose quality meets or exceeds that bound.
¡¡while results from eva are promising  in domains that require tight error bounds  eva alone may not be sufficiently fast. our second technique  ddb  dynamic disjunctive beliefs   complements eva. ddb exploits domain structure to prune large tracts of belief space that are unreachable. because ddb does not sacrifice quality  combining it with eva yields the same error bounds as the original eva. we illustrate how ddb combined with eva provides significant performance improvements over eva alone  and over earlier techniques that prune belief spaces  such as  varakantham et al.  1 .
1 background : pomdps
a	pomdp	can	be	represented	using	the	tuple
  where s is the set of states; a is the set of actions; ¦¸ is the set of observations; is the transition function;  denotes the observation model; r s a  is the reward function. as the state is partially observable  pomdp policies are expressed as belief-state-to-action mappings  where a belief state b is a probability distribution over the set of states s. the goal is to compute the policy with maximum expected reward.
¡¡currently  the most efficient exact algorithms for pomdps are value-iteration algorithms  specifically gip  cassandra et al.  1  and rbip  feng and zilberstein  1 . these are dynamic-programming algorithms  where at each iteration the value function and optimal policies are represented by a minimal set of vectors called the parsimonious set. determining the parsimonious set involves pruning out dominated vectors. this requires solving a linear program  lp  for all vectors in the original set. thus  pruning is recognized to be computationally expensive.
¡¡db : the performance of these dynamic programming algorithms can be improved considerably by exploiting the dynamics of the domain  varakantham et al.  1 . dynamic beliefs  db  implemented as a wrapper to gip  eliminates unreachable belief spaces  given information about the starting belief space  b1. it computes belief bounds over states at future time steps by solving maximization and minimization problems on the equation for belief probability updates. given an action a and observation ¦Ø  the maximization problem for belief probability of state st is:

 lagrangian techniques are used to solve these maximization problems efficiently in polynomial time. these bounds can significantly improve lp run-times  as the pruning via lps happens over a restricted belief space. the restricted belief space also leads to fewer vectors in the parsimonious set for a particular interation. this cascades through the steps of the dynamic programming leading to fewer lp calls and consequently  significant speedups. db can lead to orders of magnitude improvement over existing algorithms  varakantham et al.  1 .
¡¡point-based approaches : two leading approximate algorithms for pomdps that provide error bounds are pointbased value iteration  pbvi   pineau et al.  1  and heuristic-search value iteration  hsvi1   smith and simmons  1 . in these algorithms  a policy computed for a sampled set of belief points is extrapolated to the entire belief space. pbvi/hsvi1 are anytime algorithms where the sampled set of belief points is expanded over time. the expansion ensures that the belief points are uniformly distributed over the entire belief space  and the heuristics employed for belief set expansion differentiate pbvi and hsvi1. we compare eva against pbvi  as it is known to be one of the fastest approximate algorithms. the error bound provided by pbvi is:  with b
	1	is the entire belief
space; b is the set of expanded belief points; ¦Ã is the discount factor; rmax and rmin are the extremal single stage rewards. computing b requires solving a non-linear program  nlp1 shown in algorithm 1 . while we provide experimental comparisons with only pbvi  the drawbacks of pbvi with regard to calculating and achieving bounds are equally applicable to hsvi1 due to their similarity.
algorithm 1 non-linear program to obtain b maximize b subject to the constraints
1 expected value approximation  eva 
the value function in a pomdp is piecewise linear and can be expressed by a set of vectors. approximate algorithms generate fewer vector sets than the optimal algorithms. existing approaches generate these reduced vector sets by sampling the belief space and finding the vectors that apply only at these points. in our approach  expected value approximation  eva   we choose a reduced set of vectors by approximating the value space with a subset of vectors whose expected reward will be within a desired bound of the optimal reward.
¡¡using an approximation  subset  of the optimal parsimonious set will lead to lower expected quality at some set of belief points. let  denote the maximum loss in quality we will allow at any belief point. we henceforth refer to any vector set that is at most  away from the optimal value at all points in the belief space  as illustated in fig 1  as an parsimonious set. the key problem in eva is to determine this -parsimonious set efficiently.
to that end  we employ a heuristic that extends the prun-
ing strategies presented in gip. in gip  a parsimonious set v corresponding to a set of vectors  u is obtained in three steps:
1. initialize the parsimonious set v with the dominant vectors at the simplex points.
1. for some chosen vector u ¡Ê u  execute a lp to compute the belief point b where u dominates the current parsimonious set v.
1. compute the vector u with highest expected value in the set u at the belief point  b; remove vector u from u and add it to v.
¡¡eva modifies the first two steps  to obtain the parsimonious set:
1. since we are interested in representing the optimal parsimonious set with as few vectors as possible  the initialization process only selects one vector over the beliefs in the simplex extrema. we choose a vector with the highest expected value at the most number of simplex belief points  choosing randomly to break ties.
1. the lp is modified to check for -dominance  i.e.  dominating all other vectors by  at some belief point. algorithm 1 provides a modified lp with bmaxt	and bmint	.
algorithm 1 lp-dominate 
1: variables: d b st   st ¡Ê st
1: maximize d
1: subject to the constraints
	b  	  ¡Ý d +	 	u
1:
1: if d ¡Ý 1 return b else return nil
¡¡the key difference between the lp used in gip and the one in algorithm 1 is the  in rhs of line 1 which checks for expected value dominance of the given vector w over a vector u ¡Ê u. including  as part of the rhs constrains w to dominate other vectors by at least . in the following propositions  we prove the correctness of the eva algorithm and the error bound provided by eva. let denote the
-parsimonious and optimal parsimonious set  respectively. proposition 1  b ¡Ê ¦¤  the entire belief space  if
= argmax
+	b ¡¤
proof. we prove this by contradiction. assume  b ¡Ê ¦¤ such that . this implies  and. we now consider the situation s  when vb  is considered by eva. at these instants  there will be a current parsimonious set v and a set of vectors still to be considered u. let  b = argmax
be the belief point at whichis best w.r.t. v. let v  b = argmaxv¡Êv v ¡¤ b
be the vector in v which is best at  b. let
u  b = argmaxu¡Êu u ¡¤ b
be the vector in u which is best at  b. there are three possibilities:
1. : this impliesby the definition of  b  we have
	¡¤	 	¡¤	¡¤	 	¡¤
where v b = argmaxv¡Êv v ¡¤ b. this implies

which is a contradiction.
1.: this means vb  would have been included in the -parsimonious set   which is a contradiction.
1.will be included in v and vb  is returned to u to be considered again until one of previous two terminal conditions occur. 

v  b  vector of optimal set
v¦Å b  vector of approximate set
figure 1: eva: an example of an -parsimonious set
proposition 1 the error introduced by eva at each stage of the policy computation  is bounded byfor gip-type cross-sum pruning.
proof. the eva algorithm introduces an error of  in a parsimonious set whenever a pruning operation  prune  is performed  due to proposition 1. in gip  there are three pruning steps at each stage of policy computation.
1. va o = prune va o i : after this step  each va o is at most  away from optimal  a  o.
1. v prune va o1 ¨’ va o1 ¡¤¡¤¡¤ ¨’ which is away from opti-
mal by at most . each pruning operation adds 1 to the bound   for the new termfor the prune .
there are |¦¸|   1 prune operations. thus  each va o is away from the optimal by at most .
1.  is bounded by the error of va. the  leading to a total one-stage error bound of . 
proposition 1 the total error introduced by eva  for giptype cross-sum pruning  is bounded by  for a thorizon problem.
proof. let denote the eva-policy and optimal value function  respectively  at time  is the set of actions at the roots of all policy-trees associated with   the eva vector set for time t and et = maxb¡Êb{ t       t    }  then 

the last inequality is due to proposition 1  and the other relations are by definition. the above implies that et 1 =  and with   we have the error for the eva-policy at the beginning of execution  the
total error bound for eva . 
¡¡similarly  it can be proved that for ¦Ã-discounted infinite horizon problems  the total error bound is .
1 dynamic disjunctive beliefs  ddb 

figure 1: ddb: an example of ddb vs. db
¡¡by eliminating reasoning about optimal policies at unreachable beliefs  it is possible to improve policy computation time greatly without sacrificing any solution quality. our key insight is that the reachable belief space is composed of small belief polytopes; hence we introduce an efficient technique for exploiting these smaller polytopes. this contrasts with the previous work on dynamic beliefs  db by  varakantham et al.  1   which assumed a single large belief polytope. in particular  db obtains the maximum belief probability for a state given an action and observation  ba ¦Ø t max s  by solving a constrained optimization problem using lagrangian techniques. similarly minimum possible belief probability is also computed. the dynamic belief polytope at epoch t was created as follows:
1. find the maximum and minimum possible belief foreach state over all actions and observations: bmaxt  s  = maxa ¦Ø ba ¦Ø t max s   bmint  s  = maxa ¦Ø ba ¦Ø t min s .
1. create a belief polytope that combines these bounds overall states: btdb =
	 bmint	 s1  bmaxt	 s1   ¡Á ¡¤¡¤¡¤ ¡Á  bmint	 s|s|  bmint	 s|s|  .
¡¡while this is an appropriate bound in that any belief outside btdb is not possible at time t  in many domains there is not a single belief polytope  but a collection of smaller belief polytopes - because future beliefs depend on particular action-observation pairs. unfortunately  db insists on a single belief polytope  thus including many unreachable beliefs. figure 1 illustrates this for a two-observation twoaction system over a two-dimensional support. while btdb is smaller than the entire space of potential beliefs  it is larger than necessary; it is not possible to believe anything outside ai ¦Øj
of ¡Èi=1;j=1bt . in contrast  our new method for expressing feasible beliefs is referred to as dynamic disjunctive belief  ddb  bounds. the ddb method for computing feasible belief spaces is to eliminate conditioning on action and observations  eliminate taking max over a ¦Ø in  1  above :
1. obtain ba ¦Ø t max s  and ba ¦Ø t min s    a ¡Ê a  ¦Ø ¡Ê ¦¸ 1. create multiple belief polytopes  one for each actionobservation pair: btai ¦Øi =
 bati ¦Øi min s1  bati ¦Øi max s1   ¡Á ¡¤¡¤¡¤
¡¤¡¤¡¤ ¡Á  btai ¦Øi min s|s|  bati ¦Øi max s|s|  .
¡¡the feasible belief space is btddb = ¡Èa ¦Øbta ¦Ø. however  this is disjunctive and cannot be expressed in the lp that is used for pruning. instead  the prune occurs over each bta ¦Ø and we take the union of dominant vectors for these belief spaces. this increases the number of lp calls for a fixed epoch but the lps cover smaller spaces and will yield fewer vectors at the end. the cascading effects of the reduced vector sets  as in eva  over the dynamic programming process can lead to fewer policies at the next stage  resulting in significant computational savings.
1 experiments
our experiments1 show how eva and ddb address key shortcomings of current pomdp algorithms when a policy that meets a pre-specified error bound is desired. three key results provided in this section relevant to eva are:  1  eva can guarantee the generation of a policy that meets any prespecified error bound where point-based algorithms cannot  section 1 .  1  the cost of calculating the error bound of a policy  which eva avoids  can significantly impact performance of point based approaches section 1 .  1  there are domains  where point-based algorithms cannot provide functional bounds  section 1 . for ddb  we show an orderof-magnitude speedup over the previously best technique for belief space pruning  section 1 .

figure 1: error bounds for hallway and hallway1
1 bound achievability
in the general case  point-based approaches can always encounter problems where they cannot generate a policy that meets a pre-specified error bound. if the span of belief support  chosen independently from the value function  is smaller than the feasible belief space  there exists a value function for which the algorithm will have a minimum error  i.e.  the solution quality cannot exceed some threshold. if the prespecified error bound is smaller than this minimum error  the algorithm will never be able to provide a satisfactory policy. current point-based approaches exacerbate this problem by  a  starting with one belief point and  b  only considering points that are reachable from this initial point. thus  they potentially restrict the span of their belief support severely which implies a significant minimum error. figure 1 presents the pbvi error bound for the hallway and hallway1 problems as the belief set is expanded over time. the x-axis represents the number of belief set expansions  and the y-axis represents the error bound  by solving an nlp for each belief set expansion . the error bound remains constant for both problems after the fifth expansion  around 1 belief points in the set . after the tenth expansion  around 1 belief points  the nlp becomes too expensive to calculate. thus  for instance  for hallway1  it is not verifiable if pbvi would be able to reach an error bound of 1. however with eva  by using an   in algorithm 1  corresponding to the pre-specified error bound  obtained by equating error bound to    any error bound is achievable.
1 bound calculation cost
now let us consider the case where a given point-based approach can generate a policy whose quality is within a prespecified error bound. these algorithms still do not know the quality loss in their policy as they expand their belief support. as indicated earlier  they require an expensive nlp to calculate the error bound 1. we ran both eva and pbvi for the tiger-grid  hallway  hallway1  aircraft  scotland yard and tag problems. as pbvi depends on the starting belief point  for each problem we obtained the bound for 1 randomly selected initial belief points. for each given initial belief point  we ran pbvi for a fixed number of belief set expansions and computed the bound of the resulting policy with an nlp. the deviations in the obtained bounds were extremely small. this gives an advantage to pbvi by assuming that it has the correct belief support for the obtained bound immediately. additionally  we only counted the average run-time of the final expansion.
¡¡we then ran eva with the equivalent error bound  forcing eva to obtain a policy as good or better  bound calculation for eva is trivial  i.e.  constant-time computation . furthermore  eva was also run with a tighter bound  half the value obtained for pbvi   to address the possibility that an alternate belief expansion heuristic in pbvi led to a better bound. the results are shown in figure 1. the nlp bar denotes the time taken for computing the error bound in pbvi  while the pbvi bar shows the run-time for computing the policy corresponding to the final expansion  not including the nlp run-time . the bars for eva  same bound  and eva  half bound indicate the time taken by eva when provided with a bound equal to and half of that used by pbvi respectively.

1
we employed an industrial optimization software called lingo
1 for solving nlps. machine specs for all experiments: intel xeon
1 ghz processor  1gb ram
we note that:  1  the same-bound eva run-times are less than nlp run-times  i.e. just the bound computation ovehead of pbvi is a significant burden.  1  even without the nlp  eva run-times are better than pbvi run-times in all cases.  1  the half-bound eva run-times are better than nlp run-times in all but one case. we see that a hampered eva outperformed an ameliorated pbvi. hsvi1  smith and simmons  1  outperforms pbvi in run-time but suffers from the same problems i.e. the time to compute the bound and the unattainability of the bound. beyond the experimental results  point-based approaches will suffer as their belief supports get larger because the nlp cost increases exponentially.

figure 1: run time comparison of eva and pbvi
1 bound functionality
even if the nlp cost vanished  there are domains where the expression for error bound in point-based approaches yields a value that is not functional. consider the task management problem  tmp   varakantham et al.  1  in software personal assitant domains. there is a penalty   associated with certain state-action pairs to control agent autonomy. in this case  the output of the nlp yields an error bound so loose with respect to the actual solution quality that it renders the bound useless. there is no method for a pointbased approach to determine when to stop expanding the set of belief sets in tmps.
1 ddb
as shown in figure 1  the tmp domain also illustrates the benefits of ddb . gip and rbip did not terminate for tmp within the pre-specified time limit. with a sufficiently large bound  eva produced a policy within our time limits. to improve performance  one option was to use eva with a larger bound  but this would have meant an additional sacrifice in solution quality. eva+db reduced run-time without sacrificing solution quality. we see that eva+ddb  combining our two new techniques  provided an order-of-magnitude speedup beyond eva+db  also without losing solution quality. runtimes for eva+ddb and eva+db included the overhead for computation of the belief bounds.
1 experiment summary
we demonstrated the drawbacks of current point-based approaches when tasked with generating policies with prespecified error bounds. these limitations include the inability to guarantee a policy within the bound  an expensive cost to compute the bound even if achievable  and a non-functional expression of the bound even if computable. we also show that the run-times of eva are not only shorter than the runtimes of pbvi but also to the nlp calculation run-times alone. finally  we exemplify the significant benefit of ddb in domains with structure in belief evolution as it most tightly bounds the dynamic belief polytope.

figure 1: comparison of gip  rbip eva  eva+db  eva+ddb
1 related work
as mentioned earlier  techniques for solving pomdps can be categorized as exact and approximate. gip  cassandra et al.  1  and rbip  feng and zilberstein  1; 1  belong to the class of exact algorithms that provide optimal policies at the expense of enormous computational complexity. db is a wrapper that exploits the dynamics of the domain to provide improved performance.
¡¡there has been much work in the area of approximate algorithms which are faster than exact algorithms at the cost of solution quality. point-based  smith and simmons  1; pineau et al.  1   policy-search  braziunas and boutilier  1; poupart and boutilier  1; menleau et al.  1   and grid  hauskrecht  1; zhou and hansen  1  approaches dominate other algorithms. point-based approaches are discussed in section 1. policy-search approaches typically employ a finite state controller  fsc  to represent the policy. the fsc is updated until convergence to a stable controller. gridbased methods are similar to point-based approaches. the difference is maintaining  values  at belief points  as opposed to  value gradients  in point-based techniques. another approach that scales well is the dimensionality-reduction technique in  roy and gordon  1  which applies e-pca  an improvement to principal component analysis  on a set of belief vectors to obtain a low-dimensional representation of the original state space. though all these algorithms significantly improve the run-time performance  they provide loose or no quality guarantees on the solutions.
1 summary
despite the wide-ranging applicability of pomdps  obtaining optimal policies are often intractable and current approximate approaches do not sufficiently address solution quality. indeed  many domains demand tight error bounds on policies generated in pomdps. to that end  we present two new techniques that are complementary to existing optimal policy generation algorithms: eva and ddb. eva is a new type of approximation technique that directly approximates in the value space. due to this  it is able to obtain solutions efficiently for a pre-specified error bound. furthermore  it does not require costly computations to compute the bound. ddb prunes out large tracts of belief space that are unreachable in many real-world problems  allowing faster policy computation without any sacrifice in optimality. the combination of the two techniques is shown to provide high quality solutions efficiently in domains with dynamic structure.
