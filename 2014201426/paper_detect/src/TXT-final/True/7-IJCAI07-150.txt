
choosing good features to represent objects can be crucial to the success of supervised machine learning algorithms. good high-level features are those that concentrate information about the classification task. such features can often be constructed as non-linear combinations of raw or native input features such as the pixels of an image. using many nonlinear combinations as do svms  can dilute the classification information necessitating many training examples. on the other hand  searching even a modestly-expressive space of nonlinear functions for high-information ones can be intractable. we describe an approach to feature construction where task-relevant discriminative features are automatically constructed  guided by an explanation-based interaction of training examples and prior domain knowledge. we show that in the challenging task of distinguishing handwritten chinese characters  our automatic feature-construction approach performs particularly well on the most difficult and complex character pairs.
1 introduction
sensitivity to appropriate features is crucial to the success of supervised learning. the appropriateness of a feature set depends on its relevance to the particular task. various notions of relevance have been proposed  blum and langley  1; kohavi and john  1  and statistical tools to evaluate and select relevant feature sets are available  e.g. hypothesis testing  random probes  cross-validation . unfortunately  most feature construction strategies focus on feature subset selection  e.g. filters  almuallim and dietterich  1; kira and rendell  1   wrappers  kohavi and john  1   embedded methods  guyon et al.  1   from either a predetermined set or a space defined by some feature construction operators  markovitch and rosenstein  1  and require many training examples to evaluate.
¡¡in challenging domains  the  native   observable features  e.g. pixel values of an image  are high-dimensional and encode a large amount of mostly irrelevant information. there are standard statistical techniques such as principal components analysis  pca  and linear discriminant analysis  lda  to reduce the dimensionality of the input features. but often the high-level features that simplify the task are complex nonlinear combinations of the native features. we believe the most effective approach is to incorporate additional information in the form of prior world knowledge. in this direction  one approach gabrilovich and markovitch  1 utilizes the largerepository of the open directoryproject to aid in natural language classification. we address a very different problem of handwritten chinese character recognition. our additional world knowledge is derived from a relatively small  imperfect  and abstract prior domain theory.
¡¡explanation-based learning  ebl  is a method of dynamically incorporating prior domain knowledge into the learning process by explaining training examples. in our character recognition task for example  we know that not all pixels in the input image are equally important.  sun and dejong  1  used this observation to learn specialized feature kernel functions for support vector machines. these embody a specially constructed distance metric that is automatically tailored to the learning task at hand. that approach automatically discovers the pixels in the images that are more helpful in distinguishing between classes; the feature kernel function magnifies their contribution.
¡¡however  we know that it is not the raw pixels that intrinsically distinguish one character from one another. rather  the pixels are due to strokes of a writing implement and these strokes in relation to one another distinguish the characters. not telling the learner of these abstract relationships means that it must perform the moral equivalent of inventing the notion of strokes from repeated exposure to patterns of raw pixels  making the learning task artificially difficult. our system learns a classifier that operates directly on the native features but which appreciates the abstractions of the domain theory as illustrated by the training examples.
¡¡in section 1  we describe what makes a goodhigh-levelfeature. we then detail the role of explanations and give our general algorithm in section 1. in section 1 we illustrate our approach with detailed feature construction algorithms for chinese character recognition  and in section 1 we show some experimental results. section 1 concludes.
1 high level features and generalization
the criteria for feature construction are usually based on how well a particular set of features retains the information about the class while discarding as much irrelevant and redundant information as possible. information-theoreticmethods  based on mutual information  channel coding and ratedistortion theoremshave been applied to this problem battiti  1; tishby et al.  1; torkkola  1 . let x and y be randomvariables that represent the input and the label respectively  and there is an underlying joint-distribution on  x y  . it can be shown  feder and merhav  1  that the optimal bayes error is upperboundedby . therefore a feature f x  that retains as much information as possible about
the label will have  where
represents the amount of information loss that we are willing to tolerate. on the other hand  discarding irrelevant information can be achieved by minimizing h f x |y   while satisfying the condition on h y |f x  . intuitively  this implies that most information about the class label is preserved  while irrelevant information  e.g. within-class variations  is discarded. we show how this bounds the actual risk for the binary case in the appendix  regardless of what hypothesis space is used.
	when comparing alternative features 	each satisfying
  we therefore prefer the one
with the smallest h f x |y  . alternatively  we may aim at minimizing the following functional:

unfortunately  without additional knowledge about the underlying probability distribution  it is impossible to accurately estimate the conditional entropy empirically from the data when the training set is small. for example  when all the training examples have different x  one can simply define a feature f based on the nearest neighbor rule  itself a classifier  and empirically  with a naive estimator  achieve
j  f  = 1. there is no reason to believe that the feature f x  and the resulting classifier will generalize to unseen examples. in this sense  building a right feature is as hard as building the right classifier  that is  it does not work without any inductive bias.
¡¡however  it is possible that for some tasks  there exist features that are known a priori to have j f  ¡Ö 1. such features capture our knowledge about patterns that are unique to a particular class of objects. what is not known  however  is a reliable way to detect or extract the feature from any unlabeled input. the problem of constructing a good feature can therefore be viewed as the problem of building a good detector for such  high-level  features  based on the training data. if we can ensure that the detector produces the right output for the right reason  i.e.  it detects the intended high-level feature  then we will have high confidence that the resulting feature will generalize well.
¡¡how do we verify that a detector produces the right output for the right reason  doing so statistically  if possible at all  will require too many training examples for many tasks. instead  if available domain knowledge can be used to build explanations for the training examples  then they can be used to verify whether a feature's output is consistent with our prior knowledge. we show how to use this idea to automatically construct features that focus on the most informative part of any input object with regard to the particular task.
1 explanation-based feature construction
in classical ebl  an  explanation  is a logical proof that shows how the class label of a particular labeled example can be derived from the observed inputs. but our version is weaker. we only require the explanation to identify potential low level evidence for the assigned classification label. we then use training data to calibrate and evaluate the strength of that evidence.
¡¡our prior knowledge includes ideal stroke models of the characters of interest  roughly of the sort one obtains from a vector font  and the model of a stroke as a connected straight line of finite width.
¡¡feature construction is performed by the following steps which we state abstractly and describe specifically in the context of the chinese character domain.
1. explain each training example to obtain the association between the assigned label and the observed native features mediated by high-level domain theory concepts. after this step  each pixel is associated with a stroke  the line that most likely made it  constrained so that the line configurations match the stroke requirements of the assigned character.
1. using the prior knowledge  identify 1  high-level features that are similar in both categories  and 1  highlevel features that are different between the categories. the first set form our reference strokes. these can be confidently found in new test images since a similar stroke should be present for either instance type. the second set are information-bearing; they are character dependent.
1. with the generated explanations  evaluate each potential similarity statistically using the training set  keeping the ones that can be detected efficiently and with high confidence. these are strokes that are easily found in an unlabeled image and form a frame of reference to guide us to the high class-distinguishing information.
1. using the training examples  optimize the process of finding detection features from the reference features. this identifies the high informationimage regions w.r.t. the reference strokes. the regions chosen to be larger if over the training set there is greater variance of the location of the detection strokes w.r.t. the reference strokes and tighter otherwise.
¡¡generally  finding lines in an image is problematic. many lines will be missed and often non-lines will be found. the process can be expensive. however  this is only done for labeled examples during the learning phase. thus  we know what lines we should find and their approximate geometrical configuration. this greatly improves the line-finder's performance. but what if we can find no reference strokes  if there are no easily-found similarities between the categories  then the two classes must be very different; the classification task should be simple. many features and algorithms should be able to differentiate them  and our knowledge-basedapproach is unnecessary. next we examine this process in more detail for chinese characters.
1 classifying handwritten chinese characters
offline handwritten chinese character recognition remains a challenging task due to the large variability in writing styles and the similarity of many characters. approaches are either structural or feature-based  suen et al.  1 . the former extract strokes from a new input image and try to match the extracted strokes to the known stroke-level models. extracting strokes is unreliable and consequently the modelmatching process is problematic. feature-based approaches utilize statistics on well-designed features and have proven to be more robust. however  the features are hand-crafted and it is not easy to exploit prior knowledge during the learning process; similar characters are difficult to differentiate using such globally-defined features.
¡¡in this paper  we focus our attention to the task of distinguishing pairs of similar characters. in our approachautomatically constructed features are tailored to best differentiate the characters.
consider the pair of chinese characters in figure 1.

figure 1: two very similar chinese characters
¡¡they are almost identical except the leftmost radical. extracting a global feature that summarizes the whole character dilutes the class-relevant information concentrated on the far left of the image.
¡¡once the informative region has been identified  there is still much variability in the exact location of the informative region. figure 1 illustrates the variability among the first character of the pair.

figure 1: within-class variability
¡¡attempting to define an absolute set of pixels  say  one 1rd of the image from the left  would result in noisy features. too small the region we risk missing the important stroke for some of the characters  too large the region our advantage of focused feature is lost. this is where we utilize our knowledge about similarities between the two characters. the three long  roughly vertical strokes present in both characters may serve as  reference strokes.  finding them allows the target region to be more accurately identified.
1 building explanations
our prior model of each character is a graph  where nodes represent a stroke and edges represent the relationship between strokes. each stroke is itself modeled as a line segment with 1 parameters  x y ¦È l t  denoting its center  direction  length and thickness. such models can either be hand-specified  or obtained from existing character-stroke database.1 the model need not be highly accurate as the explanation process relies primarily on its structural information. the model is used to explain each training example by finding the most likely parameters for each requisite character stroke.
¡¡in general  searching for the best set of parameters is a combinatorial problem for the general graph. this may still be acceptable since the size of the graph is small and the process is done only once for each character during training.
¡¡for efficiency  we structure these graphs into trees to employ a dynamic programming approach to produce the explanations. we use an algorithm based on  felzenszwalb and huttenlocher  1 . our implementation uses two automatically generated trees  focusing on horizontally and vertically oriented strokes separately. figure 1 shows a character model and an example explanation.

figure 1: a character model and an explained example
1 identifying potential similarities
given the models for a particular pair of characters  we perform graph matching to identify strokes that are similar in terms of location  direction and length. the result of this process is the identification of a set of strokes m which have a match in both characters. we refer to this set as the matching set m. these are the candidates for reference strokes. figure 1 shows an example.
1 finding efficient reference stroke detector
any efficient feature extractor can be used in this step. since we are concerned with lines  we use a hough transform  forsyth and ponce  1 . in particular  we performa hough transform on an input image  and look for a local minimum in a specified region which reflects the variability of the matching set stroke in the training data. we refer to this as the  hough detector . not every stroke in m can be reliably detected by the hough detector. we use the following algorithm

figure 1: the strokes in m are shown as dotted lines
to select from the matching set a set of reference strokes that can be reliably detected. the explanation for each training example is used to measure how accurately the hough detector detects a particular stroke.
1. initialize the set of reference stroke r to empty
1. for each stroke s in m
 a  find the range of directions and offsets for thisstroke among all the training examples  from the explanations   namely  find the smallest bounding rectangle with the center  ¦È ¡¥ ¦Ñ¡¥   the width ¦¤¦È  and the height ¦¤¦Ñ.
 b  for each ¦Á ¡Ê {1 1.1.1.1} and each ¦Â ¡Ê
{1 1.1.1.1}
i. define the bounded region in hough space as arectangle centered at  ¦È ¡¥ ¦Ñ¡¥  with width ¦Á¦¤¦È and height ¦Â¦¤¦È.
ii. for each training example
a. search for the highest peak in the region
b. check whether the peak is within a thresholddistance ¦Ó from the actual stroke orientation
iii. record the hit ratio h ¦Á ¦Â   percentage of reference strokes correctly detected using the specified parameters 
 c  find ¦Á  and ¦Â  with the highest h ¦Á ¦Â 
 d  if h ¦Á  ¦Â     h1 then add s to r
¡¡the range of the detector window  ¦Á and ¦Â  in the above algorithm is chosen for simplicity. the thresholds ¦Ó  distance in hough space  and h1 are optimized using cross-validation.
1 learning the final feature
once reference strokes are identified  the system estimates the informative region relative to the parameters of the reference strokes. we use a simple definition for our  informative region . in each character  every stroke that is not in m is considered a potentially informative stroke. from the explanations  we know the location of these strokes in the training examples. using these locations  we find the smallest rectangle that includes each stroke. whenever there are overlapping strokes  the two rectangles are combined into a single larger rectangle bounding both strokes. figure 1 illustrates this.
¡¡feature points  which can be the center or endpoints of a reference stroke  receive a distance score with respect to each edge of the target window  where the score is define as a¦Ä + b¦Ò. this combines the mean distance  ¦Ä  to the window edge and its standard deviation given the reference strokes. the

figure 1: the ideal  target  rectangles
feature point with the smallest score is selected. if none of the feature points qualifies  then the edge of the image is used as the definition of the target window. figure 1 illustrates this.

figure 1: the actual  target  rectangles with respect to the feature-points. note that there are no feature-points for the left and the bottom edge.
¡¡we assume a joint-distribution on the location of the reference strokes and the target  window  as defined by the feature-points. each reference stroke is parameterized by the direction and the offset ri =  ¦Ñi ¦Èi  obtained from the hough detector. each target window is parameterized by 1 parameters l =  l1 l1 l1 l1  which correspond to left  top  right and the bottom of the window. for example  k reference strokes and one target window form a joint-distribution on  r l  where r =  r1 r1 ... rk . the joint distribution is estimated from the training examples  since we know both r and l from the explanations. for unlabeled examples  we first apply the hough detector to localize the reference strokes  r =  r1 ... rk   then find the maximum-likelihood location of the window according to the conditional probability p l|r = r . assuming that the joint-distribution is
gaussian with mean

and covariance
		 
the conditional is itself gaussian with mean
¦Ìl|r = ¦Ìl + ¦²lr¦² rr1  r   ¦Ìr 
and covariance
	¦²l|r = ¦²ll   ¦²lr¦² rr1 ¦²rl	.
pairsvmsvm ebl pairsvmsvm ebl 1.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.1111.1.111table 1: error rate  %   1-fold cross-validation 
1 experiments
we evaluate our system on pairwise classifications between difficult pairs of characters. we use the etl1b database  a popular database of more than 1 chinese and japanese characters  each with 1 examples . we first learn a literature-standard multiclass classifier using linear discriminants to identify 1 most difficult pairs  i.e. pairs of characters that result in greatest confusion . these are  as expected  pairs of very similar characters. we use the weighted direction code histogram  wdh   kimura et al.  1  as features. these features are generally the best or among the best for handwritten chinese character recognition  ding  1 .
¡¡for each pair of characters  we learn a classifier using a linear support vector machine. we observe that the support vector machine performs significantly better than those with linear discriminants. for our system  we use the same classifier  but the input to the svm are the wdh features extracted only from the target feature window found with respect to the detected reference strokes. table 1 shows the results of the experiment on the 1 most challenging pairs of characters. even though the svm is generally robust in the presence of irrelevant features  our system managed to achieve significant improvement on many of these pairs.1
1 conclusion
we believe that the key to generalization is to incorporate available domain knowledge into the learning process. we show that tailored discriminative features can be constructed from comparisons of generative models built according to the prior domain knowledge. this approach is particularly attractive in problems where training examples are few but high level domain knowledge is available  as demonstrated in the task of classifying handwritten chinese characters.
acknowledgements
this material is based upon work supported by the national science foundation under award nsf iis 1. any opinions  findings  and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the national science foundation.
appendix
we show how a feature f x  with small h f x |y   and small h y |f x   leads to better generalization bound in terms of rademacher complexity  for the specific case of binary classification. we make use of the following theorem:
theorem 1.  bartlett and mendelson  1  let p be a probability distribution on x ¡Á{¡À1}  let f be a set of {¡À1}valued functions defined on x  and let  be training samples drawn according to pn. with probability at least
1   ¦Ä  every function f in f satisfies

where p n is the empirical risk. rn f  is the rademacher complexity of f  given by

where ¦Ò1  ...  ¦Òn are independent {¡À1}-valued random variables.
¡¡the following lemma shows that rn g  is bounded if h f x |y   is bounded.
lemma 1. given discriminative feature f  with h f  x |y   ¡Ü ¦Â   1  where y takes values from
{¡À1}  the rademacher complexity of a set g of {¡À1}-valued functions is bounded as follows

proof. from the concavity of entropy  it can be shown that given the label y = y  there exists v such that
pv|y	=	pr f  x  = vy|y  =	max	pr f  x  = u|y 
u:f  x =u
	¡Ý	1   ¦Â1	.	 1 
we will call this vy the  typical  value of f  x  given y. given the training samples  x1 y1  ...  xn yn   and the rademacher random variables ¦Ò1 ... ¦Òn  let n  and n+ denote the expected number of examples with y =  1 and y = +1 according to p y = y . the expected number of examples where f  x  is typical is then given by n t = n pv| 1 and n+t = n+pv|+1 respectively.
consider the rademacher average defined in theorem 1.
supposethat the hypothesisspace g contains all {¡À1}-valued functions  that is  there is always a hypothesis g  ¡Ê g that attains the maximum rademacher average. however  regardless of the function g  chosen  only one label can be assigned to xi with the same value  in particular  those with the typical value. the value ¦Òi = ¡À1 is assigned with equal probability to all xi  therefore the expected number of examples that are wrongly labeled is. since there are at least
n other examples with typical value that will be correctly labeled  the expected sum of these will cancel each other. from equation  1  . the
rademacher average is therefore
	.

theorem 1. given discriminative feature f  with h f  x |y   ¡Ü ¦Â   1  where y takes values from
{¡À1}. let g be a set of {¡À1}-valued functions defined on {f  x  : x ¡Ê x}. with probability at least 1   ¦Ä  every function g in g satisfies

proof. apply lemma 1 to theorem 1.	
corollary 1. given discriminative feature f  with h y |f  x   ¡Ü ¦Á and h f  x |y   ¡Ü ¦Â   1  where
y takes values from {¡À1}. given a sufficiently rich space of hypothesis space g  the risk bound will be minimized if both ¦Á and ¦Â are minimized.
proof. from the risk bound in theorem 1  the first term on the right hand side  which is the empirical risk  can be minimized  given the assumption that g is sufficiently rich  by minimizing h y |f  x     or equivalently  by minimizing ¦Á. the second term on the right hand side can be minimized by minimizing ¦Â.	
¡¡we note that the bounds in this section are only meaningful when h f  x |y     1. in practice  this cannot be easily achieved while preserving a near zero h y |f  x    and this suggests that the interaction between domain knowledge and data should play an important role in the construction of features that may eventually approach this ideal quality. this motivates our algorithm.
