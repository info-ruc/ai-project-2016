 
we articulate the three major fmdings of ai to date: 
 1  the knowledge principle: if a program is to perform a complex task well  it must know a great deal about the world in which it operates.  1  a plausible extension of that principle  called the breadth hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge  and analogizing to specific but far-flung knowledge.  1  ai as empirical inquiry: we must test our ideas experimentally  on large problems. each of these three hypotheses proposes a particular threshold to cross  which leads to a qualitative change in emergent intelligence. together  they determine a direction for future ai research. 
1. the knowledge principle 
for over three decades  our field has pursued the dream of the computer that competently performs various difficult cognitive tasks. the evidence suggests the need for the computer to have and use domain-specific knowledge. 
intelligence is the power to rapidly find an adequate solution in what appears a priori  to observers  to be an immense search space. so  in those same terms  we can summarize the empirical evidence:  knowledge is power  or  more cynically  intelligence is in the eye of the  uninformed  beholder.  the knowledge as power hypothesis has received so much confirmation that we now assert it as: 
the knowledge principle  kp : a system exhibits intelligent understanding and action at a high level of competence primarily because of the specific knowledge that it can bring to bear: the concepts  facts  representations  methods  models  metaphors  and heuristics about its domain of endeavor. 
the word specific in the kp is important. knowledge is often considered compiled search; despite that  the kp claims that only a small portion of the knowledge can be generalized so it applies across domains  without sacrificing most of its power. why  many searches are costly  and it's not costly to preserve the knowledge for future use. we all know about electricity  but few of us have flown kites in thunderstorms. in other words  generality is not enough; if you stop after acquiring only the general methods  your search for solutions to problems will not be constrained adequately. 
there is a continuum between the power of already knowing and the power of being able to search for the solution; in between lie  e.g.  generalizing and analogizing and plain old observing  for instance  noticing that your 
edward a. feigenbaum 
computer science department 
stanford university 
stanford  ca 1 
opponent is castling.  even in the case of having to search for a solution  the method to carry out the search may be something that you already know  or partial-match to get  or search for in some other way. this recursion bottoms out in things  facts  methods  etc.  that are already known. though the knowledge/search tradeoff is often used to argue for the primacy of search  we see here that it equally well argues for the primacy of knowledge. 
before you can apply search or knowledge to solve some problem  though  you need to already know enough to at least state the problem in a well-formed fashion: 
the well-formedness threshold: for each task  there is some minimum knowledge needed for one to even formulate it. 
a more positive way to view this threshold is that a large part of solving a problem is accomplished by having a good representation; that determines what distinctions to make explicitly and which ones are irrelevant. 
beyond this bare minimum  today's expert systems  es  also include enough knowledge to reach the level of a typical practitioner performing the task  feigenbaum 1 . up to that  competence  level  the knowledge-search tradeoff is strongly tipped in favor of knowledge: 
the competence threshold: difficult tasks succumb nonlinearly to knowledge. there is an ever greater  payoff  to adding each piece of knowledge  up to some level of competence  e.g.  where an np complete problem becomes polynomial . beyond that  additional knowledge is useful but not frequently needed  e.g.  handling rare cases.  
crossing the competence threshold  one enters the realm of experts. there  the knowledge-search tradeoff is fairly evenly balanced; otherwise  the general practitioners would have all acquired such knowledge themselves. most current es  in what we would still call the  first era  of expert systems  incorporate an amount of knowledge greater than that minimal level of competence for a task  yet less than all the existing expert knowledge about that task: 
the total expert threshold: eventually  almost all of the rare cases are handled as well. continuing to add knowledge beyond this expert level is even less useful  per piece of knowledge added . 
human experts in a field are distributed between the competent and total expert levels  see figure 1 . this does not mean that other knowledge is useless  just that it is not already understood to be relevant; e.g.  even very far-flung knowledge might be useful to analogize to. 
1 

perfor 
mance 
	amount known 	-* 
figure 1. the level of performance of a program for some task  as a function of the amount of knowledge it embodies. 
the thresholds are  w ell-formed   c ompetent  and total  e xpert. beyond that last one lies  unrelated  knowledge. 
the above arguments describe how the kp might work; but why does it work so frequently  many useful real-world tasks are sufficiently narrow that the competence threshold can be crossed with only 1 -1 if/then rules  and an equal number of additional rules takes one much of the way toward the total expert threshold. moreover  current experts don't already have all those rules explicitly codified; standard software design methodology can't build a program  in one pass  to perform the task. however  as the developing es makes mistakes  the experts can correct them  and those corrections incrementally accrete the bulk of the hitherto unexplicated rules. in this manner  the system 
incrementally approaches competence and even expertise. 
the newly added rules need to seamlessly interface to the existing ones; to put this the other way  you can never be sure in advance how the knowledge already in the system is going to be used  or added to  in the future. thus: 
explicit knowledge principle: much of the knowledge in an intelligent system needs to be represented explicitly  although compiled forms of it may also be present . 
when knowledge - including procedural knowledge - is represented as explicit objects  mcta-rules can apply to it  
e.g.  helping to acquire  check  or debug other rules. such knowledge objects can be more easily analogized to  and enable generalizations to be structurally induced from them. 
what about the control structure of an intelligent system  even granted that lots of knowledge is necessary  might we not need sophisticated as-yet-unknown reasoning methods  
knowledge is all there is hypothesis: no sophisticated  as-yet-unknown control structure is required for intelligent behavior; control strategies are knowledge  and a standard evaluator can apply them. 
on the one hand  we already understand deduction  induction  analogy  specialization  generalization  etc.  etc.  well enough to have knowledge be our bottleneck  not control strategies. on the other hand  all such strategies and methods are themselves just pieces of knowledge. the control structure of the intelligent system can be opportunistic: select one strategy  apply it for a while  monitor progress  and perhaps decide to switch to another strategy  when some other piece of knowledge suggests it do so.  
1. evidence for the knowledge principle 
fifty years ago  before the modern era of computation began  turing's theorems and abstract machines gave a hint of the fundamental idea that the computer could be used to model the symbol-manipulating processes that make up that most human of all behaviors: thinking. 
thirty years ago  following the 1 dartmouth summer conference on ai  the work began in earnest. the founding principle of the ai research paradigm is really an article of faith  first concretized by newell and simon: 
the physical symbol system hypothesis: the digital computer has sufficient means for intelligent action; to wit: representing real-world objects  actions  and relationships internally as interconnected structures of symbols  and applying symbol manipulation procedures to those structures. 
the early dreaming included intelligent behavior at very high levels of competence. turing speculated on wideranging conversations between people and machines  and also on expert level chess playing programs. newell and simon also wrote about champion chess programs  and began working with cliff shaw toward that end. gelemter  moses  samuel  and many others shared the dream. 
at stanford  lederberg and feigenbaum chose to pursue the 
ai dream by focusing on scientific reasoning tasks. with buchanan and djerassi  they built dendral  a program that solved structure elucidation problems at a high level of competence. many years of experimenting with dendral led to some hypotheses about what its source of power might be  how it was able to solve chemical structure problems from spectral data. namely  the program worked because it had enough knowledge of basic and spectral chemistry. 
figure 1  below  shows that as each additional source of chemical knowledge was added  the dendral program proposed fewer and fewer candidates  topologically plausible structures  to consider. see  buchanan et al . when rules of thumb for interpreting nmr data were also added to the program  many problems -- such as the one illustrated -resulted in only a single candidate isomer being proposed as worth considering! threatened by an a priori huge search space  dendral managed to convert it into a tiny search space. that is  dendral exhibited intelligence. 
	information source 	# of structures generated 
topology  limits of 1d space  chemical topology  valences  
mass spectrograph  heuristics  
chemistry  first principles  
nmr  interpretation rules  1 1 
1 1 
1 1 
1 1 
1 figure 1: dendral working on a typical problem: finding all atom-bond graphs that could have the formula c1n. 
when searching a space of size 1  it is not crucial in what order you expand the candidate nodes. if you want to speed up a blind search by a factor of 1 million  one could perhaps parallelize the problem and  say  by 1  employ a 

1 	panels and invited talks 

1-mega-processor; but even back in 1 one could  alternatively  talk with the human experts who routinely solve such problems  and then encode the knowledge they 

bring to bear to avoid searching. 	distant  e.g.  a failed love or a broken arm . 

obvious  perhaps  in retrospect. but at the time  the prevailing view in ai  e.g  the advice taker  ascribed power to the reasoning processes  to the inference engine and not to the knowledge base. the knowledge as power hypothesis stood as a contra-hypothesis. it stood awaiting further empirical testing to either confirm it or falsify it. 
the 1 were the time to start gathering evidence for or against the knowledge principle. medical and scientific problem solving provided the springboard. shortliffe's mycin program formed the prototype for a large suite of expert-level advisory systems which we now label  expert systems.   feigcnbaum  its reasoning system was simple  exhaustive backward chaining  and ad hoc in parts. dec has been using and extending mcdermott's rl program since 1; its control structure is also simple: exhaustive forward chaining. these es could interact with professionals in the jargon of the specialty; could explain their line of reasoning by displaying annotated traces of rulefirings; had subsystems  resp.  teresias and salt  which aided the acquisition of additional knowledge by guiding the expert to find and fix defects in the knowledge  rule  base. 
in the past decade  thousands of expert systems have mushroomed in engineering  manufacturing  geology  molecular biology  financial services  machinery diagnosis and repair  signal processing and in many other fields. 
very little ties these areas together  other than that in each one  some technical problem-solving is going on  guided by heuristics: experiential  qualitative rules of thumb - rules of good guessing. their reasoning components are weak; in their knowledge bases lies their power. in the details of their design  development  and performing lies the evidence for the various adjunct propositions from sec. 1. 
in the 1's  many other areas of ai research began making the shift over to the knowledge-based point of view. it is now common to hear that a program for understanding natural language must have extensive knowledge of its domain of discourse. or: a vision program must have an understanding of the  world  it is intended to analyze scenes from. or even: a machine learning program must start with a significant body of knowledge which it will expand  rather than trying to learn from scratch. 
1  the breadth hypothesis 
a limitation of first-era expert systems is their brittleness. they operate on a high plateau of knowledge and competence until they reach the extremity of their knowledge; then they fall off precipitously to levels of ultimate incompetence. people suffer the same difficulty  too  but their plateau is much broader and their fall is more graceful. part of what cushions the fall are layer upon layer of weaker  more general models that underlie their specific knowledge. 
for example  if an engineer is diagnosing a faulty circuit s/he's unfamiliar with  s/he can bring to bear general electronics knowledge  circuit analysis techniques  experiences with the other products manufactured by the same company  handbook data for the individual components  common sense familiarity with water circuits  looking for leaks  or breaks   electrical devices  turn it off and on a 
few times   or mechanical devices in general  shake it or smack it a few times.  s/he mightanalogize to the last few times their car's engine failed  or even to something more are we  of all people  advocating the use of general problem solving methods and a breadth of knowledge  yes! that does not contradict the kp  since most of the power still derives from a large body of specific task-related expertise. but a significant component of intelligence is still due to: 
the breadth hypothesis  bh : intelligent performance often requires the problem solver to fall back on increasingly general knowledge  and/or to analogize to specific knowledge from far-flung domains. 
domain-specific knowledge represents the distillation of experience in a field  nuggets of compiled hindsight. in a situation similar to the one in which they crystallized  they can powerfully guide search. but when confronted by a novel situation  we turn to generalizing and analogizing. 
generalization often involves accessing a body of general knowledge  one that's enormous  largely present in each person  yet rarely passed on explicitly from one person to another. it is consensus reality:  water flows downhill    living things get diseases    doing work requires energy    people live for a single  contiguous  finite interval of time . lacking these simple common sense concepts  es' mistakes often appear ridiculous in human terms: a skin disease diagnosis program might decide that a ten year old car with reddish spots on its body had measles. 
analogy involves partial-matching from your current situation to another  often simpler  one. why does it work  there is much common causality in the world; that leads to similar events a and b; people  with our limited perception  then notice a little bit of that shared structure; finally  since we know that human perception is often limited  people come to rely on the following rule of thumb: 
analogical method: if a and b appear to have some unexplained similarities  then it's worth your time to hunt for additional shared properties. 
this rule is general but inefficient. there are many more specialized ones for successful analogizing in various task domains  in various user-modes  e.g.  by someone in a hurry  or a child   among analogues with various epistemological statuses  depending on how much data there is about a and b  and so on. these are some of the n dimensions of analogy-space; we can conceive having a special body of knowledge - an es -in each cell of that n-dimensional matrix  to handle just that sort of analogical reasoning. 
why focus on causality  if cause a  and cause b  have no specific common generalization  then similarities between a 
and b are more likely to be superficial coincidences  useful perhaps as a literary device but not as a heuristic one. 
the above is really just a rationalization of how analogy might work. the reason this frequently succeeds has to do with three properties that happen to hold in the real world: 
 1  the distribution of causes wrt effects. if there were a vast number of distinguishable kinds of causes  or if there were only a couple  then analogy would be less useful. 
 1  the moderately high frequency with which we must cope with novel situations  and the moderate degree of novelty they present. lower frequency  or much higher or lower novelty  would decrease the usefulness of analogy. 
	lenat and feigenbaum 	1 

 1  the obvious metric for locating relevant knowledge -namely   closeness of subject matter  - is at best an imperfect predictor. far-flung knowledge can be useful. 
analogizing broadens the relevance of the entire knowledge base. it can be used to construct interesting and novel interpretations of situations and data; to retrieve knowledge that has not been stored the way that is now needed; to guess values for attributes; to suggest methods that just might work; and as a device to help students learn and remember. today  we suffer with laborious manual knowledge entry in building ¡ês  carefully codifying knowledge and placing it in a data structure. analogizing may be used in the future not only as an inference method inside a program  but also as an aid to adding new knowledge to it 
successful analogizing often involves components of both vertical  simplifying  and horizontal  cross-field  transformation. for instance  consider reifying a country as if it were an individual person:  russia is angry . that accomplishes two things: it simplifies dealing with the other country  and it also enables our vast array of firsthand experiences  and lessons learned  about inter-personal relations to be applied to international relations. 
1. evidence for the breadth hypothesis 
if we had as much hard evidence about the bh as we do for the kp  we would be calling it the breadth principle. still  the evidence is there  if we look closely at the limits of what ai programs can do today. for brevity  we will focus on natural language understanding  nl  and machine learning  ml   but similar results are appearing in most other areas of ai as well. as mark stefik recently remarked to us   much current research in ai is stalled. progress will be held back until a sufficient corpus of knowledge is available on which to base experiments.  
1 the limits of natural language understanding 
to understand sentences in a natural language  one must be able to disambiguate which meaning of a word is intended  what the referent of a pronoun probably is  what each ellipsis means ... these are knowledge-intensive skills. 
1 saw the statue of liberty flying over new york. 
1. the box is in the pen. the ink is in the pen. 
1. mary saw a dog in the window. she wanted it 
1. napolean died on st. helena. wellington was saddened. 
figure 1. sentences presume world knowledge furiously. 
consider the first sentence in fig. 1. who's flying  you or the statue  clearly we aren't getting any clues from english to do that disambiguation; we must know about people  statues  passenger air travel  the size of cargo that is shipped by air  the size of the statue of liberty  the ease or difficulty of seeing objects from a distance ... on line 1  one  pen  is a corral  the other is a writing implement. on line 1  does  it  refer to the dog or the window  what if we'd said  she smashed it   a program which understood line 1 should be able to answer 'did wellington hear of 
napolean's death ' and  did wellington outlive napolean   
for any particular sample text  an nl program can incorporate the necessary body of twentieth century americana  of common sense facts and scripts  may be 
1 	panels and invited talks 
required for semantic disambiguation  question answering  anaphoric reference  and so on. but then one turns the page; the new text requires more semantics to be added. 
in a sense  the nl researchers have cracked the language understanding problem. but to produce a general turingtestable system  they would have to provide more and more semantic information  and the program's semantic component would more and more resemble the immense kb mandated by the breadth hypothesis. 
have we overstated the argument  hardly; if anything we have drastically understated it! look at almost any newspaper story  e.g.  and attend to how often a word or concept is used in a clearly metaphorical  non-literal sense. once every few minutes  you might guess  no! reality is full of surprises. the surprise here is that almost every sentence is packed with metaphors and analogies  lakoff . an unbiased sample: here is the first article we saw today  april 1   the lead story in the wall street journal: 
'texaco lost a major ruling in its legal battle with pennzoil. the supreme court dismantled texaco1 s protection against having to post a crippling $1 billion appeals bond  pushing texaco to the brink of a chapter 1 filing.  
lost  major  battle  dismantled  posting  crippling  pushing  brink  the example drives home the point that  far from overinflating the need for real world knowledge in language understanding  the usual arguments about disambiguation barely scratch the surface.  drive  home  the point  far  overinflating  scratch  surface  oh no  i can't stop!!!  these layers of analogy and metaphor eventually  bottom out  at physical - somatic - primitives: up  down  forward  back  pain  cold  inside  see  sleep  taste  growth  containment  movement  birth  death  strain  etc. 
nl researchers - and dictionaries - usually get around analogic usage by allowing several meanings to a word. definition #1 for  war  is the literal one  and the other definitions are various common metaphorical uses of  war.  
there are many hundreds of thousands - perhaps a few million - things we authors can assume you readers know about the world: the number of tires an auto has; who reagan is; what happens if you fall asleep when driving what we called consensus reality. to use language effectively  we select the best consensus image to quickly evoke in your mind the complex thought we want to convey. if our program doesn't already know most of those million shared concepts  experiences  objects  processes  patterns ...   it will be awkward for us to communicate with it in nl. 
it is common for nl researchers to acknowledge the need for a large semantic component nowadays; schank and others were saying similar things a decade ago! but the first serious efforts have only recently begun to try to actually build one  cyc  lenat 1  and the japanese electronic dictionary research  edr  project   so we will have to wait several years until the evidence is in. 
1. the limits of machine learning  induction  
we will pick on am and eurisko because they exemplify the extreme knowledge-rich end of the current ml spectrum. many experiments in machine learning were performed on them. we had many surprises along the way  and we gained an intuitive feel for how and why heuristics 

work  for the nature of their power and their brittleness.  lcnat &brown  presents many of those surprises. 
despite their relative knowledge-richness  the ultimate limitations of these programs derive from their small size. not their small number of methods  which were probably adequate  but the small initial knowledge base they had to draw upon. one can analogize to a campfire fire that dies out because it was too small  and too well isolated from nearby trees  to start a major blaze. as porter recently remarked to us: nothing new is learned except with respect to what's already known. minsky cites a variant of this relationship in his afterword to  vinge : the more you know  the more  and faster  you can learn. 
learning can be considered a task. like other tasks  it is subject to the knowledge principle. the inverse of this enabling relationship is a disabling one  and that's what ultimately doomed am and eurisko: 
knowledge facilitates learning  catch 1 : if you don't know very much to begin with  don't expect to learn a lot quickly. 
this is the standard criticism of pure baconian induction.  to get ahead  get a theory.  without one  you'll be lost. it will be difficult  or time-consuming  to determine whether or not each new generalization is going to be useful. this theme is filtering into ml in the form of explanation-based learning and goal-based learning. 
don't human beings violate this catch  starting from nothing  maybe  but it's not clear what we start with. evolution has produced not merely physically sophisticated structures  but also brains whose architecture is well suited to learning many of the simple facts that are worth learning about the world. other senses  e.g.  vision  are carefully tuned as well  to supply the brain with data that is already filtered for meaning: edges  shapes  motion  etc. the exploration of those issues is beyond the scope of this paper  and probably beyond the scope of twentieth century science  but neonatal brains are far from tabula rasae. 
besides starting from well-prepared brain structures  humans also have to spend a lot of time learning. it is unclear what processes go on during infancy. once the child begins to communicate by speaking  then we are into the symbolic sort of learning that ai has traditionally focused on. 
1. the empirical inquiry hypothesis 
we scientists have a view of ourselves as terribly creative  but compared to nature we suffer from a poverty of the imagination; it is thus much easier for us to uncover than to invent premature mathematization keeps nature's surprises hidden. e.g.  contrast the astonishing early empirical studies by piaget  stages of development  with his subsequent five decades of barren attempts to mathematize them. this attitude leads to our central methodological hypothesis  our paradigm for ai research: 
empirical inquiry hypothesis  eh : intelligence is still so poorly understood that nature still holds most of the important surprises in store for us. so the most profitable way to investigate ai is to embody our hypotheses in programs  and gather data by running the programs. the surprises usually suggest revisions that start the cycle over again. progress depends on these experiments being able to falsify our hypotheses; i.e.  these programs must be capable of behavior not expected by the experimenter. 
early ai programs often surprised their builders: newell and simon's lt program and gelernter's geometry program  circa 1. then fascination with axiomatizing and proving set in  and surprises from  the real world  became rare. the inverse to the empirical inquiry hypothesis is cruel: 
if one builds urograms which cannot possibly surprise him/her  then one is using the computer either  a  as an engineering workhorse  or  b  as a fancy sort of word processor  to help articulate one's hypothesis   or  at worst   c  as a  self-  deceptive device masquerading as an experiment. 
most expert systems work falls into the former category; 
dart 's use of mrs exemplifies the middle  genesereth ; pup1  by the young lenat  and hacker  by the young sussman  exemplify the latter category. 
pup1 could not avoid synthesizing the one program it was built to synthesize; it  succeeded  but taught us nothing about intelligence. the am program was the direct result of lenat's violent recoil away from that methodology. there was no particular target behavior that am was designed with; rather  it was an experiment: what would happen if a moderate sized body of a few hundred math heuristics were applied to a starting set of 1 simple math concepts. am provided hundreds of surprises  including many experiments that led to the construction of eurisko. eurisko ran for several thousand cpu hours  in half a dozen varied domains. once again  the ultimate limitation was not what we expected  cpu time   or hoped for  the need to learn new representations   but rather something at once surprising and daunting: the need to have a massive fraction of consensus reality already in the machine. progress along our path was due to running large experiments: 
difficult problems hypothesis: there are too many ways to solve simple problems. raising the level and breadth of competence we demand of a system makes it easier to test and raise its intelligence. 
cognitive psychology  e.g.  traditionally sidesteps hard-toquantify phenomena such as scientific creativity or reading and comprehending a good book  in favor of very simple tasks such as remembering nonsense syllables  tf a  messy  task is studied  then usually either  1  it is abstracted and simplified beyond recognition  e.g.  bacon   or  1  the psychologist focuses on  and varies  one specific variable  so  respectable  statistical tests for significance can be run. 
much of the confusion in our field may be due to our casual mixing together of two quite different things: ai goals and ai strategies for achieving those goals. the confusion arises because many entries appear on both lists. but almost any strategy can apply toward any goal. e.g.   1  an expert system strategy for a language understanding goal might be to build a rule based system containing rules like  if a person gets excited  they break more grammatical rules than usual.  by contrast   1  a language understanding strategy for an expert system goal might be an english front end that helps an expert enter and edit rules. 
all scientific disciplines adopt a paradigm: a list of the problems that are acceptable and worthwhile to tackle  a list 
lanat and feiaenbaum 1 
of the methods that can and should be tried  and the standards by which the results are judged. adopting a paradigm is done for reasons of cognitive economy  but each paradigm is one narrow view. adding to the confusion  some paradigms in ai have grown up both around the various goals and around the various strategies! 
finer distinctions can be drawn  involving the tactical choices to be made  but this turns out to be misleading. how  tactics that appear to be superficially different may share a common source of power  lenat 1 : e.g.  predicate calculus and frames both rely on a judicious dividing up of the world. and some tactics which appear superficially similar to each other may draw on very different sources of power  e.g.  if/then rules representing logical assertions  versus if/then rules of good guessing.  
the kp and bh and eh are all strategic statements. each could be prefaced by the phrase  whichever of the ultimate goals for ai you are pursuing ...  the strategic level is  apparently  the level where one needs to take a stand. this is rarely stated explicitly  and it is rarely taken into account by news media or by conference organizers. 
1. a programme for ai research 
ai must somehow get to that stage where - as called for by kp and bh -- learning begins to accelerate due to the amount already known. learning will not be an effective means to get to that stage  unfortunately; we shall have to hand-craft that large  seed  kb one piece at a time. in terms of the graph in figure 1  all the programs that have ever been written  including am and eurisko  lie so far toward the left edge of the x-axis that the learning curve is more or less horizontal. several of the more successful recent additions to the suite of ml techniques can be interpreted as pushes in the direction of adding more knowledge from which to begin the learning. 

figure 1. the rate at which one can learn new knowledge. one can also integrate these three curves wrt time  to see how the total amount known might grow over time. 

the graph in figure 1 shows learning by discovery constantly accelerating: the more one knows  the faster one can discover more. once you speak fluently  learning by talking with other people is more efficient than rediscovery  until you cross the frontier of what humanity already knows  the vertical line at x=f   at which point there is no one to tell you the next piece of knowledge. 
by contrast  the rate of hand coding of knowledge is fairly constant  though it  too  drops to zero once we cross the boundary of what is already known by humanity. the hand-coding rate may slope down a bit  since the time to find related concepts will increase perhaps as the log of the 
1 	panels and invited talks 
size of the kb. or  instead  the hand-coding rate may slope up a bit  since copy & edit is a powerful technique for knowledge entry  and  as the kb grows  there will be more chance that some very similar concept is already present. 
this is an example of eh  the empirical inquiry hypothesis presented in section 1 : only by trying to hand-code the kb will we see which of those two counteracting factors outweighs the other  and by how much. only by continued work on nl and ml will we determine whether or not there is a region  near where all three curves meet  where ml temporarily surpasses nl as a way to grow the kb. only after our program crosses the frontier f will we find out if the discovery curve begins to slope up or down. 
figure 1 suggests a sweeping three-stage research programme for the coming three decades of ai research: 
 * slowly hand-code a large  broad knowledge base 
 * when enough knowledge is present  it will be faster to acquire more through reading  assimilating data bases  etc. 
 * to go beyond the frontier of human knowledge  the system will have to rely on learning by discovery  carrying out research and development projects to expand its kb. 
three decades  not three centuries  yes! the usefulness and timeliness of the bh rests on this quantitative assumption: 
breadth is within our grasp: a kb of under a million frames will provide a significant performance increase  due to generalization and analogy; this will consume - 1 person-centuries of time  
 $1 million  and -1 decade. why such a  small size   that's about all that people know! 
 under a million things! what an insult!  you may say.  you just argued that the world is a complicated place. surely we human beings each know an effectively infinite number of things! it's hopeless to try to represent an appreciable fraction of that  so we may as well settle on writing programs that know only 1 specific things.  
what goes on during the 1 hours between birth and age 1   certainly most of it is spent gathering experiences  building up long term memories; some conscious time  and perhaps some sleep time  is spent generalizing and organizing one's memories. much of what we're learning is quite specific to our selves  our home  our family  our friends  and our culture.  humor results from thrusting someone into a different culture; e.g.  crocodile dundee  a connecticut yankee  beverly hillbillies  the gods must be crazy.  
three recent independent estimates of the number of concepts  frames  needed for full breadth of knowledge all came up with a figure of approximately one million:  1  cyc: 1 articles x 1 frames per article 
 1  edr: 1k words x 1 frame for each of a few languages 
 1  minsky: 1 ltm entries/hour from birth to adulthood 
two other ways for bounding the  bits  a human brain can store lead to much larger numbers:  1  counting neurons and synapses; but its unclear how memories are stored in them;  1  counting pixels in our  mental images ; but controversy rages in cognitive psychology over whether mental imagery is just an illusion caused by the consistency and regularity in the world that lets us fill in missing pieces of memories - and of dynamic sensory experiences - with default values. so it's unclear what those larger numbers signify.  also  though it's clearly an over-simplification  having a million entries means that there can be a trillion one-step inferences involving pairs of them.  

here again is a case where various theories give various estimates  and the way to settle the issue - and  perhaps  achieve the goal of having the kb we want - is to go off and try to build the large kb. along the way  it will no doubt become clear how big it is growing and what the actual obstacles are that must be overcome. 
1. differences with other positions 
1. our position regarding the aesthetes 
people do prefer -- and should prefer - the simplest consistent hypothesis about any phenomenon. that doesn't make them correct  of course. a geocentric cosmology is the proper one to have  until data to the contrary piles up. chapter 1 of  lakatos  presents the historical series of mathematicians' retreats from the initial form of the euler-descartes conjecture to increasingly longer  less elegant versions  with more and more terms to take obscure cases and exceptions into account 
if only there were a secret ingredient for intelligence the maxwell's equations of thought. if only we could axiomatize the world  and deduce everything. if only our learning program could start from scratch. if only our neural net were big enough. if only the world were like that. but it isn't. the evidence indicates that almost all the power is in the bulk knowledge. as whitehead remarked   god is in the details.  
this difference about the elegance and simplicity  or not  of the world leads to a deep methodological difference between our way of doing ai and the aesthetes'. following the difficult problems hypothesis  we are firmly convinced that the ai researcher must make a major time commitment to the domain s  in which his/her programs are to be competent; e.g.  the two years that stefik and friedland spent to learn about molecular biology before doing molgcn; the decade-long time frame for cyc  lenat et al  1 . 
we may be exhausting the range of potent experimental ai theses that can be carried out in two years  by a student starting more or less from scratch; witness the trend to give the computers and thought award to increasingly less recent graduates. the presence of a large  widely-accessable  testbed  kb should enable a new round of important theses. 
many ai researchers quest for an elegant solution in a desperate desire for scientific respectability. the name of our field -  artificial intelligence  - invites everyone to instantly form an opinion. too bad it wasn't called quantum cognodynamics.  but perhaps  by interposing a layer of mathematical formalism  we can come to be accepted as hard scientists.  hence the physics-envy! 
formalizing has never driven any early science along. in designing new drug molecules  the biochemist knows it's too inefficient to apply schroedinger's wave equation to compute the energy minimizations  hence from his/her point of view the fact that such a deep understanding even exists is irrelevant to the task at hand. s/he relies on crude design heuristics  and the drug companies using this methodology occasionaly are enriched. as minsky remarked about the a* algorithm in 1:  just because it's mathematical doesn't mean it deserves to be taught.  
eventually  we will want layers of increasing  neatness.  e.g.  in physics  students learn each year that last year's equations were a special case. we always try to reason at the highest  most superficial  most efficient level at which we can  and delve down one level deeper when we are forced to. but devoting so much effort to the attempt at  neatness  today just drains time and intellectual energy away from the prime enterprise of the field. 
one popular type of aestheticism that deserves mention is the trend to highly parallel  e.g.  connectionistic  and ever faster devices. the trouble is that most difficult tasks in knowledge-rich areas can't be highly parallelized. if we were to set a million people to work trying to find a cure for cancer  we wouldn't find one in .1 seconds. each cancer experiment  takes months or years to perform  and there are only a moderate number of promising experiments to do at any one time; their results will determine what the next round of promising experiments should be. 
parallelism is useful at one extreme for implementing very carefully engineered algorithms  e.g.  systolic algorithms   and at the other extreme for allowing a community of meaningfully-individuated agents act independently  asynchronously. for most technical tasks  until we understand the task very well  the size of such an actor community that we can design is typically only -1. 
the time to perform a task often increases exponentially with its size  e.g.  looking ahead n moves in chess.  taking a microcoding approach or a parallelizing approach cuts off a constant factor; taking a knowledge based approach may add a constant overhead but more importantly  for the long run  it may chip at the exponent. cf. figure 1 again. on the other hand  it is worth remarking that there are some special tasks where the desired level of performance  x-coordinate  is fixed: beating all humans at chess  understanding spoken words in real time  tracking the space shuttle in real time  etc. in such a case  getting a large enough constant factor speedup really could solve the problem  with no need to apply the kp  bh  or eh. as our ambition to attack ever more difficult problems grows  though  the exponential nature of the search hurts worse. 1. our position regarding expert systems 
the kp underlies the current explosion of work on expert systems. still  there are additional things our position argues for  that arc not yet realized in today's es. 
one major power source for es  the reason they can be so readily constructed  is the synergistic additivity of many rules. using a blackboard  erman et al.  or partitioned rule sets  it is possible to combine small packets of rules into mega-rules: knowledge sources for one large expert system. 
the analogue at the next higher level would be to hook hundreds of large es together  and achieve even greater synergy. that dream fails to materialize. as we increase the domain of each  element  we are trying to couple together  the  glue  we need gets to be larger and more sophisticated. it seems to us that it will require the large system mandated by the breadth hypothesis  before the true potential of es technology will be realized. 
plateau-hopping requires breadth: to couple together a large number of disparate expert systems will require something approaching full consensus reality - the million abstractions  models  facts  rules of thumb  representations  etc  that we all possess and that we assume everyone else does. 
as we try to combine es from various tasks  even 
	lenat and feigenbaum 	1 

somewhat related tasks  their particular simplifications and idiosyncracies prevent synergy. the simplifying was done in the interests of highly efficient and competent problem solving; breadth was not one of the engineering goals. 
this naturally results in each es being a separate  simplified  knowledge universe. when you sit down to build an es for a task -- say scheduling machines on a factory floor -- you talk to the experts and find out the compiled knowledge they use  the ways they finesse things. for instance  how do they avoid general reasoning about time and belief  probably they have a very simple  very specialized data structure that captures just the bits of information about time and belief that they need to solve their task. how do they deal with the fact that this milling machine m has a precise location  relative to all the others; that its base plate is a solid slab of metal of such and such a size and orientation; that its operator is a human; that only one operator at a time can use it; etc.  
if someone accidentally drills a hole through the base plate  most human beings would realize that the machine can still be used for certain jobs but not for others - e.g.  it's ok if you want to mill a very large part  but not a very small one that might fall through the hole! people can fluidly shift to the next more detailed grain size  to reason out the impact of the hole in the base plate  even if they've never thought of it happening before; but the typical es would have had just one particular level built in to it  so it couldn't adapt to using the crippled milling machine. 
a remark that is appropriate both to es and to the logicians  section 1  is that there is no need - and probably not even any possibility -- of achieving a global consistent unification of a large set of es' kbs. large  broad systems need local consistency -- what we call coherence. e.g.  physics advanced for many decades with inconsistent particle and wave models for light. knowledge space in toto is still a set of self-supporting buttes. in a coherent system  inferring an inconsistency is only slightly more serious than the usual sort of  dead-end ; the system should still just back up a bit and continue on. 
1. problems and solutions 
problem 1: possible  in-principle  limitations. there are several extremes that one can point to where the knowledge principle and breadth hypothesis would be inapplicable or even harmful: perceptual and motor tasks; certain tasks which must be performed in small pieces of real time; tasks that involve things we don't yet know how to represent well  time  space  belief  mass nouns  counterfactuals ... ; tasks for which an adequate algorithm exists; tasks so poorly understood that no one can do it well yet; and tasks involving large amounts of common sense. 
our response ~ in principle and in cyc - is to describe perception  emotion  motion  etc.  down to some level of detail that enables the system to understand humans doing those things  and/or to be able to reason simply about them. as discussed under problem 1  below  we let a large body of examples dictate what sorts of knowledge  and to what depth  are required. 
a similar answer applies to all the items which we don't yet know very clearly now to represent. in building cyc  e.g.  a large amount of effort is being spent on capturing an adequate body of knowledge  including representations  for rime  space  and belief. we did not set out to do this  the 
1 	panels and invited talks 
effort is driven completely by need  empirically: looking at encyclopedia and newspaper articles  and developing machinery that can handle those cases encountered. 
tasks which can be done without knowledge  or which require some that no one yet possesses  should be shied away from. one does not use a hammer type with. 
the huge kb mandated by the breadth hypothesis is al's  mattress in the road . knowing that we can go around it one more time  ai researchers build a system in six months that will perform adequately on a narrow version of task x; they don't pause for a decade to pull the mattress away. this research opportunity is finally being pursued; but until cyc or a similar project succeeds  the knowledge based approach must shy away from tasks that involve a great deal of wide-ranging common sense or analogy. 
the remainder of the problems in this section are primarily pragmatic  engineering problems  dealing with the mechanics of constructing systems and making them more usable. as can be seen from our response to the in-principle limitations  we personally view problem 1 in that very same category! that is a view based on the eh  of course. 
problem 1: how exactly do we get the knowledge  knowledge must be extracted from people  from data bases  from the intelligent systems' kbs themselves  e.g.  thinking up new analogies   and from nature directly. each source of knowledge requires its own special extraction methods. 
in the case of the cyc project  the goal is to capture the full breadth of human knowledge. to drive that acquisition task  lenat and his team are going through an encyclopedia  sentence by sentence. they aren't just entering the facts stated  but - much more importantly - are encoding what the writer of that sentence assumed the reader already knew about the world. they are the facts and heuristics which one would need in order to understand the sentence  things which would be insulting or confusing for the writer to have actually stated explicitly  e.g.  if coke is consumed to turn ore into metal  then coke and ore must both be worth less than metal.  they also generalize each of these as much as possible  e.g.  the products of commercial processes are more valuable than their inputs.  another useful place they focus is the inter-sentential gap: in a historical article  what actions should the reader infer have happened between each sentence and the next one  yet another focus: what questions should anyone be able to answer having just read that article  these foci drive the extraction process. eventually  cyc itself should help add knowledge  e.g.  by proposing analogues  extending existing analogies  and noticing gaps in nearly-symmetric structures. 
this methodology will collect  e.g.  all the facts and heuristics about water that every article in the encyclopedia assumed its reader already knew; we expect this will be close to what everyone does know and needs to know about water. this is in contrast to  for instance  naive physics and other approaches that aim to somehow capture a deeper theory of water in all its various forms. 
problem 1: how do we adequately represent it  human experts choose or devise representations that enable the significant features of the problem to remain distinguished  for the relevant connections to be quickly found  etc. thus  one can reduce this to a special case of problem 1  and try to elicit appropriate representations from human experts. cyc takes a pragmatic approach: when 

something proves awkward to represent  add new kinds of slots to make it compactly representable. 
problem 1: how will it be used  the representation chosen will of course impact on what inference methods are easy or difficult to implement. our inclination is again to apply eh: when you find out that some kind of operation needs to be performed often  but it's very inefficient  then you need to adjust the representation  or the inference methods available  or both. as with problem 1  there is a temptation to early specialization: it is a local optimum  like swerving around a mattress in the road. pulling this mattress aside means assembling a large repertoire of reasoning methods  and heuristics for choosing  monitoring  and switching among them. earlier  we sketched an opportunistic  non-monolithic  control structure which utilizes items in the control-strategy region of the kb. 
to take a more specific version of this question: how do we expect to efficiently  index  -- find relevant partial matches  our answer is to finesse it for now. wait until our programs are finding many  far-flung analogies  e.g.  but only through large searches. then investigate what additional knowledge people bring to bear  to eliminate large parts of the search space in those cases. codify the knowledge so extracted  and add it to the system. this is a combined application of the difficult problems hypothesis and the eh. it is a claim that the true nature of the indexing problem will only become apparent in the context of a large problem running in an already very large kb. 
problem 1: how can someone interact  naturally  with kb systems  knowledge based systems built so far share with their knowledge-free predecessors an intolerant rigidity of stylistic expression  vocabulary  and concepts. they rarely accept synonyms and pronouns  never metaphors  and only acknowledge users willing to wear a rigid grammatical straitjacket. the coming few years should witness the emergence of systems which begin to overcome this problem. as is only fitting  they will overcome it with knowledge: knowledge of the user  of the system's domain  of discourse  of metaphor. they will employ pictures and sound as well as text  as means of input and output. many individual projects  oncocin  cyc  and expert system tools  art  kee  are already moving in this direction. 
problem 1: how can you combine several enterersvsystems' knowledge  one solution is to sequentialize the entry  but it's not a good solution. many emycin-based programs designated someone to be the knowledge base czar  with whom all the other experts would discuss the knowledge to be entered. eurisko  built on rll  tried explicitly enforced semantics. each slot would have a description of its intended use  constraints that could be checked statically or dynamically  e.g.  each rule's if-mayberelevant slot should take less cpu time to execute than its if-truly-relevant slot . when someone enters rules that violate that constraint  the system can complain to them  to get everyone back on track using the same semantics again. cyc extends this to implicitly enforced semantics: having such a large existing kb that copy&edit is the clear favorite way of entering new knowledge. when one copies&edits an existing frame  virtually all of its slots' semantics  and even most of their values!  carry right over. 
we have already described the important  gluing  role that coherence  local consistency  will play. at a more exotic level  one can imagine mental immune systems providing  in the background  constant cross-checking  healthy skepticism  advice  and criticism. 
problem 1: how can the system builder  and the system user  not get lost   getting lost  is probably the right metaphor to extend here  because what they need to do is to successfully navigate their way through knowledge space  to find and/or extend the relevant parts. many systems  including cyc  are experimenting with various exploration metaphors and orientation tools: helicoptering through semantic nets; exploring a museum with alician entry into display cases and posters  etc. for more elaborately scripted interface metaphors  see  vinge  or  lenat 1b . the latter suggests clip-on filters to shade or highlight certain aspects of what was seen; models of groups and each individual user; and simulated tour-guides with distinct personalities. 
problem 1: how big a fraction of the million pieces of  consensus reality  do you need to represent  we believe the answer is around 1%. why  when communicating with an intelligent entity  having chosen some concept x  we would expect the  listener  to be familiar with x; if it fails several times in a row ~ often! - then it is missing too much of consensus reality. a similar argument applies to analogizing  and to generalizing. now to have a 1% chance for the chosen analogue to be already known by the listener  he/she/it might have to know 1% of the concepts that are analogized to. but how uniformly are good analogues distributed in concept-space  lacking more data  we assume that they are uniformly distributed  which means the system should embody 1% of the full corpus of consensus reality. the distribution is quite possibly not uniform  which is why  the eh again  we need to build the kb and see. 
1. conclusion: beyond local maxima 
our position includes the statements that 
* one must include domain-specific knowledge to solve difficult problems effectively 
* one must also include both very general knowledge  to fall back on  and very wide-ranging knowledge  to analogize to   to cope with novel situations. 
* we already have plenty of theories about mechanisms of intelligence; we need to proceed empirically: go off and build large testbeds for performing  analogizing  ml  nl... 
* despite the progress in learning  language understanding  and other areas of ai  hand-crafting is still the fastest way to get the knowledge into the program in the 1's. 
* with a large kb of facts  heuristics  and methods  the fastest way would tip toward nl  and then toward ml. * the hand-crafting and language-based learning phases may each take about one decade  culminating in a system with human-level breadth and depth of knowledge. 
each of those statements is more strongly believed than the one following it. there is overwhelming evidence for the kp and eh. there is strong evidence in favor of the bh. there is a moderate basis for our three-stage programme. and there is suggestive evidence that it may be possible to carry out the programme this century. 
the knowledge principle is a mandate for humanity to concretize the knowledge used in solving hard problems in various fields. this might lead to faster training based on explicit knowledge rather than apprenticeships. it has already led to over a thousand profitable expert systems. 
the breadth hypothesis is a mandate to spend the resources necessary to construct one immense knowledge base. it should extend horizontally across an encyclopedic 
	lenat and felgenbaum 	1 
span of human thought and experience  and vertically extend upward from a moderate level of detail all the way up to encompass the most sweeping generalities. 
as a partial application of the breadth hypothesis  consider the task of building a knowledge based system covering most of engineering design. interestingly  this task was chosen independently by the japanese edr project and by bob kahn's national research institute. both groups see this task as a moderate-term  -1  goal. it is certainly much broader than any single expert system  yet much narrower than the universal knowledge base mandated by the bh. 
slightly narrower   lawyers  workstations  or  botanists' workstations   etc.  are similar sorts of compromises  partial applications of bh  worth working on. they would possess a crown of very general knowledge  plus their specific field's next level of generalities  useful representations  etc.  and some detailed knowledge including  e.g.  methods for extracting and using entries in that field's online databases. these have the nice side effect of enticing the experts to use them  and then modify them and expand them. 
we are betting our professional lives  the few decades of useful research we have left in us  on kp  bh  and eh. that's a scary thought  but one has to place one's bets somewhere  in science. it's especially scary because  a  the hypotheses are not obvious to most ai researchers  b  they are unpalatable even to us  their advocates! 
why are they not obvious  most ai research focuses on very small problems  attacking them with machinery  both hardware and search methods  that overpower them. the end result is a program that  succeeds  with very little knowledge  and so kp  bh  and eh are irrelevant. one is led to them only by tackling problems in difficult  real  areas  with the world able to surprise and falsify. 
why are our three hypotheses not particularly palatable  because they are unacsthetic! until forced to them  occam's razor encourages us to theorize more elegant solutions. 
section 1 listed several limitations and problems. we do not sec any of them as insurmountable. the biggest hurdle has already been put well behind us: the enormous local maximum of knowledge-free systems; on the far side of that hill we found a much larger payoff  namely expert systems. 
and yet we see expert systems technology  too  as just a local maximum. ai is finally beginning to move on beyond that threshold. this paper has presented what its authors glimpse on the far side of the expert systems hill: the promise of very large scale knowledge bases  vlsk   the promise of analogical reasoning and common sense knowledge. 
the impact of systems mandated by the kp and bh cannot be overestimated. public education  e.g.  is predicated on the unavailability of an intelligent  competent tutor for each individual for each hour of their life. ai will change that. our present entertainment industry is built largely on passive viewing. ai will turn  viewers  into  doers . what will happen to society as the cost of wisdom declines  and 
society routinely applies the best of what it knows  will a knowledge utility arise  like the electric utility  and how might it  and other ai infrastructures  effect what will be economically affordable for personal use  
man-machine synergy prediction: in that  second era  of knowledge systems  the  system  will be reconceptualized as a kind of colleagular relation-
ship between intelligent computer agents and intelligent people. each will perform the tasks that he/she/it does best  and the intelligence of the system will be an emergent of the collaboration. 
the interaction may be sufficiently seamless and natural that it will hardly matter to anyone which skills  which knowledge and which ideas resided where  in the head of the person or the knowledge structures of the computer.  it would be inaccurate to identify intelligence  then  as being  in the program . from such man-machine systems will emerge intelligence and competence surpassing the unaided human's. beyond that threshold  in turn  lie wonders which we  as unaided humans  literally cannot today imagine. 
acknowledgements 
many of the ideas in this paper emerged during a series of meetings with mark stefik  marvin minsky  alan kay  bob kahn  bob wilensky  ron brachman  and several others. the careful reader will also detect the welcome influence of many of our colleagues on our way of thinking: woody bledose  john brown  john mcdermott  allen newell  george polya  roger schank  and herbert simon. chuck dement  carl hewitt  david kirsch  and bruce porter provided very useful examples and critiques. 
