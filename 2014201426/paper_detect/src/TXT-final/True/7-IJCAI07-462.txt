
estimating the rate of web page updates helps in improving the web crawler's scheduling policy. but  most of the web sources are autonomous and updated independently. clients like web crawlers are not aware of when and how often the sources change. unlike other studies  the process of web page updates is modeled as non-homogeneous poisson process and focus on determining localized rate of updates. then various rate estimators are discussed  showing experimentally how precise they are. this paper explores two classes of problems. firstly the localized rate of updates is estimated by dividing the given sequence of independent and inconsistent update points into consistent windows. from various experimental comparisons  the proposed weibull estimator outperforms duane plot another proposed estimator  and other estimators proposed by cho et la. and norman matloff in 1% 1%  of the whole windows for synthetic real web  datasets. secondly  the future update points are predicted based on most recent window and it is found that weibull estimator has higher precision compared to other estimators.
1 introduction
most of the information available on the web are autonomous and are updated independently of the clients. estimating the rate at which web pages are updated  is important for web clients like web crawlers  web caching etc. to improve their performance. since the sources are updated on their own  such a client does not know exactly when and how often the sources change. web crawlers need to revisit the sources repeatedly to keep track of the changes and maintain the repository upto-date. further  if a source is revisited before it is changed  then it will be a wastage of resources such as cpu time  file system and network bandwidth for both client and server. such a visit is defined as a premature visit. for a client like a web crawler with a collection of thousands of millions of such sources  it is important to schedule its visits optimally such that number of unobserved changes between the subsequent visits and the number of premature visits are minimum. therefore  study of the changing behavior of web pages and estimating their changing rate can benefit in improving web crawler's scheduling policy  web page popularity measure  preprocessing the accessed schedules etc. this is the main motivation of this work. another question that is attempted to answer in this paper is the localized rate of changes of the sources. for example  at what rate was a web page updated during the month of january last year  how does the rate of updates change during the month of january for the last five years  this paper focuses mainly on estimating localized rate of updates rather than global update rates. since the rate of updates of the sources may vary with time  localized estimation provides more detail  useful and accurate information.
¡¡contrast to the previous studies  cho and garcia-molina  1; matloff  1   the process of web page updates is modeled as time-varying poisson process. it is because of the fact that sources are updated by its owner autonomously and independently whenever there are new developments and the rate of updates may vary with time. therefore  it is more appropriate to assume the update process as a time varying poisson process rather than time homogeneous poisson process. the advantage of modeling the problem as non-homogeneous poisson process is its wide range of flexibilities. it can be noted that homogeneous poisson process is a special case of non-homogeneous poisson process. a major difficulty in modeling the problem as non-homogeneous poisson process is that it has infinitely many parameters. in particular  it is parameterized by its arrival-rate ¦Ë t   which varies with time. to simplify the problem  the attention is restricted to certain assumptions of ¦Ë t  i.e.  constant  increasing or decreasing. such assumption help in reducing the number of parameters. this assumption may not be correct over a long period of observations  say  over years of page update records   but reasonable over appropriate subintervals  say  over few weeks . therefore  the whole observation space is divided into a sequence of windows of consistent behavior  see section 1 . the above assumption is reasonable as this paper is focussed more on the instanteneous page update rates.
1 accessing the web pages
one major problem in modeling the changing behavior of web page is that  the complete information of the sequence of updates is not available. web pages are accessed repeatedly through normal activities such as periodic crawling of web pages. it is only possible to determine  whether a page had changed  but not possible to determine the number of changes between two subsequent accesses  which results in loss of information. the accuracy of the estimation of the rate of changes depends on how small the number of unobserved updates is.
¡¡assuming that the web pages are accessed periodically with a small interval ¦Ó  let m i  be the number of page updates between the  i   1 th and ith accesses and
{t i 1  t i 1  t i 1  ... t i m i  } be the time sequence of corresponding updates. the changes can be observed in the following two ways:
-last date of change: some web servers provide the information about when the page was last modified. it is not possible to obtain the complete update information between accesses. in such case  we only observe t i m i   and {t i 1  t i 1  t i 1  ... t i m i  1 } are left unobserved. there is no way to obtain the unobserved information. assuming that m i  is small  we ignore the unobserved updates and count only one update during   i   1 ¦Ó i¦Ó  if changes occur and consider t i m i   as the renewal point.
-existence of change: some web servers do not provide such information i.e.  last date of change. in such case  it is only possible to determine whether the page has changed between accesses comparing with the old image of the page. we can not even determine t i m i  . it could have changed any number of times anywhere between  i   1 th access and ith access  i   1. if changes occur  it is counted as one.
1 formulating the process
in the studies  cho and garcia-molina  1; matloff  1   the two classes of observing updates are handled separately. unlike these studies  the observed information is preprocessed and a model is formulated to handle both the cases identically simplifying the problem. for the case of last date of change  the t k m k   is considered as the update point of the kth interval and the process of update time is formulated as {t i  | t i  = t k m k   i = n k¦Ó  i k   1}  where n t  is the number of updates during the period  1 t .
¡¡for the case of existence of change  the t k m k   is not available. assuming that sources are accessed periodically with an optimal interval  we can approximate t i   say the middle point or the end point of the period or a point determined from an appropriate distribution. here  the t i  is approximated as the middle point and the process of update time is formulated as {t i  | t i  =  k   1 ¦Ó + ¦Ó/1 i = n k¦Ó  i k   1}  where n t  is the number of observed updates during the period  1 t .
¡¡thus  the time sequence {t i  : i = 1 1 ...} represents the sequence of the update points for both the cases satisfying the properties t 1  = 1 and t i    t i + 1   where t i  is the time of the ith update. since the client does not know exactly when and how often the web page updates  the sequence {t i  : i = 1 1 ...} is an independent and identically distributed random process  which is often modeled as a poisson process in several studies  brewington and cybenko  1; cho and garcia-molina  1; matloff  1 . this study also assumes the model as a poisson process  but as a nonhomogeneous poisson process. one of the advantages of modeling the process as a non-homogeneouspoisson process includes the ability to determine the instantaneous update rate i.e.  ¦Ë t . in summary  this paper makes the following contributions:
¡¡-handles the last date of change and existence of change cases identically.
¡¡-models the system as a non-homogeneous poisson process.
¡¡-proposes two ¦Ë t  estimators namely weibull estimator and duane plot estimator and compares the estimates using various datasets with the estimators proposed in existing literatures.
¡¡the rest of the paper is organizedas follows. section 1 discusses non-homogeneous poisson distribution. in section 1  we discuss different estimators of rate of updates. in section 1  we divide the whole observationspace into windows of consistent behavior. experimental verifications are discussed in section 1. then we conclude the paper in section 1.
1 non-homogeneous poisson process
a non-homogeneous poisson process over  1 ¡Þ  is determined by a rate function ¦Ë t  ¡Ý 1  known as intensity density of the process  see chapter 1  trivedi  trivedi  1  . let n t  be the number of updates during the time period  1 t . then  the intensity function of the process  i.e. mean number of updates during the period  1 t  is defined as follows.
		for
now  the probability that the number of updates during the period  1 t  equals to k  is given by

for k = 1 1 ... the time homogeneous poisson process is a special case  where ¦Ë t  = ¦Ë for all time instances t ¡Ý 1. now  the probability of n t  becoming zero is equal to pr n t  = 1  = e ¦Ì t . thus we can write its cumulative distribution function  cdf  as f t  = 1   e ¦Ì t  and its probability density function  pdf  as
. using the expression of f t  and f t  above  the instantaneous ¦Ë t  can be expressed as
		 1 
1 estimating ¦Ë t 
as stated in section 1  the whole observation space is divided into a sequence of windows of consistent behavior. a window is a sequence of updatepoints satisfying the propertythat t i    t i 1   where t i  is the time of the ith update. since the sequence of update points are independent and identically distributed and satisfies the memoryless property  each window can be processed independently to determine the ¦Ë t .
definition 1 a window is said to be consistent  iff either one of the followings is true.
1. ¦Ë t i     ¦Ë t j    i   j j   1 i.e.  increased
1. ¦Ë t i     ¦Ë t j    i   j j   1 i.e.  decreased
1. ¦Ë t i   = ¦Ë t j    i   j j   1 i.e.  constant
1 intuitive measure: n t /t
if n t  is the number of updates during the period  1 t   the rate of updates at the time t is ¦Ë t  = n t /t and instantaneous mean time between updates  mtbu  at time t  mtbu t  = t/n t . this estimation is suitable when the window satisfies the third consistency property i.e.  the rate of update is constant. it is the biased estimation of ¦Ë t  in a homogeneous poisson process  cho and garcia-molina  1 .
1 estimation using weibull distribution
if the rate of update is either increasing or decreasing or constant  the weibull distribution is a good mechanism to model the process. the two parameter weibull pdf is given by
	 	 1 
where ¦Â is known as shape parameter and ¦Ç is known as scale parameter and ¦Â ¦Ç   1  see chapter 1  patrick  o'connor 
1    lieblein  1  . then its cdf is given by f t  = 1 e  t/¦Ç ¦Â. substituting equation 1 and f t  in equation 1  we get
		 1 
from the study  lieblein  1   we have  1  if ¦Â = 1  then ¦Ë t  = 1/¦Ç  a constant  the process reduces to the homogeneous poisson process    1  if ¦Â   1  then ¦Ë t  decreases as t increases and  1  if ¦Â   1  then ¦Ë t  increases as t increases. a window is always in one of the above three states. therefore  weibull's distribution is obviously a good choice of estimating ¦Ë t .
¡¡in  tsokos  1   the author found the relationship between mtbu t  and ¦Ë t  as follows.
for ¦Â = 1 for ¦Â   1 for ¦Â   1
thus  1/¦Ë t  defines the bounds of mtbu t . now our task is to estimate the parameters ¦Â and ¦Ç. we estimate these two parameters using maximum likelihood estimator as given below. the conditional pdf of the ith event given that the  i   1 th event occurred at ti 1 is given by f ti | ti 1  =
	¦Â	¦Â	¦Â
	.	now  the likelihood
function can be defined as

taking log on both sides we get

assuming c = 1/¦Ç¦Â  we can write

taking partial derivative w.r.t. c and equating to zero  we get
	.	 1 
again  taking partial derivative w.r.t. ¦Â and equating to zero  we get
		 1 
lemma 1 ¦Â is biased for small value of n and equal to
proof as reported in  calabria et al.  1   1n¦Â/¦Â  ¡« z  where z is a chi-square random variable with 1 n   1  degree of freedom. then we can write 
.	now  the expectation of ¦Â  can
be derived as 
is small  then
¦Â  is biased. 
now  lemma 1 suggests correcting the ml by a factor of
 n   1 /n and thus obtain
.
we can easily prove that e ¦Â   is unbiased i.e.  e ¦Â   =
e n n1¦Â   = n n1e ¦Â   = ¦Â.	thus we get .
substituting the value of ¦Â  and ¦Ç  in equation 1  we determine the value of ¦Ë  t  = n¦Ât  ¦Â  1/t¦Ân . in  calabria et al.  1   it is proved that ¦Ë  t  is less biased i.e.  e ¦Ë  t   = ¦Ë t  n   1 / n   1 . again  we correct ¦Ë  t  by a factor
 n 1 / n 1  to get an unbiased estimation and thus obtain
.
it can easily be proved that e ¦Ë¡¦ t   is unbiased i.e.  e ¦Ë¡¦  =
. we use this estimator in
our experiment. another unbiased estimate of ¦Ë t  is ¦Ë¡¥ t  =  n   1 ¦Ât  ¦Â  1/t¦Ân   which is obtained by correcting ¦Ë  t  by a factor of  n   1 /1  calabria et al.  1 . once we get windows of consistent behavior  see section 1   we can thus estimate ¦Ë t .
1 estimation using duane plots
in this subsection  we discuss another ¦Ë t  estimator using duane plot  duane  1 .
property 1 let  be the cumulative mean time between updates at ith update. if the sequence of mtbu ti  is either increasing or decreasing or constant as i increases and if we plot a graph between mtbu ti  along y-axis and ti along x-axis on log-log graph paper  the points tend to line up following a straight line. this plot is known as duane plot.
the slope of the straight line passing through points is known as duane plot slope. from the straight line  we can write logmtbu t  = log¦Á + ¦Ã logt  where ¦Ã is the duane plot slope and ¦Á is the intercept when t = 1. equating mtbu to its expected value  we get mtbu t  = ¦Át¦Ã. similarly  cumulative ¦Ë t  can be written as . now  the slope ¦Ã can be calculated as
	.	 1 
as we consider the ¦Ë t  estimation only within the consistent windows  the duane plot can be applied to estimate the ¦Ë t . this estimation works fine as long as both the points  mtbu ti  ti  and  mtbu tj  tj  are on the best straight line through the points. but  it is not the case in reality. therefore  the straight line which is closest to all the observed points is considered and the ¦Ã is the slope of the obtained straight line. the simple linear regression is applied to find the best straight line closest to all the observed points.
1 estimation using linear regression
if we consider a consistent window with either constant or increasing or decreasing ¦Ë t   linear regression is an effective mechanism to estimate ¦Ë t . under such conditions  the relationship between ¦Ë t  and t can be defined by a straight line. linear regression defines a straight line closest to the data points. we can define the equation of the closest straight line as  where a and b are regression coefficients and defined as. we apply simple linear regression to get the best straight line closest to all the observed points in the duane plot. if we consider x = logti and y = logmtbuc ti   then b is equal to the slope of the duane plots ¦Ã and a is equal to the intercept ¦Á.
1 measuring error
an error at a point i is defined as the difference between the observed value and predicted value at that point i i.e. 
. the error in measuring
¦Ë t  is minimum if error in measuring mtbu t  is minimum and vice versa. again  overall error in mtbu t  is minimum when
 is minimum. thus   can be used as a measure to compare different estimations of ¦Ë t .
the ¦Ë t  with the minimum is considered as the best estimate. but  in the case of linear regression mechanism
. to avoid this problem  we can use mean of square erroras a measure to compare different estimators.
1 generating consistent window
in reality  most of the web page update period sequence is inconsistent. it is found to be a sequence of constant  increasing and decreasing over subsequent period of times in any order. in this section  a procedure to generate the consistent windows  as define in definition 1  out of a sequence of update points is discussed. first the localized ¦Ë t  is
datasetminmaxmeanstd. de.#pagesreal11.1synthetic11.1table 1: characteristics of the datasets.  min:minimum number of updates in a page  max: maximum number of updates in a page  mean: mean number of updates and std. de.: standard deviation 
defined. if w be the set of update points  then the average number of update points within the window w can be defined as  where ¦Ë x  is locally integrable. now the localized ¦Ë x  can be defined as
  where n w  denotes the
number of updates within w and x is an update point. thus  it is necessary for the window w to be consistent so as to be able to predict pr n w  = 1 .
¡¡now  the total observation space is divided into windows wi of consistent mtbu i.e. either constant or increasing or decreasing in nature. the length of a window | wi k  | is defined by  tj+k   tj  j   1  where wi k  starts just after jth update and k is the number of update points within the window. the accuracy of the estimation depends on the nature of the update points within the window. so  how do we decide the window size  the important factor in deciding the window size i.e.  k is the number of consistent points within the window  not | wi k  |.
consistent window: the initial window consists of the last  first  three update points and extended backward  forward . a window is always in one of the three states  either constant or increasing or decreasing state. the window extends if the next point at tj is consistent with the state of the current window. we often do not find consistent windows of large size. to increase the size of the windows  we allow few inconsistent update points within the window with a limit on the amount of discrepancies. even if an update point tj is not exactly consistent with the current state  we consider the tj as a consistent point if the amount discrepancy is below some threshold value  otherwise we create the next window.
1 experimental evaluation
the comparisons of different estimators are based on both synthetic datasets  collected from the ucla webarchive project data available at http://webarchive.cs.ucla.edu/  and real web datasets. synthetic datasets consist of 1 1 pages' records and each record contains 1 access units. the real web datasets were collected locally. the 1 number of frequently access web pages were identified from our local proxy server log file. these pages were crawled every day for two months  1th april  1 to 1th june  1  to keep track of the changes.
¡¡table1 shows the characteristics of the two datasets. the pages having less than four update points during the observation period are ignored. the proposed estimators are compared with the estimators proposed in  cho and garciamolina  1  and  matloff  1 . the ¦Ë proposed by cho
datasetestimatortotalconstincrdecrsynthetic¦Ëcho ¦Ënorman1
11
11
11
1weibull1111duane1111real¦Ëcho ¦Ënorman1
11 11
11
1weibull1111duane1111table 1: average mse for different estimators over different window types. windows were generated with a threshold value of 1.
et al. can be stated as  where n is the number of intervals of ¦Ó and x is the number of intervals with at least one update point. again  the ¦Ë proposed by matloff n. can be stated as  where n is the number of intervals of ¦Ó and mn is the number of intervals with at least one unobserved update point. to get the best result out of the available information  ¦Ó is set to 1.
¡¡the experimental evaluations consists of two parts. in the first part  the localized ¦Ë t  is estimated using the estimators discussed in section 1 from the sequence of data points in the observation space. the sequence of consistent windows are generated using the procedure discussed in section 1. the consistent windows with a minimum size of four update points are considered. if the size of the window is less than four  then one half is merged with the left window and another half with the right window in the sequence introducing some discrepancies. for each window  the ¦Ë t  is estimated at every update point t using different estimators. for simplicity  mtbu t  is approximated as mtbu t  ¡« 1/¦Ë t  and then regenerate the whole observation space again using the estimated mtbu t . the difference between actual update point and estimated update point is considered as an error. the errors in every estimated update points are recorded and the average mse for each window is determined to compare the accuracy of different estimators.
restchonorweiduasyncho nor1% 1%1%1%1% 1%1%
1%wei1%1%1%1%dua1%1%1%1%realcho nor1% 1%1%1%1% 1%1%
1%wei1%1%1%1%dua1%1%1%1%table 1: estimator in the row outperforms estimator in the column. windows were generated with a threshold value of 1.  syn :synthetic  wei:weibull  dua: duane  cho:¦Ëcho  nor:¦Ënorman 
¡¡table 1 compares the average mse of different estimators in terms of different window classes. the proposed weibull

 a  synthetic data

 b  real data
figure 1: window size versus mse between different estimators. windows were generated with a threshold value of
1.
estimator outperforms other estimators for all the cases. for the case of constant state windows  all these four estimators performs equally well with very small average mse. but  weibull estimator outperforms the rest with significantly smaller average mse for both increasing and decreasing state windows. in table 1  the percentage of the number of windows out of the whole windows  in which row estimator outperforms the column estimator  is shown. it shows that weibull estimator outperforms the rest in 1% of the whole windows for synthetic datasets and in 1% of the whole windows for real web datasets. whereas the previous estimators ¦Ëcho and ¦Ënorman outperforms the rest almost in 1% for both synthetic and real datasets. again we compare different estimators in terms of the average mse for different window sizes as shown in figure 1. it clearly shows that error decreases as we increase the size of window. for the larger size windows  all estimators perform equally well  still weibull outperforms by smaller extents   but for the smaller size windows  weibull outperforms the other estimators by almost double.
¡¡the main motivation behind investigating localized ¦Ë t   rather than global ¦Ë is to use the piecewise information to improve the prediction of future update points. for example  the trend in ¦Ë t  during the month of january for last five years can be used  or combined with recent trend  to predict future ¦Ë t  of next january. due to not having enough datasets to make use of such information  such study is left as the future work.
¡¡however in the second part of the experiment  the effectiveness of different estimators is investigated by predicting the future update points from the past information running a simulation program. simulation starts with first consistent window in the observation space and based on this window  the next update point is predicted. then the window is extended to include the newly predicted point and again predict the next update point and so on. as pointed out in section 1  an efficient estimator should be able to detect good amount of update points with minimum amount of resource consumption. each premature visit is a wastage of resources such as cpu time  file system  network bandwidth etc.  see section 1 . to compare different estimators   a measure of precision is defined and stated as follows.
precision = 
#total v isits
the precision of an estimator represents the probability of detecting an update per visit. it is reasonable to conclude that estimator with the highest precision is the best estimator  with the highest probability of detecting update points. there are 1 number of update points in real dataset and 1 in synthetic dataset. table 1 shows the comparisons among the four estimators in terms of the percentage of updates detected  premature visits  precision and number of visits. though the ¦Ëcho and ¦Ënorman estimators detect the highest number of the update points  it is very expensive  almost 1%  1%  of the total visits over real  synthetic  dataset are premature visits. moreover  the number of visits scheduled by both ¦Ëcho and ¦Ënorman estimators are very large compared to the number of visits scheduled by weibull estimator. whereas weibull estimator has the highest precision i.e. the probability of detecting updates per visit  almost 1  1  for real  synthetic  datasets and minimum number of visits. if the number of visits is normalized  weibull estimator detects the highest number of update points  next followed by duane  then ¦Ënorman and ¦Ëcho. if the number of visits is normalized to the number of visits of weibull estimator  both the estimotors ¦Ëcho and ¦Ënorman detect only around 1%  1%  and duane estimator detects around 1%  1%  for synthetic  real  datasets. note that the best policy is the one that detects the highest number of update points with minimum resource consumption  in other word highest number of update points detected from the same number of visits. therefore  weibul estimator outperforms other estimators.
1 conclusions
this paper models the process of web page updates as nonhomogeneous poisson process. in a scenario with inconsistent rate of updates  piecewise estimation provides more detail and useful information compared to global estimation. in the study  the whole observation space is divided into independent consistent windows to estimate localized rate of
%obs%prem%prec%visitssyn¦Ëcho ¦Ënorman1%
1%1%
11
11% 1%weibull1%1%11%duane1%1%11%real¦Ëcho ¦Ënorman1%
1%1% 1%1
11%
1%weibull1%1%11%duane1%1%11%table 1: comparison of % of observed updates  % of premature visits  precision  % of visits over weibull estimator.  syn: synthetic  obs: observed update points  prem: premature visits  prec: precision . windows were generated with a threshold value of 1
updates. two update rate estimation mechanisms namely weibull and duane plot are proposed. tests on both synthetic and real datasets confirm that weibull's estimator outperforms duane plot and both the existing estimators ¦Ëcho and ¦Ënorman while estimating localized rate of updates. while investigating the effectiveness of using these estimators to predict future update points  weibull estimator has highest probability of detecting update points.
