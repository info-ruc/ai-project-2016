 
¡¡¡¡¡¡the question of whether concepts expressible as disjunctions of conjunctions can be learned from examples in polynomial time is investigated. positive results are shown for significant subclasses that allow not only propositional predicates but also some relations. the algorithms are extended so as to be provably tolerant to a certain quantifiable error rate in the examples data. it is further shown that under certain restrictions on these subclasses the learning algorithms are well suited to implementation on neural networks of threshold elements. the possible importance of disjunctions of conjunctions as a knowledge representation stems from the observations that on the one hand humans appear to like using i t   and  on the other  that there is circumstantial evidence that significantly larger classes may not be learnable in polynomial time. an np-completeness result corroborating the latter is also presented. 
	1. 	introduction 
¡¡¡¡¡¡the ability of humans to learn new concepts is remarkable and mysterious. l i t t l e progress has been made in the problem of simulating this process by computer. worse s t i l l there is l i t t l e agreement on what realistic specifications of the desired behavior of such a simulation might look like. the approach taken to this problem in a previous paper  was to appeal to computational complexity theory to provide some guidelines on this last question. in particular  various classes of propositional concepts were explored that could be learned from examples  or sometimes oracles  in a 
polynomial number of computational steps. presumably biological systems cannot learn classes that are computationally intractable. 
¡¡¡¡¡¡the overall model was hierarchical. a concept to be learned was expressed as a propositional expression in terms of concepts already known. if this expression was too complex it was computationally infeasible to learn i t . the maximal complexi t y of such expressions that could be learned feasibly was studied. 
¡¡¡¡¡¡that investigation pointed to one particular concept class that especially warranted further investigation--those that could be expressed as a 
 supported in part by national science foundation grant mcs-1. 
disjunction of conjunctions  called disjunctive normal form  dnf . the attraction of this class is that humans appear to like it for representing knowledge as is evidenced  for example  by the success of the production system paradigm  and of horn clause logics . our investigations suggest that significantly larger classes may not be learnable in polynomial time. hence this class may turn out to have a central role. 
¡¡¡¡¡¡this paper is organized as follows. in section 1 the formal framework w i l l be described f i r s t . it w i l l then be shown that significant subclasses of propositional dnf expressions can be learned in polynomial time just from examples. whether the whole class can be so learned remains an open problem. 
¡¡¡¡¡¡the section following that w i l l discuss how the above results can be extended to allow variables and relations in the manner of predicate calculus. 	the extension is to a very restricted 
part of the predicate calculus where there is only existential quantification and a limited number of variables. 
¡¡¡¡¡¡in section 1 the question as to whether the above algorithm can be made robust to errors in the data is discussed. 	it is shown that the learning algorithms can indeed be made robust to a small rate of even maliciously constructed errors. 
¡¡¡¡¡¡the suitability of our learning algorithms to implementations on distributed  brain-like  models of computation is the subject of section 1. 	in the propositional case  under certain natural restrictions  such implementations are immediate  but this does not appear to extend to the case allowing relations. 
¡¡¡¡¡¡finally in section 1 we observe that the limits of learnability are not far off. even if it is known that a single purely conjunctive expression explains f i f t y percent of the occurrences of a concept  discovering or approximating this conjunction 
may be np-hard. 
¡¡¡¡¡¡the word learning has been used in numerous senses in philosophy  psychology and ai. a useful distinction to make in this context is whether the available data from which the learning or induction is to be made is consistent with many varied hypothesis  or essentially with just one. in the former situation  which is often tacitly accepted in the philosophy of science  and also in machine learning 

l. valiant 1 


1 l. valiant 
from calls of examples+ or examples  for the given concept f. this is feasible if appropriate conditions hold for the distributions  such as the f o l lowing : 
definition. 	distributions 	{d+ d-} generable monomial sets  p.g.m.s.  if there is an algorithm t h a t can c a l l examples+ and examples-  runs in time polynomial in 	t 	and 	h  and generates a set of monomials t h a t w i t h p r o b a b i l i t y at l e a s t has the f o l l o w i n g p r o p e r t y : 
where summation is over every 	v 	such t h a t 
f o r any 	 n.b. 	i 	s elsewhere  
v and m are i d e n t i f i e d w i t h boolean functions in the obvious way. thus v - m means t h a t the t r u t h values defined in v make m t r u e     
¡¡¡¡¡¡the m o t i v a t i o n f o r t h i s l a s t d e f i n i t i o n i s t o j u s t i f y c e r t a i n s t r a t e g i e s f o r f i n d i n g an m1 
would otherwise appear to be u n j u s t i f i e d h e u r i s t i c s . 
example 1 . 1 . suppose t h a t f o r some small parameter  we c a l l e d examples+ a c e r t a i n number of times and put i n t o m1 every monomial t h a t appears as a submonomial of the p o s i t i v e examples w i t h frequency at l e a s t  for some d i s t r i b u t i o n s t h i s set would be exponentially l a r g e . for c e r t a i n o t h e r s   which one could d e f i n e   the set would be only polynomially l a r g e . 
example 1. a good p r a c t i c a l p o s s i b i l i t y might be to l e t m1 be the i n t e r s e c t i o n of some m k  of example 1 w i t h the set generated in example 1. for example if we choose k =1 and  then the monomial set is simply a l l conjuncts of sets of up to three predicates t h a t are ever s a t i s f i e d s i m u l t a neously in some p o s i t i v e example. 
       the q u a n t i t a t i v e aspects of our learning s t r a t egies are captured by the f u n c t i o n l   h   s   : suppose we have an urn containing a large number  possibly i n f i n i t e   of marbles of at most s k i n d s . we want to sample a number x of marbles  with replacement  so t h a t w i t h p r o b a b i l i t y at least 1 - h - 1 the sam-
p l e is such t h a t if a f u r t h e r marble is picked at random then the p r o b a b i l i t y t h a t t h i s new marble is not already represented in the previous sample is less than h - 1 . i n  i t i s shown t h a t there i s a f u n c t i o n l h s   1h s + loge h  t h a t is an upper bound on t h i s x f o r a l l possible r e l a t i v e f r e quencies of the s kinds of marbles. 
       our basic r e s u l t s about learning dnf expressions from examples alone w i l l be expressed w i t h respect to our previous d e f i n i t i o n s of a concept having d i s t r i b u t i o n s d+  d- where 
       w i t h summation over some 		it 	is e s s e n t i a l l y the dual of theorem a in   1   . 
theorem 1 . 1 . there is an algorithm e c a l l i n g examples+ and examples  such t h a t  i  whether f o r some m1 t h a t is p . g . d . or   i i   whether {d+ d~} has a p . g . m . s .   e w i l l generate a dnf formula g in time polynomial in h and t t h a t w i l l have p r o b a b i l i t y a t l e a s t  o f having to v e r i f y  a  we note t h a t in case  i  the monomials of f consist only of members of m1 and w i l l always be contained in the monomial set of g. 
hence 		in case 	  i i   
some monomials of f w i l l be missing from g because they were already missing from m1. but by the d e f i n i t i o n of p.g.m.s. the p r o b a b i l i t y summed over a l l v such t h a t  f o r any is bounded by h-1 w i t h p r o b a b i l i t y at least 
 hence  summed over v such t h a t g v  =1 is bounded by the same q u a n t i t y . 
	to show 	 b  	suppose to the contrary t h a t the 
sum is at least  suppose we consider each t h a t is not a monomial of f to be a marble and t h a t a c a l l of v -examples  is i n t e r preted as t a k i n g a sample of at least one marble    i . e . the submonomials of v cannot be monomials 
o f 	f   . 	then a	f	t	s	u	c	h c a l l s w i t h 
p r o b a b i l i t y at least the set of monomials in m1 t h a t are not in f and have not been chosen have t o t a l p r o b a b i l i t y of occurrence 

¡¡¡¡¡¡algorithms in the s t y l e of algorithm e are n a t u r a l i n several contexts i n i n d u c t i v e inference   1   . in the current context it is made noteworthy by i t s provably good behavior f o r a l l d i s t r i b u t i o n s . 
	1. 	dnf expressions with relations 
¡¡¡¡¡¡in ai applications there is often need for referring to several distinct objects and to expressing relationships among them. for sheer 
expressive power the full predicate calculus is very useful  of course  but it clearly includes much power  such as arbitrary alternations of quantifiers  that are l i t t l e used. it is an interesting question to delineate the minimal part of the predicate calculus that is s t i l l expressive enough for conventional ai applications. only a few attempts have been made in this direction . 
       here we are concerned with the question of how much the class of expressions that is learnable can be extended to allow relations without sacrificing polynomial time learnability. we shall now allow object variables x1 . . xr. instead of propositional predicates we will allow relation predicates 
p1 ... pt each of which has a number  its arity  that defines the number of arguments it has. a monomial is now an existentially quantified conjunction of relational predicates  such as: 

for notational brevity the existential quantification w i l l be suppressed. 
¡¡¡¡¡¡this notation is sufficient to express a predicate on a scene that depends on the existence of certain subobjects in the scene that obey certain relations among themselves. 
       for a fixed r and a fixed set of predicates p1 ... pt we can define the class mq of all monomials that can be formed from then in the obvious way. we regard two monomials are identical i f f they can be obtained from each other by permuting the variable names. 
definition. a set  is polynomial generable deterministically  p.g.d.  i f f there is a deterministic algorithm that in time polynomial in t and r terminates and generates all the members of m1. 
example 1. for any fixed numbers r and k the set of monomials consisting at most k predicates  negated or not  is p.g.d. in fact there are no more than  1r+1t k of them1 
       in treating the probabilistic generation of monomials from examples a problem would arise if we had to relate the variables {xi} to the primitive 
input vectors 	{v}. 	in the case under consideration  
however  this whole problem can be completely finessed. let d+  d- be probability distributions on the positive and negative examples of the primitive input vectors {v}. we extend d+  d- to the domain of monomials mo as follows: d+ m  is the probability that the conjunctive relation m holds for a random output v of examples+. similarly for d-. 
definition. distributions {d+ d-} have polynomially generably monomial sets  p.g.m.s.  if there is an algorithm calling examples+ and examples  that runs in time polynomial in h  t and r and generates a set of monomials that with probability at least- has the following property: 
l valiant 	1 
where summation is over every 	v 	such that 
for any m€m-   i.e.  as in the propositional case the monomials of f missing from m1 account for a small fraction of the probability space {v}. now  however   has the complex meaning that the monomial expression m is true of primitive input v .   . 
examples 1 and 1. 	the analogues of examples 
1 and 1 can be defined in the obvious way. 
theorem 1. the statement and proof are identical to those of theorem 1 except that the monomial set m1. has the more general interpretation allowing relations. 	o 
       in summary  the learning algorithm we are suggesting  if we use example 1 with  as the monomial set  is the following: list all monomials with up to r object variables and k predicates  that hold in at least one observed positive example. eliminate from this list all monomials that occur in some observed negative example. take the disjunction of the remaining monomials as the desired expression. 
	1. 	robustness 
¡¡¡¡¡¡how much tolerance to erroneous data should we require of a learning system  on the one hand experience suggests that learning is difficult enough from error-free data and becomes well nigh impossible if the data is seriously flawed. this suggests the possibility that the learning phenomenon is only feasible with very low error rates. on the other hand some robustness is clearly essent i a l and it is reasonable to investigate the maximum error rate that can be compensated for. 
       in this section we will modify algorithm e and show that the modified form algorithm e*  is resistant to a quantifiable  if low  error rate  whether in the propositional or extended relational case of section 1. 
definition. if a is an oracle then we say that oracle b is  oracle a with error rate if at each call of b  i  with probability it calls oracle a correctly  and  ii  with probability  it produces an arbitrary answer possibly chosen maliciously by an adversary. 
¡¡¡¡the modified algorithm that has access only to an oracle  examples  with error rate  will now perform cycles and w i l l depend on a 
parameter and t are related below. 1 is an integer array indexed by the members of m1 and initialized to zero. algorithm e* is the following: 


1 	l. valiant 

end 
the following analogue to theorems 1 and 1 can be proved. we shall restrict ourselves to the deterministic case. the other case  when m1 has 
p.g.m.s.  can be treated similarly by detecting any maliciously generated monomials having too many submonomials by their low frequency of occurrence. thus the result refers to an arbitrary function f that can be expressed as the sum of monomials from m1. 
theorem 1 . 1 . 	there is an algorithm 	e* 	calling 
examples  with error rate  that w i l l generate a dnf formula g in time polynomial in h and t such that with probability at least has the following properties 
proof. we w i l l show that the described algorithm e* suffices if 
¡¡¡¡ a  a monomial m of f can be missing from g only because it has been removed from m1 by having occurred with frequency greater than e in the negative examples. this can only happen if the oracle behaved erroneously at least et times in t trials where the probability of each such occurrence is at most  by the binomial distribution  e.g.   the probability of this happening is less than under the assumed bounds on t and hence the probability of this happening to any m in f is at most h-1. 
¡¡¡¡¡¡ b  a monomial m such that 	for 
some v such that m = v can be included in g if there have been fewer than ¡êt negative examples containing m. now if some m €m1 is implied with frequency at least by d- then the probability of having found at most negative examples in t t r i a l s each with probability of success at least 
¡¡¡¡¡¡¡¡  i . e .   allowing for the 1 error rate  can be bounded above by 
hence the probabil-
i t y that at least one such monomial with frequency 
at least 1e has been wrongly included in g is at most  under the assumed bounds. 
hence if only those monomials remain that have frequency less than then their total contribution to is at most 	d 
	1. 	naive neural modelling 
¡¡¡¡¡¡the methodology behind this study is that of determining the largest classes of  concepts   that can be learned in a feasible total number of computation steps. since learning is exhibited by biological systems it is an interesting afterthought 
to determine whether the algorithms discovered hap-
pen to be well suited to implementation on a  brainlike  model of computation  such as considered by feldman  or ackley et al.   for example. 
¡¡¡¡¡¡in this section we observe that a partial implementation of our algorithm on networks of linear threshold devices can be found that is noteworthy for its simplicity. in the propositional case it is able to learn dnf expressions if the distributions are such that the monomial sets are generable in a certain sense on these networks. while this class is s t i l l plausible and accommodates robustness exactly as in section 1  it does require more restricted distributions than before and does not allow for the relations of section 1. 
¡¡¡¡¡¡the essential idea is that if the input nodes of the network are exposed to the vectors constituting the examples  then the monomials in the monomial set m1 w i l l be learned in unsupervised fashion at various distinct nodes in the network. the final disjunction over these is learned in supervised mode at a specified node. to accomplish the unsupervised stage symmetry in the network is broken by assuming randomness in the network  either of the connections or of the weights . 
¡¡¡¡we consider the following distributed model of computation. an instance of a net is described by a triple where v is a set of n nodes {1 ... n   is a set of directed edges and a:e --   1  labels each edge   i   j   with a numerical weight a i j . each node i has a state s. that at any time has value 1 or 1. 
¡¡¡¡¡¡a set of nodes  say { l   . . .   t } is regarded as constituting the input. when an example is presented to the net  for learning or recognition  the states s 1   . . .   s t are set to the boolean values corresponding to the truth values of the propositional predicates p   . . .   p   
     a node j  fires    i . e .   sj =1  in a time interval if in the previous interval 

a node continues to fire as long as the above condition holds. 
¡¡¡¡¡¡the learning mechanism allowed is the following: when a new example is presented we wait for the states to stabilize and assume that they do. a 
weight aij can be changed only if node i or node j is f i r i n g . its new value depends only on the current values of si sj and on the values of aij and aji before the presentation of the current example. the new value of aij becomes operative only when a further example has been presented. 
¡¡¡¡¡¡we consider two senses in which a net learns a function f p1 ... pt  as a consequence of a training session of examples. f has been learned in unsupervised mode if the weights have been so modi-
	l. valiant 	1 
of the training session a distinguished node  say 	j fied during the training session that some node j now fires whenever  i.e. almost whenever  f   s 1 . . .   s t   - 1 . f has been learned in supervised mode if the same has been achieved but in the course was controlled by an external omnipotent agent and made to f i r e if and only if f  s1 ...  st  = 1 was true of the training example  thus overriding possibly the threshold condition above . 
¡¡¡¡¡¡for implementing counters on the weights we use an arbitrary continuous strictly monotone function 

¡¡¡¡¡¡first  we notice that if f is a simple monotone disjunction  e.g.   p1 vp1 vp1  or a simple 
monotone conjunction then it can be learned in supervised mode at any node j to which a l l the relevant inputs are connected. the learning rule for disjunctions involves changing those a.. where 
in both cases of theorem 1 . 1 . if each aij is initialized to 1  say  then each  w i l l tend to 1 or 1 as desired according to wherner its frequency in the corresponding examples is less than or greater than 

¡¡¡¡¡¡to recognize disjunctions of conjunctions we w i l l show how certain monomial sets m1 can be generated in unsupervised mode from examples. a disjunction over any subset of m1 can then be learned by the above method at any node that is connected to a l l the nodes computing m1 . 
¡¡¡¡¡¡the basic idea is illustrated by the simple case of wanting to learn the monomial p1 . . . pk from a distribution d* of examples when the nodes l   1   . . .   k   . . .   d   are a l l connected to some node j  d   note that here d* w i l l be a distribution that typically includes both positive and negative examples of f and no-one informs node j as to which is the case.  to accomplish this we define a as above but scaled to the range  1  k-1 1   and choose it so that  is always suitably small compared with k-1. if each aik is i n i t i a l -

provided the d i s t r i b u t i o n d* has favorable propert i e s such as the f o l l o w i n g : 
for the remaining values of for the majority of inputs and hence alj w i l l tend to zero as desired. 
¡¡¡¡to make arbitrary monomials of length k   i . e .   m k    learnable in unsupervised mode  one approach is to make the network random so that for each set of k out of the n nodes  there w i l l be at least one  and at most a few  nodes to which a l l k are connected. connecting each node to a random set of about others achieves this. then any monomial can be learnt at some node  and  equally importantly  w i l l be learnt at only a few  thus allowing the other nodes to learn different monomials. in this way about n distinct monomials from m k  can be learnt. to make a l l this possible d* has to be so defined that at each node at most one monomial from m k  has overriding influence in the sense of the previous paragraph. 
¡¡¡¡to make the above concrete consider the case k = 1 and suppose that we want p1 to be learnable. then d* has to be such that if we pick a set of pi's to include p1/p1 but to be otherwise random  then with high likelihood the following holds: if v €d* is chosen randomly to have at least two of the chosen pi true  then with probability greater than a half these w i l l include p1 and p1* 
¡¡¡¡¡¡to summarize we have shown that a restricted version of algorithm e of section 1 can be implemented on a network of threshold elements. 	it w i l l build up recognizers for a set 	m* 	of monomials that occur in examples provided the members of 	m* are suitably separated by 	d* 	in the manner i n d i cated above. 	the network can then learn arbitrary disjunctions over 	m* 	in supervised mode. 
	1. 	simple rules of thumb may be hard to learn 
¡¡¡¡¡¡there is evidence that c e r t a i n s i g n i f i c a n t extensions to the concept classes here or in  render them computationally i n t r a c t a b l e . one source of evidence is cryptography as described in the l a t t e r paper. here we s h a l l describe an np-completeness r e s u l t that shows t h a t extensions in a c e r t a i n other d i r e c t i o n are probably computationally i n t r a c table also. 
       in the learnable classes considered a simple formula was assumed to account f o r a hundred percent of a l l p o s i t i v e examples of a concept. suppose t h a t the simple formula is now a s i n g l e monomial t h a t accounts f o r j u s t f i f t y percent  and the r e s t i s accounted f o r by a f u n c t i o n whose complexity is u n s p e c i f i e d . we s h a l l show t h a t in t h i s s i t u a t i o n i t i s np-hard t o discover e i t h e r t h i s monomial  o r 

1 	l. valiant 
one that approximates it in the sense of accounting for  of the distribution for arbitrarily large h. 
theorem 1 . 1   given a function f expressed as a sum of monomials and distributions d+ and d- of 
positive and negative examples  it is np-hard to determine whether a monomial m of the following kind exists  or to construct one if it does: 

 the descriptions of d and d do not need to be counted in the input size.  
proof  we reduce the balanced complete bipartite subgraph problem proved np-complete by garey and johnson    p. 1  to this problem. in the former problem we are given a bipartite graph  and we 
have to determine whether there are subsets 

¡¡¡¡¡¡then the distribution ensures that any m satisfying the theorem must have at least t/1 
predicates defined as true  and  also  must imply at least t/1 of the mi's. but this is exactly the condition that the graph g has a 
 t/1  x  t/1  	complete bipartite subgraph. 
¡¡¡¡¡¡the above shows that determining whether the desired m exists or not is np-hard. constructing it on the assumption that this good rule of thumb 
exists is therefore also np-hard since the decision procedure reduces to this. o 
also if no such m exists then the value of summed over a l l  is at most 
	and the constant 	1 	cannot be 
approached to within h-1 uniformly in a number of steps polynomial in h and t  unless np-completeness does not imply i n t r a c t a b i l i t y   . 
¡¡¡¡¡¡finally note that it is not known whether the size of the largest balanced complete bipartite subgraph can be approximated to arbitrarily good 
multiplicative constant factors in polynomial time. hence it is possible that the above reduction establishes a stronger result than claimed  namely that finding monomials accounting for  of the 
distribution for sufficiently small constant e is also d i f f i c u l t . 
