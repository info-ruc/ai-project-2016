 
as the field of computational vision matures  more efforts are devoted to vision systems that are active and need to interact with their environment in real time. a prerequisite for integrating vision and action is the development of a set of representations of the visual system's space-time  where space includes the system itself. thus we are faced with the problem of studying the nature of appropriate representations and also with the computational task of acquiring them in a robust manner and in real time. both of these problems are addressed in this paper from a computational point of view. in particular  we study representations needed by active visual systems in order to understand 
their self-motion and the structure of their environment. 
the representations are of less metric information content than the ones traditionally used  including depth  surface normals  curvature and 1-d metric values for the parameters of rigid motion  etc.; but they are rich enough to allow the system to perform a large number of actions. these representations  indexed in image coordinates  are the direction of translation and the direction of rotation for the case of motion and a monotonic function of the depth value in the case of shape description. their advantage comes from the fact that they can be computed from minimal and well-defined input  flow or disparity values along image gradients   as opposed to the traditional ones which require image correspondence or the utilization of assumptions about the environment. 
1 	introduction: active vision revisited 
if computer vision was once limited to the study of mappings of a given set of visual data into representations on a more abstract level  it now has become clear that image understanding should also include the process of selective acquisition of data in space and time. this has led to a series of influential studies published under the headings of active  animate  purposive  or 
1 	action and perception 
behavioral vision. however  with a formal theory integrating perception and action still lacking  most studies have treated active vision  aloimonos et a/.  1; bajcsy  1; ballard and brown  1  as an extension of the classical reconstruction theory  employing activities only as a means to regularize the classical ill-posed inverse problems  in order to recover a metric representation of space-time which is general-purpose and can be used for accomplishing any task. in other words  the concept  of selective acquisition and processing of data in space-time was understood only through the manipulation of the geometric and physical parameters of the sensory apparatus  focusing  fixation  self-motion  etc. . 
　an important point was missed: that an intelligent vision system exists in space-time  where space-time includes the system itself  and in order for the system to function properly  i.e.  to act appropriately in a variety of situations  it should be able to develop robust descriptions of space-time. that is  it should be able to develop representations that would be adequate for accomplishing a set of tasks. but  classical geometry modeling the 1/1-d sketch has been inescapably with us all the time. recovering the depth of surfaces in view or quantities that  are subject of differential geometry such as surface normals and curvatures seems to have been about the only goal of 1-d computer vision in the past 1 years. during the past few years  however  it has been repeatedly argued that this could be a misplaced goal  simply because it is too difficult to recover such complete descriptions of shape and space  faugeras  1; koenderink and van doom  1 . 
　in this paper we propose novel representations of an active observer's space-time and we show how they can be obtained in real time using pattern matching techniques in the spatiotemporal gradients of the image intensity function. the essence of these representations is that they are derived from well-defined input  no correspondence of features is required  and from global computations not affected by local errors of various sorts. in particular  we present new solutions for two basic problems in vision  the one of estimating an observer's motion and the one of recovering a shape description for an active binocular system whose exact extrinsic parameters are not known. the next section describes these representations. 
1 retinotopic motion and shape representations 
consider an observer moving in some environment. although the organism as a whole might move in a nonrigid manner head  arms  legs and wings undergo different motions-the eyes move rigidly  i.e.  as a sum of instantaneous translation and rotation. the points where the translational and rotational velocity vectors intersect the retina  called the foe  focus of expansion   and the aor  axis of rotation   are respectively the representation of egomotion. that is  estimation of a system's egomotion amounts to recognizing two locations on the retina. 
　similarly  consider a binocular observer fixating on a point in space. the traditional representation used for the observer's extrapersonal space is the depth of features in the scene in view  or its derivatives such as shape  curvature  etc. we argue that the shape representations to be recovered for an active vision system are different  from the metric ones traditionally used in the sense that they cannot provide the 1/1-d sketch or functions of it. we call such representations  qualitative structures ; they basically amount to recovering a function of the depth of the scene  where partial information about this function is available. more specifically  if z x  y  is the function providing the depth z at  points  x y  in the image plane  and if /   i - 1 ...  n is a set  of functions not necessarily exactly known  then fi z x  y    i = 1  . . .  n is a set of qualitative structures of the scene. for example  if / is a monotonic function in z with the property z x y    z x' y'   =  f z x y     f z x' y'    then / constitutes a qualitative representation that we call  ordinal structure   since knowledge about  it only allows to order the values of the scene's depth. functions with different properties amount to qualitative representations of a different nature. in particular  in this paper we will show that an active binocular observer not aware of its exact extrinsic parameters capable of fixating at environmental points can recover an ordinal structure of the scene  without relying on the computation of exact correspondence of features in the two images. the concept  of ordinal depth computations has its origin in the relief transformations suggested already by the turn of the century psychologist  helmholtz and recently taken up in the work of various perceptual studies  koenderink and van doorn  1; todd and reichel  1; carding et a/.  1 . however  in these studies either correspondence is assumed or special assumptions are made about the structure of the scene in view. 
1 motion and stereo fields and patterns of gradients 
most current motion estimation techniques require the computation of exact image motion  optic flow or correspondence of features . this however amounts to an ill-posed problem. on the basis of local information only the component of the optical flow perpendicular to edges  the so-called normal flow  is well-defined  although in many cases  it is possible to obtain additional flow information for areas  patches  in the image. similarly  the computational analysis of binocular shape perception has been based on the two-dimensional disparity field  which is a special case of a motion field. in a limited area of the image around the fixation point the disparity measurements are small and thus they can be treated in a differential manner  just as optical flow. as in the case of motion  on the basis of local information only the component of the disparity vector perpendicular to edges  the so-called normal disparity  is well defined. the basic thesis in this paper is the following:  a  for the case of the motion problem there exists a set of orientation fields on the retina which possess the property that measurements of motion available along these orientations have a global structure  i.e.  they form simple patterns whose location and form encode the 1-d motion parameters.  to obtain an intuition see figures le and d; 1a  b and c; and 1a  b and c.  
 b  for the case of the stereo problem  there exists a set of orientation fields with the property that measurements of normal disparity that happen to be along these orientations have an ordinal structure regarding depth  i.e.  their values are sufficient for ordering the depth of the corresponding scene points  figure 1a . measurements in one orientation field allow the derivation of partial ordinal structure. successive fixations allow the merging of these representations by filling in ordinal depth information from different fixations  thus building up a global ordinal depth map. 
1 the case of motion: preliminaries 
to begin with  let us review the geometry of visual motion. to obtain a better intuition we first use a spherical retina. if the motion is a translation t  the motion field is along the great circles containing the vector t  figure la   pointing away from the focus of expansion  foe  and towards the focus of contraction  foc . if the motion of the eye is a rotation of velocity w where the rotation axis cuts the sphere at points aor  axis of rotation point  and -aor  the motion field is along the circles resulting from the intersection of the sphere with planes perpendicular to the rotation axis  figure lb . for general rigid motion the motion field on the sphere is the addition of a translational field and a rotational field. however  as input to the motion interpretation process we do not consider the motion field but the sign of the projection of motion vectors on appropriately chosen orientations. 
1 	selection of flow orientations 
two classes of orientations are introduced which are defined with regard to an axis. consider an axis s passing from the center of a spherical eye and cutting the sphere at points n and s. the unit vectors tangential to the great circles containing .s define a direction for every point on the retina  figure lc . these orientations are called s-longitudinal. similarly  the s-latitudinal orientations are defined as the unit vectors tangential to the circles resulting from the intersection of the sphere with planes perpendicular to the axis s  figure id . at each point the s-longitudinal and latitudinal vectors are 
	fermuller and al1im1s 	1 


figure 1:  a  translational motion field   b  rotational motion field   c  a longitudinal vector field defined by axis s.  d  a latitudinal vector field defined by axis s. 
perpendicular to each other. some properties of these directions will be of use later: consider two axes s1  n1 s1  and s1  n1 . each axis defines at every point a longitudinal and a latitudinal direction. figure 1 explains the locus of points where the s1 longitudinal or latitudinal vectors are perpendicular to the .s1 longitudinal or latitudinal vectors. 

figure 1:  a  on the sphere  the great circles containing 
s1 and s1 are perpendicular to each other on two closed second order curves  whose form depends on the angle between s1 and s1. these curves are defined as the set of points r on the sphere for which  s1 x r  ＊  s1 x r  - 1 or  s1 r  s1 -r  - s1   s1-  b  the s1-longitudinal vectors are perpendicular to the s1-latitudinal vectors along the great circle defined by s1 and s1. 
　next  the structure of the projections of a rigid motion field on the s-longitudinal and latitudinal vectors is examined. more accurately  the sign of the projections of the motion field on the longitudinal and latitudinal vectors is investigated  since this is the information employed as input to the motion interpretation process. for this purpose it is necessary to agree upon a definition of the directions  s  ns  -longitudinal vectors are called positive  +   if they point away from n  negative  -  if 
1 	action and perception 
they point away from 1  and zero  1  otherwise. similarly  s-latitudinal vectors are referred to as positive   +   if their direction is counterclockwise with respect to s  negative   -  if their direction is clockwise  and zero  1  otherwise. 
1 	the geometry of image motion patterns 
since a rigid motion field is the addition of a translational and a rotational field  the cases of pure translation and pure rotation are first presented separately. 
　if the observer moves with a pure translation of velocity t  the motion field on the sphere is along the direction of the /-longitudinal vectors  figure la . pro-
jecting the translational motion field of figure la on the s-longitudinal vectors of figure lc  the resulting vectors will be either zero  positive or negative. the vectors will be zero on two curves as shown in figure 1a  symmetric around the center of the sphere  whose shape depends on the angle between the vectors t and s. the area inside the curves will contain negative vectors and the area outside the curves will contain positive vectors  figure 1a . 

figure 1:  a  translational image motion along an slongitudinal vector field: on two curves  like the ones in figure 1a  passing through the points where /  -t  s and - s intersect the sphere the value is zero. inside the curves the values are negative and outside they are positive   b  rotational image motion along an slongitudinal vector field: on the great circle defined by 
w and s the values are zero. in one hemisphere the values are positive and in the other they are negative   c  a general rigid image motion defines a pattern along every .s-longitudinal vector field: an area of negative values  an area of positive values and an area of values whose signs are unknown. 
　if the observer moves purely rotationally with velocity w;  the motion field on the sphere is along the direction of the w-latitudinal vectors  figure lb . projecting the rotational motion field of figure lb on the s  ns' longitudinal vectors of figure lc  the resulting vectors will be either zero  positive or negative. the projections will be zero on the great circle defined by s and w  positive in one hemisphere and negative in the other  figure 1b . 
　if the observer translates and rotates with velocities t and w the projection of the general motion field on any set of s-longitudinal vectors can be classified for parts of the image. if at a longitudinal vector the projection of both the translational and the rotational vectors is positive  then the projection of the image motion  the sum of the translational and rotational vectors  will also be positive. similarly  if the projections of both the translational and rotational vectors on a longitudinal vector at a point are negative  the projection of the motion vector at this point will also be negative. otherwise the sign will be unknown because it depends on the value of the translational and rotational vector components. 
　thus  the distribution of the sign of image motion along the s-longitudinal set of directions defines a pattern on the sphere. considering a general rigid motion field due to translation t and rotation w on an s  ns' -longitudinal set of directions  a pattern like the one shown in figure 1c is obtained  which consists of an area of strictly positive values  an area of strictly negative values  and an area in which the values can not be determined without more information. the pattern is characterized by one great circle containing w and s and by two quadratic curves containing the points foe  foc  n and s. 
　it is worth stressing that the pattern of figure 1c is independent of the scene in view and depends only on a subset of the 1-d motion parameters. in particular  the great circle is defined by one rotational parameter and the quadratic curve by two translational parameters. thus the pattern is of dimension three. also  the pattern is different for a different choice of the vector s. 
　similarly  considering the projection of a rigid motion field on the s latitudinal directions  defined by the vector s ns    another pattern is obtained which is dual to the one of figure 1c. this time the translational latitudinal flow is separated into positive and negative by a great circle and the rotational flow by two closed quadratic curves. 
1 egomotion estimation through pattern matching 
the geometric analysis described above allows us to formulate the problem of egomotion estimation as a pattern recognition problem. assume that the system has the capability of estimating the sign of the retinal motion along a set of directions defined by various s-longitudinal or latitudinal fields that happen to be perpendicular to the local edges.  in a number of cases the sign may be estimated in other directions as well . if the system can locate the patterns of figure 1c in each longitudinal vector field  and the dual pattern in the latitudinal field   then it has effectively recognized the directions t and w. if information  positive  negative or zero  is not available in many directions  there might be an uncertainty in the computations in the sense that more than one set of patterns may be fitted to the data and the foe and aor may be obtained only within bounds. 
　for the case of a planar retina the latitudinal and longitudinal fields take a different form which is easily computed by projecting them on a plane tangential to the sphere  figure 1a b . in this case areas with negative and positive normal motion measurements lie in areas separated by a conic section  circle  ellipse  hyperbola  or parabola  and a straight line. figure 1c pictures an example of a longitudinal pattern. figure 1 shows results from experiments on planar images from an outdoor scene. 
figure 1:  a  in the plane the s-longitudinal vectors become perpendicular to conic sections defined by a family of cones with an axis parallel to s.  b  the s-latitudinal vectors become perpendicular to straight lines passing through the intersection s1 of s with the plane   c  in the plane the longitudinal vectors form patterns defined by a conic section and a straight line  dark negative  light positive  white  don't know  . here  s1 is denoted as  fa/c fb/c . 
figure 1: a camera mounted on the unmanned ground 
vehicle  developed by martin marietta under a contract for the u.s. government  captured a sequence of images as the vehicle moved along rough terrain in the countryside  thus undergoing continuously changing rigid motion   a  shows one frame of the sequence with the normal flow field overlaid   b    d  and  f  show the positive  light color  and negative  dark color  vectors of the longitudinal patterns corresponding to the x-  y- and ;-axes.  c    e  and  g  show the corresponding fitted patterns   i  shows superimposed on the image the boundaries of the patterns whose intersections provide the foe and the aor.  j  because measurements are not everywhere available  strong spatial gradients appear sparse  a set of patterns can possibly be fitted resulting in two bounded areas as solutions for the foe and the aor. 
	fermuller and al1im1s 	1 

1 the case of fixating stereo: preliminaries 
consider an active binocular observer capable of fixating on an environmental point. the geometry of the system can be described as a constrained rigid motion  between the left and right eye. if we fix a coordinate on the left eye with the 1-axis aligned with its optical axis the y-axis perpendicular to the fixation plane  then the transformation relating the right eye to the left is a rotation around the y-axis and a translation in the xz plane  figure 1 . at the fixation point the disparity measurements are zero and in a neighborhood around it relatively small. thus  it is legitimate to approximate the disparity measurements through a continuous velocity field. this amounts to the small baseline approximation that has been used in the literature  carding et a/.  1 . 
ferent fixations. this way we merge values in different fixations and build a global ordinal shape representation. 
1 	ordinal depth from one fixation 
an active binocular stereo system capable of changing its geometric parameters in a controlled vay should be aware of the pose of its eyes with regard to some head frame centered coordinate system. thus it should know the angle the optical axis encloses with the baseline  which amounts to knowing the parameter x1. if for a particular system this knowledge is not available  utilizing the constraints described in section 1  the direction of the translation x1 can be derived from the patterns of the normal disparity field  utilizing only the sign of the disparity measurements. 
　we do not know  however  the amount of rotation /1 and we also don't have to know the distance between the 

two eyes. using equation  1  it is possible to obtain an ordinal depth representation for the scene whose image 
points lie on families of curves: dividing equation  1  by -nx we obtain 
1 	action and perception 

	fermuller and aloimonos 	1 

　our computational strategy utilized information about x1  i.e.  the angle formed between the x-axis of the left eye and the baseline . although the theory described in section 1 can be used to derive this information  the system could derive x1 from motor information without involving the imagery; however   1 can only be computed from image information. its derivation requires exact fixation  i.e.  the exact intersection of the two cameras' optical axes on a surface point   which is practically impossible. figure 1 describes experimental results with an active binocular head/eye system. 
　finally  it should be noted that instead of computing an ordinal representation in the depth z  we could derive an ordinal representation in the distance r from the nodal point  fermuller and aloimonos  1 . if we use spherical eyes and employ a spherical coordinate system  we obtain similarly as in section 1 families of curves defined as functions in   and c  along which ordinal distance information can be derived. such a representation provides an advantage in the phase of merging data from different fixations. since the distance r remains the same in the different coordinate systems of different fixations  which are only rotated to each other   any two distance measurements on the same family of curves in any fixation and not only those on lines can be used to obtain additional distance information in the original frame. 
1 conclusions 
in the past few years  it has become clear that vision  and perception in general  should not be studied in isolation but in conjunction with the physiology and the tasks that systems perform. in this paper we argued that the synthesis of vision and action must happen through spatiotemporal representations computed from well defined input. we showed how an active observer by appropriately selecting subsets of the input can estimate representations of motion and structure through pattern matching. 
1 acknowledgements 
the support of the national science foundation under grant iri-1 and the austrian  fonds zur forderung der wissenschaftlichen forschung  project no.s1 is gratefully acknowledged. 
