
this paper proposes to enhance similarity-based classification by virtual attributes from imperfect domain theories. we analyze how properties of the domain theory  such as partialness and vagueness  influence classification accuracy. experiments in a simple domain suggest that partial knowledge is more useful than vague knowledge. however  for data sets from the uci machine learning repository  we show that vague domain knowledge that in isolation performs at chance level can substantially increase classification accuracy when being incorporated into similarity-based classification.
1 introduction
one of the most prominent challenges in machine learning is to identify appropriate features for representing instances  since learning performance depends heavily on the features used. particularly  the performance of similarity-based classification degrades with the number of irrelevant features  griffiths and bridge  1 . it is also known from work on constructive induction  ci  that adding features can improve classification accuracy  aha  1 . while ci is a bottom-up approach  this paper proposes a top-down approach on identifying abstract features. the focus of ci was mainly on logical and rule-based processes  whereas this paper shows how additional features can extend similarity measures for similarity-based classification.
　the main contribution of this paper is to show that additional features can be derived from domain theories that are imperfect. this will alleviate the knowledge acquisition bottleneck  as it reduces the requisites of obtaining expert knowledge. although similarity-based classification is only used in domains where no perfect domain theories exist  usually there is imperfect domain knowledge and isolated chunks of knowledge  aamodt  1; bergmann et al.  1; cain et al.  1; porter et al.  1 . for example  in  aamodt  1  open and weak domain theories were integrated into a case-based reasoning system. similarly  matching knowledge was used to improve the performance of the well-known protos system  porter et al.  1 . furthermore  it was shown that the combination of cbr and a domain theory outperforms both cbr and the theory itself  cain et al.  1 . in contrast to those weak theories  strong domain theories were used to filter irrelevant features  bergmann et al.  1 .
　we present a new approach that exploits imperfect domain knowledge in similarity-based classification by inferring additional abstract features. furthermore  we analyze the impact of the knowledge's vagueness and partialness.
　the next section specifies the representation of cases  the similarity measure  and domain theories. section 1 gives an overview over how additional features can improve classification accuracy. section 1 reports experiments with two domains from the uci machine learning repository  blake and merz  1 . finally  the last section concludes and outlines future work.
1 representation of the cbr modules
1	cases and the similarity measure:
a case c is made up of a set of attributes {a1 a1 ... an}. while the original attributes can be either discrete or numeric  the additional virtual attributes in this paper are binary.
　following the well-known local-global principle  we compute the similarity between two cases as the weighted average aggregation of the attributes' similarities:
n
sim a b  = x ωi   si 
i=1
where si = sim ai bi  are the local similarity values  and the ωi are the corresponding weights.
1	domain knowledge:
in this paper  we examine only domain knowledge that can be represented as a domain theory of the following form: a domain theory is a set of inference rules that relate concepts to each other. these rules specify which concepts exist in the domain and describe how abstract concepts can be inferred from more primitive ones  bergmann et al.  1 . for example  consider the relation in the rule a1 ○ a1… a1   a1 which says that there are the binary attributes a1 a1 and the ordinal attributes a1 a1. also  the rule states that a1 is satisfied  if a1 is true and a1 is smaller than a1. we assume that the case representation language is compatible with the

figure 1: properties of domain theories. the theories describe parts of the target concept  of which there are positive  +  and negative  -  instances. left: partial knowledge  only parts of the concept boundaries are known. middle: vague knowledge  concept boundaries are believed to be somewhere within the shaded areas. right: inconsistent knowledge  different rules make differing predictions.
language of the domain theory  either by sharing primitives or by using a bridging language. the formal definition of the domain theory is skipped here  since it is equivalent to horn clauses  including logical connectors  equality  and comparison operators.
　according to  mooney and ourston  1   the concepts in a domain theory can be divided into three types: observables are attributes that are directly represented in the cases. classification goals are attributes that are to be inferred or approximated. all other attributes are called intermediates.
　intermediate attributes are the focus in this paper  because they are natural candidates for virtual attributes  that is  they can be added to the similarity measure in order to enhance the classification accuracy.
1	properties of domain theories:
domains in which cbr is applied usually lack a perfect domain theory. hence  the domain theories  or parts thereof  that we work with have at least one of the following properties  cf. figure 1 :
  partialness: this is the case if some parts of the domain are not modelled  for example a  if conditions are used but not defined  or b  the relation of intermediates or observables to the classification goal is not known  or c  the classification goal does not exist in the rulebase at all. note that these situations correspond to gaps at the  top  or  bottom  of the domain theory  mooney and ourston  1 .
  vagueness: values can only be given within a certain confidence interval. if a value is picked from such an interval  it is likely to be incorrect.
  inconsistency: there are two or more rules  or even alternative theories  that make different classifications and it is not known which one is correct. cbr is often used to overcome this problem  because the cases provide knowledge on which classification is correct for certain cases.
in this paper  we focus on partial and vague theories.
1 virtual attributes
virtual attributes  richter  1  are attributes that are not directly represented in the cases but can be inferred from the al-
	age	age	age

	income	income	income
figure 1: types of virtual attributes. left: a binary virtual attributes divides the instance space into instances satisfying or not satisfying it. middle: a conjunction of binary attributes. right: the most general type of virtual attributes is to add a dimension to the instance space.
ready existing attributes. they are useful if the monotonicityprinciple is violated. if sim a b    sim a c  is necessary to reflect class membership  then there must at least be one pair of local similarities  so that sim ai bi    sim ai ci . if such a pair does not exist  the similarity measure must make use of interdependencies between attributes. for example  the similarity may not depend on two attributes a1  a1 themselves  but on their difference a1 a1. virtual attributes can express such interdependencies  e.g.  deposit a  = income a  spending a   and can also encapsulate non-linear relations.
　we propose to use intermediate concepts of domain theories as virtual attributes. virtual attributes can easily be added to the set of attributes of each instance.
　every virtual attribute forms an additional dimensions of the instance space  see figure 1  right  . this is most intuitive for numerical attributes. an example is the concept expectedwealthtillretirement c  =  1   age c     income c  unfortunately  these dimensions can change assumptions about instance distributions and are most likely not orthogonal to the other dimensions  since they are inferrable from other attributes.
　in this paper we focus on binary virtual attributes. although formally they are additional dimensions  they can be visualized as separating lines within the original instance space  see figure 1  left  . they divide the instance space into two regions. for example  taxfree c  ○ income c    1 may divide some instance space into salaries that are or are not subject to paying taxes in germany. we will show that virtual attributes that describe target concept boundaries are especially useful
　intermediate attributes that are fully defined  i.e.  that do not have gaps at the bottom of the domain theory  can be computed from the values of observables and other intermediates. in order to use an intermediate as a virtual attribute  it is added to the local similarities of the similarity measure  that is  si = 1  iff both instances satisfy the intermediate concept or both do not satisfy it  and si = 1 otherwise. in the following  virtual attributes are assumed to be discrete.
　let us look at how binary virtual attributes influence classification. assume for sake of illustration that the instance space is formed by the attributes temp and press denoting the temperature and pressure of a manufacturing oven. let us assume furthermore that the  to be approximated  target

figure 1: distribution of errors for the target concept hardened c  ○ temp c    1 … temp c    1 … press c    1 … press c    1 without virtual attributes  top  and with the virtual attribute v  c  ○ press c   = 1
 bottom .
concept is hardened c  ○ temp c    1… temp c    1… press c    1… press c    1.
　the error distribution of an unweighted knn-classifier for the target concept is depicted in figure 1  top . not surprisingly  the misclassifications occur at the boundaries of the target concept.
　now let us analyze the effect of different amounts and different qualities of domain knowledge on the classification. in order to control the independent variables like partialness and correctness of the domain knowledge  we created a simple test domain. there were two continuous attributes x and y   uniformly distributed over the interval  1 . the target concept was t c  ○ x c    1… x c    1… y  c   
1… y  c    1. we used a square centered in the instance space as target concept  because it is one of the few concepts for which the optimal weight setting for knn-classification can be calculated analytically. the optimal weight setting for the target concept is to use equal weights  ling and wang  1 . thus  the accuracy of 1-nn with equal weights is the optimal accuracy that can be achieved without adding additional attributes. there were 1 randomly generated cases in the case-base and 1 test cases were used. each experiment was repeated 1 times with random cases in the case-base and random test cases.
1	partialness of the domain theory:
we operationalize the partialness of the domain knowledge as number of known target concept boundaries. the more

figure 1: percentage of correctly classified cases with different numbers of target concept boundaries described by virtual attributes.
boundaries are known  the less partial it is.
　adding virtual attributes that correctly specify a boundary of the target concept makes the misclassifications at those boundaries disappear  see figure 1  bottom  . thus  by adding virtual attributes that describe a boundary correctly  the classification accuracy is increased  see figure 1 .
　obviously  even partial knowledge  e.g. adding only one virtual attribute  can improve classification accuracy. however  in this experiment we assumed that the virtual attributes were correct. in the next experiment we analyzed the influence of the correctness of virtual attributes.
1	correctness:
vague knowledge can be informally described as knowing that an attribute should be more or less at a certain value. the higher the vagueness  the higher is the probability for high incorrectness. we operationalize correctness of a virtual attribute as its distance from the correct value. we created virtual attributes of the form v  c  ○ x c    c  where c was varied from 1 to 1 at steps of 1. remember that the correct x-value  which was used in the domain theory to generate the cases  was 1. the accuracy of classification when adding these virtual attributes is depicted in figure 1.
　the results are a bit disappointing. the accuracy drops rapidly if the virtual attribute is inaccurate. fortunately  the accuracy with inaccurate virtual attributes is not much lower than using no virtual intermediates  the similarity measure with no virtual attribute is equivalent to setting c = 1 or c = 1 . the second peak at x = 1 which is the other boundary on the x-attribute is due to the fact that similaritybased classification is direction-less: only the position of the concept boundary has to be known  the side on which positive and negative instances are located is encoded in the cases.
　these experiments with a simple domain suggest that partial knowledge is more useful than vague knowledge. adding partial knowledge is likely to increase the classification accuracy  whereas vague knowledge is only useful if there is good evidence that the knowledge is correct.
in the next section we will evaluate the influence of virtual

c
figure 1: accuracy of similarity measures using the virtual attribute v  c  ○ x c    c  where c forms the horizontal axis. c-axis is stretched at the positions of the concept boundaries.
attributes in several domains from the uci machine learning repository  blake and merz  1 .
1 experiments
1	the domains:
the domain of the previous section allowed us to vary the correctness and partialness of the domain theory. however  since the domain was handcrafted and simple  we ran additional experiments with two data sets from the uci machine learning repository. note that some data sets in the repository come along with perfect domain models  as the instances were created by those models. however  we used only data sets whose domain theories were imperfect.
  japanese credit screening  jcs : this domain comes with a domain theory that was created by interviewing domain experts. accordingly  the theory is imperfect and classifies only 1% of the cases correctly.
  promoter gene sequences  pgs : this domain theory reflects the knowledge of domain experts in the field of promoter genes. it is highly inaccurate and performs at chance level when used in isolation  towell et al.  1 . we included this domain to serve as a worst case scenario  since the domain knowledge is most inaccurate.
　it is known that not all intermediate concepts will increase classification accuracy when used as virtual attributes  steffens  1 . hence  mechanisms to select or weight virtual attributes are necessary. in this paper we investigate whether weighting virtual attributes is more appropriate than selecting them. in the experiments we apply several existing weighting approaches which will be described in section 1.


figure 1: the domain theory of the jcs domain  top  and of the pgs domain  bottom .
1	the virtual attributes:
the domain theories of jcs and pgs have been created by domain experts for real world applications. hence  they do not separate positive from negative instances in a perfect way. the accuracy of the jcs domain theory is 1%  the accuracy of the pgs domain theory is only 1%. the structure of both theories is depicted in figure 1.
　most of the concepts process several observables. for example  rejected age unstable work processes the observables age and number years1:
rejected age unstable work s  :-
age test s  n1   1   n1 
number years test s  n1  
n1   1.
　although the concepts are very imperfect  i.e.  they miscategorize training cases   our experiments described in section 1 show that these concepts can improve classification accuracy when used as virtual attributes.
1	weighting methods:
according to the classification of weighting methods as proposed in  wettschereck et al.  1   we selected four methods with performance bias  and six with preset bias  i.e.  statistical and information-theoretic methods .
  performance bias: weighting methods with a performance bias classify instances in a hill-climbing fashion. they update weights based on the outcome of the classification process. the performance bias performs well if there are many irrelevant features  wettschereck et al.  1 . since the intermediate concepts of the domain theories can be assumed to be relevant  we expected performance bias methods to perform badly.
1. each  salzberg  1  increases the weight of matching features and decreases the weight of mismatching features by a hand-coded value.
1. ib1  aha  1  is a parameter-free extension of each. it makes use of the concept distribution and is thus sensitive to skewed concept distributions. it assumes that the values of irrelevant features are uniformly distributed.
1. relief  kira and rendell  1  is a feature selection- rather than feature weighting-algorithm. it calculates weights based on the instance's most similar neighbors of each class and then filters attributes whose weights are below a hand-coded threshold. we used extensions for non-binary target classes and knn with k   1 as proposed in  kononenko  1 .
1. isac  bonzano et al.  1  increases weights of matching attributes and decreases weights of mismatching attributes by a value that is calculated from the ratio of the prior use of the instance. the more often the instance was retrieved for correct classifications  the higher the update value.
  preset bias: the bias of the following methods is based on probabilistic or information-theoretic concepts. they process each training instance exactly once.
1. ccf  creecy et al.  1  binarizes attributes and weights them according to the classes' probability given a feature.
1. pcf  creecy et al.  1  is an extension of ccf which takes the distribution of the feature's values over classes into account. it calculates different weights for different classes.
1. mi  daelemans and van den bosch  1  calculates the reduction of entropy in the class distribution by attributes and uses it as the attribute weight.
1. cd  nunez et al.  1  creates a correlation matrix of the discretized attributes and the classes. the weight of an attribute increases with the accuracy of the prediction from attribute value to class.
1. vd  nunez et al.  1  extends cd in that it considers both the best prediction for a class and the predictions of all attributes.
1. cvd  nunez et al.  1  combines cd and vd.
1	results:
for brevity  we will refer to the similarity measure which uses only observables as the non-extended measure. the similarity measure which uses virtual attributes will be called extended. we used the leave-one-out evaluation method.
table 1: classification accuracies of the non-extended and the extended similarity measures. the columns report the accuracies for the unweighted classification and for several weighting methods.
domain	unw.	each	relief	ib1	isac
jcs  w/o 11111jcs  w/ 11111pgs  w/o 11111pgs  w/ 11111ccf	pcf	mi	cd	vd	cvd

1	1	1	1	1	1

1	1	1	1	1	1

　for most of the weighting methods  the extended similarity measure performs better than the non-extended one. in table 1 we underline the accuracy of the extended similarity measure if it outperformed the non-extended similarity measure when using the same weighting method. in the pgs domain  seven of ten weighting methods perform better if the similarity measure is extended with virtual attributes. even more so  in the jcs domain the accuracies of eight of ten weighting methods were improved by using virtual attributes.
　in its optimal setting  with an accuracy of 1% our approach performs also better than the results from the literature reported for the pgs domain. the accuracy of kbann in  towell et al.  1  is 1%  which to our knowledge was the highest accuracy reported so far and also used the leave-one-out evaluation. we found no classification accuracy results for jcs in the literature1.
　obviously  these improvements are not restricted to a certain class of weighting methods. methods with performance bias  most notably isac   information-theoretic bias  i.e. mi   and with a statistical correlation bias  e.g. vd  benefit from processing virtual attributes.
　even in the pgs domain  the improvements are substantial. this is surprising  since the domain knowledge is the worst possible and classifies at chance level when used for rulebased classification. this is a promising result as it shows that adding intermediate concepts may increase accuracy even if the domain theory is very inaccurate. we hypothesize that this is due to the fact that even vague rules-of-thumb provide some structure in the instance space which will be exploited by the similarity measure.
1 conclusion and future work
the main contribution of this paper is to show that imperfect domain knowledge can benefit similarity-based classification. this facilitates knowledge elicitation from domain experts as it removes the requirements of completeness and accurateness. our experiments in a simple domain suggest that partial knowledge is more useful than vague knowledge. however  we showed in the domains from the machine learning repository that even highly inaccurate domain knowledge can be exploited to drastically improve classification accuracy. future work includes experiments in further domains and transforming intermediate attributes by feature generation  fawcett and utgoff  1 .
