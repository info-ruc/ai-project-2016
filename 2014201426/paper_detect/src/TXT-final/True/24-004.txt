 
recent research suggests the utility of performing induction over explanations. this process identifies commonalities across explanations that cannot be extracted solely by explanation-based techniques. this has important implications for the 'correctness' of learned knowledge  flann and dietterich  1  and  as we show  on the efficiency with which learned knowledge can be reused. specifically  we illustrate that inductive concept formation can abstract and organize explanatory knowledge for efficient reuse in a domain of algebra story problems. 
1 	introduction: principles of memory organization 
an increasingly well-accepted view in psychology and ai is that problem solving is a process of classification. performance improves by learning patterns that discriminate between problem-solving choices. production system models of learning and problem solving  e.g.   langley  1   make the 'problem solving as categorization' view explicit  as do models of case-based reasoning  kolodner  1 . another paradigm concerned with learning and problem solving is explanation-based learning  mitchell et at  1; minton  1 . each of these three strategies aims to improve the efficiency with which knowledge is accessed and reused in novel problem-solving situations. 
　of these three approaches to improving problemsolving performance  we will focus on explanation-based learning  though the general approach that we describe is relevant to other paradigms as well. in particular  we have looked at explanation-based learning as a model of learning to solve algebra story problems imayer  1 . 
consider the problem  
 a train leaves a station and travels east at 1 km/h. three hours later a second train leaves and travels east at 1 km/h. how long will it take to overtake the first train   
   *this research was supported by nasa ames grant ncc 1. we thank the reviewers for useful comments. 
1 	learning and knowledge acquisition 

figure 1: a generalized problem solution trace  explanation . 
a learner with a set of primitive rules that constitute domain-specific knowledge  e.g.  distance = rate 
* time  and general algebraic knowledge  e.g.  a*b = c forms a solution to this overtake prob-
lem that is abbreviated in figure 1. the time until overtake  time = distance / rate  may be obtained from the distance that must be made up by the faster train  where d1 is the distance traveled by the first train before the second train starts   and the relative rate of travel of the second train  r = r1 - rl . explanation-based learning will generalize the arguments  i.e.  by turning them to variables in a controlled way  of this and-tree solution trace so that it can be reused on future problems. 
　unfortunately  even after variablization  the applicability of this solution trace will be highly limited. con-
sider a second opposite-direction problem  
 two trains leave the same station at the same time. they travel in opposite directions. one train travels 1 km/h and the other 1 km/h. in how many hours will they be 1 km apart   
which has a solution structure almost identical in form to the solution of figure 1; it differs only in the structure of the boxed subtree. nonetheless  the earlier solution cannot be used to solve the new problem unless the basic ebg process is altered. flann and dietterich  and others  hirsh  1; pazzani  1  suggest the utility 

of performing induction over explanations   i o e   which can generalize an explanation beyond simple variablizar tion. i o e does this by superimposing a set of and-tree explanations  and pruning subtree structures that are not shared by all trees. for example  performing i o e over the explanation trees for an opposite-direction and overtake problems would yield a tree structure with the boxed rightmost subtree of figure 1 severed. 
   using i o e   the common substructure of multiple explanations can be reused in a wider variety of situations. the substructure does not provide the complete explanation  but it ideally provides a sizable chunk that can be completed using the primitive rules of the original domain theory. thus  induction over explanations abstracts redundant substructures out  thus promising to improve the efficiency with which applicable learned knowledge can be found and reused.however  unconstrained induction can remove all the benefits of explanation-based learning. consider that there may be radically different explanations of why oat-bran cereal and baked fish are h e a l t h - f o o d . abstracting out the common substructure might yield an 'explanatory' substructure  h e a l t h - f o o d   x     which is a trivial statement of the target concept. thus  on one hand maximallyoperational explanations provide a complete explanation for new situations  but it is difficult to find applicable explanations since redundant structures must be searched multiple times. conversely  maximally-general structures  e.g.  c u p   x     h e a l t h - f o o d   x     make it easy to find applicable past 'experience'  but nothing useful is gleaned in having done so. 
　the tradeoffs of an explanation-based memory are closely related to tradeoffs traditionally found in inductive concept formation  fisher  1; lebowitz  1; kolodner  1 . ideal object concepts are those where 
.many features are predictable  i.e.  favoring specific concepts   but many features are also predictive  i.e.  favoring general concepts . our concern with 'prediction' accuracy in explanation-based learning may seem at odds with the traditional view that efficiency is the critical performance dimension in explanation-based learning. however  choices in a domain theory search constitute predictions; informed and accurate decisions at choice points result in an efficient search  while erroneous decisions are the cause of backtracking and inefficiency. the hypothetical curve of figure 1 illustrates performance trends that might be expected. maximizing predictiveness  generality  will underfit the data  necessitating uninformed prediction  e.g.  uninformed search through a domain theory . conversely  maximizing predictability  operationality  will overfit the data and introduce redundancies into the search for applicable past experience. an ideal abstraction should be relatively unique and easily identified; it can then predict a sizable portion of the complete explanation structure; the domain theory is only required to complete the proof  e.g.  of relative rate . 
   this paper shows that inductive methods of concept formation  gennari et al  1  can abstract redundancy out of explanations and organize shared substructure s  to improve the efficiency of finding explanations 

figure 1: prediction error  and search efficiency . 
for reuse. section 1 describes our system  e x o r  explanation organizer   which clusters and classifies explanations based on shared structure. in section 1 we report experimental results which illustrate that ex or effectively improves problem-solving efficiency in a domain of algebra story problems. sections 1 and 1 analyzes the strengths and weaknesses of the approach  and details its relation to ongoing research in explanation-based learning. 
1 	concept formation over explanations 
our system  e x o r   performs concept formation over exlanations. we use the domain of algebra story problems mayer  1  to describe and evaluate our system. in particular  e x o r embeds i o e within a control structure for building abstraction hierarchies that was inspired by lebowitz's  u n i m e m and fisher's  c o b w e b . figure 1 gives an example of the type of abstraction hierarchy formed by e x o r over algebra story problems that span 1 types  e.g.  overtake  opposite direction  roundtrip  taken from mayer  mayer  1 . solutions to these problems range from very similar to quite different. the domain theory includes formulae like those described above with a variety of ways for solving the quantities: distance  time  and rate. within each node of the abstraction hierarchy is a generalized-explanation subtree that is common to all descendents of the node. if there is no common substructure over the entire set of observed explanations  then the root of the hierarchy will be empty. 
   to incorporate an explanation into a classification tree  the explanation is compared to the explanation substructure of a node of the abstraction hierarchy  initially the root  and the remainder of the i o e procedure is applied to generalize the new explanation and the node's substructure. if this results in a generalization that is equivalent to the node's generalized explanation  then the new explanation must be more specific than the node's partial explanation; in this case classification of the explanation proceeds to the children of the node. 


figure 1: a classification tree over explanations  problem solving traces . 

if ioe yields a structure that is more general than the current node  then the node's explanation structure is not more general than the new explanation; in this case  the new explanation is made a sibling of the node. of course  to be useful the hierarchy must not simply be used to store explanations  but it should facilitate explanation construction. in this case  exor solves a target concept  e.g.  distance  using a problem statement of operational predicates and the classification tree. if the subexplanation stored at the node is applicable to the new problem and the variable instantiation constraints  if any exist  can be satisfied by the new problem then one of the node's children is selected for investigation. 
figure 1 illustrates the basic classification process. 
as search down the classification tree proceeds  exor extends the solution to the current problem using the subexplanations that are stored at each node along the path. if a contradiction occurs between a node's partial explanation and the known conditions of the problem statement  then the node is abandoned  its conditions  the partial explanation  are retracted  and search control looks to a sibling of the node. that is  control returns to the node's parent and another child is explored. if all of a node's children result in contradictions then an attempt is made to complete the partial explanation accumulated thus far by using the domain theory. this 'last resort' is represented in figure 1 by the 'house-shaped'  dashed boxes that emanate from examined nodes. if this fails then the node is abandoned as above  i.e.  its conditions are retracted and backtracking returns control to the node's parent . 
　figure 1 indicates that classification is not necessarily deterministic. some search of the tree  and domain theory  is still required  though we hope that this is less than an uninformed search of the domain theory. to 
 	learning and knowledge acquisition classification path 

problem solved 
figure 1: problem solving by classification. 
direct classification  a measure of category utility  gluck and corter  1  is used to rank the promise of children under a node. in addition to storing a partial explanation at a node  which is true of all of the node's descendents  we store statistics on the distribution of operational predicates of explanations stored under the node - statistical trends in operational predicates can be used to heuristically guide the selection of nodes from which exor builds an explanation for the current problem. 
　intuitively  category utility is a measure of the predictability and predictiveness of a new problem's operational predicates relative to a category - i.e.  a classification tree node. the predictability of a predicate fk relative to a category  node  ni is given as the probability that fk will participate in an explanation stored under ni. the predictiveness of a predicate is given by . the probability that an explanation with fk will be stored under ni. recalling the discussion from section 1  category utility is a tradeoff between these two factors:  

where p fk  weights the importance of the tradeoff for the most frequently observed predicates. when a problem statement is presented to exor  candidate nodes are ranked by their category utility scores over the operational predicates. the highest scoring nodes are investigated first. contradictions may still arise  causing e x o r to abandon a proposed node  but the inductive assumption is that the distribution of operational predicates provides considerable heuristic guidance. 
1 	experimental results 
e x o r ' s ability to improve problem-solving efficiency in the domain of 1 algebra problems was tested. a subset of 1 problems were selected for training and 1 were selected for testing. at intermittent points in training  i.e.  every four problems   the performance of the e x o r classification tree was evaluated. in particular  we compared the total number of predicates instantiated using a domain theory search  no learning   an ebg-like system  and the heuristically-guided ex or tree search over the 1 test problems.1 
   figure 1 illustrates the total number of predicates instantiated during a domain theory search and an e x o r classification tree search as training proceeds over the 1 training problems. the experiment has been run ten times with different random orderings of the data. the dashed horizontal line reflects the total work performed from domain theory search alone  i.e.  no learning  over the 1 test problems. the amount of work performed by e x o r trees is also graphed  but recall from the description of the explanation-construction procedure that search using an e x o r classification tree stems from two sources. first  e x o r searches a path in the classification tree to find a maximally-specific node that appears to be applicable to the new problem. the partial explanation at such a maximally-specific node may not contradict the operational  observed  predicates of the new problem  but the partial explanation may not be successfully extended by any of the node's children. however  before a node is abandoned  a final effort is made to extend the partial explanation using the domain theory; these  nested  domain theory searches are the second source of search. the shaded  lower  increasing  area reflects search in tree nodes; the upper  decreasing  curve gives 
1
　　 there are many dimensions over which we could have compared performance  and did : the total number of rules examined  and the total number of backtracks. we report predicate instantiations  as this is the most granular dimension. we did not choose to systematically investigate cpu time since the 'no learning' condition and our ebg implementation take direct advantage of the prolog interpreter in applying rules  while exor includes some overhead that is implementation-based  and not of theoretical consequence. this is not to say that all of our system's overhead is simply implementation-based. for example  the use of category utility and probabilistic matching are legitimate theoretical concerns in terms of rule match cost. future work will have to untangle theoretical from implementation overhead. suffice it to say that currently exor and ebg appear comparable in terms of cpu time  though this is a dubious comparison in favor of ebg. see segre  elkan  and russell  for a good summary of possible performance dimensions. 

                     training examples figure 1: performance as a function of training. 
the total amount of search required to solve all 1 test problems including the domain theory search  i.e.  the difference between the upper and lower curves  to complete partial solutions. thus  e x o r reduces the overall effort required to solve problems  i.e.  the decreasing curve ; the effort that is required is increasingly borne by the ex or classification tree  while the domain theory plays a corresponding smaller role as training proceeds. 
   the efficiency of e b g has not been graphed  but it was tested on the same data. after 1 training problems  e b g required 1 predicate instantiations to solve 1 test problems. this is considerably more than either the domain theory alone or e x o r . e x o r ' s relative success stems from its ability to exploit shared partial solutions. for example  e x o r can exploit the generalized solution from an 'opposite direction' and 'overtake' problem to partially solve a 'closure' problem; something that e b g can not do. this limitation of e b g is magnified when there are many explanations that differ in very minor ways. for example  suppose that an opposite-direction problem describes a car traveling east and the other traveling west  and there are domain theory inference rules that tell us that east and west are o p p o s i t e - d i r e c t i o n s   as are n o r t h and south. e b g will not be able to exploit the solution to the first problem in its attempt to solve a new problem  which is identical to the first  except that the cars are traveling n o r t h and south. in fact  the great similarity between problems may lead to a considerable amount of redundant search until a contradiction is found. this was the case with many of the problems in our domain  thus the poor performance of e b g . however  e b g proves quite adequate on problems that structurally match previouslyobserved problems. in the following section we will discuss this and other issues of explanation-based learning  and the manner in which these limitations are addressed by e x o r using lessons adapted from inductive learning. 
1 selective u t i l i z a t i o n a n d p r u n i n g 
in addition to our experimental demonstrations  exor classification hierarchies also suggest natural approaches to specific issues in explanation-based learning. one of these concerns the selective utilization  markovitch and scott  1; mooney  1  of learned knowledge. under what conditions should learned knowledge be exploited and when is it best to rely solely on the initial domain theory  markovitch and scott's lassy accumulates statistics on how frequently each antecedent of a rule will fail; learned rules are only used in attempts to prove antecedents that have succeeded sufficiently often  e.g.  at least 1% of the time . the justification for this strategy is that if a subgoal is likely to fail then one should not search for a subproof in vain twice once with learned rules and once with the domain theory from which the learned rules were constructed. mooney's  eggs uses a technique that is similarly motivated. these approaches mitigate the utility problem and improve efficiency  but nonetheless suffer from two limitations. first  the likelihood of subgoal failures is only estimated within the context of a single  domain theory  rule; intuitively  one might expect that the likelihood of a subgoal failure would be dependent on the more complete problem solving context. second  lassy and eggs decide to make all learned rules available for examination or none; rather we believe that the relevance of learned rules will vary with problem solving context. it should be possible to ignore rules that are deemed irrelevant. 
　an exor classification hierarchy addresses both limitations. each node represents the status of the complete problem-solving context. the system maintains statistics much like lassy's number of backtracks  failures  at each node. nodes with an unacceptable number of backtracks  e.g.  greater than 1% of the time  are pruned  quinlan  1 . if all of a node's children are pruned then this effectively identifies the problem solving contexts in which the system should rely exclusively on domain theory. those children that do remain serve to identify learned extensions that have been previously applicable; learned rules that are not present in these children are not considered when attempting to extend the explanation from the current node. figure 1 illustrates the classification process after pruning low utility nodes. rather than overfit the problem solution  exor turns to the domain theory at 'suitable' levels of specificity. 
1 c o s t - e f f e c t i v e 	f e a t u r e s 
in addition to pruning  we have also investigated a second extension. initially  exor's search procedure was guided by a category utility score computed solely over predicates that are known to be true from the problem statement. thus  problem statements draw an initial boundary of operationality  braverman and russell  1 . however  this boundary is not necessarily optimal for purposes of efficiency. for example  recall from an earlier example that predicates such as east and vest convey little information per $e - it is the inferred predicate  opposite-direction  that distinguishes solutions 
learning and knowledge acquisition 
classification path 

figure 1: classification after pruning low utility nodes. 
to which a problem corresponds. thus  improved performance is expected by combining some forward-chaining capabilities  e.g.  inference from the problem statement to an appropriate boundary  with the backward chaining mechanisms that currently dominate exor's processing. 
　the success of forward chaining depends on identifying predicates  e.g.  opposite-direction  that differentiate categories of different problem solving experiences  thus better focusing the search for solutions to new problems. however  proving or disproving the truth of a predicate requires effort as well. thus  an ideal boundary of operationally includes predicates with greater efficiency benefits than costs. we can formalize this notion in terms of the expected number of problem-solving steps  or predicates instantiated  or any of several other measures of cost  required to solve a problem with and without knowledge of a predicate's truth. let e c n  be the expected cost of solving an arbitrary problem beginning at node n of an exor classification tree. assume that we investigate the children  c -  of tv in order of probability. our inductive assumption is that children will successfully extend the current problem with roughly the same probability that they successfully classified earlier problems. thus  is the expected cost  e.g.  number of steps  of successfully finding the problem's solution under a child  cj  and i is the expected cost in an unsuccessful search of cj for a solution to the problem. thus  the cost of finding a solution in the second most probable subtree of n  cmax-1  includes the cost of having first searched the most probable node unsuccessfully. these quantities can be computed or at least approximated from the statistics that are maintained in the tree  e.g.  p cj   and from the structure of the tree itself  e.g.  

table 1: performance comparison of e x o r with different options. 
| original exor /w pruning and forward chaining | improvement  %  | tree search 1 1 1% dt search 1 1 1 % | total search 1 1 | 	1 % 	j 
e c cj  . using similar constructions we can approximate the expected cost when the truth of a predicate fk is known  e c n  fk   and the expectation in the case of 

   putting these quantities together  it is useful to forward chain in an attempt to verify a predicate if 

where e cp fk  is the expected cost to prove the predicate fk  p fk n  is the probability that we will be able to prove the truth of fk and 1 - p fk n  is the probability that it is not true. in general  knowing the complement to be true can also be predictive of a particular course of action  and exor exploits this knowledge as well. p fk n  can be approximated by the proportion of explanations stored under n  that contain fk and the proportion that do not. e cp fk  and e cp - f k   are currently approximated from the domain theory: how many rules contain fk as an consequent and how many rule combinations can conclude fk - assuming that the combinations that are responsible for concluding fk are equiprobable  the expected cost is proportional to approximately 1 the cardinality of this set. intuitively  the greater the branching factor and distance of fk from the initial operational  problem statement  boundary  the greater the cost of inferring it.1 
   notice that the utility of forward chaining on a particular predicate fk varies with the problem-solving context  i.e.  node n ; we only expend work on inferring fk when it will help distinguish n's descendents. once identified  statistics on these predicates are used in the category utility calculation to bias search in the most promising directions. 
   table 1 shows the result of the e x o r ' s performance after the 1 training examples with forward chaining and pruning. these extensions reduce the search within the e x o r tree by 1% and the saving from the domain theory search to complete a partial solution is 1%. the total savings from forward chaining and pruning is 
1%. 
1 	concluding remarks 
e x o r abstracts redundant explanation substructures and organizes them hierarchically for reuse  thus yielding advantages in terms of problem-solving efficiency. 
1
　　notice that e cp fk  is not conditioned on n  though this is clearly a preferable strategy; explanations under n may exhibit a relatively small number of combinations to prove n. thus  we wish to approximate e cp fk n  in the future. 
more generally  our research seeks to unify principles of inductive and explanation-based learning. first  traditional explanation-based concerns with efficiency can be cast in terms of the traditional inductive performance dimension of prediction accuracy: accurate prediction along search choice points result in a more efficient search  carlson et a/.  1; fisher and chan  1 . to some extent this relationship was recognized in earlier work that treated search-control learning as a problem of concept induction  mitchell et at.  1; langley  1 . however  we have strengthened this connection in several ways. notably  principles of feature predictiveness and predictability  which play a considerable role in inductive concept formation systems  lebowitz  1; kolodner  1; fisher  1   are also used to identify informative  cost-effective predicates and to guide the search for relevant past experience with these predicates. second  too much emphasis on feature predictability  specificity  can lead to data overfitting in explanation-based learning  as well as in inductive systems  quinlan  1; fisher and chan  1  where ithas been a long recognized problem. pruning in ebl contexts  as with inductive systems  mitigates the problem. finally  e x o r ' s inductively-motivated approach addresses some specific research concerns in ebl  notably the problem of selective utilization  mooney  1; markovitch and scott  1  and identifying appropriate boundaries of operationality  braverman and russell  1 . 
   a second research direction is to extend e x o r to other domains  particularly fault diagnosis. many engineering projects  e.g.  designing a purifier/pump system  construct a fault tree  malasky  1   which is an a n d / o r structure that describes the events  singly and in combination  that may lead to a top-level fault  e.g.  loss of pump flow . search for causes in this a n d / o r space is analogous to a domain theory search  and is thus amenable to speedup. as a human-engineered artifact however  there are often inconsistencies in the fault tree. thus  we plan to use the fault tree as an initial domain theory  but to use exor to organize experience and more efficiently guide diagnosis; logical inconsistencies may remain or they may be 'pruned' out  but in any case explanation patterns that better reflect the system's true behavior will come to statistically dominate e x o r ' s reasoning. thus  this approach to inconsistent domain theories is similar in intent to systems like towell  shavlik  and noordewier's  towell et a/.  1  neural net/ebl system  albeit with very different approaches to the inductive learning component. 
