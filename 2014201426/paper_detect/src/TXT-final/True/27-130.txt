 
this paper  along with the following paper by 
john mccarthy  introduces some of the topics to be discussed at the ijcai1 event 'a philosophical encounter: an interactive presentation of some of the key philosophical problems in ai and ai problems in philosophy.' philosophy needs ai in order to make progress with many difficult questions about the nature of mind  and ai needs philosophy in order to help clarify goals  methods  and concepts and to help with several specific technical problems. whilst philosophical attacks on ai continue to be welcomed by a significant subset of the general public  ai defenders need to learn how to avoid philosophically naive rebuttals. 
1 	ai as philosophy 
most ai researchers regard philosophy as irrelevant to their work  though some textbooks  e.g.  boden  1; russell and norvig  1   treat the two as strongly related  as does mccarthy  one of the founders of ai. if we ignore explicit statements of objectives  and survey the variety of research actually to be found in ai conferences  ai journals  ai books and ai departments  we find that ai includes: the general study of self modifying information-driven control systems  
  both natural  biological  and artificial  
  both actual  evolved or manufactured  and pos-sible  including what might have evolved but did not  or might be made at some future date . 
   this is extraordinarily close to a major concern of philosophers  namely asking what sort of minds are possible  and what makes them possible in a physical world. some  like kant  make the mistake of assuming that there is a unique set of necessary conditions for a mind  whereas ai research suggests that human-like mentality is not a simple all-or-nothing feature  but amounts to possession of a very large number of distinct capabilities  such as: many kinds of learning  seeing occluded surfaces as continuing behind obstructions  using quantifiers  making conditional plans  using nested sentences  and deferring goals. different subsets can occur in different organisms or machines. even humans have different subsets  according to age  culture  inherited dispositions  and whether they have suffered brain damage or disease. thus 'mind' is a cluster concept referring to an ill defined collection of features  rather than a single property that is either present or absent. 
　since different collections of capabilities define different kinds of minds  the old philosophical task of explaining what a mind is  is replaced by exploration of what minds are  through a study of their mechanisms  their capabilities  how they develop  and how some of them might evolve. t have described this   1a; 
1   as exploring mappings between 'design space' and 'niche space'  where niche space is the space of sets of requirements and constraints which may be satisfied  in varying ways and to varying degrees  by diverse designs. 
this undermines two opposing philosophical views: 
 a  that there is a single major division between things with and things without minds and  b  that there is a continuum of cases with only arbitrary divisions. both are wrong because there are many discontinuities in design space  corresponding to the presence or absence of particular capabilities  e.g. those listed above  that do not admit of degrees. 
   another topic on which ai can advance philosophy concerns 'qualia'  sometimes also referred to as 'raw feels'. these are defined variously as the contents of our experience  the answer to what it is like to feel  see or want something  and so on   dennett  1l  . some philosophers require that qualia have no physical effects and claim that different people may have different qualia without any objectively detectable evidence existing for the difference. 
　one reacticn is to argue against their existence  as dennett does. a deeper response will emerge from detailed work on the design of human-like agents. from an ai viewpoint it is obvious that a complete autonomous agent  unlike simple expert systems  must have myriad distinct  coexisting  interacting  information stores  including both long term collections of general information  personal history  procedural information  and short term stores corresponding to current goals and plans  suppositions  imaginings  thoughts  different levels in perceptual processing   marr  1; minsky  1; sloman  1    and motor control. what is not so obvious is that an agent needs to be able to attend to and control some of its internal databases   minsky  1; sloman  1; mccarthy  1   and may need to be 
	sloman 	1 

able to inform others about them  which we can do with varying degrees of accuracy  e.g. describing how we feel or how things look to us  or painting pictures  or setting up a situation that recreates the experience for others . by describing one's discomfort one can sometimes enable an expert  e.g. parent  or doctor  to prescribe a remedy. attention to internal states may also play an important role in learning. 
   whatever they may think  i claim that philosophers who talk about qualia are actually referring to internally detected states that are essential to the high level functional architecture of a sophisticated agent. fleas may not need them. of course  internal perception  like external perception  is liable to error  omission or oversimplification. in both cases  we can distinguish how things appear to the perceiver and how they actually are  e.g. from the standpoint of a scientist . similarly a software system may misreport the contents of its data-structures. of course  the agent or the system  cannot be wrong about how things appear to it  not because of privileged access but because that's what 'how they appear to it' means. our ability sometimes to switch attention from the environment to these internal information states will not be explained until we have a detailed account of an information processing architecture that replicates and explains typical human capabilities  including introspection. on that basis we shall  in principle  be able to build a robot that has qualia and may wish to talk about them and may even propose the philosophical thesis that qualia exist in a non-physical realm. 
   but the robot's qualia  like ours  will be complex information processing states  whose identity depends on an intricate web of causal and functional relationships to other states and processes  just as the identity of a 
spatial location depends on a complex web of spatial relationships with other things. in both cases  if we change the relationships the question whether we still have the same thing becomes undetermined. 
   there is a powerful illusion that  by focusing attention on the thing itself  we can uniquely identify what we are talking about and ask whether some other thing  another's experiences  a location seen later  is the same as the original. arguments showing the absurdity of this tendency are powerfully articulated in  dennett  1 . in some philosophers  the tendency is incurable. perhaps teaching them how to design robots with qualia will finally cure some who resist all other treatments. but some incurables will always remain. one day  their ranks will include robot philosophers who claim to have qualia. only when we understand why this is inevitable  will we have a complete theory of qualia. 
   there are many other ways in which ai can  and will  contribute to philosophy. there are unanswered questions about the nature of mathematical concepts and knowledge  discussed for centuries by philosophers in their armchairs. we shall gain a deeper understanding by doing experimental epistemology and studying designs for human-like information processing architectures that can learn about numbers in the ways that children do  including learning to distinguish between  a  empirical discoveries  e.g. adding two drops of water 
1 	panels 
to three drops can sometimes produce one large patch of water  and counting the same set twice sometimes gives different answers  and  b  non-empirical discoveries  e.g. counting elements of a set in two different orders should give the same result  two plus three equals five  there is no largest prime number . such mechanisms will require forms of learning and discovery not  yet addressed in ai  including the ability to reflect on the nature of their own discovery processes  e.g. distinguishing results where the environment's input is essential from those determined entirely by the structure of the mechanisms and processes  as kant argued . 
   designing testable working systems will teach us new  detailed  precise  answers to questions in other areas of philosophy. a good specification of a mind-like architecture can be used systematically to generate a family of concepts of mental states  processes and capabilities  just as our theory of the architecture of matter enabled us to create new concepts of kinds of stuff  and the architecture of an operating system allows us to define states it can get into  e.g. deadlock and thrashing. such a taxonomy of mental states will bp far more complex and open-ended than the periodic table: for there is but one physical reality while there are many kinds of minds supporting different families of concepts. 
   a new potentially important area of influence of ai on both philosophy and psychology concerns the study of motivation and emotions. as designs for complete or 'broad'   bates et a/.  1   agent architectures develop  we can expect to obtain a much deeper grasp of how motivational and emotional states arise  along with moods  attitudes  personality  and the like. these are all important aspects of the mind as a control system  a point made in simon's seminal paper  and developed in various ways since then e.g.  sloman and croucher  1; minsky  1; beaudoin and sloman  1 . 
   philosophy benefits also from computer science and software engineering  which provide concepts such as 'virtual1 or 'abstract  machine  'implementation' and 'implementation hierarchy   and show how causal relations can hold between information states  i've argued in  1b  that this answers philosophical questions about 'supervenience'  the converse of implementation  and shows how supervenient states can have causal powers  contrary to the view that only physical events have causal relations. 
　this undermines a common interpretation of newell's and simon's 'physical symbol system hypothesis'  e.g.  newell  1    for most of the symbols ai is concerned about are not physical  but structures in virtual machines. in fact  data-structures like sparse arrays show that there can be symbols that exist in a virtual machine without having any separable physical implementation: a large sparse array may contain far more items than the computer has memory locations. only in the context of the whole implementation do all the array locations exist. similar but more subtle global implementation relations probably hold between mental states and brain states  making the search for physical correlates of individual mental phenomena  including the detailed contents of qualia  futile. and yet these indirectly im-

plemented structures can exist  and have causal powers. 
1 	philosophy as ai 
not only does philosophy need ai to help with age-old problems  ai needs philosophy. to mis-quote santayana: those who are ignorant of philosophy are doomed to reinvent it  often badly. 
   in fact  much ai already builds on work by philosophers. an obvious example is the use of speech act theory  developed originally by philosophers such as john austin  john searle and paul grice. there are also various uses of specialised logics  e.g. deontic logic  epistemic logic  and modal logics  originally developed by philosophers in an attempt to clarify concepts like 'permission' and 'obligation'  deontic logic   'knows' and 'believes'  epistemic logic   and 'necessarily' and 'possibly'  modal logic . these contributions from philosophy are not passively accepted in at: putting them to use in designing working systems often reveals shortcomings and suggests further development. 
   there are much older contributions from philosophy. one was kant's proof in critique of putt reason that learning from experience was impossible without some sort of prior  innate  conceptual apparatus. another was frege's heroic  but unsuccessful  attempt a century ago to show that all arithmetical concepts could be reduced to logical concepts and all arithmetical knowledge could be derived from logical axioms and rules. this led him to a number of extremely important results  including the first ever accurate analysis of the role of variables in mathematical expressions  discovery of the notion of higher order functions and invention of predicate calculus  accomplished independently by c.s.peirce . this led  via work by russell  church and others  to lambda calculus  type theory  and other important notions in computer science and formalisms for ai. more recently the old philosophical controversy about varieties of forms of representations  e.g. logical and pictorial   which i discussed in   has become a topic of active ai research   narayanan  1  . 
   another recent development is recognition of deep connections between the ai task of understanding what sort of knowledge an intelligent system requires and the older philosophical activities of metaphysics  especially what strawson  described as 'descriptive metaphysics'  including ontology  the attempt to characterise in a systematic way what exists. the word 'ontology' is now commonplace in the darpa knowledge sharing effort   kqml  1  . this is required both as part of the methodology of knowledge elicitation for expert systems  and also for design of robots intended to communicate with humans  act on human goals  use human criteria for resolving conflicts and deal with the unexpected in ways that are acceptable to humans   mccarthy  1  . this extends the process outlined in chapter 1 of  sloman  1   linking conceptual analysis in philosophy with articulation of knowledge for intelligent artefacts. mccarthy's paper gives more examples of connections between ai and philosophy. see also  mccarthy and hayes  1; hayes  1 . 
1 	two way influences  and more 
i have listed some topics on which ai informs philosophy and others on which philosophy informs ai. in fact this is a spurious separation  for in all these areas the two activities inform each other  and as the depth of analysis increases  the amount of feedback increases  the work becomes more technical and specialised and the boundary between ai and philosophy will disappear. 
　philosophers and ai theorists have worked independently on the role of rationality in intelligence. much work by philosophers has been directed at clarifying conditions for rationality. dennett's 'intentional stance'  chapter 1  attributes beliefs and desires to agents on the assumption that they are rational. newell's knowledge level   1; 1   is also defined in terms of a presupposition of rationality. however deeper analysis shows   sloman  1b   that mechanisms of intelligence can be understood at the information processing level without assuming rationality. something closer to the design stance than to the intentional stance underpins ordinary concepts like 'belief  'desire'  'intention'. the designs implicitly presupposed by folk psychology will  of course  need to be superseded. 
　a design for an intelligent agent may be constrained by resource limits and inevitable gaps in knowledge  requiring mechanisms and strategies that mostly work but cannot be justified as 'rational'. sometimes the designer of a system can be regarded as rational even when the system isn't. more generally  though biological evolution  in effect  uses a fitness function to select the mechanisms on which our mental states and processes depend  the function need not be one that serves our goals. evolution's goals are not our goals  except when the mechanisms it implants in us serve its wider  implicit  purposes. an example is the drive to produce  feed and shelter young  often at great cost to parents. 
　human information processing mechanisms are extremely complex and unstable and easily diverted into states that serve neither the individual nor anything else. only from the design stance can we understand the resulting pathological behaviour  where the assumption of rationality is clearly invalid  despite efforts of some therapists to portray mental illness as rationally based.  insights from ai will eventually make a deep impact on psychotherapy.  
　the disappearing boundary between ai and philosophy is nothing new. it is often said that as philosophers discover how to make progress in some area  that area ceases to be philosophy and becomes a new technical discipline: e.g. physics  biology  psychology  logic  linguistics  or political science. compare the absorption of 
ai concepts and techniques by computer science  
　this illustrates the artificiality of academic boundaries: often they exist only because of academic politics  or the organisation of research funding agencies  rather than because the problems and techniques have clear boundaries. in fact  the topics discussed here in the overlap between ai and philosophy will increasingly have to merge with studies in other disciplines  not least neuroscience  psychology  social science  and the empirical and theoretical analysis of how complex informa-
	sloman 	1 

tion processing systems like ourselves and other animals could have evolved in a world that originally contained only physical processes. 
   this short paper barely begins to list the myriad links between ai and philosophy. there are many topics i have not had room to address  including: consciousness and free will  both of them 'cluster' concepts rather than names for something that is either present or absent ; issues raised by searle and penrose in their attacks on a i ; how machines can understand the symbols they use   sloman  1  ; the relevance of metamathematical incompleteness theorems; confusions surrounding the turing test; the role of states like pain and pleasure in intelligent agents; ethical issues about the rights and responsibilities of intelligent artefacts; debates about the philosophical significance of the choice between connectionist implementations and symbolic implementations  i have argued elsewhere   sloman  1b   that architecture dominates mechanism ; whether mentality requires causal embedding in an external physical environment  as argued in the 'systems' reply to searle ; whether ai needs non-computational as well as computational mechanisms; analysis of the concept of 'computation'; and prospects for future forms of intelligence  including distributed minds. some of these issues may turn up during discussions at ijcai1. many will recur at future ai conferences. 
