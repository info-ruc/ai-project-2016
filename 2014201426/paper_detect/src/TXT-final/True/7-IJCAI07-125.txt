
we present a conceptual framework for creating qlearning-based algorithms that converge to optimal equilibria in cooperative multiagent settings. this framework includes a set of conditions that are sufficient to guarantee optimal system performance. we demonstrate the efficacy of the framework by using it to analyze several well-known multi-agent learning algorithms and conclude by employing it as a design tool to construct a simple  novel multiagent learning algorithm.
1 introduction
multiagent reinforcement learning systems are interesting because they share many benefits of distributed artificial intelligence  including parallel execution  increased autonomy  and simplicity of individual agent design  stone and veloso  1; cao et al.  1 . q-learning  watkins  1  is a natural choice for studying such systems because of its simplicity and its convergence guarantees. also  because the q-learning algorithm is itself so well-understood  researchers are able to focus on the unique challenges of learning in a multiagent environment.
모coaxing useful behavior out of a group of concurrentlylearning q-learners is not a trivial task. because the agents are constantly modifying their behavior during the learning process  each agent is faced with an unpredictable environment which may invalidate convergence guarantees. even when the agents converge to individually optimal policies  the combination of these policies may not be an optimal system behavior.
모poor group behavior is not the universal rule  of course. many researchers have observed emergent coordination in groups of independent learners using q-learning or similar algorithms  maes and brooks  1; schaerf et al.  1; sen et al.  1 . however  failed coordination attempts occur frequently enough to motivate a host of q-learning adaptations for cooperative multiagent environments.  claus and boutilier  1; lauer and riedmiller  1; littman  1; wang and sandholm  1 . these algorithms are critical steps towards a better understanding of multiagent q-learning and of multiagent reinforcement learning in general. however  most of these algorithms become intractable as the number of agents in the system increases. some of them rely on global perceptions of other agents' actions or require a unique optimal equilibrium  conditions that do not always exist in real-world systems. as reinforcement learning and qlearning are applied to real-world problems with real-world constraints  new algorithms will need to be designed.
모the objective of this paper is to understand why the algorithms cited above are able to work effectively  and to use this understanding to facilitate the development of algorithms that improve on this success. we do this by isolating three factors that can cause a system to behave poorly: suboptimal individual convergence  action shadowing  and the equilibrium selection problem. we prove that the absence of these three factors is sufficient to guarantee optimal behavior in cooperative q-learning systems. hence  any algorithm that effectively addresses all three factors will perform well  and system designers can select means of addressing each problem that are consistent with the constraints of their system.
1 background and terminology
the simplicity of the q-learning algorithm  watkins  1  has led to its frequent use in reinforcement learning research and permits a clear  concise study of multiagent coordination problems in simple environments. multiagent coordination problems are not a direct consequence of the q-learning algorithm  however. thus  although we focus on q-learning in this paper  the analysis presented should be applicable in other reinforcement learning paradigms as well.
모a q-learning agent may be described as a mapping from a state space s to an action space a. the agent maintains a list of expected discounted rewards  called q-values  which are represented by the function q s a  where s 뫍 s and a 뫍 a are the current state and chosen action. the agent's objective is to learn an optimal policy 뷇  : s 뫸 a that maximizes expected discounted reward over all states s. at each time step  the agent chooses an action at 뫍 a  receives a reward r st at   and updates the appropriate q-value as
붟q st at  = 붸 r st at  + 붺 maxq st+1 a    q st at  
모모모모모모모모모모모모모모모모모모모a where 1   붸 뫞 1 is the learning rate and 1 뫞 붺   1 is the discount factor. at any point in time  the agent's best estimate of the optimal policy is its learned policy 뷇  s  = argmaxa{q s a }. the learned policy may differ from the optimal policy because it is based on q-value estimates. in q-learning  the learned policy also differs from the exploration policy executed during learning  off-policy learning . note that other approaches might employ identical learned and exploration policies  on-policy learning .
모under certain conditions  tsitsiklis  1   q-learning will converge to a set of optimal q-values

where t = {1 ...ロ and p st|st 1 at 1  is the probability of transitioning to state st given the previous state and action. if the agent has converged to an optimal q-value set  then the optimal and learned policies are identical: 뷇  s  = 뷇  s  = argmaxa{q s a }.
1 q-learning in multiagent systems
let si and ai represent the state and action space of the ith agent in an n-agent system. the state of the system can be expressed as a vector of individual agent states s =  s1 ... sn   si 뫍 si  and the combination of the agents' individual actions is a =  a1 ... an   ai 뫍 ai  resulting in joint state and action spaces  s and a.
모the combined policy for the system is a mapping 붫 : s 뫸 a  where 붫 st  =  뷇1 s1 t   ... 뷇n sn t   . in each time step  a joint action a = 붫 st  is executed  and each agent receives a reward ri si a  and updates the corresponding q-value qi si ai . note that the rewards the agents receive are based on the joint action  and individual state   but the agents update q-values for their corresponding individual action  and state . thus  multiple joint actions are aliased to a single action perception of the agent.
모it is sometimes useful to describe system behavior in terms of which joint policy an agent would prefer if it maintained a separate q-value for each joint action. this preferred joint policy is a mapping 붫 i : si 뫸 a and represents the joint policy which provides maximum discounted reward for agent i over all si. an agent's joint q-values qi si a  can then be defined as the average  over all joint states that contain si  expected reward received by agent i when joint action a is executed in si and 붫 i is followed thereafter. the relationship between the joint q-values and the agent's actual q-value set qi si ai  is described by

a
where p a|ai  is the probability of joint action a being executed when agent i selects individual action ai. note that the probability is conditional because for some a  those that do not contain ai  the probability is 1 but for others it is not. for those a that do contain ai  the probability depends on the actions of the other agents  in other words  p a|ai  is a function of the joint exploration policy 붫 s t  . the agent's optimal joint q-values are defined as
q i  si a  = ri si a 
|
	t	si t 
where t = {1 ...ロ and p si t |si t 1  at 1  is the probability of transitioning to individual state si t  given the previous state and action. an agent's preferred joint policy can be described in terms of the optimal joint q-values 
붫 i si  = argmaxa.
definition 1. a system of q-learning agents is cooperative if and only if any joint action that maximizes q i si ai  for one agent also maximizes it for all agents.
모this definition of cooperation does not require that the agents share the same reward signal for all joint actions; it requires only that the agents share a set of mutually-preferred joint actions. it thus allows for scenarios where the agents' preferences differ  but where the agents' needs can  and must  be simultaneously satisfied.
모we consider a solution to be system optimal if it is both a nash equilibrium and pareto-optimal. in a cooperative setting  this definition of optimality can be restricted to a subset of pareto-optimal nash equilibria called coordination equilibria. a coordination equilibrium is strict if the inequality in definition 1 is strict  the inequality will be strict if there is exactly one joint action that results in all agents receiving their max reward; in other cases there are multiple such joint actions and some sort of coordinated equilibrium selection becomes necessary .
definition 1. joint action a  is a coordination equilibrium if and only if
 {a i | ai 뫍 a and ai  뫍 a } qi si ai   뫟 qi si ai .
1 factors that can cause poor system behavior
we now identify three factors that can cause poor system behavior. the first and third factors are represented in various guises in the literature. the second factor  action shadowing  is likely to be less familiar to readers. in the next section  we will show that the absence of these three factors is sufficient to guarantee that the system will perform optimally.
1 poor individual behavior
an agent has learned an individually optimal policy 뷇i  si  = argmaxai{q i si ai } if its behavior is a best response to the strategies of the other players. q-learning is guaranteed to converge to an optimal q-value set  and hence to an optimal policy  in the individual case. however  this convergence guarantee breaks down in multiagent systems because the changing behavior of the other agents creates a non-markovian environment.
모despite this loss of theoretical guarantees  q-learning agents often converge to optimal policies in multiagent settings because  1  the agents do not necessarily need to converge to an optimal q-value set in order to execute an optimal policy and  1  if all agents are playing optimally  they must settle to a nash equilibrium  and nash equilibria tend to be self-reinforcing. individual behavior of q-learning agents is well-studied in the literature and in what follows we focus particularly on the following two potential problems.
1 action shadowing
action shadowing occurs when one individual action appears better than another  even though the second individual action is potentially superior. this can occur because typical q-learning agents maintain q-values only for individual actions  but receive rewards based on the joint action executed by the system. as a consequence  the agent's optimal policy may preclude the possibility of executing a coordination equilibrium.
definition 1. a joint action a  is shadowed by individual action a i in state s if and only if a i = 뷇i  si  and  a|a i 뫍 a q i si a     q i si a .
모a special case is maximal action shadowing  which occurs when the shadowed joint action provides maximal possible reward for the affected agent:
definition 1. an agent i experiences maximal action shadowing in state s if and only if there exist a  and a i such that a  is shadowed by a i and  a 뫍 a  q i  si a   뫟 q i  si a 
모the cause of the action shadowing problem lies in the qvalue update function for agents in a multiagent system. in each time step  each agent receives a reward ri si a  based on the joint action space  but it updates the q-value qi si ai   based on its individual action selection. consequently  agent i is unable to distinguish between distinct rewards that are all aliased to the same individual action. action shadowing is a consequence of the well-known credit assignment problem but is more precisely defined  and thus addressable .
모action shadowing is sometimes prevented by on-policy learning - agents seeking to maximize individual reward will tend to gravitate toward coordination equilibria. onpolicy learning does not assure that an action shadowing problem will not occur  however. maximal action shadowing is likely to occur despite on-policy learning in situations where failed coordination attempts are punished  as in penalty games  claus and boutilier  1 . maximal action shadowing will always cause a coordination problem when agent interests do not conflict.
1 equilibrium selection problems
an equilibrium selection problem occurs whenever coordination between at least two agents is required in selecting between multiple system optimal solutions. this is a somewhat stricter definition than the standard game-theoretic term  which refers to the task of selecting an optimal equilibrium from a set of  possibly suboptimal  potential equilibria.
definition 1. an equilibrium selection problem occurs whenever 
q	i	.
모the existence of an equilibrium selection problem does not necessarily result in suboptimal system behavior. however  an equilibrium selection problem creates a potential that the agents will mis-coordinate. whether or not this happens will depend on the complete reward structure of the task as well as on the exploration strategy used by the system. partially exploitive exploration strategies have proven particularly effective at encouraging convergence to a set of mutually compatible individual policies  claus and boutilier  1; sen and sekaran  1 .
1 achieving optimal performance
cooperating q-learners will behave optimally if each agent learns an individually optimal policy and if maximal action shadowing and the equilibrium selection problem are absent.
 theorem	1.	for	any	cooperative	q-learning	system   is a system optimal solution if the following conditions hold:
	a i 뫍 a  q	 
qi  si a 
proof. let a   be the joint action selected by the learned joint policy of the system. then  i a i = 뷇 i si  and by condition  1   i a i = 뷇i  si .
모we know from condition  1  that there cannot be a joint action a  such that. this implies that a i enables one or more joint actions that maximize agent i's joint q-value function:  {a1 ... am}|  j 뫍
{1 ... m} a  a i 뫍 aj and q.
모because the system is cooperative  any joint action that maximizes expected discounted reward for one agent must maximize it for all other agents as well. hence  we have a set of joint actions {a1 ... am} such that   i j 뫍
.
모from condition  1  we know that there can be at most one joint action that maximizes the expected discounted reward for all agents. it follows that m = 1 and there is a unique joint action a1 such that.
모since each agent's individual action a i enables a joint action that maximizes its expected discounted reward  it must be the case that  a = a1. because it maximizes expected discounted reward for every agent   a is a  strict  coordination equilibrium  by definition 1  and hence must be a system optimal solution.	
모naturally  these are not the only possible sufficient conditions to guarantee optimal system behavior. however  the conditions that make up our framework are preferable over many other possibilities because they can be addressed by modifying the learning algorithm directly  without placing additional constraints on the cooperative learning environment.
1 improving system performance
given the framework imposed by theorem 1  we consider various approaches to preventing coordination problems. topics are grouped according to two factors that affect system behavior: action shadowing  and equilibrium selection. the third factor  suboptimal individual convergence  is quite prevalent in the q-learning literature  and is too broad a topic to be examined here.
모there are two basic ways to improve system performance: control the task or modify the learning algorithm. in taskoriented approaches  reward structures are constrained so that action shadowing and the equilibrium selection problem are not present for any agent. algorithm-oriented approaches attempt to design algorithms that cope effectively with the above-mentioned problems. in general  algorithm-oriented approaches are superior because they enable the creation of general-purpose learners that learn effective policies regardless of the reward structure. still  it is useful to be acquainted with task-oriented approaches because algorithms designed for constrained environments will not need to explicitly address issues that are implicitly resolved by the environment.
1 task-oriented approaches for action shadowing
dominant strategies: a dominant strategy is a policy that maximizes an agent's payoff regardless of the actions of the other agents. if the task is structured in a way that creates dominant strategies for all agents  no agent can experience action shadowing.
1 algorithm-oriented approaches for action shadowing
joint action learning: if each agent is able to perceive the actions of its counterparts  then it will be able to distinguish between high and low payoffs received for different joint actions  rather than indiscriminately attributing the payoffs to a single individual action. this technique is often called joint action learning  claus and boutilier  1 . other examples of joint action learning include friend-or-foe q-learning  littman  1   nash q-learning  hu and wellman  1   and cooperative learners  tan  1 .
모optimistic updates and optimistic exploration: for deterministic environments  distributed reinforcement learning  lauer and riedmiller  1  has the same effect as joint action learning  but without giving the agents any extra information - agents optimistically assume that all other agents will act to maximize their reward  and thus store the maximum observed reward for each action as that action's utility. a variation for stochastic domains uses a weighted sum of the actual q-value and an heuristic to select an action for execution  kapetanakis and kudenko  1  . this heuristic results in effective convergence to optimal equilibria in some stochastic climbing games  but does not do so in all stochastic environments.
모variable learning rate: another approach to addressing the problem is to minimize the effect that the learning of other agents has on a given agent's own learning. this is the approach taken by wolf variants  bowling  1   in which a variable learning rate for updating the q-values has the effect of holding some agents' policies constant while others learn against the  temporarily  stationary environment.
1 task-oriented approaches for equilibrium selection
unique optimal solution: the simplest way to prevent the equilibrium selection problem is to design a system that has only a single optimal solution. when this is the case  the agents do not need to coordinate in selecting between multiple optimal equilibria. this is the premise behind the convergence proofs in  hu and wellman  1  and  littman  1 . it is also the only possibility for avoiding an equilibrium selection problem using wolf  bowling  1   note that the wolf variants have focused on adversarial general sum games  and not at all on cooperative ones .
1 algorithm-oriented approaches for equilibrium selection
emergent coordination: emergent coordination describes the tendency of a set of non-communicating reinforcement learners to learn compatible policies because each agent is constantly seeking a best response to the other agents' actions. this has been demonstrated  for example  in hexapedal robot locomotion  maes and brooks  1   network load balancing  schaerf et al.  1   and a cooperative boxpushing task  sen et al.  1 .
모social conventions: a social convention is a pre-arranged constraint on behavior that applies to all agents  such as driving on the right side of the street. this is the premise behind social learners  mataric  1  and homo egualis agents  nowe et al.  1   and it has also been used as a coordination mechanism in q-learning systems  lauer and riedmiller  1 .
모strategic learners: strategic learners are agents that model their counterparts and select an optimal individual strategy based on that model. one commonly-used model in games where the agents can see each others' actions is fictitious play  claus and boutilier  1  and its variant  adaptive play  wang and sandholm  1; young  1 . another example is that of concurrent reinforcement learners  mundhe and sen  1 .
1 applying the framework: incremental policy learning
in this section we describe a simple learning algorithm designed by addressing each of the conditions of theorem 1. this algorithm  called incremental policy learning  addresses the issues of optimal individual convergence  action shadowing and the equilibrium selection problem. it consistently learns to play a coordination equilibrium in deterministic environments.
모achieving optimal individual behavior: incremental policy learning achieves optimal individual behavior by using a standard q-learning update equation to estimate q-values.
모preventing action shadowing: following the example of  claus and boutilier  1   incremental policy learning prevents action shadowing by learning q-values over the entire joint action space. each agent can perceive the action selections of its counterparts  but only its own reward signal  and uses this information to learn q-values for all possible joint actions. this enables the agents to clearly determine which individual actions may lead to coordination equilibria.
모addressing the equilibrium selection problem: incremental policy learning uses a sequence of incremental policy adjustments to select between multiple optimal equilibria. each agent maintains a probability distribution p = {p a1  ... p am } over its available action set a = {a1 ... am}. these probabilities are initialized and modified according to the algorithm described below.
1 the incremental policy learning algorithm
  initialization
 v is arbitrarily chosen such that i=1 i and i   1.
  action selection
모in each time step t  the agent selects an action a t  뫍 a according to probability distribution p. the agent executes this action  receives a reward signal r t   updates its joint q-values  and updates p as described below.   probability updates
let rmax be the maximum q-value stored in the joint action table
let 1   붸 뫞 1.
if r t  뫟 rmax then  i: if
	if	 
모here rmax is the reward for the target equilibrium  the reward for the preferred joint action . intuitively  whenever rmax is received as a reward  the action selection probability distribution is skewed somewhat toward the action that resulted in its receipt.
모a proof sketch that the ipl algorithm meets the criteria of theorem 1  and thus will result in optimal system performance  proceeds as follows. condition 1 is met because the individual agents use standard q-learning. condition 1 is met because the agents are allowed to see the joint action space. an argument for meeting condition 1 is given in  fulda and ventura  1 .
1 results
we first allow the agents to select random actions until their joint q-values converge  and only then use the coordination mechanism described above. this results in the agents consistently learning to play a coordination equilibrium. however  such strictly controlled situations are of limited interest.
모we next experiment with two agents learning q-values and the coordination policy simultaneously. these agents repeatedly play a  stateless  single-stage game in which each agent has five possible action selections. each cell of the payoff matrix was randomly initialized to an integer between 1 and 1  different random payoffs were assigned to each agent   with the exception of five randomly placed coordination equilibria whose payoff was 1 for both agents. the algorithm was tested in both deterministic and stochastic environments  each reward signal was summed with gaussian noise .
모figure 1 shows the algorithm's performance as a function of 붸  averaged over 1 trials. because the q-values and the

figure 1: incremental policy learning performance as a function of 붸
policy are learned simultaneously  the agents do not always achieve their maximum expected rewards. this occurs because the agents' policies sometimes settle before the q-value estimates for the coordination equilibria are large enough to be distinguished from the q-values of less desirable actions. as expected  the algorithm performs better with lower values of 붸. the smaller 붸 is  the more likely it is that the joint qvalues will converge to their correct values before the agents' policies settle  which in turn enables the agents to easily learn a coordination equilibrium. interestingly  even when 붸 approaches 1  the performance of the algorithm degrades rather gracefully.
1 discussion
the methods used by incremental policy learning are simple  but the principle demonstrated is powerful. an algorithm that successfully achieves individual optimal performance  avoids maximal action shadowing  and addresses the equilibrium selection problem will learn an optimal group behavior in cooperative environments. incremental policy learning satisfies these requirements in deterministic environments when 붸 is sufficiently small. in fact  the algorithm performs well even when these requirements are violated.
모incremental policy learning is particularly suited to environments with small numbers of interacting agents. if the number of agents becomes very large  a method of addressing the action shadowing problem other than joint action learning would be required. a possible alternative is to represent only significant subsets of the joint action space  as in  fulda and ventura  1 .
1 conclusion
we have identified a set of conditions sufficient to guarantee optimal performance for systems of cooperative  concurrently learning agents. each condition can be met in multiple different ways  thus enabling the creation of learning algorithms that are suited to the constraints of a particular environment or task. as an example  a learning algorithm has been presented that addresses each of the conditions.
모the major advantage of our framework is that the conditions can all be satisfied through algorithm-oriented approaches. in contrast  many other conditions sufficient for optimal performance require additional constraints on the environment or the task  for example  iterated strict dominance games or the generic condition in which all solutions are coordination equilibria . since our stated objective is to assist in creating algorithms that are uniquely adapted to the environment  we require conditions that can be addressed through the algorithm itself.
모future work will concentrate on generalizing the approach to competitive environments and eventually to environments of conflicting interest  such as battle of the sexes and the prisoner's dilemma . one approach to this replaces the qfunction with an evaluation function so that although agent preferences may differ  they will coordinate in seeking a compromise between those preferences  essentially converting an adversarial system into a cooperative one. this is the approach taken by hu and wellman's nash q-learning  in which the agents seek to play nash equilibria rather than seeking to maximize rewards directly  hu and wellman  1 .
모also  the framework presented here makes an underlying assumption of the independence of the individual states si. that is  it assumes that state si of agent i  at time t  will not affect the state sj of agent j at some later time. it would be interesting to generalize this work to consider the case when this independence assumption does not hold.
