
image alignment refers to finding the best transformation from a fixed referenceimage to a new image of a scene. this process is often guided by similarity measures between images  computed based on the image data. however  in time-critical applications state-of-the-art methods for computing similarity are too slow. instead of using all the image data to compute similarity  one can use a subset of pixels to improve the speed  but often this comes at the cost of reduced accuracy. this makes the problem of image alignment a natural application domain for deliberation control using anytime algorithms. however  almost no research has been done in this direction. in this paper  we present anytime versions for the computation of two common image similarity measures: mean squared difference and mutual information. off-line  we learn a performance profile specific to each measure  which is then used on-line to select the appropriate amount of pixels to process at each optimization step. when tested against existing techniques  our method achieves comparable quality and robustness with significantly less computation.
1 introduction
the need to align  or register  two images is one of the basic problems of computer vision. it can be defined as the task of finding the spatial mapping that places elements in one image into meaningful correspondence with elements in a second image. it is essential for data fusion tasks in medical imaging  hajnal et al.  1  and remote sensing  e.g.  colerhodes et al.  1  . it is also widely applied in tracking and automatically mosaicking photographs  szeliski  1 .
¡¡one of the most straightforward and widely used approaches is referred to as direct image alignment. it works by defining a similarity measure  d  as a function of a reference image  and a template image warped by a transformation with some parameters  ¦Õ. the computation of d typically requires examining all pixels in each image. the alignment problem becomes that of finding the values of ¦Õ that maximize the chosen similarity measure. a number of optimization techniques for smooth functions  such as gradient descent  have been used for this problem  and provide good solutions on a wide range of image types. however  these approaches can be slow  which reduces their usefulness in time-sensitive applications such as real-time video registration  e.g.  wildes et al.  1   and medical image registration during surgery e.g.  pennec et al.  1  . it is possible to increase the speed of processing by using only a subset of the pixels to compute d  but this can easily lead to a reduction in accuracy and reliability. determining the size of the subset to use is typically done in an ad-hoc fashion  or using heuristics which are applicable only to certain domains. furthermore  since a different number of pixels may be needed at different stages in the optimization  a fixed subset is necessarily a compromise.
¡¡in this paper  we propose a deliberation control framework using anytime algorithms  dean and boddy  1; horvitz  1  to arrive at a principled solution to the speed vs. accuracy trade-off in this problem. the first step is to learn the properties of the similarity measure under consideration  in terms of accuracy vs. computation time  by training off-line on image pairs for which the transformation parameters are known. given a new pair of images to align  we then use this knowledge to determine the number of pixels that need to be considered at each step of the optimization. in this paper  we explore the effectiveness of this approach using two common similarity measures  mean squared difference and mutual information  and a gradient descent optimizer. we tested the algorithm on several types of images: images of everyday scenes  multi-modal medical images and earth observation data  i.e. landsat and radarsat images . in all cases  using a deliberation control approach is faster than computing the transformation using all the image data and gives more reliable results than simply performing the optimization using an arbitrary  fixed  percentage of the pixels.
¡¡the remainderof this paperis organizedas follows. in section 1 we review the image alignment problem and in section 1 we review methods of deliberation control using anytime algorithms. the details of how deliberation control has been implemented in the context of image alignment are given in section 1. finally  sections 1 and 1 describe our experimental setup  results and conclusions.
1 image alignment
direct approaches to image alignment work by defining a function d that measures the similarity between a fixed reference image  r x  and a new image t w x ¦Õ  . here  we consider the images r x  and t x  to be continuous functions of position  x  defined on some space of coordinates x. the coordinate space is warped by w x ¦Õ  which is a mapping from x to x  parameterized by ¦Õ  e.g.  a translation or rotation . since our images are actually sets of pixels located at integer coordinate positions  we use linear interpolation as needed to determine the values of t w x;¦Õ   when w x;¦Õ  is not an integer. the similarity measure  d  is thus a function of the transformation parameters ¦Õ  and the problem of image alignment becomes an optimization problem  which can be solved using many standard techniques  e.g.  gradient descent  second-order methods  stochastic programming etc. all these techniques require the repeated calculation of d  and/or its gradient   ¦Õd  at different points in the space of possible transformations. this is by far the most computationally intensive part of the process.
¡¡recently  feature based approaches to alignment have seen considerable success  and can operate faster than direct approaches for many applications. nevertheless  the direct approaches can yield higher overall accuracy and continue to be used as a final adjustment step  szeliski  1 . furthermore  it is difficult to match features reliably when the images in question are not of the same modality. hence  direct approaches are the method of choice in applications such as medical imaging  hajnal et al.  1  and geomatics  colerhodes et al.  1   where high precision is required and multimodal imagery is common.
¡¡it has long been known that numerical optimization approaches can be significantly accelerated by using only a subset of the pixels in the images to estimate the similarity function. for example  many implementations of mutual information  e.g.  ibanez et al.  1   use a random subset of the image data. it has been suggested that it may be more efficient to include only pixels with high derivatives in the calculation  szeliski  1 . however  the size of the subset to be used is fixed in an ad-hoc fashion. unfortunately  any fixed size is usually too much for some regions of the parameter space and too little for others. furthermore  different types of images behave differently  so it is difficult to come up with a subset size that is appropriate for all cases. instead  we will use a deliberation control mechanism to choose how much computation to perform at each step of the optimization.
1 deliberation control with anytime algorithms
in many artificial intelligence tasks  e.g. planning  the quality of the solution obtained depends on the amount of time spent in computations. hence  trade-offs are necessary between the cost of sub-optimal solutions and the cost of spending time doing further computation. this process  called deliberation control  has been investigated in the context of real-time artificial intelligence and a number of approaches have been proposed  horvitz and zilberstein  1 . deliberation control methods rely on two key components: algorithms that support partial evaluation  and knowledge about how those algorithms perform after different amounts of computation.
¡¡a class of algorithms supportingpartial evaluation are anytime algorithms  horvitz  1; dean and boddy  1   which providea solution when run for anylength of time. the solution quality is guaranteed to improve with the amount of computation performed. deliberation control strategies using anytime algorithms have been applied to both theoretical and practical problems including robot control  vlassis et al.  1   constraint satisfaction  wah and chen  1  and shape extraction in image processing  kywe et al.   1 .
¡¡to formulate an effective deliberation control strategy using anytime algorithms it is necessary to have meta-level knowledge of their performance as a function of the amount of computation performed  dean and boddy  1; horvitz  1 . this knowledge is stored in a performance profile. performance profiles may be based on theoretical knowledge of the algorithm  on empirical testing of its performance at different computation levels  or a combination of the two. in any case  the decision to continue computation will be based on an estimate of the accuracy of the current result  and an estimate of the potential improvement if the algorithm continues to run  dean and boddy  1; horvitz  1; larson and sandholm  1 .
¡¡the simplest type of performance profile is a static one  which predicts accuracy as a function of the amount of computation completed. however  for our problem of interest  this is equivalent to simply using a fixed  arbitrary percentage of the pixels. if feedback about the current run of the algorithm is available  it can be incorporated in a more sophisticated approach. a dynamic performance profile  larson and sandholm  1  uses feedback to estimate the accuracy as the algorithm progresses. it is described by two functions:  a = pfwd f  p   maps the percentage of computation completed  p  and the feedback parameter  f  to an expected accuracy   a. the other   p = prev f a   maps a and f to the expected percentage of computation required   p. conceptually  these two functions are inverses. however  both have to be maintained in general  to facilitate the decision making. a controller can use these functions to gradually increase the amount of computation performed  until the estimated accuracy is adequate for the task.
1 deliberation control in image alignment
as mentioned in section 1  the most computationally intensive part of image alignment is the repeated evaluation of the similarity measure d and its gradient  d. the optimization algorithm needs this information in order to take a step in parameter space towards the optimal setting. note that the calculation only has to be accurate enough to ensure that the next step is correct; determining these values exactly is not necessary. therefore  we propose to implement d and  d as anytime algorithms  and to learn performance profiles describing their accuracy at different levels of computation.
1 anytime similarity measures
we implemented two popular image similarity measures as anytime algorithms. the resulting implementations have to be able to support partial evaluation  as well as to continue an interrupted calculation efficiently. to achieve this  we redefine each similarity measure  d ¦Õ   as a function d ¦Õ  p  of both the parameters  ¦Õ  and the percentage of pixels to be used  p. to avoid biasing the computation towards one area of the image  the pixels are processed in a random order.
mean squared difference
the negative mean squared difference dmsd is one of the most common similarity measures  szeliski  1   and is suitable for images of the same modality. in its original form  dmsd is simply an average of the negativesquareddifferences in intensity between corresponding pixels  the negative here is simply to get a similarity  rather than a distance measure .
dmsd
where n is the total number of pixels in the image. the gradient of this similarity measure is also easy to compute:
 ¦Õdmsd
note that both dmsd ¦Õ  p  and  ¦Õdmsd ¦Õ  p  can be updated incrementally in the usual fashion.
mutual information
the mutual information mi image similarity measure viola and wells iii  1  is useful for images of different modalities. our anytime implementation is based on the efficient mutual information implementation proposed in  the¡ävenaz and unser  1   which relies on a b-spline windowed representation of the joint probability distribution of the intensity levels in the two images. specifically  let brk  where k = 1...k  be a set of k bins of width dr for the intensity values in the reference image starting at br1 = minx r x . similarly  let btl be the bins for the intensity values in the template image  where l = 1...l  the bins begin at bt1 = minx t x  and have width dt. then the unnormalized joint distribution is an array of size k ¡Ál. the entry pkl is equal to the number of pixels in r for which the intensity falls in bin k and the intensity of the corresponding pixels in the transformed image falls in bin l:

pkl ¦Õ  p =
	=1	dr	dt
where ¦Ä is equal to 1 if its two arguments are equal  and zero otherwise. the¡ävenaz and unser  1  use instead a soft version to compute the entries in the table  based on b-spline parzen windows. similarly to their work  we will use:
pkl ¦Õ  p  =
where ¦Â1 and ¦Â1 are 1th and 1rd order b-spline parzen windows respectively. the normalization factor of pkl is:
	k	l
¦Á ¦Õ  p  = ¡Æ¡Æ pkl
k=1l=1
this is true because b-splines satisfy the partition of unity constraint. the mutual information can then be computed using the usual formula:
	k	l p  ¦Õ  p 	¦Ápkl ¦Õ  p 
dmi
k
note that the second factor in the denominator above is just the intensity histogram of the original image  which is computed only once  before the optimization process.
¡¡because a 1rd order b-spline is differentiable  the gradient of the joint histogram   ¦Õpkl ¦Õ  p   can be computed in the usual way as well  and stored in |¦Õ| tables  each of dimension k ¡Ál. in  the¡ävenaz and unser  1  it is shown that when using the above formulation the derivative of dmi is:
	k	l	pkl ¦Õ  p 
 ¦Õdmi ¦Õ  p  = ¡Æ¡Æ¦Ã ¦Õpkl ¦Õ  p  k	l	
where ¦Ã is the normalization factor . thus  when the derivative is required  we can compute it from the joint probability distribution  and its gradient.
¡¡the algorithm maintains the unnormalized probability distribution  and its unnormalized partial derivatives  as described above  which can be easily updated when more pixels are added  because the table entries are simple sums. the distance measure and its gradient are then computed as needed from this table.
1 performance profiles
exploiting the partial evaluation possibilities of the anytime similarity measures requires dynamic performance profiles describing their expected accuracy at different computation levels and feedback values. the notion of accuracy must be developed in terms of the optimization method being used. in this paper  we used a simple steepest descent optimizer  which is described in  ibanez et al.  1. carter  1  has analyzeda similar class of optimizersand has proventheir convergence using the following measure of relative error:
	¦Å =	 1 
|| trued||
¡¡a dynamic performanceprofile requires a feedback parameter which indicates the progress of a particular calculation run and based on the above equation the gradient magnitude is an ideal candidate. thus  we designed our performance profiles to be tables mapping computation level and gradient magnitude to accuracy. to construct them we sampled the gradient at different computation levels at many points in the transformation space. for each computation level  p  we grouped the gradient magnitudes into bins and computed the expected accuracy  e¡¥p  of the gradient for each bin as follows:
	e¡¥p = 1 ¡Æ || 1||% d1 ¦Õ%i d   ¦Õip d|| ¦Õi || 	 1 
i
 where the ¦Õi are the sampled points in the transform space .
¡¡the optimizer uses this table to progressively increase the amount of computation performed until the estimated accuracy reaches its criterion for acceptability. the analysis in  carter  1  indicates that significant computational gains can be made  with a small  ¦Å ¡Ö 1%  reduction in accuracy. therefore we choose to have the optimizer seek an expected accuracy of 1% for each gradient that it computes. a simple example of how the table can be used to control computation is shown in figure 1. this table can act as a performance profile where two functions   a = pfwd f  p  and p  = prev f a  are implemented through simple lookup. for example  suppose an optimizer requires an accuracy of 1%. on an initial probe  f  which is in our case the current estimated performance  is 1. by examining the row applying to 1 and less  we get a prediction that with p = 1%  we will obtain the desired accuracy level  arrow 1 . after performing 1% of the computation  however  suppose f is now 1. thus the accuracy is only 1%  arrow 1  meaning more computation is required. the required p is now estimated as 1%  arrow 1   and so on. in our case  the parameter p will be the percentage of pixels processed in the image. the feedback parameter is the magnitude of the gradient.
figure 1: dynamic performance profile example: values in the table represent the expected accuracy of the results.
1 experiments
to test the anytime algorithm approach  a number of performance profiles were generated  and alignments were performed. four classes of images  shown in figure 1  with at least two image pairs each were used in the testing process. the first image class was typical digital photos  dp   images a-d . both a  and d  were self-aligned and a  was aligned affinely against several images of the same scene taken from different camera positions  images b c  using both similarity measures. the second class of images  m1   images e f  were slices from t1-weighted magnetic resonance imaging  mri  volumes which were self-registered using the dmsd measure. the third class of images  eo  are patches from georeferenced  orthorectified landsat 1 and radarsat imagery that were registered to each other using the mutual information measure  rows 1 and 1 . the final class of images  m1  are slices frompreviouslyregisteredvolumes in differentmedical imaging modalities  including t1 and t1 weighted mri  and computed tomography  ct   last row . these images were aligned to each other using the mutual information measure.

     1images 1a  1b  1c  and 1d from k. mikolajczyk http://www. inrialpes.fr/lear/people/mikolajczyk/. landsat and radarsat images  1g 1h 1i 1j  1k 1l  from natural resources canada http://geogratis.gc.ca. medical images  1e  1f 
1m  1n  and 1o  courtesy montreal neurological institute
	 a  boat-1	 b  boat-1	 c  boat-1
	 d  graffiti	 e  mri-1	 f  mri-1
	 g  landsat-1	 h  landsat-1	 i  landsat-1
	 j  radar-1	 k  radar-1	 l  radar-1
	 m  mri t1	 n  mri t1	 o  ct
figure 1: images used for the experiments 1
1 generating performance profiles
before performing any alignments  performance profiles for each combination of image class and similarity measure were generated off-line. these profiles were constructed using training image pairs of the same modalities as the ones to be aligned. this training data was separate from the testing data used later. for each training pair  the value of  d was computed at 1 points in the transform space  at 1 different computation levels  and the profile was constructed using the method described in section 1. the resulting performance profiles are shown in figure 1. note that each profile has a roughly similar shape. at large values of the feedback parameter  || ¦Õd||  little computation is needed to get a good result. for smaller values  however  progressively more computation is needed. this agrees with our intuition; since the image noise level remains constant  small values are progressively harder to measure. it is the curved shape of these graphs that allows us to realize important performance gains. a constant fraction of pixels would inevitably be too many for some feedback values and too few for others.
¡¡despite their basic similarity  however  there are important differences between the profiles. for example  note that for both similarity measures  the medical images require a much greater percentage of computation for an accurate result for

image
pairrms  pixels run time  s failure rategd1gd1gd1agdgd1gd1gd1agdgd1gd1gd1agda -a   dp 1¡À11¡À11¡À11¡À111111%1%1%1%d -d   dp 1¡À11¡À11¡À11¡À111111%1%1%1%a -b   dp 1¡À11¡À11¡À11¡À111111%1%1%1%a -c   dp 1¡À11¡À11¡À11¡À111111%1%1%1%e -e   m1 1¡À11¡À11¡À11¡À111111%1%1%1%f -f   m1 1¡À11¡À11¡À11¡À111111%1%1%1%msd-avg1¡À11¡À11¡À11¡À111111%1%1%1% a  results for the msd measure
image
pairrms  pixels run time  s failure rate  % gd1gd1gd1agdgd1gd1gd1agdgd1gd1gd1agdm -o   m1 1¡À11¡À11¡À11¡À111111%1%1%1%n -o   m1 1¡À11¡À11¡À11¡À111111%1%1%1%m -n   m1 1¡À11¡À11¡À11¡À111111%1%1%1%g -j   eo 1¡À11¡À11¡À11¡À111111%1%1%1%h -k   eo 1¡À11¡À11¡À11¡À111111%1%1%1%i -l   eo 1¡À11¡À11¡À11¡À111111%1%1%1%a -a   dp 1¡À11¡À11¡À11¡À111111%1%1%1%d -d   dp 1¡À11¡À11¡À11¡À111111%1%1%1%mi-avg1¡À11¡À11¡À11¡À111111%1%1%1% b  results for the mi measure
table 1: experimental results: results by image pair  and combined for each measure  bottom rows 
algorithms: gdx - standard algorithm  using x% of pixels; agd - anytime algorithm. entries shown in plain font are not significantly different from gd1; those in italic are ambiguous  see text  and items in bold significantly differ from gd1.figure 1: performance profiles: the percentage of computation required to achieve ep = 1%.
a given gradient magnitude. the alignment tests  discussed below  reveal that these images require more computation to align successfully. also note that the dmi performance profiles for both geographic data and medical images indicate that no less than 1% of the pixels will ever be used for these classes of images using this method.
1 alignment tests
we implementedourapproachby extendingan existing  welltested  image alignment implementation. specifically  we used an implementation similar to the example multiresimageregistration1 in  ibanez et al.  1   pp. 1  with appropriate transforms  and with extensions added to the optimizer and measures to support deliberation control. the experimental procedure was as follows. three sets of 1 random starting positions were created. each set was at a different effective distance from the identity transform in order to test the algorithms over the capture range of the optimizer. for each combination of image  similarity measure and algorithm  the true transform was composed with these starting positions  and the result was used to initialize the alignment process. alignments were performed using the standard approach  labeled gd1 in the graphs   the standard approach using only a specified percentage of pixels  labeled gdx  if x% of the pixels is used  and our anytime approach. the computational efficiency was measured in terms of running time. each reported time was obtained on a 1ghz amd athlon machine with 1gb of ram.
¡¡to determine if any observed reductions in runtime came at the expense of performance  we also measured the quality and reliability of the algorithms. the quality of a result was measured by the root-mean-square  rms  position error compared to the true transformation position. to calculate this  1 randomly placed points in the unit square were scaled to the image extent. these were transformed both using the known true transformation and the computed transformation. the reported rms is the square root of the mean of the squared coordinate differences for these points.
¡¡the reliability of each method was measured by the number of failed alignments. a particular run was considered to have failed when the registration converged to a transformation that lead to rms pixel position errors greater than 1 pixels  or when the registration failed to converge to an answer at all. in practice  we found that this criterion was rarely ambiguous. the alignment would either yield results that were much better than this rms value  or much worse. failed runs were not calculated in the average times or registration error.
since each algorithm being tested was run on the same set
of test inputs  we used paired comparisons to test for significance. for each criterion  the null hypothesis  h1  was that the algorithms had the same performance as the original. the rms errors and run times were compared using pairwise ttests  and the failure rates were compared using the mcnemar test. to adjust for multiple comparisons  we used the tukey method for the rms and timing data  and the bonferroni method for the failure rates  howell  1 . all tests were performed at the 1% confidence level.
¡¡adjusting for multiple comparisons can tend to accept h1 when it should be rejected  artificially bolstering our argument. therefore  we report all three possible cases. where all tests rejected h1 we conclude that performance differs. when h1 was rejected pairwise  but accepted when adjusted for multiple comparisons we consider the result ambiguous  and finally when h1 was accepted by all tests  we conclude that the data do not indicate a performance difference.
¡¡the experimental results are summarized in table 1 a   for msd  and b   for mi . the tables show the runtime of each method for each image pair  as well as for all runs combined. we also show the failure rate  and the rms error with standard deviation. for the msd measure  little can be concluded. all the algorithms under test show some improvement in speed  without significantly affecting failure rate or rms error. for this measure  there is little to distinguish our method from simply reducing the number of pixels. however  the results for the mi measure highlight the advantages of our method. reducing the number of pixels by a percentage frequently incurs a statistically significant loss of quality or reliability. the anytime method  however  delivers significantly faster times without sacrificing either the rms error  or the failure rate. this is particularly apparent in the overall results  bottom row table 1-b .
¡¡an advantage of our approach is its adaptability. it performed the alignment of digital photos  dp  with the mi measure using a little more than 1% of the original running time  without changing the failure rate. in other cases  particularly the multimodal medical image registration  m1   more pixels seem to be inherently required to successfully align the images. our method adapts to that requirement and maintains a low failure rate by increasing the computation performed.
1 conclusions and future work
we proposed to use deliberation control methods in order to improve the efficiency of computer vision applications. we implemented such methods for the image alignment problem and showed a significant improvement in speed without degrading the quality of the results. even when the performance gains are limited  a major advantage of this approach is that the number of pixels used is determined using a training process. our results show that arbitrarily selecting a percentage of the image data to use for alignment will lead to very different results on different classes of images. our method gives a principled way to determine how much of the image data needs to be processed to achieve reasonable results.
¡¡in the future  we plan to further investigate the problem of multi-modal image alignment using such algorithms. in this case  the amount of data is prohibitive for exact methods  especially if the data is volumetric  and if the registration has to be performed in real-time during surgery. we will investigate the use of more sophisticated methods for obtaining the performance profile and doing the deliberation. we will also look at other ways of highlighting the anytime aspect of similarity measures commonly used in computer vision.
