  
qiong goo and ming li 
department of computer science  york university 
north york  ont. m1j 1 canada 
abstract 
       our objective is to introduce rissanen's minimum description length  mdl  principle as a useful tool for character recognition and present a first application. using mdl principle  a learning system has been implemented which  after simple training  is able to online recognize th trainer's handwriting in english and chinese  including several thousand characters  with high success rate. the experimental results conform with the theoretical predictions. we will also try to give an elegant explanation of rissanen's minimum description length principle  mdlp . 
areas: b1 	- learning  knowledge acquisition; di - philosophic al foundations. 

1. introduction 
this research represents one of our efforts to apply the recently developed machine learning/inference theories  valiant 1  rissanen 1  to real world problems. in recent years  on one hand  new exciting learning theories have developed out from the computational complexity theory | vi   statistics and kolmogorov complexity  rl . these new theories received great attention in theoretical computer science and statistics. plenty theoretical researches are done. see  for example  fv1  r1  r1  r1  r1  behw  klpv . one the other hand  these recent theoretical results are not yet applied to the real world learning system design with the exception of an elegant paper by quinlan and rivest  qr   also independently by m. wax . our purpose is to try to bring theory and practice together and test some of the theories in real system designs. specifically  we choose to apply the rissancn's mdl principle to design an on-line hand-written character learning system. such a system has been implemented and its performance coincides with theoretical predictions. 
       one of the important aspects of ai research is the machine perception of natural human languages expressed in various ways. enormous amount of effort has been made for recognition of handwritten characters or character strings  su  . recognizing hand-written characters has applications  for example  in signature recognition and chinese character input. in the latter case  there is a reasonable amount practical demands. this is because that there arc several thousand independent chinese characters; no current key-board input method is nearly natural enough for casual users; some requires the user to memorize a code for each of seven thousand characters; some requires the user to know ping ying 
  this research was supported in part by onr grant n1-k-1  aro grant daal1-k-1 at harvard university  and by ihc nserc operating grant ogp1 at york university. and you still do not get what you type because there are too many homonyms; the sound recognition technique cannot help either since practically almost every commonly used chinese character has more than one commonly used homonyms. for non-professional casual users  who do not want to spend time to actually learn  the hand-written input seems to be a quite reasonable choice. 
       a variety of approaches and algorithms have been used in order to achieve high recognition rate. the recognition process is usually divided into two steps: 1  feature extraction from the sample characters  and 1  classification of unknown characters. the latter often uses either deterministic or statistical inference based on the sample data and various different theories can be applied. however  for feature extraction  whose purpose is to capture the essence from the raw data  is largely in a state of art. features such as center of gravity  moments  distribution of points  character loci  planar curve transformation  coordinates  slopes and curvatures at certain points along the character curve are the most commonly used ones. the obvious difficulty of recognition task is the variability involved in handwritten characters. not only does the shape of the characters depend on the writing style which varies from person to person  even for the same person trying to write consistently  the difference of writing is noticeable from time to time. therefore statistical and approximational approaches are usually used in order to deal with the variation. the clastic match is one of the technique successfully applied in this area  kurtzburg  1 and tapper  1 . briefly speaking  the clastic matching method takes the coordinates or slopes of certain points approximately equally spaced along the curve of the character drawing as feature to establish the character prototypes. to classify an unknown character drawing  the machine compares the drawing with all the prototypes in its knowledge base and the closest prototype is said to have the same character value as the unknown. when an unknown character is compared to a prototype  the comparison of the feature is not only made strictly between the corresponding 
gao and li 
point but also between the adjacent points in the prototype. 
       presented with an example character  what features should we take  how many features should we take  too few of them obviously cannot sufficiently describe the character; too many of them would be make the algorithm too sensitive to noise and result worse recognition performance. for example  the above mentioned elastic matching uses certain points along the character curve as features. the interval used to extract these points along the curve is a parameter. how to determine this parameter  practically speaking  we can set these interval to different values and experiment on the given sample data to see what value gives the best performance. however since the experiment is based on one particular set of data  we do not know if this interval value can give similar performance for all possible observations from the same data source. a theory is needed to guide the parameter selection in order to obtain the best description of data for future prediction. 
       rissanen's mdl principle serves this parameter selection purpose naturally. mdlp finds its root in the well known bayesian inference and not so well-known kolmogorov complexity. from the b ayes' rule  a specific hypothesis is inferred if the probability of the hypothesis takes maximum value for a given set of data and given prior probability distribution of a given set of hypotheses. when the bayes' formula is expressed in the negative logarithmic form  the two terms  one is the conditional probability of the data for given hypothesis  and the other is the prior probability of the hypothesis  become the description length of the error given the hypothesis and the description length of the hypothesis respectively. therefore  finding a maximum value of the conditional probability of a given set of hypotheses and data becomes minimizing the combined complexity or description length of the error and the hypothesis for a given set of candidate hypotheses. 
       from the viewpoint of data-compression used to encode a data set  the two description lengths are  in turn  expressed in terms of the coding lengths  i.e.  the coding length of the hypothesis and the coding length of the error which is the part of data failed to be described by the hypothesis. they complement in the following way: if a hypothesis is too simple  it may fail to capture essence of the mechanism generating the data  resulting in bigger error coding lengths. on the other hand  if a hypothesis is too complicated and tends to include everything in the data  it may contain a lot of redundance from the data and become too sensitive to minor irregularities to give accurate predictions of the future data. the mdlp states that among the given set of hypothesis  the one with the minimum combined description lengths of both the hypothesis and the error for given set of data is the best approximation of the mechanism behind data and can be used to predict the future data with best accuracy. 
　　　the objective of this work is to implement a system of handprinted  english and chinese  character recognition based rissanen's mdlp. specifically  mdlp is used in the selection of the interval of feature extraction. the result is tested experimentally to validate the application of the theory. the next section should serve as an elementary and practical introduction of the rissanen's mdlp. then in the following sections we apply this principle to our learning system. 
1. the rissanen's m d l principle and related theories 
scientists formulate their theories in two steps: first a scientist must  based on scientific observations or given data  formulate alternative hypotheses  generally there are an infinity of alternatives   and second he selects one definite hypothesis. histor-
1 machine learning 
ically this was done by many different principles  among the most dominant in statistics  the fisher's maximum likelihood principle  various ways of using bayesian formula  with different prior distributions . among the most dominant in  common sense   the so-called occam's razor principle of choosing the simplest consistent theory. however  no single principle is both theoretically sound and practically satisfiable in all situations. fisher's principle ignores the prior probability distribution  of hypotheses . the application of bayes' rule is hard usually due to unknown prior probability distribution. in order to resolve the problem of prior distributions  solomonoff  s  and then rissanen  r  proposed the following approach  which resulted the mdlp principle. by bayesian rule we have: 
		 1  
where p h id  is called the final  or a posteriori  probability  p h  is called the initial  or a priori  probability  and 
p  d ih  is the conditional probability of seeing d when // is true. the posterior probability p h  d  is obtained by modifying the prior probability p h  according to the bayes' rule  1 . the bayesian approach tells us to use the hypothesis // such that p h  d  is maximized. since p d  can be considered as a normalizing factor  we ignore it in the following discussion. now take the negative logarithm on both sides of the bayesian formula  1   we get 
  1  
since we are only concerned with maximizing the term 
p h  d  or  equivalently  minimizing the term -logp h  d   this is equivalent to minimizing 
		 1  
now to get the minimum description length principle  we only need to explain the two terms in  1  correctly. first notice that both p  d i //  and p  //  are probabilities  so each is less than or equal to 1. hence  logp d ih  and logp h  are positive numbers. 
       we explain logp h  first. p h  is so-called prior probability for hypothesis h to be true. usually this is unknown  we can only have an initial estimate. the major issue here is to reasonably approximate it. in the original solomonoff approach  s   // in general denotes a turing machine. in practice we must avoid such too general approach in order to keep things computable. in different applications  the hypothesis // can mean many different things. for example  if we infer decision trees  // is a decision tree  qr ; in case of learning finite automata  h can be a finite automaton; in case we are interested in learning boolean formulae  then h may be a boolean formula; if we arc fitting a polynomial curve to a set of data  then h may be a polynomial of some degree; in our case  h will be the model for a particular character. each such h can be encoded by a binary string from a prefix-free set  where a set of codes is prefix-free if no code in the set is a prefix of another. solomonoff suggested that we assign // the prior probability 1~k h  where k h  is informally the length of shortest prefix-free description of h  then -logp  //  is precisely the length of a minimum prefix code of the hypothesis //. more precisely k  //  is the so-called self-delimiting kolmogorov complexity of //  we will not get into this subject since it does not affect the reader's ability to understand the main issue. interested readers are referred to  lv  for an introduction and more details and references of kolmogorov complexity. in this case  by kraft's inequality  Σ1 '   1   


1. learning system development 
1. basic assumptions 
when a character is drawn on a planar surface  it can be viewed as a composite planar curve  the shape of the curve is completely determined by the coordinates of the sequence of points along the curve. the order of the sequence is determined from the time of writing the character. obviously  the shape tends to vary from person to person and from time to time  so do the coordinates of the point sequence. a key assumption here is that for a particular person writing consistently  the shape of the curve tends to converge to a mean shape  the mean of coordinates of the point sequence converge respectively to some set of values. the probability distribution for the coordinates is left unknown  but is assumed to be symmetric about the mean values  and the variance is assumed to be the same for all the character drawings. theoretically  we only assume that a fixed probability distribution of one person's hand-writing does not change. see  v . 
1. feature space  feature extraction and prototypes 
a kurta isone digitizer tablet with 1/inch resolution in both horizontal and vertical directions is used as the transducer to send the coordinates of the sequential points of the character curve on the tablet to the microprocessor of a ibm ps/1 model 1 computer. the system is implemented using programming language c. the coordinates are then standardized against 1 in both horizontal and vertical directions. the sequence of the coordinates becomes a linked list  which goes through preprocess in order to remove the repeated points due to hesitation at the time of writing  and to fill in the gaps between sampled points resulted from the sampling rate limit of the tablet. the latter needs some explanation: the digitizer has a maximum sampling rate of 1 points/second. if a person writes a character in 1 second  only 1 points on the character curve will be sampled  leaving gaps between those points. the preprocess procedure is to ensure that any pair of consecutive points on the curve after preprocessing have at least one component of coordinates  stored as integers  differing by 1 and no coordinate component differing greater than 1. the preprocessed curve coordinate list is then sent to feature extraction. so far the coordinates are still integers in the range of 1 to 1. 
　　　the coordinates of certain points along the character curves are taken as features. feature extraction is done as follows: a character may consist of more than one stroke  a stroke is the trace from a pen drop-down to pen lift-up   the starting and ending points of every stroke are mandatorily taken as features. in between  feature points are taken at a fixed interval  say  one point for every n points along the preprocessed curve  where n is called feature extraction interval. this is to ensure that the feature points are roughly equally spaced. actually the length between any two points  excluding the last point of a stroke  varies from n to v1n  v1n for the diagonal . all the feature point coordinates of a given character drawing constitute a feature vector whose dimension is 1n since every point has two coordinate components. obviously the dimension of the feature vector is also a random variable since the shape and the total number of points on the character curve varies from time to time. the dimension of the feature vector is largely determined by the feature extraction interval. 
　　　note  for chinese characters  other features are also extracted and a decision tree is formed too speedup the process. these will be ignored at present abstract. 
　　　the extracted feature vector of a character can also be viewed as a prototype of the character and saved as the knowledge of the system. 
1. comparison between feature vectors 
before the system is employed to recognize characters  it has to be trained with the character drawings from the same data source which it is supposed to work with. here the  same data source  means the same person writing consistently. the basic technique used in both training and recognition is the comparison or matching between prototypes or feature vectors. to compare any two prototypes or feature vectors  we simply take the absolute distance between the two vectors. mathematically this means subtracting each component of one vector from its corresponding component in the other feature vector  summing up the square of the differences and taking the square root of the sum. i.e. 

gao and li 

the comparison technique used here follows the spirit of this mathematical definition but is elastic. the elasticity is reflected in two aspects: first  the comparison is allowed for feature vector with different dimensions in the range of td  the dimension tolerance. the dimension tolerance is defined such that if the one character* s feature vector has n feature points  it will be compared to all the prototypes with feature vectors with number of feature points in the range of  n-tdn+td . second  the local extensibility ne is allowed so that the i th feature point of one character feature vector is compared with the feature points of the other vector with index ranging from i-ne to i+ne  the smallest difference is considered to be the  true  difference between the two vectors at ith feature point. the sum of the square of these  true  difference is considered to be the absolute difference between the two feature vector. this comparison technique is often referred as clastic matching. for our particular problem  both td and ne are set to 1 based on the experience. 
1. knowledge base and learning 
the knowledge base of the system is a collection of prototypes saved in the form of a linked list during the learning process. the establishment of the knowledge base follows the following rules: 
1  when the system is called to accept a new character draw-ing with known character value  the system checks in its existing knowledge base to see if it has any prototypes existing for that character. if there is no existing prototype for a specific character  the newly arrived feature vector for that character will be saved as the first prototype for that character. 
1  if there is at least one prototype for the character existing in the knowledge base  the newly arrived character feature vector will be compared selectively with the prototypes in the knowledge base whose number of feature point differs at most by td from that of the new character feature. the minimum absolute distance may or may not be one of the prototypes with the same character value. the new character prototype is then handled as the following: 
i  if the minimum distance  defined as dmin  is between the new character and one of the prototypes in the knowledge base with the same character value  the new prototype will be combined with the existing prototype by taking the weighted average of every the coordinate components to produce a modified prototype for that character. 
ii  if the minimum distance is between the new character and one of the prototype in the knowledge base with different character value  but the distance between the new character and its closest prototype of the same character value in the knowledge base  defined as dminc  is not greater than d min m +1  /m  where m is number of character drawings combined in the way described above to produce the closest prototype of the same character value in the knowledge base  the new character prototype will still be combined with the closest prototype of the same character value to provide a modified prototype. it is expected that the modified prototype will be able to assume dmin next time when a similar drawing of the same character value is arrived. 
iii  otherwise the new prototype will be saved in the knowledge base as a new prototype for the character. therefore more than one prototype for a single character may exist in the knowledge base. 
1 	machine learning 
1. recognition of an unknown character drawing 
when an unknown character arrives at the system  the system will compare it to all the prototypes in the knowledge base with dimension variation within the range specified by the dimension tolerance. the character of the prototype which has minimum absolute distance from the unknown is considered to be the character value of the unknown  since the prototypes are the  mean  values of the feature vectors of the characters and the variances of the distribution are assumed to be the same. 
this concludes the main procedure of training and classification. a few remarks should be made here: 
a. this process differs from the original elastic matching method in the the way of prototype construction. more than one prototypes are allowed for a single character and a prototype is the statistical mean of a number of positive examples of the character. 
b. every prototype is a feature vector which in turn is a point in the feature space of its dimension. since the classification is based on statistical inference  the rate of correct classification depends not only on how well the prototypes in the knowledge base are constructed  but also the variability of the handwriting of a person. even though more than one prototypes arc allowed for any character in the knowledge base  too many prototypes may result in over-densified feature space and when the absolute distance between two points  two prototypes in the knowledge base  in the feature space is comparable to the variability of writing  the rate of correct classification may be considerably decreased. 
c. the prototypes in the knowledge base constitute the hypothesis for the system. how well the prototypes are constructed will essentially determine the rate of correct classification  therefore the performance of the hypothesis. for the scheme described above  the prototypes is constructed by extracting points at a constant interval. generally speaking  the more points in the prototypes gives more detailed image about the character drawing but may also include some random  noise  component in the hypothesis. application of rissanen's mdlp to guide the selection of  best  feature extraction interval is what is mainly aimed at by this work. 
1. description lengths and minimization: experimental results 
the expression in rissanen's mdlp consists of two terms: the hypothesis and error coding lengths. the coding efficiency for both of this two terms must be comparable  otherwise minimizing the resulted expression of total description length will give either too complicated or too simple hypotheses. 
     although the whole system is implemented and it is able to recognize several thousands of characters now  in order to make our point clear  we will describe our experiments over 1 alphanumerals: 1 ... 1 a z a ... z. 
       for this particular problem  the coding lengths are from the practical programming consideration. a set of 1 character drawings  exactly 1 for each of the 1 alphanumeral characters  were recorded in a raw database. the character drawings were stored in standardized integer coordinate system ranged from 1 to 1 in both x and y direction. these character drawings were then input to the system to establish a knowledge base  which was the collection of prototypes with normalized real coordinates  based on a selected feature extraction interval. after the construction of knowledge base was finished  the system was tested by having it classify the same set of character drawings. the error coding length is the 

sum of the total number of points for all the incorrectly classified character drawings and the hypothesis coding length is the total number of points in all the prototypes in the machine's knowledge base multiplied by 1. the factor of 1 is from the fact that the prototype coordinates arc stored as real numbers which takes twice as much memory  in c  as the character drawing coordinates which is in integer form. one might wonder why the prototype coordinates arc real instead of integer numbers. the reason is to facilitate the elastic matching to give small resolution for comparisons of classification. 
       thus both the hypothesis and error coding lengths are directly related to the feature extraction interval. the smaller this interval  the more complex the hypothesis  but the smaller the error coding length. the effect is reversed if the feature extraction interval goes toward larger values. since the total coding length is the sum of the two coding lengths  there should be a value of feature extraction interval which rends the total coding length a minimum. this feature extraction interval is considered to be the  best  one in the spirit of mdlp and the corresponding model  the knowledge base  is considered to be optimal in the sense that it contains enough essence from the raw data but eliminates most redundance of noise component from the raw data. this optimal feature extraction interval can be found by carrying out the above described build-and-test  buiding the knowledge base then test it based on the same set of characters on which it was built.  for a number of different extraction interval. 
　　　the actual optimization process is implemented on the system and is available whenever the user wants to call. for our particular set of characters  the results of optimization is given in figure 1  which depicts three quantities: the hypothesis  the error and the total coding lengths versus feature extraction interval  sampling interval int the figure . for larger feature extraction interval  the hypothesis complexity is small but most of the character drawings are misclassified  giving the very large total coding length. on the other hand  when the feature extraction interval is at its small extremity  all the training characters gets correctly classified  thus the error coding length is zero. however the hypothesis complexity reaches its largest value  resulting in a larger total coding length also. the minimum coding length occurred at cxtracuon interval of 1  which gives 1 percent of correct classification. figure 1 illustrates the fraction of correctly classified character drawings for the training data. 
1. validation of the hypothesis 
whether the resulted  optimal  hypothesis really performs better than the hypotheses in the same class  the knowledge bases established using different feature extraction intervals  is subject to test by new data of character drawings. for this testing purpose  three sets of 1 characters were drawn by the same person who provided the raw data base to build the knowledge base. thus the new data is considered to be from the same source as the previous data set. this new data set is classified by the system using the knowledge bases built from the former data set of 1 character drawings  based on different feature extraction intervals. the testing result is plotted in figure 1 in terms of the fraction of correct classification  correct ratio  versus feature extraction interval. it is interesting to see that 1% correct classification occurred at feature extraction intervals 1  1 and 1. these values of feature extraction intervals are close to the optimized value 1. furthermore  at the lower end of feature extraction interval  the correct classification drops down  indicating the disturbance of too much redundance in the hypothesis. the recommended working feature cxtracuon interval is thus 1 for this particular type of character drawings. 
1. summary  related results  and future research 
rissanen's minimum description length principle is applied to handprinted character recognition using elastic matching and statistical technique. the hypothesis is a collection of prototypes built from raw character drawings by taking points on the curves of character drawing at a constant feature extraction interval and by combining closely related character drawings. the hypothesis is optimized in the spirit of mdl principle by minimizing the total coding length which is the sum of the hypothesis and error coding lengths against feature extraction interval. the resulted hypothesis is tested using a different set of character drawing from the same source. the result of test indicates that mdlp is a good tool in the area of handprinted character recogniuon. 
　　　the following related work were brought to the authors' attention: stanfill  aaai-1   bradshaw  mlw-1   aha and kibler  mlw-1   and gennari  langley and fisher  glf . we plan to discuss these related work in the final version. currently our experiment is obvious very preliminary. we plan to perform more experiments with more data and with different methods  for exmaple with different interval lengths as suggested by pal langley. 
acknowledgement 
we are grateful to les valiant for many discussions on machine learning and the suggestion of this research. we are also grateful to the vey helpful suggestions of pat langley and several referees whose suggestions will be implemented in the final version of this paper.  we also thank paul vitanyi and mati wax for useful discussions on mdl principle. 
