
markov decision processes are a powerful framework for planning under uncertainty  but current algorithms have difficulties scaling to large problems. we present a novel probabilistic planner based on the notion of hybridizing two algorithms. in particular  we hybridize gpt  an exact mdp solver  with mbp  a planner that plans using a qualitative  nondeterministic  model of uncertainty. whereas exact mdp solvers produce optimal solutions  qualitative planners sacrifice optimality to achieve speed and high scalability. our hybridized planner  hybplan  is able to obtain the best of both techniques
- speed  quality and scalability. moreover  hybplan has excellent anytime properties and makes effective use of available time and memory.
1 introduction
many real-world domains involve uncertain actions whose execution may stochastically lead to different outcomes. such problems are frequently modeled as an indefinite horizon markov decision process  mdp  also known as stochastic shortest path problem  bertsekas  1 . while mdps are a very general framework  popular optimal algorithms  e.g.  lao*  hansen and zilberstein  1   labeled rtdp  bonet and geffner  1   do not scale to large problems.
¡¡many researchers have argued for planning with a qualitative  non-deterministic model of uncertainty  in contrast to numeric probabilities . such contingent planners  e.g.  mbp  bertoli et al.  1   cannot make use of quantitative likelihood  and cost  information. because they solve a much simpler problem  these planners are able to scale to much larger problems than probabilistic planners. by stripping an mdp of probabilities and costs  one could use a qualitative contingent planner to quickly generate a policy  but the quality of the resulting solution will likely be poor. thus it is natural to ask  can we develop an algorithm with the benefits of both frameworks  .
¡¡using two  or more  algorithms to obtain the benefits of both is a generic idea that forms the basis of many proposed algorithms  e.g.  bound and bound  martello and toth  1  and algorithm portfolios  gomes and selman  1 . various schemes hybridize multiple algorithms differently and are aimed at different objectives. for example  algorithm portfolios run multiple algorithms in parallel and thus reduce the total time to obtain a solution; bound and bound uses a solution from one algorithm as a bound for the other algorithm. in this paper we employ a tighter notion of hybridization  where we explicitly incorporate solutions to sub-problems from one algorithm into the partial solution of the other.
¡¡we present a novel algorithm  hybplan  which hybridizes two planners: gpt and mbp. gpt  general planning tool   bonet and geffner  1  is an exact mdp solver using labeled real time dynamic programming  rtdp . mbp  model based planner   bertoli et al.  1   is a nondeterministic planner exploiting binary decision diagrams  bdds . our hybridized planner enjoys benefits of both algorithms  i.e.  scalability and quality. to the best of our knowledge  this is the first attempt to bridge the efficiencyexpressiveness gap between the planners dealing with qualitative vs. probabilistic representations of uncertainty.
¡¡hybplan has excellent anytime properties - it produces a legal solution very fast and then successively improves on this solution1. moreover  hybplan can effectively handle interleaved planning and execution in an online setting  makes use of the available time and memory efficiently  is able to converge within a desired optimality bound  and reduces to optimal planning given an indefinite time and memory. our experiments demonstrate that hybplan is competitive with the state-of-the-art planners  solving problems in the international planning competition that most other planners could not solve.
1 background
following  bonet and geffner  1   we define an indefinite horizon markov decision process as a tuple
:
  s is a finite set of discrete states. we use factored mdps  i.e.  s is compactly represented in terms of a set of state variables.
  a is a finite set of actions. an applicability function  ap : s ¡ú p a   denotes the set of actions that can be applied in a given state  p represents the power set .
  pr : s ¡Á a ¡Á s ¡ú  1  is the transition function. denotes the probability of arriving at state s
after executing action a in state s.
 is the cost model.
  g   s is a set of absorbing goal states  i.e.  the process ends once one of these states is reached.
  s1 is a start state.
¡¡we assume full observability  and we seek to find an optimal stationary policy  i.e.  a function ¦Ð: s ¡ú a  which minimizes the expected cost  over an indefinite horizon  incurred to reach a goal state. a policy ¦Ð and its execution starting from the start state induces an execution structure exec ¦Ð : a directed graph whose nodes are states from s and transitions are labeled with the actions performed due to ¦Ð. we denote this graph to be free of absorbing cycles if for every non-goal node there always exists a path to reach a goal  i.e.  the probability to reach a goal is always greater than zero . a policy free of absorbing cycles is also known as a proper policy.
¡¡note that any cost function   mapping states to the expected cost of reaching a goal state defines a greedy policy as follows:
¦Ðj s  = argmin
a¡Êap s 
¡¡the optimal policy derives from a value function  j   which satisfies the following pair of bellman equations.
j  s  = 1  if s ¡Ê g else
1 labeled rtdp
value iteration is a canonical algorithm to solve an mdp. in this dynamic programming approach the optimal value function  the solution to equations 1  is calculated as the limit of a series of approximations  each considering increasingly long action sequences. if jn s  is the value of state s in iteration n  then the value of state s in the next iteration is calculated with a process called a bellman backup as follows:

	value iteration terminates when  s	¡Ê	s  |jn s   
  and this termination is guaranteed for
furthermore  the sequence of {ji} is guaranteed to converge to the optimal value function  j   regardless of the initial values. unfortunately  value iteration tends to be quite slow  since it explicitly updates every state  and |s| is exponential in the number of domain features. one optimization restricts search to the part of state space reachable from the initial state s1. two algorithms exploiting this reachability analysis are lao*  hansen and zilberstein  1  and our focus: rtdp  barto et al.  1 .
¡¡rtdp  conceptually  is a lazy version of value iteration in which the states get updated in proportion to the frequency with which they are visited by the repeated executions of the greedy policy. specifically  rtdp simulates the greedy policy along a single trace execution  and updates the values of the states it visits using bellman backups. an rtdp trial is a path starting from s1 and ending when a goal is reached or the number of updates exceeds a threshold. rtdp repeats these trials until convergence. note that common states are updated frequently  while rtdp wastes no time on states that are unreachable  given the current policy. the complete convergence  at every state  is slow because less likely  but potentially important  states get updated infrequently. furthermore  rtdp is not guaranteed to terminate.
¡¡labeled rtdp fixes these problems with a clever labeling scheme that focuses attention on states where the value function has not yet converged  bonet and geffner  1 . specifically  states are gradually labeled solved signifying that the value function has converged for them. the algorithm backpropagates this information starting from the goal states and the algorithm terminates when the start state gets labeled. labeled rtdp is guaranteed to terminate. it is guaranteed to converge to the optimal value function  for states reachable using the optimal policy   if the initial value function is admissible. this algorithm is implemented in gpt  general purpose tool   bonet and geffner  1 .
¡¡ labeled  rtdp is popular for quickly producing a relatively good policy for discounted reward-maximization problems  since the range of infinite horizon total rewards is finite. however  problems in the planning competition are undiscounted cost minimization problems with absorbing goals. intermediate rtdp policies on these problems often contain absorbing cycles. this implies that the expected cost to reach the goal is infinite! clearly  rtdp is not an anytime algorithm for our problems because ensuring that all trajectories reach a goal takes a long time.
1 the model-based planner
an mdp without cost and probability information translates into a planning problem with qualitative uncertainty. a strong-cyclic solution to this problem - one that admits loops but is free of absorbing cycles - can be used as a legal solution to the original mdp  though it may be highly sub-optimal. however  because we are solving a much easier problem  the algorithms for solving this relaxed problem are highly scalable. one such algorithm is implemented within
mbp.
¡¡the model-based planner  mbp  bertoli et al.  1   relies on effective bdd-based representation techniques to implement a set of sound and complete plan search and verification algorithms. all the algorithms within mbp are designed to deal with qualitatively non-deterministic planning domains  defined as moore machines using mbp's input language smv. the planner is very general and is capable of accommodating domains with various state observability  e.g.  fully observable  conformant  and different kinds of planning goals  e.g.  reachability goals  temporal goals .
   mbp has two variants of strong-cyclic algorithm denoted by  global  and  local . both share the representation of the policy ¦Ð as a binary decision diagram  and the fact that it is constructed by backward chaining from the goal. the general idea is to iteratively regress from a set of solution states  the current policy ¦Ð   first admitting any kind of loop introduced by the backward step  and then removing those  bad  loops for which no chance of goal achievement exists. on top of this  the  local  variant of the algorithm prioritizes solutions with no loops  in order to retrieve strong solutions whenever possible. the search ends either when ¦Ð covers the initial state  or when a fixed point is reached. further details can be found in  cimatti et al.  1 .
1 hybplan: a hybridized planner
our novel planner  hybplan  hybridizes gpt and mbp. on the one hand  gpt produces cost-optimal solutions to the original mdp; on the other  mbp ignores probability and cost information  but produces a solution extremely quickly. hybplan combines the two to produce high-quality solutions in intermediate running times.
¡¡at a high level  hybplan invokes gpt with a maximum amount of time  say hybtime. gpt preempts itself after running for this much time and passes the control back to hybplan. at this point gpt has performed several rtdp trials and might have labeled some states solved. however  the whole cost function jn has not converged and the start state is not yet solved. despite this  the current greedy partial1policy  as given by equation 1  contains much useful information. hybplan combines this partial policy  ¦Ðgpt  with the policy from mbp  ¦Ðmbp  to construct a hybridized policy  ¦Ðhyb  that is defined for all states reachable following ¦Ðhyb  and is guaranteed to lead to the goal. we may then evaluate ¦Ðhyb by computing jhyb s1  denoting the expected cost to reach a goal following this policy. in case we are dissatisfied with ¦Ðhyb  we may run some more rtdp trials and repeat the process. we describe the pseudo-code for the planner in algorithm 1.
¡¡the construction of the hybridized policy starts from the start state and uses the popular combination of open and closed lists denoting states for which an action needs to be assigned and have been assigned  respectively. additionally we maintain a deadends list that memoizes all states starting from which we cannot reach a goal  dead-end .
deciding between gpt and mbp  lines 1 to 1 : for every state s  hybplan decides whether to assign the action using ¦Ðgpt or ¦Ðmbp. if s is already labeled solved then we are certain that gpt has computed the optimal policy starting from s  lines 1 . otherwise  we need to assess our confidence in ¦Ðgpt s . we estimate our confidence on gpt's greedy policy by keeping a count on the number of times s has been updated inside labeled rtdp. intuitively  smaller number of visits to a state s corresponds to our low confidence on the quality of ¦Ðgpt s  and we may prefer to use ¦Ðmbp s  instead. a userdefined threshold decides on how quickly we start trusting gpt.
mbp returning with failure  function 1 : sometimes mbp
algorithm 1 hybplan hybtime  threshold 1: deadends ¡û  
1: repeat
1:	run gpt for hybtime1: 1:openopen.insert ¡û  ; closeds1 	¡û  1:while open is non-empty do1:remove s from open1:closed.insert s 1:if s is labeled solved inside gpt then1: 1:¦Ðhyb s  ¡û ¦Ðgpt s 
else1:if visit s    threshold then1: 1:¦Ðhyb s  ¡û ¦Ðgpt s 
else1:assignmbpsolution s 1:for all s s.t.closed do1:	if	deadends then1:assignmbpsolution s 1:else1:open.insert 1:remove absorbing cycles from exec ¦Ðhyb 1:evaluate ¦Ðhyb by computing jhyb s1 1:	if ¦Ðhyb is the best policy found so far  cache it
1: until all resources exhaust or desired error bound is achievedfunction 1 assignmbpsolution s  if mbp s  succeeds then
¦Ðhyb s  ¡û ¦Ðmbp s 
for allclosed do open.insert 
else
if s = s1 then
problem is unsolvable; exit  
else
closed.remove s  deadends.insert s 
for all leads directly to s do
assignmbpsolution

may return with a failure implying that no solution exists from the current state. clearly  the choice of the action  in the previous step  is faulty because that step led to this dead-end. the procedure assignmbpsolution recursively looks at the policy assignments at previous levels and debugs ¦Ðhyb by assigning solutions from ¦Ðmbp. additionally we memoize the dead-end states to reduce future computation.
cleaning  evaluating and caching ¦Ðhyb  lines 1 : we can formulate the evaluation of ¦Ðhyb as the following system of linear equations:

¡¡these equations are tricky to solve  because it is still possible that there is an absorbing cycle in exec ¦Ðhyb . if the rank of the coefficient matrix is less than the number of equations then there is an absorbing cycle. we can convert the matrix into row-echelon form by using row transformations and in parallel perform the same transformations on the identity matrix. when we find a row with all zeros the non-zero entries of the same row in the transformed identity matrix reveals the states in the original system that form absorbing cycle s . we pick one of these states  assign the mbp action for it and repeat this computation. note that this system of equations is on a very small fraction of the overall state space  which are in exec ¦Ðhyb    hence this step is not expensive. as we find intermediate hybridized policies  we cache the best  i.e.  the one with minimum jhyb s1   policy found so far  line 1 .
termination  line 1 : we may terminate differently in different situations: given a fixed amount of time  we may stop gpt when the available time is about to expire and follow it with a hybridized policy computation and terminate. given a fixed amount of memory  we may do the same with memory. if we need to terminate within a desired fraction of the optimal  we may repeat hybridized policy computation at regular intervals and when we find a policy whose error bound is within the desired limits  we terminate.
error bound: we develop a simple procedure to bound the error in ¦Ðhyb. since labeled rtdp is always started with an admissible heuristic  gpt's current cost function jn remains a lower bound of the optimal  jn ¡Ü j  . however the hybridized policy is clearly worse than the optimal and hence
jhyb ¡Ý j . thus  bounds the error of ¦Ðhyb.
properties of hybplan: hybplan uses the current greedy policy from gpt  combines it with solutions from mbp for states that are not fully explored in gpt and ensures that the final hybridized policy is proper  i.e.  free of absorbing cycles. thus hybplan has excellent anytime properties  i.e.  once ¦Ðmbp s1  has returned with success  hybplan is capable of improving the quality of the solution as the time available for the algorithm increases. if infinite time and resources are available for the algorithm  then the algorithm reduces to gpt  whereas if the available resources are extremely limited  then it reduces to mbp. in all other cases  the hybridized planner demonstrates intermediate behavior.
1 two views of the hybridized planner
our hybridized planner may be understood in two ways. the first view is mbp-centric. if we run hybplan without any gpt computation then only ¦Ðmbp will be outputted. this solution will be a legal but possibly low quality. hybplan successively improves the quality of this basic solution from mbp by plugging in additional information from gpt.
¡¡an alternative view is gpt-centric. we draw from the intuition that  in gpt  the partial greedy policy  ¦Ðgpt  improves gradually and eventually gets defined for all relevant states accurately. but before convergence  the current greedy policy may not even be defined for many states and may be inaccurate for others  which have not been explored enough. hybplan uses this partial policy as much as reasonable and completes it by adding in solutions from mbp; thus making the final policy consistent and useful. in essence  both views are useful: each algorithm patches the other's weakness.
1 implementation of hybplan
we now address two different efficiency issues in implementing hybplan. first  instead of pre-computing an mbp policy for the whole state space  we do this computation on demand. we modify mbp so that it can efficiently solve the sub-problems without repeating any computation. we modify mbp's  local  strong cyclic planning algorithm in the following ways - 1  we cache the policy table  ¦Ðmbp  produced by previous planning episodes  1  at each planning episode  we analyze the cached result ¦Ðmbp  and if the input state is solved already  search is skipped  1  we perform search by taking ¦Ðmbp as a starting point  rather than the goal .
¡¡second  in our implementation we do not evaluate ¦Ðhyb by solving the system of linear equations. instead  we approximate this by averaging repeated simulations of the policy from the start state. if any simulation exceeds a maximum trajectory length we guess that the hybrid policy has an absorbing cycle and we try to break the cycle by recursively assigning the action from ¦Ðmbp for a state in the cycle. this modification speeds up the overall algorithm. although in theory this takes away the guarantee of reaching the goal with probability 1 since there could be some low probability trajectory not explored by the simulation that may contain an absorbing cycle  in practice this modification is sufficient as even the planning competition relies on policy simulation for evaluating planners. in our experiments  our hybridized policies reach the goal with probability 1.
¡¡we finally remark that while gpt takes as input a planning problem in probabilistic pddl format  mbp's input is a domain in smv format. our translation from pddl to smv is systematic but only semi-automated; we are implementing a fully automated procedure.
1 experiments
we evaluate hybplan on the speed of planning  quality of solutions returned  anytime behavior  and scalability to large problems. we also perform a sensitivity experiment testing the algorithm towards sensitivity to the parameters.
methodology
we compare hybplan with gpt and mbp. in the graphs we plot the expected cost of the cached policy for both hybplan and gpt as a function of time. the first value in the
hybplan curve is that of mbp  since initially for each state s  visit s  = 1  and thus ¦Ðhyb = ¦Ðmbp . we also plot the current jn s1  value from labeled rtdp. as this admissible value increases  the error bound of the solution reduces.
¡¡we run the experiments on three large probabilistic pddl domains. the first two domains are probabilistic variants of the rovers and machineshop domains from the 1 aips planning competition. the third is the elevators domain from the 1 icaps planning competition. the largest problem we attempted was in the elevators domain and had 1 state variables.
¡¡for our experiments we terminate when either labeled rtdp terminates or when the memory goes out of bound. for most experiments  we initialize hybplan with hybtime = 1 sec and threshold = 1. we also perform experiments to analyze the sensitivity to these parameters.

figure 1: anytime properties of hybplan: on one y-axis we show the expected cost of the cached policy and on the other the jn s1  values. jn converges when the two curves meet. we find that hybplan's policy is superior to gpt's greedy policy. the first time when gpt's policy has non-infinite expected cost occurs much later in the algorithm.anytime property
we first evaluate the anytime properties of hybplan for moderate sized planning problems. we show results on two problems  one from the rovers domain and the other from the machineshop domain  figures 1 a   1 b  . these problems had 1 state variables and about 1 actions each. we observe that ¦Ðhyb has a consistently better expected cost than did ¦Ðgpt  and that the difference between the two algorithms is substantial. for example  in figure 1 b  the first time  when ¦Ðgpt has a non-infinite expected cost  i.e.  all simulated paths reach the goal   is after 1 seconds; whereas hybplan always constructs a valid policy.
¡¡figure 1 a  is for a domain without any dead-ends whereas in the domain of figure 1 b  there are some 'bad' actions from which the agent can never recover. while hybplan obtains greater benefits for domains with dead-ends  for all the domains the anytime nature of hybplan is superior to the gpt. also notice the jn s1  values in figure 1. hybplan sometimes takes marginally longer to converge because of overheads of hybrid policy construction. clearly this overhead is insignificant.
¡¡recall that the first expected cost of ¦Ðhyb is the expected cost of following the mbp policy. clearly an mbp policy is not of very high quality and is substantially improved as time progresses. also  the initial policy is computed very quickly.
scaling to large problems
if we desire to run the algorithm until convergence then hyb-
plan is no better than gpt. for large problems  however  running until convergence is not a practical option due to limited resources. for example  in figure 1 we show experiments on a larger problem from the rovers domain where the memory requirements exceed our machine's 1 gb.  typically  the memory fills up after the algorithm explores about 1 states  in such cases hybridization provides even more benefits  table 1 . for many of the large domains gpt is in fact unable to output a single policy with finite expected cost. while one might use mbp directly for such problems  by hybridizing the two algorithms we are able to get consistently higher quality solutions.

figure 1: plot of expected cost on a problem too large for gpt to converge.
¡¡notice the elevator problems in table 1. there are 1 variables in this domain. these problems were the largest test problems in the elevators domain for the planning competition 1 and few planners could solve it. thus hybplan's performance is very encouraging.
sensitivity to parameters
hybplan is controlled by two parameters: hybtime and threshold. we evaluate how sensitive hybplan is to these parameters. we find that increasing hybtime reduces the total algorithm time but the difference is marginal  implying that the overhead of hybrid policy construction is not significant. however  smaller values result in repeated policy construction and this helps in finding a good quality solution early. thus small values of hybtime are overall more effective.
¡¡varying threshold does not affect the overall algorithm time. but it does marginally affect the first time a good solution is observed. increasing threshold implies that an mbp policy is used until a state is sufficiently explored in gpt. for rovers domain this translates to some extra time before a good policy is observed. for machineshop we observe the
problemstime before memory exhaustsgptexpected cost
mbphybplanrover1¡« 1 sec111rover1¡« 1 sec¡Þ11mach1¡« 1 sec111mach1¡« 1 sec¡Þ11elev1¡« 1 sec¡Þ11elev1 sec11	¡«	¡Þ
figure 1: scalability of hybplan: best quality solutions found  before memory exhausts  by gpt  mbp and hybplan for large problems. hybplan outperforms the others by substantial margins.
opposite behavior suggesting that we are better off using mbp policies for less explored regions of the space. while overall the algorithm is only marginally sensitive to this parameter  the differences in two domains lead us to believe that the optimal values are domain-dependent and in general  intermediate values of threshold are preferable.
¡¡finally  we also compare with symbolic lao*  feng and hansen  1  to determine whether our speedup is due primarily to the use of a qualitative uncertainty representation or instead to exploitation of symbolic techniques. we observe that for our large problems symbolic lao* does not converge even after many hours  since backing up the whole add takes a huge amount of time. thus  we conclude that the speedup is due to hybridization with a simpler  qualitative model of uncertainty.
1 discussion and related work
hybridizing planners were introduced recently in the planning community  mausam and weld  1; mcmahan et al.  1 . mausam and weld used hybridization in the context of concurrent probabilistic temporal planning by hybridizing interwoven epoch and aligned epoch planning. mcmahan et al. used a sub-optimal policy to achieve policy guarantees in an mdp framework  an extension of bound and bound to mdps . however  they did not provide any method to obtain this sub-optimal policy. we are the first to present a principled  automated hybridized planner for mdps.
¡¡the strength of our work is in the coupling of a probabilistic and qualitative contingent planner. while there have been several attempts to use classical planners to obtain probabilistic solutions  e.g.  generate  test and debug  younes and simmons  1   ff-replan   they have severe limitations because conversion to classical languages results in decoupling various outcomes of the same action. thus the classical planners have no way to reject an action that has two outcomes - one good and one bad. however  this kind of information is preserved in domains with qualitative uncertainty and combining them with probabilistic planners creates a time-quality balanced algorithm.
¡¡although the computation of a proper policy is a strength of hybplan  it is also a weakness because there exist problems  which we term  improper    which may not contain even a single proper policy. for these improper problems the algorithm  in its present form  will deem the problem unsolvable  because we have specifically chosen gpt and the strong-cyclic planning algorithm of mbp. instead  we could hybridize other algorithms  e.g.  paragraph  little and thiebaux  1  or gpt supplemented with a high but non-infinite cost of reaching a dead-end  with mbp's weak planning algorithms. for better results we could combine mbp's strong-cyclic and weak planning algorithms sequentially - if strong-cyclic planner returns failure then apply weak planner. a planner hybridized in this manner would be able to handle these improper problems comfortably and will also guarantee reaching a goal if it is possible.
¡¡the idea of hybridizing planners is quite general and can be applied to various other settings. we list several planning problems that could make good use of a hybridized planner:
1. partially observable markov decision processes  pomdp : the scalability issues in pomdps are much more pronounced than in mdps due to the exponential blowup of the continuous belief-state representation. we could hybridize gpt or point-based value iteration based methods  pineau et al.  1;
poupart  1  with disjunctive planners like mbp  bbsp  or pond  bertoli et al.  1; rintanen  1; bryce et al.  1  to gain the benefits of both the probabilistic and disjunctive representations.
1. deterministic over-subscription planning: the objective of over-subscription planning problem is to maximize return by achieving as many goals as possible given the resource constraints  smith  1 . the problem may be modeled as a search in the state space extended with the goals already achieved. a heuristic solution to the problem could be to achieve the goals greedily in the order of decreasing returns. we can hybridize the two algorithms to obtain a better than greedy  faster than optimal solution for the problem.
1. probabilistic planning with continuous resources: ahybrid ao* algorithm on an abstract state space solves this problem. here each abstract node consists of the discrete component of the state space and contains the value function for all values of continuous resources  mausam et al.  1 . this hybrid ao* algorithm may be hybridized with an algorithm that solves a simpler problem assuming deterministic resource consumption for each action with the deterministic value being the average of consumption distribution.
1. concurrent mdp: a concurrent mdp is an mdp with afactored action representation and is used to model probabilistic planning problems with concurrency  mausam and weld  1 . a concurrent mdp algorithm and the equivalent single action mdp algorithm could be hybridized together to obtain a fast concurrent mdp solver.
1 conclusions
our work connects research on probabilistic planning with that on qualitative contingent planning - with exciting results. this paper makes the following contributions:   we present a novel probabilistic planning algorithm  hybplan  that combines two popular planners in a principled way. by hybridizing an optimal  but slow  planner  gpt  with a fast  but sub-optimal  planner  mbp  we obtain the best of both worlds.
  we empirically test hybplan on a suite of medium and large problems  finding that it has significantly better anytime properties than rtdp. given limited resources  it is able to solve much larger problems while still computing high quality solutions. furthermore  hybplan is competitive with the best planners in the 1 international planning competition.
  our notion of hybridization is a general one  and we discuss several other planning applications where we believe that the idea will be effective.
acknowledgments
we thank blai bonet for providing the source code of gpt. we thank doug aberdeen  olivier buffet  zhengzhu feng and sungwook yoon for providing code for their software. we also thank paul beame  gaurav chanda  alessandro cimatti  richard korf  eric hansen  subbarao kambhampati  sylvie thiebaux  and h kan younes for very helpful suggestions. pradeep shenoy  ravikiran sarvadevabhatla and the anonymous reviewers gave useful comments on prior drafts. this work was supported by nsf grant iis-1  onr grants n1-1  n1-1 and the wrf / tj cable professorship.
