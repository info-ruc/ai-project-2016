 
in this paper  we present and compare automatically generated titles for machine-translated documents using several different statistics-based methods. a na ve bayesian  a k-nearest neighbour  a tf-idf and an iterative expectation-maximization method for title generation were applied to 1 original english news documents and again to the same documents translated from english into portuguese  french or german and back to english using systran. the 
autosummarization function of microsoft word was used as a base line. results on several metrics show that the statistics-based methods of title generation for machine-translated documents are fairly language independent and title generation is possible at a level approaching the accuracy of titles generated for the original english documents. 
1. 	introduction 
before we discuss generating target language titles for documents in a different source language  let's consider in general the complex task of creating a title for a document: one has to understand what the document is about  one has to know what is characteristic of this document with respect to other documents  one has to know how a good title sounds to catch attention and how to distill the essence of the document into a title of just a few words. to generate a title for a machine-translated document becomes even more challenging because we have to deal with syntactic and semantic translation errors generated by the automatic translation system. 
 
generating text titles for machine translated documents is very worthwhile because it produces a very compact target language representation of the original foreign language document  which will help readers to understand the important information contained in the document quickly  without requiring a translation of the complete document. from the viewpoint of machine learning  studies on how well general title generation methods can be adapted to errorful machine translated documents and which methods perform better than others will be very helpful towards general understanding on how to discover knowledge from errorful or corrupted data and how to apply learned knowledge in 'noisy' environments. 
 
historically  the task of title generation is strongly connected to more traditional document summarization tasks  goldstein et al  1  because title generation can be thought of as extremely short summarization. traditional summarization has emphasized the extractive approach  using selected sentences or paragraphs from a document to provide a summary  strzalkowski et al.  1  salton et al  1  mitra et al  1 . most notably  mckeown et al.  1  have developed systems that extract phrases and recombine elements of phrases into titles.  
 
more recently  some researchers have moved toward  learning approaches  that take advantage of training data  witbrock and mittal  1 . their key idea was to estimate the probability of generating a particular title word given a word in the document. in their approach  they ignore all document words that do not appear in the title. only document words that effectively reappear in the title of a document are counted when they estimate the probability of generating a title word wt given a document word wd as: p wt|wd  where wt = wd. while the witbrock/mittal na ve bayesian approach is not in principle limited to this constraint  our experiments show that it is a very useful restriction. kennedy and hauptmann  1  explored a generative approach with an iterative expectation-maximization algorithm using most of the document vocabulary. jin and hauptmann  1a  extended this research with a comparison of several statistics-based title word selection methods. 
 
in our approach to the title generation problem we will assume the following:  
first  the system will be given a set of target language training data. each datum consists of a document and its corresponding title. after exposure to the training corpus  the system should be able to generate a title for any unseen document. all source language documents will be translated into the target language before any title generation is attempted. 

 
we decompose the title generation problem into two phases:  
  learning and analysis of the training corpus and  
  generating a sequence of words using learned statistics to form the title for a new document.  
 
for learning and analysis of the training corpus  we present five different learning methods for comparison. all the approaches are described in detail in section 1. 
  a na ve bayesian approach with limited vocabulary. this closely mirrors the experiments reported by witbrock and mittal  1 .  
  a na ve bayesian approach with full vocabulary. here  we compute the probability of generating a title word given a document word for all words in the training data  not just those document words that reappear on the titles.  
  a knn  k nearest neighbors  approach  which treats title generation as a special classification problem. we consider the titles in the training corpus as a fixed set of labels  and the task of generating a title for a new document is essentially the same as selecting an appropriate label  i.e. title  from the fixed set of training labels. the task reduces to finding the document in the training corpus  which is most similar to the current document to be titled. standard document similarity vectors can be used. the new document title will be set to the title for the training document most similar to the current document. 
  iterative expectation-maximization approach. this duplicates the experiments reported by kennedy and hauptmann  1 .  
  term frequency and inverse document frequency  tfidf  method  salton and buckley  1 . tfidf is a popular method used by the information retrieval community for measuring the importance of a term related to a document. we use the tfidf score of a word as a measurement of the potential that this document word will be adopted into the title  since important words have higher chance of being used in the title. 
 
for the title-generating phase  we can further decompose the issues involved as follows: 
  choosing appropriate title words. this is done by applying the learned knowledge from one of the six methods examined in this paper. 
  deciding how many title words are appropriate for this document title. in our experiments we simply fixed the length of generated titles to the average expected title length for all comparisons. 
  finding the correct sequence of title words that forms a readable title 'sentence'. this was done by applying a language model of title word trigrams to order the newly generated title word candidates into a linear sequence. finally  as a baseline  we also used the extractive summarization approach implemented as autosummarize in microsoft word that selects the  best  sentence from the document as a title. 
 
the outline of this paper is as follows: this section gave an introduction to the title generation problem. details of our approach and experiments are presented in section 1. the results and their analysis are presented in section 1. section 1 discusses our conclusions drawn from the experiment and suggests possible improvements. 
 
1. title generation experiment across multiple languages.  
 
we will first describe the data used in our experiments  and then explain and justify our evaluation metrics. the six learning approaches will then be described in more detail at the end of this section.  
1 data description 
the experimental dataset comes from a cd of 1 broadcast news transcriptions published by primary source media  1 . there were a total of roughly 1 news documents and corresponding titles in the dataset. the training dataset was formed by randomly picking four documents-title pairs from every five pairs in the original dataset. the size of training corpus was therefore 1 documents and their titles. we fixed the size of the test collection at 1 items from the unused document-title pairs. by separating training data and test data in this way  we ensure strong overlap in topic coverage between the training and test datasets  which gives the learning algorithms a chance to play a bigger role. 
 
since we did not have a large set of documents with titles in multiple parallel languages  we approximated this by creating machine-translated documents from the original 1 training documents with titles as follows: 
each of the 1 test documents was submitted to the 
systran 	machine 	translation 	system 
 http://babelfish.altavista.com  and translated into french. the french translation was again submitted to the systran translation system and translated back into english. this final retranslation resulted in our french machine translation data. the procedure was repeated on the original documents for translation into portuguese and german to obtain two more machine translated sets of identical documents. for all languages  the average vocabulary overlap between the translated documents and the original documents was around 1%. this implies that at least 1% of the english words were translated incorrectly during translation from english to a foreign language and back to english. 
1 evaluation metric 
in this paper  two evaluations are used  one to measure selection quality and another to measure accuracy of the sequential ordering of the title words.  
 
to measure the selection quality of title words  an f1 metric was used  van rjiesbergen  1 . for an automatically generated title tauto  f1 is measured against the correspondent human assigned title thuman as follows: 
1〜precision〜recall
f1 = 	 precision + recall
precision and recall is measured as the number of identical words in tauto and thuman over the number of words in tauto and the number of words in thuman respectively. since we are focusing here on the choice of appropriate title words  f1 is the most appropriate measure for this purpose. obviously the sequential word order of the generated title words is ignored by this metric. however  preliminary tests with human ratings for the automatically generated titles suggest a strong correlation between f1 and human quality judgments.  
 
accuracy of the sequential ordering: to measure how well a generated title compared to the original human generated title in terms of word order  we measured the number of correct title words in the hypothesis titles that were in the same order as in the reference titles using the dynamic alignment algorithm described by  nye  1 . 
 
to make all approaches comparable  except ms word autosummarize and knn   only 1 title words were generated by each method  as 1 was the average number of title words in the training corpus. since autosummarize in microsoft word selects a complete sentence from the test document as the title  the restriction to a title length of exactly 1 words often prevents autosummarize from producing any title sentence. thus  we allow longer titles for autosummarize in microsoft word. the knn method always uses the complete title of the document in the training corpus most similar to the test document as the title for the test document and thus the restriction of six words does not apply to titles generated by knn. since we wanted to emphasize content word accuracy  stop words were removed throughout the training and testing documents and titles. 
1 description of title generation approaches 
as we mentioned in the introduction  we compared five statistics-based title generation methods together with the baseline  extractive  approach. they were: 1. na ve bayesian approach with limited vocabulary  nbl . essentially  this algorithm duplicates the work by witbrock and mittal  1   which tries to capture the correlation between the words in the document and the words in the title. for each title word tw  it counts the occurrences of document word dw  if dw is the same as tw  i.e. dw = 
tw . to generate a title  we merely apply the statistics to the test documents for generating titles and select the top title words tw where p tw | dw  is largest. 
1. na ve bayesian approach with full vocabulary  nbf . the previous approach counts only the cases where the title word and the document word are the same. this restriction is based on the assumption that a document word is only able to generate a title word with same surface string. the constraint can be easily relaxed by counting all the document-wordtitle-word pairs and apply this full statistics on generating titles for the test documents. in all other respects  this approach is the same as the previous one. 
1. k nearest neighbor approach  knn . this algorithm is similar to the knn algorithm applied to topic classification in  yang et al  1 . it treats the titles in the training corpus as a set of fixed labels. for each new document  instead of creating a new title  it tries to find an appropriate  label   which is equivalent to searching the training document set for the closest related document. this training document title is then used for the new document. in our experiment  we use smart  salton  1  to index our training documents and test documents with the weight schema  atc . the similarity between documents is defined as the dot product between document vectors. the training document closest related to the test document is found by computing the similarity between the test document and each training document  k=1 . 
1. iterative expectation-maximization approach  em . this algorithm reproduces the work by kennedy and hauptmann  1   which treats title generation as a translation problem. we view a document as written in a 'verbose' language and its corresponding title as written in a 'concise' language. the approach builds a translation model  brown et al.  1   between verbose and concise languages  based on the documents and titles in the training corpus and applies the learned translation model to generate titles for new documents. the essential difference between this approach and na ve bayesian approaches is that em treats the title word generation probability given a document as the sum of the title word generation probability from all the document words while the na ve bayesian approach treats it as the product of the title word generation probability from all document words. 
1. term frequency and inverse document frequency approach  tf.idf . term frequency tf  i.e. the frequency of words occurring in a document  shows how important a word is inside a document. inverse document frequency idf  i.e. the log of the total number of documents divided by the number of documents containing this word  shows how rarely a term appears in the collection. the product of these 
two factors  i.e. tf.idf  gives the importance of a term related to a document  salton and buckley  1 . the highest-ranking tf.idf document words are chosen for the title word candidates. the inverse document frequency for each term is computed based on how often it appears in the training corpus. 
1. extractive summarization approach  auto . we use the autosummarize function built into microsoft word as a demonstration of an extractive approach  which select the  best  sentence from the document as the title.  
1 	the sequencing process for title word candidates 
to generate an ordered  linear set of candidates  equivalent to what we would expect to read from left to right  we built a statistical trigram language model using the cmu-cambridge spoken language modeling toolkit  clarkson and rosenfeld  1  and the 1 titles in the training set. this language model was used to determine the most likely order of the title word candidates generated by the nbl  nbf  em and tf.idf methods. the knn and auto generated titles were already in natural sequence.  
1 	experimental results and discussion 
to illustrate the quality of the results  we first show an example of machine-generated titles and then present quantitative results and their analysis. 
1 	example 
the following is an excerpt of a document translated from english to portuguese and back to english together with its original  human-assigned english title. the translation shows all the typical characteristics and problems of machine-translated documents. the corresponding machine-generated titles are shown in table 1. 
 

original title: o.j. simpson civil trial - case goes to jury soon document: 
... the civil experimentation de simpson of the j will be soon in the hands of the jury.  the lawyers for both the sides 
give to its additions the advanced following week and then the 
deliberations start.  the jury in the civil experimentation is jane clayson of b c. here. de simpson of the j is is of the cut today when the judge and the lawyers in 
both the sides to work for are of instructions jury. the plaintiffs had finished its case it rebuttal with some harmful testimony of a connoisseur who 
of the photograph oms authenticated 
thirty pictures recently discovered de 
simpson it j that consumes low shoes the same de bruno magli style rare the assassin consumed.  the theories of the contamination of the defense are 
cita  es absolutely ridiculous of citations.  the arguments de f they are programados still to start in tuesday.  
this jury could start the case and start to deliberate in thursday.  ... 
 method 	title 
nbl 	white house simpson civil case jury 
nbf continuing coverage simpson civil trial president 
em continuing coverage simpson civil trial jury 
auto civil experimentation de the arguments de f 
knn 	oj simpson civil trial 
tf.idf simpson jury start de jane additions 
table 1 shows the machine-generated titles for a document translated from english to portuguese and back to english. 
1 	results and discussion 
we compared the machine-generated titles against reference titles using both f1 metrics and number of correct title words in the correct order. figure 1 and 1 show the f1 scores and the average number of correct title words in the correct order for each method over both original documents and translated documents. 
 
performance of learning methods is relatively language independent. according to both the metrics of f1 and the average number of title words in the correct order  the performance for documents translated from english to french and portuguese and back to english is quite similar for all six different methods. performance for documents translated from english to german and back to english is somewhat worse than for french and portuguese. this may be due to the underlying systran translation system  or inherent in the inflectional and noun-compounding characteristics of german which distinguish it from the other languages examined here. 
 
autosummarization in microsoft word performs poorly. in terms of f1 and the average number of title words in the correct word  autosummarization from microsoft word performs much worse than the methods knn  tf.idf  nbl and em over either the original documents or translated documents. the only exception is that autosummarization appears to work fine for the original documents in terms of the average number of title words in the correct order. we believe this is due to the fact that autosummarization is allowed a much longer title length  which increases the chance to catch the right title word and improves the average number of title words in the correct order. this confirms other research  which found that extractive summarization will only work well if there is much redundancy in the data and the summarization is much greater than 1% of the document size  mittal et al 1 . furthermore  the extraction-based approach is unable to take full advantage of the training corpus. 

figure 1 shows the f1 scores for the machine-generated titles. for each method  there are four bars representing the f1 scores for the titles generated from the original documents and the translated documents. the legends en-fr-en  en-pt-en and en-de-en represent the documents translated from english document to french  portuguese and german and back in english  respectively. 
k-nearest neighbor  knn  performs extremely well. for both the original documents and the translated documents  knn performs better than the other methods according to both the metrics of f1 and the average number of title words in the correct order. knn works well here because the training and test sets were constructed to guarantee good overlap in content coverage. even though our second best method  tf.idf  shows performance close to knn for the original documents  it degrades much more than knn on all the three sets of machine-translated documents. there is almost no degradation for knn over the documents translated from english to french and portuguese and back to english. the large degradation for knn over the document translated from english to german and back to english may be attributed to the fact that german is a quite different language from english as was already discussed. actually  jin and hauptmann  1b  have shown that knn is also resilient to corruption from automatic speech recognition in generating titles for speech recognized documents. thus  we conclude that knn is a good approach for automatic title generation because of its simplicity and robustness to corrupt data.  
 
naive bayesian with limit vocabulary  nbl  performs much better than na ve bayesian with full vocabulary  nbf . the difference between nbf and nbl is that nbl assumes a document word can only generate a title word with the same surface string. this very strong assumption discards information about a lot of words. however  the results tell us that some information can be safely ignored. in nbf  nothing distinguishes between important words and trivial words  and the cooccurrence between all document words and title words is measured equally. this lets frequent  but unimportant words dominate the document-word-title-word correlation. as an extreme example  stop words show up frequently in every document. however  they have little effect on choosing title words. thus  even though nbf seems to exploit more knowledge than nbl  it introduces more noise by not limiting the effects of frequent  but unimportant words.  
comparison of # of correct title words  in the correct order 
 1 
1 
knn tfidf nbl nbf em auto
methods original 
documents en-fr-en documents en-pt-en documents en-de-en documents  figure 1 shows the average number of correct title words in the correct order. for each method  there are four bars representing the average number of correct title words in the correct order for the titles generated from the original documents and the translated documents. the legends en-fr-en  en-pt-en and en-de-en represent the documents translated from english document to french  portuguese and german and back into english  respectively. 
tf.idf performs surprisingly well compared to the other true learning approaches. surprisingly  the 'heavy' learning approaches  which take full advantage of the training corpus  such as na ve bayesian with limit vocabulary  nbl   na ve bayesian with full vocabulary  nbf  and expectation-maximization  em  didn't outperform the shallow learning approach tf.idf. even though nbl  nbf and em try to learn the association between title words and document words  they show no advantage over the simple tf.idf approach  which selects the title words from the document based on the tf.idf score  without learning anything about the titles. 
one suspicion is that learning the association between document words and title words by directly inspecting the document and its title is very problematic. many words in a document don't reflect its content. for example  in many documents there are extraneous paragraphs and copyright notices. those word occurrences will blur the statistics and mislead the title word selection. a better strategy may be to first distill the document into essential content words and then compute the association between the distilled documents and their titles. 
1 conclusion 
while title generation is far from a solved problem in one language  in this research we have  for the first time  applied learning approaches to title generation across languages. the research results show that automatic title generation is feasible on foreign language documents  despite gross errors in machine translation. due to the flexibility and human readability issues of titles  the automatic evaluation metrics may not be able to reflect correctly the quality of titles. thus  more work is needed to determine the human readability of the automatically generated titles  as well as the consistency between automatic evaluation metrics and human judgment. a validation with real cross-lingual documents is also desirable. in real life you would want to translate from french documents to english titles. you would train on english documents with english titles  and test on french documents translated into english. 
references 
 goldstein et al  1  j. goldstein  m. kantrowitz  v. mital  and j. carbonell. summarizing  text documents: sentence selection and evaluation metrics  proceedings of sigir 1  berkeley  ca  august 1. 
 strzalkowski et al.  1  t. strzalkowski  j. wang  and b. wise  a robust practical text summarization system  aaai intelligent text summarization workshop  pages 1--1  stanford  ca  march 1. 
 salton et al  1  g. salton  a. singhal  m. mitra  and c. buckley  automatic text structuring and summary  info. proc. and management  1 : 1  march 1. 
 mitra et al  1  m. mitra  a. sighal  and c. buckley  automatic text summarization by paragraph extraction  proceedings of the acl'1/eacl'1 workshop on intelligent scalable text summarization  madrid  spain.  
 mckeown et al  1  k. mckeown  j. robin and k. kukich  generating concise natural language summaries  information processing and management  1  1   pp.1  1. 
 witbrock and mittal  1  m. witbrock and v. mittal  ultra-summarization: a statistical approach to generating highly condensed non-extractive summaries  proceedings of sigir 1  berkeley  ca  august 1 
 kennedy and hauptmann  1  p. kennedy and a.g. hauptmann   automatic title generation for the informedia multimedia digital library  acm digital libraries  dl1  san antonio texas  may 1  in press. 
 salton and buckley  1  g. salton and c. buckley  term-weighting approaches in automatic text retrieval  information processing and management  1  1  1 
 yang and chute  1  y. yang and c.g. chute  an example-based mapping method for text classification and retrieval  acm transactions on information systems  tois   1 : 1. 1. 
 van rjiesbergen  1  van rjiesbergen. butterworths  information retrieval  chapter 1. london  1. 
 salton  1  g. salton  the smart retrival system: experiments in automatic document proceeding  prentice hall  englewood cliffs  new jersey. 1. 
 clarkson and rosenfeld  1  p.r. clarkson and r. 
rosenfeld. statistical language modeling using the cmu-
cambridge toolkit proceedings esca eurospeech 1 
 mittal  et al  1  v. mittal  m. kantrowitz  j. goldstein and j. carbonell  selecting text spans for document summaries: heuristics and metrics  aaai-1  1.  
 broadcast news  1  primary source media  broadcast news cdrom  woodbridge  ct  1 
 nye  1  h. nye  the use of a one stage dynamic programming algorithm for connected word recognition  ieee transactions on acoustics  speech and signal processing  vol. aasp-1  no 1  pp. 1  april 1. 
  jin and hauptmann  1a  r. jin and a.g. hauptmann  cross lingual title generation: initial steps  workshop on 
interactive searching in foreign-language collections  human-computer interaction laboratory  university of maryland  college park  md. june 1  1. http://www.clis.umd.edu/conferences/hcil1  
 jin and hauptmann  1b  r. jin and a.g. hauptmann  
title generation for spoken broadcast news using a training corpus  in proceedings of the 1th international conference on spoken language processing  beijing  p.r.china  1 
 brown et al. 1  p. brown  s. cocke  s. della pietra  
della pietra  f. jelinek  j. lafferty  r. mercer  and roossin  a statistical approach to machine translation  computational linguistics v. 1  no. 1  june 1. 
dealing with dependencies between content planning and surface realisation in a pipeline generation architecture
kalina bontcheva and yorick wilks
department of computer science  university of sheffield  1 portobello st.  sheffield s1dp  uk
kalina yorick  dcs.shef.ac.ukabstract
the majority of existing language generation systems have a pipeline architecture which offers efficient sequential execution of modules  but does not allow decisions about text content to be revised in later stages. however  as exemplified in this paper  in some cases choosing appropriate content can depend on text length and formatting  which in a pipeline architecture are determined after content planning is completed. unlike pipelines  interleaved and revision-based architectures can deal with such dependencies but tend to be more expensive computationally. since our system needs to generate acceptable hypertext explanations reliably and quickly  the pipeline architecture was modified instead to allow additional content to be requested in later stages of the generation process if necessary.
1	introduction
the astonishing explosion of the world wide web is leading to a growing need to personalise user experience in cyberspace  e.g.  myyahoo amazon's bookrecommendations . since research in natural language generation  nlg  has already investigated ways to tailor automatically-generated texts to user goals and characteristics  e.g.   paris  1; zuckerman and mcconachy  1    these methods could be used to generate dynamic hypertext1 which takes into account the interaction context and user preferences and characteristics.
　since users expect real-time interaction  efficient and robust applied nlg techniques are typically used for hypertext generation. for instance  ilex  knott et al.  1  uses a combination of canned stories and templates; exemplars  white  1  is rule-based; and peba  milosavljevic et al.  1  uses text schemas  mckeown  1  and a phrasal lexicon. also for efficiency reasons  dynamic hypertext generation systems have pipeline architectures where modules are executed sequentially and no module later in the architecture can request informationfrom an earlier module. for example  in such an architecture it is not possible to take into account text formatting and length  which are determined towards the end  when choosing the text content  which happens in the beginning .
　the goal of our dynamic hypertext generation system  hylite+  is to produce encyclopaedia-style explanations of domain terminology  see figure 1 below . the corpus analysis of online encyclopaedia  bontcheva  1  and previous empirical studies  e.g.   reinking and schreiner  1   have shown the positive effect of additional information- e.g.  definition of key vocabulary  less-technical content  supply of background information and illustrations - on the subjects' reading comprehension and reading behaviour. on the other hand  hypertext usability studies  nielsen  1  have shown that people read 1% slower on the screen  so hypertextneeds to be concise with formatting  that facilitates skimming. our empirical studies have shown  bontcheva  1  that users prefer different additional information depending on the chosen formatting and desired explanation length.
　this paper discusses several ways to provide additional information about unknown terms in generated encyclopedic entity descriptions. when such information is needed  the most appropriate clarification needs to be chosen depending on formatting  user knowledge and constraints  e.g.  concise versus detailed pages . each alternative requires different text content to be selected at the start of the generation process but the choice of alternative can only happen after the content and formatting for the main description have already been determined. therefore  the original pipeline architecture was extended to allow limited module feedback. in the resulting recursive pipeline architecture additional content can be requested in later stages of the generation process  e.g.  during surface realisation .
　in effect  the content planner first produces the text plan for the concise hypertext which contains only facts about the explained concept  e.g.  personal computer . then during surface realisation  after formatting has been decided  the most suitable adaptivity alternative is chosen. often this leads to the posting of a new communicative goal  which results in expanding the basic text with extra information.
　the paper is structured as follows. section 1 describes briefly hylite+ - the dynamic hypertext generation system in the context of which the recursive pipeline architecture  section 1  was developed. section 1 exemplifies the use of the architecture for generating additional information that clarifies terms unknown to the user. the approach is also put in the context of previous work on interleaved  opportunistic  and revision-based language generation  section 1 . finally the paper concludes with a discussion of some known limitations and future work.
1	hylite+ in a nutshell
hylite+ generates encyclopedic explanations of terms in the chemical and computer domains. the user interacts with the system in an ordinary web browser  e.g.  netscape  internet explorer  by specifying a term she wants to look up. the system generates a hypertext explanation where further information can be obtained by following links or specifying another query. similar to all web applications  see  nielsen  1    hylite+ needs to  i  respond in real-time  i.e.  avoid algorithms with associated high computational cost; and  ii  be robust  i.e.  always produce a response. consequently the system uses some efficient and well-established applied nlg techniques such as text schemas and a phrasal lexicon  see  reiter and dale  1  .
　similar to other applied systems  see  reiter  1    hylite+ was initially implementedas a single-pass pipeline system  i.e.  the generation modules were executed sequentially. the system input specifies the concept to be explained and the system parameters chosen by the user  e.g.  concise versus detailed descriptions . the output is the generated hypertext explanation in html format  which is viewed by the user in a conventional browser  see figure 1 below .
　the system consists of several modules organised in two main stages:  i  content organisation  which includes content selection  text organisation and semantic aggregation; and  ii  surface realisation modules  bontcheva  1 . as shown in figure 1  the high-level modules use a discourse history and an agent model  viewgen   ballim and wilks  1  which contains both the system domain knowledge and user beliefs  stored in nested environments. the surface realisation modules use language-specific resources such as lexicons  morphology  and grammars.
　the text planner uses high-level discourse patterns similar to text schemas  mckeown  1  which have been derived from analysing entries in encyclopediaand terminological dictionaries  bontcheva  1 . for instance  entities  i.e.  concepts inheriting from entity in the hierarchy  are defined by their supertype s  or type definition  characteristics  functions  constituents and examples. if the entity has several synonymous terms  the query one is used throughout the explanation and the rest are given in brackets when the entity is first introduced.
1	adding recursion to the pipeline architecture
one way of improving the user understanding of the gen-
erated hypertext is to clarify important unknown terms that occur in definitions  descriptions of parts and subtypes  for further detail see  bontcheva  1  . however  different types of such information - definitions  familiar superconcepts  separate paragraphs  or just links - are preferred at different times  mainly depending on page length  formatting  and user preference for concise versus detailed explanations. in a sequentially organised pipeline architecture some of this information can be generated only if the necessary additional facts have already been extracted during content selection and incorporated in the text plan.
　for instance  the decision whether it is appropriate to use a parenthetical definition or a paragraph-length description depends on their length  the chosen hypertext formatting  and the target length for the generated page1. however formatting and length information are only available during surface realisation  not content planning  so hylite+ either has to extract in advance clarifying information for each unknown concept  or the architecture needs to be modified to allow such information to be requested later.
　selecting all potentially useful clarifying information in advance is likely to be computationally expensive as the text size grows. even if such pre-selection is computationallyfeasible  the content planner still cannot commit to a particular strategy  e.g.  a parenthetical definition or a familiar superconcept  because it needs to know  among other things  the length of the definition text itself1.
　one way to resolve this problem is to implement a text revision mechanism which would analyse the content and structure of the generated hypertext and choose between alternative ways of including clarificatory information. however  despite the gains in fluency and coherence  revision-based approaches tend to suffer from computational problems due to the large number of alternatives that need to be explored  e.g.   robin and mckeown  1  .
　also  since hylite+ was built on the basis of an existing pipeline-based generation system  our goal was to find a solution that would both provide the needed module feedback functionality and work in combination with the existing sequentially-designed modules. in other words  we needed a modification of the pipeline  that will allow the surface realiser to invoke the generator with new goals  so additional information is extracted and included only when appropriate. consequently  the existing pipeline architecture was modified to allow later stages to request additional information to be included  see figure 1 . a special monitoring module was added to detect opportunities for adapting the generated hypertext. the monitor uses the complete text plan  received from the content planning stage  together with the propositions realised to this point to estimate the length of the main explanation. based on this information  user preferences  and the chosen document formatting  the monitor decides whether to post additional high-priority goals on the generator's agenda  e.g.  define microprocessor : a goal to define a concept  example microprocessor : give an example .
　when such goals are posted to the generator  surface realisation is temporarily suspended while the text for the new

figure 1: the hylite+ recursive pipeline architecture
goal is generated. this effectively opens a new discourse segment  grosz and sidner  1   which is dominated by the main explanation. all previously mentioned entities become part of the new focus space  so the generator can refer to them if necessary. the monitor also specifies some system parameters which influence the form of the newly generated text. for example  term definitions in brackets need to be noun phrases  instead of full sentences  see example in figure 1   so the system configuration is changed from the default preference for sentences. when the text corresponding to the new goal is generated  the monitoring module evaluates the result and  if suitable  integrates it with the main text. finally  text realisation is resumed until a new interruption occurs or the entire text plan has been verbalised.
　if the newly generated text is found to be unsuitable  e.g.  the definition is too long and will disturb the flow of the main text   the monitoring module tries another adaptivity technique if such is available  e.g.  provide a known superconcept instead . otherwise it leaves the text content as originally planned and the unknown concept is realised with its corresponding term with a hypertext link to a separate page.
　in effect  the content planner first produces the text plan for the concise  unadapted hypertext which contains only facts about the explained concept  e.g.  personal computer . then during surface realisation  when formatting has been decided  the most suitable adaptivity alternative is chosen. in some cases this leads to the posting of a new communicativegoal to the generator  which results in expanding the basic text with extra information.
　below we will show how the recursive pipeline is used for providing clarifying information about subtypes  parts  and unknown concepts in term definitions. the present experience shows that the efficiency of the pipeline architecture and the schemas is retained while the generator's scope and flexibility is improved.
1	clarifying unknown terms
the content planner uses information from the user model to detect and annotateunknownconcepts  bontchevaand wilks  1 . during surface realisation hypertext links  leading to separate explanation pages  are added for these concepts. in addition  some clarifying information is considered for unknown concepts in definitions  parts and subtypes.
　the system mockup experiments  bontcheva  1  showedthat users prefertwo types of concise parentheticalinformation: brief term definitions and a familiar superconcept. for longer texts  paragraph-long clarifying information can be provided in a separate section  containing more detail than just the definition/superconcept. here we will only provide examples of using the recursive pipeline architecture to generate additional term definitions and paragraph-longexplanations. further details and a thorough discussion of the other adaptivity techniques are available in  bontcheva  1 .
1	generating the definitions
let us assume a user who has looked up computer programs and then followed a link to personal computer; the user has not specified preferences for types of clarifying information  so definitions can be provided where appropriate. following this request  the content planner passes the following facts for realisation  the fact listing all parts is truncated here1 :
 pc -  isa 	-  microcomp fs um state:unknown  . pc -  part of 	-  cpu fs um state:unknown  
-  part of 	-  memory fs um state:unknown  
-  part of 	-  hdd fs um state:unknown   -  part of 	-  display ...　first the realiser determines the document formatting; in this case a bullet list is chosen to enumerate all parts. then it starts generating text for the first graph. because the introduced supertype is unknown  but important for the understanding of the text  the realisation monitor decides to provide some extra material about it. the action corresponding to generating a new definition is define micro comp   which is passed as a parameter to the recursively called generator. the generation parameters are also set to very short texts with syntactic preference for noun phrases. the resulting text -  a small computer that uses a microprocessor as its central processing unit  - is evaluated for length and provided in brackets  see figures 1 and 1 .
　similarly for all unknown parts of the pc  the realisation monitor calls the generator recursively and obtains their definitions. since the parts are described in a bullet list  it is more appropriate to provide their definitions with dashes  instead of brackets  see figure 1 .

figure 1: an example of clarifying information added during surface realisation

figure 1: example of clarifying definitions integrated in the main text　the realisation monitor has a set of rules which examine the type of proposition  e.g.  definition  part of  attribute  and the formattingof the main text  in order to determine the best formatting for the new material. at present  new material which is to be integrated in unformatted text is put in brackets. since such parentheses disturb the flow of the main text  they are only provided once per sentence  usually for the most important concept in the proposition  e.g.  the supertype . other important unknown concepts in the same sentence are supplemented with a familiar supertype.
　when a concept has been defined in brackets  the generated hypertext link also has a tag saying further information to indicate that more informationis available and can be reached by following the link  see figure 1 . if the knowledge base contains only the definition and no other information  then no link is providedbecause all relevant material has already been included in the current page.
1	generating paragraph-length clarifying descriptions
so far we have discussed the generation of concise hypertext  where the main describe entity  goal is realised in a schema-based  sequential fashion and brief additional definitions of unknown terms are included by posting additional high-priority goals. however  our empirical studies showed that in some cases users can prefer longer texts.
the corpus study showed that long encyclopedic articles often provide detailed descriptions of subtypes and/or object parts. therefore  one way to adapt longer generated hypertext is to provide paragraph-length descriptions of the unknown subtypes/parts  instead of including just their definitions or familiar supertypes. the experiments showed that such pages are preferred when users need more detailed information on the topic  e.g.  for a school essay ; in this case  the additional material is best organised in a separate section.
　hylite+generates such longeradditionalmaterial in separate sections based on new communicative goals posted on its agenda by the monitor  see figure 1 . for example  if the main explanation contains propositions about several unknown object parts  e.g.  computer parts  and more detailed texts are preferred by the user  a new describe all part of  computer  goal is posted on the agenda. in this case  the goal is not marked as a highpriority one  so surface realisation is not interrupted. after the main explanation is completed  the generator fetches the new goal and starts a new section in the document describing all computer parts.
　the describe all rel  x  goal is decomposed in the following way:
forall c where rel c  x  describe c 
i.e.  describe all concepts c for which the relation rel holds with x  e.g. describe all parts of the computer. apart from the new goal  the generator is also passed parameters that specify to generate the explanations as paragraphs  not whole pages.
　in this way the generated document is regarded as an ordered set of goals and the text for each one of them is generated separately. this separation approach has the benefit of breaking down the text organisation problem into smaller parts which  i  can be more efficient to compute  but also  ii  can use different text organisation approaches for the different parts  e.g.  schemas for the more 'rigid'parts and planning otherwise.
1	related work
the way in which the recursive pipeline architecture handles the dependencies between content planning and surface realisation bears some similarity with interleaved languagegeneration. for example   rubinoff  1  describes an approach where the planner obtains feedback from the linguistic component in the form of a set of possible ways of expressing each plan element; the most suitable one is then added to the utterance. some interleaved systems  e.g.   finkler and neumann  1; de smedt  1   are also incremental  i.e.  their planner and realiser operate in parallel on different segments of an utterance. the most similar interleaved system is pauline  hovy  1  where the planner produces only a partial text plan and makes the remaining commitments only when the realisation component needs them. in this way unexpected syntactic opportunities can be exploited and longterm goals  e.g.  conciseness  politeness  taken into account during planning as well as realisation.
　the main difference between the recursive pipeline approach and interleaved systems is that the latter tend to use feedback for every choice that depends on information from another module. in hylite+ the main text is always produced in a sequential fashion; interleaved planning and realisation only occur if the user preferences allow the use of adaptivity techniques that lead to new goals. although potentially less flexible  the recursive approach enables the use of efficient techniques  schemas  sequential execution  to compute the main text reliably and quickly.
　another approach to the content adaptivity problem was explored in the ilex system  mellish et al.  1   which uses opportunistic planning to tailor the hypertext descriptions of museum exhibits. the planning mechanism is based on a structure called text potential - a graph of facts connected with thematic and rhetorical relations. facts are chosen depending on their connection to the focus of the page  the described entity   interest and importance weights. this approach  however  seems difficult to apply to generated encyclopaedic-style explanations because it requires the creation of all links and weights in the text potential. assigning values for importanceand interest are particularlydifficult because they both depend on the user goal  which is unknown to the system  and can change from one session to another  e.g.  in-depth reading for a school essay versus looking up an unknown term .
　the pipeline-based stop system  reiter  1  operates under very strict text size constraints which proved difficult to estimate at the content planning stage. in order to improve the generation results  reiter experimented with a multiple-solution pipeline1  msp  and revision-based variants of the system. the results showed that multiple-solution pipelines  for 1 solutions  and revision-based nlg are best suited for the task. the processing time per document is naturally higher - linearly increasing with the number of generated alternatives for the multiple-solution approach and the revision-based one is even slower than the msp. the recursive pipeline architecture discussed here will not be suitable for such tightly constrained texts because the length of the yet unrealised propositions can only be approximated  which might lead to text overflow.
1	conclusion and future work
this paper presented the recursive pipeline architecture developed as part of the dynamic hypertext generation system hylite+. the approach allows additional content to be requested in later stages of the generation process and integrated in the main explanation. the architecture also allows new goals to be executed after the main body of the explanation has been generated.
　although interleaved and revision-based approaches offer more flexibility  the advantages of the recursive pipeline is that it can be added to an already implemented sequential system with relatively minor modifications to the code. in addition  with our approach the main text is always computed in a sequential fashion  based on efficient applied nlg techniques  e.g.  schemas  phrasal lexicon . interleaved planning and realisation only occur if the user preferences allow the use of adaptivity techniques that lead to new goals.
　in the case when the additional text is generated during surface realisation  the monitordeterminesthe overall text length based on the length of the already realised facts plus an estimated length for all unrealised ones. therefore the result is only an approximation of the final length  which is sufficient when  as in hypertext  size constraints are important but not very strict.
　the results from a small-scale formative evaluation have confirmed that the majority of users prefer the adapted texts to the neutral version where no additional information is provided for the unknown terms. more exhaustive subject-based evaluation is currently being undertaken and the goal is to gain further insight into the users' acceptance of hypertext adaptivity.
　performance-based evaluation of the recursive pipeline versus the baseline system  a version that does not provide additional information  just generates the main explanation  revealed that the new goals only add between 1% and 1% to the overall execution time - e.g.  1% for additional information which is less than 1% of the baseline explanation; 1% for additional information which is 1% of the baseline one. with both systems the overall processing time for a half page of hypertext is less than 1 seconds - a response time which allows users to stay focused on the interaction without any need for extra feedback  nielsen  1 .
　in the future we plan to experiment with using different content organisation strategies to generate different parts of the text. for example  following the ideas of  paris  1  different kinds of knowledge can be provided depending on the user's level of expertise. also  in some cases schemas can be replaced with text planning based on rhetorical relations  e.g.   moore  1;power  1 . in this way the more 'rigid' parts of the text can be generated efficiently with schemas  while more powerful but computationally expensive planning techniques are used only when necessary. acknowledgements
　we wish to thank hamish cunninghamand the anonymous reviewers for their helpful comments and suggestions.
references
 ballim and wilks  1  a. ballim and y. wilks. artificial believers. lawrence erlbaum associates  hillsdale  new jersey  1.
 bontcheva and wilks  1  kalina bontcheva and yorick wilks. generation of adaptive  hyper text explanations with an agent model. in proceedings of the european workshop on natural languagegeneration  ewnlg'1   toulouse  france  may 1.
 bontcheva  1  kalina bontcheva. generating adaptive hypertext explanations with a nested agent model. phd thesis  university of sheffield  1. forthcoming.
 de smedt  1  koenraad de smedt. ipf: an incremental parallel formulator. in robert dale  chris mellish  and michael zock  editors  current research in natural language generation  pages 1. academic press  new york  1.
 finkler and neumann  1  wolfgang finkler and gu：nter neumann. popel-how: a distributed parallel model for incremental natural language productionwith feedback. in proceedings of the 1th international joint conference on artificial intelligence  ijcai-1   volume 1  pages 1- 1  detroit  mi  august 1  1.
 grosz and sidner  1  barbara j. grosz and candace l. sidner. attention  intentions and the structureof discourse. computational linguistics journal  1 :1  1.
 hovy  1  eduard h. hovy. generating natural language under pragmatic constraints. lawrence erlbaum  hillsdale  new jersey  1.
 knott et al.  1  alistair knott  chris mellish  jon oberlander  and mick o'donnell. sources of flexibility in dynamic hypertext generation. in proceedings of the 1th international workshop on natural language generation  inlg'1   1.
 mckeown  1  kathleen r mckeown. discourse strategies for generating natural-language text. in b. l. webber b. grosz  k. s. jones  editor  readings in natural language processing. morgan kaufmann publishers  1.
 mellish et al.  1  chris mellish  mick o'donnell  jon oberlander  and alistair knott. an architecture for opportunistic text generation. in proceedings of the international natural language generation workshop iwnlg'1  1.
 milosavljevic et al.  1  maria milosavljevic  adrian tulloch  and robert dale. text generation in a dynamic hypertext environment. in proc. of 1th australian computer science conference  melbourne  1.
 moore  1  johanna d. moore. participating in explanatory dialogues. mit press  cambridge  ma  1.
 nielsen  1  jakob nielsen. designing web usability: the practice of simplicity. new riders publishing  1.
 paris  1  ce＞cile l. paris. user modelling in text generation. francis pinter publishers  london  1.
 power  1  richard power. planning by constraint satisfaction. in proceedings of coling'1  1.
 reinking and schreiner  1  david reinking and robert schreiner. the effects of computer-mediated text on measures of reading comprehension and reading behaviour. reading research quarterly  fall:1  1.
 reiter and dale  1  ehud reiter and robert dale. building natural language generation systems. cambridge university press  cambridge  u.k.  1.
 reiter  1  ehud reiter. has a consensus nl generation architectureappeared  and is it psycholinguisticallyplausible  in proceedings of 1th int. workshop on nl generation  inlg-1   pages 1  kennebunkport  maine  usa  1.
 reiter  1  ehud reiter. pipelines and size constraints. computational linguistics  1-1  1.
 robin and mckeown  1  jacques robin and kathy mckeown. empirically designing and evaluating a new revision-based model for summary generation. artificial intelligence  1-1   1.
 rubinoff  1  robert rubinoff. integrating text planning and linguistic choice. in aspects of automated natural language generation  lecture notes in artificial intelligence  1  pages 1. springer verlag  berlin  april 1.
 white  1  michael white. designing dynamic hypertext. in 1nd workshop on adaptive hypertext and hypermedia  june 1. held in conjunction with hypertext'1  pittsburgh  usa.
 zuckerman and mcconachy  1  ingrid zuckerman and richard mcconachy. generating explanations across several user models: maximizing belief while avoiding boredom and overload. in proceedings of 1th european workshop on natural languagegeneration  ewnlg-1   1.
narrative prose generationcharles b. callaway
department of computer science
north carolina state university
raleigh  nc 1 usa cbcallaw eos.ncsu.edu
james c. lester
department of computer science
north carolina state university
raleigh  nc 1 usa lester csc.ncsu.edu

abstract
story generation is experiencing a revival  despite disappointing preliminary results from the preceding three decades. one of the principle reasons for previous inadequacies was the low level of writing quality  which resulted from the excessive focus of story grammars on plot design. althoughthese systems leveraged narrative theory via corpora analyses  they failed to thoroughly extend those analyses to all relevant linguistic levels. the end result was narratives that were recognizableas stories  but whose prose quality was unsatisfactory.
however  the blame for poor writing quality cannot be laid squarely at the feet of story grammars  as natural language generation has to-date not fielded systems capable of faithfully reproducing either the variety or complexity of naturally occurring stories. this paper presents the author architecture for accomplishing precisely that task  the storybook implementation of a narrative prose generator  and a brief description of a formal evaluation of the stories it produces.
1	introduction
despite extensive research in the fields of story generation and natural language generation  collaborative research between the two has been virtually nonexistent. a major reason for this is the difficult nature of the problems encountered respectively in these fields. story generators  meehan  1; yazdani  1; lebowitz  1; turner  1; lang  1    typically address the macro-scale development of characters and plot  slowly refining from the topmost narrative goal level down to individual descriptions and character actions by progressively adding more and more detail. meanwhile  work in natural language generation  nlg  focuses on linguistic phenomena at the individual sentence level  and only recently have nlg systems achieved the ability to produce multiparagraph text. what remains is a substantial gap between the narrative plans produced by story generators and the requirements of nlg systems.
　this is explained by the historic research programs of these two distinct fields. story generation originally descends from the application of planning formalisms to the work of sociolinguists such as vladimir propp  propp  1   who created story grammars to capture the high-level plot elements found in russian folktales. early work  figure 1  in this area  meehan  1; yazdani  1; lebowitz  1  focuses on the creation of characters and their interactions with plot elements. in the latest of these  lebowitz states   eventually  we expect universe to be able to generate connected stories in natural language form over a long period of time. for the moment  we are concentrating on generating plot outlines  and leaving problems of dialogue and other low-level text generation for later.  moreover  even the most recent story generation systems  such as minstrel and joseph  turner  1; lang  1   focus on characters and plot when generating text  without consideringthe actual linguistic structures found in the texts they are attempting to mimic  figure 1 .
　however  the lack of progress in achieving computerproduced stories characterized by high-quality prose is far from one-sided. rather than narrative generation  most full-scale nlg systems  hovy  1; young  1; horacek  1; lester and porter  1; mittal et al.  1; callaway et al.  1  instead focus on explanation generation  creating scientific or instructional text which significantly differs in the distribution and frequency of syntactic  semantic  and orthographic features from that found in narrative prose  although a few projects do address some of these issues  e.g.   kantrowitz and bates  1; robin  1; doran  1; cassell et al.  1  . in addition  the most advanced of these systems are still not capable of producing more than two paragraphs of text  while the vast majority of naturally occurring narratives are at least several pages long. finally  none of these systems are intended to accept narrative plans from a typical story generator.
　to bridge the gap between story generators and nlg systems  we have developed the author narrative prose generation architecture  callaway  1  to create high-quality narrative prose comparable to  and in some cases identical to  that routinely produced by human authors. this architecture has been implemented in storybook  an end-to-end narrative prose generation system that utilizes narrative planning  sentence planning  a discourse history  lexical choice  revision  a full-scale lexicon  and the well-known fuf/surge  elhadad  1  surface realizer to produce multi-page stories in the little red riding hood fairy tale domain.
once upon a time george ant lived
near a patch of ground. there was a nest in an ash tree. wilma bird lived in the nest. there was some water in a river.
wilma knew that the water was in the
river. george knew that the water was
in the river. one day wilma was very
thirsty. wilma wanted to get near some water. wilma flew from her nest across a meadow through a valley to the river. wilma drank the water. wilma wasn't very thirsty any more.
figure 1: prose generated by tale-spin  1
　narrative prose differs linguistically from text found in explanatory and instructional passages in a number of ways:
the existence of character dialogue with the accompanying difficulties of orthographic markers  doran  1; callaway  1   speaker-hearer relationships  locutional relations and manner clauses  interjections  and changes in pronominalization patterns. for instance  the following would never be found in explanatory text:  beware the wolves   her mother said in a hushed voice.
since explanatory text lacks dramatic characters  there is little need to include personal pronouns  highly idiomatic text about personal needs  or intentional desires such as wanting  needing  or knowing.
without character dialogue  explanatory text is usually able to get by using only present verb tenses with an occasional reference to events in the past when discussing sequencesof processes. however  dialogueand the complex interactions between characters opens up the need to perform at least simplistic temporal reasoning and realizations in complex present  future and past tenses.
because human authors employ widely differing styles in narrative  e.g.  hemingway vs. joyce  as opposed to explanatory or instructional text which tries to adhere to stricter conventions  a narrative prose generator should be capable of mimicking those different types of styles.
finally  a narrative prose generator must conform to common prose formatting conventions  such as knowing when to force paragraph breaks and being able to generate written stylistic effects like onomatopoeia  regional dialects  and emphasis  e.g.   ewwww!   b-b-but  it's s-s-scary!   mom  you can't do that!  
　storybook is capable of reproducing these phenomena  and doing so in both grammatically correct english and passable spanish  callaway et al.  1; callaway  1 .
　upon receiving a high-level story specification from a narrative planner  storybook  1  structures it into paragraph and sentence-sized chunks   1  conducts a discourse history analysis to determine indefinite references and pronominalizations   1  performs a lexical choice analysis to increase variety among concepts and event relations   1  maps actors  props and events to semantic/syntactic roles in full linguistic deep structures   1  revises paragraph-sized groups of one day it happened that peasant quarreled with the wife. when this happened  peasant felt distress. in response  peasant took a walk in the woods. peasant found a pit when he looked under the bush. when this happened  peasant desired to punish wife. in response  peasant made it his goal that wife would be in the pit. peasant tricked wife. wife was in the pit. peasant lived alone.
figure 1: prose generated by joseph  1
deep structures via aggregation and reordering to eliminate the short  choppy sentences characteristic of text produced by discourse planning systems  and  1  performs surface realization with integrated formatting to produce narrative prose similar to that found in stories written by human authors.
　to evaluate the quality of the narratives that storybook produces  we created a simplified narrative planner capable of generating two little red riding hood stories expressed in the required high-level story specification. we then created five versions of storybook variouslyablating the discourse history  lexical choice  and revision components to produce a total of 1 story versions which were then formally evaluated by a panel of judges. the results showed significant differences between the inclusion or ablation of individual architectural components.
1	narrative representation
while most researchers in story generation utilize planning mechanisms or story grammars  a growing literature on narratology  propp  1; segre  1; bal  1  posits that narrativeconsists of the fabula  or sum total of knowledgeand facts about a narrative world  and the suzjet  or the ordering and specifics about what the author presents and at which position s  it occurs in the linear narrative. the author architecture adopts this view and computationalizes it to describe the requirements of a narrative planner and a narrative prose generator: the narrative planner is responsible for creating both the fabula and suzjet  while the narrative prose generator is responsible for converting them into textually recognizable narratives.
　a narrative world is also populated with a large number of scenes  characters  props  locations  events  and descriptions. the storybook implementation explicitly represents this knowledge  which forms the basis of the fabula. initially  the fabula contains only ontological information  including the existence of broad concepts such as forest  cottage  and person  and concept relations like next-to  motherof  and moves-toward. storybook assumes that a narrative planneris responsiblefor constructingthe specific concept instances that populate a particular story  e.g.  little red riding hood lives in cottage1  which is her house  while her grandmother grandmother1 lives in a differenthouse  cottage1.
;;; fabula operators
 newnarrative meehan-narrative1 narrator1 
 addactor george-ant1 george-ant ant male  george ant  
 addactor wilma-bird1 wilma-bird bird
female  wilma bird  
 addlocation patch1 patch-area 
 addlocation ground1 ground-earth-area 
 addlocation nest1 nest-for-birds 
 addlocation ash-tree1 ash-tree 
 addprop water1 water 
 addlocation river1 river 
 addlocation meadow1 meadow 
 addlocation valley1 valley 
 addalias wilma1 wilma wilma-bird1  wilma  
 addalias george1 george george-ant1  george  
;;; narrative stream primitives
 narration-mode historical-fairy-tale mixed-dialogue simple-syntax ascii-format narrated english 
 narrator-mode narrator1 third-person disembodied 
 prop-relationship living-near george-ant1 patch1 
 refinement region-of patch1 ground1 
 specification living-near process-step-type once-upon-a-time 
 prop-property exist-being nest1 
 specification exist-being location-in ash-tree1   prop-relationship living-in wilma-bird1 nest1 
 prop-property exist-being water1 
 specification exist-being location-in river1 
 refinement quantifier-value water1 some 
 define-event being-in1 being-in water1 river1 
 actor-intent knowing wilma1 being-in1 
 specification knowing thought-binder that-binder 
 define-event being-in1 being-in water1 river1 
 actor-intent knowing george1 being-in1 
 specification knowing thought-binder that-binder 
 actor-property personal-condition wilma1 thirsty-state 
 specification personal-condition time one-day 
 refinement intensifier thirsty-state very 
 define-event getting-near1 getting-near none water1 
 actor-intent wanting wilma1 getting-near1 
 refinement quantifier-value water1 some 
 actor-action flying-from wilma1 nest1 
 refinement belonging-to nest1 wilma1 
 specification flying-from across-path meadow1 
 specification flying-from through-path valley1 
 specification flying-from destination river1 
 actor-action drinking-action wilma1 water1 
 actor-property personal-condition wilma1 thirsty-state 
 specification personal-condition duration any-more 
 specification personal-condition polarity negative 
 refinement intensifier thirsty-state very 
figure 1: fabula and narrative stream for generating fig. 1
　in addition  storybook assumes that the narrative planner is responsible for creating a stream of narrative events  the suzjet  that defines the linear ordering of events and descriptions as well as for content determination  the nlg term for deciding if particular narrative details or underlying facts are ever mentioned at all  e.g.  events can be  too obvious  or perhaps meant to be inferred  as in mystery novels . also  linearity can vary between different versions of a single story: a strict chronological ordering that states little red riding hood meets a wolf before travelling to grandmother's house wouldn't necessarily hold in the in medias res version.
　in order to computationalize the fabula and narrative stream so they can serve as an interface between a narrative planner and a narrative prose generator  the author architecture defines a set of fabula operators which can be used to construct the fabula from the original story ontology  and a set of narrative stream primitives  figure 1   which define the presentational order and content determination as well as information about what purpose that particular content serves at that point in the narrative.
　a typical fabula operator relates a new concept instance  indicated by a unique number at the end of the name  to either some element in the story ontology or a previously created concept instance. a typical narrative stream primitive consists of a narrative directive  which describes the purpose for its inclusion by the narrative planner as well as the relationship between its arguments. the ordered arguments of a narrative directive are directly tied to either the original concepts in the story ontology or the derived concept instances created by the fabula operators. furthermore  a partial order is imposed on the narrative stream forcing dependent elements to follow their modifiers  e.g. in the phrase  some water  from figure 1   water  is introduced in a narrative primitive before the  some  quantifier is introduced .
　storybook currently defines six different fabula operators as well as 1 narrative stream primitives that serve three main functions: delimitation  narrator and scene changes  dialogue marking  and formatting directives   foundation  important clause-level events and descriptions  rhetorical  intentional  and perlocutionary relations loosely based on speech act theory  austin  1; searle  1    and modification  descriptive elaboration  comparison  manner  reason  time  etc.  these have been sufficient to encode three distinctly different multi-page little red riding hood fairy tales and to allow storybook's narrative prose generator to create the narrative texts for each.
　finally  storybook assumes that the fabula and narrative stream operate in an interlingual environment  where the knowledge base encodes world knowledge in a languageneutral format  callaway et al.  1; callaway  1 . thus given a single fabula and narrative stream  we should be able to producefairy tales  or other forms of fictional narratives in a variety of languages. a significant benefit of this approach is that such a narrative prose generator could also be used to improve the output of a machine translation system in a manner analogous to that of story generation. regardless of how they are determined  the fabula and narrative stream are sent along with a set of stylistic parameters to the narrative prose generator as described in the following section.
1	the author architecture
to reproduce the complex phenomena that characterize human-generated stories  an effective narrative prose generator must be comprehensivein scope. it must address all of the requirements inherent in sentence planning  lexical choice  formatting  revising  and surface realization. author therefore takes a standard  pipelined  approach with components for each of these processes. with the exception of discourse planning  which is here replaced by a narrative planner  storybook is the first nlg system to incorporate all of these modules into an end-to-end multi-page generation system.
　upon receiving the fabula and narrative stream from the narrative planner  storybook  figure 1  first structures it into paragraph and sentence-sized chunks. it then conducts a discourse history analysis to determine pronominalizations and identify seen/unseen concepts. next  it performs a lexical choice analysis to increase variety. it then maps actors  props and events to semantic/syntactic roles in full linguistic deep structures. next it revises paragraph-sized groups of sentences via aggregation and reordering to increase propositional density before finally performing surface realization to produce narrative prose similar to that found in stories written by human authors.
1	narrative organization
because the narrative stream  figure 1  is generated by the narrative planner as one long sequence  it must be segmented into groups of narrative stream primitives which reflect natural boundariessuch as changes in speakerduringdialogueand shifts in scene or topic. because of the partial order imposed on the narrative stream  this is a relatively straightforward process. in addition  the discourse history and lexical choice modules operate by combingthroughthe narrativestream and recording data in order to make decisions about altering the narrative stream. because these three procedures involve a similar iterative analysis  they are performed by a single architectural module called the narrative organizer whose purpose is to take a flat  linear narrative stream and impose a hierarchical narrative structure onto it.
　after the narrative stream primitives have been segmented  the discourse history module opportunistically replaces concept instances with the appropriate definite/indefinite forms and pronominalizations. these features are used to decide when to replace the lexicalizations of concepts and concept instances with the appropriate new linguistic deep structure information.1 for example  a decision to make  wolf  or  butter  be indefinite when they are first mentioned in the discourse context may result in an indefinite article for the count noun   a wolf   or an indefinite determiner or determiner sequence for the mass noun   some butter  . knowing whether a concept instance has been seen or not requires computing and tracking several occurrence properties for every concept instance:
frequency: how often a concept instance has been used  lexicalized vs. pronominalized .
last-usage: if its most recent use was lexicalized.
recency: how many distinct concept instances have been used since it was last seen  including gender .
distance: the number of scenes  paragraphs  or dialogue turns since it was last seen.
　similarly  pronominalization decisions are made to replace repetitive instances of conceptinstances with appropriatepronouns  e.g.   grandmother  with the single feminine pronoun  she/her  . because knowing when an instance is repetitive involves using the same occurrence properties  the lexical chooser similarly checks for excessive repetition of concept

figure 1: a narrative prose generation architecture
instances or relations. if this happens  the lexical chooser may replace elements of each narrative primitive with synonymous concept instances or relations from the fabula. the storybook lexical choice module detects:
repetition in noun phrases: languages typically contain a large number of similar nouns. for example  little red riding hood might live in a house  cottage  shack  hut  cabin  etc.
repetition in verb phrases: similarly  events have a number of lexicalizations. little red riding hood can walk through the forest  skip  amble  stroll  etc.
repetition in thematic role ordering: many event verbs also impose different theta frames even though they describe similar actions. little red riding hood might give her grandmother the cookies or grandmother might receive the cookies from little red riding hood.
　although this does not compare to more sophisticated methods  elhadad  1; stede  1  and is by no means suggested as a solution to the problem of lexical choice  it is sufficient to satisfy our goal of preventing repetitive prose. the result of segmentation  discourse history analysis  and lexical choice is thus a modified narrative stream. however  in classic pipelined nlg architectures  discourse planners typically produce a single structure  e.g.  a frame  that corresponds to a sentence-sized chunk of the discourse plan. thus  the job of the narrative structurer is to convert the groups of narrative primitives into a sequence of specifications suitable for the sentence planner. additionally  the narrative structurer is responsible for making decisions about tense shifting  especially for character dialogue. in dialogue  conversations usually take place in present tense even though the surrounding narrative is in past tense  and references to prior events are typically in the past tense where in expository text they would be in the past perfect tense  at least for english .
　because the fabula and story ontologyexist by these stages  they can be used as knowledge sources for making appropriate decisions. for example  the discourse history module may examine the gender of a concept instance and thus know it should substitute  she  for  little red riding hood  without that knowledge having to be explicitly represented in the narrative stream. similarly  the narrative structurer may examine the lexicon entry of a narrative stream primitive's primary relation to determineits theta frame and its argument'ssemantic type restrictions for error-checking purposes.
1	sentence planning
the function of the sentence planner is to take a specification for the semantic content of a sentence  or protosentence  and to plan the roles  either semantic or syntactic  that each of its elements play in a particular sentence. because our approach utilizes an off-the-shelf surface realizer that expects particular semantic roles  we require that our sentence planner produce the deep structure linguistic representations known as functional descriptions  fds  figure 1 . functional descriptions are hybrid semantic/syntactic entities that can be used to produce text via unification with the fuf/surge  elhadad  1  surface realizer.
a sentence planner must:
guarantee that complex content units are properly and completely packaged within functional descriptions  e.g.  complex noun phrases such as  the beautiful cottage where they lived  must be  a  capable of being created as a linguistic deep structure and  b  encapsulated so that it can be manipulated as a whole by succeeding elements of the pipelined nlg architecture.
assign thematic roles to concepts. to achieve semantic equivalence between a sentence's frame specification and the corresponding deep structure  a sentence planner must ensure that relations in the specification are precisely mapped to the appropriate thematic roles in a functional description  e.g.  mother1 could be mapped to agent and nest1 to located.
robustly construct functional descriptions. a sentence planner must ensure that only fds that will create grammatical sentences can be constructed. a number of errors that degrade robustness must be curbed  e.g.  lack of appropriate lexicalizations  missing semantic roles  and sentential modifiers that conflict with the overall sentential semantics.
　once the sequence of narrative stream primitives has been processed by the sentence planner  the resulting fds  representing the deep linguistic structures for each protosentence  can be given directly to the surface realizer for text generation. however  because the quality of simple propositional
  cat clause 
 tense past 
 process   type existential   
 participants   located   cat common 
 definite no 
 lex  nest      
 pred-modif   location   cat pp 
 prep   lex  in    
 np   cat common 
 definite no   lex  ash tree         
figure 1: functional description  fd  for  there was a nest in an ash tree.  from figure 1  sentence 1.
sentences is notoriously poor  storybook revises them  iteratively saving each fd while maintaining the paragraph separations imposed by the narrative segmenter and proceeds to send paragraph-sized batches to the revision component  described in the following section  in order to improve overall prose quality.
1	revision
revision modules  dalianis and hovy  1; robin  1; callaway and lester  1; shaw  1  take a series of protosentences  simple sentences with limited content  e.g.   the wolf saw little red riding hood   and rearrange them by aggregation  i.e. combining protosentences in various ways  or by migration  i.e. permuting the order of two adjacent protosentences. the revisor component  callaway and lester  1  receives a paragraph-sized group of protosentences from the sentence planner represented as an ordered set of deep-structure functional descriptions.
　to illustrate  consider the issue of clause aggregation  a central problem in multi-sentential text generation. suppose a narrative prose generation system is given the task of constructing a fairy tale and it produces several pages of prose. although it might accurately communicate the content of the narrative plan  the terseness of each sentence makes the overall effect disjointed; in other words  content without literary form. an entire story comprised solely of protosentences is intolerable for almost any adult reader.  see sample prose in figures 1 and 1. 
　to avoid producing a series of abrupt sentences  a narrative planner could be assigned the task of predicting how particular concepts will be realized in order to optimize clause aggregation and reordering decisions. however  this approach violates modularity considerations and does not scale well: it significantly complicates the design of the narrative planner by forcing it to attend simultaneously to content selection  narrative organization  and complex syntactic issues. alternatively  the propositions could be grouped by a single-pass realization system. this approach is quite inefficient and also ineffective. reorganizing  aggregating  and realizing the specifications in a single pass poses innumerable difficulties: the realizer would somehow have to anticipate the cumulative effects of all aggregation decisions with regard to grammaticality  subordination  and lexical choice.
　an important aspect of revision in nlg is the concept of discourse constraints  which specify a partial order on the sequence of functional descriptions. for example  the narrative planner might hand down a narrative constraint stating that  in a particular narrative passage  a sequence of events are causal in nature and that to reorder them in some fashion could destroy that causality in the mind of the reader. additionally  because narrative prose includes character dialogue  it is important to prevent the reordering of character utterances. thus  discourse constraints are employed to restrict aggregation and migration revisions that would affect particular types of clause elements across critical semantic boundaries. storybook utilizes the multilingual version of the revisor component described in  callaway and lester  1  to perform all of these tasks.
1	surface realization
the revision component passes the series of revised functional descriptions one by one to the surface realizer  which is responsible for producing the actual readable text that readers see. storybook employs the fuf surface realizer  which is accompanied by the surge  systemic reusable grammar of english  grammar. surge  written as a systemic grammar  halliday  1  in the fuf formalism  is the largest generation grammar in existence in terms of coverage  containing largeportionsof quirk's comprehensivegrammarof english  quirk et al.  1  in an hpsg  pollard and sag  1  interpretation.
　modifications were made to surge to allow for dialogue orthography  callaway  1   integrated formatting to produce latex  html  and xml  as well as a number of grammatical additions to account for syntactic constructions encountered during our corpus analyses  i.e.  linguistic phenomena we encountered in narratives that were not present in our analyses of explanatory and instructional text . this allows storybook to produce webpages as output that include pre-generated graphics specified in the narrative stream as well as boldface  italics  and font size embedded into individual sentences. furthermore  we implemented a spanish version of surge as described in  callaway et al.  1  and also augmented it to produce character dialogue  etc.
1	implementation and evaluation
storybook is an end-to-end generation system capable of producing multi-page narrative prose in the little red riding hood domain like that found in figure 1  which required 1 fabula operators and 1 narrative stream primitives to generate. storybook is implemented in harlequin lisp on a dell precision workstation 1 using a 1 mhz pentium iii processor with 1 mb of memory. the initial story ontology consists of approximately 1 concepts and 1 relations  including their lexicon entries  covering three different little red riding hood narratives.
　storybook consists of approximately 1 lines of lisp  for narrative organization  sentence planning  and revision  but not surface realization . in addition  there are approximately 1 revision rules which are presently being modified to work for spanish. the spanish version of surge is approximately the same size as the english version. during story writing  surface realization is by far the largest consumer of time  usually requiring 1% of the 1 seconds needed to generate a two to three page narrative.
　　　once upon a time  a woodcutter and his wife lived in a small cottage. the woodcutter and his wife had a young daughter  whom everyone called little red riding hood. she was a merry little maid  and all day long she went singing about the house. her mother loved her very much.
　　　one day her mother said   my child  go to grandmother's house. we have not heard from her for some time. take these cakes  but do not stay too long. and  beware the dangers in the forest. 
　　　little red riding hood was delighted because she was very fond of her grandmother. her mother gave her a well-filled basket and kissed her goodbye.
　　　the road to grandmother's house led through the dark forest  but little red riding hood was not afraid and she went on as happy as a lark. the birds sang her their sweetest songs while the squirrels ran up and down the tall trees. now and then  a rabbit would cross her path.
　　　little red riding hood had not gone far when she met a wolf.
　　　 hello   greeted the wolf  who was a cunning-looking creature.  where are you going  
　　　 i am going to my grandmother's house   little red riding hood replied.
　　　 ah  well then  take care in the forest  for there are many dangers.  and then the wolf left.
　　　little red riding hood was not in a hurry. indeed  she gathered wild flowers and chased the pretty butterflies.
　　　meanwhile the wolf ran ahead very quickly and soon arrived at grandmother's house. he knocked on the door gently. the old lady asked   who is there  
　　　the wolf replied   it is little red riding hood  your granddaughter. 
　　　and so the old lady opened the cottage door. the wolf rushed in immediately and devoured the lady in one bite. then he shut the door and climbed into the old lady's bed.
　　　much later little red riding hood arrived at grandmother's house. she knocked on the door and shouted   grandmother  it is little red riding
hood. 
 pull the string. the door will open. 
　　　and so little red riding hood opened the door and walked in.  grandmother  what big eyes you have. 
 all the better to see with  dear. 
 grandmother  what big ears you have. 
 all the better to hear with  dear. 
 and  grandmother  what big teeth you have! 
　　　 all the better to eat up with!  yelled the wolf.
　　　and then the wolf jumped up and devoured little red riding hood in one bite.
figure 1: example text produced by storybook previous story generation projects such as tale-spin
 meehan  1   universe  lebowitz  1   and joseph  lang  1   which actually generated narrative prose were never subjected to an empirical evaluation to determine qualitatively or quantitatively how well their systems produced narratives. also  narrative prose generation is currently at such an early stage of development that its evaluation should be conducted in a manner that is qualitatively different from work in more mature areas such as machine learning.
　in order to assess the utility and overall contributions of our deep generation architecture to the task of narrative prose generation  we conducted a formal evaluation of the storybook system. its purpose was to establish a baseline for future nlg systems by judging the performance of three key architectural components that differentiate shallow nlg systems from deep nlg systems: the discourse history module  lexical choice module  and revision module.
　formally comparing human-authorednarratives with those produced by computer presents a difficult problem: there is no known objective metric for quantitatively evaluating narrative prose in terms of how it performs as a story. simple metrics exist for evaluation at the sentence level  e.g.  number of words  depth of embedding  etc.   but a narrative per se cannot be considered to be merely a collection of sentences that are not related to each other. we instead opted for a computer vs. computer style of evaluation involving the ablation of the three architectural components mentioned above.
　to stand in for the narrative planner  which is beyond the scope of this work   we created a modestly sized finite state automaton  containing approximately 1 states  capable of producing two stories  comprising two and three pages respectively. furthermore  we fixed the content of those stories  i.e.  the fabula and narrative stream were identical  and ran five different versions of storybook on each story:  1  all three architectural components working   1  revision turned off   1  lexical choice turned off   1  the discourse history turned off  and finally  1  a version with all three components turned off. this resulted in ten total narratives which we presented to our test subjects. while the two versions differed in the sense that particular modules were either ablated or not  the two stories differed because they were created from two separate paths through the planning automaton. thus  story #1 had some different events  descriptions  and props than story #1 did.
　twenty test subjects graded each narrative over nine grading factors  representing various stylistic and linguistic criteria  according to an a-f scale. we then converted the results to a quantified scale where a = 1  b = 1  c = 1  d = 1  and f = 1 and tallied and averaged the final scores. to determine the quantitative significance of the results  we performed an anova test over both stories. the analysis was conducted for three independent variables  test subject  story  and version  over the following grading factors:
overall: how is the story as an archetypal fairy tale 
style: did the author use an appropriate writing style 
grammaticality: how would you rate the syntactic quality 
flow: did the sentences flow from one to the next 
diction:	how appropriate were the author's word choices 
readability: how hard was it to read the prose 
logicality: did the story seem out of order 
detail: did the story have the right amount of detail  or too much or too little 
believability: did the story's characters behave as you would expect 
　the results of the anova analysis point to three significant classes of narratives due to the architectural design of the narrative prose generator. the most preferred narrative class  consisting of versions  1  and  1   were not significantly different from each other while they were rated significantly higher than all other versions. in addition  version  1  scored significantly better than the third class formed by versions  1  and  1   each of which lacked a discourse history.
　the results indicate that discourse history and revision componentsare extremelyimportant while lexical choice improvedtext significantly in story #1 but not in story #1. upon analysis of the comments in their evaluations  it became clear that a principal reason was the test subjects' belief that the increased lexical variation might prove too difficult for children to read  even though we provided no indication that the target audience was children  and thus version  1  compared less favorably to version  1  due to the more complex and varied words it contained. it is not clear whether a lexical choice component would play a much more significant role in subject matter where a more adult audience was expected or if a larger-scale component were utilized.
1	conclusions
full-scale linguistic approaches to narrative prose generation can bring about significant improvements in the quality of text produced by story generators. by integrating off-theshelf nlg components and adding a well-defined computational model of narrative  we can create a new generation of story systems whose written prose quality far surpasses that of previous attempts. this approach has been implemented in storybook  a narrative prose generator that produces multi-pagefairy tales in near realtime. this deep structure approach has also been formally evaluated  suggesting that the architectural modules responsible for a significant improvement in prose quality are components not found in shallow  template-based  generation systems currently employed by most story generators. it is hoped that this work begins to bridge the traditional gap between story generators and nlg systems.
1	acknowledgements
the authors wish to thank joy smith of nc state university for her help with the statistical analysis; bruce porter and eric eilerts for their development of the km knowledge base language; michael elhadad and jacques robin for their development of fuf/surge; michael young for helpful discussion on narratives; the anonymous reviewers for their insightful comments. support for this work was provided by the intellimedia initiative of north carolina state university.
references
 austin  1  j. l. austin. how to do things with words. oxford university press  new york  1.
 bal  1  mieke bal. narratology: introduction to the theory of narrative  1nd edition. university of toronto press  toronto  canada  1.
 callaway and lester  1  charles b. callaway and james c. lester. dynamically improving explanations:
a revision-based approach to explanation generation. in proceedings of the fifteenth international joint conference on artificial intelligence  pages 1  nagoya  japan  1.
 callaway et al.  1  c. callaway  b. daniel  and j. lester. multilingual natural language generation for 1d learning environments. in proceedings of the 1 argentine symposium on artificial intelligence  pages 1  buenos aires  argentina  1.
 callaway  1  charles callaway. narrative prose generation. phd thesis  north carolina state university  raleigh  nc  1.
 callaway  1  charles callaway. a computationalfeature analysis for multilingual character-to-character dialogue. in proceedings of the second international conference on intelligenttext processing andcomputationallinguistics  pages 1  mexico city  mexico  1.
 cassell et al.  1  j. cassell  m. stone  and h. yan. coordination and context-dependence in the generation of embodied conversation. in international natural language generation conference  mitzpe ramon  israel  1.
 dalianis and hovy  1  hercules dalianis and eduard hovy. aggregation in natural language generation. in proceedings of the fourth european workshop on natural language generation  pisa  italy  1.
 doran  1  christine doran. incorporating punctuation into the sentence grammar: a lexicalized tree adjoining grammar perspective. phd thesis  university of pennsylvania  philadelphia  pa  1.
 elhadad  1  michael elhadad. using argumentation to control lexical choice: a functional unification implementation. phd thesis  columbia university  1.
 halliday  1  michael halliday. system and function in language. oxford university press  oxford  1.
 horacek  1  helmut horacek. a model for adapting explanations to the user's likely inferences. user modeling and user-adapted interaction  1 :1  1.
 hovy  1  eduard h. hovy. automated discourse generation using discourse structure relations. artificial intelligence  1-1  1.
 kantrowitz and bates  1  m. kantrowitz and j. bates. integrated natural language generation systems. in r. dale  e. hovy  d. rosner  and o. stock  editors  aspects of automated natural language generation  pages 1. springer-verlag  berlin  1.
 lang  1  r. raymond lang. a formal model for simple narratives. phd thesis  tulane university  new orleans  la  1.
 lebowitz  1  m. lebowitz. story-tellingas planningand learning. poetics  1 :1  1.
 lester and porter  1  james c. lester and bruce w. porter. developing and empirically evaluating robust explanation generators: the knight experiments. computational linguistics  1 :1  1.
 meehan  1  j. meehan. tale-spin  an interactive program that writes stories. in proceedings of the fifth international joint conference on artificial intelligence  cambridge  ma  1.
 mittal et al.  1  v. mittal  j. moore  g. carenini  and s. roth. describing complex charts in natural language: a caption generation system. computational linguistics  1 :1  1.
 pollard and sag  1  c. pollard and i. sag. head-driven phrase structure grammar. the university of chicago press  chicago  1.
 propp  1  v. propp. morphology of the folktale. university of texas press  austin  tx  1.
 quirk et al.  1  r. quirk  s. greenbaum  g. leech  and j. svartvik. a comprehensive grammar of the english language. longman publishers  1.
 robin  1  jacques robin. revision-based generation of natural language summaries providing historical background. phd thesis  columbia university  december 1.
 searle  1  j. searle. speech acts. cambridge university press  cambridge  england  1.
 segre  1  cesare segre. introduction to the analysis of the literary text. indiana university press  bloomington  in  1.
 shaw  1  james shaw. segregatory coordination and ellipsis in text generation. in coling-acl-1: proceedings of the joint 1th meeting of the association for computational linguistics and the 1th international conference on computational linguistics  pages 1  montre＞al  canada  1.
 stede  1  manfred stede. lexical semantics and knowledge representation in multilingual sentence generation. phd thesis  university of toronto  toronto  ontario  1.
 turner  1  scott r. turner. the creative process: a computer model of storytelling and creativity. lawrence erlbaum associates  hillsdale  nj  1.
 yazdani  1  masoud yazdani. how to write a story. in proceedings of the european conference on artificial intelligence  orsay  france  july 1.
 young  1  r. michael young. using plan reasoning in the generation of plan descriptions. in proceedings of the thirteenth national conference on artificial intelligence  pages 1  1.

natural language processing
and information retrieval
natural language - learning for information extraction

adaptive information extraction from text by rule induction and generalisation
fabio ciravegna
department of computer science  university of sheffield
regent court  1 portobello street 
s1dp sheffield  uk
f.ciravegna dcs.shef.ac.uk

abstract
 lp 1 is a covering algorithm for adaptive information extraction from text  ie . it induces symbolic rules that insert sgml tags into texts by learning from examples found in a userdefined tagged corpus. training is performed in two steps: initially a set of tagging rules is learned; then additional rules are induced to correct mistakes and imprecision in tagging. induction is performed by bottom-up generalization of examples in the training corpus. shallow knowledge about natural language processing  nlp  is used in the generalization process. the algorithm has a considerable success story. from a scientific point of view  experiments report excellent results with respect to the current state of the art on two publicly available corpora. from an application point of view  a successful industrial ie tool has been based on  lp 1. real world applications have been developed and licenses have been released to external companies for building other applications. this paper presents  lp 1  experimental results and applications  and discusses the role of shallow nlp in rule induction.
1. introduction
by general agreement the main barriers to wide use and commercialization of information extraction from text  ie  are the difficulties in adapting systems to new applications. the classical ie has been focusing on applications to free texts; therefore systems often rely on approaches based on natural language processing  nlp   e.g. using parsing   humphreys et al.  1; grishman 1 . most systems require the manual development of resources  e.g. grammars  by a user skilled in nlp  ciravegna 1 . there is an increasing interest in applying machine learning  ml  to ie in order to build adaptive systems. up to now  the use of ml has been approached mainly in an nlp-oriented perspective  i.e. in order to reduce the amount of work to be done by the nlp experts in porting systems across free text based scenarios  cardie 1; miller et al. 1; yangarber et al. 1 . given the current technology  ie experts are still necessary.
in the last years  the increasing importance of the internet has stressed the central role of texts such as emails  usenet posts and web pages. in this context  extralinguistic structures  e.g. html tags  document formatting  and ungrammatical stereotypical language  are elements used to convey information. linguistically intensive approaches are difficult or unnecessary in such cases. for this reason a new research stream on adaptive ie has arisen at the convergence of nlp  information integration and machine learning. the goal is to produce ie algorithms and systems adaptable to new internet-related applications/scenarios by using only an analyst's knowledge  i.e. knowledge on the domain/scenario itself   kushmerick 1; califf 1; muslea et al. 1; freitag and mccallum 1; soderland 1; freitag and kushmerick 1 . such algorithms are very effective when applied on highly structured html pages  but less effective on unstructured texts  e.g. free texts . in our opinion this is because most successful algorithms make scarce  or no  use of nlp  tending to avoid any generalization over the flat word sequence. when they are applied to unstructured texts  data sparseness becomes a problem.
this paper presents  lp 1  an adaptive ie algorithm designed in this new stream of research that makes use of shallow nlp in order to overcome data sparseness when confronted with nl texts  while keeping effectiveness on highly structured texts. this paper first introduces the algorithm  discusses experimental results and shows how the algorithm compares successfully with the current state of the art. the role and importance of shallow nlp for overcoming data sparseness is then discussed. finally a successful industrial system for adaptive ie built around  lp 1 is presented and some conclusions and future work are drawn.
1.the rule induction algorithm
 lp 1 learns from a training corpus where a user has highlighted the information to be extracted with different sgml tags. it induces symbolic rules that insert sgml tags into texts in two steps:
1. sets of tagging rules are induced by bottom-up generalization of tag instances found in the training corpus. shallow knowledge about nlp is used in the generalization process.
1. correction rules are induced that refine the tagging by correcting mistakes and imprecision
this section presents and discusses these two steps.
1 inducing tagging rules
a tagging rule is composed of a left hand side  containing a pattern of conditions on a connected sequence of words  and a right hand side that is an action inserting an sgml tag in the texts. each rule inserts a single sgml tag  e.g.  speaker . this makes  lp 1 different from many adaptive ie algorithms  whose rules recognize whole slot fillers  i.e. insert both  speaker  and  /speaker   califf 1; freitag 1   or even multi slots  soderland 1 . the tagging rule induction algorithm uses positive examples from the training corpus for learning rules. positive examples are the sgml tags inserted by the user. all the rest of the corpus is considered a pool of negative examples. for each positive example the algorithm:  1  builds an initial rule   1  generalizes the rule and  1  keeps the k best generalizations of the initial rule. in particular  lp 1's main loop starts by selecting a tag in the training corpus and extracting from the text a window of w words to the left and w words to the right. each information stored in the 1*w word window is transformed into a condition in the initial rule pattern  e.g. if the third word in the window is  seminar   the condition on the third word in the pattern will be word= seminar . each initial rule is then generalized. in the generalization process  lp 1 uses generic shallow knowledge about natural language as provided by a morphological analyzer  a pos tagger and a userdefined dictionary  or a gazetteer . a lexical item  lexit in the following  summarizes such knowledge for each word  e.g.  companies  via: a lemma  company   a lexical category  noun   case information  lowercase  and a list of user defined classes as defined by a user-defined dictionary or a gazetteer  if available . an initial rule and associated information in the lexits is in table 1. generalization consists in the production of a set of rules derived by relaxing constraints in the initial rule pattern. conditions are relaxed both by reducing the pattern in length and by substituting constraints on words with constraints on some parts of the additional knowledge.  table 1 shows one of the many generalizations for rule in table 1. the last step of the algorithm is the selection of the best generalizations. each generalization is tested on the training corpus and an accuracy score l=wrong/matched is calculated. for each initial instance the k best generalizations are kept that:  1  report better accuracy;  1  cover more positive examples;
 1  cover different parts of input1;  1  have an error rate that is less than a specified threshold.
word indexconditionassociated informationactionwordlemmalexcatcasesemcattag1thetheartlow 1seminarseminarnounlow 1atatpreplow stime 11digitlow 1pmpmotherlowtimeid 1willwillverblow table 1: starting rule  with associated nlp knowledge  inserting  stime  in the sentence ``the seminar at  stime  1 pm will...''.
the other generalizations are discarded. retained rules become part of the best rules pool. when a rule enters the best rules pool  all the instances covered by the rule are removed from the positive examples pool  i.e. covered instances will no longer be used for rule induction   lp 1 is a covering algorithm . rule induction continues by selecting new instances and learning rules until the pool of positive examples is void.
word indexconditionactionwordlemmalexcatcasesemcattag1at  time 1digit  1timeid  table 1: a generalization for rule in table 1. the pattern is relaxed in length  conditions on words 1  1 and 1 were removed  and conditions on the other words were substituted by other constraints.
1.1 learning contextual rules
when applied on the test corpus  the best rules pool provides good results in terms of precision  but limited effectiveness in terms of recall. this means that such rules insert few tags  low recall   and that such tags are generally correct  high precision . intuitively this is because the absolute reliability required for rule selection is strict  thus only some of the induced rules will match it. in order to reach acceptable effectiveness  it is necessary to identify additional rules able to raise recall without affecting precision.  lp 1 recovers some of the rules not selected as best rules and tries to constraint their application to make them reliable. constraints on rule application are derived by exploiting interdependencies among tags. as mentioned   lp 1 learns rules for inserting tags  e.g.   speaker   independently from other tags  e.g.   /speaker  . but tags are not independent. there are two ways in which they can influence each other:  1  tags represent slots  therefore  tagx  always requires  /tagx ;  1  slots can be concatenated into linguistic patterns and therefore the presence of a slot can be a good indicator of the presence of another  e.g.  /speaker  can be used as  anchor tag  for inserting  stime 1. in general it is possible to use  tagx  to introduce  tagy .  lp 1 is not able to use such contextual information  as it induces single tag rules. the context is reintroduced in  lp 1 as an external constraint used to improve the reliability of unreliable rules. in particular   lp 1 reconsiders low precision non-best rules for application in the context of tags inserted by the best rules only. for example some rules will be used only to close slots when the best rules were able to open it  but not close it  i.e.  when the best rules are able to insert  tagx  but not  /tagx  . selected rules are called contextual rules. as example consider a rule inserting a  /speaker  tag between a capitalized word and a lowercase word. this is not a best rule as it reports high recall/low precision on the corpus  but it is reliable if used only to close an open  speaker . thus it will only be applied when the best rules have already recognized an open  speaker   but not the corresponding
 /speaker . area of application is the part of the text following a  speaker  and within a distance minor or equal to the maximum length allowed for the present slot1.  anchor tags  used as contexts can be found either to the right of the rule space application  as in the case above when the anchor tag is  speaker    or to the left as in the opposite case  anchor tag is  /speaker  . detailed description of this process can be found in  ciravegna 1a . reliability for contextual rules is computed by using the same error rate used for best rules  but only matches in controlled contexts are counted. in conclusion the sets of tagging rules  lp 1 induces are both the best rule pool and the contextual rules. figure 1 shows the whole algorithm for tagging rule induction.
loop for instance in initial-instances unless already-covered instance  loop for rule in generalise instance 
test rule  if best-rule  rule 
then insert rule  bestrules  cover rule  initial-instances 
else loop for tag in tag-list if test-in-context rule tag :right  then select-contxtl rule tag :right 
if test-in-context rule tag :left 
then select-contxtl rule tag :left figure 1: the final algorithm for rule tagging induction.
1 inducing correction rules
tagging rules when applied on the test corpus report some imprecision in slot filler boundary detection. a typical mistake is for example  at  time  1
 /time  pm   where  pm  should have been part of the time expression. for this reason  lp 1 induces rules for shifting wrongly positioned tags to the correct position.
it learns from the mistakes made in applying tagging rules on the training corpus. shift rules consider tags misplaced within a distance d from the correct position. correction rules are identical to tagging rules  but  1  their patterns match also the tags inserted by the tagging rules and  1  their actions shift misplaced tags rather than adding new ones. an example of an initial correction rule for shifting  /stime  in  at  stime  1  /stime  pm'' is shown in table 1.
the induction algorithm used for the best tagging rules is also used for shift rules: initial instance identification  generalization  test and selection.  wrong tag  and  correct tag  conditions are never relaxed. positive  correct shifts  and negative  wrong shifts of correctly assigned tags  are counted. shift rules are accepted only if they report an acceptable error rate.
1. extracting information
in the testing phase information is extracted from the test corpus in four steps: initial tagging  contextual tagging  correction and validation. the best rule pool is initially used to tag the texts. then contextual rules are applied in the context of the introduced tags. they are applied until new tags are inserted  i.e. some contextual rules can match also tags inserted by other contextual rules. then correction rules correct some imprecision. finally each tag inserted by the algorithm is validated. there is no meaning in producing a start tag  e.g.  speaker   without its corresponding closing tag   /speaker   and vice versa  therefore uncoupled tags are removed in the validation phase.
conditionadditional informationwordwrong tagcorrect taglemmalexcatcasesemcatat  atpreplow1 /stime  1digitlowpm  /stime pmotherlowtimeidtable 1: a a correction rule. the action  not shown  shifts the tag from the wrong to the correct position.
1. experimental results
 lp 1 was tested in a number of tasks in two languages: english and italian. in each experiment  lp 1 was trained on a subset of the corpus  some hundreds of texts  depending on the corpus  and the induced rules were tested on unseen texts. here we report about results on two standard tasks for adaptive ie: the cmu seminar announcements and the austin job announcements1. the first task consists of uniquely identifying speaker name  starting time  ending time and location in 1 seminar announcements  freitag 1 . table 1 shows the overall accuracy obtained by  lp 1  and compares it with that obtained by other state of the art algorithms.  lp 1 scores the best results in the task. it definitely outperforms other symbolic approaches  +1% wrt rapier califf 1   +1% wrt to whisk soderland 1    but it also outperforms statistical approaches  +1% wrt bwi  freitag and kushmerick 1  and +1% wrt hmm  freitag and mccallum 1  .  moreover  lp 1 is the only algorithm whose results never go down 1% on any slot  second best is bwi: 1% .
 lp 1bwihmmsrvrapierwhiskspeaker111111location111111stime111111etime111111all slots111111table 1: f-measure  β=1  obtained on cmu seminars. results for algorithms other than  lp 1 are taken from  freitag and kushmerick 1 .  we added the comprehensive all slots figure  as it allows better comparison among algorithms. it was computed by:
Σslot  f-measure * number of possible slot fillers 

	Σslot number of possible slot fillers	 1
concerning  lp 1 results from a 1 cross-folder experiment using half of the corpus for training. f-measure calculated via the muc scorer  douthat 1 . average training time per run: 1 min on a 1mhz computer. window size w=1.
a second task concerned ie from 1 job announcements taken from misc.jobs.offered  califf 1 . the task consists of identifying for each announcement: message id  job title  salary offered  company offering the job  recruiter  state  city and country where the job is offered  programming language  platform  application area  required and desired years of experience  required and desired degree  and posting date. the results obtained on such a task are reported in table 1.  lp 1 outperforms both rapier and whisk  whisk obtained lower accuracy than rapier  califf 1  . we cannot compare  lp 1 with bwi as the latter was tested on a very limited subset of slots. in summary   lp 1 reaches the best results on both the tasks.
slot  lp 1rapierbwislot lp 1rapierid1.1platform11title111application11company111area11salary11req-years-e11recruiter11des-years-e11state11req-degree11city11des-degree11country11post date11language11all slots11table 1: f-measure  β=1  obtained on the jobs domain using half of the corpus for training.
1. discussion
  lp 1's main features that are most likely to contribute to the excellence in the experiments are:  1  the induction of symbolic rules  see the conclusions    1  rule generalization via shallow nlp   1  the use of single tag rules and  1  the use of correction.
 lp 1 induces rules by instance generalization. generalization is also used in srv  rapier and whisk. it allows reducing data sparseness by capturing some general aspects beyond the simple flat word structure. shallow nlp is the basis for generalization in  lp 1. morphology allows overcoming data sparseness due to number/gender word realizations  while pos tagging information allows generalization over lexical categories. in principle such type of generalization produces rules of better quality than those matching the flat word sequence  rules that tend to report better effectiveness on unseen cases. this is because both morphology and pos tagging are generic nlp processes performing equally well on unseen cases; therefore rules relying on their results apply successful on unseen cases. this intuition was confirmed experimentally:  lp 1 with generalization   lp 1g  definitely outperforms a version without generalization   lp 1ng  on the test corpus  while having comparable results on the training corpus  +1% on the speaker field  +1% on the location field  +1% overall on the cmu task . moreover in  lp 1g the covering algorithm converges more rapidly than in  lp 1ng  because its rules tend to cover more cases. this means that  lp 1g need less examples in order to be trained  i.e.  rule generalization also allows reducing the training corpus size. not surprisingly the role of shallow nlp in the reduction of data sparseness is more relevant on semistructured or free texts  such as the cmu seminars  than on documents with highly standardized language  e.g. html pages  or the job announcement task . during the rule selection phase  lp 1 is able to adopt the right level of nlp information for the task at hand: in an experiment on texts written in mixed italian/english we used an english pos tagger that was completely unreliable on the italian part of the input.  lp 1g reached the same effectiveness of  lp 1ng  because the rules using the unreliable nlp information were automatically discarded. this shows that the use of nlp is always a plus  never a minus.
the separate recognition of tags is an aspect shared by bwi  while hhm  rapier and srv recognized whole slots and whisk recognizes multislots. separate tag identification allows further reduction of data sparseness  as it better generalizes over the coupling of slot start/end conditions. for example in order to learn patterns equivalent to the regular expression
 `at'|`starting from' digit `pm'|`am'    lp 1 just needs two examples  e.g.  `at'+`pm' and `starting from'+`am'  because the algorithm induces two independent rules for  time   `at' + `starting from'  and two for  /time   `am' + `pm' . in a slot-oriented rule learning strategy four examples  and four rules  will be needed  i.e. `at'+`pm'  `at'+`am'  `starting from' +`pm'  `starting from'+`am'. in a multislot approach the problem is worst and the number of training examples needed increases drastically  ciravegna 1a .
another reason for the good experimental results relies in the use of a correction step. correction is useful in recognizing slots with fillers with high degree of variability  such as the speaker in the cmu experiment   while it does not pay on slots with highly standardized fillers  such as many slots in the jobs task .  lp 1 using correction rules reports 1% more in terms of accuracy on  /speaker  than  lp 1 without correction. imprecision in tagging was also reported by  califf 1  who noted up to 1% imprecision on some slots  but she did not introduce any correction steps in rapier .
slot prerecf-measureslotprerecf-measurename11email11street11tel.11city11fax11prov.11zip11zip11table 1: results of a blind test on 1 resumees. this is not a simple named entity recognition task. a resumee may contain many names and addresses  e.g. previous work addresses  name of referees or thesis supervisors and their addresses . the system had to recognize the correct ones.
1. developing real world applications
 lp 1 was developed as a research prototype  but it quickly turned out to be suitable for real world applications. an industrial system based on  lp 1  learningpinocchio  was developed. recently learningpinocchio has been used in a number of industrial applications. moreover licenses have been released to external companies for further application development. this section reports about some industrial applications we have directly developed. the system is used for extracting information from professional resumees written in english. it is used on the results of a spider that surfs the web to retrieve professional resumees. the spider classifies resumees by topics  e.g. computer science . learningpinocchio extracts the relevant information and its output is used to populate a database. table 1 shows some results obtained in such task. application development time for the ie task required about 1 person hours for scenario definition and revision  the scenario was refined by tagging some texts in different ways and discussing among annotators . further 1 person hours were needed for tagging about 1 texts.  the rule induction process took 1 hours on a 1mhz machine  with window size w=1. finally system results validation required four person hours.
tagf 1 tagf 1 geograph area1organiz. name1currency1company sharestock exchange    name1    name1    type1    index1    category1all slots 1  table 1: results of blind test on financial news  1 texts .
two other applications were developed for kataweb  a major italian internet portal. the goal was to extract information from both financial news and classified ads written in italian and published on the portal pages. learningpinocchio is used both to generate hyperlinks for cross-referencing texts and to retrieve texts querying the content. the application is currently under final test at the customer's site. table 1 shows experimental results on financial news.
1. conclusions and future work
 lp 1 is a successful algorithm. on the one hand it outperforms the other state of the art algorithms on two very popular ie tasks. it is important to stress the fact that  lp 1 outperforms also statistical approaches  because in the last years the latter largely outperformed symbolic approaches. there is a clear advantage in using symbolic rules in real world applications. it is possible to inspect the final system results and manually add/modify/remove rules for squeezing additional accuracy  it was not done in the scientific experiments  but it was in the applications .
on the other hand  lp 1 was the basis for building learningpinocchio  a tool for building adaptive ie applications that is having a considerable commercial success. this shows that adaptive ie is able produce tools suitable for building real world applications by a final user by using only analyst's knowledge.
future work on  lp 1 will involve both the improvement of rule formalism expressiveness and the further use of shallow nlp for generalization. concerning the improvement in rule formalism expressiveness we plan to include some forms of kleene-star and optionality operators. such improvement has shown to be very effective in both bwi and rapier. concerning the use of shallow nlp for generalization  i.e.  one of the keys of the success in  lp 1  there are two possible improvements.  on the one hand  lp 1 will be used in cascade with a named entity recognizer  also implemented by using  lp 1 . this will allow further generalization over named entity classes  e.g.  the speaker is a person  so it is possible to generalize over such class in the rules . on the other hand  lp 1 is compatible with forms of shallow parsing such as chunking. it is then possible to preprocess the texts with a chunker and to insert tags only at the chunk borders. this is likely to improve precision in border identification.
an interesting question concerns the limits of the tagging-based ie approach used by many adaptive systems   lp 1 included. classic muc-like ie is based on template filling. template filling is more complex than tagging  as it implies to decide about both coreference of expressions  are  john a. smith  and  j. smith  the same person  two seminars have been identified in a text: are they separate events or are they coreferring    and slot pairing  two seminars and two speakers have been identified: which is the speaker of the first seminar  .  lp 1 is able to apply default strategies for template merging that solve simple cases of coreferences and slot pairing. such strategies are powerful enough to cope with many real world tasks  but in some other cases they are not effective enough. for example in the resumees application the customer was interested in retrieving also the triples degree/university/year. learningpinocchio was able to correctly highlight such information  but often it was not able to pair them correctly  therefore they were not used to populate the database  but only to index texts. even if some ad hoc strategies would have probably solved the problem in the specific case  it is quite clear that this is a major limitation in the approach. classical muc-like ie systems use sophisticated strategies for coreference resolution and template merging  very often based on a mix of nlp knowledge and domain knowledge  humphreys et al. 1 . two problems prevent the use of such techniques. on the one hand deep nlp is not effective in many applications in the internet realm  e.g. how can you parse an e-mail  . on the other hand it is not clear how to elicit the domain knowledge for coreference from an analyst  adaptability via analyst's knowledge is a strong constraint as mentioned above . adaptive template filling is an issue worth exploration that we are currently investigating. we work in the direction of further using shallow nlp for improving template filling and merging. to some extent this is also a step in the direction of bridging the gap between classical nlp based ie systems and fully adaptive systems.
acknowledgments
i developed  lp 1 and learningpinocchio at itc-irst  centro per la ricerca scientifica e tecnologica  trento  italy. learningpinocchio is property of itc-irst  see http://ecate.itc.it:1/cirave/learning/home.html.
the financial application mentioned above was jointly developed with alberto lavelli. thanks to daniela petrelli for revising this paper. errors  if any  are mine.
references
 califf 1  mary e. califf  relational learning techniques for natural language ie  ph.d. thesis  univ. texas  austin  www.cs.utexas.edu/users/mecaliff
 cardie 1  claire cardie  `empirical methods in information extraction'  ai journal  1   1  1.
 ciravegna et al. 1  fabio ciravegna  alberto lavelli  and giorgio satta  `bringing information extraction out of the labs: the pinocchio environment'  in ecai1  proc. of the 1th european conference on artificial intelligence  ed.  w. horn  amsterdam  1. ios press.
 ciravegna 1a  fabio ciravegna  `learning to tag for information extraction from text' in f. ciravegna  r. basili  r. gaizauskas  eds.  ecai workshop on machine learning for information extraction  berlin  august 1.
 www.dcs.shef.ac.uk/~fabio/ecai-workshop.html 
 douthat 1  aaron douthat  `the message understanding conference scoring software user's manual'  in  the 1th message understanding conf.  www.muc.saic.com
 freitag 1  dayne freitag  `information extraction from html: application of a general learning approach'  proc. of the 1th national conference on artificial intelligence  aaai-1   1.
 freitag and mccallum 1  dayne freitag and andrew mccallum: `information extraction with hmms and shrinkage'  aaai-1 workshop on machine learning for information extraction  orlando  fl  1  www.isi.edu/~muslea/rise/ml1ie/
 freitag and kushmerick 1  dayne freitag and nicholas kushmerick  `boosted wrapper induction'  in f. ciravegna  r. basili  r. gaizauskas  eds.  ecai1 workshop on machine learning for information extraction  berlin  1   www.dcs.shef.ac.uk/~fabio/ecai-workshop.html 
 grishman 1  ralph grishman  ` information extraction: techniques and challenges  . in information extraction: a multidisciplinary approach to an emerging information technology  in m.t. pazienza   ed.   springer  1.
 humphreys et al. 1  k. humphreys  r. gaizauskas  s. azzam  c. huyck  b. mitchell  h. cunningham  y. wilks: `description of the university of sheffield lasie-ii system as used for muc-1'. in proc. of the 1th message understanding conference  1  www.muc.saic.com .
 kushmerick et al. 1  n. kushmerick  d. weld  and r. doorenbos  `wrapper induction for information extraction'  proc. of 1th international conference on artificial intelligence  ijcai-1  1.
 miller et al. 1  s. miller  m. crystal  h. fox  l. ramshaw  r. schwartz  r. stone and r. weischedel  `bbn: description of the sift system as used for muc-1'  in proc. of the 1th message understanding conference  1  www.muc.saic.com .
  muslea et al. 1  i. muslea  s. minton  and c. knoblock  `wrapper induction for semi-structured  web-based information sources'  in proc. of the conference on autonomous learning and discovery conald-1  1.
  soderland 1  steven soderland  `learning information extraction rules for semi-structured and free text'  machine learning   1   1  1.
 yangarber et al. 1  roman yangarber  ralph grishman  pasi tapanainen and silja huttunen: ``automatic acquisition of domain knowledge for information extraction'' in proc. of coling 1  1th intern. conference on computational linguistics  saarbr┨cken  1.
relational learning via propositional algorithms: an information extraction case study 
	dan roth	wen-tau yih
department of computer science
university of illinois at urbana-champaign
{danr  yih} uiuc.eduabstract
this paper develops a new paradigm for relational learning which allows for the representation and learning of relational information using propositional means. this paradigm suggests different tradeoffs than those in the traditional approach to this problem - the ilp approach - and as a result it enjoys several significant advantages over it. in particular  the new paradigm is more flexible and allows the use of any propositional algorithm  including probabilistic algorithms  within it.
we evaluate the new approach on an important and relation-intensive task - information extraction - and show that it outperforms existing methods while being orders of magnitude more efficient.
1	introduction
relational learning is the problem of learning structured concept definitions from structured examples. relational representations use a first-order model to describe the problem domain and examples are given to the learner in terms of objects and object relations rather than by simple propositions.
　in a variety of ai problems such as natural language understanding related tasks  visual interpretation and planning  given a collection of objects along with some relations that hold among them  the fundamental problem is to learn definitions for some relations or concepts of interest in terms of the given relations. examples include the problem of identifying noun phrases in a sentence in terms of the information in the sentence  detecting faces in an image or defining a policy that maps states and goals to actions in a planning situation. in many of these cases it is natural to represent and learn concepts relationally; propositional representations might be too large  could lose much of the inherent domain structure and consequently might not generalize well. in recent years  this realization has renewed the interest in studying relational representations and learning.
　inductive logic programming  ilp  is an active subfield of machine learning that addresses relational learning and is a natural approach to apply to these tasks. while  in principle 

 
　　research supported by nsf grants iis-1 and iis1 and an onr muri award.
ilp methods could allow induction over relational structures and unbounded data structures  theoretical and practical considerations render the use of unrestricted ilp methods impossible. studies in ilp suggest that unless the rule representation is severely restricted the learning problem is intractable. while there are several successful heuristics for learning ilp programs  there are many practical difficulties with the inflexibility  brittleness and inefficient  generic  ilp systems. in most cases  researchers had to develop their own  problem specific  ilp systems  mooney  1  but have not always escaped problems such as search control and inefficiency especially in large scale domains like nlp.
　this paper develops a different paradigm for relational learning that allows the use of general purpose and efficient propositional algorithms  but nevertheless learns relational representations. our paradigm takes a fresh look at some of the restrictions ilp systems must make in order to work in practice and suggests alternatives to these; as a result  it enjoys several significant advantages over traditional approaches. in particular  the new paradigm is more flexible and allows the use of any propositional algorithm  including probabilistic algorithms  within it. it maintains the advantages of ilp approaches while allowing more efficient learning  improved expressivity and robustness.
　at the center of our paradigm is a knowledge representation language that allows one to efficiently represent and evaluate rich relational structures using propositional representations. this allows us to learn using propositional algorithms but results in relational concepts descriptions as outcome.
　this paper evaluates the new paradigm in the domain of information extraction  ie . this is the nlp task of extracting specific types or relevant items from unrestricted text. relational learning methods are especially appealing for learning in this domain since both the target concepts and the information in the domain  within and across documents  are often relational. we develop an ie system based on our paradigm; the learning component of our system makes use of a feature efficient learning algorithm that is especially suitable for the nature of our paradigm. however  we show that standard learning algorithms like naive bayes also work well with it. experimental comparisons show that our approach outper-
forms several ilp-based systems tried on this tasks  sometime significantly  while being orders of magnitude more efficient.
1	propositional relational representations
in this section we present a knowledge representation language that has two components:  1  a subset of first order logic  fol  and  1  a collection of structures  graphs  defined over elements in the domain.
　the relational language r is a restricted  function free  first order language for representing knowledge with respect to a domain d. the restrictions on r are applied by limiting the formulae allowed in the language to a collection of formulae that can be evaluated very efficiently on given instances  interpretations . this is done by  1  defining primitive formulae with limited scope of the quantifiers  def. 1 .  1  general formulae are defined inductively in terms of primitive formulae in a restricted way that depends on the relational structures in the domain. the emphasis is on locality with respect to these relational structures that are represented as graphs over the domain elements.
　this language allows the encoding of first order representations and relational structures as propositions and thus supports the use of better learning algorithms  including general purpose propositional algorithms and probabilistic algorithms over the elements of the language. this approach extends previous related constructions from ilp  lavrac et al.  1; khardon et al.  1  but technically is more related to the latter. in the rest of this section we present the main constructs of the language. we omit many of the standard definitions and concentrates on the unique characteristics of r. see  e.g.   lloyd  1  for general details.
　the vocabulary Σ consists of constants  variables  predicate symbols  quantifiers  and connectives.
definition 1 a primitive formula is defined inductively:  1  a term is either a variable or a constant.  1  let p be a k-ary predicate  t1 ，，， tk terms. then p t1 ，，， tk  is an atomic formula.  1  let f be an atomic formula  and z be a variable. then   zf  and   zf  are atomic formulae.  1  an atomic formula is a primitive formula.  1  if f and g are primitive formulae  then so are   f    f … g    f ‥ g .
notice that for primitive formulae in r the scope of a quantifier is always the unique predicate that occurs with it in the atomic formula. we call a variable-less atomic formula a proposition and a quantified atomic formula  a quantified proposition  khardon et al.  1 . the informal semantics of the quantifiers and connectives is as usual.
　before we move on to extend the language r we discuss the domain and the notion of an instance.
definition 1  domain  the language r is defined over a domain d which consists of a structured element d = hv gi where v is a collection of typed elements and g is a set of partial orders over v  specifically  each partial order gi （ g is an acyclic graph  vi ei   where vi   v and ei is a set of edges on vi . along with it  for each constant there is an assignment of an element in v and for each k-ary predicate  an assignment of a mapping from v k to {1}  {true  false} .
example 1 the fragment in fig. 1 forms a domain d =  v g   where v consists words time  :  1 :  1  pm and the phrase 1 : 1 pm and g consists of two linked lists  denoted in the solid line and the dashed line . in general gi

figure 1: a fragment of an article
could be more complex and represent  say  a parse tree over a sentence.
　the notion of types is used as a way to classify elements in the language according to their properties. in particular  we will think also of predicates as typed in the sense that their domain is typed. we distinguish within the set v two main types; a set o   v of objects and a set a   v of attributes. correspondingly  we define types of predicates. type 1 predicates take as their first argument an element in o and as their second argument an element in a. type 1 predicates describe properties of elements in o; thus p o a  may also be written as p o  = a  with the semantics that in a given interpretation  p o a  holds. all other predicate types will have elements in o as their arguments. these predicates will be defined via elements in g  and will have g （ g as their type. specifically  pg o1 o1  indicates that o1 o1 are nodes in the graph g （ g and there is an edge in g between them.
　elements in o represent objects in the world. for example  in nlp applications such as ours these might be words  phrases  sentences or documents. predicates of type 1 describe properties of these elements - spelling of a word  syntactic tag of a word  a phrase type  etc. the graphs in g describe relations between  sets of  objects - word w1 is before w1  w1 is the subject of the verb w1  etc.  it is possible to generalize g to a collection of hypoergraphs  but this is not needed in the current application.  as usual  the  world  in which we interpret the aforementioned constructs could be a single sentence  a document  etc.
definition 1 an instance is an interpretation lloyd  1  which lists a set of domain elements and the truth values of all instantiations of the predicates on them.
given an instance x  a formula f in r is given a unique truth value  the value of f on x  defined inductively using the truth values of the predicates in f and the semantics of the connectives. since for primitive formulae in r the scope of a quantifier is always the unique predicate that occurs with it in the atomic formula  we have the following properties. it will be clear that the way we extend the language  sec. 1  maintains this properties  proofs omitted .
proposition 1 let f be a formula in r  let x be an instance  and let tp be the time to evaluate the truth value of an atompp in f. then  the value of f on x can be evaluated in time p（f tp.
that is  f is evaluated simply by evaluating each of its atoms  ground or quantified  separately. this holds  similarly  for the following version of subsumption for formulae in r. proposition 1  subsumption  let x be an instance and let f : {1}n ★ {1} be a boolean function of n variables that can be evaluated in time tf. then the value of the clausep f f1 ...fn  on x can be evaluated in time tf + f tf  where the sum is over all n formulae that are arguments of f.
1	relation generation functions
the definition of the general formulae allowed in r will be operational and will use the structures in g. to do that we introduce a mechanism that generates more expressive formulae in a way that respects the structures in the domain  thus restricting the formulae generated.
definition 1 a formula in r maps an instance x to its truth value in x. it is active in x if it has truth value true in it. we denote by x the set of all instances - the instance space. a formula f （ r is thus a relation over x  f : x ★ {1}.
example 1 let instance x be the fragment illustrated in ex. 1. some active relations in x are word time   word pm   and number 1 .
given an instance  we would like to know what are the relations  formulae  that are active in it. we would like to do that  though  without the need to write down explicitly all possible formulae in the domain. this is important  in particular  over infinite domains or in problems domains such as nlp  where inactive relations vastly outnumber active relations.
definition 1 let x be an enumerable collection of relations on x. a relation generation function  rgf  is a mapping g : x ★ 1x that maps x （ x to a set of all elements in x that satisfy χ x  = 1. if there is no χ （ x for which χ x  = 1  g x  = φ.
rgfs can be thought of as a way to define  kinds  of formulae  or to parameterize over a large space of formulae. only when an instance x is presented  a concrete formula  or a collection of  is generated. an rgf can be thought of as having its own range x of relations.
example 1 it is impossible to list all formulae that use the number predicate in advance. however  rgf can specify formulae of this kind and  given the instance time : 1 : 1 pm  only the active relations of this kind: number 1  and number 1  - are generated.
　in order to define the collection of formulae in r we define the family of rgfs for r; the output of these define the formulae in r. rgfs are defined inductively using a relational calculus. the alphabet of this calculus consists of  i  basic rgfs  called sensors and  ii  a set of connectives. while the connectives are the same for every alphabet the sensors vary from domain to domain. a sensor is a way to encode basic information one can extract from an instance. it can also be used as a uniform way to incorporate external knowledge sources that aid in extracting information from an instance.
definition 1 a sensor is a relation generation function that maps an instance x into a set of atomic formulae in r. when evaluated on an instance x a sensor s outputs all atomic formulae in its range which are active.
example 1 following are some sensors that are commonly used in nlp.
  the word sensor over word elements  which outputs active relations word time   word :   word 1   word 1   and word pm  from  time : 1 : 1 pm .
  the length sensor over phrase elements  which outputs active relations len 1  from  1 : 1 pm .
  the is-a sensor  outputs the semantic class of a word.
  the tag sensor  outputs the part-of-speech tag of a word
the word and len sensors derive information directly from the raw data  while the is-a sensor uses external information sources such as wordnet and the tag sensor uses a prelearned part-of-speech tagger.
several mechanisms are used in the relational calculus to define the operations of rgfs. we mention here only the focus mechanism which is actually a binding mechanism that is used to define quantified formulae in r.
definition 1 let e be a set of elements in the domain. an rgf r is focused on e if  given an instance x  it generates only formulae in its range that are active in x due to elements in e. the focused rgf is denoted r e .
there are several ways to define a focus set. it can be done explicitly or using the structure g. the focus set is equivalent to the free variables in fol representations.
　the relational calculus allows one to inductively generate new rgfs by applying connective and quantifiers over existing rgfs. using the standard connectives one can define rgfs that output formulae of the type defined in def 1. details will not be given here. instead  we describe only one important type of operations within the relational calculus - structural operations. these operations exploit the structural  relational  properties of the domain as expressed in g in order to define rgfs. thus  more general formulae  that can have interactions between variables  are generated  while still allowing for efficient evaluation and subsumption  due to the graph structure. for example  the structural collocation operator  colloc  with respect to g is defined as follows.
definition 1 let s1 s1 ...sk be rgfs for r. collocg s1 s1 ...sk  is a restricted conjunctive operator that is evaluated on a chain of length k in g. specifically  let d =  v g  be a domain  with g （ g  and v1 v1 ...vk a chain in g. the formulae generated by collocg s1 s1 ...sk  are those generated by s1 v1 &s1 v1 &...&sk vk   where  1  by sj vj  we mean that the rgf sj is focused on {vj}  1  the & operator means that formulae in the output of  s&r  are active formulae of the form f…g  where f is in the range of s and g is in the range of r  evaluated on x . this is needed since each rgf in the conjunction may produce more then one formulae.
example 1 when applied with respect to the graph g which represents the linear structure of the sentence  collocg simply generates formulae that corresponds to ngrams. e.g.  given the fragment  dr john smith   rgf colloc word  word  extracts the bigrams word dr -word john  and word john -word smith .
　similarly to collocg one can define a sparse collocation operator with respect to a chain in g. this is also a restricted conjunctive operator that is evaluated on a chain in g with the following difference. formulae are generated by scollocg s1 s1 ...sk  as follows: let v1 v1 ...vn be a chain in g. for each subset vi1 vi1 ...vik of elements in v  such that ij   il when j   l  all the formulas: s1 vi1 &s1 vi1 &...&sk vik   are generated.
　notice that while primitive formulae in r have a single predicate in their scope  the structural properties provides a way to go beyond that but only in a restricted way that is efficiently evaluated. structural operations allow us to define rgfs that constrain formulae evaluated on different objects without incurring the cost usually associated with enlarging the scope of free variables. this is done by enlarging the scope only as required by the structure of the domain  modeled by g. this allows for efficient evaluation as in prop. 1  1 with the only additional cost being that of finding chain in the graph  details omitted .
1	comparison to ilp methods
propositional learning on relational features provides a different paradigm for relational learning. while  in principle  ilp methods could allow induction over relational structures and unbounded data structures and their expressivity cannot be matched by propositional methods  theoretical and practical considerations render the use of unrestricted ilp methods impossible. studies in ilp suggest that  unless the rule representation is severely restricted  the learning problem is intractable  kietz and dzeroski  1; cohen  1; cohen and page  1 . several successful heuristics for learning ilp programs  muggleton and de raedt  1; cussens  1; quinlan  1  have made different algorithmic and representational restrictions in order to facilitate ilp although there are still many practical difficulties with the inflexibility  brittleness and inefficient  generic  ilp systems.
　our approach offers different tradeoffs than those suggested by the common restrictions made by current ilp systems. while we cannot say that our paradigm dominates the traditional ilp approach in general  we believe that in many cases the alternatives it offers provide a better way to address relational learning  especially in large scale domains such as nlp. below we address some key issues that could help in developing a better understanding to the suitability of each. search: the key difference between the traditional ilp approach and ours is the way they structure the search space. in ilp   features  are generated as part of the search procedure in an attempt to find good bindings. in our case  the  features  tried by an ilp program during its search are generated up front  in a data driven way  by the rgfs. some of these are grounded and some have free variables in them. the learning algorithm will look at all of them  in parallel  and find the best representation. search control methods used by ilp methods are thus analogous to the expressivity we give to our rgfs. the order of  visiting  these features is different. knowledge: one of the key cited advantages of ilp methods is the ability to incorporate background knowledge. in our paradigm  this is incorporated flexibly using the notion of sensors. sensors allow us to treat information that is readily available in the input  external information or even previously learned concepts in a uniform way.
expressivity i : the basic building blocks of the representations we use  our formulae  are the same as those used by ilp representations. as presented in sec. 1  for a predicate r and elements a b we are not representing only the ground term r a b  but also r x y   r x b   etc. this is similar to the work in  lavrac et al.  1  only that our structural operations allow us to avoid some of the determinacy problems of that approach.
learning: the relational features generated by our rgfs provide a uniform domain for different learning algorithms. applying different algorithms is easy and straightforward. moreover  it is straightforward to use probabilistic models over this representation and in this way it provides a natural and general way of using relational representations within a probabilistic framework. on the other hand  it turns out that  generic  ilp methods suffer brittleness and inefficiency and in many cases  researchers had to develop their own  problem specific  ilp systems  mooney  1 .
　in particular  while time complexity is a significant problem for ilp methods  propositional learning is typically a lot more efficient. in our paradigm  due to the fact that our rgfs will generate a very large number of relational features  see  search  above  we adopt a specific learning methodology  following  khardon et al.  1 . while this is not necessary  as shown in sec. 1  we discuss this direction next. expressivity  ii : several issues can be mentioned in the context of using linear threshold functions as concept representation over the relational features extracted using rgfs  khardon et al.  1 . advocates of ilp methods suggest that the rich expressive power of fol provides advantages for knowledge-intensive problems such as nlp  mooney  1 . however  given strong intractability results  practical systems apply many representational restrictions. in particular  the depth of the clauses  the number of predicates in each clause  is severely restricted. thus  the learned concept is actually a k-dnf  for small k. in our paradigm  the constructs of colloc and scolloc allow us to generate relational features which are conjunctions of predicates and are thus similar to a clause in the output representation of an ilp program. while an ilp program represents a disjunction over these  a linear threshold function over these relational features is more expressive. in this way  it may allow learning smaller programs. the following example illustrates the representational issues:
example 1 assume that in several seminar announcements  fragments that represent speaker have the pattern:
，，， speaker : dr fname lname line-feed ，，，
an ilp rule for extracting speaker could then be:
before targ 1   speaker   … contains target   dr   … after targ 1  line-feed  ★speaker target 
that is  the second word before the target phrase is  speaker   target phrase contains word  dr   and the  linefeed  character is right after the target phrase. in our relational feature space all the elements of this rule  and many others  would be features  but the above conjunction is also a feature. therefore a collection of clauses of this form becomes a disjunction in our feature space and will be learned efficiently using a linear threshold element.
finally  we mention that for learning linear threshold elements there exist feature efficient algorithms  littlestone  1  that are suitable for learning in nlp-like domains  where the number of potential features is very large  but only a few of them are active in each example  and only a small fraction of them are relevant to the target concept.
1	case study - information extraction
information extraction  ie  is a natural language processing  nlp  task that processes unrestricted text and attempts to extract specific types of items from the text.
　this form of shallow text processing has attracted considerable attention recently with the growing need to intelligently process the huge amounts of information available in the form of text documents. while learning methods have been used earlier to aid in parts of an ie system  riloff  1; soderland and lehnert  1   it has been argued quite convincingly  califf and mooney  1; craven and slattery  1  that relational methods are necessary in order to learn how to directly extract the desired items from documents. the reason is that the target concepts require the representation of relations over the source document and learning those might require induction over structured examples and fol representations. indeed  previous works  califf and mooney  1; freitag  1  have demonstrated the success of ilp methods in this domain. this is therefore an ideal domain to study our proposed relational learning paradigm.
1	problem description
in this paper  the ie task is defined as locating specific fragments of an article according to predefined slots in a template. each article is a plain-text document that consists of a sequence of tokens. specifically  the data used in experiments is a set of 1 seminar announcements from cmu1. the goal is to extract four types of fragments from each article1 - those describing the start time  stime  and end time  etime  of the seminar  its location  location  and the seminar's speaker  speaker . given an article  our system picks at most one fragment for one slot. if this fragment represents the slot  then it is a correct prediction. otherwise  it is a wrong prediction  including the case that the article doesn't contain the slot at all   freitag  1 .
1	extracting relational features
the basic strategy of our ie solution is to learn a classifier that discriminates a specific desired fragment. first  we generate examples for this type of fragment. this is done by:
1. identifying candidate fragments in the document.  all fragments are candidates; in training  fragments are annotated.  note: fragments may overlap  and only a small number of them contain desired information.
1. for each candidate fragment  use the defined rgf features to re-represent it as an example which consists of all active features extracted for this fragment.
let f =  ti ti+1 ，，， tj  be a fragment  with ti representing tokens and i  i+1  ，，，  j are positions of tokens in the document. our rgfs are defined to extract features from three regions: left window  ti w ，，， ti 1   target fragment ti ，，， tj   and right window  tj+1 ，，， tj+w   where w is the window size.
　the domain formed by these three regions contains two types of elements - word elements  e.g.  ti w ，，， tj+w  and one phrase element  the target region . the rgfs are focused either on a specific word element  one free variable  or on the borders of the phrase element  two free variables  and define relational features relative to these. in the next section we provide examples of rgfs used in experiments.
1	two-stage architecture
once examples are generated  the ie task is accomplished by two learning stages. the same classifier  snow  is used in both stages  but in a slightly different way.
　snow  roth  1; carleson et al.  1  is a multi-class classifier that is specifically tailored for large scale learning tasks. the snow learning architecture learns a sparse network of linear functions  in which the targets  fragment types  in this case  are represented as linear functions over a common feature space. snow has already been used successfully for a variety of tasks in natural language and visual processing  golding and roth  1; roth et al.  1 . snow is built on a feature efficient learning algorithm  winnow  littlestone  1  and therefore is an ideal learning approach to complement our paradigm  as discussed before. while snow can be used as a classifier and predicts using a winner-takeall mechanism over the activation values of the target classes  here we rely directly on the activation value it outputs  computed using a sigmoid function over the linear sum. the normalized activation value can be shown to be a distribution function and we rely heavily on it's robustness in our twostage architecture. the two stages are  1  filtering: reduce the amount of candidates from all possible fragments to a small number  and  1  classifying: pick the right fragment from the preserved fragments by the learned classifier. theoretical justification for this architecture will be presented in a companion paper. intuitively  this architecture increases the expressivity of our classification system. moreover  eliminating most of the negative examples significantly reduces the number of irrelevant features  an important issue given the small data set.
filtering: one common property of ie tasks is that negative examples  irrelevant fragments  extremely outnumber positive examples  fragments that represent legitimate slots . in the seminar announcements data set  for example  1% of the fragments represent legitimate slots. this stage attempts to filter out most of the negative examples without eliminating positive examples. it can also be viewed as a classifier designed to achieve high recall  while the classifier in the second stage aims at high precision. the filter consists of two learned classifiers; a fragment is filtered out if it meets one of the following criteria:
1. single feature classifier: fragment doesn't contain a feature that should be active in positive examples.
1. general classifier: fragment's confidence value is below the threshold.
systemstimeetimelocspeakerprecrecf1precrecf1precrecf1prec	rec	f1snow-ie111111111111nb-ie111111111111rapier-wt111111111111rapier111111111111srv111111111111whisk111111111111table 1: results for seminar announcements task　for criterion 1  it turns out that there exists some features that are  almost  always active in positive examples. for example  in our experiments  the length of fragments satisfies this:  len fragment  ＋ 1  is always satisfied by stime fragments. also   fragment contains a word that is a noun  always holds in speaker fragments.
　for criterion 1  implemented using snow  relying on its robust confidence estimation  the problem becomes finding the right threshold. minimum activation values of positive examples in training data are used as thresholds  for the different types of slots . examples with lower activation values are filtered out.
　the two stages also differ in the rgfs used. the following  more crude rgfs are used at the filtering stage:
  target region: word  tag  word&tag  colloc word  word   colloc word  tag   colloc tag  word   colloc tag  tag  on word elements  and len on the phrase element.
  left & right window: word&loc  tag&loc  and word&tag&loc  where loc extracts the position of the word in the window.
classifying: fragments that survived the filtering stage are then classified using a second snow classifier  for the four slots. first  an additional collection of rgfs is applied to enhance the representation of the candidate fragments  thus allowing for more accurate classification. as before  in training  the remaining fragments are annotated and are used as positive or negative examples to train the classifiers. in testing  the remaining fragments are evaluated on the learned classifiers to determine if they can fill one of the desired slots. in this stage 1 different classifiers are trained  one for each type of fragments. all examples are run through all 1 classifiers. the rgfs added in this stage include: for etime and stime:
scolloc word&loc -1  l window   word&loc r window    scolloc word&loc -1  l window   tag&loc r window  
for location and speaker:
scolloc word&loc -1  l window   tag targ   tag&loc 1  r window  
　the first set of rgfs is a sparse structural conjunction of the word directly left of the target region  and of words and tags in the right window  with relative positions . the second is a sparse structural conjunction of the last two words in the left window  a tag in the target  and the first tag in the right window.
　a decision is made by each of the 1 classifiers. a fragment is classified as type t if the tth classifier decides so. at most one fragment of type t is chosen in each article  based on the activation value of the corresponding classifier.
slotpass training pass testing loss testing stime1%1%1%etime1%1%1%location1%1%1%speaker1%1%1%table 1: filtering efficiency
1	experimental results
our experiments use the same data  methodology and evaluation metrics used by several ilp-based ie systems in previous works. the systems such as rapier  califf and mooney  1   srv  freitag  1   and whisk  soderland  1  have also been tested on this data set. the data  1 documents  is randomly split into two sets of equal sizes  one for training and the other for testing. the reported results are an average of five runs. as usual  the performance is quantified in terms of precision  p  - the percentage of correct predictions - and recall  r  - the percentage of slots that are identified. we also report the  . the results of our system  snow-ie  are shown in the first row of table 1 along with the results of several other ilp-based ie systems that were tested on this task under the same conditions. an exception is whisk  for which the results are from a 1-fold validation using only 1 documents randomly selected from the training set. the systems also use somewhat different information sources. the words in the documents are used by all systems. part-of-speech tags are used both in rapierwt and snow-ie; srv uses other predicates that capture pos information to some extent. a version of rapier uses also semantic information; this can be done in our system by adding  say  an is-a sensor but  given their results we did not incorporate this information.
　in addition to our snow-ie we have also experimented with a second propositional algorithm  the naive bayes  nbie  algorithm. nb was used on exactly the same set of features  same examples  that were generated using our relational paradigm for snow  and in exactly the same way. although the results of nb-ie are not as good as those of snow-ie - the quality of the classifier is certainly an important issue - the experiments with a second propositional algorithm exhibit the fact that our relational paradigm is a general one. as indicated in  craven and slattery  1; freitag  1  a simple minded use of this algorithm is not competitive for this task; but on top of a paradigm that is able to exploit the relational nature of the data it compares favorably with ilp methods. overall  snow-ie outperforms the existing rule-based ie systems on all the four slots.
　to clarify  we note that the output representation of our system makes use of similar type of relational features as do the ilp-based systems  only that instead of a collection of conjunctive rules over these  it is represented as a linear function.
　it is difficult to isolate the contribution of our two-stage architecture to the quality of the results. we believe  though  that the ease of incorporating this and other learning architectures is an indication to the flexibility of the approach and the advantages of learning with propositional means. table 1 gives some insight into that  by showing the average performance of the filtering stage. the first two columns show the ratio of training and testing examples that pass the filter. the third column lists the ratio of positive examples in the testing set that are filtered out. these fragments do not even reach the second stage.  since an article may contain more than one fragment that represents the same slot  it is sometimes still possible for the classifier to pick the correct slot. 
1	conclusion
the use of relational methods is back in fashion. it became clear that for a variety of ai problems there is a fundamental need to learn and represent relations and concepts in terms of other relations. information extraction - the task of extracting relevant items from unrestricted text is one such task.
　this paper suggests a new paradigm for relational learning - which allows for the representation and learning of relational information using propositional means. we argue that our paradigm has different tradeoffs than the traditional approach to this problem - the ilp approach - and as a result it enjoys several significant advantages over it. in particular  the suggested paradigm is more flexible and allows the use of any propositional algorithm within it  including probabilistic approaches. as such  it addresses in a natural and general way the problem of using relational representations within a probabilistic framework  an important problem which has been studied a lot recently.
　our paradigm is exemplified on an important task - information extraction  ie . based on our paradigm  we developed a new approach to learning for ie  and have shown that it outperforms existing ilp-based methods. moreover  it is several orders of magnitude more efficient. we believe that this work opens up several directions for further work - on relational learning and knowledge representation and on practical and efficient solutions to nlp and ie problems.
references
 califf and mooney  1  m. califf and r. mooney. relational learning of pattern-match rules for information extraction. in national conference on artificial intelligence  1.
 carleson et al.  1  a. carleson  c. cumby  j. rosen  and d. roth. the snow learning architecture. technical report uiucdcs-r-1  uiuc cs dept.  may 1.
 cohen and page  1  w. cohen and d. page. polynomial learnability and inductive logic programming: methods and results. new generation computing  pages 1  1.
 cohen  1  w. cohen. pac-learning recursive logic programs: negative result. journal of artificial intelligence research  1-1  1.
 craven and slattery  1  m. craven and s. slattery. relational learning with statistical predicate invention: better models for hypertext. machine learning  1-1  1.
 cussens  1  j. cussens. part-of-speech tagging using progol. in international workshop on inductive logic programming  pages 1  prague  czech republic  1. springer. lnai 1.
 freitag  1  d. freitag. machine learning for information extraction in informal domains. machine learning  1/1 :1- 1  1.
 golding and roth  1  a. r. golding and d. roth. a winnow based approach to context-sensitive spelling correction. machine learning  1-1 :1  1.
 khardon et al.  1  r. khardon  d. roth  and l. g. valiant. relational learning for nlp using linear threshold elements. in proc. of the international joint conference of artificial intelligence  pages 1  1.
 kietz and dzeroski  1  j. kietz and s. dzeroski. inductive logic programming and learnability. sigart bulletin  1 :1- 1  1.
 lavrac et al.  1  n. lavrac  s. dzeroski  and m. grobelnik. learning nonrecursive definitions of relations with linus. in machine learning  ewsl-1   volume 1 of lnai  pages 1- 1  porto  portugal  1. springer verlag.
 littlestone  1  n. littlestone. learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. machine learning  1-1  1.
 lloyd  1  j. w. lloyd. foundations of logic progamming. springer-verlag  1.
 mooney  1  raymond j. mooney. inductive logic programming for natural language processing. in  ilp-1   volume 1 of lnai  pages 1. springer  1.
 muggleton and de raedt  1  s. muggleton and l. de raedt. inductive logic programming: theory and methods. journal of logic programming  1-1  1.
 quinlan  1  j. r. quinlan. learning logical definitions from relations. machine learning  1-1  1.
 riloff  1  e. riloff. automatically constructing a dictionary for information extraction tasks. in national conference on artificial intelligence  pages 1  1.
 roth et al.  1  d. roth  m-h. yang  and n. ahuja. learning to recognize objects. in cvpr'1  1.
 roth  1  d. roth. learning to resolve natural language ambiguities: a unified approach. in national conference on artificial intelligence  pages 1  1.
 soderland and lehnert  1  s. soderland and w. lehnert. wrap-up: a trainable discourse module for information extraction. journal of artificial intelligence research  1-1  1.
 soderland  1  s. soderland. learning information extraction rules for semi-structured and free text. machine learning  1 :1  1.
	thales research and technology	laboratoire d'informatique de paris-nord
domaine de corbeville  f-1 orsay	  av. j.-b. cl└ment  f-1 villetaneuse  france
thierry.poibeau thalesgroup.comthis paper presents a multi-domain information extraction system. in order to decrease the time spent on the elaboration of resources for the ie system and guide the end-user in a new domain  we suggest to use a machine learning system that helps defining new templates and associated resources. this knowledge is automatically derived from the text collection  in interaction with the end-user to rapidly develop a local ontology giving an accurate image of the content of the text. the system is finally evaluated using classical indicators.
information extraction  ie  is a technology dedicated to the extraction of structured information from texts. this technique is used to highlight relevant sequences in the original text or to fill pre-defined templates  pazienza  1 . below is the example of a story concerning a terrorist attack in spain:
1 ao t 1  page 1
espagne un mort au cours d'un attentat ┐ la voiture pi└g└e au pays basque.
- une personne a └t└ tu└e mardi 1 ao t en milieu de journ└e ┐ oyarzun  province basque de guipuzcoa  au cours de l'explosion d'une voiture pi└g└e dans le parking d'un hypermarch└  a indiqu└ la
1
police.  afp. 
                                                          
1  this is the translation of the text:
august 1th  1  page 1
spain: one person killed in a booby-trapped car attack in the basque country
one person was killed on tuesday august 1st during the day in oyarzun  basque province from guipuzcoa  in the explosion of a booby-trapped car on the parking of a supermarket  indicated the police.  afp 
and the corresponding entry in the database filled by the ie system.
 1 ao t 1
 espagne  pays basque
 1
 1
 voiture pri└g└e
even if ie seems to be now a relatively mature technology  it suffers from a number of yet unsolved problems that limit its dissemination through industrial applications. among these limitations  we can consider the fact that systems are not really portable from one domain to another. even if the system is using some generic components  most of its knowledge resources are domain-dependent. moving from one domain to another means re-developing some resources  which is a boring and time-consuming task  for example riloff  mentions a 1 hours development .
　moreover  when information is often changing  think of the analysis of a newswire for example   one might want to elaborate new extraction templates. this task is rarely addressed by the research studies in ie system adaptation  but we noticed that it is not an obvious problem. people are not aware of what they can expect from an ie system  and most of the time they have no idea of how deriving a template from a collection of texts can be.  on the other hand  if they defined a template  the task cannot be performed because they are waiting for information that is not contained in the texts.
　in order to decrease the time spent on the elaboration of resources for the ie system and guide the end-user in a new domain  we suggest to use a machine learning system that helps defining new templates and associated resources. this knowledge is automatically derived from the text collection  in interaction with the end-user to rapidly develop a local ontology giving an accurate image of the content of the text. the experiment also aims at reaching a better coverage thanks to the generalization process provided by the machine learning system.
　we will firstly present the overall system architecture and principles. the learning system is then what allows the learning of semantic knowledge to help define templates for new domains. we will show to what extent it is possible to speed up the elaboration of resources without any decrease in the quality of the system. we will finish with some comments on this experiment and we will show how domain-specific knowledge acquired by the learning system such as the subcategorization frame of verbs could be used to extract more precise information from texts.
the system can be divided into three main parts:
1. a machine learning engine used to produce semantic clusters from the corpus. these clusters are weighted and are intended to give to the expert a rough idea of the topic addressed in the text;
1. a system to help the creation of extraction template once the relevant topic of the corpus have been identified;
1. the information extraction system itself  that will use the resources defined at the previous stage to fill the templates.

architecture of the system
the above schema gives an overview of the overall architecture. the corpus is processed by the machine learning system  1   in order to produce semantic clusters organized in a superficial ontology. the template creation module  1  helps the expert define his own extraction template from the ontology. the lower part of the schema describes the information extraction system itself  1   processing a text to fill the extraction template.
　the information extraction system consists in a multiagent platform. each agent performs a precise subtask of the information extraction process  named entity recognition  relation analysis  template filling . a supervisor controls the overall process and the information flow  for more detail about the system architecture  see  poibeau 1  .
　apart from the information extraction system itself  the machine learning module is intended to help the end-user produce the extraction template. a representative set of documents has to be processed by the learning module to obtain an ontology and some rough resources for the domain he wants to cover. the resources have to be manually completed to obtain a good coverage of the domain.
the acquisition method combines knowledge contained in on-line resources and statistical data obtained from the training corpus of the chosen domain. this method is implemented through the lexicontuner  which was initially developed for french but can be used for any language if correct resources are provided. the method is inspired from  luk 1  and described in details in  ecran 1  and  poibeau 1 .
　semantic clusters - that is to say clusters of semantically related words - can be acquired from on-line resources including dictionaries  thesauri and taxonomies  wilks 
　  1 . dictionaries model the meanings of lexical items using textual definitions. textual definitions are written in natural language and the full information encoded in these definitions can't be extracted easily. however  partial information  which can be used as semantic clusters  can be extracted from the definitions relatively easily.
　the idea of building networks from definitions was first proposed by v└ronis and ide   along with a propagation technique for computing clusters of related words. many authors have proposed techniques for deriving some kind of clusters or associations of concepts from graphs of concepts  among others transitive closures and computation of graph  cliques   simulated annealing  etc. some of these experiments derived information from a dictionary; see for example the early work at new mexico state university  fowler and dearholt  1 . in our approach  the content words in the definitions of a word sense are used as component concepts of the word sense  luk  1   that is to say that a concept is a lexical item considered as a member of a semantic cluster. for example  the word   which is defined as  dramatic performance or composition of which music is an essential part...  in the
  can be assigned the semantic
concepts of dramatic  performance  composition and music.
　in the lexicontuner  the generation of the clusters is carried out in two steps. firstly  the list of all the concepts occurring in the corpus is generated. for each word in the corpus  the program looks for its textual definition s  in the lexicon. the words in the definitions are treated as concepts and are recorded. this step allows us to go from a french word  e.g.   to some concepts labeled with english words  dramatic  performance  composition  music   by means of a bilingual dictionary   being defined as a  dramatic performance or composition of which music is an essential part  .
　secondly  semantic clusters are generated by clustering related concepts in the list. the program uses a semantic net as its semantic knowledge source to determine the semantic relatedness between the concepts. the net is derived from the 	 oald . each node represents a word  and for any two words  and    is linked to  if  appears in the definitions of  in
oald  music will be linked to composition if music appears in the definition of composition . the link from to  is weighted inversely proportionally to the product of the number of senses of  as defined in oald and the number of words in the definition of  that contains . the clusters are formed in 1 steps:  
  every circle in the semantic net which length is equal to a user-defined number is identified. for example  if the number is 1  the system will generate all the sets consisting of three related words according to the dictionary definition. for opera  the following associations will be considered:  opera  performance  dramatic  and  dramatic  composition  music ..
  circles that share one or more nodes are merged. thus 
 opera  performance  dramatic  and  dramatic  composition  music  are merged into a single set
 opera  performance  dramatic  composition  music . this is called a  core cluster .
  lastly  peripheral words  words which are part of a circle which length is inferior to the user-defined number  are related to the core cluster. if two words like dramatic and comic are related  then comic will be added to the cluster  being related to dramatic.
this algorithm does not really capture all the complexity of dictionary definitions. for example  although cycles are common among dictionary definition terms  there may also be chains of words which are highly related but which do not form a cycle. to solve this problem   lesk  1  suggested a method to cluster word pairs based on  degree of overlap  between their head words or definition words  in the context of word sense disambiguation. in our system  the merging step and the addition of peripheral words partially allow taking into account chains of words.
　lastly  the membership of a cluster is  weighed . the weight of a core member is considered as the inverse of the number of senses of the word as defined in oald. the weight of a non-core member is considered as the mean of the weights of the core members of the cluster to which it is related. then  semantic clusters can capture notions appearing in texts independently from the lexical items expressing these notions.
not all semantic clusters are equally useful for information extraction in any particular domain. the usefulness of a semantic cluster can be determined statistically from the corpus itself.  salton  1  suggested measures that reveal the usefulness of a term: applied to the members of a cluster  these measures reveal the usefulness of a semantic cluster in the corpus  resnik  1   aguirre & rigau  1 .
　the ie prototype for french implemented a method to measure the usefulness of semantic clusters using a single function which captures the variance of the occurrence frequency  luk  1 . the variance of the occurrence frequency of a semantic cluster is given as:
         =          -          1 
where       is the occurrence frequency of  in a collection of articles  and     is the mean of  for any .
　the measurement captures both the discriminatory power and the relevancy of a semantic cluster: if the occurrence frequency of a semantic cluster has a high variance  then it must be both discriminative and relevant. if a semantic cluster has low relevancy  i.e. the cluster has low occurrence frequency in most of the articles   then the variance of occurrence frequency will be low relatively to a cluster with a higher frequency. meanwhile  if a semantic cluster has low discriminatory power  i.e. the semantic cluster has similar occurrence frequency in most articles   then the variance of occurrence frequency will also be low. lastly  it also takes into account the dependency of the discriminatory power on the occurrence frequencies  normalized according to the size of the document. the weight of a cluster is then normalized according to the size of the document using the following function:

if n d  − 1 and
	 	 *log 1/     +1
otherwise.
where 	 and 	 are the normalized and non-
normalized weights of the feature  in document   and is the number of content words in document . the use of a logarithmic expression allows to smooth the result in spite of the differences in the size of the documents. the constant 1 is the average size of a document in the corpus  without taking into account empty words.
　after a rough discrimination between the more useful semantic clusters and the less useful ones obtained by these statistical methods  domain experts can manually refine the selections of the semantic clusters  which are to be used in the information extraction system. lastly a set of representative clusters is associated to each text. these clusters have a higher weight in the text than in the corpus.
semantic classes produced by the lexicontuner are proposed to the end-user  who chooses which clusters are of interest to him. once he has chosen one cluster  the system automatically proposes him an interface to refine the cluster  aggregate a part of the closest clusters and develop a hierarchy. this hierarchy can be assimilated to a local ontology  describing a part of the world knowledge related to the event of interest for the end-user.

from corpus to template  through the ontology...
　the semantic clusters produced by the lexicontuner give to the expert a rough idea of the topics addressed in the corpus. the expert has to navigate among these clusters to select relevant topics and establish a candidate template. the chosen topics has to be named and to be associated with a slot in the template. the system automatically generates the corresponding information into the database: the creation of a new template leads to the creation of a new table and each new slot in the template corresponds to a new column in the table. this technical part of the template creation process can be compared to the tabula rasa toolkit developed at new mexico state university to help endusers define their own templates  ogden and bernick  1 .
　the evaluation of the template creation task is not obvious since it necessitates domain knowledge and text manipulation from the experts. no clear reference can be established. the only way to evaluate the contribution of the semantic clusters is to ask the expert firstly to manually elaborate templates from a corpus and secondly to do it with the help of the lexicontuner. the expert we worked with made the following comments:
  the 	semantic 	clusters 	produced 	by 	the
lexicontuner give an appropriate idea of the overall topic addressed by the texts.
  these clusters help the elaboration of templates and allows to focus on some part of information without reading large part of texts.
  however  the elaboration of the template itself remains largely dependant of the domain knowledge of the expert because  a  he knows what kind of information he wants to find in relation with a given topic and  b  the clusters are too coarse-grain to directly correspond to slots.
when the template corresponds to an event  the information to be found generally refers to classical : who  what  where and when. some additional slots can be added but most of the time they correspond to classical associations of ideas. for example  if one wants to extract information about football matches  he will immediately create a slot corresponding to the score. however  this comment is due to the fact that the system is analyzing news stories  for which one can associate stereotypic reduced templates  sometimes called  in the ie community .

 a specific reduced template   a  templette  
　we observed that the template creation task is frequently a problem in more technical domains for which no clear schema exists. in this experiment  the lexicontuner can be seen as a tool to explore the corpus and give a rough idea of tentative templates rather than a tool designed to help the creation of the content of the templates themselves. however  the lexicontuner results is also useful  for the creation of the resources of the system.
we asked an expert to define a new template and the associated resources  using the tools we presented above. we chose the terrorist event domain from the afp newswire  because it is a well-established task since the muc-1 and muc-1 conferences. moreover  similar experiments has been previously done that give a good point of reference  see  poibeau 1  and  faure and poibeau 1  .
once a new template is designed  the developer has to elaborate resources for this template. for a more detailed description of this process  resource design for a single template   please refer to  faure and poibeau 1 . we only describe here the main steps of this process.
　homogeneous semantic clusters learned by the lexicontuner are refined: a manual work of the expert is necessary to exploit semantic classes  merging of scattered classes  deletion of irrelevant elements  addition of new elements  etc. . about five hours have been dedicated  after the acquisition process  to the refinement of data furnished by lexicontuner. merging and structuring classes incrementally develop a local ontology  which nodes are related to slots in the extraction template. this knowledge is also considered as a resource for the finite-state system  intex cf.  silberztein  1   and is exploited either as dictionaries or as transducers  according to the nature of the information. if it is a general information that is not domain specific  the development guidelines advise the user to use dictionaries that can be reused  otherwise  he designs a transducer.
　a dictionary is a list of words or phrases  each one being accompanied by a tag and a list of features. the first names dictionary or the location dictionary are generic reusable resources. below is a sample of the location names dictionary1:
abidjan .n+loc+city afghanistan .n+loc+country
allemagne .n+loc+country allemagne de l'ouest .n+loc+country
allemagne de l'est .n+loc+country
these items structured in a list are convenient for the dictionary format and the semantic lists elaborated from the lexicontuner complete in an accurate manner the coverage of the initial dictionaries from intex.
　the transducer format is essentially used for more complex or more variable data where linguistic phenomena such as insertion or optionality may interfere. the figure 1 is the illustration of a transducer recognizing bless└ par
l'explosion de det n   explosion of det n  
   where the nominal phrase det n
recognizes nominal phases elaborated from the semantic class bombing where the following words appear:  
	  	  etc  	  	  grenade... .

weapon  automaton
the elaboration of such transducers requires linguistic expertise to obtain  a system recognizing the relevant sequences without too much noise. some tools and a method have been defined for the semi-acquisition and the design of resources such as subcategorization frames  completing the clusters learned by the lexicontuner  poibeau  1 . the architecture of the system is using cascading transducers  it is then important that each level has a good quality in order to allow the following analysis level to operate on a solid background. the different transducers are then minimized and determined1. the overall set of transducers is composed of 1 nodes and about 1 arrows in our experiment.
a hundred texts have been used as  training corpus  and a hundred different texts have been used as  test corpus . texts are first parsed with our system  and then some heuristics allow to fill the extraction template: the first occurrence of a number of victims or injured persons is stored. if a text deals with more than one terrorist event  we assume that only the first one is relevant. thanks to the nature of the channel  very few texts deal with more than one event.
　our results have been evaluated by two human experts who did not follow our experiment. let pos be the total number of good answers and act the number of solutions proposed by the system. our performance indicators are defined as:
   ok  if extracted information is correct;
   false  if extracted information is incorrect or not filled;
   none  if there were no extracted information and no information has to be extracted.
using these indicators  we can compute two different values:
  precision   prec  is defined as ok/pos.
  recall  rec  is defined as ok/ act .
.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1the performances are good according to the state-of-the-art and to the time spent on resource development. however  we can analyze the remaining errors as follows:
　the date of the story is nearly fully correct because the wrapper uses the formatted structure of the article to extract it. the errors for the location slot are due to two  contradictory  locations found by the system. a more complete linguistic analysis or a database providing lists of cities in different countries would reduce this kind of errors. the errors in the number of dead or injured persons slot are frequently due to silence: for example the system fails against too complex syntactic forms. the silence for the weapon slot is frequently due to incompleteness of semantic dictionaries.
the experiment that has been described is based on an external knowledge base derived from a dictionary. it is thus different from  faure and poibeau 1  which tries to acquire knowledge directly from the text. the use of an external database allows to work on middle-size corpora that are not as redundant as technical texts. we also think that using a general dictionary is interesting when dealing with general texts like a newswire. clusters contain words that were not contained in the training part of the corpus  allowing a better coverage  of the final result.
　the multi-domain extraction system is currently running in real time  on the afp newswire. about 1 templates have been defined that cover about 1% of the stories. from the remaining 1%  the system only extract surface information  especially thanks to the wrappers. the performances are between .1 and .1 p&r  if we do not take into account the date and location slots that are filled by means of wrappers. new extraction templates are defined to prove system scalability  about one new template per week . we hope to reach the number 1 templates towards summer 1.
a part of this study re-used some pieces of software developed in the framework of the ecran project  le1  1 . i would like to thank alpha luk and other people having participated to the development of the lexicon tuner. i am also indebted to david faure  tristelle kervel  adeline nazarenko and claire nedellec for useful comments and discussions on this subject.
 aguirre and rigau  1  aguirre e.  and rigau g. 
word sense disambiguation using conceptual density. in
                                     	copenhagen  1.
 ecran  1  d-1.1 - methods for lexical items modification/creation and d-1.1 - heuristics for automatic tuning. ecran project deliverable  1.
 faure and poibeau  1  faure d. and poibeau t. first experiments of using semantic knowledge learned by
asium for information extraction task using intex. in
 
during 	  berlin  1.
 fowler and dearholt  1  fowler r.h. and dearholt  d.w. pathfinder networks in information retrieval las cruces  new mexico state university  memorandums in computer and cognitive science  mccs-1 . 1.
 lesk  1  lesk m. automatic sense disambiguation:
how to tell a pine cone from an ice cream cone. in
                                             new york. 1. acm.
 luk  1  luk a. k. statistical sense disambiguation with relatively small corpora using dictionary
definitions. in 
  1.
 pazienza  1  pazienza m. t.  └d. . 
  springer verlag   lecture notes
in computer science   heidelberg  germany  1.
 ogden and bernick  1  ogden w. and bernick p. tabula rasa meta-tool: text extraction toolbuilder toolkit. technical report mccs-1. las cruces: computing research laboratory. 1.
 poibeau 1  poibeau t. a statistical clustering method to provide a semantic indexing of texts. in
 
during 	  stockholm  1.
 poibeau 1  poibeau t. corpus-based learning for information extraction. in 
　　　　　　　　　　　　　　　　  during   berlin. 1.
 poibeau 1  poibeau t. an open architecture for multi-domain information extraction. in 
 
seattle  1. aaai press.
 resnik  1  resnik using information content to evaluate semantic similarity in a taxonomy. in
  montreal  1.
 riloff  1  riloff e. little words can make a big difference for text classification. in 
   1.
 salton  1  salton g. 
addison-wesley  reading  ma.
 silberztein 1  silberztein m.  1  dictionnaires └lectroniques et analyse automatique des textes. masson  paris.
 v└ronis and ide  1  v└ronis j.  and ide n. m. word sense disambiguation with very large neural networks
extracted from machine readable dictionaries. in
　　　　　　　　　　　　　　　　　　　  	helsinki  finland.
 wilks 	  1  wilks y.  slator b.  and & guthrie l.
 
mit press  cambridge  ma.

natural language processing
and information retrieval
information extraction and retrieval

representing sentence structure in hidden markov models for information extraction
soumya ray
sray cs.wisc.edumark craven
craven biostat.wisc.edudepartment of computer sciences
university of wisconsin
madison  wisconsin 1department of biostatistics & medical informatics
university of wisconsin
madison  wisconsin 1
abstract
we study the application of hidden markov models  hmms  to learning information extractors for -ary relations from free text. we propose an approach to representing the grammatical structure of sentences in the states of the model. we also investigate using an objective function during hmm training which maximizes the ability of the learned models to identify the phrases of interest. we evaluate our methods by deriving extractors for two binary relations in biomedical domains. our experiments indicate that our approach learns more accurate models than several baseline approaches.
1	introduction
information extraction  ie  may be defined as the task of automatically extracting instances of specified classes or relations from text. in our research  we are interested in using machine learning approaches  includinghidden markov models  hmms   to extract certain relationships among objects from biomedical text sources. we present and evaluate two contributions to the state of the art in learning information extractors with hmms. first  we investigate an approach to incorporating information about the grammatical structure of sentences into hmm architectures. second  we investigate an objective function for hmm training whose emphasis is on maximizing the ability of the learned models to identify the phrases of interest rather than simply maximizing the likelihood of the training data. our experiments in two challenging real world domains indicate that both contributions lead to more accurate learned models.
　automated methods for information extraction have several valuable applications including populating knowledge bases and databases  summarizing collections of documents  and identifying significant but unknown relationships among objects. since constructing information extraction systems manually has proven to be expensive riloff  1   there has been much recent interest in using machine learning methods to learn information extraction models from labeled training data. hidden markov models are among the more successful approaches considered for learning information extractors  leek  1; freitag and mccallum  1; seymore et al.  1; freitag and mccallum  1; mccallum et al.  1 .
　previous hmm approaches to information extraction do not adequately address several key aspects of the problem domains on which we are focused. first  the data we are processing is complex natural language text. whereas previous approaches have represented their data as sequences of tokens  we present an approach in which sentences are first processed by a shallow parser and then represented as sequences of typed phrases. second  the data we are processing include many sentences that are not relevant to the relations of interest. even in relevant sentences  only certain phrases contain information to be extracted. whereas previous approaches to applying hmms for ie have focused the training process on maximizing the likelihood of the training sentences  we adopt a training method that is designed to maximize the probability of assigning the correct labels to various parts of the sentences beingprocessed  krogh  1 . our approachinvolves coupling the algorithm devised by krogh with the use of null models which are intended to represent data not directly relevant to the task at hand.
1	problem domain
our work is focused on extracting instances of specific relations of interest from abstracts in the medline database  national library of medicine  1 . medline contains bibliographicinformationand abstracts from more than 1 biomedical journals.
　an example of a binary relation that we consider in our experiments is the subcellular-localization relation  which represents the location of a particular protein within a cell. we refer to the domains of this relation as protein and location. we refer to an instance of a relation as a tuple. figure 1 provides an illustration of our extraction task. the top of the figure shows two sentences in a medline abstract. the bottom of the figure shows the instance of the target relation subcellular-localization that we would like to extract from the second sentence. this tuple asserts that the protein ubc1 is found in the subcellular compartment called the endoplasmic reticulum.
　in order to learn models to perform this task  training examples consisting of passages of text  annotated with the tuples that should be extracted from them  are needed. in our
 . . .
here we report the identification of an integral membrane ubiquitin-conjugating enzyme. this enzyme  ubc1  localizes to the endoplasmic reticulum  with the catalytic domain facing the cytosol.
. . . 
subcellular-localization ubc1 endoplasmic reticulum 
figure 1: an example of the information extraction task. the top shows part of a document from which we wish to extract instances of the subcellular-localization relation. the bottom shows the extracted tuple.
approach  each training and test instance is an individual sentence. there are several aspects of the data that make this a difficult information extraction task:  i  it involves free text   ii  the genre of text is  in general  not grammatically simple   iii  the text includes a lot of technical terminology   iv  there are many sentences from which nothing should be extracted.
　in the terminology that has been used in the information extraction literature  our task is inherently a multiple slot extraction task. since we are interested in extracting instances of -ary relations  we cannot treat each domain of such a relation as a separate unary component to be extracted  also called single slot extraction . consider the subcellularlocalization relation discussed above. a document may mention many proteins and many locations but this relation holds only among certain pairs of these proteins and locations.
　in the experiments reported here  we use two data sets representing two different binary relations. the subcellularlocalization data set includes 1 sentences that represent positive instances  labeled with tuples that should be extracted from them  and 1 sentences that represent negative instances  not labeled with any tuples . the 1 positive instances are labeled with 1 tuples in all; there are 1 unique tuples. the second data set is for a binary relation that characterizes associations between genes and genetic disorders. we refer to this relation as disorder-association  and the domains of this relation as gene and disorder. this data set contains 1 positive instances and 1 negative instances. the positive instances are labeled with 1 tuples in all  1 unique . for both data sets  the negative instances are  near misses  in that they come from the same population of abstracts as the positive instances  and in many cases they discuss concepts that are associated with the target relation.
　the target tuples for the subcellular-localization relation were collected from the yeast protein database  ypd   hodges et al.  1   and the target tuples for the disorder-association relation were collected from the online mendelian inheritance in man  omim  database  center for medical genetics   1 . relevant medline abstracts were also gathered from entries in these databases. to label the sentences in these abstracts  we matched the target tuples to the words in the sentence. a sentence which contained words that matched a tuple was taken to be a positive instance. every other sentence was considered to be a negative instance. it is clear that while this process is automatic  it will result in a noisy labeling. a sentence may have the words in a target tuple of the relation while the semantics may not refer to the relation. on the other hand  a tuple in a relation may be described by synonymous words which were not in any target tuple; therefore  sentences where tuples exist as synonymsare labeled incorrectly. we used a random sample of 1 positive and 1 negative sentences to estimate the amount of noise introduced by the labeling process. we estimate with 1% confidence that approximately 1% to 1% of the sentences are labeled incorrectly  either falsely labeled or unlabeled when they should have been  in the subcellular-localization data set. we believe that the disorder-association data set is not as noisy as the subcellular-localization data set.
1	representing phrase structure
hidden markov models  hmms  are the stochastic analogs of finite state automata. an hmm is defined by a set of states and a set of transitions between them. each state has an associated emission distribution which defines the likelihood of a state to emit various tokens. the transitions from a given state have an associated transition distribution which defines the likelihood of the next state given the current state.
　in previous hmm approaches to information extraction  sentences have been represented as sequences of tokens. we hypothesize that incorporating sentence structure into the models we build results in better extraction accuracy.
　our approach is based on using syntactic parses of all sentences we process. in particular  we use the sundance system  riloff  1  to obtain a shallow parse of each given sentence. our representation does not incorporate all of the information provided by a sundance parse  but instead  flattens  it into a sequence of phrase segments. each phrase segment consists of a type describing the grammatical nature of the phrase  and the words that are part of the phrase.
　in positive training examples  if a segment contains a word or words that belong to a domain in a target tuple  it is annotated with the corresponding domain. we refer to these annotations as labels. labels are absent in the test instances. figure 1a shows a sentence containing an instance of the subcellular-localization relation and its annotated segments  we shall discuss the other panels of this figure later . the second phrase segment in this example is a noun phrase segment  npsegment  that contains the protein name ubc1  hence the protein label . note that the types are constants that are pre-definedby our representation of sundance parses  while the labels are defined with respect to the domains of relation we are trying to extract. also note that the parsing is not always accurate  for instance  the third segment in figure 1a should really be a vpsegment  but has been typed as an npsegment by sundance.
　the states in our hmms represent the annotated segments of a sentence. like a segment  each state in the model is annotated with a type label pair1. a given state can emit only segments whose type is identical to the state's type; for
 this enzyme  ubc1  localizes to
np segment
np segment:protein
np segment pp segment
np segment:location
pp segment
np segment
vp segment
np segmentthe endoplasmic reticulum 
this enzyme ubc1
localizes to the endoplasmic reticulum with the catalytic domain
facing the cytosol
 a with the catalytic domain
det
unk
unk:protein
unk
prep art
n:location
unk:location
prep
art
n
unk
v
art
nfacing the c
this
enzyme ubc1
localizes to the endoplasmic reticulum with the
catalytic domain facing the cytosol
 b ytosol. 
protein
location
locationthis
enzyme ubc1
localizes to the endoplasmic reticulum with the catalytic domain facing the cytosol
 c figure 1: hmm input representations.  a  the phrase representation: the sentence is segmented into typed phrases.  b  the pos representation: the sentence is segmented into words typed with part-of-speech tags.  c  the token representation: the sentence is segmented into untyped words. for each representation  the labels  protein  location  are only present in thetraining sentences.
example  the segment  this enzyme  in figure 1a could be emitted by any state with type npsegment  regardless of its label. each state that has a label corresponding to a domain of the relation plays a direct role in extracting tuples.
　figure 1 is a schematic of the architecture of our phrasebased hidden markov models. the top of the figure shows the positive model  which is trained to represent positive instances in the training set. the bottom of the figure shows the null model  which is trained to represent negative instances in the training set. since our phrase representation includes 1 phrase types  both models have 1 states without labels  and the positive model also has five to six additional labeled states  one for each type label combination that occurs in the training set . we assume a fully connected model  that is  the model may emit a segment of any type at any position within a sentence.
　to train and test our phrase models  we have to modify the standard forward  backward and viterbi algorithms  rabiner  1 . the forward algorithm calculates the probability of a sentence being in state of the model after having emitted elements of an instance. when a sentence is represented as a sequence of tokens  the algorithm is based on the following recurrence:
start
 1 
where and represent the emission and transition distributions respectively  is the element in the instance  and ranges over the states that transition to .
our modification involves changing the last part of this recurrence as follows:
 1 
if
otherwise
here is the word in the phrase segment   and type is a function that returns the type of a segment or state as described above. the two key aspects of this modification are that  i  the type of a segment has to agree with the type of state in order for the state to emit it  and  ii  the emission probability of the words in the segment is computed as the product of the emission probabilities of the individual words. this latter aspect is analogous to having states use a na： ve bayes modelfor the words in a phrase. note that this equation requires a normalization factor to define a proper distribution over sentences. however  since we use these equations to make relative comparisons only  we leave this factor implicit. the modifications to the viterbi and backward algorithms are similar to this modification of the forward algorithm.
　given these modificationsto the forward and backward algorithm we couldtrain phrase-basedmodels using the baumwelch algorithm  baum  1 . however  for the models we consider here  there is no hidden state for training examples  i.e.  there is an unambiguouspath throughthe model for each example   and thus there is no need to use baum-welch. instead  we assume a fully connected model and obtain transition frequencies by considering how often segments with various type  label annotations are adjacent to each other. we smooth these frequencies over the set of possible transitions for every state using -estimates  cestnik  1 . in a similar manner  we obtain the emission frequencies of the words in each state by summing over all segments with the

figure 1: the general architecture of our phrase-based hmms. the top part of the figure shows the positive model and the bottom part of the figure shows the null model.
same type  label annotations in our training set. we smooth these frequency counts using	-estimates over the entire vocabulary of words.
　once the model has been constructed  we use it to predict tuples in test sentences. we use the viterbi algorithm  modified as described above  to determine the most likely path of a sentence through the positive model. we consider a sentence to represent a tuple of the target relation if and only if two conditions hold:
1. the likelihood of emission of the sentence by the positive model is greater than the likelihood of emission by the null model:   where and
refer to the positive and null models respectively and the sentence has segments.
1. in the viterbi path for the positive model  there are segments aligned with states corresponding to all the domains of the relation. for example  for the subcellularlocalization relation  the viterbi path for a sentence must pass through a state with the protein label and a state with the location label.
　note that even after phrases have been identified in this way  the extraction task is not quite complete  since some of the phrases might contain words other than those that belong in an extracted tuple. consider the example in figure 1a. the location phrase contains the word  the  in addition to the location. therefore  tuple extraction with these models must include a post-processing phase in which such extraneous words are stripped away before tuples are returned. we do not address this issue here. instead  we consider a prediction to be correct if the model correctly identifies the phrases containing the target tuple as a subphrase.
　it is possible to have multiple predicted segments for each domain of the relation. in this case  we must decide which combinations of segments constitute tuples. we do this using two simple rules:
1. associate segments in the order in which they occur. thus for subcellular-localization  the first segment matching a protein state is associated with the first segment matching a location state  and so on.
1. if there are fewer segments containing an element of some domain  use the last match of this domain to construct the remaining tuples. for instance  if we predicted one protein phrase and two location phrases and   we would create two tuples based on and .
1	experiments
in the experiments presented in this section  we test our hypothesis that incorporating phrase-level sentence structure into our model provides improved extraction performance in terms of precision and recall. we test this hypothesis by comparing against several hidden markov models that represent less information about the grammatical structure of sentences. henceforth  we refer to the model described above as the phrase model.
　the first model we compareagainst  which we call the pos model  is based on the representation shown in figure 1b. this model represents some grammatical information  in that it associates a type with each token indicating the part-ofspeech pos  tag for the word  as determined by sundance . however  unlike the phrase model  the pos model represents sentences as sequences of tokens  not phrases. this model is comparable in size to the phrase model. the positive component of this model has 1 states without labels and six to ten states with labels  dependingon the training set . the null component of the model has 1 states without labels. the other models we consider  which we call the token models  are based on the representation shown in figure 1c. this representation treats a sentence simply as a sequence of words. we investigate two variants that employ this representation. the simpler of the two hidden markov models based on this representation  which we refer to as token model 1  has three states in its positive model and one state in its null model  not counting the start and end states . none of the states in this model have types. two of the states in the positive model represent the domains of the binary target relation  while the remaining states have no labels. the role of the latter set of states is to model all tokens that do not correspond to the domains of the target relation. a more complex version of this model  which is illustrated in figure 1  has three unlabeled states in its positive model. we define the transitions and train these models in such a way that these three states can specialize to  i  tokens that come before any relation instances   ii  tokens that are interspersed between the domains of relation instances  and  iii  tokens that come after relation instances.
　the training algorithm used for the pos model is identical to that used for the phrase model. the training algorithm for the token models is essentially the same  except that there are no type constraints on either the tokens or states.
　since we consider a prediction made by the phrase model to be correct if it simply identifies the phrases containing the words of the tuple  we use a similar criterion to decide if the predictions made by the pos model and token models are

figure 1: the architecture of token model 1.
correct. we consider pos model and token model predictions to be correct if the labeled states of these models identify sequences of tokens that contain the words of the tuple. these models are not penalized for extracting extra adjacent words along with the actual words of a target tuple.
　we process the hmm input data  after parsing in cases where sundance is used  by stemming words with the porter algorithm  porter  1   and replacing words that occur only once in a training set with a generic unknown token. the statistics for this token are then used by the model while emitting out-of-vocabulary words encountered during prediction. similarly  numbers are mapped to a generic number token.
　positive predictions are ranked by a confidence measure which is computed as the ratio of the likelihood of the viterbi path of a sentence through a model to the likelihood of the model to emit that sentence  i.e. confidence
　　　　　here is a sentence of segments  is the likelihood of the most probable path of all
segments threaded through to the end state  and
is the comparable value calculated by the forward algorithm. we construct precision-recall graphs for our models by varying a threshold on the confidence measures.
　for both data sets we measure precision and recall using 1-fold cross validation. the data is partitioned such that all of the sentences from a given medline abstract are in the same fold. this procedure ensures that our experiments model the nature of the real application setting. for training  we sample the negative instances so that there are an equal number of positive and negative instances per fold. we have observed that we get better recall consistently by doing this.
　figure 1 shows the precision-recall curves for the subcellular-localization data set. the curve for the phrase model is superior to the curves for both token models. at low levels of recall  the pos model exhibits slightly higher precision than the phrase model  but the latter is superior at higher recall levels  and the phrase model has a significantly higher recall endpoint. these results suggest that there is value in representing grammatical structure in the hmm architectures  but the phrase model is not definitively more accurate.
	figure	1	shows	the	precision-recall curves for	the

figure 1: precision vs. recall for the four models on the subcellular-localization data set.

figure 1: precision vs. recall for the four models on the disorder-association data set.
disorder-association data set. here  the differences are much more pronounced. the phrase model achieves significantly higher levels of precision than any of the other models  including the pos model. the recall endpoint for the phrase model is also superior to those of the other models. we conclude that the experimentspresented here support our hypothesis that incorporating sentence structure into the models we build results in better extraction accuracy.
1	improving parameter estimates
standard hmm training algorithms  like baum-welch  are designed to maximize the likelihood of the data given the model. specifically  if is a sentence in the training set  baum-welch  and the method we used earlier  tries to find parameters such that
we hypothesize that more accurate models can be learned by training with an objective function that aims to maximize the likelihood of predicting the correct sequence of labels for a given sentence  as before  we assume that states and phrases

figure 1: combined model architecture. the positive and null models refer to the corresponding models in figure 1.
without labels have an implicit empty label . let be the known sequence of labels for a sentence in our training set. we would like to estimate parameters such that
 1 
		 1 
this is similar to the task of optimizing parameters to recover the sequence of states given a set of observations mccallum et al.  1 . krogh has devised an hmm training algorithm that tries to optimize this criterion. after transforming this objective function into one which aims to minimize the negative log likelihood of the above equation  the following incremental update rule is obtained:
 1 
where is the parameter  is the expected number of times is used by the sentence on correct paths through the model  is the expected number of times is used by the sentence on all paths through the model  is a normalizing constant  and is the learning rate. the and terms can be calculated using the forward-backward procedure. note that the update rule represents an online training procedure.
　in our previous experiments  we used a separate null model to represent negative instances. we would like to use krogh's algorithm with this configuration to observe if it results in more accurate models. however  the null model as we have described it is a separate entity which is trained separately. with this architecture  krogh's algorithm would be unable to correct false positives in the training set since doing so might require adjusting the parameters of the positive model in response to a negative instance. to remedy this problem  we propose an alternative to having a separate null model  which we refer to as a combined model. a combined model consists of two submodels sharing common start and end states. a schematic is shown in figure 1. the shared start and end states allow the training algorithm to update parameters in both parts of the model in response to a given training sentence.
1	experiments
to evaluate this algorithm  we train the combined model configuration on the subcellular-localization and the disorderassociation data sets. we compare these models against the phrase model trained on the corresponding data sets in our previous experiments.
　the methodologyfor this experimentis the same as before. note that for the combined model  prediction is simpler than with a separate null model  since it suffices to consider the viterbi path of a sentence through the model to extract tuples  if any. we do not train the combined model to convergence to avoid overfitting. instead  we set the number of iterations for which to do gradient descent to a fixed constant value of 1.

figure 1: effect of krogh's algorithm on the combined model for the subcellular-localization data set.
　figure 1 shows the precision-recall curves for this experiment for the subcellular-localization data set. for each precision-recall curve  we also show 1% confidence intervals. from the figure  we observe that there is some improvement in the precision of the model on this data set  while recall is held nearly constant. while the improvement is small  we have observed it consistently across the various model architectures we have explored. figure 1 shows the corresponding precision-recall curves and confidence intervals for the experiment on the disorder-association data set. here  the differencebetween the initial model and the trained model is more pronounced. the model trained with krogh's algo-

figure 1: effect of krogh's algorithm on the combined model for the disorder-association data set.
rithm has significantly better precision than the initial model  while maintaining a similar level of recall. we conclude that this training algorithm is appropriate for our task  and can improve accuracy  sometimes significantly.
1	conclusion
we have presented two contributions to learning hidden markov models for information extraction  and evaluated these contributions on two challenging biomedical domains. we have presented an approach to representing the grammatical structure of sentences in an hmm. comparative experiments with other models lacking such information shows that this approach learns extractors that have increased precision and recall performance. we have also investigated the application of a training algorithm developed by krogh to our models. this algorithm consistently provides an accuracy gain over our original models. we believe that these are promising approaches to the task of deriving information extractors for free text domains.
acknowledgments
this research was supported in part by nih grant 1 lm1  and nsf career award iis-1. the authors would like to thank michael waddell for his work on building the disorder-association data set  and peter andreae  joseph bockhorst  tina eliassi-rad  and jude shavlik for critiquing the initial draft.
references
 baum  1  l. e. baum. an equality and associated maximization technique in statistical estimation for probabilistic functions of markov processes. inequalities  1-1  1.
 center for medical genetics   1  center for medical genetics  johns hopkins university and national center for biotechnology information. online mendelian inheritance in man  omim  tm   1. http://www.ncbi.nlm.nih.gov/omim/.
 cestnik  1  b. cestnik. estimating probabilities: a crucial task in machine learning. in proceedings of the ninth european conference on artificial intelligence  pages 1  stockholm  sweden  1. pitman.
 freitag and mccallum  1  d. freitag and a. mccallum. information extraction with hmms and shrinkage. in working notes of the aaai-1 workshop on machine learning for information extraction  orlando  fl  1. aaai press.
 freitag and mccallum  1  d. freitag and a. mccallum. information extraction with hmm structures learned by stochastic optimization. in proceedings of the seventeenth national conference on artificial intelligence  austin  tx  1. aaai press.
 hodges et al.  1  p. e. hodges  w. e. payne  and j. i. garrels. yeast protein database  ypd : a database for the complete proteome of saccharomyces cerevisiae. nucleic acids research  1-1  1.
 krogh  1  a. krogh. hidden markov models for labeled sequences. in proceedings of the twelfth international conference on pattern recognition  pages 1  jerusalem  israel  1. ieee computer society press.
 leek  1  t. leek. information extraction using hidden markov models. master's thesis  department of computer science and engineering  university of california  san diego  ca  1.
 mccallum et al.  1  a. mccallum  d. freitag  and f. pereira. maximum entropy markov models for information extraction and segmentation. in proceedings of the seventeenth international conference on machine learning  pages 1  stanford  ca  1. morgan kaufmann.
 national library of medicine  1  national library of medicine. the medline database  1. http://www.ncbi.nlm.nih.gov/pubmed/.
 porter  1  m. f. porter. an algorithmfor suffix stripping. program  1 :1  1.
 rabiner  1  l. r. rabiner. a tutorial on hidden markov models and selected applications in speech recognition. proceedings of the ieee  1 :1  1.
 riloff  1  e. riloff. an empirical study of automated dictionary construction for information extraction in three domains. artificial intelligence  1-1  1.
 riloff  1  e. riloff. the sundance sentence analyzer  1. http://www.cs.utah.edu/projects/nlp/.
 seymore et al.  1  k. seymore  a. mccallum  and r. rosenfeld. learning hidden markov model structure for information extraction. in working notes of the aaai workshop on machine learning for information extraction  pages 1. aaai press  1.
sequentially finding the n-best list in hidden markov models
dennis nilssonjacob goldbergeraalborg universitythe weizmann institute of sciencebajers vej 1 e  1 aalborg  strehovot  1denmarkisraelnilsson math.auc.dkjacob wisdom.weizmann.ac.ilabstract
we propose a novel method to obtain the n-best list of hypotheses in hidden markov model  hmm . we show that the entire information needed to compute the n-best list from the hmm trellis graph is encapsulated in entities that can be computed in a single forward-backward iteration that usually yields the most likely state sequence. the hypotheses list can then be extracted in a sequential manner from these entities without the need to refer back to the original data of the hmm. furthermore  our approach can yield significant savings of computational time when compared to traditional methods.
1	introduction
in many tasks of large vocabulary speech recognition it is desirable to find from the hmm graph the n most likely state sequences given the observed acoustic data. the recognizer chooses the utterance hypotheses on the basis of acoustic information and a relatively simple language model. the existence of an n-best list enable us to combine additional knowledge sources such as complicated acoustic and language models into the recognition process  ostendorf 1 . given the additional knowledge sources the list of sentence can be rescored and reordered. even without additional knowledge sources the n-best paradigm can be used to improve the recognition rate  stolcke et al. 1 . in this paper we concentrate on the step of computing the n-best list. the most likely state sequence can be found using a single iteration of the viterbi algorithm  rabiner 1 . a direct generalization of this algorithm can be used to obtain the n best state sequences. the only change is that for each time index t and for each state we have to keep the n best subsequences terminating at this state. however in this generalized viterbi approach we have to decide in advance on the size of n and we can not change it in the middle of the process. several modification of this algorithm have been proposed in the last decade. these algorithms are based either on a viterbi search of a trellis or on a  search  schwartz and chow 1   schwartz et al. 1 .
　we propose a novel method to obtain the n-best list in hmm. the obtained algorithm is inspired by the divide and conquer algorithm in  nilsson  1  for finding the m most likely configurations in probabilistic expert systems. the present algorithm also has the advantage of being an anytime algorithm since we need not in forehand specify the number of n best hypothesis that is wanted. furthermore  our algorithm can yield significant savings in computational time compared to the traditional viterbi algorithm.
1	basic structure
consider a hmm with m hidden markovian random variables x1  ... xm and m observed variables y1 ... ym such that the distribution of yt is determined by xt. denote x = {x1 ... xm} and y = {y1 ... ym}. typical values that x and y can take are denoted x =  x1 ... xm  and y =  y1 ... ym  respectively. the joint probability function is:


figure 1: the hmm structure
　this paper deals with the following decoding problem: given observations y1 ...ym  find the n most likely state sequences of the unobserved state-variables. in other words we want to find the n values of x that maximize the conditional probability function p x y   viewed as a function of x . a single iteration of the forward-backward algorithm  rabiner 1  yields the following terms for each time index t:
	ft s 	=	max	p x y 	 1 
{x|xt=s}
	ft t+1 s s1 	=	max	p x y 
{x| xt xt+1 = s s1 }
we shall show that the entire information needed to compute the n-best list is encapsulated in the expressions defined in
 1 . in other words  once ft s  and ft t+1 s s1  are given  there is no need to refer again to the trellis graph.
　as a first example of the usefulness of ft and ft t+1  we apply them to obtain the most likely state sequence. one can observe that the probability of the most likely state sequence is :
maxp x y  = maxft s  x	s
this equality remains the same for each index t = 1 ... m. to find the most likely state sequence x  =  x 1 ... x m   when this is uniquely determined  we note that x t = argmaxs ft s   t = 1 ... m
however to allow for the possibility that the maximizing state sequence is not unique  it is better to apply the following routine:
 x 1 x 1  = argmax s s1  f1 s s1 
 1 
x t = argmaxs ft 1 t x t 1 s   t   1
1	introducing the algorithm
denote the entire ensemble of possible state sequences by x and let the lth most likely state sequence be denoted
.
　suppose at some stage in the algorithm that x1 ... xl 1 have been identified. then xl is found by performing the following steps:
partition phase: here x   {x1 ... xl 1} is partitioned into subsets.
candidate phase: for each subset in the above partitioning we compute the probability of its most likely state sequence. these probabilities are referred to as the 'candidates'.
identification phase: the state sequence associated with the highest candidate is identified.
　in partition phase  there is a large number of possible partitions that one may consider. we seek a partitioning with two properties. firstly  it must be easy to compute the candidates. secondly  the number of subsets in the partitioning must be small  since eventually we need to compare the different candidates.
　in the successively subsections it is shown how the second and third most likely state sequences are found. in section 1 the general case is considered.
1	the second most likely state sequence
suppose the most likely state sequence x1 has now been identified. to identify x1 we carry out the following steps.
partition phase
here  x {x1} is partitioned into subsets a1 ... am defined by
a1 = {x | x1= x1}
　　　 1  ai = {x | x1 = x1 ... xi 1 = xi1 xi 1= xi1}  i − 1.
candidate phase
we let p  ai  stand for the probability of the most likely state sequence in the subset ai  i.e.
 
and use a similar notation for other subsets.
the functions ft t+1 satisfy that

　　　 1  p  ai  = maxs1=x1i fi 1 i x1i 1 s   i   1
so  the second most likely state sequence has probability p x1 y  = maxp  ai . i
identification phase
if p  ai  = max{p  aj  : j = 1 ... m}  then x1 （ ai  and x1 can be identified by carrying out the following steps:
  x1j = xj1 for j = 1 ... i   1.
  xi1 = argmaxs1=xi1 fi 1 i xi1 s 
  xk1 = argmaxs fk 1 k x1k 1 s  for k = i + 1 ... m.
1	the third most likely state sequence
the identification of the third most likely state sequence is done by a similar procedure as the case with x1.
partition phase
here  a partition of x  {x1 x1} is constructed by refining the above partitioning of x   {x1}. this is done in the following way.
　suppose x1 （ ai. then we partitioning ai   {x1} into subsets bi ... bm defined by
bi =  x | x1 = x1 ... xi 1 = x1i 1 xi （ {/ x1i xi1} 
 1 
bk =  x | x1 = x1 ... x{	k 1 = x−1k 1} xk =1	x{1k   k   i.1	}
 thus  the subsets in bj : j i   and aj : j = i   constitute a partitioning of x   {x1 x1}.
candidate phase
as we shall prove in the next section  theorem 1   the probability p  bk  for k − i is a simple function of p  ai :
 1 
.
now  the third most likely state sequence has probability
p x1 y  = max maxp  aj    maxp  bj  . j:j=1 i	j:j−i
identification phase
if the third most likely state sequence x1 belongs to one of the subsets aj  j =1 i   then it can be identified in a similar way as x1 was identified. on the other hand  if x1 （ bj for some j − i  then
 
and the sequence x1j ... x1m is now successively identified as follows:
xj1 =   argmax	1j 1j	if j   i argmaxs/（{x  x } fj 1 j x1j 1 s 	if j = i
x1k = argmaxs fk 1 k xk1 s   for k = j + 1 ... m
example to illustrate our algorithm  we consider a simple hmm with 1 hidden variables x1 ... x1  and 1 observed variables y1 ... y1. each of the variables are assumed binary with possible states y and n. the initial conditional probability functions associated with the hmm are given in table 1.
	n	y
p x1 
.1.1 
nxt y.1
.1.1
.1.1
.1.1
.1p xt | xt 1p xt | yt 	xt n	y
	xt 1nyyt	yn
table 1: the intial probabilities in the hmm
suppose we obtain 'evidence' of the form y =  y1 y1 ... y1  =  n n y y y y n n y y .
　to compute the n-best list given evidence  we initially compute the ft t+1 functions shown in table 1. by applying the routine in  1   we can now obtain the most likely state sequence:
x1 =  n n y y y y y y y y 
with probability p x1 y  = .1 ， 1.
　for the computation of the second most likely state sequence we proceed as follows. first  we partition the space x   {x1} into subsets a1 ...a1 defined in  1 :

f1	x1	f1	x1 n	y	n	y
.1 .1 .1 .1x1 n y.1 .1
.1 .1x1 n y
f1	x1	f1	x1 n	y	n	y
.1 .1 .1 .1x1 n y.1 .1
.1 .1x1 n y
f1	x1	f1	x1 n	y	n	y
.1 .1 .1 .1x1 n y.1 .1
.1 .1x1 n y
f1	x1	f1	x1 n	y	n	y
.1 .1 .1 .1x1 n y.1 .1
.1 .1x1 n y
f1	x1 n	y
.1 .1 .1 .1x1 n y
table 1: the ft t+1 functions  the numbers are multiplied by 1 
　next  we use  1  to compute the probability of the most likely state sequence in each subsets ai:
p  a1 	=	max{f1 y n  f1 y y }	=	.1
p  a1 	=	f1 n y 	=	.1
p  a1 	=	f1 n n 	=	.1 p  a1 	=	f1 y n 	=	.1
p  a1 	=	f1 y n 	=	.1
p  a1 	=	f1 y n 	=	.1 p  a1 	=	f1 y n 	=	.1
p  a1 	=	f1 y n 	=	.1
p  a1 	=	f1 y n 	=	.1
p  a1 	=	f1 y n 	=	.1
thus  the second most like state sequence x1 belongs to subset a1  and can be found by carrying out steps  see section
1 :
 
and
	=	argmax{f1 n s } = n
s
	=	argmax{f1 n s } = y
s
	=	argmax{f1 y s } = y
s
this identifies the second most likely state sequence.
　to compute the third most likely state sequence we proceed in a similar manner: first  we partition a1   {x1} in subsets of the form in  1 :

then  we compute the probability of the most likely state sequence within each of the subsets bi. from  1  we immediately obtain  p  b1  = 1 since b1 =   :

　thus  the third most like state sequence x1 belongs to subset a1  since max{maxp  ai  p  b1  p  b1  p  b1 } = p  a1   i1
and can be found by carrying out the steps described in section 1. this is left to the reader.
table 1 shows the three most likely state sequences.
configuration   probability  multiplied by 1 x1= n n y y y y y y y y p x1 y  = .1x1= n n y y y y n n y y p x1 y  = .1x1= n y y y y y n n y y p x1 y  = .1table 1: the 1-best list
1	the general algorithm
the general algorithm for computing the n-best list identifies the n most likely state sequences in a sequentialy manner.
　let xi denote the possible states that the variable xi can take.
suppose at some stage in the algorithm that
  x1 ... xl have been identified;
  a partitioning  say pl  of x  {x1 ... xl 1} is given.
  for each element d in pl  p  d  is known.
  xl （ d for some element d in the partitioning pl  and d has the following form:

where   are known states  and xi1 is a known subset of xi.
the tasks involved in proving the correcteness of our algorithm consist of:
partition-phase: construct a partitioning of d   {xl};
candidate-phase: compute the probability of the most likely state-sequence in each element in the partitioning
of d   {xl};
identification-phase:   compare the 'new candidates' computed above with the remaining candidates.
  identify xl+1.
these three tasks are described in subsequent subsections.
1	partition phase
suppose xl （ d  where d has the form in  1 . for the following analysis it is convenient to write d as
	 	.	 1 
for k = i ... m we define the subsets dk as

where x．k is a subset of xk given by
1
	x．k =   {xxklk“ {} xlk }	ifif kk   i= i
the reader may easily verify that the subsets di ... dm partition d {xl}. furthermore  it can be seen that each element dk has a similar form as that of d.
1	candidate phase
in this section we provide an efficient method to compute the probabilities p  dk   k = i ... m . the method presented here is similar to the the method in  nilsson 1   in that it locally computes the probabilities of all candidates directly from the functions ft t+1.
　a keypoint in proving our main result is that we can write the joint probability function expressed in  1  as
	.	 1 
for a proof  the interested reader is referred to  dawid 1   where it is shown for more general graphical models  the junction trees.
　in addition  we will use that the functions ft t+1 have the following property  termed max-consistency:
	.	 1 
now we have
lemma 1 let e be a subset given by
.
then
.
proof: the probability p  e  can be found by instantiating  in  1   and then maximize over the
remaining variables. because of max-consistency  1  this reduces to
 
which completes the proof.  
lemma 1 let d be given as in  1   and suppose xl = argmaxp x y .
x（d
then for all k = i ... m we have
.
proof: let k − i  and define d1 as
.
by lemma 1 it suffices to prove that max p x y  = maxp x y . x（d1	x（d
clearly 
max p x y  ＋ maxp x y  x（d1	x（d
since d1   d. furthermore  we have that max p x y  − maxp x y  x（d1	x（d
because xl （ d1 and xl = argmaxx（d p x y . the result
follows.  
now we are ready to state the main theorem. it presents a straightforward procedure for finding the probability of each subset dk  defined in  1   from the probability p  d .
theorem 1 let d be given as in  1   and suppose xl = argmaxp x y .
x（d
then for all dk defined in  1  we have:
.
proof: by lemma 1 we have
	 	 1 
furthermore  since dk can be written as
.
we have from lemma 1 that

the result now follows from  1  and  1 .  
1	identification phase
suppose that we are given a subset d of the form
.
and want to identify the most likely state sequence  say x   in
d. first  one note that
  .
now the sequence x i  ... xm  is successively found by x i = argmax x k = argmaxs fk 1 k x k 1 s   k = i + 1 ... m.
1	the algorithm
a pseudo code for our n-best list algorithm is given below.
procedure n-best list:
step 1  identify x1 :
1. compute the functions ft t+1;
1. identify the most likely state sequences x1 as shown in section 1;
step 1  identify x1 :
 partition phase partition x   {x1} in subsets a1 ... am as in  1 ;
 candidate phase compute p  ai  as in section 1.
 identification phase
1. set the candidate list cl = {p  ai };
1. pick the highest candidate  say p  ai   in cl;
1. identify x1 （ ai as in section 1; .
step 1  identify x1 ... xn  for l = 1 ... n   1 do
 partition phase:
1. suppose xl （ d;
1. partition d   {xl} in subsets dk ... dm as in  1 .
 candidate phase: compute p  dk  ... p  dm  as in theorem 1.
 identification phase:
1. augment the candidate list with the new candidates: cl := cl “ {p  dk  ... p  dm };
1. pick the highest candidate  say p  d    from cl;
1. identify xl+1 = argmaxx（d  p x y  as in section
1;
1. retract p  d   from the candidate list: cl := cl   {p  d  }.
end
　accordingly  the probability of the lth most likely state sequence is either one of the candidates computed in the lth step of the algorithm or it is one of the candidates computed earlier and still on the candidate list. the search over all the candidates can be effectively performed in the following way. we can sort the candidates on the candidate list with their associated subsets. in partition phase we split the subsets that contained xl 1  and hence was at the top of the list  into several subsets. in candidate phase we merge the new candidates into the sorted list according to their probabilities. now the state sequence xl belongs to the subset at the top of the updated list.
　because the algorithm finds the n-best list sequentially  we need not in forehand specify the number of n best hypothesis that is wanted. in stead  we may chose to find the n most likely hypothesis whose total probability is at least α  where 1   α ＋ 1.
1	complexity issues
we would claim that our method can be implemented with lower complexity than traditional methods for finding the nbest list.
　to support this  we would briefly discuss the computational complexity of our method by computing the number of elementary operations needed to perform the various tasks in the algorithm. let
γ = maxi |xi| =	the maximum number of elements that a variable can have.
the operations performed by our algorithm can be divided into three parts
1 computing the functions ft t+1 and ft defined in  1 . and for each l = 1 ... n:
1 in candidate phase  to compute the candidates as in theorem 1.
1 in identification phase  to compare the candidates on the candidate list  and identify xl.
finding the functions in part 1 can be done by less than
γ1m operations.
in candidate phase  we generate for each step l at most m new candidates. the computation of a candidate takes place via theorem 1  and can be done with γ computaions  i.e. computing all candidates in step l = 1 ... n demands at most γmn computations.
in identification phase  we compare  in step l at most ml elements  since at most m candidates are generated in each step . if we store the elements in a 1 search tree  then the comparisons needed for inserting m new elements and update the search tree is o mlog ml  . doing this for all l = 1 ... n  the total number of comparisons needed is in the order of
o.
finally  in identification phase  we identify xl as described in section 1  and the number of comparisons needed is no larger than mγ. thus the total number of comparisons cannot exceed
γmn.
adding up these terms  we obtain that the whole process of finding x1 ... xn is in the order of
o γ1m  + o mγn  + o mn log mn  .
to compare  a straightforward implementation of the viterbi search as described in  forney  1  uses in the order of
o γ1nm .
we conclude that for large γ  our method can yield significant savings of computational time when comparing to traditional viterbi search.
1	conclusion
we have presented in this paper a novel method to compute the n most likely state sequences in hmms. the algorithm has two advantages compared to traditional methods. firstly it can yield significant gain in computational time. secondly  it is an anytime algorithm  and thus is also effective in cases where we do not know in advance how many solutions are needed. the main concept is to perform a small preprocessing computation and then we can produce the sequences in an incremental manner. we have concentrated in this paper on applications of the algorithm to speech recognition problems. the proposed algorithm  however  can be applied to many other sources of information that are organized in a hidden markov model e.g. analysis of dna sequences and real time robot navigation.
acknowledgment
this research was supported by dina  danish informatics network in the agricultural sciences   funded by the danish research councils through their pift programme.
references
dawid  a. p.  1 . applications of a general propagation algorithm for probabilistic expert systems. statistics and computing  1  1.
forney  g.  1 . the viterbi algorithm. proc. ieee  1  1.
nilsson  d.  1 . an efficient algorithm for finding the m most probable configurations in probabilistic expert systems. statistics and computing  1  1.
ostendorf  m. e. a.  1 . integration of diverse recognition methodologies through reevaluation of n-best sentence hypotheses. in proceedings  darpa speech and natural language processing workshop  pp. 1.
rabiner  l.  1 . a tutorial on hidden markov models and selected application in speech recognition. proceedings of the ieee  1   1   1.
schwartz  r. and chow  y.  1 . a comparison of several approximate algorithms for finding multiple  n-best  sentence hypotheses. in proceedings ieee international conference on acoustics  speech  and signal processing  pp. 1.
schwartz  r.  nguyen  l.  and makhoul  j.  1 . multiplepass search strategies. in automatic speech and speaker recognition  pp. 1. kluwer academic publishers.
stolcke  a.  konig  y.  and weintraub  m.  1 . explicit word error minimization in n-best list rescoring. proc. eurospeech  1  1.
nlp-driven ir: evaluating performances over a text classification task
roberto basili and alessandro moschitti and maria teresa pazienza
university of rome tor vergata
department of computer science  systems and production
1 roma  italy 
basili moschitti pazienza  info.uniroma1.itabstract
although several attempts have been made to introduce natural language processing  nlp  techniques in information retrieval  most ones failed to prove their effectiveness in increasing performances. in this paper text classification  tc  has been taken as the ir task and the effect of linguistic capabilities of the underlying system have been studied. a novel model for tc  extending a well know statistical model  i.e. rocchio's formula  ittner et al.  1   and applied to linguistic features has been defined and experimented. the proposed model represents an effective feature selection methodology. all the experiments result in a significant improvement with respect to other purely statistical methods  e.g.  yang  1    thus stressing the relevance of the available linguistic information. moreover  the derived classifier reachs the performance  about 1%  of the best known models  i.e. support vector machines  svm  and -nearest neighbour  knn   characterized by an higher computational complexity for training and processing.
1	introduction
although in literature poor evidence assessing the relevance of natural language processing  nlp  in improving information retrieval  ir  has been derived  a shared belief exists that linguistic processingcan capturecritical semantic aspects of document content that simple word matching cannot do. it has been also stressed  e.g.  grefenstette  1    that vector space models are inadequate to deal with retrieval from web via commonly available simple and short queries. language processing enables to enrich the document representation with semantic structuresalthoughthe nature and methods for doing this are still under debate. which specific information  structures and dependencies can be suitably derivedand which combineduse of linguistic processes and ir models are to be applied represent still open questions.
　in order to derive more insight on the above issues  a systematic experimental framework has to be defined  where tasks and performance factors can be assessed and measured.
among other ir tasks  text classification  tc  is a promising process for our objectives. it plays a major role in retrieval/filtering processes. moreover  given the rich experimental evidence on well-assessed benchmarking collections  tc better supports a comparative evaluation of the impact of linguistic information with respect to approaches based on word matching.
　the classification problem is traditionally described as follows: given a set of classes     i.e. topics/subtopics labels  e.g.  politics / foreign politics   and an extensive collection of examples classified into these classes  often called training set  the classification problem is the derivation of a decision function that maps documents
    into one or more classes  i.e. . as the specific topics  classes  are fixed  the extraction of content from document  for retrieval purposes  can be more systematic  given their focused semantics  and less complex than in other ir scenarios. therefore represents a suitable environment for testing the capabilities of to capture such semantic aspects.
　the role of linguistic content in relates to the definition of able to provide specific and selective information about training and test documents and  consequently  about the target classes. basic language processing capabilities allow to extend the knowledge on the words occurring in documents  e.g. their canonical form  i.e. the morphological derivation from a lemma  and their syntactic role  i.e. their part-of-speech  pos  in the input context . previous works on nlp-driven text classification  e.g.  basili et al.  1b   suggest that such informationimproves performances. in particular  lemmatization and recognition  i.e. removal of proper nouns from the set of selective feature  provide a linguistically principled way to compress the features set  usually obtained by traditional crude methods like stop lists or statistical thresholds  e.g.  . statistical unsupervised terminological extraction has been also applied to tc training. it allows detecting more complex and relevant features  i.e. complex nominal groups typical of the different topics. the results are improved performances  although the contribution given by such modules has not yet been accurately measured.
　the main reason for poor improvements if any  when nlp is applied to ir is the noise introducedby the linguistic recognition errors which provides drawbacks comparable to the significant advantages. in the specific case of tc  when more complex features  e.g. words and their pos tag or terminological units  are captured it can be even more difficult to select the relevant ones among the set of all features. data sparseness effects  e.g. the lower frequency of -grams wrt simple words  interact with wrong recognitions  e.g. errors in pos assignment  and the overall information about a class looses its potential selectivity.
　the traditional solution is usually the feature selection  discussed for example in  yang and pedersen  1 . by appplying statistical methods   information gain    mutual information ...   the not relevant features are removed. major drawbacks are that features irrelevant for a class may be removedevenif they are importantfor anotherone.
but rare or specific may be cut in this way  as also noted in  joachims  1 . the crucial issue here is how to give the right weight to a given feature in different classes. this is even more important when nlp  and  especially  terminology recognition  is applied: some technical terms can be perfectly valid features for a class and  at the same time  totally irrelevant or misleading for others.
　in this paper an original tc model for selection and weighting of linguistically motivated features  as an extension of the the rocchio classifier   ittner et al.  1    has been designed and implemented. it has been experimented on feature sets extracted by nlp techniques: terminological expressions  part-of-speechtagged lemmas and proper nouns. in section 1 the novel feature selection model with its weighting capabilities is presented. section 1 describes the nlp functionalities adopted for extracting the feature sets from the target documentsduring training and testing. in section 1 the experiments aiming to measure the impact of the feature selection on the classification performances as well as of the contribution of linguistic information are described.
1	text classification
two main approaches to the construction of a non-parametric classifier have been proposed and experimented in literature  lewis et al.  1 .
　profile-based  or linear  classifiers are characterized by a function that is based on a similarity measure between the representation of the incoming document and each class
　. both representations are vectors and similarity is traditionally estimated as the cosine angle between the two vectors. the description of each target class     is usually called profile  that is the vector summarizing the content of all the training documents pre-categorized under . the vector components are called features and refer to independent dimensions in the space in which similarity is estimated. traditional techniques  e.g.  salton and buckley  1; salton  1   make use of single words as basic features. the -th components of a vector representing a given document is a numerical weight associated to the -th feature of the dictionary that occurs in . similarly  profiles are derived from the grouping of positive instances in class  
i.e.	.
　example-based are other types of classifiers  in which the incoming document	is used as a query against the training data  i.e. the set of training documents . similarity between and class is evaluated as cumulative estimation between the input document and a portion of the training documents belonging to that class. the categories under which the training documents with the highest similarity are categorized  are considered as promising classification candidates for . this approach is also referred as document-centered categorization. for both the above models a document is considered valid for a given class iff the similarity estimation overcomes established thresholds. the latter are parameters that adjust the trade-off between precision and recall.
1	the problem of feature selection
feature selection techniques have been early introduced in order to limit the dimensionality of the feature space of text categorization problems. the native feature space consists of the unique terms  words or phrases  that occur in documents  which can be hundreds of thousands of terms even for a small text collection. this size prevents the applicability of many learning algorithms. few neural models  for example  can handle such a large number of features usually mapped into input nodes.
　automatic feature selection methods foresee the removal of noninformative terms according to corpus statistics  and the construction of new  i.e. reduced or remapped  feature set. common statistics parameters are: the information gain  e.g.  yang and pedersen  1   aggressively reduces the document vocabulary  according to a naive bayes model; a decision tree approach to select the most promising features wrt to a binary classification task; mutual information and a statistic have been used to select features for input to neural networks; document clustering techniques estimating probabilistic  term strength ; inductive learning algorithms that derive features in disjunctive normal form.
　as pointedout in  yang and pedersen  1 documentfrequency      and information gain provide the best selectors able to reduce the feature set cardinality and produce an increment of text classifier performances. the following equations describes four selectors among those experimented in  yang and pedersen  1 . they are based on both mutual information and statistics:
	max	 
	max	 
where is the probability of a generic document belong-
ing to a class	  as observed in the training corpus is a generic feature is the mutual information between	and	  is the	value between	and
　after the ranking is derived  selection is carried out by removing the features characterized by the lowest scores  thresholding . each of the above models produces a ranking of the different features that is the same for all the classes: each of the above formulas suggests only one weight depending on all the classes. for example  the selector of a feature by applies the average function to the set of scores: every dependence on the -th class disappear resulting in one single ranking. the same is true for and .
　notice that this ranking  uniform throughout categories  may select features which are non globally informative but are enough relevant only for a given  or few  class es   e.g. the or  . the selection cannot take into account differences in the relevance among classes. classes that are more generic  e.g. whose values of  or   tend to be low  may result in a very poor profile  i.e. fewer number of selected features. this is in line with the observation in  joachims  1  where the removal of features is suggested as a loss of important information  as the number of truly irrelevant features is negligible. moreover  functions like are even more penalizing as they flatten the relevance of a single feature for each class to an  ideal  average value. notice that this weakness is also reflected by the poorer results reported in  yang and pedersen  1 .
　in order to account for differences in the distribution of relevance throughout classes  we should depart from the idea of a uniform ranking. features should be selected with respect to a single category. this can lead to retain features only when they are truly informative for some classes. moreover a suitable class-based ranking is obtained  so that the feature scores  e.g. the mutual information   can be straightforwardly assumed as weights for the features in class .
　in next section an extension of the rocchio formula aiming to obtain such desirable feature weights is presented.
1	rocchio classifiers
the rocchio classifier is a profile based classifier  presented in  ittner et al.  1   which uses the rocchio's formula for building class profiles. given the set of training documents classified under the topics   the set of the documents not belonging to   and given a document and a feature   the weight of in the profile of is:
	max			 1 
　where represent the weights of features in documents1. in eq. 1 the parameters and control the relative impact of positive and negative examples and determine the weight of in the -th profile. in  ittner et al.  1    1  has been firstly used with values = 1 and = 1: the task was categorisation of low quality images. the success of these values possibly led to a wrong reuse of them in other fields  cohen and singer  1 .
　these parameters indeed greatly depend on the training corpus and different settings of their values produce a significant variation in performances. poor performances have been obtained indeed in  yang  1   and a wrong and setting  maybe the orginal ittner one  is a possible explanation.
in  joachims  1   the trial with a small set of values for
　    is carried out and increased performance wrt those previously assessed by other authors are obtained. however  the corresponding values are not mentioned. the impact of the adjustment of and is significant if optimal values are systematically estimated from the training corpus. experimental evidence will be further shown in section 1.
tuning rocchio's formula parameters
as previously discussed  the rocchio classifier strongly relies on the and setting. however  the relevance of a feature deeply depends on the corpus characteristic and  in particular  on the differences among the training material for the different classes  i.e. size  the structure of topics  the style of documents  .... this varies very much across text collections and across the different classes within the same collection.
　notice that  in equation 1  features with negative difference between positive and negative relevance are set to 1. this implies a discontinuous behavior of the values aroundthe 1. this aspect is crucial since the 1-valuedfeatures are irrelevant in the similarity estimation  i.e. they give a null contribution to the scalar product . this form of selection is rather smooth and allows to retain features that are selective only for some of the target classes. as a result  features are optimally used as they influence the similarity estimation for all and only the classes for which they are selective. in this way  the minimal set of truly irrelevant features  giving 1 values for all the classes  can be better captured and removed  in line with  joachims  1 .
　moreover  the and setting that is fitted with respect to the classification performance has two main objectives:
first  noise is drastically reduced by the rocchio formula smoothing and without direct feature deletion.
second  the resulting ranking provides rocchio-based scores that can be directly used as weights in the associated feature space. the higher is the positive evidence  wrt to the negative one  the higher is the relevance and this may vary for each target class.
　notice now that each category has its own set of relevant and irrelevant features and eq. 1 depends for each class on and . now if we assume the optimal values of these two parameters can be obtained by estimating their impact on the classification performance  nothing prevents us from driving this estimation independently for each class . this will result in a vector of       couples each one optimizing the performance of the classifier over the -th class. hereafter we will refer to this model as the classifier.
　notice that the proposed approach could converge to the traditional rocchio weighting if and only if a single optimal value for and is obtained  i.e. and . this has not been the case as in section 1 will be shown.
　finally  it has to be noticed that combined estimation of the two parameters is not required. for each class  we fixed one parameter   indeed  and let vary until the optimal performance is reached. the weighting  ranking and selection scheme used in the for classifier is thus the following:
	max			 1 
　in our experiments  has been set to 1  equation 1 has been applied given the parameters that for each class lead to the maximum breakeven point1 of .
1	the role of nlp in feature extraction
one of the aim of this work was to emphasize the role of linguistic information in the description  i.e. feature extraction  of different classes in a tc task. it is to be noticed that these latter are often characterized by sets of typical concepts usually expressed by multi-words expressions  i.e. linguistic structures synthesizing widely accepted definitions  e.g.  bond issues  in topics like  finance or stock exchange  . these sets provide useful information to capture semantic aspects of a topics. the multi-word expressions are at least in two general classes useful for tc:
proper nouns  pn   which usually do not bring much selective information in tc. most named entities are locations  persons or artifacts and are rarely related to the semantics of a class. an evidence of this is discussed in  basili et al.  1b  where pn removal is shown to improve performances.
terminological expressions  i.e. lemmatized phrase structures or single terms. their detection results in a more precise set of features to be included in the target vector space.
　the identification of linguistically motivated terminological structures usually requires external resources  thesaura or glossaries : as extensive repositories are costly to be developed and simply missing in most domains  an enumerative approach cannot be fully applied. automatic methods for the derivation of terminological information from texts can thus play a key role in content sensitive text classification.
　as terms embody domain specific knowledge we expect that their derivation from a specialized corpus can support the matching of features useful for text classification. once terms specific to a given topics are available  and they can be estimated from the training material for    their matching in future texts should strongly suggest classification of in
.
　several methods for corpus-driven terminology extraction have been proposed  e.g.  daille  1; arppe  1; basili et al.  1  . in this work  the terminology extractor described in  basili et al.  1 has been adopted in the training phase. each class  considered as a separate corpus  gives rise to a set of terms  . when available  lemmatized phrase structures or sinle lemmas in can be matched in future test documents. they are thus included in the final set of features of the target classifier.
　other features provided by linguistic processing capabiliites are lemmas and their associated pos information able to capture word syntactic roles  e.g. adjective  verb  noun 1.
　a further novel aspect of the classifier proposed in this paper is the application of the equation 1 as a weighting scheme of the linguistically derived features. this allows to better separate the relevantinformationfrom the irrelevantone  possibly introduced by errors in the linguistic processing  e.g. wrong pos assignment . finally  those irrelevant features  that are not necessarily produced via complex linguistic processing  e.g. single words   are correctly smoothed by eq. 1 and this also helps in a more precise measurement of the nlp contribution.
1	experimenting nlp-driven classification
the experiments have been carried out in two phases. first  we experimented rocchio classifiers over a standard feature set  i.e. simple words. this serves two purposes. first  it provides the evaluation of the breakeven point reachable by a
rocchio classifier  via estimation of the suitable  but global  parameter . this allows a direct and consistent comparisons with the rocchio models proposed in literature. furthermore the first expriment also suggests the parameter setting that provides the best breakeven point of the extended model. in a second phase this optimal
model has been experimented by feeding it with linguistic features. comparative analysis of the two outcomes provides evidence of the role of this augmented information.
　as a reference collection the reuters  version 1  corpus prepared by apte＞  yang  1; apte＞ et al.  1  has been used. it will be hereafter referred as reuters1. it includes 1 documents for 1 classes  with a fixed splitting between test and learning data  1 vs. 1 . every experiment thus allows direct comparisons with others models described in section 1.
1	deriving a baseline classifier
in these experiments  only words are taken as features and no other nlp facility has been applied in agreement with other methods described in literature. the feature weight in a document is the usual product between the logarithm of the feature frequency  inside the document  and the associated inverse document frequency. the best rocchio classifier performances has been derived by systematically setting different values of and optimizing performance  i.e. the breakeven point  bep . a sequence of classifiers has thus been obtained. in figure 1 the plot of beps with respect to is shown. the two plots refer to two different feature sets: simplefeatures refers to single words while lingfeatures refers to the model using all the available linguistic information  see sect. 1 . first of all  performances depend strongly on the parameters. the best performance of the simplefeatures model is reached with = 1  and =1 : these values significantly differ from the =1 and =1 used elsewhere. second  significant higher performances characterize the language-driven model for all the values: this shows an inherent superiority of the source linguistic information.
however  when a single for all the classes is used  a suitable adjustment let the simplefeatures model to approximate the behaviour of the linguistic one. this is no longer true when more selective estimation of the parameter  i.e.   is applied.

figure 1: break-evenpoint performancesof the rocchio classifier according to different values
　in a second experiment indeed the parameter estimation process has been individually applied to each class   and the optimal sequence of values has been obtained. in the table 1 are shown the performances of three rocchio classifiers: simple rocchio  as in  ittner et al.  1   with    as well as rocchio and characterized by one global parameter and individual parameters  respectively. the three tests have been carried out by using only simple
table 1: breakeven points of three rocchio-based models on reuters1
	rocchio	rocchio

	1%	1%	1% - 1%
words as features  in line with traditional techniques. notice that both the optimized rocchio and
models  proposed in this paper  outperformall the best results obtained in literature for rocchio classifiers  e.g.  joachims  1; cohen and singer  1  .
1	comparing different nlp-based classifiers
once the best weighting technique has been assessed as the optimal estimation of parameters in the previous experiments  it is possible to selectively measure the contribution given by nlp. in fact an independent baseline model  with minimal noisy information   i.e. the model in table 1   is used contrastively to correctly measure the contribution brought by the augmented features. in the following experiment  the novel sets of features described in section 1 have been added to the standard set. they consist of:
proper nouns  +pn or -pn if recognition in text is followed by removal during tc training 
terminological expressions  +te 
lemmas  -pos 
lemmas augmented with their pos tags in context  +pos 
terminologicalexpressions have been firstly derived from the training material of one class: for example  in the class
 i.e.	  of the reuters1  among the 1 different features about 1 are represented by terminological expressions or complex proper nouns    . the model has been selectively applied to three linguistic features set: the first includes only the lemmas associated to the pos tag  +pos   the second lemmas only  -pos   and the third proper nouns and terminological expressions  +pos+pn+te .
　in table 1 is reported the bep of the three feature sets: the comparison is against the baseline  i.e. the best non linguistic result of tab. 1  although reestimation of the parameters has been carried out  as shown by fig. 1 .
　we observe that both pos tag  column 1 vs column 1  and terminological expressions  column 1 vs column 1  produce improvements when included as features. moreover pns seems not to bring more information than pos tags  as column 1 suggests. the best model is the one using all the linguistic features providedby nlp. this increases performance     which is not negligible if considering the very high baseline.
table 1: breakeven points of on three feature set provides with nlp applied to reuters version 1
	base-line	+pos-pn	+pn+te	+pn+te+pos

	1%	1%	1%	1%
1	discussion
in table 1 the performances of the most successfull methods proposed in literature are reported. some of their distinctive aspects are here briefly summarized. support vector machines recently proposed in  joachims  1  is based on the structural risk minimisation principle. it uses quadratic programming technique for finding a surface that  best  separates the data points  the representation of training documents in the vector space model  in two classes. nearest neighbour is an example-based classifier   yang and liu  1   making use of document to document similarity estimation that selects a class for a document through a nearest heuristics. ripper  cohen and singer  1  uses an extended notion of profile  by learning contexts that are positively correlated with the target classes. a machine learning algorithmsallows the  contexts of a word to affect how  or whether  presence/absence of contribute actually to a classification. classi is a system that uses a neural networkbased approach to text categorization  h.t. ng  1 . the basic units of the network are only perceptrons. dtree  quinlan  1  is a system based decision trees. the dtree model allows to select relevant words  i.e. features   according to an information gain criterion. charade  i. moulinier and ganascia  1  and swap1  apte＞ et al.  1  use machine learning algorithms to inductively extract disjunctive normal form rules from training documents. sleeping experts  experts   cohen and singer  1  are learning algorithms that works on-line. they reduce the computation complexity of the training phase for large applications updating incrementally the weights of n-gram phrases.
　two major conclusions can be drawned. first of all the parameter estimation proposed in this paper is a significant improvement with respect to other proposed uses of the rocchio formula. the application of this method over crude featable 1: bep of best classifiers on reuters1 - revised
	knn	+nlp
1%1%1%1%ripperclassidtreeswap1%1%1%1%charadeexpertrocchionaive bayes	1%	1%	1%  1% - 1% 	1%-1%
ture sets  i.e. simple words and without any selection  improve significantly with respect to the best obtained rocchio methods   vs  . this weighting scheme is a robust filtering technique for sparse data in the training corpus. it has been suitably applied to derive the baseline figures for contrastive analysis of the role of linguistic features.
　the comparativeevaluationof simpler feature sets with linguistically motivated information  i.e. pos tagged lemmas and terminological information  suggests the superiority of the latter. this is mainly due the adoption of the optimal selection and weightingmethod proposed. it provides a systematic way to employ the source linguistic information. it has to be noiced that in the set of 1 features  including linguistic ones  derived from the training material of the reuters1 category  only 1   1%  assumes a weight greater than 1 after optimization is carried out. notice that the use of a single  global  value over linguistic features  i.e. +pos+pn+te   shown in fig. 1  lingfeature plot   reaches a best bep of about 1: this is improved of more that 1% in bep when selective setting is applied  tab. 1 . this form of weighting is thus responsible for an optimal employment of linguistic information that is  by its nature  often affected by data sparseness and noise.
1	conclusion
in this paper a new model able to exactly measure the contribution given by nlp has been designed and experimented. it brings significant evidence of the role of natural language processing techniques in the specific tc area of ir. the benefits of nlp methods are the efficient extraction of linguistically motivated complex features  including multi-word patterns . a novel weighting method has been also proposed. it provides a systematic feature selection functionality with a systematic estimation of the parameters in the rocchio formula. the method is robust and effective wrt noise as analysis over non linguistic feature sets demonstrates. this gave us the possibility of focusing on the measurement of relevance of nlp derived features. the applied to linguistic material supports thus a computationally efficient classification  typical of purely statistical models  and produces performances  about   comparable with the best  but computationally more expensive  classifiers  e.g. knn and svm .
references
 apte＞ et al.  1  chidanand apte＞  fred damerau  and sholom weiss. automated learning of decision rules for text categorization. acm transactions on information systems  1 :1  1.
 arppe  1  a. arppe. term extraction from unrestricted text. in nodalida  1.
 basili et al.  1  r. basili  g. de rossi  pazienza  and m.t. inducing terminology for lexical acquisition. in proceedings of the second conference on empirical methods in nlp  providence  usa  1.
 basili et al.  1a  r. basili  l. mazzucchelli  and m.t. pazienza. an adaptive and distributed framework for advanced ir. in in proceeding of 1th riao conference  content-based multimedia information access  collge de france  paris  france  1.
 basili et al.  1b  r. basili  a. moschitti  and m.t. pazienza. language sensitive text classification. in in proceeding of 1th riao conference  content-based multimedia information access  collge de france  paris  france  1.
 cohen and singer  1  william w. cohen and yoram singer. context-sensitive learning methods for text categorization. in proceedings of the 1th annual international acm sigir conference on research and development in information retrieval  sigir 1'   pages 1  1.
 daille  1  b. daille. study and implementation of combined techniques for automatic extraction of terminology. in the balancing act workshop of the 1nd annual meeting of the acl  1.
 grefenstette  1  gregory grefenstette. short queries linguistic expansion techniques: palliating one-word queries by providing intermediate structures to text. in m.t. pazienza  editor  information extraction. springer verlag  berlin  1.
 h.t. ng  1  k.l. low h.t. ng  w.b. goh. features selection  perceptron learning  and a usability case study for text categorization. in proceedings of the 1th acm sigir conference  pages 1  1.
 i. moulinier and ganascia  1  g. raskinis i. moulinier and j. ganascia. text categorization: a symbolic approach. in in proceedings of the 1th annual symposium on document analysis and information retrieval  1.
 ittner et al.  1  david j. ittner  david d. lewis  and david d. ahn. text categorization of low quality images. in proceedings of sdair-1  1th annual symposium on document analysis and information retrieval  pages 1  las vegas  us  1.
 joachims  1  t. joachims. transductive inference for text classification using support vector machines. in i. bratko and s. dzeroski editors  proceedings of icml-1  1th international conference on machine learning  pages 1 1.
 joachims  1  thorsten joachims. text categorization with support vector machines: learning with many relevant features. in in proceedings of ecml-1  pages 1  1.
 lewis et al.  1  david d. lewis  robert e. schapiro  james p. callan  and ron papka. training algorithms for linear text classifiers. in proceedings of 1th acm sigir-1  pages 1  zu：rich  ch  1.
 quinlan  1  j.r. quinlan. induction of decision trees. in machine learning  pages 1  1.
 salton and buckley  1  g: salton and c. buckley. termweighting approaches in automatic text retrieval. information processing and management  1 :1  1.
 salton  1  g. salton. development in automatic text retrieval. science  1-1  1.
 yang and liu  1  yiming yang and xin liu. a re-examination of text categorization methods. in in proceedings of acm sigir conference on research and development in information retrieval  1.
 yang and pedersen  1  yiming yang and jan o. pedersen. a comparative study on feature selection in text categorization. in proceedings of 1th icml-1  pages 1  nashville  us  1.
 yang  1  y. yang. an evaluation of statistical approaches to text categorization. information retrieval journal  1.

natural language processing
and information retrieval
natural language explanation and argumentation

dialog-driven adaptation of explanations of proofs
armin fiedler
universita：t des saarlandes  fr informatik
postfach 1 1  d-1 saarbru：cken  germany afiedler cs.uni-sb.de

abstract
in order to generate high quality explanations in mathematical domains  the presentation must be adapted to the knowledge of the intended audience. most proof presentation systems only communicate proofs on a fixed degree of abstraction independently of the addressee's knowledge. in this paper  we shall present the proof explanation system p.rex. based on assumptions about the addressee's knowledge  its dialog planner chooses a degree of abstraction for each proof step to be explained. in reaction to the user's interactions  which are allowed at any time  it enters clarification dialogs to revise its user model and to adapt the explanation.
1	introduction
a person who explains to another person a logical line of reasoning adapts his explanations to the addressee's knowledge. a computer program designed to take over the explaining part should also adopt this principle.
　assorted systems take into account the intended audience's knowledge in the generation of explanations  see e.g.   cawsey  1; paris  1; wahlster et al.  1; horacek  1  . most of them adapt to the addressee by choosing between different discourse strategies. feedback form the user himself can be an important source for user modeling.  moore and swartout  1  presents a contextsensitive explanation facility for expert systems that  one the one hand  allows the user to ask follow-up questions and  on the other hand  actively seeks feedback from the user to determine whether the explanations are satisfactory.  mooney et al.  1  emphasizes that the user must be able to interrupt the explanation system at any time.
　whereas a mathematician communicates a proof on a level of abstraction that is tailored to the audience and reacts to the audience's needs  most proof presentation systems such as proverb  huang and fiedler  1  verbalize proofs on a fixed degree of abstraction given by the initial representation of the proof without allowing for user interaction.
　drawing on results from cognitive science  we have been developing an interactive proof explanation system  called p.rex  for proof explainer   which adapts its explanations to the user's assumed expertise. the system explains each proof step of a proof on the most abstract level that the user is assumed to know. while the explanation is in progress  the user can interrupt p.rex anytime  when he is not satisfied by the current explanation. analyzing the user's interaction and entering a clarification dialog when needed  the system tries to identify the reason why the explanation was not satisfactory and re-plans a better adapted explanation  for example  by switching to another level of abstraction.
　hence  driven by the dialog  p.rex adapts its explanations to the user in reaction to his interactions. however  in the current experimental stage  only a small set of interactions is allowed.
　in this paper  we shall first introduce proofs as the domain objects that are subject to explanation in section 1. next  in section 1  we shall give an overview of the architecture of p.rex. then  in section 1  we shall describe the dialog planner in more detail. section 1 is devoted to example dialogs.
1	the domain
the objects of our domain that are to be explained are proofs of mathematical theorems. an example for a proof is given below. each line consists of four elements  label  antecedent  succedent  and justification  and describes a node of the proof. the label is used as a reference for the node. the antecedent is a list of labels denoting the hypotheses under which the formula in the node  the succedent  holds. this relation between antecedent and succedent is denoted by .
hyp
def
hyp
def
 -lemma case
　we call the fact in the node. the proof of the fact in the node is given by its justification. a justification consists of a rule and a list of labels  the premises of the node. denotes an unspecified justification. hyp and def stand for a hypothesis and the definition of   respectively. has two justifications on different levels of abstraction: the less abstract justification with the inference rule case  i.e.  the rule for case analyses  and the more abstract justification with the

figure 1: the architecture of p.rex
inference rule -lemma  which stands for an already proven lemma about a property of . by agreement  if a node has more than one justification  these are sorted from most abstract to least abstract.
　the proof is as follows: from we can conclude that by the -lemma. if we do not know the -lemma  we can come to the conclusion by considering the case analysis with the cases that or   respectively. in each case  we can derive that by the definition of .
1	the architecture of p.rex
p.rex is a generic explanation system that can be connected to different theorem provers. an overview of its architecture is provided in figure 1.
　a formal language for specifying proofs and mathematical theories is the interface by which theorem provers can be connected to p.rex. mathematical theories are organized in a hierarchical knowledge base. each theory in it may contain axioms  definitions  theorems along with proofs  as well as proof methods  and control rules how to apply proof methods.
　a proof of a theorem can be represented hierarchically such that it makes explicit the various levels of abstraction by providing several justifications for a single proof node  where each justification belongs to a different level of abstraction.
　the central component of the system is the dialog planner. it is implemented in act-r  anderson and lebiere  1   a goal-directed production system that aims to model human cognition. act-r has two types of knowledgebases  or memories  to store permanent knowledge in: declarative and procedural representations of knowledge are explicitly separated into the declarative memory and the procedural production rule base  but are intimately connected.
　procedural knowledge is represented in production rules  or simply: productions  whose conditions and actions are defined in terms of declarative structures. a production can only apply if its conditions are satisfied by the knowledge currently available in the declarative memory. an item in the declarative memory is annotated with an activation that influences its retrieval. the application of a production modifies the declarative memory  or it results in an observable event.
　to allowfor a goal-orientedbehavior of the system  act-r manages goals in a goal stack. the current goal is that on the top of the stack. only productions that match the current goal are applicable.
　the plan operators of the dialog planner are defined in terms of productions and the discourse structure is represented in the declarative memory. since presumed declarative and procedural knowledge of the user is encoded in the declarative memory and the production rule base  respectively  the dialog planner is modeling the user.
　in order to explain a particular proof  the dialog planner first assumes the user's supposed cognitive state by updating its declarative and procedural memories  which were recorded during a previous session. an individual model for each user persists between the sessions. the individual user models are stored in the database of user models. each user model contains assumptions on the knowledge of the user that are relevant to proof explanation. in particular  it makes assumptions on which mathematical theories the user knows  which definitions  proofs  proof methods and mathematical facts he knows  and which productions he has already learned.
　after updating the declarative and procedural memories  the dialog planner sets the global goal to show the conclusion of the proof's theorem. act-r tries to fulfill this goal by successively applying productions that decompose or fulfill goals. thereby  the dialog planner not only produces a dialog plan  cf. section 1   but also traces the user's cognitive states in the course of the explanation. this allows the system both to always choose an explanation adapted to the user  cf. section 1   and to react to the user's interactions in a flexible way: the dialog planner analyzes the interaction in terms of applications of productions. then it plans an appropriate response  cf. section 1 .
　the dialog plan produced by the dialog planner is passed on to the presentation component. currently  we use a derivate of proverb's micro-planner  huang and fiedler  1 to planthe scopeand internal structureof the sentences  which are then realized by the syntactic generator tag-gen  kilger and finkler  1 .
　the uttered sentences are finally displayed on the interface  which also allows the user to enter remarks  requests and questions. an analyzer receives the user's interactions and passes them on to the dialog planner. in the currentexperimental stage  we use a simplistic analyzer that understands a small set of predefined interactions.
1	discourse planning
in the community of nlg  there is a broad consensus that it is appropriate to generate natural language in three major steps  reiter  1;cahill et al.  1 . first  a macro-planner text planner  determines what to say  that is  content and order of the information to be conveyed. then  a micro-planner  sentence planner  determines how to say it  that is  it plans the scope and the internal structure of the sentences. finally  a realizer  surface generator  produces the surface text. in this classification  the dialog planner is a macro-planner for managing dialogs.
　the dialog planner of p.rex plans the dialog by building a representation of the structure of the discourse that includes speech acts as well as relations among them. the discourse structure is represented in the declarative memory. the plan operators are defined as productions.
1	discourse structure
drawing on rhetorical structure theory  rst   mann and thompson  1   hovy argues in favor of a single tree to represent a discourse  hovy  1 . he considers a discourse as a structured collection of clauses  which are grouped into segments by their semantic relatedness. the discourse structure is expressed by the nesting of segments within each other according to specific relationships  i.e.  rst relations . similarly to hovy's approach  we describe a discourse by a discourse structure tree  where each node corresponds to a segment of the discourse. the speech acts  which correspond to minimal discourse segments  are represented in the leaves. we achieve a linearization of the speech acts by traversing the discourse structure tree depth-first from left to right.
1	speech acts
speech acts are the primitive actions planned by the dialog planner. they represent frozen rhetorical relations between exchangeable semantic entities. the semanticentities are represented as arguments to the rhetorical relation in the speech act. each speech act can always be realized by a single sentence. we use speech acts in p.rex not only to represent utterances that are produced by the system  but also to represent utterances from the user in the discourse.
　we distinguish two major classes of speech acts. first  mathematical communicative acts  mcas  are employed to convey mathematical concepts or derivations. mcas suffice for those parts of the discourse  where the initiative is taken by the system. second  interpersonal communicative acts  icas  serve the dialog  where both the system and the user alternately take over the active role.
mathematical communicative acts
mathematical communicative acts  mcas  are speech acts that convey mathematical concepts or derivations. our class of mcas was originally derived from proverb's pcas  huang  1   but has been substantially reorganized and extended. we distinguish two classes of mcas:
derivational mcas convey steps of the derivation  which are logically necessary. failing to produce a derivational mca makes the presentation logically incorrect. the following is an example for a derivational mca given with a possible verbalization:
	 derive :reasons  	 
:conclusion
:method -lemma 
	 since	or	 	by the	-
lemma. 
explanatory mcas comment on the steps ofa derivation or give information about the structure of a derivation. this information is logically unnecessary  that is  omission leaves the derivation logically correct. however  inclusion of explanatory mcas makes it much easier for the addressee to understand the derivations  since these comments keep him oriented. for example  an mca of type case introduces a case in a case analysis:
	 case :number 1 :hypothesis	 
	 case 1: let	. 
interpersonal communicative acts
mcas  which only convey information to the dialog partner without prompting any interaction  suffice to present mathematical facts and derivations in a monolog. to allow for dialogs we also need interpersonal communicative acts  icas   which are employed for mixed-initiative  interpersonal communication. in our taxonomization we distinguish four classes of icas: questions  requests  acknowledgments and notifications. note that the user never enters speech acts directly into the system. instead  the user's utterances are interpreted by the analyzer and mapped into the corresponding speech acts.
　icas are especially important to allow for clarification dialogs. if the system failed to successfully communicate a derivation  it starts a clarification dialog to detect the reason for the failure. then  it can re-plan the previously failed part of the presentation and double-check that the user understood the derivation. we shall come back to this issue in section 1.
1	plan operators
operational knowledge concerning the presentation is encoded as productions. each production either fulfills the current goal directly or splits it into subgoals. let us assume that the following nodes are in the current proof:
..
.
an example for a production is:
 p1 if	the current goal is to show and	is the most abstract known rule justifying the current goal
	and	are known
then produce mca	 derive :reasons  	 
	:conclusion	:method  
insert it in the discourse structure tree and pop the current goal
by producing the mca the current goal is fulfilled and can be popped from the goal stack. an example for a production decomposing the current goal into several subgoals is:
 p1 	if	the current goal is to show and	is the most abstract known rule justifying the current goal
	and	is unknown for
	then for each	  push the goal to show
note that the conditions of and only differ in the knowledge of the premises for rule . introduces the subgoals to prove the unknown premises in . as soon as those are derived  can apply and derive the conclusion.
hence  and in principle suffice to plan the presentation of a proof starting from the conclusion and traversing the proof tree towards its hypotheses. however  certain proof situations call for a special treatment. assume that the following nodes are in the current proof:
hyp
hyp
case
　a specific production managing such a case analysis is the following:
 p1 if the current goal is to show and case is the most abstract known rule justifying the current goal
and	is known and and are unknown
then push the goals to show	andand produce mca
 case-analysis :goal
　　　　　　　　　:cases  	   and insert it in the discourse structure tree
this production introduces new subgoals and motivates them by producing the mca.
　since more specific productions such as treat common communicative standards used in mathematical presentations  they are preferred to more general ones. note that the productionsensure that only those inferencerules are selected for the explanation that are known to the user.
1	user interaction
the ability for user interaction is an important feature of explanation systems.  moore and swartout  1  presents a context-sensitive explanation facility for expert systems that  one the one hand  allows the user to ask follow-up questions and  on the other hand  actively seeks feedback from the user to determine whether the explanations are satisfactory.  mooney et al.  1  emphasizes that the user must be able to interrupt the explanation system at any time.
　in p.rex  the user can interact with the system at any time. when the system is idle-for example  after starting it or after completion of an explanation-it waits for the user to tell it the next task. during an explanation  p.rex checks after each production cycle if the user wishes to interrupt the current explanation. each interaction is analyzed by the analyzer and passed on to the dialog planner as a speech act  which is included in the current discourse structure tree.
　we allow for three types of user interaction in p.rex: a command tells the system to fulfill a certain task  such as explaining a proof. an interruption interrupts the system to inform it that an explanation is not satisfactory or that the user wants to insert a different task. in clarification dialogs  finally  the user is prompted to give answers to questions that p.rex asks when it cannot identify a unique task to fulfill. in this paper  we shall concentrate on interruptions.
interruptions
the user can interrupt p.rex anytime to enter a new command or to complain about the current explanation. the following speech acts are examples for messages that can be used to interrupt the system:
 too-detailed :conclusion  
the explanation of the step leading to is too detailed  that is  the step should be explained at a more abstract level.
 too-difficult :conclusion  
the explanation of the step leading to is too difficult  that is  the step should be explained in more detail.
the reaction to too-detailed
when the user complains that the derivation of a conclusion was too detailed  the dialog planner checks if there is a higher level of abstraction on which can be shown. if so  the corresponding higher level inference rule is marked as known  so that it is available for future explanations. then  the explanation of the derivation of is re-planned. otherwise  the dialog planner informs the user  that there is no higher level available for presentation. this reaction of the system is encoded in the following two productions:
 p1 if	the user message is
 too-detailed :conclusion  
and the inference rule was used to justify and there is an inference rule justifying that
is more abstract than
then mark as known by the user and push the goal to show p1 if	the user message is
 too-detailed :conclusion  
and the inference rule was used to justify and there is no inference rule justifying that
is more abstract than
then produce ica most-abstract-available :rule   and insert it in the discourse structure tree
　an example dialog where the user complained that the original explanation of a proof was too detailed shall be given in example 1 in section 1.
the reaction to too-difficult
when the user complains that the derivation of a conclusion was too difficult  the dialog planner enters a clarification dialog to find out which part of the explanation failed to remedy this failure. the control of the behavior of the dialog planner is displayed in figure 1. note that every arrow in the figure corresponds to a production such that we cannot give the productions here due to space restrictions.
　to elucidate the diagram in figure 1  an example dialog where the user complained that the original explanation of a proof was too difficult shall be given in example 1 in the following section.
1	example dialogs
let us examine more closely how p.rex plans the discourse with the help of two example dialogs. in both dialogs  p.rex

figure 1: the reaction of the dialog planner if a step was too difficult.
explains the proof given in figure 1. note that this proof consistsof two similarlyproved parts with and as roots  respectively.
example 1	let us consider the following situation:
the current goal is to show the fact in . the next goal on the stack is to show the fact in
the rules hyp  case  and def are known  the rule -lemma is unknown.
the facts in and are known  the facts in               and are unknown.
　since case is the most abstract known rule justifying the current goal  both decomposing productions and are applicable. recall that more specific productions are preferred to more general ones. therefore  the dialog planner chooses for the explanation  thus producing the mca
 case-analysis :goal
　　　　　　　　　　　:cases  	   which can be realized as  to prove	let us consider the cases that	and	   and then explains both cases.
　suppose now that the user interrupts the system throwing in that the presentation is too detailed. then  the analyzer passes the speech act  too-detailed :conclusion
           to the dialog planner. since the inference rule -lemma  which is more abstract than case  also justifies the conclusion  production applies. hence  the inference rule -lemma is marked as known and the goal to show
hyp
def
hyp
def
-lemma
case
hyp
def
hyp
def
　　　　　　　　　　　　　　　　-lemma case figure 1: a proof to be explained by p.rex.
the fact in is again pushed onto the goal stack. then  is the only applicable production. since -lemma is more abstract than case and both are known  it is chosen to instantiate . hence  the dialog planner produces the mca
	 derive :reasons  	 
:conclusion
　　　　　　　　:method -lemma  whichcan be verbalizedas  since	or	  by the	-lemma. 
　since -lemma is now marked as known by the user  it can also be used for presentation in subsequent situations  for example  when is to be shown. the whole dialog takes place as follows:
p.rex: in order to prove that	let us consider the following cases.
case 1: let . then by the definition of .
case 1: let . that implies that by the definition of .
user: this derivation is too detailed.
p.rex: since or   by the lemma. since or   by the -lemma.　to elucidate the behavior of the dialog planner as depicted in figure 1  let us examine the following example:
example 1	we now consider the following situation:
the current goal is to show the fact in	. the next goal on the stack is to show the fact in the rules hyp  case  def   and	-lemma are known 
the facts in and are known  the facts in               and are unknown.
	this time  the only applicable production is	. since
 -lemma is more abstract than case and both are known  it is chosen to instantiate . hence  the dialog planner produces the mca
	 derive :reasons  	 
:conclusion
:method -lemma 
whichcan be verbalized as  since or   by the -lemma. 
　suppose now that the user points to this utterance and interrupts p.rex throwing in that this step was too difficult. the analyzer translates the user's interaction to the speech act
 too-difficult :conclusion  . now  the dialog planner enters the clarification dialog as displayed in figure 1. since all premises were explicitly mentioned  the system does not reverbalize the step  but asks whether all premises are understood  what the user affirms. hence  the system checks whether there is a lower level of abstraction  at which the step can be presented. since this is the case  p.rex replans the explanation of the step by marking the inference rule -lemma as unknown and pushing the goal to show the fact in onto the goal stack. now  is not applicable  since -lemma is unknown  but and are applicable. hence  the system proceeds as in the beginning of example 1 and verbalizes the case analysis. since -lemma is unknown  it is not used in subsequent situations either. therefore  is also explained by a case analysis. the whole dialog takes place as follows:
p.rex: since	or	 	by the
-lemma.
user: this step is too difficult.
p.rex: do you understand the premises 
user: yes.
p.rex: in order to prove that let us consider the following cases.
case 1: let . that leads to by the definition of .
case 1: let . then by the definition of .
do you understand this step 
user: yes.
p.rex: in order to prove that let us consider the following cases.
case 1: let . therefore by the definition of .
case 1: let . then by the definition of .1	conclusion
in this paper  we presented the dialog planner of the proof explanation system p.rex. based on assumptions about the addressee's knowledge  e.g.  which facts does he know  which definitions  lemmas  etc.   the dialog planner chooses a degree of abstraction for each proof step to be explained. in reaction to the user's interactions  it enters clarification dialogs to revise its user model and to adapt the explanation. the architecture of the dialog planner can also be used to adapt content selection and explicitness reactively to the audience's needs. the rationale behind the architecture should prove to be useful for explanation systems in general.
　however  in the currentexperimental stage  only a smallset of user interactions is allowed. more elaborate interactions that call for more complex reactions are desirable. therefore  empirical studiesof teacher-student interactions in mathematics classes are necessary.
references
 anderson and lebiere  1  john r. anderson and christian lebiere. the atomic components of thought. lawrence erlbaum  1.
 cahill et al.  1  lynne cahill  christy doran  roger evans  chris mellish  daniel paiva  mike reape  donia scott  and neil tipper. in search of a reference architecture for nlg systems. in proceedingsof the 1th europeanworkshop on natural language generation  pages 1  toulouse  france  1.
 cawsey  1  alison cawsey. generating explanatory discourse. in robert dale  chris mellish  and michael zock  editors  current research in natural language generation  number 1 in cognitive science series  pages 1. academic press  san diego  ca  1.
 horacek  1  helmut horacek. a model for adapting explanations to the user's likely inferences. user modeling and useradapted interaction  1-1  1.
 hovy  1  eduard h. hovy. automated discourse generation using discourse structure relations. artificial intelligence  1- 1  1.
 huang and fiedler  1  xiaorong huang and armin fiedler. proof verbalization as an application of nlg. in martha e. pollack  editor  proceedings of the 1th international joint conference on artificial intelligence  ijcai   pages 1  nagoya  japan  1. morgan kaufmann.
 huang  1  xiaorong huang. human oriented proof presentation: a reconstructive approach. phd thesis  fachbereich informatik  universita：t des saarlandes  saarbru：cken  germany  1.
 kilger and finkler  1  anne kilger and wolfgang finkler. incremental generation for real-time applications. researchreport rr-1  dfki  saarbru：cken  germany  july 1.
 mann and thompson  1  william c. mann and sandra a.
thompson. rhetorical structure theory: a theory of text organization. isi reprintseries isi/rs-1  univerisity of southern california/information science institute  marina del rey  ca  1.
 mooney et al.  1  david j. mooney  sandra carberry  and kathleen mccoy. capturing high-level structure of naturally occurring  extended explanations using bottom-up strategies. computational intelligence  1-1  1.
 moore and swartout  1  johanna d. moore and william r. swartout. a reactive approach to explanation: taking the user's feedback into account. in ce＞cile l. paris  william r. swartout  and william c. mann  editors  natural language generation in artificial intelligence  pages 1  boston  ma  usa  1. kluwer.
 paris  1  ce＞cile paris. the role of the user's domain knowledge in generation. computational intelligence  1-1  1.
 reiter  1  ehud reiter. has a consensus nl generation architecture appeared  and is it psycholinguistically plausible  in proceedings of the 1th internationalworkshop on natural language generation  pages 1  kennebunkport  maine  usa  1.
 wahlster et al.  1  wolfgang wahlster  elisabeth andre＞  wolfgang finkler  hans-ju：rgen profitlich  and thomas rist. planbased integration of natural language and graphics generation. artificial intelligence  1-1  1.
generating tailored examples to support learning via self-explanation 
 
cristina conati and giuseppe carenini 
department of computer science  
university of british columbia 
vancouver  bc  canada  v1t 1 
{conati  carenini} cs.ubc.ca  
abstract 
we describe a framework that helps students learn from examples by generating example problem solutions whose level of detail is tailored to the students' domain knowledge. the framework uses natural language generation techniques and a probabilistic student model to selectively introduce gaps in the example solution  so that the student can practice applying rules learned from previous examples in problem solving episodes of difficulty adequate to her knowledge. filling in solution gaps is part of the meta-cognitive skill known as selfexplanation  generate explanations to oneself to clarify an example solution   which is crucial to effectively learn from examples. in this paper  we describe how examples with tailored solution gaps are generated and how they are used to support students in learning through gap-filling self-explanation. 
1 	introduction 
studying examples is one of the most natural ways of learning a new skill. thus  substantial research in the field of intelligent tutoring systems  its  has been devoted to understand how to use examples to enhance learning. most of this research has focused on how to select examples that can help a student during problem solving  e.g.  burrow and weber 1; aleven and ashley 1 . in this paper  we focus on how to describe an example solution so that a student can learn the most by studying it previous to problem solving. in particular  we address the issue of how to vary the level of detail of the presented example solution  so that the same example can be equally stimulating for learners with different degrees of domain knowledge.  
 this problem is novel in its  as it requires sophisticated natural language generation  nlg  techniques. while the nlg field has extensively studied the process of producing text tailored to a model of the user's inferential capabilities  e.g.  horacek 1; korb  mcconachy et al. 1; young 1   the application of nlg techniques in its are few and mainly focused on managing and structuring the tutorial dialogue  e.g.  moore 1; freedman 1   rather than on tailoring the presentation of instructional material to a detailed student model. 
 the rationale behind varying the level of detail of an example solution lies on cognitive science studies showing that those students who self-explain examples  i.e.  generate explanations to themselves to clarify an example solution  learn better than those students who read the examples without elaborating them  chi 1 . one kind of selfexplanation that these studies showed to be correlated with learning involves filling in the gaps commonly found in textbook example solutions  gap filling self-explanation . however  the same studies also showed that most students tend not to self-explain spontaneously. in the case of gap filling  this phenomenon could be due to the fact that gap filling virtually requires performing problem solving steps while studying an example. and  because problem solving can be highly cognitively and motivationally demanding  sweller 1   if the gaps in an example solution are too many or too difficult for a given student  they may hinder self-explanations aimed at filling them.  
 we argue that  by monitoring how a student's knowledge changes when studying a sequence of examples  it is possible to introduce in the examples solution gaps that are not too cognitively demanding  thus facilitating gap filling self-explanation and providing a smooth transition from example study to problem solving. we are testing our hypothesis by extending  the se-coach  a framework to support self-explanation of physics examples  conati and vanlehn 1 .  
 the se-coach already effectively guides two other kinds of self-explanations that have been shown to trigger learning  chi 1 :  i  justify a solution step in terms of the domain theory  step correctness ;  ii  map a solution step into the high-level plan underlying the example solution  step utility . the internal representation of an example solution used by the se-coach to monitor students' self-explanation is generated automatically. however  because the se-coach does not include any nlg capability  the example description presented to the student and the mapping between this description and the internal representation is done by hand. thus  each example has a fixed description  containing virtually no solution gaps.  
 in this paper  we describe how we extended the se-coach with nlg techniques to  i  automatically generate the example presentation from the example internal representation  ii  selectively insert gaps in the example presentation  tailored to a student's domain knowledge.  
 several nlg computational models proposed in the literature generate concise text by taking into account the inferential capabilities of the user.  young 1  generates effective plan descriptions tailored to the hearer's plan reasoning capabilities.  horacek 1  is an example of models that take into account the hearer's logical inference capabilities. and  korb  mcconachy et al. 1  proposes a system that relies on a model of user's probabilistic inferences to generate sufficiently persuasive arguments. 
 in contrast  our generation system tailors the content and organisation of an example to a probabilistic model of the user logical inferences  which allows us to explicitly represent the inherent uncertainty involved in assessing a learner's knowledge and reasoning processes. furthermore  our system maintains information on what example parts are not initially presented  i.e.  solution gaps   which is critical to support gap-filling self-explanations for those students who tend not to self-explain autonomously.    
 in the following sections  we first illustrate our general framework for example generation. we then describe in detail the nlg techniques used and an example of the tailored presentations they generate.  finally  we show how the output of the nlg process supports an interface to guide gap filling self-explanation.  
1 	the framework for example generation  

figure 1: framework for example generation 
figure 1 shows the architecture of our framework for generating tailored example presentations. the part of the framework labelled  before run-time  is responsible for generating the internal representation of an example solution from  i  a knowledge base  kb  of domain and planning rules  for physics in this particular application ;  ii  a formal description of the example initial situation  given quantities and sought quantities  conati and vanlehn 1 . a problem solver uses these two knowledge sources to generate the example solution represented as a dependency network  known as the solution graph.  the solution graph encodes how each intermediate result in the example solution is derived from a domain or planning rule and from previous results matching that rule's preconditions. consider  for instance  the physics example in figure 1  example1 . figure 1 shows the part of solution graph that derives the first three steps mentioned in example1 solution: establish the goal to apply newton's 1nd law; select the body to which to apply the law; identify the existence of a tension force on the body. 
 in the solution graph  intermediate solution facts and goals  f- and g- nodes in figure 1  are connected to the rules  r- nodes  used to derive them and to previous facts and goals matching these rules' enabling conditions. the connection goes through rule-application nodes  ra- nodes in figure 1   explicitly representing the application of each rule in the context of a specific example. thus  the segment of network in figure 1 encodes that the rule r-try-newton-1law establishes the goal to apply newton's 1nd law  node g-trynewton-1law  to solve the goal to find the force on jake  node g-force-on jake .  

figure 1: sample newtonian physics example 
 the rule r-goal-choose-body sets the  subgoal to find a body to apply the newton's 1nd  law  node g-goal-choosebody   while the rule r-find-forces sets the subgoal to find all the forces on the body  node g-find-forces . the rule rbody-by-force dictates that  if one has the goals to find the force on an object and to select a body to apply newton's 1nd law  that object should be selected as the body. thus  in 
figure 1 this rule selects jake as the body for example1  node  f-jake-is the body .  the rule r-tension-exists says that if an object is tied to a taut string  then there is a tension force exerted by the string on the object. when applied to example1  this rule generates the fact that there is a tension force on jake  node f-tension-on-jake in figure 1 . 
 the solution graph can be seen as a model of correct selfexplanation for the example solution  because for each solution fact it encodes the various types of selfexplanations relevant to understand it: step correctness   what domain rule generated that fact   step utility  what goal that fact fulfils  and gap filling  how the fact derives from previous solution steps .  
 

figure 1: segment of solution graph for example1 
 in the se-coach  every time a student is shown an example  the corresponding solution graph provides the structure for a bayesian network  see right bottom side of figure 1  that uses information on  how the student reads and self-explains that example to generate a probabilistic assessment of how well the student understands the example and the related rules  conati and vanlehn 1 . the prior probabilities to initialise the rule nodes in the bayesian network come from the long-term student model  see figure 1   which contains a probabilistic assessment of a student's current knowledge of each rule in the kb. this assessment is updated every time the student finishes studying an example  with the new rule probabilities computed by the corresponding bayesian network.   
   in the se-coach  the solution graph and bayesian network described above are used to support students in generating self-explanations for correctness and utility only.  no explicit monitoring and support for gap filling selfexplanation is provided. this is because in the se-coach  the description of the example solutions presented to the student and the mapping between these descriptions and the corresponding solution graphs are done by hand. this makes it impossible to tailor an example description to the dynamically changing student model by inserting gaps at the appropriate difficulty level for a given student. we have overcome this limitation by adding to the se-coach the example generator  see right part of figure 1   a nlg system that can automatically tailor the detail level of an example description to the student's knowledge  in order to stimulate and support gap-filling self-explanation. 
1 	the example generator  eg  
 eg is designed as a standard pipelined nlg system  reiter and dale 1 . a text planner  young and moore 
1  selects and organizes the example content  then a microplanner and a sentence generator realize this content into language. in generating an example  eg relies on two key communicative knowledge sources  right part of figure 1 :  i  a set of explanation strategies that allow the text planner to determine the example's content  organization and rhetorical structure;  ii  a set of templates that specifies how the selected content can be phrased in english. 
 the design of these sources involved a complex acquisition process. we obtained an abstract model of an example's content and organisation from a detailed analysis of the rules used to generate the solution graph. this was combined with an extensive examination of several physics textbook examples  which also allowed us to model the examples' rhetorical structure and the syntactic and semantic structure of their clauses. to analyse the rhetorical structure of the examples  we followed relational discourse analysis  rda   moser  moore et al. 1   a coding scheme devised to analyse tutorial explanations. the semantic and syntactic structure of the examples' clauses was used to design the set of templates that map content into english.   
 we now provide the details of the selection and organisation of the example content. in eg  this process relies on the solution graph and on the probabilistic long term student model.  it consists of two phases  text planning and revision   to reduce the complexity of the plan operators and increase the efficiency of the planning process. text planning selects from the solution graph a knowledge pool of all the propositions  i.e.  goals and facts  necessary to solve a given example  and it organizes them according to ordering constraints also extracted from the solution graph. the output of this phase  if realized  would generate a fully detailed example solution. after text planning  a revision process uses the assessment in the student's long-term model to decide whether further content selection can be performed to insert appropriate solution gaps. text planning and revision are described in the following sub-sections.  
1 	text planning process 
	 b 	 describe example1 
 a 	 inform-about-problem find-force 	enable goal:act
 describe-solution-method  method
 :constraints
            find-steps  method  steps 
 :sub-actions
       a1  inform-about  method  
        a1  describe-method-steps  steps   
:relations
       r1  enable a1 a1  
        r1  goal:act a1 a1     describe-solution-method newton's-1nd-
law   inform-about-method newton's-1nd-law 
	 describe-substeps-method newton's-1nd-law 	enable goal:act
 describe-step  choose-body 
　　　　　　　　　　　　　　 inform-about  choose-simple-body jake   enable preparation:act
graphical
	 describe-step   body's-properties 	 show free-body-diagram 	actions...
joint step1:step1
　　　　　　　　　　　　　　　　　 inform-about  act-on jake tension    describe-step all-forces-on-body 
 describe-step specify-component-equations  inform-about  act-on jake weight  
 describe-step choose-coordinate-axes 
 describe-step write-component-equations 
	communicative action decomposition	intentional/informational relations
 
figure 1:  a  sample explanation strategy.  b  portion of the text plan  the input to the text planner consists of  i  the abstract communicative action of describing an example solution;  ii  the example solution graph;  iii  the explanation strategies. the planning process selects and organizes the content of the example solution by iterating through a loop of communicative action decomposition1. abstract actions are decomposed until primitive communicative actions  executable as speech acts  are reached. in performing this task  the text planner relies on the set of explanation strategies that specify possible decompositions for each communicative action and the constraints dictating  when they may be applied. these constraints are checked against     the solution graph and when they are satisfied the decomposition is selected and appropriate content is also extracted from the solution graph. for illustration  figure 1 a  shows a simplified explanation strategy that decomposes the communicative action describe-solutionmethod. possible arguments for this action are  for instance  the newton's-1nd-law and the conservation-of-energy methods. looking at the details of the strategy  the function find-steps  :constraints field   checks in the solution graph whether the method has any steps. if this is the case  the steps are retrieved from the solution graph and the describesolution-method action is decomposed in an inform-about primitive action and in a describe-method-steps abstract action. the output of the planning process is a text plan  a data structure that specifies what propositions the example should convey  a partial order over those propositions and the example rhetorical structure. a portion of the text plan generated by eg for example1 is shown in figure 1 b . 
 the propositions that the example should convey are specified as arguments of the primitive actions in the text plan. in figure 1 b  all primitive actions are of type inform. for instance  the primitive action  inform-about  act-on jake weight   specifies the proposition  act-on jake weight   which is realized in the example description as  the other force acting on jake is his weight . in the text plan  the communicative actions are partially ordered. this ordering is not shown in the figure for clarity's sake; the reader can assume that the actions are ordered starting at the top. the example rhetorical structure consists of the action decomposition tree and the informational/intentional relations among the communicative actions. for instance  in  b   the rhetorical structure associated with the action describe-solution-method specifies that  to describe the solution method  the system has to perform two actions:  i  inform the user about the method adopted;  ii  describe all the steps of the method. between these two actions the enable intentional relation and the goal:act informational relation hold. all the informational /intentional relations used in eg are discussed in  moser  moore et al. 1 . we clarify here only the meaning of the enable relation because this relation is critical in supporting gap-filling selfexplanations. an intentional enable relation holds between two communicative actions if one provides information intended to increase either the hearer's understanding of the material presented by the other  or her ability to perform the domain action presented by the other. 
1 	the revision process 
once the text planner has generated a text plan for the complete example  the revision process revises the plan to possibly insert solution gaps that can make the example more stimulating for a specific student. the idea is to insert solution gaps of adequate difficulty   so that the student can practice applying newly acquired knowledge without incurring in the excessive cognitive load that too demanding problem solving can generate  sweller 1 .  
 the revision process performs further content selection by consulting the probabilistic long-term student model that estimates the current student's domain knowledge. more specifically  the revision process examines each proposition specified by a primitive communicative action in the text plan and  if according to the student model  there is high probability that the student knows the rule necessary to infer that proposition  the action is de-activated. de-activated actions are kept in the text plan but are not realized in the text  thus creating solution gaps. however  as we will see in the next section  de-activated actions may be realized in follow-up interactions.  
 as an illustration of the effects of the revision process on content selection  compare the example solutions shown in figure 1 and figure 1. figure 1 displays the worked out solution for  example1 which   similarly to example1  does not contain any solution gaps. in contrast  the same portion of example1 solution shown in figure 1 is much shorter  including several solution gaps. as previously described  eg determines what information to leave out by consulting the long-term probabilistic student model. in particular  the concise solution in figure 1 is generated by eg if the student had previously studied example1 with the secoach and generated self-explanations of correctness and utility providing sufficient evidence that she understands the rules used to derive example1 solution. when selecting the content for example1  eg leaves out all the propositions derived from the rules that the student has learned from example1. notice  for instance  that the concise solution in figure 1 does not mention the solution method used and the weight force.  also  the choice of the body and of the coordinate system is only conveyed indirectly. 

figure 1 portion of example1 without solution gaps 
even if a student has sufficient knowledge to fill in the solution gaps inserted by the revision process  she may  not actually perform the required inferences when studying the example. as a matter of fact  cognitive science studies show that most students tend not to self-explain spontaneously  chi 1 . thus  once the text plan is revised and realized  the system presents the concise example with tools designed to stimulate gap filling self-explanation as we illustrate in the next section. 
1 	support for gap filling self-explanation 
 to support gap-filling self-explanation  we have extended the interface that the se-coach uses to support selfexplanations for step correctness and utility. in this interface  each example's graphical element and solution step presented to the student is covered with grey boxes. figure 1 a  shows a segment of the example solution in figure 1 as presented with the masking interface.  
 to view an example part  the student must move the mouse over the box that covers it  thus allowing the 
interface to track what the student is reading. when the  
 

figure 1 portion of example1 with solution gap 

solution filling missing steps text item for gap fill in the following missing step s  the force n exerted on the wagon by the ground is a normal force text item for gap submit done  b 
figure 1 interface tools for  gap filling self-explanation student uncovers an example part  a  self-explain  button appears next to it   see figure 1 a  . clicking on this button generates more specific prompts that suggest one or more of the self-explanations for correctness  utility or gap filling  depending upon which of them are needed by the current student to fully understand the uncovered step. in particular  the text plan produced by eg is the key element in determining whether a prompt for gap filling is generated. a prompt for gap filling is generated whenever some of the primitive communicative actions that  were de-activated during the revision process are related through an enable intentional relation to the communicative action expressing the uncovered example part.  the rationale behind this condition is that a solution gap with respect to an example part comprises all the solution steps that were left out  but whose understanding is a direct precondition to derive that example part. for instance  given the example part uncovered in figure 1 a   there is only one solution gap preceding it  namely the one corresponding to the communicative action inform-about  choose-simple-bodyjake 1. as shown in figure 1 a   the prompt for gap filling is generated by adding the item  filling in missing steps  to the self-explain menu. if the student clicks on this item  the interface inserts in the solution text an appropriate number of masking boxes  representing the missing steps  see figure 1 b   left panel  first box from top . the interface also activates a dialogue box containing a blank for each missing step  that the student can use to fill in the step  see figure 1 b   right panel . since the interface currently does not process natural language input  the student fills each blank by selecting an item in the associated pull-down menu. eg generates the entries in this menu by applying the realisation component to unrealised communicative actions in the text plan  see figure 1 .  
  the student receives immediate feedback on the correctness of his selection  which is also sent to the bayesian network built for the current example  see figure 1 . the network fact node that corresponds to the missing step is clamped to either true or false  depending on the correctness of the student's selection  and the network updates the probability of the corresponding rule consequently. thus  if the student's actions show that he is not ready to apply a given rule to fill a solution gap  this rule's probability will decrease in the long-term student model. as a consequence  the next presented example involving this rule will include the solution steps the rule generates  giving the student another opportunity  to see how the rule is applied.   
1 	conclusions and future work 
 we have presented a tutoring framework that integrates principles and techniques from its and nlg to improve the effectiveness of example studying for learning. our framework uses an nlg module and a probabilistic student model to introduce solution gaps in the example solutions presented to a student. gaps are introduced when the student model assesses that the student has gained from previous examples sufficient knowledge of the rules necessary to derive the eliminated steps. the goal is to allow the student to practice applying these rules in problem solving episodes of difficulty adequate for his knowledge.   
 our framework is innovative in two ways. first  it extends its research on supporting the acquisition of the learning skill known as self-explanation  by providing tailored guidance for gap filling self-explanation. second  it extends nlg techniques on producing user-tailored text by relying on a dynamically updated probabilistic model of the user logical inferences.  
 the next step in our research will be to test the effectiveness of our framework through empirical studies. these studies are crucial to refine the probability threshold currently used to decide when to leave out a solution step  and possibly to identify additional principles to inform the text plan revision.  additional future work involves nlg research on how the example text plan can be used to maintain the coherence of the other example portions  when the student fills a solution gap. 
 
references  
 
 aleven and ashley 1  aleven  v. and k. ashley. teaching case-based argumentation through a model and examples: empirical evaluation of an intelligent learning environment. aied'1  kobe  japan  august 1. 
 burrow and weber 1  burrow  r. and g. weber. example explanation in learning environments. intelligent 
tutoring systems - proceedings of the third international conference  its '1  springer  june 1. 
 chi 1  chi  m. t. h. self-explaining expository texts: the dual processes of generating inferences and 
repairing mental models. advances in instructional psychology. r. glaser. mahwah  nj  lawrence erlbaum associates: 1  1. 
 conati and vanlehn 1   conati  c. and k. vanlehn.  providing adaptive support to the understanding of instructional material . proc. of iui 1  internationa conference on intelligent user interfaces  santa fe  nm  january 1. 
 conati and vanlehn 1  conati  c. and k. vanlehn. 
 toward computer-based support of meta-cognitive 
skills: a computational framework to coach selfexplanation.  int. journal of ai in education 1  1. 
 freedman 1  freedman  r. plan-based dialogue management in a physics tutor. sixth applied natural language processing conference   seattle  wa  1. 
 horacek 1  horacek  h.  a model for adapting explanations to the user's likely inferences.  umuai 1 : 1  1. 
 korb  mcconachy et al. 1  korb  k. b.  r. mcconachy  et al. a cognitive model of argumentation. proc. 1th cognitive science conf  stanford  ca  august 1. 
 moore 1  moore  j.  discourse generation for instructional applications: making computer-based tutors more like humans.  journal of ai in education 1   1. 
 moser  moore et al. 1  moser  m. g.  j. d. moore and e. glendeling. instructions for coding explanations: 
identifying segments  relations and minimal units  tr 1 univ. of pittsburgh  dept. of computer science  1. 
 reiter and dale 1  reiter  e. and r. dale. building nlg systems  cambridge university press  1. 
 sweller 1  sweller  j.  cognitive load during problem 
solving: effects on learning.  cognitive science 1: 1  1. 
 young 1  young  m.  using grice's maxim of quantity to select the content of plan descriptions.  artificial intelligence journal 1 : 1  1. 
 young and moore 1  young  m. and j. moore. dpocl: a principled approach to discourse planning. proc. 1th int. workshop on nlg  1  1. 
an empirical study of the influence of user tailoring on evaluative
argument effectivenessgiuseppe carenini
department of computer science
university of british columbia
vancouver  b.c. canada v1t 1 carenini cs.ubc.ca
johanna d. moore
the human communication research centre  university of edinburgh 
1 buccleuch place  edinburgh eh1lw  uk. jmoore cogsci.ed.ac.uk

abstract  
the ability to generate effective evaluative arguments is critical for systems intended to advise and persuade their users. we have developed a system that generates evaluative arguments that are tailored to the user  properly arranged and concise. we have also devised an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users. this paper presents the results of a formal experiment we performed in our framework to verify the influence of user tailoring on argument effectiveness.
1     introduction
evaluative arguments are pervasive in natural human communication. in countless situations  people attempt to advise or persuade their interlocutors that something is good  vs. bad  or right  vs. wrong . for instance  doctors need to advise their patients on which treatment is best for them  the patients . a teacher may need to convince a student that a certain course is  is not  the best choice for the student. and a sales person may need to compare two similar products and argue why her current customer should like one more than the other. with the explosion of the information available on-line and the ever-increasing availability of wireless devices  we are witnessing a proliferation of computer systems that aim to support or replace humans in similar communicative settings. clearly  the success of these systems serving as personal assistants  advisors  or shopping assistants  e.g.   chai  budzikovaska et al. 1   may crucially depend on their ability to generate and present effective evaluative arguments.
 in the last decade  considerable research has been devoted to develop computational models for automatically generating and presenting evaluative arguments. several studies investigated the process of selecting and structuring the content of the argument  e.g.   morik 1; elzer  chucarroll et al. 1; klein 1    while  elhadad  mckeown et al. 1  developed a detailed model of how the selected content should be realised into natural language. all these approaches to evaluative argument generation follow a basic guideline from argumentation theory  mayberry and golden 1 : effective evaluative arguments should be constructed considering the values and preferences of the audience towards the information presented. in practice  this means that all previous approaches tailor the generated arguments to a model of the user's values and preferences.
 however  a key limitation of previous work is that none of the proposed approaches has been empirically evaluated. thus  in particular  it is not clear whether and to what extent tailoring an evaluative argument to a model of the user increases its effectiveness. the work presented in this paper is a first step toward addressing this limitation.  by recognising the fundamental role of empirical testing in assessing progress  generating new research questions and stimulating the acceptance of a technique as viable technology  we have performed an experiment to test the influence of user-tailoring on argument effectiveness. in the remainder of the paper  we first provide a short description of our system for generating evaluative arguments tailored to a model of the user's preferences. then  we briefly present a framework to measure the effectiveness of evaluative arguments with real users. next  we discuss the experiment we ran within the framework to test the influence of user tailoring on evaluative argument effectiveness.
1    user tailored evaluative arguments

figure 1 top: amvf for two sample users; for clarity's sake only a few component value functions are shown. bottom: arguments about house-1 tailored to the two different modelsour generation system  known as the generator of evaluative argument  gea   carenini 1   generates evaluative arguments whose content  organisation and phrasing are tailored to a quantitative model of the user's values and preferences. the model is expressed as an additive multiattribute value function  amvf   a conceptualization based on multiattribute utility theory  maut   clemen 1 . besides being widely used in decision theory  where they were originally developed   conceptualizations based on maut have recently become a common choice in the user modeling field  jameson  schafer et al. 1 . furthermore  similar models are also used in psychology  in the study of consumer behaviour  solomon 1 . in gea  a user specific amvf is a key knowledge source in all the phases of the generation process.  gea is implemented as a standard  pipelined generation system  including a text planner  a microplanner  and a sentence realizer.
1	amvfs and their use in gea
an amvf is a model of a person's values and preferences with respect to entities in a certain class. it comprises a value tree and a set of component value functions. a value tree is a decomposition of an entity value into a hierarchy of entity aspects  called objectives in decision theory   in which the leaves correspond to the entity primitive objectives  see top of figure 1 for two simple value trees in the real estate domain . the arcs in the tree are weighted to represent the importance of an objective with respect to its siblings  e.g.  in figure 1  location for usera is more than twice as important as quality in determining the housevalue . the sum of the weights at each level is always equal to 1. a component value function for a primitive objective expresses the preferability of each value for that objective as a number in the  1  interval  with the most preferable value mapped to 1  and the least preferable one to 1. for instance  in figure 1 the victorian value of the primitive objective architectural-style is the most preferred by userb  and a distance-from-park of 1 mile has for userb preferability  1 -  1.1 * 1  =1. formally  an amvf predicts the value v e  of an entity e as follows:
v e  = v x1 ... xn  = Σwi vi xi   where
-  x1 ... xn  is the vector of primitive objective values for an entity e
-   primitive objective i  vi is the component value function and  wi is its weight  with 1＋ wi ＋1 and Σwi =1; wi is equal to the product of all the weights on the path from the root of the value tree to the primitive objective i.
 thus  given someone's amvf  it is possible to compute how valuable an entity is to that individual. although for lack of space we cannot provide details here  given a user specific amvf and an entity  gea can also compute additional precise measures that are critical in generating a user-tailored evaluative argument for that entity. first  gea can compute how valuable any objective of the entity is for that user. this information plays an essential role in phrasing the argument by determining the selection of scalar adjectives  e.g.  convenient   which are the basic linguistic resources to express evaluations. second  gea can identify what objectives can be used as supporting or opposing evidence for an evaluative claim. third  gea can compute for each objective the strength of supporting  or opposing  evidence it can provide in determining the evaluation of its parent objective. in this way  in compliance with argumentation theory  evidence can be arranged according to its strength and concise arguments can be generated by only including sufficiently strong evidence  carenini and moore 1 .  the measure of evidence strength and the threshold that defines when a piece of evidence is worth mentioning were adapted from  klein 1 .
 a final note on amvf's applicability. according to decision theory  in the general case  when uncertainty is present  user's preferences for an entity can be represented as an amvf only if her preferences for the primitive objectives satisfy a stringent condition  i.e.  additive independence . however  evidence has shown that an amvf is a reasonable model of most people's preferences under conditions of certainty  clemen 1 . we felt that we could safely use amvfs in our study  because we selected the objectives to avoid possible violations of additive independence. and we considered a situation with no uncertainty.
1   an example: generating arguments for two different users
figure 1 illustrates how the content  organization and phrasing of the arguments generated by gea are sensitive to the model of a user's preferences. the top of the figure shows two different models of actual users in the real-estate domain. the bottom of the figure shows two evaluative arguments generated for the same house but tailored to the two different models. the primitive objectives' values for the house are reported in the middle of the figure. notice how the two arguments differ substantially. different objectives are included  the objectives included are underlined in the two models . furthermore  the objectives are ordered differently  e.g.  in the first argument location comes before quality  whereas the opposite is true in the second argument . finally  the evaluations are also different. for instance  quality is good for usera  but excellent for userb.
1	the evaluation framework
to run our formal experiment  we used an evaluation framework based on the task efficacy evaluation method  carenini 1 . this method allows the experimenter to evaluate a generation model indirectly  by measuring the effects of its output on user's behaviors  beliefs and attitudes in the context of a task. aiming at general results  we chose a basic and frequent task that has been extensively studied in decision analysis: the selection of a subset of preferred objects  e.g.  houses  out of a set of possible alternatives. in our evaluation framework  the user performs this task by using a system for interactive data exploration and analysis  idea   see figure 1. let's now examine how gea can be evaluated in the context of the selection task  by going through the evaluation framework architecture.
1  the evaluation framework architecture
as shown in figure 1  the evaluation framework consists of four main sub-systems: the idea system  the user model refiner  the new instance generator and gea. the framework assumes that a model of the user's preferences  an amvf  has been previously acquired from the user  to assure a reliable initial model. at the onset  the user is assigned the task to select from the dataset the four most preferred alternatives and to place them in a hot list  see figure 1  upper right corner  ordered by preference. whenever the user feels that the task is accomplished  the ordered list of preferred alternatives is saved as her preliminary hot list  figure 1  1  . after that  this list and the initial model of user's preferences are analysed by the user model refiner to produce a refined model of the
user's preferences  figure 1  1  . then a new instance  newi  is designed on the fly by the new instance generator to be preferable for the user given her refined preference model  figure 1  1  .  at this point  the stage is set for argument generation. given the refined model of the user's preferences  the argument generator produces an evaluative argument about newi tailored to the model  figure 1  1    which is presented to the user by the idea system  figure 1  1   see also figure 1 for an example . the argument goal is to persuade the user that newi is worth being considered. notice that all the information about newi is also presented graphically.
  once the argument is presented  the user may  a  decide immediately to introduce newi in her hot list  or  b  decide to further explore the dataset  possibly making changes and adding newi to the hot list  or  c  do nothing.  
figure 1 shows the display at the end of the interaction  when the user  after reading the argument  has decided to introduce newi in the hot list first position  figure 1  top right .
 whenever the user decides to stop exploring and is satisfied with her final selection  measures related to argument's effectiveness can be assessed  figure 1   1  . these measures are obtained either from the record of the user interaction with the system or from user self-reports in a final questionnaire  see figure 1 for an example of selfreport  and include:
- measures of behavioral intentions and attitude change:  a  whether or not the user adopts newi   b  in which position in the hot list she places it and  c  how much she likes newi and the other objects in the hot list.

figure 1 the evaluation framework architecture

figure 1 the idea environment display at the end of the interaction- a measure of the user's confidence that she has selectedthe best for her in the set of alternatives.
- a measure of argument effectiveness derived by explicitlyquestioning the user at the end of the interaction about the rationale for her decision  olso and zanna 1 . this can provide valuable information on what aspects of the argument were more influential on the user's decision.
- an additional measure of argument effectiveness isderived by explicitly asking the user at the end of the interaction to judge the argument with respect to several dimensions of quality  such as content  organization  writing style and convincingness. however  evaluations based on judgements along these dimensions are clearly weaker than evaluations measuring actual behavioural and attitudinal changes  olso and zanna 1 .
 to summarize  our evaluation framework supports users in performing a realistic task by interacting with an idea system. in the context of this task  an evaluative argument is generated and measurements are collected on its effectiveness. we now discuss an experiment we have performed within the evaluation framework to test to what extent tailoring an evaluative argument to a model of the user preferences increases its effectiveness.
1	the experiment
given the goal of our empirical investigation  we have performed a between-subjects experiment with three experimental conditions:  i  no-argument - subjects are simply informed that newi came on the market.  ii  tailored - subjects are presented with an evaluation of newi tailored to their preferences.  iii  non-tailored - subjects are presented with an evaluation of newi that  instead of being tailored to their preferences  is tailored to the preferences of a default average user  for whom all aspects of a house are equally important  i.e.  all weights in the amvf are the same . a similar default preference model is used for comparative purposes in  srivastava  connolly et al. 1 . in the three conditions  all the information about the newi is also presented graphically  so that no information is hidden from the subject.
   our hypotheses on the experiment are the following. first  we expect arguments generated for the tailored condition to be more effective than arguments generated for the nontailored condition. second  the tailored condition should be somewhat better than the no-argument condition  but to a lesser extent  because subjects  in the absence of any argument  may spend more time further exploring the dataset  thus reaching a more informed and balanced decision. finally  we do not have strong hypotheses on comparisons of argument effectiveness between the noargument and non-tailored conditions. the experiment is organized in two phases. in the first phase  the subject fills out a questionnaire on the web which implements a method from decision theory to acquire an amvf model of the subject's preferences  edwards and barron 1 . in the second phase  to control for possible confounding variables  including subject's argumentativeness  infante and rancer 1   need for cognition  cacioppo  petty et al. 1   intelligence and self-esteem  the subject is randomly assigned to one of the three conditions. then  the subject interacts with the evaluation framework and at the end of the interaction measures of the argument effectiveness are collected  as described in section 1.
 after running the experiment with 1 pilot subjects to refine and improve the experimental procedure  we ran a formal experiment involving 1 subjects  1 in each experimental condition. each subject had only one interactive session with the framework.
1 	experiment results
1 	a precise measure of satisfaction
according to literature on persuasion  the most important measures of argument effectiveness are the ones of behavioral intentions and attitude change  olso and zanna 1 . as explained in section 1  in our framework these measures include  a  whether or not the user adopts newi   b  in which position in the hot list she places it   c  how much she likes the proposed newi and the other objects in the hot list. measures  a  and  b  are obtained from the record of the user interaction with the system  whereas measures in  c  are obtained from user self-reports.
a     how would you judge the houses in your hot list  the more you like the house the closer you should  put a cross to  good choice 
 1st house
bad choice  :   :  :  :   :  :  :  :x :  : good choice
1nd house new house 
bad choice  :   :  :  :   :  :  :x :  :  : good choice
1rd house
bad choice  :   :  :  :   :  :  :x :  :  : good choice
1th house bad choice  :   :  :  :   :x :  :  :  :  : good choice
figure 1 sample filled-out self-report on user's satisfaction with houses in the hot list1
a closer analysis of the above measures indicates that the measures in  c  are simply a more precise version of measures  a  and  b . in fact  not only do they assess  like  a  and  b   a preference ranking among the new alternative and the other objects in the hot list  but they also offer two additional critical advantages:
 i  self-reports allow a subject to express differences insatisfaction more precisely than by ranking. for instance  in the self-report shown in figure 1  the subject was able to specify that the first house in the hot list was only one space  unit of satisfaction  better then the house following it in the ranking  while the third house was two spaces better than the house following it.
 ii  self-reports do not force subjects to express a total orderbetween the houses. for instance  in figure 1 the subject was allowed to express that the second and the third house in the hot list were equally good for her.
 furthermore  measures of satisfaction obtained through self-reports can be combined in a single  statistically sound measure that concisely expresses how much the subject liked the new house with respect to the other houses in the hot list. this measure is the z-score of the subject's selfreported satisfaction with the new house  with respect to the self-reported satisfaction with the houses in the hot list. a z-score is a normalized distance in standard deviation units of a measure xi from the mean of a population x. formally: xi（ x; z-score  xi  x  =  xi - μ  x   / σ x 
   for instance  the satisfaction z-score for the new instance  given the sample self-reports shown in figure 1  would be:  1 - μ  {1 1}   /  σ {1 1}  = 1
the satisfaction z-score precisely and concisely integrates all the measures of behavioral intentions and attitude change. we have used satisfaction z-scores as our primary measure of argument effectiveness.

figure 1 results for satisfaction z-scores. the average zscores for the three conditions are shown in the grey boxes
1 	results
as shown in figure 1  the satisfaction z-scores obtained in the experiment confirmed our hypotheses. arguments generated for the tailored condition were significantly more effective than arguments generated for the non-tailored condition  p=1 . the tailored condition was also significantly better than the no-argument condition  p=1 . and this happened despite the fact that subjects in the no-argument condition spent significantly more time further exploring the dataset after newi was presented  as indicated in table 1  p=1 . finally  we found no significant difference in argument effectiveness between the no-argument and tailored-verbose conditions.
   with respect to the other measures of argument effectiveness mentioned in section 1  we have not found any significant differences among the experimental conditions1.
no-argumentnon-tailoredtailored1:1:11:1table 1 average time spent by subjects in the three conditions further exploring the dataset after the new house is presented  from logs of the interaction .
1	conclusions and future work
argumentation theory indicates that effective arguments should be tailored to a model of the user's preferences. although previous work on automatically generating evaluative arguments has followed this basic indication  the effect of tailoring on argument effectiveness has never been measured empirically. as an initial attempt to address this issue  we have compared in a formal experiment the effectiveness of arguments that were tailored vs. nontailored to a model of the user preferences. the experiment results show that tailored arguments are significantly better.
 as future work  we plan to perform further experiments. we intend to repeat the same experiment in a different domain to test for external validity. we also envision an experiment in conditions of uncertainty  in which we may compare arguments tailored to an amvf  with argument tailored to more sophisticated models of user's preferences  that consider interactions among objectives.
references
 cacioppo  petty et al. 1  cacioppo  j. t.  r. e. petty  et al. effects of need for cognition on message evaluation  recall  and persuasion. journal of personality and social psychology 1 : 1  1.
 carenini 1  carenini  g. generating and evaluating evaluative arguments. ph.d. thesis  intelligent system program  university of pittsburgh  1.
 carenini 1  carenini  g. a task-based framework to evaluate evaluative arguments. first international conference on natural language generation  mitzpe ramon  israel: 1  1.
 carenini and moore 1  carenini  g. and j. moore. a strategy for generating evaluative arguments. first international conference on natural language
generation  mitzpe ramon  israel: 1  1.
 chai  budzikovaska et al. 1  chai  j.  m. budzikovaska  et al. natural language sales assistant. 1th annual meeting of the association for computational linguistics: 1  1.
 clemen 1  clemen  r. t. making hard decisions: an introduction to decision analysis. belmont  california  duxbury press  1.
 edwards and barron 1  edwards  w. and f. h. barron. smarts and smarter: improved simple methods for multiattribute   utility measurements. organizational behavior and human decision processes 1: 1  1.
 elhadad  mckeown et al. 1  elhadad  m.  k. mckeown  et al. floating constraints in lexical choice. computational linguistics 1 : 1  1.
 elzer  chu-carroll et al. 1  elzer  s.  j. chu-carroll  et al. recognizing and utilizing user preferences in collaborative consultation   dialogues. proc. of fourth int. conf. of user modeling  hyannis  ma: 1  1.
 infante and rancer 1  infante  d. a. and a. s. rancer. a conceptualization and measure of argumentativeness. journal of personality assessment 1: 1  1.
 jameson  schafer et al. 1  jameson  a.  r. schafer  et al. adaptive provision of evaluation-oriented information: tasks and techniques. proceedings of 1th ijcai  montreal  1.
 klein 1  klein  d. decision analytic intelligent systems: automated explanation and knowledge
acquisition  lawrence erlbaum associates  1.
 mayberry and golden 1  mayberry  k. j. and r. e. golden. for argument's sake: a guide to writing effective arguments  harper collins  college publisher  1.
 morik 1  morik  k. user models and conversational settings: modeling the user's wants. user models in dialog systems. a. kobsa and w. wahlster  springerverlag: 1  1.
 olso and zanna 1  olso  j. m. and m. p. zanna. attitudes and beliefs: attitude change and attitudebehavior consistency. social psychology. r. m. baron and w. g. graziano  1.
 solomon 1  solomon  m. r. consumer behavior: buying  having  and being  prentice hall  1.
 srivastava  connolly et al. 1  srivastava  j.  t. connolly  et al. do ranks suffice  a comparison of alternative weighting approaches in value elicitation. organizational behavior and human decision process 1 : 1  1.

natural language processing
and information retrieval
statistical processing of natural language grammars

refining the structure of a stochastic context-free grammar

joseph bockhorst
joebock cs.wisc.edu
department of computer sciences
university of wisconsin
madison  wisconsin 1
mark craven
craven biostat.wisc.edu
department of biostatistics & medical informatics
university of wisconsin
madison  wisconsin 1

abstract
we present a machine learning algorithm for refining the structure of a stochastic context-free grammar  scfg . this algorithm consists of a heuristic for identifying structural errors and an operator for fixing them. the heuristic identifies nonterminals in the model scfg that appear to be performing the function of two or more nonterminals in the target scfg  and the operator attempts to rectify this problem by introducing a new nonterminal. structural refinement is important because most common scfg learning methods set the probability parameters while leaving the structure of the grammar fixed. thus  any structural errors introduced prior to training will persist. we present experimentsthat show our approach is able to significantly improve the accuracy of an scfg designed to model an important class of rna sequences called terminators.
1	introduction
stochastic context-free grammars  scfgs  have long been used in the nlp community  where the are more commonly known as probabilistic context-free grammars  and recently have been applied to biological sequence analysis tasks  in particular rna analysis. one typical learning problem is to create an scfg froma set of unparsedtraining sequencesthat will accurately recognize previously unseen sequences in the class being modeled. the first step of this two step process is to create the underlying context-free grammar  which we refer to as the structure of the model. the second step  the assignment of the probability parameters  is usually performed through an application of an expectation maximization  em  algorithm called the inside-outside algorithm  lari & young  1 . one of the limitations of this approach is that if the structure is incomplete or in error  there may be no assignment of probabilities that would result in an accurate model. this is the problem we address in this paper. we introduce a refinement step that can identify and fix a class of structural errors that occur when a nonterminal in the model grammar is performing the function of more than one nonterminal in the target. the particular location refined at each step is determined  diagnostically  by analyzing the interaction between the model and a training set. this method may be contrasted
y
 a  b 
	a/ 	a/ 
	s	 f	s  f
	b/ 	x	b/ 
figure 1: an example where an incorrect structure prevents the learning of an accurate model.  a  a target grammar to be learned.  b  the structure used to model sequences generated by the grammar shown in  a . state s is the begin state and f the end state. arcs are labeled with the character emitted and the probability of taking that arc. the question marks in  b  represent the probability parameters to be learned. because state x is overloaded in the learned model  it will be unable to correctly model the probabilities of the second character in the sequence.
to the standard state-space search methodology of applying the best of several competing operations as determined by a heuristic function applied to the successor states. this refinement step can be iterated with the inside-outside algorithm to refine both the structure of the model and the probability parameters. through experiments with an scfg designed to model a family of rna sequences called terminators we show how this method may be used to learn more accurate models than inside-outside alone.
　to see the impact a structural error can have  consider trying to learn the simple stochastic regular grammar shown in figure 1 a . imagine that we have many sequences generated by this grammar for training and the structure we choose is shown in figure 1 b . given enough training data  the maximum likelihoodestimate of each state transition probability is 1. the probability of each length-two sequence under this model is therefore 1 resulting in a rather inaccurate model. the problem with this structure is that state in the model is overloaded; it is performing the function of both state and in the target. with the approach presented herein  errors
such as this can potentially be corrected.
1	problem domain
the application of scfgs we consider here is that of modeling of a class of rna sequences called terminators. rna sequences are strings from the alphabet a c g u corresponding to the four different types of bases in the nucleotides of rna molecules. the bases a and u are complementary to one another  as are c and g  because they are able to easily form base pairs by becoming chemically bonded1.
　the three dimensional shape that an rna sequence assumes to a large degree determines its function  and the shape itself is strongly influenced by which particular bases are paired. for example  an element of rna structure called a stem is formed when a number of adjacent bases pair with a complementary set of bases later in the sequence. if the intervening bases are unpaired  the resulting structure is called a stem-loop. figure 1 a  shows an rna sequence that could assume a stem-loop  and 1 b  shows the stem-loop it forms. the consequence of this is that families of rna sequences of similar function will share a pattern of dependency between bases in the sequence that are likely to be paired.
　terminators are rna sequences which signal when to stop the process of transcription  one of the key steps in the expression of a gene to form a protein.
1	stochastic context free grammars
the structure of an scfg is a context-free grammar and is defined by a set of nonterminals   a set of terminal symbols   a start nonterminal and a set of rewrite rules  or productions  of the form   . that is  the right hand side of a production consists of any combination of terminal and nonterminal symbols other the start nonterminal. when we refer to 's productions  we mean the productions with on the left hand side. nonterminals and terminals are denoted by upper and lower case letters respectively. a grammar can be made into an scfg by associating with each nonterminal a probability distribution    over 's productions.
　scfgs have proven important in rna analysis tasks because the defining characteristics of rna families manifest themselves in sequences throughlong range dependenciesbetween bases. such dependencies are troublesome to represent with regular grammars but can be naturally represented by context-free grammars. figure 1 c  shows an scfg for a specific set of stem-loops and figure 1 d  shows the parse tree for an example sequence under this grammar. there are three fundamental computational tasks relating scfgs and sequences.
1. compute   the probability of a sequence given a full model.
1. find the most probable parse tree for a sequence.
1. given and a set of sequences   set the probabilities to maximize the likelihood
.
　briefly  the first and second problems are efficiently solvable with dynamic programming algorithms similar to the forward and viterbi algorithms  but there is no known efficient global solutionfor the third problem. the most common
 a 	ucacacucguga	 b 	c	u ac
 d  c 	sx	 1 
	xa x u	 1 
	xc x g	 1 
	xg x c	 1 
	xu x a	 1 
	xl	 1 
	u  c  a  c  a  c  u  c  g  u  g  a	la c u c	 1 
figure 1:  a  a simple rna sequence that will form a stem-loop structure.  b  the stem loop structure; paired bases are connected by dashes.  c  an scfg model of sequences that form stem-loops. probabilities are listed to the right of their associated production.  d  the parse tree for the sequence in  a  using the grammar in  c .
heuristic is the inside-outside  lari & young  1  method which  like other em algorithms  converges to a local maximum.
　the inside algorithm  used to solve task 1 above  is a dynamic programming algorithm that fills a three dimensional matrix . if run on the sequence   the elements of the matrix    contain the sum of the probabilities of all parse trees of the subsequence rooted at . so  if the length of is and   then .
　the partner of the inside algorithm  the outside algorithm calculates a similar dynamic programming matrix . an element is the sum of the probabilities of all parse trees of the complete sequence rooted at the start nonterminal  excluding all parse subtrees of rooted at .
　from and the expected number of times each nonterminal is used in the derivation of a sequence can be determined. the sum of these counts over all sequences in a training set are used by the inside-outside algorithm to iteratively re-estimate the production probabilities. in the next section we will show how and play a role in identifying overloaded nonterminals.
1	grammar refinement approach
when the structure of a hypothesis grammar is different from the structure of the target grammar   its accuracy may suffer. we would like to be able to identify and fix grammars in this situation using only a set of training sequences. in this section  we present a grammar refinement operator and a heuristic to do this. the operator is able to fix certain structural errors in that occur when a nonterminal in the hypothesis is performing the function of more than one nonterminal of . we refer to such a nonterminal in as overloaded. the heuristic identifies overloaded nonterminals by locating data dependencies that are not represented by the structure of .
　before we delve into the details of the grammar refinement algorithm  it is helpful to consider the ways in which a hypothesis grammar may differ from the target grammar. figure 1 gives a taxonomy of possible errors in a hypothesis grammar. the right branch out of the root contains all
incorrect scfg
incorrect
probabilities
	overloaded	other missing
	nonterminal	nonterminal
error
figure 1: possible inaccuracies of a hypothesis scfg with respect to a target grammar
table 1: the expand operator. the operator creates a new nonterminal to take the place of nonterminal on the right hand side of production
expand 	 	  /*	is on the rhs of production	*/
1. create new nonterminal
1. replace	with	in
1. for each production	  create
1. initialize probabilities for new productions
scfgs that have a correct structure; that is  there is some assignment of probabilities that results in a grammar equivalent to . most learning algorithms for scfgs  including inside-outside are aimed at this situation.
　the left branch contains two categories of incorrectly structured grammars: those whose structures can be made correct by adding only productions and those that require additional nonterminals as well. among the kinds of errors that can occur from missing nonterminals  the class we address occurs when a nonterminal in the hypothesis is overloaded because it is trying to perform the function of two or more nonterminals in the target. for example  the nonterminal in the scfg of figure 1 c  would be overloaded if the distribution of the first base pair in the family of sequences being modeled was different from the rest. next  we present a grammar refinement algorithm to identify and fix overloaded nonterminals.
　the grammar refinement problem starts with set a sequences which we can think of as generated by an unknown target scfg   an initial hypothesis grammar  and a goal of discovering a model 'close' to . we navigate the space of grammar structures through the application of a grammar modification operator  applied at each step to a part of the current grammar chosen by a heuristic.
1	refinement operator
our current grammar refinement procedure has a single operator  expand  shown in table 1  which is applied to what we call a context. we define a context to be a production and a nonterminal on the right hand side of that production. we denote a context either by the pair where is a nonterminal on the right hand side of production or by where r indexes into the productions in which nontermi-
s

u  ...                ... a
figure 1: a partial parse tree where the nonterminal is used in the two different contexts       and   a u   .
table 1: the structure of the scfgformed following an application of expand      to the grammar in figure 1 c . the new nonterminal x' replaces x in the production of the context being expanded  . productions created by expand are shown in boldface.
s x' x a x u x' a x u x c x g x' c x g x g x c x' g x c
x	a x u	x'	u x a
x	l	x'	l
l	a c u c
nal appears on the right hand side. every nonterminal node in a parse tree defines a context. for example  the nonterminal in the parse tree fragment of figure 1 is in the context       the first time it appears and   a u    the second. when applied to   expand creates a new nonterminal   replaces with in and creates a production with on the left hand side from each of 's productions. table 1 shows the structure that is formed after applying expand    to the scfg of figure 1 c . we explainhow to set the probabilitiesof the new productions below.
1	refinement heuristics
the heuristic we use to guide the search expandsa single context at each step. it chooses to expand the nonterminal that is used most differently in that context compared to its expected usage based on its probability distribution. let be the vector of expected usage counts of the productions of in context  i.e.  on the right hand side of the production indicated by   for the training sequences and let be an element of this vector that refers to the expected number of times that the production with on the left hand side is used in context . expand    sets the probability distribution of the nonterminal it creates to the one defined by . given a measurement of difference between the data distribution and 's probability distribution we define our heuristic to choose to expand the context that maximizes this difference. we have investigated two difference measures  one based on the kullback-leibler  kl  divergence and the other based on the statistic.
　the kl divergence  also called relative entropy  between two probability distributions and with the same discrete
table 1: example of values used in computing the heuristics for the context       from the scfg in figure 1 c . the columns indicate:  1  the productions of    1  the observed number of times it is used in the context   1  the observed probability distribution in the context   1  the expected usage counts if the context's distribution were the same as the distribution over all contexts   1  the distribution over all contexts.
context   	 allproduction observed expectedcountsprcountspr	a	u1111	c	g1111	g	c1111	u	a11111111sum1111domain is defined as

consider the example in table 1. column  1  in this table shows for the productions of in the context      . we can use kl divergence to compare this distribution to   shown in column  1   which is the probability distribution for the productions of over all of 's contexts. using this measure  we select the nonterminal in context that maximizes
where is the probability distribution defined by   and the sum calculates the expected total number of times is in . by one interpretation  this measure selects the context that would waste the most bits in transmitting a message for each production applied in that context if the encoding used is the optimal encoding defined by instead of .
　the second heuristic we consider  the statistic  is commonly used to determine if the hypothesis that a data sample was drawn according to some distribution can be rejected. using the vector of expected usage counts of nonterminal in context     as the  observed  counts and for the expected counts 
our method selects for expansion the context which maximizes . returning to the example in table 1  we can use to assess the significance of differences between column  1   the observed counts for the productions applied in the given context  and column  1  the expected observations given the probabilities in column  1 .
　to apply either of these heuristics  we must first calculate the counts . these can be calculated from the inside and outside matrices and in a similar way as the usage counts of productions and nonterminals are calculated in the
table 1: the structure of our terminator grammar. nonterminals are capitalized and the terminals are a  c  g and u. the notation
x	y	z is shorthand for the two productions x	y and x
   z. productions containing and are shorthand for the family of 1 productions where and can be any of the four terminals. productions containing and have a similar interpretation except that and can also be null allowing for unpaired bases in the interior of the stem.
startprefix	stem bot1	suffixprefixb b b b b b b bstem bot1stem bot1stem bot1stem midstem top1stem midstem midstem top1stem top1stem top1stem top1looploopb b loopmidloop midb loop mid	b bsuffixb b b b b b b bba	c	g	uinside-outside algorithm. for example  the expected number of times that the production c g is used in the context in the derivation of the sequence
is
where
and is an indicator variable that is 1 if and   and 1 otherwise. also  the counts
are smoothed by adding 1 to each	.
1	experiments
we have constructed an scfg model of terminatorsequences by incorporating known features of terminators identified in the terminator literature. the structure of the terminator grammar we start with  shown in table 1  models the eight bases before and eight bases after the stem-loop and the stem-loop itself. the recursive nature of stemmid and loopmid's productions enables variable length stems and loops to be represented. an unpaired base in the interior of the stem  a bulge  can also be represented.
　positive examples consist of the sequences of 1 known or proposed terminators of e. coli. each of these sequences is 1 base pairs long with the base aligned to the assumed termination point. we have 1 negative sequences  also of length 1  which were collected from the regions following genes that are presumed to not contain terminators.
　we performed a five fold cross validation experiment using the kl and heuristic as well as a control where the context to expand at each step is randomly chosen. the following steps were repeated.
1. extract   the most probable subsequence of each positive sequence in the training set given the current model.

	1%	1% 1% 1% 1% 1%
false positive rate
figure 1: roc curves of original grammar and the refined grammars after 1 additional nonterminals have been added using the   kl divergence and random heuristics. note that the order of the models in the key reflects the order of the curves.
1. fit the probability parameters by running inside-outsideto convergence on .
1. expand the context chosen by the heuristic.
the first step is needed because the positive sequences contain more bases than what is being modeled by the grammar. the probabilities in the stem were initialized with a preference for complementarybase pairs; elsewhere  a uniform distribution was used.
　for each test sequence  we determine the probability of its most probable subsequence as given by each model. to compare various methods  we rank our test-set predictions by these probability values and then construct roc curves. we get a single roc curve for each approach by pooling predictions across test-sets.
　figure 1 shows the roc curves that result from the original grammar and from runningour grammarrefinementalgorithm for 1 iterations using the   kl  and random heuristics. the random curve is the average over 1 random runs. the curves in this figure show that our heuristics provide better predictive accuracy than both the original grammar and our control of randomly selecting the context to expand at each step. these results support our hypothesis that directed changes to the structure of the grammar can result in more accurate learned models.
　to consider how the predictive accuracy of the grammars changes as a function of the number of refinement iterations  we construct roc curves after each iteration and then calculate the area under each curve. figure 1 plots the area under these curves versus the number of nonterminals added for the three heuristics considered in figure 1. the -axis for this figure starts at 1 which is the expected area under the curve for a model that guessed randomly  such a model would result in an roc  line  defined by: tp rate = fp rate . the results in this figure show that steady improvement is seen through about the 1th additional nonterminal using the kl heuristic and through about the 1th using . our approach does not appear to overfit the training data  at least through

figure 1: plot of the area under the roc curves for three heuristics:
　  kl divergence  and random. the error bars on the random plot denote the 1% confidence interval. note that the -axis starts at 1 which corresponds to the expected area under the curve for random predictions.
the 1 iterations for which we have run it.
　one possible reason that the heuristic results in more accurate models than the kl heuristic is the following. we know that our calculations of kl divergence are biased because they are calculated with finite samples  herzel & grosse  1   but that this bias is not uniformacross the nonterminals and contexts we are comparing because the sample sizes differ. therefore  one avenue for future research is to investigate measures that correct for the finite-sample bias of our kl divergence calculation.
　we have examined the sequence of contexts expanded for both the and the kl divergence heuristics  and noticed that several of the early refinements adjust a part of the grammar that describes the first few positions following the stem. there is known preference for t bases in this part of terminators  but our initial grammar did not encode it. this result suggests that our method can compensate for a known inadequacy in the initial grammar. however  we note that the algorithm makes additional modifications to other parts of the initial grammar that represent our best effort at encoding relevant domain knowledge.
　one of the limitations of this algorithm as presented is the one-way nature of the search. that is  there is no way to make the grammar more general. if the initial grammar encodes the most general knowledge of the domain expert  this limitation may not be problematic initially. however  as expand creates more nonterminals  it is likely that the grammar would become overly specific in places. the most obvious way of addressing this problemis by introducinga generalizationoperator. for example  a merge operator could be used to combinetwo nonterminalswith the same rhs domain if their probability distributions are sufficiently similar. note however  as mentioned by stolcke  1   that this operator will not introduce any new embedding structure into the grammar. this may not be a problem if such structure is present in the initial grammar.
1	related work
the approach we present here is related to the grammar induction algorithms of stolcke  1  and chen  1 . these works both address the induction of scfgs from text corpora for use as language models. both methods incorporate a prior probability distribution over model structures and perform a search through posterior model probability space where stolcke proceeds specific to general and chen general to specific. one of the key differences between these algorithms and ours is that each of these addresses tabula rasa grammar induction while ours beginswith a grammarstructurecreated from prior knowledge. another key difference is the way in which steps in the search space are selected. the methods of both stolcke and chen evaluate a candidate operator application by doing one-step lookahead. since it can be expensive to calculate the posterior probability of the data given the changed model  heuristics are used to estimate this value. the operator application that results in the greatest estimated posterior probability is accepted. our approach  on the other hand  is more  diagnostic.  instead of doing one-step lookahead  our heuristics try to directly identify places in which the grammar structure is inadequate. this approach may somewhat insulate our method from the overfitting pitfalls of likelihood driven searches. a third difference is that stolcke and chen use a richer set of operators than we do. as suggested in the previous section  however  we believe that our diagnostic approach can be generalized to work with other operators.
　a different view of our approach is as an instance of a theory refinement algorithm  pazzani & kibler  1; ourston & mooney  1; towell & shavlik  1 . in theory refinement  the goal is to improve the accuracy of an incomplete or incorrect domain theory  from a set of labeled training examples. the primary difference between our work and previous work in theory refinement is the representation used by our learned models. whereas previous theory-refinement methods have focused on logic-based and neural network representations  our learned models are represented using stochastic context free grammars.
　the task we address is also similar to the problem of learning the structure of bayesian networks  heckerman  geiger  & chickering  1; chickering  1   where the statistical properties of a training set are used to guide the modifications of the network structure. again  the principal difference between our approach and this body of work is the difference in the representation language.
　there has also been related work in learning scfgs for rna modeling tasks  eddy & durbin  1; sakakibara et al.  1 . these iterative methods both update grammar structure using information gathered from the most probable parse of each of the training sequences at each step. although we intend to compare our refinement algorithm to both of these approaches in future work  we consider our work to be complementary to these methods because it could be run on every one of their iterations.
1	conclusion
we have considered the problem of refining the structure of a deficient scfg using a set of training sequences. we introduced a grammar refinement operator  expand  for repairing a type of scfg structural error resulting from an overloaded nonterminal as well as a pair of heuristics  based on kl divergence and   for locating them. preliminary results indicate that our method is able to improve the accuracy of an scfg designed to model terminators by correcting known structural errors as well making modifications to parts of the grammar with no known deficiencies.
acknowledgments
this research was supported in part by nsf career award iis-1 and nih grant 1 lm1. thanks to soumya ray for helpful comments on an earlier version of this paper.
references
 chen  1  chen  s. 1. building probabilistic models for natural language. ph.d. dissertation  harvard university.
 chickering  1  chickering  d. m. 1. learningequivalence classes of bayesian-network structure. in proceedings of the twelfth international conference on uncertainty in artificial intelligence. san francisco  ca: morgan kaufmann.
 eddy & durbin  1  eddy  s. r.  and durbin  r. 1. rna sequence analysis using covariance models. nucleic acids research 1-1.
 heckerman  geiger  & chickering  1  heckerman  d.; geiger  d.; and chickering  d. m. 1. learning bayesian networks. machine learning 1-1.
 herzel & grosse  1  herzel  h.  and grosse  i. 1. correlations in dna sequences: the role of protein coding segments. physical review e 1 .
 lari & young  1  lari  k.  and young  s. j. 1. the estimation of stochastic context-free grammars using the inside-outside algorithm. computer speech and language 1-1.
 ourston & mooney  1  ourston  d.  and mooney  r. 1. theory refinement combining analytical and empirical methods. artificial intelligence 1 :1.
 pazzani & kibler  1  pazzani  m.  and kibler  d. 1. the utility of knowledge in inductive learning. machine learning 1 :1.
 sakakibara et al.  1  sakakibara  y.; brown  m.; hughey  r.; mian  i. s.; sjo：lander  k.; underwood  r. c.; and haussler  d. 1. stochastic context-free grammars for trna modeling. nucleic acids research 1-1.
 stolcke  1  stolcke  a. 1. bayesian learning of probabilistic language models. ph.d. dissertation  university of california  berkeley.
 towell & shavlik  1  towell  g.  and shavlik  j. 1. knowledge-based artificial neural networks. artificial intelligence 1 1 :1.
automatically extracting and comparing lexicalized grammars for different languages
fei xia  chung-hye han  martha palmer  and aravind joshi
 university of pennsylvania philadelphia pa 1  usa
fxia chunghye mpalmer joshi  linc.cis.upenn.eduabstract
in this paper  we present a quantitative comparison between the syntactic structures of three languages: english  chinese and korean. this is made possible by first extracting lexicalized tree adjoining grammars from annotated corporafor each language and then performing the comparison on the extracted grammars. we found that the majority of the core grammar structures for these three languages are easily inter-mappable.
1	introduction
the comparison of the grammars extracted from annotated corpora  i.e.  treebanks  is important on both theoretical and engineering grounds. theoretically  it allows us to do a quantitative testing of the universal grammar hypothesis. one of the major concerns in modern linguistics is the establishment of an explanatorybasis for the similarities and variations among languages. the working assumption is that languages of the world share a set of universal linguistic principles and the apparent structural differences attested among languages can be explained as variation in the way the universal principles are instantiated. comparison of the extracted syntactic trees allows us to quantitatively evaluate how similar the syntactic structures of different languages are. from an engineering perspective  the extracted grammars and the links between the syntactic structures in the grammars are valuable resources for nlp applications  such as parsing  computational lexicon development  and machine translation  mt   to name a few.
　in this paper we first briefly discuss some linguistic characteristics of english  chinese  and korean  and introduce the treebanks for the three languages. we then describe a tool that extracts lexicalized tree adjoining grammars  ltags  from the treebanks and the results of its application. next  we describe our methodology for automatic comparison of the extracted treebank grammars  which consists primarily of matching syntactic structures  namely  templates  sub-templatesand context-freerules  in each pair of treebank
grammars. the ability to perform this type of comparison for different languages enables us to distinguish languageindependent features from language-dependent ones. therefore  our grammar extraction tool is not only an engineering tool of great value in improving the efficiency and accuracy of grammar development  but it is also very useful for investigating theoretical linguistics.
1	our annotated corpora
in this section  we briefly discuss some linguistic characteristics of english  chinese  and korean  and introduce the treebanks for these languages.
1	differences between the three languages
these three languages belong to different language families: english is germanic  chinese is sino-tibetan  and korean is altaic  comrie  1 . there are several major differences between these languages. first  both english and chinese have predominantly subject-verb-object  svo  word order  whereas korean has underlying sov order. second  the word order in korean is freer than in english and chinese in the sense that argument nps are freely permutable  subject to certain discourse constraints . third  korean and chinese freely allow subject and object deletion  but english does not. fourth  korean has richer inflectional morphology than english  whereas chinese has little  if any  inflectional morphology.
1	treebank description
the treebanks that we used in this paper are the english penn treebank ii  marcus et al.  1   the chinese penn treebank  xia et al.  1b   and the korean penn treebank  han et al.  1 . the main parameters of these treebanks are summarized in table 1 the tagsets include four types of tags: part-of-speech  pos  tags for head-level annotation  syntactic tags for phrase-level annotation  function tags for grammatical function annotation  and empty category tags for dropped arguments  traces  and so on.
　we chose these treebanks because they all use phrase structure annotation and their annotation schemata are similar  which facilitates the comparison between the extracted treebank grammars. figure 1 shows an annotated sentence from the english penn treebank.
languagecorpus sizeave sentencetagset words lengthsizeenglish1k1 words1chinese1k1 words1korean1k1 words1table 1: sizes of the treebanks and their tagsets
  s  pp-loc  in at 
	     	 np  nnp fnx  
 np-sbj-1  nns underwriters  
 advp  rb still  
 vp  vbp draft 
 np  nns policies  
 s-mnr  
 np-sbj  -none- *-1  
 vp  vbg using 
 np
 np  nn fountain   nns pens  
 cc and 
 np  vbg blotting   nn papers        
figure 1: an example from the english penn treebank
1	extracting grammars
in this section  we give a brief introduction to the ltag formalism and to a system named lextract  which was built to extract ltags from the treebanks  xia  1; xia et al.  1a .
1	the grammar formalism
ltags are based on the tree adjoining grammar formalism developed by joshi and his colleagues  joshi et al.  1; joshi andschabes  1 . the primitiveelements of an ltag are elementary trees  etrees . each etree is associated with a lexical item  called the anchor of the tree  on its frontier. ltags possess many desirable properties  such as the extended domain of locality  which allows the encapsulation of all arguments of the anchor associated with an etree. there are two types of etrees: initial trees and auxiliary trees. an auxiliary tree represents a recursive structure and has a unique leaf node  called the foot node  which has the same syntactic category as the root node. leaf nodes other than anchor nodes and foot nodes are substitution nodes. etrees are combined by two operations: substitution and adjunction. the resulting structure of the combined etrees is called a derived tree. the history of the combination process is expressed as a derivation tree. figure 1 shows the etrees  the derived tree  and the derivationtree for the sentence underwriters still draft policies. foot and substitution nodes are marked by and   respectively. the dashed and solid lines in the derivation tree are for adjunction and substitution operations  respectively.
1	the target grammars
without further constraints  the etrees in the target grammar  i.e.  the grammar to be extracted by lextract  could be of various shapes. lextract recognizes three types of relations between the anchor of an etree and other nodes in the etree; namely  predicate-argument  modification  and coordination relations. it imposes the constraint that all the etrees to be

draft #1 
underwriters #1  
	 b  derived tree	 c  derivation tree
figure 1: etrees  derived tree  and derivation tree for underwriters still draft policies
	m	q	xm
w
	 a  spine-etree	 b   mod-etree	 c  conj-etree
figure 1: three types of elementary trees in the target grammar
extracted should fall into exactly one of the three patterns  as in figure 1 :1
　spine-etrees for predicate-argument relations: is the head of and the anchor of the etree. the etree is formed by the spine	and the arguments of .
　mod-etrees for modification relations: the root of the etree has two children  one is a foot node with the same label as the root node  and the other node is a modifier of the foot node. is further expanded into a spine-etree whose head is the anchor of the whole mod-etree.
　conj-etrees for coordination relations: in a conj-etree  the children of the root are two conjoined constituents and a node for a coordinating conjunction. one conjoined constituent is marked as the foot node  and the other is expanded into a spine-etree whose head is the anchor of the whole tree.
　spine-etrees by themselves are initial trees  whereas modetrees and conj-etrees are auxiliary trees.
1	the lextract algorithm
the core of lextract is an extraction algorithm that takes a treebank sentence such as the one in figure 1 and treebankspecific information provided by the user of lextract  and
	#1:	#1:	#1:	#1:	#1:	#1:
	s	np	np	vp	s	np
	vp*	npnns
innp
policies 
	at	still	draft
	#1:	#1:	#1:	#1:	#1:	#1:
	vp	np	np	np	np
	vp*	np*	np
blotting
paper
using
figure 1: the extracted etrees from the phrase structure in figure 1
produces a set of etrees as in figure 1 and a derivation tree. lextract's extraction algorithm has been described in  xia  1  and is completely language-independent. it has been successfully applied to the development of language processing tools such as supertaggers  xia et al.  1a  and statistical ltag parsers  sarkar  1 .
1	extracted grammars
the results of running lextract on english  chinese  and korean treebanks are shown in table 1. templates are etrees with the lexical items removed. for instance  #1  #1  and #1 in figure 1 are three distinct etrees  but they share the same template. lextract is designed to extract ltags  but simply reading context-free rules off the templates in an extracted ltag will yield a context-free grammar. the last column in the table shows the numbers of the non-lexicalized contextfree rules.
　in each treebank  a small subset of template types  which occur very frequently in the treebank and can be seen as members of the core of the treebank grammar  covers the majority of template tokens in the treebank. for instance  the top 1  1  1 and 1  respectively  template types in the english penn treebank cover 1%  1%  1% and 1%  respectively  of the tokens  whereas about half  1  of the template types occur once  accounting for only 1% of the template tokens in total.
templateetreewordcontext-freetypestypestypesrulesenglish1 1 1chinese1 1 1korean1 1 1table 1: grammars extracted from three treebanks
1	comparing treebank grammars for different languages
in this section  we describe our methodology for comparing treebank grammars and the experimental results.
1	methodology
to compare treebank grammars  we need to ensure that the treebank grammars are based on the same tagset. to achieve
	vp	spine: pp-  p
vp*subcat:  p   np 
　　　with root pp mod-pair:   vp*  pp 
 a  template b  sub-templatesfigure 1: the decomposition of an etree template  in subtemplates    marks the anchor in subcategorization frame  * marks the modifiee in a modifier-modifiee pair. 
that  we first create a new tagset that includes all the tags from the three treebanks. then we merge several tags in this new tagset into a single tag.1 next  we replace the tags in the original treebanks with the tags in the new tagset  and then re-run lextract to build treebank grammars from those treebanks.
　now that the treebank grammars are based on the same tagset  we can compare them according to the templates  context-freerules  and sub-templates that appear in more than one treebank - that is  given a pair of treebank grammars  we first calculate how many templates occur in both grammars;1 second  we read context-free rules off the templates and compare these context-free rules; third  we decompose each template into a list of sub-templates  e.g.  spines and subcategorization frames  and compare these sub-templates. a template is decomposed as follows: a spine-etree template is decomposed into a spine and a subcategorization frame; a mod-etree template is decomposed into a spine  a subcategorizationframe  anda modifier-modifieepair; a conj-etreetemplate is decomposed into a spine  a subcategorization frame  and a coordination tuple. figure 1 shows the decomposition of a mod-etree template.
1	initial results
after the tags in original treebanks have been replaced with the tags in the new tagset  the numbers of templates in the new treebank grammars decrease by about 1%  as shown in the second column of table 1  cf. the second column in table 1 . table 1 also lists the numbers of context-free rules and sub-templates e.g.  spines and subcategorization frames  in each grammar.
templatescontext-free rulessubtemplatesspinessubcat framesmod-pairsconj-tuplestotaleng1111ch1111kor1111table 1: treebank grammars with the new tagset
templatescontext-free rulessub-templates eng  ch type  # 11token  % 1/11/11/1 eng  kor type  # 11token  % 1/11/11/1 ch  kor type  # 11token  % 1/11/11/1table 1: comparisons of templates  context-free rules  and sub-templates in three treebank grammars　the third column of table 1 lists the numbers of template types shared by each pair of treebank grammars and the percentage of the template tokens in each treebank that are covered by these common template types. for example  there are 1 template types that appear in both english and chinese treebank grammars. these 1 template types account for 1% of the template tokens in the english treebank  and 1% of the template tokens in the chinese treebank. the table shows that  although the numbers of matched templates are not very high  most of these templates have high frequency and therefore account for the majority of the template tokens in the treebanks. for instance  in the  eng  ch  pair  the 1 template types that appear in both grammars is only 1% of all the english template types  but they cover 1% of the template tokens in the english treebank.
　if we compare sub-templates  rather than templates  the percentages of matched sub-template tokens  as shown in the last column in table 1  are higher than the percentages of matched template tokens. this is because two distinct templates may have common sub-templates. similarly  the percentages of matched context-freerules  see the fourth column in table 1  are higher than the percentages of matched template tokens.
1	results using thresholds
the comparison results shown in table 1 used every template in the treebank grammars regardless of the frequency of the template in the corresponding treebank. one potential problem with this approach is that some annotation errors in the treebanks could have a substantial effect on the comparison results. one such scenario is as follows: to compare languages a and b  we use treebanks for language a and treebank for language b. let and be the grammars extracted from and   respectively  and let be a template that appears in both grammars. now suppose that is a linguistically valid template for language a and it accounts for 1% of the template tokens in   but is not a valid template for language b and it appears once in treebank b only due to annotation errors. in this scenario  if excluding template covers 1% of the template tokens in treebank a  then including covers 1% of the template tokens in treebanka. in other words  the single error in treebank b  which results in template being included in   changes the comparison results dramatically.
　because most templates that are due to annotation errors occur very infrequently in the treebanks  we used a threshold to discard from the treebanks and treebank grammars all the templates with low frequency in order to reduce the effect of treebank annotation errors on the comparison results  table 1 shows the numbers of templates in the treebank grammars when the threshold is set to various values. for example  the last column lists the numbers of templates that occur at least 1 times in the treebanks.
　table 1 shows the numbers of matched templates and the percentages of matched template tokens when the low frequency templates are removed from the treebanks and treebank grammars. as the value of the threshold increases  for each language pair the number of matched templates decreases. the percentage of matched template tokens might decrease a fair amount at the beginning  but it levels off after the threshold reaches a certain value. this tendency is further illustrated in figure 1. in this figure  the x-axis is the threshold value  which ranges from 1 to 1; the y-axis is the percentage of matched template tokens in each treebank when the templates with low frequency are discarded. the curve on the top is the percentage of template tokens in the chinese treebank that are covered by the english grammar  and the curve on the bottom is the percentage of template tokens in the english treebank that are covered by the chinese grammar. both curves become almost flat once the threshold value reaches 1 or larger. this result implies that most templates due to annotation errors occur less than six times in the treebanks.
　to summarize  in order to get a better estimate of the percentage of matched template tokens  we should disregard the low frequency templates in the treebanks. we have shown that this strategy reduces the effect of annotation errors on the comparison results  see table 1 and figure 1 . this strategy also makes the difference between the sizes of our three treebanks less important because once treebanks reach a certain size  the new templates extracted from additional data tend to have very low frequency in the whole treebank.

11111english11111chinese11111korean11111table 1: the numbers of templates in the treebank grammars with the threshold set to various values
threshold11111 eng  ch type  # 11111token  % 1/11/11/11/11/11/11/11/11/1 eng  kor type  # 11111token  % 1/11/11/11/11/11/11/11/11/1 ch  kor type  # 11111token  % 1/11/11/11/11/11/11/11/11/1table 1: matched templates in the treebank grammars with various threshold values
figure 1: the percentages of matched template tokens in the
english and chinese treebanks with various threshold values
1	unmatched templates
our experiments  see table 1 and 1  show that the percentages of unmatched template tokens in three treebanks range from 1% to 1%  depending on the language pairs and the threshold value. given a language pair  there are various reasons why a template appears in one treebank grammar  but not in the other. we divide those unmatched templates into two categories: spuriously unmatched templates and truly unmatched templates.
spuriously unmatched templates spuriously unmatched templates are those that either should have found a matched template in the other grammar or should not have been created by lextract in the first place if the treebanks had been complete  uniformly annotated  and error-free. a spuriously unmatched template might exist because of one of the following reasons:
　 s1  treebank coverage: the template is linguistically sound in both languages  and  therefore  should belong to the grammars for these languages. however  the template appears in only one treebank grammar because the other treebank is too small to include such a template. figure 1 s1  shows a template that is valid for both english and chinese  but it appears only in the english treebank  not in the chinese treebank.
figure 1: spuriously unmatched templates
　 s1  annotation differences: treebanks may choose different annotations for the same constructions; consequentially  the templates for those constructions look different. figure 1 s1  shows the templates used in english and chinese for a vp such as  surged 1  dollars  . in the template for english  the qp projects to an np  but in the template for chinese  it does not.
　 s1  treebank annotation errors: a template in a treebank may result from annotation errors in that treebank. if no corresponding mistakes are made in the other treebank  the template in the first treebank will not match any template in the second treebank. for instance  in the english treebank the adverb about in the sentence about 1 people showed up is often mis-tagged as a preposition  resulting in the template in figure 1 s1 . not surprisingly  that template does not match any template in the chinese treebank.
truly unmatched templates a truly unmatched template is a template that does not match any template in the other treebank even if we assume both treebanks are perfectly annotated. here  we list three reasons why a truly unmatched template might exist.
　 t1  word order: the word order determines the positions of dependents with respect to their heads. if two languages have different word orders  the templates that include dependents of a head are likely to look different. for example  figure 1 t1  shows the templates for transitive verbs in chinese and korean grammars. they do not match because of the different positions of the object of the verb.
　 t1  unique tags: for each pair of languages  some partof-speech tags and syntactic tags may appear in only one

figure 1: truly unmatched templates
language. therefore  the templates with those tags will not match any templates in the other language. for instance  in korean the counterparts of preposition phrases in english and chinese are noun phrases  with postpositions attaching to nouns   as shown in the right figure in figure 1 t1 ; therefore  the templates with pp in chinese  such as the left one in figure 1 t1   do not match any template in korean.
　 t1  unique syntacticrelations: some syntactic relations may be present in only one of the pair of languages being compared. for instance  the template in figure 1 t1  is used for the sentence such as  you should go   said john  where the subject of the verb said appears after the verb. no such template exists in chinese.
s1s1s1t1t1t1totaltype # 1111token % 1111111table 1: the distribution of the chinese templates that do not match any english templates
　so far  we have listed six possible reasons for unmatched templates. we have manually classified templates that appear in the chinese grammar  but not in the english grammar.1 the results are shown in table 1. the table shows that for the chinese-english pair  the main reason for unmatched templates is  t1 ; that is  the chinese treebank has tags for particles  such as aspect markers and sentence-ending particles   which do not exist in english. for other language pairs  the distribution of unmatched templates may be very different. for instance  table 1 indicates that the english grammar covers 1% of the template tokens in the korean treebank. if we ignore the word order in the templates  that percentage increases from 1% to 1%. in other words  the majority of the template tokens that appear in the korean treebank  but not in the english treebank  are due to the word order difference in the two languages. note that the word order difference only accounts for a small fraction of the unmatched templates in the chinese-english pair  see the fifth column in table 1 . this contrast is not surprising considering that english and chinese are predominantly head-initial  whereas korean is head-final.
1	conclusion
we have presented a method of quantitatively comparing grammars extracted from treebanks. our experimental results show a high proportion of easily inter-mappable structures  providing support for the universal grammar hypothesis. we have also described a number of reasons why a particular template does not match any templates in the other languages and tested the effect of word order on matching percentages.
　there are two natural extensions of this work. first  running an alignment algorithm on parallel bracketed corpora would produce word-to-word mappings. given such wordto-word mappings and our template matching algorithm  we can automatically create lexicalized etree-to-etree mappings  which can be used for semi-automatic transfer lexicon construction. second  lextract can build derivation trees for each sentence in the corpora. by comparing derivation trees for parallel sentencesin two languages  instances of structural divergences  dorr  1  can be automatically detected.
