
linear discriminant analysis  lda  is a popular feature extraction technique in statistical pattern recognition. however  it often suffers from the small sample size problem when dealing with the high dimensional data. moreover  while lda is guaranteed to find the best directions when each class has a gaussian density with a common covariance matrix  it can fail if the class densities are more general. in this paper  a new nonparametric feature extraction method  stepwise nearest neighbor discriminant analysis snnda   is proposed from the point of view of the nearest neighbor classification. snnda finds the important discriminant directions without assuming the class densities belong to any particular parametric family. it does not depend on the nonsingularity of the within-class scatter matrix either. our experimental results demonstrate that snnda outperforms the existing variant lda methods and the other stateof-art face recognition approaches on three datasets from att and feret face databases.
1 introduction
the curse of high-dimensionality is a major cause of the practical limitations of many pattern recognition technologies  such as text classification and object recognition. in the past several decades  many dimensionality reduction techniques have been proposed. linear discriminant analysis  lda   fukunaga  1  is one of the most popular supervised methods for linear dimensionality reduction. in many applications  lda has been proven to be very powerful.
¡¡the purpose of lda is to maximize the between-class scatter while simultaneously minimizing the within-class scatter. it can be formulated by fisher criterion:
	 	 1 
where w is a linear transformation matrix  sb is the betweenclass scatter matrix and sw is the within-class scatter matrix.

 
¡¡¡¡the support of nsf of china  1  and  1  is acknowledged.¡¡a major drawback of lda is that it often suffers from the small sample size problem when dealing with the high dimensional data. when there are not enough training samples  sw may become singular  and it is difficult to compute the lda vectors. for example  a 1 ¡Á 1 image in a face recognition system has 1 dimensions  which requires more than 1 training data to ensure that sw is nonsingular. several approaches liu et al.  1; belhumeur et al.  1; chen et al.  1; yu and yang  1  have been proposed to address this problem. a common problem with all these proposed variant lda approaches is that they all lose some discriminative information in the high dimensional space.
¡¡another disadvantage of lda is that it assumes each class has a gaussian density with a common covariance matrix. lda guaranteed to find the best directions when the distributions are unimodal and separated by the scatter of class means. however  if the class distributions are multimodal and share the same mean  it fails to find the discriminant direction fukunaga  1 . besides  the rank of sb is c   1  where c is the number of classes. so the number of extracted features is  at most  c   1. however  unless a posteriori probability function are selected  c   1 features are suboptimal in bayes sense  although they are optimal with regard to fisher criterion  fukunaga  1 .
¡¡in this paper  a new feature extraction method  stepwise nearest neighbor discriminant analysis snnda   is proposed. snnda is a linear feature extraction method in order to optimize nearest neighbor classification  nn . nearest neighbor classification  duda et al.  1  is an efficient method for performing nonparametric classification and often used in the pattern classification field  especially in object recognition. moreover  the nn classifier has a close relation with the bayes classifier. however  when nearest neighbor classification is carried out in a high-dimensional feature space  the nearest neighbors of a point can be very far away  causing bias and degrading the performance of the rule  hastie et al.  1 . hastie and tibshirani  hastie and tibshirani  1  proposed a discriminant adaptive nearest neighbor  dann  metric to stretch the neighborhood in the directions in which the class probabilities don't change much  but their method also suffers from the small sample size problem.
¡¡snnda can be regarded as an extension of nonparametric discriminant analysis fukunaga and mantock  1   but it doesn't depend on the nonsingularity of the within-class scatter matrix. moreover  snnda finds the important discriminant directions without assuming the class densities belong to any particular parametric family.
¡¡the rest of the paper is organized as follows: section 1 gives the review and analysis of the current existing variant lda methods. then we describe stepwise nearest neighbor discriminant analysis in section 1. experimental evaluations of our method  existing variant lda methods and the other state-of-art face recognition approaches are presented in section 1. finally  we give the conclusions in section 1.
1 review and analysis of variant lda methods
the purpose of lda is to maximize the between-class scatter while simultaneously minimizing the within-class scatter.
¡¡the between-class scatter matrix sb and the within-class scatter matrix sw are defined as
 1 
 1 
where c is the number of classes; mi and pi are the mean vector and a priori probability of class i  respectively; m =  is the total mean vector; si is the covariance ma-
trix of class i.
¡¡lda method tries to find a set of projection vectors w ¡Ê rd¡Ád maximizing the ratio of determinant of sb to sw 
	w = argmax 	 1 
w
where d and d are the dimensionalities of the data before and after the transformation respectively.
¡¡from eq. 1   the transformation matrix w must be constituted by the d eigenvectors ofcorresponding to its first d largest eigenvalues  fukunaga  1 .
¡¡however  when the small sample size problem occurs  sw becomes singular and sw 1 does not exist. moreover  if the class distributions are multimodal or share the same mean  for example  the samples in  b   c  and  d  of figure 1   it can fail to find the discriminant direction fukunaga  1 . many methods have been proposed for solving the above problems. in following subsections  we give more detailed review and analysis of these methods.
1 methods aimed at singularity of sw
in recent years  many researchers have noticed the problem about singularity of sw and tried to overcome the computational difficulty with lda.
¡¡to avoid the singularity of sw  a two-stage pca+lda approach is used in  belhumeur et al.  1 . pca is first used to project the high dimensional face data into a low dimensional feature space. then lda is performed in the reduced pca subspace  in which sw is non-singular. but this method is obviously suboptimal due to discarding much discriminative information.
¡¡liu et al.  liu et al.  1  modified fisher's criterion by using the total scatter matrix st = sb + sw as the denominator instead of sw. it has been proven that the modified criterion is exactly equivalent to fisher criterion. however  when sw is singular  the modified criterion reaches the maximum value  namely 1  for any transformation w in the null space of sw. thus the transformation w cannot guarantee the maximum class separability |wtsbw| is maximized. besides  this method still needs to calculate an inverse matrix  which is time consuming. chen et al.  chen et al.  1  suggested that the null space spanned by the eigenvectors of sw with zero eigenvalues contains the most discriminative information. a lda method  called nlda in the null space of sw was proposed. it chooses the projection vectors maximizing sb with the constraint that sw is zero. but this approach discards the discriminative information outside the null space of sw. figure 1 a  shows that the null space of sw probably contains no discriminant information. thus  it is obviously suboptimal because it maximizes the between-class scatter in the null space of sw instead of the original input space. besides  the performance of the nlda drops significantly when n   c is close to the dimension d  where n is the number of samples and c is the number of classes. the reason is that the dimensionality of the null space is too small in this situation and too much information is lost  li et al.  1 . yu et al.  yu and yang  1 proposed a direct lda  dlda  algorithm  which first removes the null space of sb. they assume that no discriminative information exists in this space. unfortunately  it be shown that this assumption is incorrect. fig.1 b  demonstrates that the optimal discriminant vectors do not necessarily lie in the subspace spanned by the class centers.
1.1
class 1 nlda 
class 1 lda 1.1.1
1.1
1
 1 1  1 1  1 1
 1 1
	 1	 1	 1	 1	 1  a1   1	1	1	 1	 1	 1	 b1  	1	1.1
figure 1:  a  shows that the discriminant vector  dashed line  of nlda contains no discriminant information.  b  shows that the discriminant vector  dashed line  of dlda is constrained to pass through the two class centers m1 and m1. but according to the fisher criteria  the optimal discriminant projection should be solid line  both in  a  and  b  .
1 methods aimed at limitations of sb
when the class conditional densities are multimodal  the class separability represented by sb is poor. especially in the case that each class shares the same mean  it fails to find the discriminant direction because there is no scatter of the class means fukunaga  1 .
¡¡notice the rank of sb is c   1  so the number of extracted features is  at most  c   1. however  unless a posteriori probability function are selected  c   1 features are suboptimal in bayes sense  although they are optimal with regard to fisher criterion  fukunaga  1 .
¡¡in fact  if classification is the ultimate goal  we need only estimate the class density well near the decision boundary hastie et al.  1 .
¡¡fukunaga and mantock  fukunaga and mantock  1  presented a nonparametric discriminant analysis  nda  in an attempt to overcome these limitations presented in lda. in nonparametric discriminant analysis the between-class scatter sb is of nonparametric nature. this scatter matrix is generally full rank  thus loosening the bound on extracted feature dimensionality. also  the nonparametric structure of this matrix inherently leads to the extracted features that preserve relevant structures for classification. bressan et al.  bressan and vitria  1`   explored the nexus between nonparametric discriminant analysis  nda  and the nearest neighbors  nn  classifier and gave a slight modification of nda which extends the two-class nda to a multi-class version.
¡¡although these nonparametric methods overcomes the limitations of sb  they still depend on the singularity of sw or s w . the rank of s w must be no more than n   c.
1 stepwise nearest neighbor discriminant analysis
in this section  we propose a new feature extraction method  stepwise nearest neighbor discriminant analysis snnda . snnda also uses nonparametric between-class and withinclass scatter matrix. but it does not depend on singularity of within-class scatter matrix and improves the performance of nn classifier.
1 nearest neighbor discriminant analysis criterion
assuming a multi-class problem with classes ¦Øi i = 1 ... c   we define the extra-class nearest neighbor for a sample x ¡Ê ¦Øi as xe = {x1 ¡Ê/ ¦Øi| ||x1   x|| ¡Ü ||z   x||  z /¡Ê ¦Øi}.  1 
¡¡in the same fashion  the set of intra-class nearest neighbors are defined as
	xi = {x1 ¡Ê ¦Øi| ||x1   x|| ¡Ü ||z   x||  z ¡Ê ¦Øi}.	 1 
¡¡the nonparametric extra-class and intra-class differences are defined as
 e = x   xe 	 1   i = x   xi.	 1 
.
¡¡the nonparametric between-class and within-class scatter matrix are defined as
s b=xn
   wn  en   en t  n=1 1 s w=xn
wn  in   in t  1 n=1
where wn is the sample weight defined as
	 	 1 
|| n|| + || n||
where ¦Á is a control parameter between zero and infinity. this sample weight is introduced to deemphasize the samples in the class center and give emphases to the samples near to the other class. the sample that has a larger ratio between the nonparametric extra-class and intra-class differences is given an undesirable influence on the scatter matrix. the sample weights in eq. 1  take values close to 1 near the classification boundaries and drop to zero as we move to class center. the control parameter ¦Á adjusts how fast this happens. in this paper  we set ¦Á = 1.
¡¡from the eq. 1  and  1   we can see thatrepresents the distance between the sample xn and its nearest neighbor in the different classes  and represents the distance between the sample xn and its nearest neighbor in the same class. given a training sample xn  the accuracy of the nearest neighbor classification can be directly computed by examining the difference
	 	 1 
where  e and  i are nonparametric extra-class and intraclass differences and defined in eq. 1  and  1 .
¡¡if the difference ¦¨n is more than zero  xn will be correctly classified. otherwise  xn will be classified to the false class. the larger the difference ¦¨n is  the more accurately the sample xn is classified.
¡¡assuming that we extract features by the d ¡Ád linear projection matrix w with a constraint that wtw is an identity matrix  the projected sample xnew = wtx. the projected nonparametric extra-class and intra-class differences can be written as ¦Äe = wt e and ¦Äi = wt i. so we expect to find the optimal w to make the difference ||¦Äne||1  ||¦Äni||1 in the projected subspace as large as possible.
n
	wc = argmax	.	 1 
n=1
¡¡this optimization problem can be interpreted as: find the linear transform that maximizes the distance between classes  while minimizing the expected distance among the samples of a single class.
considering that 


figure 1: first projected directions of nnda  solid  and lda  dashed  projections  for four artificial datasets.
xn
	 tr wt 	wn in  in t w 
n=1
= tr wts bw    tr wts ww 
¡¡= tr wt s b   s w w  	 1  where tr ¡¤  is the trace of matrix  s b and s w are the nonparametric between-class and within-class scatter matrix  as defined in eq. 1  and  1 .
so eq. 1  is equivalent to
wc = argmaxtr wt s b   s w w .	 1  w
¡¡we call eq. 1  the nearest neighbor discriminant analysis criterion nnda .
¡¡the projection matrix wc must be constituted by the d eigenvectors of  s b   s w  corresponding to its first d largest eigenvalues.
¡¡figure 1 gives comparisons between nnda and lda. when the class density is unimodal   a    nnda is approximately equivalent to lda. but in the cases that the class density is multimodal or that all the classes share the same mean   b   c  and  d    nnda outperforms lda greatly.
1 stepwise dimensionality reduction
in the analysis of the nearest neighbor discriminant analysis criterion  notice that we calculate nonparametric extra-class and intra-class differences   e and  i  in original high dimensional space  then project them to the low dimensional space  ¦Äe = wt e and ¦Äi = wt i   which does not exactly agree with the nonparametric extra-class and intra-class differences in projection subspace except for the orthonormal transformation case  so we have no warranty on distance preservation. a solution for this problem is to find the projection matrix wc by stepwise dimensionality reduction method. in each step  we re-calculate the nonparametric extra-class and intra-class differences in its current dimensionality. thus  we keep the consistency of the nonparametric extra-class and intra-class differences in the process of dimensionality reduction.
¡¡figure 1 gives the algorithm of stepwise nearest neighbor discriminant analysis.
  give d dimensional samples {x1 ¡¤¡¤¡¤ xn}  we expect to find d dimensional discriminant subspace.
  suppose that we find the projection matrix wc via t steps  we reduce the dimensionality of samples to dt in step t  and dt meet the conditions: dt 1   dt   dt+1  d1 = d and dt = d.
  for t = 1 ¡¤¡¤¡¤ t
1. calculate the nonparametric between-class and within-class scatter matrix in the current dt 1 dimensionality 
1. calculate the projection matrixmatrix.
1. project the samples by the projection matrix	 
.
  the final transformation matrix	.figure 1: stepwise nearest neighbor discriminant analysis
1 discussions
snnda has an advantage that there is no need to calculate the inverse matrix  so it is a more efficient and stable method. moreover  though snnda optimizes the 1-nn classification  it is easy to extend it to the case of k-nn.
¡¡however  a drawback of snnda is the computational inefficiency in finding the neighbors when the original data space is high dimensionality. a improved method is that pca is first used to reduce the dimension of data to n 1  the rank of the total scatter matrix  through removing the null space of the total scatter matrix. then  snnda is performed in the transformed space. yang et al.  yang and yang  1  shows that no discriminant information is lost in this transformed space.
1 experiments
in this section  we apply our method to face recognition and compare it with the existing variant lda methods and the other state-of-art face recognition approaches  such as pca  turk and pentland  1   pca+lda  belhumeur et al.  1   nlda  chen et al.  1   nda  bressan and vitria  1`   and bayesian  moghaddam et al.  1  approaches. all the experiments are repeated 1 times independently and the average results are calculated.
1 datasets
to evaluate the robustness of snnda  we perform the experiments on three datasets from the popular att face database  samaria and harter  1  and feret face database  phillips et al.  1 . the descriptions of the three datasets are below:
att dataset this dataset is the att face database  formerly 'the orl database of faces'   which contains 1 images  1 ¡Á 1  of 1 persons  1 images per person. the images are taken at different times  varying lighting slightly  facial expressions  open/closed eyes  smiling/non-smiling  and facial details  glasses/no-glasses . each image is linearly stretched to the full range of pixel values of  1 . fig.1 shows some face examples in this database. the set of the 1 images for each person is randomly partitioned into a training subset of 1 images and a test set of the other 1. the training set is then used to learn basis components  and the test set for evaluate.

figure 1: face examples from att database
feret dataset 1 this dataset is a subset of the feret database with 1 subjects only. each subject has 1 images:  a  one taken under controlled lighting condition with a neutral expression;  b  one taken under the same lighting condition as above but with different facial expressions  mostly smiling ; and  c  one taken under different lighting condition and mostly with a neutral expression. all images are pre-processed using zeromean-unit-variance operation and manually registered using the eye positions. all the images are normalized by the eye locations and are cropped to the size of 1 ¡Á 1. a mask template is used to remove the background and the hair. histogram equalization is applied to the face images for photometric normalization. two images for each person is randomly selected for training and the rest one is used for test.
feret dataset 1 this dataset is a different subset of the feret database. all the 1 people from the feret fa/fb data set are used in the experiment. there are two face images for each person. this dataset has no overlap between the training set and the galley/probe set according to the feret protocol  phillips et al.  1 . 1 people are randomly selected for training  and the remaining 1 people are used for testing. for each testing people  one face image is in the gallery and the other is for probe. all images are pre-processed by using the same method in feret dataset 1.
1 experimental results
fig. 1 shows the rank-1 recognition rates with the different number of features on the three different datasets. it is shown that snnda outperforms the other methods. the recognition rate of snnda can reach almost 1% on att dataset.
the recognition rate of snnda have reached 1% on two feret dataset surprisedly when the dimensionality of samples is about 1  while the other methods have poor performances in the same dimensionality. moreover  snnda does not suffer from overfitting. except snnda and pca  the rank-1 recognition rates of the other methods have a descent when the dimensionality increases continuously.
¡¡fig. 1 shows cumulative recognition rates on the three different datasets. from it  we can see that none of the cumulative recognition rates can reach 1% except snnda.
¡¡when dataset contains the changes of lighting condition  such as feret dataset 1   snnda also has obviously better performance than the others.
¡¡different from att dataset and feret dataset 1  where the class labels involved in training and testing are the same  the ferer dataset 1 has no overlap between the training set and the galley/probe set according to the feret protocol  phillips et al.  1 . the ability of generalization from known subjects in the training set to unknown subjects in the gallery/probe set is needed for each method. thus  the result on feret dataset 1 is more convincing to evaluate the robust of each method. we can see that snnda also gives the best performance than the other methods on feret dataset 1.
¡¡a major character  displayed by the experimental results  is that snnda always has a stable and high recognition rates on the three different datasets  while the other methods have unstable performances.
1 conclusion
in this paper  we proposed a new feature extraction method  stepwise nearest neighbor discriminant analysis snnda   which finds the important discriminant directions without assuming the class densities belong to any particular parametric family. it does not depend on the nonsingularity of the within-class scatter matrix either. our experimental results on the three datasets from att and feret face databases demonstrate that snnda outperforms the existing variant lda methods and the other state-of-art face recognition approaches greatly. moreover  snnda is very efficient  accurate and robust. in the further works  we will extend snnda to non-linear discriminant analysis with the kernel method. another attempt is to extend snnda to the k-nn case.
