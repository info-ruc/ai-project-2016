
in this paper  we present an algorithm to identify different types of objects from 1d and 1d laser range data. our method is a combination of an instance-based feature extraction similar to the nearest-neighbor classifier  nn  and a collective classification method that utilizes associative markov networks  amns . compared to previous approaches  we transform the feature vectors so that they are better separable by linear hyperplanes  which are learned by the amn classifier. we present results of extensive experiments in which we evaluate the performance of our algorithm on several recorded indoor scenes and compare it to the standard amn approach as well as the nn classifier. the classification rate obtained with our algorithm substantially exceeds those of the amn and the nn.
1 introduction
in this paper  we consider the problem of identifying objects in 1d and 1d range measurements. we believe that the segmentation of these data into known object classes can be useful in many different areas  such as map registration  robot localization  object manipulation  or human-robot interaction. for example  a robot that is able to classify the 1d data acquired by a range sensor has a better capability of finding corresponding data points in different measurements. in this way  the scan registration can be carried out more reliably. other application areas  in which object recognition is important  include seek-and-rescue tasks such as the one aimed by the robocup rescue initiative.
¡¡what makes the object detection especially hard is the fact that it involves both  the segmentation and the classification of the segments. to simultaneously solve this segmentation and classification problem  recently collective classification approaches have become a popular approach. they are based on the assumption that in many real-world domains spatial neighbors in the data tend to have the same labels. for example  anguelov et al.  formulate the problem of segmenting 1d range data into known classes as a supervised learning task and apply collective classification to solve the task. they use a technique called associative markov networks  amns   which combines the concept of relational learning with collective classification.
¡¡so far  amns have been used only based on the log-linear model  which means that there is only a linear relationship between the extracted features and the weight parameters learned by the algorithm. this results in classification algorithms that learn hyper-planes in the feature space to separate the object classes. however  in many real-world applications the assumption of the linear separability of the features is often not justified. one way to overcome this problem may be to extend the log-linear model  but this is still subject to ongoing research. in this paper  we propose a different approach. by combining the ideas of instance-based classification with the amn approach  we obtain a classifier that is more robust against the error introduced by the linear-separability assumption. we present a way to compute new feature vectors from a given set of standard features  i.e.  we transform the original features  and show that these features are better suited for classification using the amn approach.
¡¡this paper is organized as follows. the next section gives an overview of the work that has been presented so far in this area. then we summarize the concepts of collective classification using associative markov networks and describe shortly how learning and inference is done in amns. then  a detailed description of our approach is presented. finally  we present the results of experiments  which illustrate that our method provides better classifications than previous approaches.
1 related work
the problem of recognizing objects from 1d range data has been studied intensively in the past. most of the approaches can be distinguished according to the features used. one popular approach are spin images  johnson  1; de alarco¡än et al.  1; frome et al.  1   which are rotationally and translationally invariant local descriptors. spin images are also used as features in the work presented here. other types of 1d features include local tensors  mian et al.  1   shape maps  wu et al.  1   and multi-scale features  li and guskov  1 . osada et al.  proposed a 1d object recognition technique based on shape distributions. whereas this approach requires a complete view of the objects  our method can deal with partially seen objects. additionally  huber et al. present an approach for parts-based object recognition. this method provides a better classification because nearby parts that are easier to identify than others help to guide the classification. a similar idea of detecting object components has been presented by ruiz-correa et al.   who introduced symbolic surface signatures. another approach to object recognition proposed by boykov and huttenlocher  is based on markov random fields  mrfs . they classify objects in camera images by incorporating statistical relationships between nearby object parts. a relational approach using mrfs has been proposed by limketkai et al. . the idea here is to exploit the spatial relationship between nearby objects  which in this case consist of 1d line segments. the work that is mostly related to the approach described in this paper is presented by anguelovet al.   in which an amn approach is used in a supervised learning setting to classify 1d range data. in this paper  we combine this approach with techniques from instance-based classification to obtain improved classification results.
1 range scan classification
suppose we are given a set of n data points p1 ... pn taken from a 1d or 1d scene and a set of k object classes c1 ... ck. a data point in this context may be a 1d scan point or a 1d grid cell of an occupancy grid. the following formulations are identical in these two cases.
¡¡for each data point pi  we are also given a feature vector xi. later we will describe in detail how the features are defined in the context of this paper. the task is to find a label yi ¡Ê {1 ...  k} for each pi so that all labels yi ... yn are optimal given the features. the notion of optimality in this context depends on the classification method we choose. in the standard supervised learning approach  we define a likelihood function p¦Ø y | x  of the labels y given the features x. then  the classification problem can be subdivided into two tasks: first we need to find good parameters ¦Ø  for the likelihood function p¦Ø y | x . then we seek for good labels y  that maximize this likelihood. assuming we are given a training data set  in which the labels y  have been assigned by hand  we can formulate the classification as follows:
  learning step: find ¦Ø  = argmax¦Øp¦Ø y  | x 
  inference step: find y  = argmaxyp¦Ø  y | x 
1 collective classification
in most standard classification methods  such as bayes classification  nearest neighbor  adaboost etc. the classification of a data point only depends on its local features  but not on the labeling of nearby data points. however  in practice one often observes a statistical dependence of the labelings associated to neighboring data points. for example  if we consider the local planarity of a scan point as a feature  it may happen that the likelihood p y | x  of the class label 'wall' is higher than that of the class label 'door'  although all other scan points in the vicinity of this point belong to the class 'door'. methods that use the information of neighboring data points are denoted as collective classification techniques  see chakrabarti and indyk . recently  a collective classification approach to classify 1d range scan data has been presented by anguelov et al. . the authors propose the use of associative markov networks  amns   which are a special type of relational markov random fields  described in taskar et al.. in the following we will briefly describe amns.
1 associative markov networks
an associative markov network is an undirected graph in which the nodes are represented by n random variables y1 ... yn. in our case  these random variables are discrete and correspond to the labels of each of the data points p1 ... pn. each node yi and each edge  yi yj  in the graph has an associated non-negativevalue   xi yi  and ¦× xij yi yj  respectively. these are also known as the potentials of the nodes and edges. the node potentials reflect the fact that for a given feature vector xi some labels are more likely to be assigned to pi than others  whereas the edge potentials encode the interactions of the labels of neighboring nodes given the edge features xij. whenever the potential of a node or edge is high for a given label  yi  or a label pair  yi yj   the conditional probability of these labels given the features is also high. the conditional probability that is represented by the network can be expressed as
n
1
	p y | x  =   xi yi   ¦× xij yi yj .	 1 
z
	i=1	 ij ¡Êe
here  z denotes the partition function which by definition is given as z = y in=1   xi yi  ij ¡Êe ¦× xij yi yj .
¡¡it remains to describe the node and edge potentials   and ¦×. in taskar et al.  the potentials are defined using the log-linear model. in this model  a weight vector wk is introduced for each class label k = 1 ...  k. the node potential   is then defined so that log  xi yi  = wkn ¡¤ xi where k = yi. accordingly  the edge potentials are defined as log¦× xij yi yj  = wke l ¡¤ xi where k = yi and l = yj. note that there are different weight vectors wkn ¡Ê rdn and wke l ¡Ê rde for the nodes and edges.
¡¡for the purpose of convenience we use a slightly different notation for the potentials  namely
k
	log  xi yi 	=  wkn ¡¤ xi yki	 1 
k=1
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡k	k log¦× xij yi yj 	=  wek l ¡¤ xij yki ylj 	 1 
k=1 l=1
where yki is an indicator variable which is 1 if point pi has label k and 1  otherwise.
¡¡in a further refinement step of our model  we introduce the constraints wke l = 1 for k  l and wke k ¡Ý 1. this results in ¦× xij k l  = 1 for k  l and ¦× xij k k  = ¦Ëkij  where ¦Ëkij ¡Ý 1. the idea here is that edges between nodes with different labels should be penalized over edges between equally labeled nodes. this last specification of amns makes it possible to run the inference step efficiently using graph cuts  see  boykov et al.  1; taskar  1  .
1 learning and inference with amns
in this section  we describe how learning and inference can be carried out with amns. first  we reformulate equation  1  as the conditional probability pw y | x  where the parameters ¦Ø are expressed by the weight vectors w =  wn we . by plugging in equations  1  and  1  we obtain that log pw y | x  equals
n	k	k
 wkn ¡¤ xi yki +  wke k ¡¤ xij yikykj   logzw x .	 1 
i=1 k=1	 ij ¡Êe k=1
note that the partition function z only depends on w and x  but not on the labels y.
1 learning
as mentioned above  in a standard supervised learning task the goal is to maximize pw y | x . however  the problem that arises here is that the partition function z depends on the weights w. this means that when maximizing log pw y  | x   the intractable calculation of z needs to be done for each
w. however  if we instead maximize the margin between the optimal labeling y  and any other labeling y defined by
           log p¦Ø y  | x    log p¦Ø y | x    1  the term zw x  cancels out and the maximization can be done efficiently. this method is referred to as maximum margin optimization. the details of this formulation are omitted here for the sake of brevity. we only note that the problem is reduced to a quadratic program  qp  of the form:
	1 + c¦Î	 1 
minw
1
n
	s.t. wxy  + ¦Î   ¦Ái ¡Ý n;	we ¡Ý 1;
i=1
¦Ái   i k;
ij ji¡Êe
  ¦Ákij ¦Ákji ¡Ý 1   ij ¡Ê e k
here  the variables that are solved for in the qp are the weights w =  wn we   a slack variable ¦Î and additional variables ¦Ái  ¦Áij and ¦Áji. we refer to taskar et al. for details.
1 inference
once the optimal weights w have been calculated  we can perform inference on an unlabeled test data set. this is done by finding the labels y that maximize log pw y | x . as mentioned above  z does not depend on y so that the maximization in equation  1  can be carried out without considering the last term. with the constraints imposed on the variables yki this leads to a linear program of the form
	n	k	k
	max  wkn ¡¤ xi yki +  wke ¡¤ xij ykij	 1 
	i=1 k=1	ij¡Êe k=1
k
s.t. yki ¡Ý 1   i k; yi = 1   i
k=1
ykij ¡Ü yki   ykij ¡Ü ykj   ij ¡Ê e k

figure 1: example of the feature transform ¦Ó for a two-class problem with two features. left: training data and test data with ground truth labeling. right: test data after applying ¦Ó.
here  we introduced variables ykij representing the labels of two points connected by an edge. the last two inequality conditions are a linearization of the constraint ykij = yki ¡Ä ykj.
¡¡in ourcurrentimplementation we performthe learning and inference step by solving the quadratic and linear program from equations  1  and  1 . the implementation is based on the c++ library ooqp  gertz and wright  1 . as mentioned above  the inference step can be performed more efficiently using graph-cuts. however  in our application  in which the data sets were comparably small  the linear program solver turned out to be fast enough.
1 instance-based extension
the main drawback of the amn classifier  which is based on the log-linear model  is that it separates the classes linearly. this assumes that the features are separable by hyperplanes  which is not justified in all applications. this does not hold for instance-based classifiers such as the nearestneighbor  nn  classifier. in nn classification  a query data point p  is assigned to the label that corresponds to the training data point p whose features x are closest to the features x  of p . in the learning step  the nn classifier simply stores the entire training data set and does not compute a reduced set of training parameters  which is why it is also called lazy classification.
¡¡the idea now is to combine the advantage of instancebased nn classification with the amn approach to obtain a collective classifier that is not restricted to the linear separability requirement. this will be presented in the next section.
1 the transformed feature vector

 a  training data	 b  test data  ground truth 	 c  nn	 d  amn	 e  iamn
figure 1: results on 1d data of an occupancy grid map.suppose we are given a general k-class classification problem where for each training data point p  we are given a feature vector x  of length l and a label  y ¡Ê {1 ...  k}. now  if we assume that the correct label for a new query feature vector x  corresponds to the label  y  of its closest training example x  in feature space  then the nn algorithm yields the optimal classification. of course  this assumption is not valid in general  but we can at least say that the label  y  will be more likely than any other label. as an example  consider the two-class problem with two features depicted in the left part of fig. 1. the two-dimensional feature space is defined by the training data shown as boxes. now  the true labels of an arbitrary test data set  here shown as triangles  will be related to the labels of their closest training feature points. in our example  the probability of the label  y ¡Ê {1} corresponding to x  is proportional to a gaussian distribution n ¦Ìk ¦Ò  with ¦Ìk = d x  x k  and k ¡Ê {1}. here  x k denotes the training example with label k that is closest to x  and d ¡¤  ¡¤  the distance in feature space. the variance ¦Ò is set to 1.
¡¡from the left side of fig. 1  we can see that any attempt to separate the classes using hyperplanes  in this case lines  will lead to severe classification errors. however  if we introduce a feature transform ¦Ó : rl ¡ú rk in such a way that ¦Ó x   =  d x  x 1  ... d x  x k    then the transformed features  t := ¦Ó x   are more easily separable by hyperplanes. this is illustrated on the right side of fig. 1. the reason for this is that the nn classifier always chooses the label corresponding to the smallest component of  t. this means  that the set tk of all transformed feature points t =  t1 ... tk  that will be assigned the label k can be described by
tk = {t ¡Ê rk|tk   t1 ¡Ä¡¤ ¡¤ ¡¤¡Ätk   tk 1 ¡Ätk   tk+1 ¡Ä¡¤ ¡¤ ¡¤¡Ätk   tk}.
as can be seen from this equation  the border of the set tk is described by k   1 hyperplanes passing through the origin. in the case of our two-class example  the only separating hyperplane  line  is described by the equation t1 = t1.
1 m nearest neighbors
one problem of the nn classifier is that the assignment of a label to a query point p  only depends on the labeling of one instance in the training set  namely the one whose feature vector is closest to x . however  it is possible that the features of other training instances are also very close to x   although they are labeled differently. for example  suppose that the distances of x  to the two closest training features x 1 and x 1 are very similar  and the corresponding labels  y1 and  y1 are different  the decision of assigning the label  y1 to p  may be wrong  especially in the presence of noise. therefore we proceed as follows: for the feature vector x  that corresponds to p  we compute the m nearest training instances in each of the k classes c1 ... ck and the corresponding distances d x  x mk   where k = 1 ...  k and m = 1 ...  m. these are used to
define the transformed feature vector ¦Óm x   as ¦Óm x   =  ... d x  x 1k  ... d x  x km  ...   1 
experiments show that higher values of m increase the classification rate  but for large m the improvement is very small. in our experiments  m = 1 turned out to be a good choice.
1 experimental results
we performed a series of experiments with 1d and 1d data to compare our instance-based amn  iamn  algorithm with the nn and the amn classifier. the results of these experiments demonstrate that the iamn outperforms the two other algorithms  independent of the features used.
1 implementation details
to speed up the learning process  we need to represent all feature vectors such that the nearest neighbor search in the feature space can be performed efficiently. to this end  we use kd-trees k1 ...  kk to store the training feature vectors of each class c1 ... ck. this way  the computational effort of the nearest neighbor lookup becomes logarithmic in the number of the stored instances.
¡¡thus  the training step consists of computing the features for the training data  transforming these features according to eq. 1 and assigning the transformed features to the nodes of the amn. the edge features of the amn consist of a constant scalar value  as described by anguelov et al. . after solving the quadratic program we obtain the weight vectors wk. then  in the inference step  we use the transformed features ¦Óm x  of the test data as node features for the amn. again  the edge features are constant.
feature computation depending on the input data used  we computed different types of features for the data points. in the case of the 1d grid data  each point in the map is represented by a set of geometrical features which are extracted from a laser range scan covering 1o field of view. each laser is simulated and centered at each point in the map representing a free space  mozos et al.  1 . because the number of geometrical features is huge  more than 1  we select the best ones using the adaboost algorithm.
¡¡for the 1d data set we computed spin images  johnson  1  with a size of 1 ¡Á 1 bins. the spherical neighborhood for computing the spin images had a radius between 1 and 1cm  depending on the resolution of the input data.

	 a  ground truth	 b  nn

	 c  amn	 d  iamn
figure 1: classification results for one scene from the multi data setdata reduction when classifying data sets with many points  e.g.  1d scans with over 1 scan points  the time and memory requirements of the amn classifier grows very large. in these cases  we perform a data set reduction  again using kd-trees: after inserting all data points into the tree  we prune the tree at a fixed level ¦Ë. all points in the subtrees below level ¦Ë are then merged into one mean point. the spin image features  however  are still computed on the whole data set to provide a high density in the feature extraction.
1 1d map annotation
for the 1d classification experiment we used an occupancy grid map of the interior of a building. the map was annotated with three different labels  namely 'corridor'  'room' and 'lobby'. then the map was divided into two non-overlapping submaps  one for training and one for testing. fig. 1 a  shows the submap used for training and fig. 1 b  the ground truth for the classification. the results obtained with the nn and the standard amn approach are shown in figs. 1 c  and 1 d   while fig 1 e  shows the result of our iamn algorithm. it can be seen that the iamn approach gives the best result. the classification rate of the nn is with 1% higher than that of the standard amn  which was 1%  but the classification is very noisy. in contrast  the iamn result is more consistent wrt. neighboring data points and has the highest classification rate with 1%.
1 1d scan point classification
furthermore  we evaluated the classification algorithms on three different 1d data sets with an overall number of 1 scanned scenes. the scans were obtained with a 1d laser range scanner. the first data set is called human and consists of 1 recorded scenes with two humans in varying poses. the scans were labeled into the four classes 'head'  'torso'  'legs'  and 'arms'. the second data set named single consists of 1 different 1d scans of the object classes 'chair'  'table'  'screen'  'fan'  and 'trash can'. each scan in the single data set contains just one object of each class  apart from tables  which may have screens standing on top of them. the last data set is named multi and consists of seven scans with multiple objects of the same types as in the single data set.
¡¡the human and single data sets were evaluated using cross validation. for the multi data set we used the object instances from the single set for training  because those were not occluded by other objects. the classification was performed on the complete scans.
¡¡fig. 1 shows a typical classification result for a scan from the multi test set. we can see that nn assigns wrong labels to different points while their neighbors are classified correctly. the amn results show that areas of points tend to be classified with the same label. however  due to the restriction to linearly separable data  many object parts are misclassified  especially in complex objects like chairs and fans. the results obtained with iamn are better even in these complex objects  because the transformed feature vectors computed by iamn are better suited for a classification based on separating hyper-planes. table 1 shows the resulting classification rates. we can see that the iamn classifier outperforms both of the others in all three data sets.
¡¡a statistical analysis using a t-student test with a significance level of 1 is shown in fig. 1. as can be seen  for the multi set our algorithm performs significantly better than both the nn and the standard amn. a detailed analysis of the classification results is depicted in fig. 1 wchich demonstrates that our iamn yields highly accurate results for all three data sets.
data setnnamniamnhuman1%1%1%single1%1%1%multi1%1%1%table 1: classification results for all three data sets
1 conclusions
in this paper we proposed an algorithm that combines associative markov networks with an instance-based approach similar to the nearest neighbor classifier for identifying ob-


figure 1: individual classification rates for all scenes.figure 1: results of a t-student test with significance level 1 on the three different 1d data sets. for the multi data set we obtain a significant improvement over nn and standard amn classification.
jects in 1d range data. in contrast to the approach suggested by anguelovet al.   our method is able to classify more complex objects with a diverse set of features per class. by using the distances of features to their nearest neighbors  the transformed feature space becomes more easily linearly separable. accordingly  it improves the performance of the amn training step. while the amn inference task can be solved using graph-cuts  the drawback of our approach is  that by storing instances the resulting classifier becomes a lazy classification method. the inference step requires to compute the distance between the instance to be classified and the known training instances. this is computational expensive in general. however  by utilizing efficient data structures like kd-trees the computational effort becomes logarithmic in the number of stored instances.
¡¡despite these encouraging results  there are several aspects that warrant future research. one way to improve the approach presented here could be to classify the objects on a lower complexity level  i.e.  by classifying object parts following the idea by huber et al. . as features of less complex objects are distributed more compactly in the feature space  they might be easier to separate.
acknowledgments
this work has partly been supported by the german research foundation under contract number sfb/tr1 and by the european union under contract number fp1-cosy.
