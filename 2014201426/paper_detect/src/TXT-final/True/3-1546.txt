
this paper introduces and illustrates blog  a formal language for defining probability models over worlds with unknown objects and identity uncertainty. blog unifies and extends several existing approaches. subject to certain acyclicity constraints  every blog model specifies a unique probability distribution over first-order model structures that can contain varying and unbounded numbers of objects. furthermore  complete inference algorithms exist for a large fragment of the language. we also introduce a probabilistic form of skolemization for handling evidence.
1 introduction
human beings and ai systems must convert sensory input into some understanding of what's out there and what's going on in the world. that is  they must make inferences about the objects and events that underlie their observations. no pre-specified list of objects is given; the agent must infer the existence of objects that were not known initially to exist.
﹛in many ai systems  this problem of unknown objects is engineered away or resolved in a preprocessing step. however  there are important applications where the problem is unavoidable. population estimation  for example  involves counting a population by sampling from it randomly and measuring how often the same object is resampled; this would be pointless if the set of objects were known in advance. record linkage  a task undertaken by an industry of more than 1 companies  involves matching entries across multiple databases. these companies exist because of uncertainty about the mapping from observations to underlying objects. finally  multi-target tracking systems perform data association  connecting  say  radar blips to hypothesized aircraft.
﹛probability models for such tasks are not new: bayesian models for data association have been used since the 1s  sittler  1 . the models are written in english and mathematical notation and converted by hand into specialpurpose code. in recent years  formal representation languages such as graphical models  pearl  1  have led to general inference algorithms  more sophisticated models  and automated model selection  structure learning . in sec. 1  we review several first-order probabilistic languages  fopls  that explicitly represent objects and the relations between them. however  most fopls only deal with fixed sets of objects  or deal with unknown objects in limited and ad hoc ways. this paper introduces blog  bayesian logic   a compact and intuitive language for defining probability distributions over outcomes with varying sets of objects.
﹛we begin in sec. 1 with three example problems  each of which involves possible worlds with varying object sets and identity uncertainty. we describe generative processes that produce such worlds  and give the corresponding blog models. sec. 1 observes that these possible worlds are naturally viewed as model structures of first-order logic. it then defines precisely the set of possible worlds corresponding to a blog model. the key idea is a generative process that constructs a
﹛world by adding objects whose existence and properties depend on those of objects already created. in such a process  the existence of objects may be governed by many random variables  not just a single population size variable. sec. 1 discusses how a blog model specifies a probability distribution over possible worlds.
﹛sec. 1 solves a previously unnoticed  probabilistic skolemization  problem: how to specify evidence about objects-such as radar blips-that one didn't know existed. finally  sec. 1 briefly discusses inference in unbounded outcome spaces  stating a sampling algorithm and a completeness theorem for a large class of blog models  and giving experimental results on one particular model.
1 examples
in this section we examine three typical scenarios with unknown objects-simplified versions of the population estimation  record linkage  and multitarget tracking problems mentioned above. in each case  we provide a short blog model that  when combined with a suitable inference engine  constitutes a working solution for the problem in question.
example 1. an urn contains an unknown number of balls- say  a number chosen from a poisson distribution. balls are equally likely to be blue or green. we draw some balls from the urn  observing the color of each and replacing it. we cannot tell two identically colored balls apart; furthermore  observed colors are wrong with probability 1. how many balls are in the urn  was the same ball drawn twice 
﹛the blog model for this problem  shown in fig. 1  describes a stochastic process for generating worlds. the first 1
1 type color; type ball; type draw;
1 random color truecolor ball ;
1 random ball balldrawn draw ; 1 random color obscolor draw ;
1 guaranteed color blue  green;
1 guaranteed draw draw1  draw1  draw1  draw1;
1 #ball ‵ poisson  ;
1 truecolor b  ‵ tabularcpd  1  1    ;
1 balldrawn d  ‵ uniform {ball b} ;
1 obscolor d 
1 if  balldrawn d  != null  then
1 ‵ tabularcpd  1  1    1  1   1  truecolor balldrawn d   ;
figure 1: blog model for the urn-and-balls scenario of ex. 1 with four draws.
lines introduce the types of objects in these worlds-colors  balls  and draws-and the functions that can be applied to these objects. lines 1 specify what objects may exist in each world. in every world  the colors are blue and green and there are four draws; these are the guaranteed objects. on the other hand  different worlds have different numbers of balls  so the number of balls that exist is chosen from a prior-a poisson with mean 1. each ball is then given a color  as specified on line 1. properties of the four draws are filled in by choosing a ball  line 1  and an observed color for that ball  lines 1 . the probability of the generated world is the product of the probabilities of all the choices made.
example 1. we have a collection of citations that refer to publications in a certain field. what publications and researchers exist  with what titles and names  who wrote which publication  and which publication does each citation refer to  for simplicity  we just consider the title and author-name strings in these citations  which are subject to errors of various kinds  and we assume only single-author publications.
﹛fig. 1 shows a blog model for this example  based on the model in  pasula et al.  1 . the blog model defines the following generative process. first  sample the total number of researchers from some distribution; then  for each researcher r  sample the number of publications by that researcher. sample the publications' titles and researchers' names from appropriate prior distributions. then  for each citation  sample the publication cited by choosing uniformly at random from the set of publications. sample the title and author strings used in each citation from string corruption models conditioned on the true attributes of the cited publication; finally  generate the full citation string according to a formatting model.
example 1. an unknown number of aircraft exist in some volume of airspace. an aircraft's state  position and velocity  at each time step depends on its state at the previous time step. we observe the area with radar: aircraft may appear as identical blips on a radar screen. each blip gives the approximate position of the aircraft that generated it. however  some blips may be false detections  and some aircraft may not be detected. what aircraft exist  and what are their trajectories  are there any aircraft that are not observed 
﹛the blog model for this scenario  fig. 1  describes the following process: first  sample the number of aircraft in the
1 guaranteed citation cite1  cite1  cite1  cite1;
1 #researcher ‵ numresearchersdistrib  ;
1 #publication:  author  -   r  ‵ numpubsdistrib  ;
1 title p  ‵ titleprior  ; 1 name r  ‵ nameprior  ;
1 pubcited c  ‵ uniform publication p ;
1 titlestring c  ‵ titleobs title pubcited c   ;
1 authorstring c  ‵ authorobs name author pubcited c    ; 1 citstring c  ‵ citdistrib titlestring c  authorstring c  ;
figure 1: blog model for ex. 1 with four observed citations  type and function declarations are omitted .
area. then  for each time step t  starting at t = 1   choose the state  position and velocity  of each aircraft given its state at time t   1. also  for each aircraft a and time step t  possibly generate a radar blip r with source r =a and time r =t. whether a blip is generated or not depends on the state of the aircraft-thus the number of objects in the world depends on certain objects' attributes. also  at each step  generate some false alarm blips r∩ with source r∩ =null. finally  sample the position for each blip given the true state of its source aircraft  or using a default distribution for a false-alarm blip .
1 syntax and semantics: possible worlds
1 outcomes as first-order model structures
the possible outcomes for ex. 1 through ex. 1 are structures containing many related objects  which we will treat formally as model structures of first-order logic. a model structure provides interpretations for the symbols of a first-order language.  usually  first-order languages are described as having predicate  function  and constant symbols. for conciseness  we view all symbols as function symbols; predicates are just functions that return a boolean value  and constants are just zero-ary functions. 
﹛for ex. 1  the language has function symbols such as truecolor b  for the true color of ball b; balldrawn d  for the ball drawn on draw d; and draw1 for the first draw.  statisticians might use indexed families of random variables such as {truecolorb}  but this is mainly a matter of taste. 
1 type aircraft; type blip;
1 random r1vector state aircraft  naturalnum ; 1 random r1vector apparentpos blip ;
1 nonrandom naturalnum pred naturalnum  = predecessor;
1 generating aircraft source blip ; 1 generating naturalnum time blip ;
1 #aircraft ‵ numaircraftdistrib  ;
1 state a  t 
1 if t = 1 then ‵ initstate  
1 else ‵ statetransition state a  pred t   ;
1 #blip:  source  time  -   a  t 
1 ‵ detectiondistrib state a  t  ;
1 #blip:  bliptime  -   t 
1 ‵ numfalsealarmsdistrib  ;
1 apparentpos r 
1 if  blipsource r  = null  then ‵ falsealarmdistrib  
1 else ‵ obsdistrib state source r   time r   ;
figure 1: blog model for ex. 1.
﹛to eliminate meaningless random variables  we use a typed  free logical language  in which every object has a type and in which functions may be partial. each function symbol f has a type signature  而1 ... 而k   where 而1 is the return type of f and 而1 ... 而k are the argument types. a partial function applied to arguments outside its domain returns the special value null  which is not of any type.
﹛the truth of any first-order sentence is determined by a model structure for the corresponding language. a model structure specifies the extension of each type and the interpretation for each function symbol:
definition 1. a model structure 肋 of a typed  free  first-order language consists of an extension  而 肋 for each type 而  which may be an arbitrary set  and an interpretation  f 肋 for each function symbol f. if f has return type 而1 and argument types 肋	肋	肋
而1 ... 而k  then  f  is a function from  而1  ℅ ﹞﹞﹞ ℅  而k  to 肋
 而1  ﹍ {null}.
﹛a blog model defines a probability distribution over a particular set of model structures. in ex. 1  identity uncertainty arises because  balldrawn 肋  draw1  might be equal to  balldrawn 肋  draw1  in one structure but not another. the set of balls   ball 肋  can also vary between structures.
1 outcomes with fixed object sets
blog models for fixed object sets have five kinds of statements. a type declaration  such as the two statements on line 1 of fig. 1  introduces a type. certain types  namely boolean  naturalnum  integer  string  real  and rkvector  for each k ≡ 1  are already provided. a random function declaration  such as line 1 of fig. 1  specifies the type signature for a function symbol whose values will be chosen randomly in the generative process. a nonrandom function definition  such as the one on line 1 of fig. 1  introduces a function whose interpretation is fixed in all possible worlds. in our implementation  the interpretation is given by a java class  predecessor in this example . a guaranteed object statement  such as line 1 in fig. 1  introduces and names some distinct objects that exist in all possible worlds. for the built-in types  the obvious sets of guaranteed objects and constant symbols are predefined. the set of guaranteed objects of type 而 in blog model m is denoted gm 而 . finally  for each random function symbol  a blog model includes a dependency statement specifying how values are chosen for that function. we postpone further discussion of dependency statements to sec. 1.
﹛the first four kinds of statements listed above define a particular typed first-order language lm for a model m. the set of possible worlds of m  denoted  m  consists of those model structures of lm where the extension of each type 而 is gm 而   and all nonrandom function symbols  including guaranteed constants  have their given interpretations.
﹛for each random function f and tuple of appropriately typed guaranteed objects o1 ... ok  we can define a random variable  rv  vf  o1 ... ok  肋     f 肋  o1 ... ok . the possible worlds are in one-to-one correspondence with full instantiations of these basic rvs. thus  a joint distribution for the basic rvs defines a distribution over possible worlds.
1 unknown objects
in general  a blog model defines a generative process in which objects are added iteratively to a world. to describe such processes  we first introduce generating function declarations  such as lines 1 of fig. 1. unlike other functions  generating functions such as source or time have their values set when an object is added. a generating function must take a single argument of some type 而  namely blip in the example ; it is then called a 而-generating function.
﹛generative steps that add objects to the world are described by number statements. for instance  line 1 of fig. 1 says that for each aircraft a and time step t  the process adds some number of blips r such that source r =a and time r =t. in general  the beginning of a number statement has the form: #而:  g1 ... gk  -   x1 ... xk  where 而 is a type  g1 ... gk are 而-generating functions  and x1 ... xk are logical variables.  for types that are generated ab initio with no generating functions  the    -     is omitted  as in fig. 1.  the inclusion of a number statement means that for each appropriately typed tuple of objects o1 ... ok  the appropriate types are the return types of g1 ... gk   the generative process adds some number  possibly zero  of objects q of type 而 such that  gi 肋  q  = oi for i = 1 ... k.
﹛object generation can even be recursive: for instance  in a model for phylogenetic trees  we could have a generating function parent that takes a species and returns a species; then we could model speciation events with a number statement that begins: #species:  parent  -   p .
we can also view number statements more declaratively:
definition 1. let 肋 be a model structure of lm  and consider a number statement for type 而 with generating functions g1 ... gk. an object q ﹋  而 肋 satisfies this number statement applied to o1 ... ok in 肋 if  gi 肋  q =oi for i=1 ... k  and  g 肋  q = null for all other 而-generating functions g.
﹛note that if a number statement for type 而 omits one of the 而-generating functions  then this function takes on the value null for all objects satisfying that number statement. for instance  source is null for objects satisfying the false-detection number statement on line 1 of fig. 1. also  a blog model cannot contain two number statements with the same set of generating functions. this ensures that each object o has exactly one generation history  which can be found by tracing back the generating functions on o.
﹛the set of possible worlds  m is the set of model structures that can be constructed by m's generative process. to complete the picture  we must explain not only how many objects are added on each step  but also what these objects are. it turns out to be convenient to define the generated objects as follows: when a number statement with type 而 and generating functions g1 ... gk is applied to generating objects o1 ... ok  the generated objects are tuples { 而  g1 o1  ...  gk ok  n  : n=1 ... n}  where n is the number of objects generated. thus in ex. 1  the aircraft are pairs  aircraft 1    aircraft 1   etc.  and the blips generated by aircraft are nested tuples such as  blip  source  aircraft 1    time 1  1 . the tuple encodes the object's generation history; of course  it is purely internal to the semantics and remains invisible to the user.
﹛the set of all objects  nested tuples  of type 而 that can be generated from the guaranteed objects via finitely many recursive applications of number statements is called the universe of 而  and denoted um  而 . in each possible world  the extension of 而 is some subset of um  而 .
definition 1. for a blog model m  the set of possible worlds  m is the set of model structures 肋 of lm such that: 1. for each type 而  gm 而     而 肋   um  而 ;
1. nonrandom functions have the specified interpretations;
1. for each number statement in m with type 而 and generating functions g1 ... gk  and each appropriately typed tuple of generating objects  o1 ... ok  in 肋  the set of objects in  而 肋 that satisfy this number statement applied to these generating objects is { 而  g1 o1  ...  gk ok  n  : n=1 ... n} for some natural number n;
1. for every type 而  each element of  而 肋 satisfies some number statement applied to some objects in 肋.
﹛note that by part 1 of this definition  the number of objects generated by any given application of a number statement in world 肋 is a finite number n. however  a world can still contain infinitely many non-guaranteed objects if some number statements are applied recursively  or are triggered for every natural number  like the ones generating radar blips in ex. 1 .
﹛with a fixed set of objects  it was easy to define a set of basic rvs such that a full instantiation of the basic rvs uniquely identified a possible world. to achieve the same effect with unknown objects  we need two kinds of basic rvs:
definition 1. for a blog model m  the set vm of basic random variables consists of:
  for each random function f with type signature  而1 ... 而k  and each tuple of objects  o1 ... ok  ﹋ um  而1  ℅ ﹞﹞﹞ ℅ um  而k   a function application rv vf  o1 ... ok  肋  that is equal to  f 肋  o1 ... ok  if o1 ... ok all exist in 肋  and null  or false for boolean rvs  otherwise;
  for each number statement with type 而 and generating functions g1 ... gk that have return types 而1 ... 而k  and each tuple of objects  o1 ... ok  ﹋ um  而1 ℅﹞﹞﹞℅ um  而k   a number rv n 而 g1 ... gk   o1 ... ok  肋  equal to the number of objects that satisfy this number statement applied to o1 ... ok in 肋.
﹛intuitively  each step in the generative world-construction process determines the value of a basic variable. the crucial result about basic rvs is the following:
proposition 1. for any blog model m and any complete instantiation of vm  there is at most one model structure in  m consistent with this instantiation.
﹛equating objects with tuples might seem unnecessarily complicated  but it becomes very helpful when we define a bayes net over the basic rvs  which we do in sec. 1 . for instance  the sole parent of vapparentpos   blip  source  aircraft 1    time 1  1   is vstate   aircraft 1  1 . it might seem more elegant to assign numbers to objects as they are generated  so that the extension of each type in each possible world would be simply a prefix of the natural numbers. specifically  we could number the aircraft arbitrarily  and then number the radar blips lexicographically by aircraft and time step. then we would have basic rvs such as vapparentpos   representing the apparent aircraft position for blip 1. but blip 1 could be generated by any aircraft at any time step. in fact  the parents of vapparentpos  would have to include all the #blip and state variables in the model. so  defining objects as tuples yields a much simpler bayes net.
1 syntax and semantics: probabilities
1 dependency statements
dependency and number statements specify exactly how the steps are carried out in our generative process. consider the dependency statement for state a t  beginning on line 1 of fig. 1. this statement is applied for every basic rv of the form vstate  a t  where a ﹋ um  aircraft  and t ﹋ n. if t=1  the conditional distribution for state a t  is given by the elementary cpd initstate; otherwise it is given by the elementary cpd statetransition  which takes state a pred t   as an argument. these elementary cpds define distributions over objects of type r1vector  the return type of state . in our implementation  elementary cpds are java classes with a method getprob that returns the probability of a particular value given a list of cpd arguments.
﹛a dependency statement begins with a function symbol f and a tuple of logical variables x1 ... xk representing the arguments to this function. in a number statement  the variables x1 ... xk represent the generating objects. in either case  the rest of the statement consists of a sequence of clauses. when the statement is not abbreviated  the syntax for the first clause is if cond then ‵ elem-cpd arg1  ...  argn . the cond portion is a formula of lm  in which only x1 ... xk can occur as free variables  specifying the condition under which this clause should be used to sample a value for a basic rv. more precisely  if the possible world constructed so far is 肋  then the applicable clause is the first one whose condition is satisfied in 肋  assuming for the moment that 肋 is complete enough to determine the truth values of the conditions . if no clause's condition is satisfied  or if the basic rv refers to objects that do not exist in 肋  then the value is set by default to false for boolean functions  null for other functions  and zero for number variables. if the condition in a clause is just  true   then the whole string  if true then  may be omitted.
﹛in the applicable clause  each cpd argument is evaluated in 肋. the resulting values are then passed to the elementary cpd. in the simplest case  the arguments are terms or formulas of lm  such as state a pred t  . an argument can also be a set expression of the form {而 y :  }  where 而 is a type  y is a logical variable  and   is a formula. the value of such an expression is the set of objects o ﹋  而 肋 such that 肋 satisfies   with y bound to o. if the formula   is just true it can be omitted: this is the case on line 1 of fig. 1.
﹛we require that the elementary cpds obey two rules related to non-guaranteed objects. first  a cpd should never assign positive probability to objects that do not exist in the partially completed world 肋. thus  we allow an elementary cpd to assign positive probability to a non-guaranteed object only if the object was passed in as a cpd argument. second  an elementary cpd cannot  peek  at the tuple representations of objects that are passed in: it must be invariant to permutations of the non-guaranteed objects.
1 declarative semantics
so far we have explained blog semantics procedurally  in terms of a generative process. to facilitate both knowledge engineering and the development of learning algorithms  we would like to have declarative semantics. the standard approach - which is used in most existing fopls - is to say that a blog model defines a certain bayesian network  bn  over the basic rvs. in this section we discuss how that approach needs to be modified for blog.
﹛we will write 考 to denote an instantiation of a set of rvs vars 考   and 考x to denote the value that 考 assigns to x. if a bn is finite  then the probability it assigns to each complete instantiation 考 is p 考  = qx﹋vars 考  px 考x|考pa x    where px is the cpd for x and 考pa x  is 考 restricted to the parents of x. in an infinite bn  we can write a similar expression for each finite instantiation 考 that is closed under the parent relation  that is  x ﹋ vars 考  implies pa x    vars 考  . if the bn is acyclic and each variable has finitely many ancestors  then these probability assignments define a unique distribution  kersting and de raedt  1 .
﹛the difficulty is that in the bn corresponding to a blog model  variables often have infinite parent sets. for instance  the bn for ex. 1 has an infinite number of basic rvs of the form vtruecolor  b : if it had only a finite number n of these rvs  it could not represent outcomes with more than n balls. furthermore  each of these vtruecolor  b  rvs is a parent of each vobscolor  d  rv  since if balldrawn d  happens to be b  then the observed color on draw d depends directly on the color of ball b. so the vobscolor  d  nodes have infinitely many parents. in such a model  assigning probabilities to finite instantiations that are closed under the parent relation does not define a unique distribution: in particular  it tells us nothing about the vobscolor  d  variables.
﹛we required instantiations to be closed under the parent relation so that the factors px 考x|考pa x   would be well-defined. but we may not need the values of all of x's parents in order to determine the conditional distribution for x. for instance  knowing vballdrawn  d  = ball 1  and vtruecolor   ball 1   =blue is sufficient to determine the distribution for vobscolor  d : the colors of all the other balls are irrelevant in this context. we can read off this context-specific independence from the dependency statement for obscolor in fig. 1 by noting that the instantiation
 vballdrawn  d  = ball 1  vtruecolor   ball 1   =blue 
determines the value of the sole cpd argument truecolor balldrawn d  . we say this instantiation supports the variable vobscolor  d   see  milch et al.  1  .
definition 1. an instantiation 考 supports a basic rv v of the form vf  o1 ... ok  or np  o1 ... ok  if all possible worlds consistent with 考 agree on  1  whether all the objects o1 ... ok exist  and  if so  on  1  the applicable clause in the dependency or number statement for v and the values for the cpd arguments in that clause.
﹛note that some rvs  such as nball  in ex. 1  are supported by the empty instantiation. we can now generalize the notion of being closed under the parent relation.
definition 1. a finite instantiation 考 is self-supporting if its instantiated variables can be numbered x1 ... xn such that for each n ≒ n  the restriction of 考 to {x1 ... xn 1} supports xn.
﹛this definition lets us give semantics to blog models in a way that is meaningful even when the corresponding bns contain infinite parent sets. we will write pv  v | 考  for the probability that v 's dependency or number statement assigns to the value v  given an instantiation 考 that supports v .
definition 1. a distribution p over  m satisfies a blog model m if for every finite  self-supporting instantiation 考 with vars 考    vm:

where  考 is the set of possible worlds consistent with 考 and x1 ... xn is a numbering of 考 as in def. 1.
﹛a blog model is well-defined if there is exactly one probability distribution that satisfies it. recall that a bn is welldefined if it is acyclic and each variable has a finite set of ancestors. another way of saying this is that each variable can be  reached  by enumerating its ancestors in a finite  topologically ordered list. the well-definedness criterion for blog is similar  but deals with finite  self-supporting instantiations rather than finite  topologically ordered lists of variables.
theorem 1. let m be a blog model. suppose that vm is at most countably infinite 1 and for each v ﹋ vm and 肋 ﹋  m  there is a self-supporting instantiation that agrees with 肋 and includes v . then m is well-defined.
﹛the theorem follows from a result in  milch et al.  1  that deals with distributions over full instantiations of a set of rvs. prop. 1 makes the connection between full instantiations of vm and possible worlds.
﹛to check that the criterion of thm. 1 holds for a particular example  we need to consider each basic rv. in ex. 1  the number rv for balls is supported by the empty instantiation  so in every world it is part of a self-supporting instantiation of size one. each truecolor b  rv depends only on whether its argument exists  so these variables participate in self-supporting instantiations of size two. similarly  each balldrawn variable depends only on what balls exist. to sample an obscolor d  variable  we need to know balldrawn d  and truecolor balldrawn d    so these variables are in selfsupporting instantiations of size four. similar arguments can be made for examples 1 and 1. of course  we would like to have an algorithm for checking whether a blog model is well-defined; the criteria given in thm. 1 in sec. 1 are a first step in this direction.
1 evidence and queries
because a well-defined blog model m defines a distribution over model structures  we can use arbitrary sentences of lm as evidence and queries. but sometimes such sentences are not enough. in ex. 1  the user observes radar blips  which are not referred to by any terms in the language. the user could assert evidence about the blips using existential quantifiers  but then how could he make a query of the form   did this blip come from the same aircraft as that blip  
﹛a natural solution is to allow the user to extend the language when evidence arrives  adding constant symbols to refer to observed objects. in many cases  the user observes some new objects  introduces some new symbols  and assigns the symbols to the objects in an uninformative order. to handle such cases  blog includes a special macro. for instance  given 1 radar blips at time 1  one can assert:
{blip r: time r  = 1} = {blip1  blip1  blip1  blip1};
this introduces the new constants blip1 ... blip1 and asserts that there are exactly 1 radar blips at time 1.
﹛formally  the macro augments the model with dependency statements for the new symbols. the statements implement sampling without replacement; for our example  we have
blip1 ‵ uniform {blip r :  time r  = 1 } ;
﹛blip1 ‵ uniform {blip r :  time r  = 1  &  blip1 != r } ; and so on. once the model has been extended this way  the user can make assertions about the apparent positions of blip1  blip1  etc.  and then use these symbols in queries.
﹛these new constants resemble skolem constants  but conditioning on assertions about the new constants is not the same as conditioning on an existential sentence. for example  suppose you go into a new wine shop  pick up a bottle at random  and observe that it costs $1. this scenario is correctly modeled by introducing a new constant bottle1 with a uniform cpd. then observing that bottle1 costs over $1 suggests that this is a fancy wine shop. on the other hand  the mere existence of a $1+ bottle does not suggest this  because almost every shop has some bottle at over $1.
1 inference
because the set of basic rvs of a blog model can be infinite  it is not obvious that inference for well-defined blog models is even decidable. however  the generative process intuition suggests a rejection sampling algorithm. we present this algorithm not because it is particularly efficient  but because it demonstrates the decidability of inference for a large class of blog models  see thm. 1 below  and illustrates several issues that any blog inference algorithm must deal with. at the end of this section  we present experimental results from a somewhat more efficient likelihood weighting algorithm.
1 rejection sampling
suppose we are given a partial instantiation e as evidence  and a query variable q. our rejection sampling algorithm starts by imposing an arbitrary numbering on the the basic rvs. to generate each sample  it starts with an empty instantiation 考. then it repeats the following process: scan the basic rvs in the imposed order until we reach the first rv v that is supported by 考 but not already instantiated in 考; sample a value v for v according to v 's dependency statement; and augment 考 with the assignment v =v. it continues until all the query and evidence variables have been sampled. if the sample is consistent with the evidence e  then it increments a counter nq  where q is the sampled value of q. otherwise  it rejects this sample. after n accepted samples  the estimate of p q=q | e  is nq/n.
﹛this algorithm requires a subroutine that determines whether a partial instantiation 考 supports a basic rv v   and if so  returns a sample from v 's conditional distribution. for a basic rv v of the form vf  o1 ... ok  or np  o1 ... ok   the subroutine begins by checking the values of the relevant number variables in 考 to determine whether all of o1 ... ok exist. if some of these number variables are not instantiated  then 考 does not support v . if some of o1 ... ok do not exist  the subroutine returns the default value for v . if they do all exist  the subroutine follows the semantics for dependency statements discussed in sec. 1. first  it iterates over the clauses in the dependency  or number  statement until it reaches a clause whose condition is either undetermined or determined to be true given 考  if all the conditions are determined to be false  then it returns the default value for v  . if the condition is undetermined  then 考 does not support v . if it is determined to be true  then the subroutine evaluates each of the cpd arguments in this clause. if 考 determines the values of all the arguments  then the subroutine samples a value for v by passing those values to the sample method of this clause's elementary cpd. otherwise  考 does not support v .
﹛to evaluate terms and quantifier-free formulas  we use a straightforward recursive algorithm. the base case looks up the value of a particular function application rv in 考 and returns  undetermined  if it is not instantiated. a formula may be determined even if some of its subformulas are not determined: for example  汐 ＿ 汕 is determined to be false if 汐 is false. it is more complicated to evaluate set expressions such as {blip r: time r  = 1}  which can be used as cpd arguments. a naive algorithm for evaluating this expression would first enumerate all the objects of type blip  which requires certain number variables to be instantiated   then select the blips r that satisfy time r =1. but fig. 1 specifies that there may exist some blips for each aircraft a and each natural number t: since there are infinitely many natural numbers  some worlds contain infinitely many blips. fortunately  the number of blips r with time r =1 is necessarily finite: in every world there are a finite number of aircraft  and each one generates a finite number of blips at time 1. we have an algorithm that scans the formula within a set expression for generating function restrictions such as time r =1  and uses them to avoid enumerating infinite sets when possible. a similar method is used for evaluating quantified formulas.
1 termination criteria
in order to generate each sample  the algorithm above repeatedly instantiates the first variable that is supported but not yet instantiated  until it instantiates all the query and evidence variables. when can we be sure that this will take a finite amount of time  the first way this process could fail to terminate is if it goes into an infinite loop while checking whether a particular variable is supported. this happens if the program ends up enumerating an infinite set while evaluating a set expression or quantified formula. we can avoid this by ensuring that all such expressions in the blog model are finite once generating function restrictions are taken into account.
﹛the sample generator also fails to terminate if it never constructs an instantiation that supports a particular query or evidence variable. to see how this can happen  consider calling the subroutine described above to sample a variable v . if v is not supported  the subroutine will realize this when it encounters a variable u that is relevant but not instantiated. now consider a graph over basic variables where we draw an edge from u to v when the evaluation process for v hits u in this way. if a variable is never supported  then it must be part of a cycle in this graph  or part of a receding chain of variables v1 ↘ v1 ↘ ﹞﹞﹞ that is extended infinitely.
﹛the graph constructed in this way varies from sample to sample: for instance  sometimes the evaluation process for vobscolor  d  will hit vtruecolor   ball 1    and sometimes it will hit vtruecolor   ball 1  . however  we can rule out cycles and infinite receding chains in all these graphs by considering a more abstract graph over function symbols and types.
definition 1. the symbol graph for a blog model m is a directed graph whose nodes are the types and random function symbols of m  where the parents of a type 而 or function symbol f are:
  the random function symbols that occur on the right hand side of the dependency statement for f or some number statement for 而;
  the types of variables that are quantified over in formulas or set expressions on the right hand side of such a statement;
  the types of the arguments for f or the return types of
generating functions for 而.
﹛if the sampling subroutine for a basic rv v hits a basic rv u  then there must be an edge from u's function symbol  or type  if u is a number rv  to v 's function symbol  or type  in the symbol graph. this property  along with ideas from  milch et al.  1   allows us to prove the following:
theorem 1. suppose m is a blog model where:
1. uncountable built-in types do not serve as function arguments or as the return types of generating functions;
1. each quantified formula and set expression ranges over afinite set once generating function restrictions are taken into account;
1. the symbol graph is acyclic.
then m is well-defined. also  for any evidence instantiation e and query variable q  the rejection sampling algorithm described in sec. 1 converges to the posterior p q|e  defined by the model  taking finite time per sampling step.
﹛the criteria in thm. 1 are very conservative: in particular  when we construct the symbol graph  we ignore all structure in the dependency statements and just check for the occurrence of function and type symbols. these criteria are satisfied by the models in figures 1 and 1. however  the model in fig. 1 does not satisfy the criteria because there is a self-loop from state to state. the criteria do not exploit the fact that state a t  depends only on state a pred t    and the nonrandom function pred is acyclic. friedman et al.  have already dealt with this issue in the context of probabilistic relational models; their algorithm can be adapted to obtain a stronger version of thm. 1 that covers fig. 1.
1 experimental results
milch et al.  describe a guided likelihood weighting algorithm that uses backward chaining from the query and evidence nodes to avoid sampling irrelevant variables. this algorithm can also be adapted to blog models. we applied this algorithm for ex. 1  asserting that 1 balls were drawn and all appeared blue  and querying the number of balls in the urn. the top graph of fig. 1 shows that when the prior for the number of balls is uniform over {1 ... 1}  the posterior puts more weight on small numbers of balls; this makes sense because the more balls there are in the urn  the less likely it is that they are all blue. the bottom graph  using a poisson 1  prior  shows a similar but less pronounced effect. note that the posterior probabilities computed by the likelihood weighting algorithm are very close to the exact values  computed by exhaustive enumeration . these results could not be obtained using any algorithm that constructed a single fixed bn  since the number of potentially relevant vtruecolor  b  variables is infinite in the poisson case.

figure 1: distribution for the number of balls in the urn  ex. 1 . dashed lines are the uniform prior  top  or poisson prior  bottom ; solid lines are the exact posterior given that 1 balls were drawn and all appeared blue; and plus signs are posterior probabilities computed by 1 independent runs of 1 samples  top  or 1 samples  bottom .
1 related work
gaifman  was the first to suggest defining a probability distribution over first-order model structures. halpern  defines a language in which one can make statements about such distributions: for instance  that the probability of the set of worlds that satisfy flies tweety  is 1. probabilistic logic programming  ng and subrahmanian  1  can be seen as an application of this approach to horn-clause knowledge bases. such an approach only defines constraints on distributions  rather than defining a unique distribution.
﹛most first-order probabilistic languages  fopls  that define unique distributions fix the set of objects and the interpretations of  non-boolean  function symbols. examples include relational bayesian networks  jaeger  1  and markov logic models  domingos and richardson  1 . prologbased languages such as probabilistic horn abduction  poole  1   prism  sato and kameya  1   and bayesian logic programs  kersting and de raedt  1  work with herbrand models  where the objects are in one-to-one correspondence with the ground terms of the language.
﹛there are a few fopls that allow explicit reference uncertainty  i.e.  uncertainty about the interpretations of function symbols. among these are two languages that use indexed rvs rather than logical notation: bugs  gilks et al.  1  and indexed probability diagrams  ipds   mjolsness  1 . reference uncertainty can also be represented in probabilistic relational models  prms   koller and pfeffer  1   where a  single-valued complex slot  corresponds to an uncertain unary function. prms are unfortunately restricted to unary functions  attributes  and binary predicates  relations . probabilistic entity-relationship models  heckerman et al.  1  lift this restriction  but represent reference uncertainty using relations  such as drawn d b   and special mutual exclusivity constraints  rather than with functions such as balldrawn d . multi-entity bayesian network logic  mebn   laskey  1  is similar to blog in allowing uncertainty about the values of functions with any number of arguments.
﹛the need to handle unknown objects has been appreciated since the early days of fopl research: charniak and goldman's plan recognition networks  prns   can contain unbounded numbers of objects representing hypothesized plans. however  external rules are used to decide what objects and variables to include in a prn. while each possible prn defines a distribution on its own  charniak and goldman do not claim that the various prns are all approximations to some single distribution over outcomes.
﹛some more recent fopls do define a single distribution over outcomes with varying objects. ipds allow uncertainty over the index range for an indexed family of rvs. prms and their extensions allow a variety of forms of uncertainty about the number  or existence  of objects satisfying certain relational constraints  koller and pfeffer  1; getoor et al.  1  or belonging to each type  pasula et al.  1 . however  there is no unified syntax or semantics for dealing with unknown objects in prms. mebns take yet another approach: an mebn model includes a set of unique identifiers  for each of which there is an  identity  rv indicating whether the named object exists.
﹛our approach to unknown objects in blog can be seen as unifying the prm and mebn approaches. number statements neatly generalize the various ways of handling unknown objects in prms: number uncertainty  koller and pfeffer  1  corresponds to a number statement with a single generating function; existence uncertainty  getoor et al.  1  can be modeled with two or more generating functions
 and a cpd whose support is {1} ; and domain uncertainty  pasula et al.  1  corresponds to a number statement with no generating functions. there is also a correspondence between blog and mebn logic: the tuple representations in a blog model can be thought of as unique identifiers in an mebn model. the difference is that blog determines which objects actually exist in a world using number variables rather than individual existence variables.
﹛finally  it is informative to compare blog with the ibal language  pfeffer  1   in which a program defines a distribution over outputs that can be arbitrary nested data structures. an ibal program could implement a blog-like generative process with the outputs viewed as logical model structures. but the declarative semantics of such a program would be less clear than the corresponding blog model.
1 conclusion
blog is a representation language for probabilistic models with unknown objects. it contributes to the solution of a very general problem in ai: intelligent systems must represent and reason about objects  but those objects may not be known a priori and may not be directly and uniquely identified by perceptual processes. our approach defines generative models that create first-order model structures by adding objects and setting function values; everything else follows naturally from this design decision. much remains to be done  especially on inference: we expect to employ mcmc with userdefined and possibly adaptive proposal distributions  and to develop algorithms that work directly with objects rather than at the lower level of basic rvs.
