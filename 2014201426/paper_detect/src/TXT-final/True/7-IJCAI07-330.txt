
many current state-of-the-art planners rely on forward heuristic search. the success of such search typically depends on heuristic distance-to-the-goal estimates derived from the plangraph. such estimates are effective in guiding search for many domains  but there remain many other domains where current heuristics are inadequate to guide forward search effectively. in some of these domains  it is possible to learn reactive policies from example plans that solve many problems. however  due to the inductive nature of these learning techniques  the policies are often faulty  and fail to achieve high success rates. in this work  we consider how to effectively integrate imperfect learned policies with imperfect heuristics in order to improve over each alone. we propose a simple approach that uses the policy to augment the states expanded during each search step. in particular  during each search node expansion  we add not only its neighbors  but all the nodes along the trajectory followed by the policy from the node until some horizon. empirical results show that our proposed approach benefits both of the leveraged automated techniques  learning and heuristic search  outperforming the state-of-the-art in most benchmark planning domains.
introduction
heuristic search has been the most successful and dominant approach for suboptimal ai planning  hoffmann & nebel 1; alfonso gerevini & serina 1; vidal 1 . the success of this approach is largely due to the developmentof intelligent automated heuristic calculation techniques based on relaxed plans  rps   plans that ignore the action effect delete lists. rp-based heuristics provide an effective gradient for search in many ai planning domains. however  there are some domains where this gradient provides inadequate guidance  and heuristic search planners fail to scale up in such domains.
모for some of those domains  e.g. blocksworld  there are automated techniques  martin & geffner 1; fern  yoon  & givan 1  that can find good policies through machine learning  enabling planners to scale up well. however  induced policies lack deductive guarantees and are in practice prone to be faulty-even more so when resource constraints

copyright all rights reserved.
limit the size of the training data  as occurs in planning competitions. nevertheless such policies still capture useful  though imperfect  constraints on good courses of action. the main goal of this paper is to develop and evaluate an approach for combining such imperfect policies and heuristics in order to improve over the performance of either alone.
모there are techniques that can improve on imperfect policies. policy rollout  bertsekas & tsitsiklis 1  and limited discrepancy search  lds   harvey & ginsberg 1  are representative of such techniques. policy rollout uses online simulation to determine for each encountered state  as it is encountered  which action performs best if we take it and then follow the learned base policy to some horizon. policy rollout performs very poorly if the base policy is too flawed to find any reward  as all actions look equally attractive. this occurs frequently in goal-based domains  where policy rollout cannot improve on a zero-success-ratio policy at all.
모discrepancy search determines a variable search horizon by counting the number of discrepancies from the base policy along the searched path  so that paths that agree with the policy are searched more deeply. the search cost is exponential in the number of discrepancies tolerated  and as a result the search is prohibitively expensive unless the base policy makes mostly acceptable/effective choices.
모due to limitations on the quantity of training data and the lack of any guarantee of an appropriate hypothesis space  machine learning might produce very low quality policies. such policies are often impossible to improve effectively with either policy rollout or lds. here  we suggest using such learned policies during node expansions in heuristic search. specifically  we propose adding not only the neighbors of the node being expanded  but also all nodes that occur along the trajectory given by the learned policy from the current node. until the policy makes a bad decision  the nodes added to the search are useful nodes.
모in contrast to discrepancy search  this approach leverages the heuristic function heavily. but in contrast to ordinary heuristic search  this approach can ignore the heuristic for long trajectories suggested by the learned policy. this can help the planner critically in escaping severe local minima and large plateaus in the heuristic function. policy evalua-
tion is typically cheaper than heuristic function calculation and node expansion  so even where the heuristic is working well  this approach can be faster.
모we tested our proposed technique over all of the strips/adl domains of international planning competitions  ipc  1 and 1  except for those domains where the automated heuristic calculation is so effective as to produce essentially the real distance. we used some competition problems for learning policies and then used the rest of the problems for testing our approach. note that this approach is domain-independent. empirical results show that our approach performed better than using policies alone and in most domains performed better than state-of-the-art planners.
planning
a deterministic planning domain d defines a set of possible actions a and a set of states s in terms of a set of predicate symbols p  action types y   and objects c. each symbol  in p or y has a defined number of arguments it expects  denoted by arity  . a state s is a set of state facts  where a state fact is an application of a predicate symbol p to arity p  objects from c. an action a is an application of an action type y to arity y  objects from c.
모each action a 뫍 a is associated with three sets of state facts  pre a   add a   and del a  representing the precondition  add  and delete effects respectively. as usual  an action a is applicable to a state s iff pre a    s  and the application of an  applicable  action a to s  results in the new state a s  =  s   del a   뫋 add a .
모given a planning domain  a planningproblem is a tuple  s a g   where a   a is a set of actions  s 뫍 s is the initial state  and g is a set of state facts representing the goal. a solution plan for a planning problem is a sequence of actions  a1 ... al  from a  where the sequential application of the sequence starting in state s leads to a goal state s where
.
learning policies
a reactive policy 뷇 for a planning domain d is a function that maps each state s to an action a that is applicable to the state s. we desire a policy 뷇 for which iterative application of 뷇 to the initial state s of a problem p in d will find a goal  so that g   뷉뷇 뷉뷇  ...뷉뷇 s     writing 뷉뷇 s  for the next state under 뷇  i.e.   뷇 s   s . good reactive decisionlist policies have been represented and learned for many ai planning domains  khardon 1; martin & geffner 1; yoon  fern  & givan 1; 1 .
taxonomic decision list policies
we represent a reactive policy as an ordered list of rules  rivest 1   each of which specifies constraints on the arguments of an action:
dl = {rule1 ... rulen}
rule
here  m is arity y  for action type y and each cij is a concept expression specifying a set of objects  as described below. a rule can fire on any tuple of objects o1 ... om such that each oi is in set specified by the corresponding cij- upon firing the rule suggests action y o1 ... om . the earliest rule in the list that can be fired in the current state will be fired. if more than one action is suggested by that rule  the tie is broken lexicographically based on an arbitrary unchanging ordering of the domain objects.
the syntax for the concepts is the following.
	c	=	any-object | c 뫌 c |  c |
 p c1 ...ci 1   ci+1 ...carity p  
here  p is a predicate symbol  from p or as specified below. the predicate symbol p is applied to smaller class expressions  but with one argument omitted  indicated by a  * . such applications denote the class of objects that make the predicate true when filled in for  *   given that the other arguments of p  if any  are provided to match the class expressions given. the semantics of the other constructs as classes of objects is as expected-please refer to  yoon  fern  & givan 1; 1; mcallester & givan 1  for the detailed specification of the syntax and semantics. note that this syntax automatically derives from predicate symbols of the target planning domain.
모the evaluation of the semantics of each class c is relative to the relational database d constructed from the current state  the goal information and other information that is deduced from the current state and goal information. in our case this includes the relaxed plan from the current state to a goal state  a form of reasoning to construct features  geffner 1  and the reflexive transitive closure p  for each binary predicate p. predicate symbols other than those in p are allowed in order to represent goal information and features of the relaxed plan  as described in detail in  yoon  fern  & givan 1; 1 .
모as an example decision list policy  consider a simple blocksworld problem where the goal is clearing off a block a. the following decision-list policy is an optimal policy. {putdown x1  : x1 뫍 holdingunstack x1  : x1 뫍 clear. the first rule says  putdown any block that is being held  and the second rule says  unstack any block that is above the block a and clear .
learning reactive policies
for our evaluation  we learn reactive policies from solved small problems. for this purpose we deploy a learner similar to that presented in  yoon  fern  & givan 1 . given a set of training problems along with solutions we create a training set to learn a classifier that will be taken to be the learned policy. the classifier training set contains states labeled by positive and negative actions. the positive actions are those that are contained in the solution plan for each state  and all other actions in a state are taken to be negative. the learning algorithm conducts a beam search through the candidate class expressions  greedily seeking constraints on each argument of each action type to match the positive actions and not match the negative actions. note that the training set is noisy in the sense that not all actions labeled as negative are actually bad. that is  there can be many optimal/good actions from a single state  though only one is included in the training solutions. this noise is one reason that learned policies can be imperfect.
using non-optimal policies
typically learned policies will not be perfect due to the bias of the policy language and variance of the learning procedure. in some domains  the imperfections are not catastrophic and the policies still obtain high success rates. in other domains  the flaws lead to extremely poor success rates. nevertheless  in many states  even learned policies with poor success rates suggest good actions and we would like methods that can exploit this information effectively. first  we describe two existing techniques  rollout and discrepancy search that utilize search to improve upon an imperfect policy. our experiments will show that these techniques do not work well in many planning domains  typically where the quality of the learned policy is low. these failures led us to propose a new approach to improving policies with search  which is discussed at the end of this section.
policy rollout

	qa1 = length1	qai = lengthi	qan = lengthn
	+ heuristic s1 	+ heuristic si 	+ heuristic sn 
figure 1: policy rollout. at a state s  for each action a  rollout the input policy 뷇 from state a s  until some horizon. for deterministic domains  the length of the rollout trajectory plus the heuristic at the end can be used as the q-value for that action. select the best action.
모policy rollout  bertsekas & tsitsiklis 1  is a technique for improving the performance of a non-optimal  base  policy using simulation. policy rollout sequentially selects the action that is the best according to the one-step lookahead policy evaluations of the base policy. figure 1 shows the one-step lookahead action selection. for each action  this procedure simulates the action from the current state and then simulates the execution of the base policy from the resulting state forsome horizon or until the goal is found. each action is assigned a cost equal to the length of the trajectory following it plus the value of a heuristic applied at the final state  which is zero for goal states . for stochastic domains the rollout should be tried several times and the average of the rollout trials is used as a q-value estimate. policy rollout then executes the action that achieved the smallest cost from the current state and repeats the entire process from the resulting state. while policy rollout can improve a policy that is mostly optimal  it can perform poorly when the policy commits many errors  leading to inaccurate action costs. multi-level policy rollout  e.g. as used in  xiang yan & van roy 1   is one way to improve over the above procedure by recursively applying rollout  which takes time exponential in the number of recursive applications. this approach can work well when the policy errors are typically restricted to the initial steps of trajectories. however  for policies learned in planning domains the distribution of errors does not typically have this form; thus  even multi-step rollout is often ineffective at improving weak policies.
discrepancy search
a discrepancy in a search is a search step that is not selected by the input policy or heuristic function. limited discrepancy search  lds   harvey & ginsberg 1  bounds the search depth to some given number of discrepancies. discrepancy search limited to n disrepancies will consider every search path that deviates from the policy at most n times.

figure 1: an example of discrepancy search. thin edges in the figure represent discrepancies. nodes are labeled with their discrepancy depth.
모figure 1 shows an example of discrepancy search. the thick lines are choices favored by the heuristic function and the thin lines are discrepancies. each node is shown as a rectangle labeled by the number of discrepancies needed to reach that node. consider the depth of the goal in the search tree and the number of discrepancies needed to reach the goal node from the root node. in figure 1  the goal node is at depth three and discrepancy depth one from the root node. plain dfs or bfs search will search more nodes than limit one disrepancy search. dfs and bfs need to visit 1 nodes beforereaching the goal  while discrepancy search with limit one will find the goal after a 1-node search. greedy heuristic search on this tree needs to backtrack many times before it reaches the goal node. the search space of lds has size exponential in the discrepancy bound  and so  like policy rollout  lds also is only effective with policies or heuristic functions with high quality. heuristics can be incorporated by applying the heuristic at the leaves of the discrepancy search tree and then selecting the action that lead to the best heuristic value.
incorporating policies into heuristic search
in this work  we consider an alternative approach for incorporating imperfect policies into search. we attempt to take advantage of both approaches  automated heuristic calculation and automated policy learning. the main idea is to use policies during node expansions in best-first heuristic search  as described by the node expansion function shown in figure 1. at each node expansion of the best-first search  we add to the search queue the successors of the current best node as usual  but also add the states encountered by following the policy from the current best node for some horizon. our approach is similar to marvin  coles & smith 1   macroff  botea et al. 1   and yahsp  vidal 1 . however  unlike these approaches  we do not just add the final state encountered by the policy or macro  but rather add all states along the trajectory.
모embedding the policy into node expansion during heuristic search yields two primary advantages over pure heuristic search. first  when the input policy correlates well with the heuristic values  the embedding can reduce the search time. typically  the direct policy calculation is much faster than greedy heuristic search because greedy heuristic search needs to compute the heuristic value of every neighbor  while direct policy execution considers only the current state in selecting actions. second  like blocksworld  where heuristic calculation frequently underestimates the true distance  our node expansion can lead the heuristic search out of local minima.

figure 1: node expansion for a heuristic search with a policy. add nodes that occur along the trajectory of the input policy as well as the neighbors
experiments
we evaluated the above approaches on the strips domains from the recent international planning competitions ipc1 and ipc1  and on blocksworld. we show the performance of each planning system in each row of the following figures. to test the effectiveness of our approach  we tested our base system  len  as well as our proposed technique  ph. the len system uses best-first search with the relaxedplan-length  rpl  heuristic. we also compare against the planner ff  and  for each ipc1 domain  against the best planner in the competition on the particular domain. finally  we compare against policy rollout  pr  and limited discrepancy search  d  techniques. for each system and domain  we report the number of solved problems  the average solution time for solved problems and the average length of the solved problems in separate columns of the results.
모we used the first 1 problems as training data in each domain and the remaining problems for testing. we considered a problem to be unsolved if it was not solved within 1 minutes. for pr  d and ph systems  we used a horizon of 1. all the experiments were run on linux machines with a 1 xeon processor and 1gb of ram.
blocksworld
figure 1 shows the results on blocksworld from track 1 of ipc 1. in this domain  len  ph  d solved all the problems but ff and pr failed to solve some problems. we used 1 cpu secs in learning the policy. the learned policy  뷇blocksworld solved 1 problems. thus  all of the policy improvement approaches  pr  d  and ph improved the input policy. the ph system solved all of the problems and improved the solution time over len  pr and d  showing the benefit of our approach for speeding-up planning  though ph produced slightly longer plans than ff and d.
blocksworld  ipc1 systemssolved  1 뫺time 뫻length 뫻ff1.1.1len1.1.1pr1.1.1d1.1ph1.1figure 1: blocksworld results
ipc1
figure 1 summarizes the results on the ipc1 domains. the domains are significantly more difficult to learn policies for and we used 1h  1h and 1h of cpu time for learning the policies in depots  driverlog  and freecell respectively. although the learning times are substantial  this is a one-time cost: such learned policies can be resused for any problem instance from the domain involved. in these domains  the learned policies cannot solve any of the tested problems by themselves.
모we see that all of pr  d and ph are able to improve on the input policies. ph solved more problems overall than ff  though ff has a slight advantage in solution length in the depots domain. the freecell domain has deadlock states and our policy performs poorly  probably by leading too often to deadlock states. still the experiments show that our approach attained better solution times  demonstrating again that using policies in search can reduce the search time by quickly finding low-heuristic states.
모ph outperforms len decisively in depots and driverlog  showing a clear benefit from using imperfect policies in heuristic search for these domains.
ipc1
figure 1 shows the results for the ipc1 domains. we used 1  1  1  1 and 1 hours of cpu time for learning poli-
ipc1domainsystemssolved 1  뫺time 뫻length 뫻ff1.1.1depotslen1.1.1pr1.1d1.1.1ph1.1.1ff11driverloglen11pr1--d1--ph1.1.1ff1.1.1freecelllen1.1.1pr1.1.1d1.1.1ph1.1.1figure 1: ipc1 results
cies for pipesworld  pipesworld-tankage  psr  philosopher and optical telegraph respectively. here we evaluated one more system  b  which in each domain denotes the best performer from that domain from ipc1. the numbers for b were downloaded from the ipc web site and cannot be directly compared to our systems results. still  the numbers give some idea of the performance comparison. note that the learned policies alone can solve all the problems from the philosopher and optical telegraph domains.
모ph generally outperforms ff in solved problems  rendering solution length measures incomparable  except in psr where the two approaches essentially tie.
모for pipesworld and pipesworld-with-tankage domains  the best performers of those domains in the competition performed much better than our system ph. however  ph still performed better than len  ff  pr and d  showing the benefits of our approach. marvin  macroff and yahsp participated the competition. each of these macro action based planners solved 1 or so problems. the best planners for each domain combined solved 1 problems. ph solved 1 problems  showing a clear advantage for our technique.
모overall  the results show that typically our system is able to effectively combine the learned imperfect policies with the relaxed-plan heuristic in order to improve over each alone. this is a novel result. neither policy rollout nor discrepancy search are effective in this regard and we are not aware of any prior work that has successfully integrated imperfect policies into heuristic search planners.
heuristic value trace in search
in order to get a view of how incorporating policies can improve the heuristic search  we plotted a trace of the heuristic value for each node expansion during the search for problem 1 of depots. figure 1 shows a long plateau of heuristic values by the len system  while figure 1 shows large jumps in heuristic value for the ph system  using far fewer node expansions  indicating that the policies are effectively helping to escape plateaus in the search space.
ipc1domainsystemssolved  1  뫺time 뫻length 뫻ff1.1.1len1.1.1pipesworldb1.1.1pr1.1.1d1.1.1ph1.1.1ff1.1.1len1.1.1pipesworld
tankageb1.1.1pr1.1.1d1.1.1ph1.1.1ff1.1.1psrlen1.1.1 middle complied b1.1.1pr1.1.1d1.1.1ph1.1.1ff1--len1--philosophersb1.1.1pr1.1.1d1.1.1ph1.1.1ff1--len1--optical telegraphb1.1.1pr1.1.1d1.1.1ph1.1.1figure 1: ipc1 results
모in figures 1 and 1  we show the heuristic traces for len and ph on a freecell problem where ph failed. here ph is unable to escape from the plateau  making only small jumps in heuristic value over many node expansions. interestingly  the trace of len system shows a plateau at a higher heuristic value than that for ph followed by a rapid decrease in heuristic value upon which the problem is solved. we are currently investigating the reasons for this behavior. at this point  we speculate that the learned policy manages to lead the planner away from a useful exit.
conclusion
we embedded learned policies in heuristic search by following the policy during node expansion to generate a trajectory of new child nodes. our empirical study indicates advantages to this technique  which we conjecture to have three sources. when the policy correlates well with the heuristic function  the embedding can speed up the search. secondly  the embedding can help escape local minima during the search. finally  the heuristic search can repair faulty action choices in the input policy. our approach is easy to implement and effective  and to our knowledge represents the first demonstration of effective incorporation of imperfect policies into heuristic search planning.
