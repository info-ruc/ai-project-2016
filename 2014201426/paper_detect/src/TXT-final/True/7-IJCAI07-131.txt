
this paper employs state similarity to improvereinforcement learning performance. this is achieved by first identifying states with similar sub-policies. then  a tree is constructed to be used for locating common action sequences of states as derived from possible optimal policies. such sequences are utilized for defining a similarity function between states  which is essential for reflecting updates on the action-value function of a state onto all similar states. as a result  the experience acquired during learning can be applied to a broader context. effectiveness of the method is demonstrated empirically.
1 introduction
reinforcementlearning  rl  is the problemfaced by an agent that must learn behavior through trial-and-error interactions with a dynamicenvironment by gainingperceptsand rewards from the world and taking actions to affect it. in most of the realistic and complex domains  the task that an agent tries to solve is composed of various subtasks and has a hierarchical structure formed by the relations between them  barto and mahadevan  1 . each of these subtasks repeats many times at different regions of the state space. although all instances of the same subtask  or similar subtasks  have almost identical solutions  without any  self  guidance an agent has to learn these solutions independently by going through similar learning stages again and again. this situation affects the learning process in a negative way  making it difficult to converge to optimal behavior in reasonable time.
¡¡the main reason of this problem is the lack of connections  that would allow to share solutions  between similar subtasks scattered throughout the state space. one possible way to build connections is to use temporally abstract actions  taas   which generalize primitive actions and last for a period of time  sutton et al.  1; dietterich  1; barto and mahadevan  1 . taas can be included in the problem definition  which requires extensive domain knowledge and becomes more difficult as the complexity of the problem increases  or be constructed automat-

¡¡¡¡  this work was supported by the scientific and technological research council of turkey under grant no. 1 hd-1 .ically either by finding sub-goal states and generating corresponding options that solve them  stolle and precup  1; menache et al.  1; mannor et al.  1; simsek et al.  1  or by identifying frequently occurring patterns over the state  action and reward trajectories  mcgovern  1; girgin et al.  1 . since different instances of the same or similar subtasks would probably have different subgoals in terms of state representation  with subgoal based methods they will be discovered and treated separately. although sequence based methods are capable of combining different instances of the same subtask  multiple abstractions may be generated for each of them.
¡¡taa based methods try to solve the solution sharing problem inductively. based on the fact that states with similar patterns of behavior constitute the regions of state space corresponding to different instances of similar subtasks  the notion of state equivalence can be used as a more direct mechanism of solution sharing. by reflecting experience acquired on one state to all similar states  connections between similar subtasks can be established implicitly. in fact  this reduces the repetition in learning  and consequently improves the performance. state equivalence is closely related with model minimization in markov decision processes  mdps   and various definitions exist.  givan et al.  1  define equivalence of states based upon stochastic bisimilarity  such that two states are said to be equivalent if they are both action sequence and optimal value equivalent. based on the notion of mdp homomorphism   ravindran and barto  1; 1  extended equivalence over state-action pairs  which as a result allows reductions and relations not possible in case of bisimilarity. furthermore  they applied state- action  equivalence to the options framework to derive more compact options without redundancies  called relativized options. while relativized options are defined by the user  instances are generated automatically.  zinkevich and balch  1  also addressed how symmetries in mdps can be used to accelerate learning by employing equivalence relations on the stateaction pairs; in particular for the multi-agent case  where permutations of features corresponding to various agents are prominent  but without explicitly formalizing them.
¡¡in this paper  following homomorphism notion  we propose a method to identify states with similar sub-policies without requiring a model of the mdp or equivalence relations  and show how they can be integrated into rl framework to improve the learning performance. using the collected history of states  actions and rewards  traces of policy fragments are generated and then translated into a tree form to efficiently identify states with similar sub-policy behavior based on the number of common action-reward sequences. updates on the action-value function of a state are then reflected to all similar states  expanding the influence of new experiences. the proposed method can be treated as a metaheuristic to guide any underlying rl algorithm. we demonstrate the effectiveness of this approach by reporting test results on two domains  namely various versions of the taxi cab and the room doorway problems. further  the proposed method is compared with other rl algorithms  and a substantial level of improvement is observed on different test cases. also  we present how the performance of out work compares with hierarchical rl algorithms  although the approaches are different. the tests demonstrate the applicability and effectiveness of the proposed approach.
¡¡the rest of this paper is organized as follows: in section 1  we briefly describe the standard rl framework of discrete time  finite mdps  define state equivalence based on mdp homomorphisms  and show how they can be used to improve the learning process. our approach to how similar states can be identified during the learning process is presented in section 1. experimental results are reported and discussed in section 1. section 1 is conclusions.
1 background and motivation
in this section  we briefly overview the background necessary to understand the material introduced in this paper. we start by defining mdp and related rl problem.
¡¡an mdp is a tuple   where s is a finite set of states  a is a finite set of actions  ¦·   s¡Áa is the set of admissible state-action pairs  t : ¦· ¡Á s ¡ú  1  is a state transition function such that  and is a reward function. r s a  is the immediate expected reward received when action a is executed in state s.1 a  stationary  policy  ¦Ð : ¦· ¡ú  1   is a mapping that defines the probability of selecting an action in a particular state. the value of a state s under policy ¦Ð  v ¦Ð s   is the expected infinite discounted sum of rewards that the agent will gain if it starts in state s and follows ¦Ð. in particular  if the agent takes action a at state s and then follows ¦Ð  the resulting sum is called the value of the state-action pair  s a  and denoted q¦Ð s a . the objective of an agent is to find an optimal policy  ¦Ð   which maximizes the state value function for all states. based on the experience in the form of sample sequences of states  actions  and rewards collected from on-line or simulated trial-and-error interactions with the environment  rl methods try to find a solution to an mdp by approximating optimal value functions. detailed discussions of various approaches can be found in  kaelbling et al.  1; sutton and barto  1; barto and mahadevan  1 .
¡¡rl problems  in general  inherently contain subtasks that need to be solved by the agent. these subtasks can be represented by simpler mdps that locally preserve the state transition and reward dynamics of the original mdp. let m =

	 a 	 b 
figure 1:  a  markov decision process m  and  b  its homomorphic image m. directed edges indicate state transitions. each edge label denotes the action causing the transition between connected states and the associated expected reward.

be two mdps  such that s¦´ is an absorbing state with a single admissible action a¦´ which  once executed  transitions back to s¦´ with certainty. a surjection
is a partial mdp homomorphism  pmdph  from  if for all states that do not map to s¦´  state-action pairs that have the same image under h also have the same block transition behavior in m and the same expected reward1. under  is called the partial homomorphic image of m1. two state-action pairs  s1 a1  and  s1 a1  in m are said to be equivalent if h s1 a1  = h s1 a1 ; states s1 and s1 are equivalent if there exists a bijection ¦Ñ : as1 ¡ú as1 such that  a ¡Ê as1  h s1 a  = h s1 ¦Ñ a  . the set of state-action pairs equivalent to  s a   and the set of states equivalent to s are called the equivalence classes of  s a  and s  respectively.  ravindran and barto  1  proved that if h above is a complete homomorphism  then an optimal policy ¦Ð  for m can be constructed from an optimal policy for. this makes it possible to solve an mdp by solving one of its homomorphic images which can be structurally simpler and easier to solve. however  in general such a construction is not viable in case of pmdphs  since the optimal policies would depend on the rewards associated with absorbing states.
¡¡for example  consider the deterministic mdp m given in fig. 1 a . if the discount factor  ¦Ã  is set as 1  then optimal policy at s1 is to select action given in fig. 1 b  is a partial homomorphic image of m1 and one can easily determine that q  s1 a1  = 1 and q  s1 a1  = 1   c  where c is the expected reward for executing action a¦´ at the absorbing state. accordingly  the optimal policy for is to select a1 if c   1  and a1 otherwise  which is different than the optimal policy for m. this example demonstrates that unless reward functions are chosen carefully  it may not be possible to solve a given mdp by employing the solutions of its partial homomorphicimages. however  if equivalence classes of state-action pairs and states are known  they can be used to speed up the learning process. let  s a  and  be two equivalent state-action pairs in a given mdp m based on the partial homomorphic imageunder the surjection h.
suppose that while learning the optimal policy  the agent selects action a at state s which takes it to state t with an immediate reward r  such that t is mapped to a non-absorbing state under h. let ¦¸ be the set of states in m which are accessible from s by taking action a  i.e. probability of transition is non-zero  and are mapped to the same state in under h. by definition  for any  the probability of experiencing a transition from upon taking action a is equal to that of from s to t upon taking action a. also  since  s a  and  are equivalent  we have  i.e.  both state-action pairs have the same expected reward  and since t is mapped to a non-absorbing state  independent of the reward assigned to the absorbing state  they have the same policy. therefore  for any state can be regarded as a virtual experience tuple and can be updated similar to q s a  based on observation o  i.e.  pretending as if action a transitioned the agent from state s to state t with an immediate reward r.
¡¡the method described above assumes that equivalence classes of states and corresponding pmdphs are already known. if such information is not available prior to learning  then  for a restricted class of pmdphs  it is still possible to identify states that are similar to each other with respect to state transition and reward behavior based on the history of events  as we show in the next section. experience gathered on one state  then  can be reflected to similar states to improve the performance of learning.
1 finding similar states
for the rest of the paper  we will restrict our attention to direct pmdphs in which  except the absorbing state and its associated action  the action sets of an mdp and its homomorphic image are the same and there is an identity mapping on the action component of state-action tuples1. let be a given mdp. starting from state s  a sequence of states  actions and rewards ¦Ò = s1 a1 r1 ... rn 1 sn  such that s1 = s and each ai  1 ¡Ü i ¡Ü n   1  is chosen by following a policy ¦Ð is called a ¦Ð-history of s with length n. ar¦Ò =
a1a1 ...an 1rn 1 is the action-reward sequence of ¦Ò  and the restriction of ¦Ò to ¦²   s  denoted by ¦Ò¦²  is the longest prefix s1 a1 r1 ... ri 1 si of ¦Ò  such that for all j = 1..i sj ¡Ê ¦² and si+1 ¡Ê/ ¦². if two states of m are equivalent under a direct pmdph h  then the set of images of ¦Ð-histories restricted to the states that map to non-absorbing states under h  and consequently  the list of associated actionreward sequences are the same 1. this property of equivalent states leads to a natural approach to calculate the similar-

1
 gs a  = a  for every s that maps to a non-absorbing state. 1
¡¡¡¡let  s a  andbe two state-action pairs that are equivalent to each other based on partial homomorphic image 
 un-
der direct pmdph h. consider a ¦Ð-history of state s of length n  and let ¦²h   s be the inverse image of s under h  and ¦Ò¦²h = s1 a1 r1 ... rk 1 sk k ¡Ü n be the restriction of ¦Ò on ¦²h. the image of ¦Ò¦²h under h  denoted by f ¦Ò¦²h   is obtained by mapping each state si to its counterpart in s  i.e.  f ¦Ò¦²h  = h s1  a1 r1 h s1  a1 ... rk 1 h sk . by definition  f ¦Ò¦²h  is a ¦Ð-history of  and sinceare equivalent  there
ity between any two states based on the number of common action-reward sequences.
given any two states  be the set of
¦Ð-histories of state s with length i  and calculated as

be the ratio of the number of common action-reward sequences of ¦Ð-histories ofwith length up to i to the number of action-reward sequences of ¦Ð-histories of s with length up to will be close to 1  if s is similar to s in terms of state transition and reward behavior; and close to 1  in case they differ considerably from each other. note that  even for equivalent states  the action-reward sequences will eventually deviate and follow different courses as the subtask that they are part of ends. as a result  for i larger than some threshold value   i would inevitably decrease and no longer be a permissible measure of the state similarity. on the contrary  for very small values of i  such as 1 or 1   i may over estimate the amount of similarity since number of common action-reward sequences can be high for short action-reward sequences. also  since optimal value of i depends on the subtasks of the problem  to increase robustness  it is necessary to take into account action-reward sequences of various lengths. therefore  the maximum value or weighted average of over a range of problem specific i values  kmin and kmax  can be used to combine the results of evaluations and approximately measure the degree of similarity between states s and s  denoted by. once is calculated  s is regarded as equivalent to is over a given threshold value ¦Ósimilarity. likewise  by restricting the set of ¦Ð-histories to those that start with a given action a in the calculation of  similarity between state-action pairs  s a  and  can be measured approximately.
¡¡if the set of ¦Ð-histories for all states are available in advance  then equivalent states can be identified prior to learning. however  in general  the dynamics of the system is not known in advance and consequently such information is not accessible. therefore  using the history of observed events  i.e. sequence of states  actions taken and rewards received   the agent must incrementally store the ¦Ð-histories of length up to kmax during learning and enumerate common actionreward sequences of states in order to calculate similarities between them. for this purpose  we propose an auxiliary structure called path tree  which stores the prefixes of actionreward sequences of ¦Ð-histories for a given set of states. a path tree is a labeled rooted tree  where n is the set of nodes  such that each node represents a unique actionreward sequence; and  is an edge from u to v with label  indicating that action-reward sequence v is obtained by appending a r to u  i.e.  v = uar. the root node represents the empty action sequence. furthermore  each node u holds a list oftuples  stating that state s has one or more ¦Ð-histories starting with

exists a ¦Ð-history such that the image of its restriction to ¦²h under h is equal to f ¦Ò¦²h   i.e. ; and furthermore.
algorithm 1 q-learning with equivalent state update.

1: initialize q arbitrarily  e.g.  q ¡¤ ¡¤  = 1  1: let t be the path tree initially containing root node only.
1: repeat
1:	let s be the current state
1:	repeat	 for each step
1: choose and execute a from s using policy derived from q with sufficient exploration. observe r and the next state s; append
1:	 t at 	 s a 
1:	let	s.
1:	until s is a terminal state
1: using the observation history  generate the set of ¦Ð-histories of length up to kmax and add them t.
1: traverse t and update eligibility values of the tuples in each node; prune tuples with eligibility value less than ¦Îthreshold.
 and determine similar states 
  s s     ¦Ósimilarity.
1: clear observation history. 1: until a termination condition holds

action sequence ¦Òu. ¦Î is the eligibility value of ¦Òu for state s  representing its occurrence frequency. it is incremented every time a new ¦Ð-history for state s starting with action sequence ¦Òu is added to the path tree  and gradually decremented otherwise. a ¦Ð-history h = s1r1 ...rk 1sk can be added to a path tree by starting from the root node  following edges according to their label. let n  denote the active node of the path tree  which is initially the root node. for i = 1..k 1  if there is a node n such that n  is connected to n by an edge with label
  then either ¦Î of the tupleis incremented or a new tuple is added to n if it does not exist  and n  is set to n. otherwise  a new node containing tuple is created  and n  is connected to this node by an edge with label . the new node becomes the active node.
¡¡after each episode  or between two termination conditions such as reaching reward peaks in case of non-episodic tasks  using the consecutive fragments of the observed sequence of states  actions and rewards  a set of ¦Ð-histories of length up to kmax are generated and added to the path tree using the procedure described above. then  the eligibility values of tuples in the nodes of the tree are decremented by a factor of 1   ¦Îdecay   1  called eligibility decay rate  and tuples with eligibility value less than a given threshold  ¦Îthreshold  are removed to keep the tree of manageable size and focus the search on recent and frequently used action-reward sequences. using the generated path tree  can be calculated incrementally for all  by traversing it in breadth-first order and keeping two arrays. ¦Ê s  denotes the number of nodes containing s  and denotes the number of nodes containing both and in their tuple lists. initially  and ¦Ê s  are set to 1.
at each level of the tree  the tuple lists of nodes at that level are processed  and ¦Ê s  and are incremented accordingly. after processing level i  kmin ¡Ü i ¡Ü kmax  for every pair that co-exist in the tuples list of a node at that level  is compared with  and updated if the latter one is greater. note that  since eligibility values stored in the nodes of the path tree is a measure of occurrence frequencies of corresponding sequences  it is possible to extend the similarity function by incorporating eligibility values in the calculations as normalization factors. this would improve the quality of the metric and also result in better discrimination in domains with high degree of non-determinism  since the likelihood of the trajectories will also be taken into consideration. in order to simplify the discussion  we opted to omit this extension. once   values are calculated  equivalent states  i.e.  state pairs with   greater than ¦Ósimilarity  can be identified and incorporated into the learning process. the proposed method applied to q-learning algorithm is presented in alg. 1.
1 experiments
we applied the similar state update method described in section 1 to q-learning and compared its performance with different rl algorithms on two test domains: a six-room maze1and various versions of the taxi problem1  fig. 1 a  . also  its behavior is examined under various parameter settings  such as maximum length of ¦Ð-histories and eligibility decay rate. in all test cases  the initial q-values are set to 1  and  -greedy action selection mechanism  where action with maximum qvalue is selected with probability and a random action is selected otherwise  is used with. the results are averaged over 1 runs. unless stated otherwise  the path tree is updated using an eligibility decay rate of ¦Îdecay = 1 and an eligibility threshold of ¦Îthreshold = 1  and in similarity calculations the following parameters are employed:
kmin = 1  kmax = 1  and ¦Ósimilarity = 1.

	 d 	 e 	 f 
	 g 	 h 	 i 
figure 1:  a  six-room maze and 1 ¡Á 1 taxi problems   b  results for the six-room maze problem; results for the  c  1 ¡Á 1 and  d  1 ¡Á 1 taxi problems;  e  experience replay with the same number of state updates; effect of ¦Îdecay and kmin max on 1 ¡Á 1 taxi problem:  f h  reward obtained  and  h i  average size of the path tree.¡¡fig. 1 b  shows the total reward obtained by the agent until goal position is reached in six-room maze problem when equivalent state update is applied to q-learning and sarsa ¦Ë  algorithms. based on initial testing  we used a learning rate and discount factor of ¦Á = 1  and ¦Ã = 1  respectively. for the sarsa ¦Ë  algorithm  ¦Ë is taken as 1. state similarities are calculated after each episode. as expected  due to backward reflection of received rewards  sarsa ¦Ë  converges much faster compared to q-learning. the learning curve of q-learning with equivalent state update indicates that  starting from early states of learning  the proposed method can effectively utilize state similarities and improve the performance considerably. since convergence is attained in less than 1 episodes  the result obtained using sarsa ¦Ë  with equivalent state update is almost indistinguishable from that of q-learning with equivalent state update.
¡¡the results of the corresponding experiments in 1 ¡Á 1 taxi problem showing the total reward obtained by the agent is presented in fig. 1 c . the initial position of the taxi agent  location of the passenger and its destination are selected randomly with uniform probability. ¦Á is set to 1  and ¦Ë is taken as 1 in sarsa ¦Ë  algorithm. state similarities are computed every 1 episodes starting from the 1th episode in order to let the agent gain experience for the initial path tree. similar to the six-room maze problem  sarsa ¦Ë  learns faster than regular q-learning. in smdp q-learning  bradtke and duff  1   in addition to primitive actions  the agent can select and execute hand-coded options  which move the agent from any position to one of predefined locations using minimum numberof steps. althoughsmdp q-learning has a very steep learning curve in the initial stages  by utilizing abstractions and symmetries more effectively q-learning with equivalent state update performs better in the long run. the learning curves of algorithms with equivalent state update reveals that the proposedmethod is successful in identifyingsimilar states which leads to an early improvement in performance  which allows the agent to learn the task more efficiently. the results for the larger 1 ¡Á 1 taxi problem presented in fig. 1 d  demonstrate that the improvement becomes more evident as the complexity of the problem increases.
¡¡the proposed idea of using state similarities and then updating similar states leads to more state-action value  i.e.  qvalue  updates per experience. it is known that remembering past experiences and reprocessing them as if the agent repeatedly experienced what it has experienced before  which is called experience replay  lin  1   speed up the learning process by accelerating the propagation of rewards. experience replay also results in more updates per experience. in order to test whether the gain of the equivalent state update method in terms of learning speed is simply due to the fact that more q-value updates are made or not  we compared its performance to experience replay using the same number of updates. the results obtained by applying both methods to regular q-learning algorithm on the 1 ¡Á 1 taxi problem are presented in fig. 1 e . even though the performance of learning improves with experience replay  it falls short of q-learning with equivalent state update. this indicates that in addition to number of updates  how they are determined is also important. by reflecting updates in a semantically rich manner based on the similarity of action-reward patterns  rather than neutrally as in experience replay  the proposed method turns out to be more efficient. furthermore  in order to apply experience replay  the state transition and reward formulation of a problem should not change over time as past experiences may no longer be relevant otherwise. the dynamic nature of the sequencetree allows the proposed method to handle such situations as well.
¡¡in order to analyze how various parameter choices of the proposed method affect the learning behavior  we conducted a set of experiments under different settings. the results for various ¦Îdecay values are presented in fig. 1 f g . as the eligibility decay decreases  the number of ¦Ð-histories represented in the path tree also decrease  and recent ¦Ð-histories dominate over existing ones. consequently  the less number of equivalent states can be identified and the performance of the method also converges to that of regular qlearning. fig. 1 h i  shows how the length of ¦Ð-histories affect the performance in the taxi domain. different kmax values show almost indistinguishable behavior  although the path tree shrinks considerably as kmax decreases. despite the fact that minimum and maximum ¦Ð-history lengths are inherently problem specific  in most applications  kmax near kmin is expected to perform well as restrictions of pmdphs to smaller state sets will also be pmdphs themselves.
1 conclusions
in this paper  we proposed and analyzed the interesting and useful characteristics of a tree-based approach that  during learning  identifies states similar to each other with respect to action-reward patterns  and based on this information reflects state-action value updates of one state to multiple states. experimentsconductedon two well-knowndomains highlighted the applicability and effectiveness of utilizing such a relation between states in the learning process. the reported test results demonstrate that experience transfer performed by the algorithm is an attractive approach to make learning systems more efficient. however  when the action set is very large  the parameters  such as eligibility decay  must be chosen carefully in order to reduce the computational cost and keep the size of the tree manageable. also  instead of matching actionreward sequences exactly  they can be matched partially and grouped together based on their reward values  ex.   neighborhood . this is especially important in cases where the immediate reward has a distribution. we are currently working on adaptation of the method to continuous domains  which also covers these issues. furthermore  since the similarity between two states is a value which varies between 1 and 1  rather than thresholding to determine equivalent states and reflecting the whole experience  it is possible to make use of the degree of similarity  for example  by updating q-values in proportion to their similarity. future work will examine possible ways of extending and improving the proposed method to consider such information.
