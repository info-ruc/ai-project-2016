
we present a new approach to ensemble classification that requires learning only a single base classifier. the idea is to learn a classifier that simultaneously predicts pairs of test labels-as opposed to learning multiple predictors for single test labels- then coordinating the assignment of individual labels by propagating beliefs on a graph over the data. we argue that the approach is statistically well motivated  even for independent identically distributed  iid  data. in fact  we present experimental results that show improvements in classification accuracy over single-example classifiers  across a range of iid data sets and over a set of base classifiers. like boosting  the technique increases representational capacity while controlling variance through a principled form of classifier combination.
1 introduction
supervised learning has been by far the most studied task in machine learning research. the problem is to take a finite set of observed training examples  x1 y1  ...  xn yn  and produce a classifier f : x 뫸 y that achieves small misclassification error on subsequent test examples. most research has tended to adopt a standard  iid  assumption that the training and test examples are independent and identically distributed. in fact  this assumption is fundamental to much of the theoretical research on the topic  anthony and bartlett  1  and also characterizes most standard learning methods-as exemplified by the fact that most machine learning methods classify each test pattern in isolation  independently of other test patterns.
모recently  however  increasing attention has been paid to problems where the training and test labels are not independent  but instead strongly related. for example  in domains such as part of speech tagging and webpage classification  each word-tag or webpage-label depends on the tag or label of proximal words or webpages  in addition to just the features of the immediate word or webpage. various forms of  relational  learning models have been developed to handle these kinds of problems over the last few years. a notable example is work on probabilistic relational models  prms   where the correlation between the class labels of different instances is explicitly represented in a directed graphical model  getoor et al.  1; 1 . other approaches for learning multivariate classifiers include conditional random fields  crfs   lafferty et al.  1   relational markov networks  rmns   taskar et al.  1   and maximum margin markov networks  m1n   taskar et al.  1 . all of these methods have led to substantial progress on learning classifiers that make dependent predictions of test labels that are explicitly related.
모although learning multivariate predictors is an exciting problem  we nevertheless focus on the classical iid case in this paper. however  we demonstrate what we believe is a surprising and counterintuitive connection: that learning multivariate dependent predictions is a beneficial idea even in the iid setting. in particular  we develop a relational learning strategy that classifies test patterns by connecting their labels in a graphical model-hence correlating the subsequent predictions-even when it is explicitly assumed that all training and test examples are iid.
모before explaining the rationale behind our approach and explaining why dependent prediction still makes sense in the iid setting  we note that standard relational learning approaches  such as prms  crfs  rmns and m1ns  do not naturally correlate predictions on iid data. that is  these techniques only consider label dependencies that are explicitly asserted to hold in the true underlying model of the domain. in the iid case  since no dependencies are asserted between test labels  these standard relational approaches reduce to singlelabel learning techniques  such as univariate logistic regression and support vector machines. however  what we are proposing is different: we intentionally add dependencies between test labels  even when all labels are explicitly assumed to be independent in the underlying data generation process. surprisingly  we demonstrate that correlating predictions can still be advantageous.
모after introducing our basic approach below  we then motivate and justify our technique in three separate ways. first  we show that predicting correlated test labels is statistically justified in the iid setting  even when the independence assumptions are explicitly taken into account. in fact  we show that it is incorrect to conclude that a learned predictor can sufficiently treat test cases as independent simply because they come from an iid source. second  we show that our proposed relational learning technique can be viewed as a natural generalization of similarity-based learning techniques.
moreover  it can also be viewed as a simple form of ensemble learning method that has some advantages over standard approaches. third  we show empirically that our proposed method can achieve improvements in classification accuracy across a range of iid domains  for different base learning algorithms.
1 learning coordinated label predictors
we begin by simply introducing our learning method  and then attempt to motivate it more thoroughly below. initially  we will focus on using probabilistic classifiers  although we briefly consider an extension to nonprobabilistic classifiers in section 1 below .
모in the iid setting  the standard approach to probabilistic classification is to learn a univariate model p y|x 붿  that asserts a conditional probability distribution over a single classification variable y given an input pattern x  where 붿 represents the parameters of the model. given such a representation  there are two key steps to building a univariate classifier: the first is to learn a specific predictive model p y|x 붿  given training data  x1 y1  ...  xn yn   based on using a principle such as maximum  conditional  likelihood or maximum a posteriori estimation. then  given a set of test patterns x1  ... xm    one classifies each xi  independently by computing the label y i that maximizes the estimated conditional probability  y i = argmaxy p y|xi  붿 . natural examples of this approach are learning naive bayes classifiers  friedman et al.  1   logistic regression classifiers  hastie et al.  1   kernel logistic regression classifiers  zhu and hastie  1   sigmoid network classifiers  neal  1   and bayesian network classifiers  greiner and zhou  1 .
모our approach is different. instead of learning a univariate classifier that predicts only a single label  we instead propose to learn a pairwise label classifier p yiyj|xixj 뷋  that takes an arbitrary pair of input patterns  xi and xj  and asserts a conditional joint distribution over the pair of labels yi and yj. for example  if there are two classes  say 1 and 1  then a pairwise classifier would assert the conditional probability of the four possible pair labelings  yi yj  뫍 { 1   1   1   1 } given the two input patterns xi and xj. in general the pairwise predictor does not assume that yi and yj are independent given xi and xj  even though they are indeed assumed to be independent in the true model  we present a justification of this in section 1 below . we refer to a pairwise label classifier of this form as a  coordination classifier  to highlight the fact that it attempts to model any coordination that might appear between the labels yi and yj given the input patterns xi and xj. given the alternative representation p yiyj|xixj 뷋  we next describe the two main processes of  first  training a coordination classifier from data  and then using it to label test patterns.
1 training a coordination classifier
a coordination classifier doubles the number of input features and squares the number of output classes from an original univariate classifier. despite the increase in model complexity  training a coordination classifier remains conceptually straightforward. assume a standard training sample  x1 y1  ...  xn yn  is given. first we construct a set of pairs from the set { xixj yiyj } and then supply these as a conventional training set for learning a predictive model p yiyj|xixj 뷋  from data.  in our experiments below we ignore duplicate pairs but otherwise include both orderings of each distinct pair to ensure that the learned model is symmetric.  once the training data is constructed  the parameters of the model  뷋  can then be estimated in the same way as the univariate case  either by using a maximum  conditional  likelihood or maximum a posteriori estimation principle. for example  given a linear logistic representation for p yi|xi 붿   we use an analogous linear logistic representation for p yiyj|xixj 뷋  but over the joint feature space xixj; thus training the two models under the same estimation principle  but using different  although related  data sets.
모a coordination model learned in this way will generally not make independent predictions of yi and yj  since the extended parameters 뷋 are not constrained to enforce independence.1 that is  we expect the model to learn to make dependent  coordinated predictions of the two labels from the corresponding input patterns. interestingly  learning a coordination classifier has the advantage of potentially squaring the number of available training examples  even though this advantage is mitigated by subsampling and the increase in the complexity of the model being learned.
1 classifying test data with coordination
 given a coordination classifier  we require a principle for classifying individual test patterns x . in fact  the problem of classifying test patterns becomes much more involved in this case. the approach we take is to consider the set of training examples  x1 y1  ...  xn yn  and test patterns xas a whole. that is  rather than classify each test pattern xi  in isolation  we instead seek to classify test patterns in a dependent manner. to perform classification  we proceed in three stages reminiscent of conditional random fields: first  we construct a graph over the test and training labels. once the graph has been constructed  we then use the learned coordination classifier p yiyj|xixj 뷋  to assign  potentials  to the possible labelings of each edge  yi yj . these potentials can then be used to define a markov random field over test label assignments  thereby establishing a joint probability distribution over the labelings. finally  we compute a joint labeling of the test examples that maximizes  or at least approximately maximizes  the joint label probability. we describe each of these three steps in more detail below.
모defining the graph first  to construct the graph  we only consider edges that connect a pair of test labels  yi  yj    or a test label and a training label  yi  yj . that is  after training we do not make any further use of edges between training labels.
모to classify test patterns  the simplest approach  conceptually  is to consider the complete graph that connects each test label yi  to all other test and training labels. however  we have found that it is usually impractical to consider all m  m   1 /1 + n  test pairs  ignoring duplicate pairs . therefore  we reduce the number of edges by adding a few restrictions. the natural alternatives we consider below are:  i  connecting each test label only to training labels  which  as we will see  is analogous to standard similarity and kernel based learning methods    ii  connecting each test label only to other test labels  which  surprisingly  gives the best results in our experiments below   and  iii  connecting each test label to both training and test labels. to further reduce the overall number of edges  we then uniformly subsample edges  subject to these different restrictions.
모defining the potentials once a graph has been constructed  we then assign potentials to the configurations of each edge. there are two cases depending on whether the edge connects two test labels  or a test label and a training label.
모for an edge that connects only two test labels   yi  yj    we simply assign the potential 뷍 yi  yj   = p yi yj |xi xj  뷋  given by the learned coordination classifier.
모for an edge that connects a test and a training label   yi  yj   we assign a unit potential to the singleton node  yi   given by the conditional probability of yi  given yj. that is  we assign

once a potential has been assigned to the singleton  yi   we then remove the edge  yi  yj  from the graph. thus  the resulting graph only has edges between test labels  and possibly a combination of singleton potentials on nodes  yi   and pairwise potentials on edges  yi  yj  .
모once all of the potentials have been assigned  we then define a joint probability distribution over node labelings in the same manner as a markov random field  by taking the product form

and normalizing by an appropriate constant z.
모computing the labeling finally  given a joint probability distribution defined by the markov random field  our goal is to compute the joint test pattern labeling that has maximum probability.  since we are only interested in computing the maximum probability assignment  we can ignore the normalization constant above.  depending on which edge model we use  there are different implications.
모first  assuming model  i   test labels only connect to training labels   there are no pairwise potentials and the markov random field becomes completely factored. in this case  computing the maximum probability assignment is easy and can be determined independently for each test pattern. essentially  removing test-test edges reduces our technique to a classical method in which each test pattern is classified independently. here the learned coordination model

figure 1: in an iid setting  the true test labels y1 and y1 are independent given the true conditional model. however  they are not independent given a learned estimate of the model.
p yi yj|xi xj 뷋  plays the role of a generalized similarity measure for classifying yi  in terms of  x1 y1  ...  xn yn . the only difference is that the coordination model is learned in an earlier training phase  rather than being fixed beforehand.
모the remaining cases  ii  and  iii  are more difficult because they introduce edges between test labels  which causes the labels to become dependent. surprisingly  we have found that exploiting test label dependence can actually improve classification accuracy  even when the test data is known to be iid.  this is one of the main points of the paper.  for these models  computing the maximum probability assignment is hard  because the graph can contain many loops. to cope with the problem of performing probabilistic inference in a complex graphical model  we use loopy belief propagation to efficiently compute an approximate solution  murphy et al.  1 . below we find this gives adequate results.
1 rationale and discussion
before presenting our experimental results  it is important to explain the rationale behind our technique and suggest why coordinated classification even makes sense in an iid setting.
모given the assumption that the training and test data are independent  we are proposing to predict test labels by building a graph  asserting joint potentials over pairs of labels  from a learned coordination classifier   and using belief propagation to make dependent predictions. why does it make sense to make dependent predictions of iid labels  it turns out that this approach is justified even when taking the independence assumptions into account. figure 1 illustrates the basic argument. in a standard machine learning setting  it is indeed true that  given the correct model for generating iid data  the label y1 for an input pattern x1 is independent of the label y1 for another pattern x1. however  note that this requires knowledge of the correct model  or at least its correct structure   which is rarely the case in classification learning. instead  given only an estimate of the true model obtained from training data  y1 and y1 remain dependent  as figure 1 clearly shows. that is  in the context of supervised learning it is generally the case that test labels are dependent given a learned model. in fact  it is obvious that supervised learning algorithms correlate the labels on the training data. our observation is simply that the same principle can also be applied to test data.
모although using a relational technique for an iid problem might still appear awkward  it has a well known precedent in machine learning research: transductive learning  vapnik  1; zhu et al.  1 . in transduction  the learner knows the set of test patterns beforehand  and exploits this knowledge to make predictions that are ultimately dependent. in fact  this idea has been exploited in recent approaches to semisupervised learning using markov random fields  zhu et al.  1 . what we are proposing is a general framework for extending standard probabilistic learning algorithms to be transductive in a similar fashion.
모our method can be further motivated by noting that it is a natural extension of standard ideas in supervised iid classification. as observed  learning a coordination classifier p yiyj|xixj 뷋  is a natural generalization of learning methods that use a similarity measure k xi xj  to classify test examples xi  based on similarities k xi  x1  ... k xi  xn  to the training patterns. in fact  this corresponds to our graph choice  i  above  which only connects test labels to training labels. coordination classification extends the standard similarity based approach by first learning how patterns predict dependencies between the labels  using standard methods applied in a novel way   and then correlating test predictions over a graph. although there has indeed been recent work on learning kernels for classification  lanckriet et al.  1   as well as transductive learning with kernels  xu et al.  1   thus far these formulations have remained hard to extend and apply in practice.
모another interesting view of coordination classification is that it is a novel form of ensemble method. that is  the label for a test pattern xi  is computed by a combination of votes from multiple predictors associated with different test  and training  patterns. in fact  even remotely connected patterns can influence a classification via belief propagation.
모as an ensemble method  coordination classification has some useful features. first  it only requires the training of a single base classifier p yiyj|xixj 뷋   rather than multiple base classifiers trained from perturbed data. second  as with boosting and bagging  coordination classification increases the representational capacity of an original  univariate  classifier. that is  given a classifier representation for a single label p yi|xi 붿   as mentioned previously  a coordination classifier p yiyj|xixj 뷋  doubles the number of input features and squares the number of output classes. in addition  the prediction of a test label can  in principle  depend on all training and test patterns. of course  simply increasing the representational capacity of a base classifier increases the risk of overfitting. however  the advantage of ensemble methods is that the resulting classifier  although more complex  is  smoothed  by a principled form of model combination  which helps avoid overfitting while exploiting added representational complexity. that is  the process of model combination can be used to reduce the variance of the learned predictor. in our case  we base our model combination principle on inference in a markov random field. we will see below that  in fact  coordination classification is competitive as an ensemble technique.
모the biggest drawback of coordination classification is the need to perform probabilistic inference  via loopy belief propagation  in order to label test instances. however  we have still found the method to be robust to approximations  like running only a single iteration of loopy belief propagation  or even just taking local votes or products.
1 experimental results
we implemented the proposed coordination classification technique for a few different forms of probabilistic classifiers and using various standard iid data sets. our intent was to determine whether the approach indeed had merit and was also robust to alterations in classifiers and data sets. our experiments were conducted on standard two-class benchmark data sets from the uci repository. the data sets used were: 1. australian  1. breast  1. chess  1. cleve  1. corral  1. crx  1. diabetes  1. flare  1. german  1. glass1  1. heart  1. hepatitis  1. mofn-1  1. pima  and 1. vote. all of our experimental results were obtained by 1-fold cross validation  repeated 1 times for different randomizations of the graph structures. the tables and plots report averages of these results  with standard deviations included in the tables.
모table 1 and figure 1 show the results of our first experiment. in this case  we implemented a standard logistic regression model  using unaltered input features  to learn a base coordination classifier p yiyj|xixj 뷋 . classification was performed by running loopy belief propagation until the test labels stabilized  usually after 1 to 1 iterations . in the first experiment we used a graph over test labels only  to determine whether introducing label dependency would have any beneficial effect  see  edge  results . here we subsampled test-test edges uniformly at random for an overall density of 1 edges per test example. table 1 and figure 1 show the resulting misclassification error obtained by coordination classification in comparison to learning a standard logistic regression model  p yi|xi 붿 . here we see a notable reduction in overall misclassification error  1%뫸1%   with a significant improvement on some data sets  breast  -1%  diabetes  -1%  mofn  -1%  pima  -1%   and a minor increase on two data sets  cleve  +1%  and corral  +1% .
모table 1 also compares the error of coordination classification to boosting the base logistic regression model p yi|xi 붿 . here we used 1 rounds of adaboost  freund and schapire  1; 1   thereby combining approximately the same number of votes per test pattern as coordination classification. this experiment shows that as an ensemble method  coordination classification performs competitively in this case. an advantage of coordination classification is that it only needs to learn a single base classifier  as opposed to the multiple training episodes required by boosting. the need to run loopy belief propagation on the output labels is a disadvantage however.
모to investigate the robustness of the method  we repeated the previous experiments using a different base classifier. table 1 and figure 1 show the results of an experiment using naive bayes instead of logistic regression as the base classification method. here the results are not as strong as the first case we tried  although they are still credible. note that boosting obtains a few larger improvements  but also larger losses. classification coordination appears to be fairly stable.
모
table 1: a comparison of average misclassification error  %  on uci data using logistic regression as a base model.   = average improvement over base.
baseboosted    stdedge   std11-1 11111 11111 1111-1 11111 1111-1 11111 11111 11111 1111-1 1111-1 11111 11111 1111-1 11111 1111-1 11base vs boosted

classification error of boosted base vs edge

classification error of edge
figure 1: a comparison of average misclassification error on uci data sets using logistic regression. top plot: base model versus boosted logistic regression. bottom plot: base model versus  edge -based coordination classification.
       a comparison of average misclassification error  %  on uci data using naive bayes as a base model.   = average improvement over base.
baseboosted    stdedge   stdaustralian111 111 1breast111 111 1chess11-1 11-1 1cleve111 111 1corral111 111 1crx111 11-1 1diabetes111 111 1flare11-1 111 1german111 11-1 1glass1.1.1-1 11-1 1heart111 111 1hepatitis111 11-1 1mofn-111-1 11-1 1pima11-1 111 1vote11-1 11-1 1average11-1 11-1 1base vs boosted

classification error of boosted base vs edge

classification error of edge
figure 1: a comparison of average misclassification error on uci data sets using naive bayes. top plot: base model versus boosted naive bayes. bottom plot: base model versus  edge -based coordination classification.
모the above experiments were run using only edges between test labels. to compare to standard approaches for iid data  we repeated the experiments using only edges between test and training labels  hence decoupling the test labels and eliminating the need for belief propagation  as discussed in section 1 . in this case  test labels are predicted independently. table 1 and figure 1  top  still show  however  that this approach generally improves the base logistic regression classifier p yi|xi 붿   see  node  results . we conclude that correlating the test labels appears to be a beneficial idea  even in an iid setting. the marginal improvement of label dependence  although positive  might be secondary to the benefit of learning a similarity measure.
모all of the previous results were obtained by subsampling edges at a density of 1 edges per test label. to test the sensitivity of our results to the edge density  we repeated the first experiment  test-test edges  using different edge densities. figure 1 shows that the performance of coordination classification does not appear to be sensitive to edge density.
모finally  we experimented with the combined edge approach  iii  which randomly mixed test-test edges and testtrain edges yielding the results of table 1 and figure 1  bottom . the results in this case do not appear to surpass the performance of using test only edges  see  mix  results .
1 extensions
all of our results so far have involved probabilistic classifiers whose output is a conditional distribution over the label y given the input pattern x. of course  many of the most important classification learning technologies  such as decision trees and support vector machines do not naturally produce probabilistic classifications over the class label  although they can be extended in various ways to do this  platt  1  . this raises the obvious question of generalizing our technique to consider nonprobabilistic classifiers.
모it is always possible to extend a classification learning method to learn to predict label pairs  yi yj  given paired input patterns  xi xj . the difficulty is combining several paired predictions to render a sensible classification for a single test pattern. a convenient way to do this would be to convert the predicted outputs to nonnegative potentials over labelings. however  rather than do this  we tried the simpler alternative of combining pair classifications by a simple vote to classify a single test pattern xi . this is a less powerful combination method than loopy belief propagation  but requires fewer extensions to existing methods to test the coordination classifier idea in these cases.
모to test this simple idea  we conducted an experiment on the same uci data using a neural network classifier. specifically we used a feedforward neural network with one hidden layer and logistic activation functions. the base neural network used one output unit  whereas the pairwise neural network used four output units  two units to indicate the class of each of the two input vectors  and double the number of input units. in each case the number of hidden units was set to 1  subject to the constraint that  nin + nout  뫄 nhidden 뫞 train size/1. we trained the networks to minimize cross entropy error using the quasi-newton method from netlab alternative comparison of average error  %  on uci data using logistic regression as a base model.   = average improvement over base.
basenode   stdmix   stdaustralian11-1 11-1 1breast11-1 11-1 1chess111 111 1cleve111 111 1corral111 111 1crx11-1 11-1 1diabetes11-1 11-1 1flare111 111 1german11-1 11-1 1glass1.1.1.1 111 1heart11-1 11-1 1hepatitis11-1 11-1 1mofn-111-1 11-1 1pima11-1 11-1 1vote11-1 11-1 1average11-1 11-1 1base vs node

classification error of node base vs mix

classification error of mix
figure 1: alternative comparison on uci data using logistic regression. top plot: base model versus  node -based coordination classification. bottom plot: base model versus a mix of  edge  and  node  based coordination classification.

figure 1: average misclassification error of  edge -based coordination classification using logistic regression  comparing different ratios of edges to the number of test patterns.
 nabney  1   www.ncrg.aston.ac.uk/netlab .
모once a pairwise neural network classifier was learned  we classified test examples according to the previous  edge  model  again by building a random graph between test labels  using an average of 1 edges per test label as before   using the learned coordination neural network to make hard predictions for each edge  and then combining the edge predictions using a simple vote to classify each test example. table 1   edge   and figure 1 show the results of this experiment. surprisingly  we still obtain a slight overall reduction in misclassification error over the base level neural network classifier  while again competing well against boosting.
모although encouraging  our results are not as positive in every case. we also conducted a simple experiment with decision trees  quinlan  1  as the base coordination classifier  once again combining these predictions with a simple vote to label specific test patterns. unfortunately  we did not observe an overall improvement over the base decision tree classifier in this case; see figure 1. this result suggests that a more powerful model combination idea might be required to achieve robust improvements more generally.
1 conclusion
we have proposed a novel classification learning strategy that coordinates the prediction of test labels on a graph over the data. the coordination classification idea can be used to extend any probabilistic classification approach quite naturally  and even seems to be applicable to nonprobabilistic techniques as well  although more research needs to be done .
모one insight behind the technique is that making correlated predictions of test labels is justified  even advantageous  in the standard iid setting. this fact has often been overlooked in classification learning  therefore we believe it to be a point worth emphasizing. the ability to learn and predict coordinated test labels  combining them with probabilistic inference  provides a flexible new tool for improving classification accuracy on iid data.
       a comparison of average misclassification error  %  on uci data using a neural network as a base model.   = average improvement over base.
baseboosted    stdedge   stdaustralian11-1 11-1 1breast111 111 1chess111 111 1cleve11-1 11-1 1corral11 11 1crx11-1 11-1 1diabetes111 11-1 1flare111 111 1german11-1 11-1 1glass1.1.1-1 111 1heart11-1 11-1 1hepatitis11-1 11-1 1mofn-111 11 1pima111 11-1 1vote111 111 1average11-1 11-1 1base vs boosted

classification error of boosted base vs edge

classification error of edge
figure 1: a comparison of average misclassification error on uci data sets using a neural network. top plot: base model versus boosted neural network. bottom plot: base model versus  edge -based coordination classification.
모
base vs edge

classification error of edge
figure 1: a comparison of average misclassification error on uci data sets using c1 as the base classifier.
모one idea for future work that we are considering is to extend the notion of a pairwise edge classifier to a more general clique classifier. we are also investigating alternative principles for defining potentials on label pairs to see perhaps if there are other combination ideas that work more effectively. finally  we are also investigating whether combining standard  single node  classifiers with our generalized  edge  predictors might lead to further accuracy improvements.
acknowledgments
research supported by the alberta ingenuity centre for machine learning  nserc  mitacs  and the canada research chairs program.
