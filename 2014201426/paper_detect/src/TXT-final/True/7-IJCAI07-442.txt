
this paper presents a method for designing a semisupervised classifier for multi-componentdata such as web pages consisting of text and link information. the proposed method is based on a hybrid of generative and discriminative approaches to take advantage of both approaches. with our hybrid approach  for each component  we consider an individual generative model trained on labeled samples and a model introduced to reduce the effect of the bias that results when there are few labeled samples. then  we construct a hybridclassifier by combining all the models based on the maximum entropy principle. in our experimental results using three test collections such as web pages and technical papers  we confirmed that our hybrid approach was effective in improving the generalization performance of multi-component data classification.
1 introduction
data samples such as web pages and multimodal data usually contain main and additional information. for example  web pages consist of main text and additional components such as hyperlinks and anchor text. although the main content plays an important role when designing a classifier  additional contentmay contain substantial informationfor classification. recently  classifiers have been developedthat deal with multicomponent data such as web pages  chakrabarti et al.  1; cohn and hofmann  1; sun et al.  1; lu and getoor  1   technical papers containing text and citations  cohn and hofmann  1; lu and getoor  1   and music data with text titles  brochu and freitas  1 .
모in supervised learning cases  existing probabilistic approaches to classifier design for arbitrary multi-component data are generative  discriminative  and a hybrid of the two. generative classifiers learn the joint probability model  p x y   of feature vector x and class label y  compute p y|x  by using the bayes rule  and then take the most probable class label y. to deal with multiple heterogeneous components  under the assumption of the class conditional independence of all components  the class conditional probability density p xj|y  for the jth component xj is individually modeled  brochu and freitas  1 . by contrast  discriminative classifiers directly model class posterior probability p y|x  and learn mapping from x to y. multinomial logistic regression  hastie et al.  1  can be used for this purpose.
모it has been shown that discriminative classifiers often achieve better performance than generative classifiers  but that the latter often provide better generalization performance than the former when trained by few labeled samples  ng and jordan  1 . therefore  hybrid classifiers have been proposed to take advantage of the generative and discriminative approaches  raina et al.  1 . to construct hybrid classifiers  the generative model p xj|y  of the jth component is first designed individually  and all the component models are combined with weight determined on the basis of a discriminative approach. it has been shown experimentally that the hybrid classifier performed better than pure generative and discriminative classifiers  raina et al.  1 .
모on the other hand  a large number of labeled samples are often required if we wish to obtain better classifiers with generalization ability. however  in practice it is often fairly expensive to collect many labeled samples  because class labels are manually assigned by experienced analysts. in contrast  unlabeled samples can be easily collected. therefore  effectively utilizing unlabeled samples to improve the generalization performance of classifiers is a major research issue in the field of machine learning  and semi-supervised learning algorithms that use both labeled and unlabeled samples for training classifiers have been developed  cf.  joachims  1; nigam et al.  1; grandvalet and bengio  1; fujino et al.  1b   see  seeger  1  for a comprehensive survey .
모in this paper  we focus on semi-supervised learning for multi-component data classification  ssl-mc  based on probabilisticapproach and present a hybrid classifier for sslmc problems. the hybrid classifier is constructed by extending our previous work for semi-supervised learning  fujino et al.  1b  to deal with multiple components. in our formulation  an individual generative model for each component is designed and trained on labeled samples. when there are few labeled samples  the class boundary provided by the trained generative models is often far from being the most appropriate one. namely  the trained generative models often have a high bias as a result of there being few labeled samples. to mitigate the effect of the bias on classification performance  for each component  we introduce a bias correction model. then  by discriminatively combining these models based on the maximum entropy principle  berger et al.  1   we obtain our hybrid classifier. the bias correction models are trained by using many unlabeled samples to incorporate global data distribution into the classifier design.
모we can consider some straightforward applications of the conventional generative and discriminative semi-supervised learning algorithms nigam et al.  1; grandvalet and bengio  1  to train the above-mentioned classifiers for multicomponent data. using three test collections such as web pages and technical papers  we show experimentally that our proposed method is effective in improving the generalization performance of multi-component data classification especially when the generative and discriminative classifiers provide similar performance.
1 straightforward approaches to semi-supervised learning
1 multi-component data classification
in multi-class  k classes  and single-labeled classification problems  one of the k classes y 뫍 {1 ... k ... k} is assigned to a feature vector x by a classifier. here  each feature vector consists of j separate components as x = {x1 ... xj ... xj}. in semi-supervised learning settings  the classifier is trained on both labeled sample set dl =  and unlabeled sample set .
usually  m is much greater than n. we require a framework that will allow us to incorporate unlabeled samples without class labels y into classifiers. first  we consider straightforward applications of conventional semi-supervised learning algorithms  nigam et al.  1; grandvalet and bengio  1  with a view to incorporating labeled and unlabeled samples into generative  discriminative  and hybrid classifiers proposed for multi-component data.
1 generative classifiers
generative classifiers model a joint probability density p x y . however  such direct modeling is hard for arbitrary components that consist of completely different types of media. to deal with multi-component data in the generative approach  under the assumption that all components have class conditional independence  the joint probability model can be expressed as
 cf.  brochu and freitas  1    where 붿jk is the jth component model parameter for the kth class and 붣 = {붿jk}j k is a set of model parameters. note that the class conditional probability model p xj|k;붿jk  should be selected according to the features of the jth component. the class posteriors for all classes are computed according to the bayes rule  such as

where  is a parameter estimate set. the class label y of x is determined as the k that maximizes p k|x;붣   .
모for the semi-supervised learning of the generative classifiers  unlabeled samples are dealt with as a missing class label problem  and are incorporated in a mixture of joint probability models  nigam et al.  1 . that is  xm 뫍 du is drawn from the marginal generative distribution p x;붣  =
. model parameter set 붣 is computed by maximizing the posterior p 붣|d   map estimation . according to the bayes rule  p 붣|d  뫚 p d|붣 p 붣   we can provide the objective function of model parameter estimation such as
		 1 
here  p 붣  is a prior over 붣. we can obtain the 붣 value that maximizes f 붣  using the expectation-maximization  em  algorithm  dempster et al.  1 .
모the estimation of 붣 is affected by the number of unlabeled samples used with labeled samples. in other words  when
  model parameter 붣 is estimated as almost unsupervised clustering. then  training the model by using unlabeled samples might not be useful in terms of classification accuracy if the mixture model assumptions are not true for actual classification tasks. to mitigate the problem  a weighting parameter 뷂 뫍  1  that reduces the contribution of the unlabeled samples to the parameter estimation was introduced  em-뷂  nigam et al.  1  . the value of 뷂 is determined by cross-validation so that the leave-one-out labeled samples are  as far as possible  correctly classified.
1 discriminative classifiers
discriminative classifiers directly model class posterior probabilities p y|x  for all classes. we can design a discriminative model p y|x  without the separation of components. in multinomiallogistic regression  mlr   hastie et al.  1   the class posterior probabilities are modeled as
	 	 1 
where w = {w1 ... wk ... wk} is a set of unknown model parameters. wk 몫 x represents the inner product of wk and x.
모certain assumptions are required if we are to incorporate unlabeled samples into discriminativeclassifiers  because p x  is not modeled in the classifiers. a minimum entropy regularizer  mer  was introduced as one approach to the semi-supervised learning of discriminativeclassifiers  grandvalet and bengio  1 . this approachis based on the empirical knowledge that classes should be well separated to take advantage of unlabeled samples because the asymptotic information content of unlabeled samples decreases as classes overlap. the conditional entropy is used as a measure of class overlap. by minimizing the conditional entropy  the classifier is trained to separate unlabeled samples as well as possible.
모applying mer to mlr  we estimate w to maximize the following conditional log-likelihood with the regularizer:

 estimation as
.here  뷂 is a weighting parameter  and p w  is a prior over w. a gaussian prior  chen and rosenfeld  1  is often employed as p w .
1 hybrid classifiers
hybrid classifiers learn an individual class conditional probability model p xj|y;붿jy  for the jth component and directly model the class posterior probability by using the trained component models  raina et al.  1; fujino et al.  1a . namely  each component model is estimated on the basis of a generative approach  while the classifier is constructed on the basis of a discriminative approach. for the hybrid classifiers  the class posteriors are provided as
	 	 1 
where 붣  = {붿 j k}j k is a parameter estimate set of component generative models {p xj|k;붿jk }j k  and 붞 = is a parameterset that providesthe combination weights of components and class biases. in supervised learning cases  붞 is estimated to maximize the crossvalidation conditional log-likelihood of labeled samples.
모to incorporate unlabeled samples in the hybrid classifiers  we can consider a simple approachin which we train the component generative models using em-뷂 as mentioned in section 1. with this approach  we incorporate unlabeled samples into the component generative models and then discriminatively combine these models. for convenience  we call this approach cascade hybrid  ch .
1 proposed method
as mentioned in the introduction  we propose a semisupervised hybrid classifier for multi-component data  where bias correction models are introduced to use unlabeled samples effectively. for convenience  we call this classifier hybrid classifier with bias correction models  h-bcm classifier . in this section  we present the formulation of the hbcm classifier and its parameter estimation method.
1 component generative models and bias correction
we first design an individual class conditional probability model  component generative model  for the jth component xj of data samples that belong to the kth class  where 붣 = {붿jk}j k denotes a set of model parameters over all components and classes. in our formulation  the component generative models are trained by using a set of labeled samples  dl. 붣 is computed using map estimation: 붣 =  max붣{logp dl|붣  + logp 붣 }. assuming 붣 is independent of class probability p y = k   we can derive the
 1 
here  p 붿jk  is a prior over 붿jk.
모when there are few labeled samples  the classifier obtained by using the trained component generative models often provides a class boundary that is far from the most appropriate one. namely  the trained component generative models often have a high bias. to obtain a classifier with a smaller bias  we newly introduce another class conditional generative model per component  called bias correction model. the generative and bias correction models for each component belong to the same model family  but the set of parameters 붱 = {뷍jk}j k of the bias correction models is different from 붣. we construct our hybrid classifier by combining the trained component generative models with the bias correction models to mitigate the effect of the bias.
1 discriminative class posterior design
we define our hybrid classifier by using the class posterior probability distribution derived from a discriminative combination of the component generative and bias correction models. the combination is provided based on the maximum entropy  me  principle  berger et al.  1 .
모the me principle is a framework for obtaining a probability distribution  which prefers the most uniform models that satisfy any given constraints. let r k|x  be a target distribution that we wish to specify using the me principle. a constraint is that the expectation of log-likelihood 
  with respect to the target distribution r k|x  is equal to the expectation of the log-likelihood with respect to the empirical distribution yn /n of the training samples as

x k
		 1 
x k
whereis the empirical distribution of x. the equation of the constraint for can be represented in the same form as eq.  1 . we also restrict r k|x  so that it has the same class probability as seen in the labeled samples  such that
	 1  x	x
by	maximizing	the	conditional	entropy	h r 	=
 under these constraints  we
can obtain the target distribution:

where  is a set of lagrange multipliers. 붺1j and 붺1j represent the combination weights of the generative and bias correction models for the jth component  and 뷃k is the bias parameter for the kth class. the distribution r k|x 붣  붱 붞  gives us the formulation of the discriminative classifier  which consists of the trained component generative models and the bias correction models.
모according to the me principle  the solution of 붞 in eq.  1  is the same as the 붞 that maximizes the log-likelihood for r k|x;붣  붱 붞  of labeled samples  xn yn  뫍 dl  berger et al.  1; nigam et al.  1 . however  dl is also used to estimate 붣. using the same labeled samples for both 붞 and 붣 may lead to a bias estimation of 붞. thus  a leave-oneout cross-validation of the labeled samples is used to estimate 붞  raina et al.  1 . let 붣   n  be a generative model parameter set estimated by using all the labeled samples except
 xn yn . the objective function of 붞 then becomes

where p 붞  is a prior over 붞. we used the gaussian prior  chen and rosenfeld  1  as p 붞  뫚
. global con-
vergence is guaranteed when 붞 is estimated with fixed 붣   n  and 붱  since f1 붞|붱  is an upper convex function of 붞.
1 training bias correction models
in our formulation  the parameter set 붱 of the bias correction models is trained with unlabeled samples to reduce the bias that results when there are few labeled samples. according to r k|x;붣  붱 붞  as shown in eq.  1   the class label y of a feature vector x is determined as the k that maximizes the discriminative function:

here  when the values of gk x;붱  for all classes are small  the classification result for x is not reliable  because gk x;붱  is almost the same for all classes. thus  we expect our classifier to provide a large gk x;붱  difference between classes for unseen samples  by estimating 붱 that maximizes the sum of the discriminative function of unlabeled samples:

where p 붱  is a prior over 붱. we incorporate unlabeled samples into our hybrid classifier directly by maximizing f1 붱|붞  in the same way as for a mixture model in generative approaches  where joint probability models are regarded as discriminative functions.
모if 붞 is known  we can estimate the 붱 that provides the local maximum of f1 붱|붞  around the initialized value of 붱  with the help of the em algorithm  dempster et al.  1 . however  붞 is not a known value but an unknown parameter that should be estimated using eq.  1  with 붣 and 붱 fixed. therefore  we estimate 붱 and 붞 iteratively and alternatively. 붱 and 붞 are updated until some convergence criterion is met.
1 experiments
1 test collections
empirical evaluations were performed on three test collections: webkb 1  cora 1  and 1newsgroups  1news  1datasets  which have often been used as benchmark tests for classifiers in text classification tasks  nigam et al.  1 .
모webkb contains web pages from universities. this dataset consists of seven categories  and each page belongs to one of them. following the setup in  nigam et al.  1   we used only four categories: course  faculty  project  and student. the categories contained 1 pages. we extracted four components  main text  mt   out-links  ol   in-links  il   and anchor-text  at   from each page. here  mt is the text description except for tags and links. the ol for a page consists of web page urls linked by the page. the il for a page is the set of web page urls linking the page. at is the anchor text set for each page  which consists of text descriptions expressing the link to the page found on other web pages. we collected il and at from the links within the dataset. for the mt and at components  we removed stop words and vocabulary words included in only one web page. we also removed urls included in only one page for the ol and il components. there were 1 and 1 vocabulary words in the mt and at of the dataset  respectively. ol and il contained 1 and 1 different urls  respectively.
모cora contains more than 1 summaries of technical papers  and each paper belongs to one of 1 groups. for our evaluation  we used 1 papers included in 1 groups: /artificial intelligence/machine learning/*. we extracted three components  main text  mt   authors  au   and citations  ci  from each paper. here  mt consists of the text distribution included in the papers  and au is the set of authors. ci consists of citations to other papers. we removed vocabulary words  authors  and cited papers for each component in the same way as for webkb. there were 1 vocabulary words  1 authors  and 1 cited papers in the dataset.
모1news consists of 1 different usenet discussion groups. each article belongs to one of the 1 groups. the test collection has been used to evaluate a supervised hybrid classifier for multi-component data  raina et al.  1 . following the setup in  nigam et al.  1   we used only five groups: comp.*. there were 1 articles in the groups. we extracted two components  main  m  and title  t   from each article  where t is the text description following  subject:  and m is the main content of each article except for the title. we removed stop words and vocabulary words included in only one article. there were 1 and 1 vocabulary words in components m and t of the dataset  respectively.
1 experimental settings
we used a naive bayes  nb  model  nigam et al.  1  as a generative model and a bias correction model for each component  assuming that different features  works  links  citations  or authors  included in the component are independent. xj was provided as the feature-frequency vector of the jth component. for the map estimation in eqs  1    1   and  1   we used dirichlet priors as the priors over nb model parameters.
모for our experiments  we randomly selected labeled  unlabeled  and test samples from each dataset. we made ten different evaluation sets for each dataset by this random selection. 1 data samples from each dataset were used as the test samples in each experiment. 1  1  and 1 unlabeled samples were used with labeled samples for training these classifiers in webkb  cora  and 1news  respectively. the average classification accuracy over the ten evaluation sets was used to evaluate the methods.
모we compared our h-bcm classifier with three semisupervised classifiers for multi-component data as mentioned in section 1: an nb based classifier with em-뷂  nigam et al.  1   an nlr classifier with mer  mlr/mer   grandvalet and bengio  1   and a ch classifier. we also examined three supervised classifiers: nb  mlr  and hybrid  hy  classifiers. nb  mlr  and hy classifiers were trained only on labeled samples.
모in our experiments  for em-뷂  the value of weighting parameter 뷂 was set in the manner mentioned in section 1 note that in our experiments we selected the value from fourteen candidate values of {1 1 1 1 1 1 1  1 1 1 1 1 1 1}to save computationaltime  but we carefully selected these candidate values via preliminary experiments.
모for mlr/mer  the value of weighting parameter 뷂 in eq.  1  was selected from sixteen candidate values of {{1뫄 1 n 1 뫄 1 n 1 뫄 1 n}1n=1} that were carefully selected via the preliminary experiments. for a fair comparison of the methods  the value of 뷂 should be determined using training samples  for example  using a cross-validation of labeled samples  grandvalet and bengio  1 . we determined the value of 뷂 that gave the best classification performance for test samples to examine the potential ability of mlr/mer because the computation cost of tuning 뷂 was very high.
1 results and discussion
table 1 shows the average classification accuracies over the ten different evaluation sets for webkb  cora  and 1news for different numbers of labeled samples. each number in parentheses in the table denotes the standard deviation of the ten evaluation sets. |dl|  |du|  represents the number of labeled  unlabeled  samples.
모as reported in  raina et al.  1; fujino et al.  1a   in supervised cases  the hybrid classifier was useful for achieving better generalization performance for multi-component data classification. in our experiments  the average classification accuracy of hy was similar to or better than that of nb and mlr.
모we examined em-뷂  mlr/mer  ch  and h-bcm classifiers for semi-supervised cases. the h-bcm classifier outperformed the other semi-supervised classifiers except when the generative classifier performed very differently from the discriminative classifier for webkb. we confirmed experimentally that the h-bcm classifier was useful for improving the generalizationperformanceof multi-componentdata classification with both labeled and unlabeled samples.
모more specifically  the h-bcm classifier outperformed the mlr/mer classifier except when there were many labeled samples for webkb. this result is because mlr/mer tends to be overfitted to few labeled samples. in contrast  h-bcm inherently has the characteristic of the generative models  whereby such an overfitting problem is mitigated. when many labeled samples are available such that the overfitting problem can be solved  it would be natural for the discriminative classifier to be better than the h-bcm classifier.
모the classification performance with h-bcm was better that with em-뷂 except when there were few labeled samples for webkb. it is known that discriminative approaches often provide better classification performance than generative approaches when there are many labeled samples  ng and jordan  1 . our experimentalresult indicates that there might be an intrinsic limitation preventing em-뷂 from achieving a high level of performance because only weighting parameter 뷂 is trained discriminatively.
모h-bcm provided better classification performance than ch except when there were few labeled samples for webkb. the h-bcm and ch classifiers are constructed based on a hybrid of generative and discriminative approaches  but ch differs from h-bcm  in that the former does not contain bias correction models. this experimental result indicates that introducing bias correction models was effective in incorporating unlabeled samples into the hybrid classifier and thus improves its generalization ability.
1 conclusion
we proposed a semi-supervised classifier design method for multi-component data  based on a hybrid generative and discriminative approach  called hybrid classifier with bias correction models  h-bcm classifier . the main idea is to design an individual generative model for each component and to introduce models to reduce the bias that results when there are few labeled samples. for evaluation  we also considered straightforward applications of conventional generative and discriminativesemi-supervised learning algorithms to the classifiers proposed for multi-component data. we confirmed experimentally that h-bcm was more effective in improving the generalization performance of multi-component data classification than the straightforwardapproaches. our experimental results using three test collections suggest that the hbcm classifier is useful especially when the generative and discriminative classifiers provide similar performance. future work will involve applying h-bcm to multimodal data in which different generative models are employed.
