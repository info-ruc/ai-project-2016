 
artificial intelligence methods may be used to model human intelligence or to build intelligent  expert  computer systems. al has already reached the stage of human simulation where it can model such  ineffable  phenomena as intuition  insight and inspiration. this paper reviews the empirical evidence for these capabilities. 
1 	introduction 
i am deeply honored to receive this mark of esteem and friendship from my colleagues in artificial intelligence. it is now just forty years since al newell  cliff shaw and i took the plunge into the exhiliarating waters of al. we called it  complex information processing   but that label had no chance of survival in competition with john mccarthy's more vivid  artificial intelligence   a term that he introduced at about the same time. 
   whatever the label  research in al has provided as exciting and satisfying a lifetime adventure as any scientist could desire. for me  the stored-program computers that first came to my attention about 1 brought an end to many years of frustration  during which i had been searching for a formal language capable of expressing theories of human thinking  problem solving and decision making. beginning in the middle 1s  al newell  cliff shaw and 1 undertook to use computer languages for this purpose. 
1 computer programs as theories 
during the 1s and '1s  and into the early '1s  i had carried my diogenes' lantern through many fields of mathematics seeking the right tools for studying human thought  but neither analysis nor finite math seemed to fill the bill. to use these mathematical tools  one had to force the phenomena into the procrustean bed of real numbers or algebraic and topological abstractions that seemed to leave much of the content behind. computer languages  with their ability to handle symbols of any kind  changed all that by permitting one to implement a very literal representation of human symbol processing in the machine's memories and processes. 
   computer programs written in these languages are  at the most abstract level  simply systems of difference equations  with all of the power of such equations to describe the states and temporal paths of complex symbol systems. to be sure  these equation systems can almost never be solved in closed form; but the computer itself  in providing the powerful tool of simulation  offers a solution to that problem too.1 
   as you are well aware  the requirements of simulating the behavior of physical symbol systems called for symbolmanipulating languages quite different from the algebraic languages used in numerical computing  and led to the invention of list processing languages like the ipl's and lisp  and later to production-system languages like ops-1 and logic-programming languages like prolog. with these languages the computer simulation can produce symbolic outputs that can be compared directly  and with very little translation  with human outputs  especially verbal protocols. 
1 artificial intelligence and cognitive psychology 
my interest in al has been  from the beginning  primarily an interest in its application to psychology. equally exciting opportunities emerged at the same time for designing computer programs that  without necessarily imitating human methods  could perform difficult tasks at expert professional levels. as the construction of expert systems has played second fiddle to human simulation in my own research program  i shall have little to say about it today. my focus will not be on computer achievement of humanoid skills  but on computer imitation of the processes people use to manifest such skills. 
in this research  the computer program is not a 
 metaphor  but a precise language of theory for cognitive psychology in the same sense that differential equations are a language of theory for physics. theories written in al list processing languagesare tested in exactly the same way 
simulation is increasingly employed within traditional mathematics as well  for the increasingly complex systems under study there also defy closed solution. 
simon 
as theories written in differential equations. we use the theories to make predictions  which are then tested against behavior captured in the laboratory or observed in the field.1 
   psychology is an empirical science. it is the study of how human beings behave and of the processes occurring in their minds  that is  their brains  that bring this behavior about. the science of psychology proceeds by observing the phenomena of thinking  by building theories to describe and explain the phenomena  and by laying phenomena and theory side by side to see how closely they match. the proceeding three sentences would be no more and no less true if for  psychology  we substituted  physics  or  geology  or  biology   with corresponding changes in the names of the phenomena studied. 
   the fact that psychology is studied by scientists who themselves are human beings is of no more account than the fact that physics is studied by scientists who consist of atoms or that biology is studied by scientists who eat  breathe and procreate. what we are interested in  in all of these cases  are not the scientists but the phenomena and the theories that describe and explain the phenomena. at the general level  good methodology in physics or chemistry is good methodology in psychology. at more specific levels  each field has to invent methods and instruments for observing and theorizing that are appropriate to the phenomena of interest. the methods arc to be judged by the same standards in every case. 
   some of you may have heard me express these last sentiments before or have read them in my published papers  newell and simon  1 . i feel obliged to repeat them here because books  written in armchair comfort  continue to be published from time to time that try to evaluate by philosophical means psychological theories written in computer languages. 1 must confess that i have not in recent years read any books of this kind  but 1 have seen reviews of them. before you convict me of bigotry for ignoring them  let me explain why i do so. 
1 	cognition's empirical base 
as psychology is an empirical science  we can only judge whether and to what extent particular theoretical proposals are valid by comparing them with data. in the face of such comparisons  philosophical speculation is superfluous; in 
the absence of such comparisons  it is helpless.1 therefore  if we wish to evaluate the claims of theories of thinking  whether these theories take the form of computer 
1 the theories of physics consist not only of the differential equations  but also deducible properties of these equations  e.g.  the principle of conservation of energy in mechanics . theories defined by difference equations  programs  may also possess deducible properties  which then become part of the theory. for example  in epam  the short-term memory capacity can be deduced from the structure and parameters of the program. 
1 should perhaps explain that i selected the topic and title of my talk for this occasion before learning that there would be a session at this meeting on ai and philosophy -- news that i obviously greeted without enthusiasm. 
programs or some other form   and especially if we wish to broadcast the results of our evaluations  we would do well to spend most of our time studying the empirical evidence and making the explicit comparisons with the computer traces. 
   by now  such evidence is voluminous. this is not the place to review it  but i'll cite just one very specialized example. in the book. protocol analysis  1   that anders ericsson and i have written  treating the methodology for testing cognitive theories by comparing human think-aloud protocols with computer traces  there are 1 pages of references. it is not unreasonable to ask anyone who proposes to evaluate the validity of verbal reports as data either to become acquainted with a substantial portion of this literature or to announce clearly his or her amateur status. similarly  it is not unreasonable to ask anyone proposing to pronounce on memory capacity or the acquisition and response speeds of human memory to become acquainted with that large literature. 
   there are  of course  comparably large literatures on problem solving  reasoning  perceiving  and many other topics. any serious assessment of our knowledge of human thought processes or of the veridicality of theories that purport to describe or explain these processes must rest on the data reported in this literature.  notice that i am not asking anyone to read it  but just to refrain from public comment if they haven't read it.  
   what theories are available for testing  and what kinds of phenomena do they address  again  i can only cite a few examples  some from my own work and some from the work of others. an early example is the general problem solver  gps   whose central mechanism  means-ends analysis  has been shown empirically  in numerous studies  to be a much-used heuristic in human problem solving.  a small fraction of these empirical tests are discussed in newell and simon  1; you will find others in the two volumes of my models of thought  1  1 . contemporary with gps is epam  a model of human perceptual and memory processes due originally to feigenbaum  which has been tested successful against empirical data from experiments on verbal learning  expert memory performances in several domains of expertise  including expertise in mnemonics   and concept attainment  for some of the empirical tests see feigenbaum and simon  1; and richman  et a/.  1 . 
   a somewhat later system is john anderson's act*  1   which focuses especially on semantic memory and the explanation of contextual effects through spreading activation. a very different and still newer theory  or set of theories  are  neural  networks of the connectionist variety that have shown capacities to learn in a variety of tasks 
 mcclelland and rumelhart  1 . quite recently  allen newell  in collaboration with john laird  paul rosenbloom and others  has produced soar  a vigorous push from gps into a far more general and unified architecture  which demonstrates the relevance of multiple problem spaces and learning by chunking  newell  1 . 
   still closer to the topics i shall address in the remainder of this talk is the bacon system  see langley  et al.  
1  and its close relatives  glauber  stahl  kekada  kulkarni and simon  1   live  shen  1  and others that simulate many of the discovery processes that are discernable in the activities of scientists. some of the models i have mentioned are complementary  some are competitive  as theories are in any science. 
   again  i must remind you that to understand these systems  not just as interesting examples of artificial intelligence but as theories of human thinking  and to adjudicate among them when they conflict  you must devote just as much attention to the experimental and other empirical evidence about the phenomena they model as to the structures and behaviors of the programs themselves. errors in the evaluation of these programs as psychological theories are caused less often by lack of knowledge or inaccurate knowledge about the programs than by lack of knowledge or inaccurate knowledge about how human subjects behave when they are confronted with the same tasks as the programs were tested on. 
   for one example  the brittleness of computer programs when they wander outside the task domain for which they are programmed is often mentioned as a defect of these programs  viewed as psychological theories  without noticing the extraordinary brittleness of human behavior when it wanders outside the arena of the actor's experiences.  inexperienced urbanites lost in a wilderness frequently freeze or starve to death in circumstances where experienced savages survive. novices playing their first bridge hand bid and discard almost randomly.  theories cannot be compared with facts unless the theories are specified precisely and the facts known thoroughly. 
1 	limits of explanation  
in the remainder of my talk 1 shall put the information processing explanation of thinking to what is usually regarded as a severe test. the idea that the processes humans use in everyday  relatively routine and wellstructured tasks can be modeled by computers has gained  over the years  a considerable amount of acceptance -- more among experimental psychologists than among people who are more distant from the data. the idea that these models can be extended to ill-structured tasks of the kinds that require ingenuity  perhaps even creativity  when performed by humans is less widely accepted. this is no more a philosophical question than the questions that i have discussed previously. it is a question about certain kinds of human behavior and whether these kinds of behavior can be modeled by computers. it is to be settled by comparing the records of human behavior with the output of computer models. 
   i shall focus on three terms that appear frequently in the literature and in popularized psychology  not always with the same meanings  and which have been used to label behaviors that are often claimed to be beyond explanation by programmable mechanisms. the three terms are  intuition    insight  and  inspiration.  in addressing the cognitive phenomena associated with each of these terms  i shall first define the term so that we can determine when the corresponding phenomena are being exhibited. without clear tests that unable us to identify the occasions of  intuition    insight  and  inspiration   there are no phenomena to explain. 
   i cannot claim that the definitions i shall propose represent the only ways in which these terms are  or can be  used. i will claim that they correspond to the usual meanings  and that the operational tests on which they are based are the operational tests that are commonly used to determine when people are being  intuitive    insightful   or  inspired.  these are the properties the definitions should possess if they are to be used in theories of intuition  insight and inspiration. 
   having established operational tests for the phenomena  we shall look at the evidence as to whether people and computers exhibit the process in question  and if so  under what circumstances. what 1 shall show is  first  that the presence or absence of phenomena like these  sometimes claimed to be ineffable  can be determined objectively  and second  that certain computer programs are mechanisms that exhibit these phenomena and thereby provide explanations for them. 
1 intuition 
let me start with the process of human thinking that is usually called  intuition.  before we can do research on intuition  we have to know what it is; in particular  we must have some operational definition that tells us when intuition is being exhibited by a human being and when it is not. it is not too difficult to construct such a definition. 
   the marks that are usually used to attribute an intelligent act  say  a problem solution  to intuition are that:  1  the solution was reached rather rapidly after the problem was posed  and  1  the problem solver could not give a veridical account of the steps that were taken in order to reach it. typically  the problem solver will assert that the solution came  suddenly  or  instantly.  in the few instances where these events have been timed   suddenly  and  instantly  turn out to mean  in a second or two   or even  in a minute or two.  
   that's essentially the way my dictionary defines intuition  too:  the power or facility of knowing things without conscious reasoning.  let us take the criteria of rapid solution and inability to report a sequence of steps leading up to the solution as the indications that people arc using intuition. these are the criteria we actually use to judge when intuition is being exhibited. applying these criteria  we now have some clearly designated phenomena to be explained; we can try to construct some difference equations  computer programs  that behave intuitively. 
   intuitive thinking is frequently contrasted with  logical  thinking. logical thinking is recognized by being planful and proceeding by steps  each of which  even if it fails to reach its goal  has its reasons. intuitive thinking  as we have seen  proceeds by a jump to its conclusions  with no conscious deliberateness in the process  but intuitive and logical thinking can be intermingled. the expert  faced with a difficult problem  may have to search planfully and deliberately  but is aided  at each stage of the search  by intermediate leaps of intuition of which the novice is incapable. using what appear to be macros  the intermediate steps of which are these intuitions  the expert takes long strides in search  the novice tiny steps. 
simon 
1 a theory of intuition 
after specifying how we shall recognize intuition when it occurs  the next task in building a theory of it is to design a computer program  or find one already built  that will solve some problems intuitively - as determined by exactly the same criteria as we employ to determine when people are using intuition. the program will solve these problems  if they are easy  in a  simulated  second or two and will be unable to provide a  simulated  verbal report of the solution process. fortunately  at least one such program already exists: the epam program  which first became operative about 1. it was not designed with intuition in mind  but rather to simulate human rote verbal learning  for which there already existed at that time a large body of empirical data from experiments run over the previous 1 years. epam accounted for the main phenomena found in these data. 
   the core of epam is a tree-like discrimination net that grows in response to the stimuli presented to it and among which it learns to discriminate  and a short-term memory that will hold a few familiar symbols  1б└1    but will retain them more than 1 seconds only if it has time to rehearse them. epam's discrimination net is somewhat similar to the rete nets that are used to index production systems. epam learns the correct discriminations by experience  with only feedback of  right  or  wrong  to its responses. epam nets have been taught to discriminate among nearly 
1 different stimuli  and there is nothing final about that number. 
   these learned patterns  once acquired  can now be recognized when presented to epam because it sorts them through its net  the recognition time being logarithmic in the total number of stimuli in the net. if the net has a branching factor of 1  then recognition of a net discriminating among a million stimuli could be achieved by performing about ten tests  1 = 1 1 . the epam model  its parameters calibrated from data in verbal learning experiments  can accomplish such a recognition in a tenth to a fifth of a second. if we add additional time for utterance of a response  the act of recognition takes a second or less. 
   now suppose we confront epam with a situation that is recognizable from its previous experience  a collection of medical symptoms  say . it can now access  in less than a second  information about a disease that is presumably responsible for these symptoms. as epam is able to report symbols that reach its short-term memory  where the result of an act of recognition is stored   it can report the name of the disease. as it cannot report the results of the individual tests performed on the symptoms along the path  it cannot describe how it reached its conclusions. even if it can report the symptoms that were given it  because it stored some of them in memory during the presentation   it cannot give a veridical account of which of these were actually used to make the diagnosis or how they were considered and weighed during the recognition process.1 we might add   even as you and i   for these are also the characteristics of human diagnosis: the physician can report what disease he or she has recognized  but cannot give a veridical report of which symptoms were taken into account  or what weights were assigned to them. 
   to simulate the diagnostic process in more complex cases  we need a system that contains  in addition to epam's discrimination net and the long-term memory it indexes and accesses  some capabilities for solving problems by heuristic search -- a combination of epam with a sort of general problem solver  gps  or soar. then we will observe this combined system not only recognizing familiar symptoms and their causes  but also reasoning to infer what additional tests might discriminate among alternative diagnoses that have been recognized as possible causes of the initial symptoms. 
   automatic medical diagnosis systems now exist that perform diagnostic tasks far more accurately than epam alone could  for they take into account alternative diagnoses  do some simple reasoning about relations among symptoms  and are able to request additional tests on the patient to achieve greater discriminatory power and accuracy. these systems  of course  are using a combination of intuition  as usually defined  and  logical  thought  including means-ends analysis in some form . our current interest is not in machine competence in medical diagnosis but in models of intuition. epam  as described  is exhibiting intuition  and modeling at least the first stage of thought  the recognition stage  of an experienced physician confronted with a set of symptoms. 
1 	testing the recognition model 
what grounds do we have for regarding this basic recognition mechanism  which lies at the core of epam  as a valid theory of the process that causes people to have intuitions  simply that it has the same manifestations as human intuition: it occurs on the same time scale accompanied with the same inability to explain the process. nor was it explicitly  cooked up  to exhibit these properties: they are basic to a system that was designed with quite other simulation tasks in mind. this is exactly the test we apply in validating any theory: we look at the match between the theory and the phenomena and at the ratio of amount of data explained to number of parameters available for fitting. 
   we can extend the tests of this theory of intuition further. it is well known that human intuitions that turn out to be valid problem solutions rarely occur to humans who are not well informed about the problem domain. for example  an expert solving a simple problem in physics takes a few computational steps without any pre-planning and reports the answer. the recorded verbal protocol shows the steps  but no evidence of why they were taken  no mention of the goals  operators  the algebraic expressions in which numbers were substituted . 
short-term memory. the trace of non-reportable processes must be distinguished from the simulation of processes the theory claims to be reportable. 
   a novice solving the same problem works backwards from the variable to be evaluated  explicitly stating goals  the equations used and the substitutions in the equations. in one experiment  the novice's protocol was approximately four times as long as the expert's  simon and simon  1  and exhibited no intuition -- only patient search. novices who replace this search by guessing seldom guess correct answers. this is exactly what epam predicts: that there is no recognition without previous knowledge  and no intuition without recognition. notice that intuition can be as fallible as the recognition cues on which it is based. 
   there are a number of experimental paradigms for carrying out tests on the theory that intuition is simply a form of recognition. the expert/novice paradigm has already been mentioned: experts should frequently report correct intuitive solutions of problems in their domain  while novices should seldom report intuitions  and if they report any  a large proportion should be incorrect. experts who are able to report intuitions in their domains should be unable to do so in domains where they are not expert. by making cues more or less obvious  it should be possible to increase or decrease the frequency of correct intuitions; misleading cues should induce false intuitions. hints of various kinds should draw attention to cues  hence facilitate intuition. these are only the most obvious possibilities. 
   experiments on intuition are best carried out on tasks where the correctness of answers can be verified  at least after the fact. we would want to identify  false intuition  to explain the cases  probably very frequent but hard to pinpoint in domains where objective criteria of correctness are lacking  where the presence of certain features in a situation leads subjects to announce a sudden solution although the connection between the cue and the inferences drawn from it is invalid. determining the circumstances that encourage or discourage false intuition would involve research on the characteristics of situations that subjects attend to  and the beliefs they hold that lead them to the erroneous solutions. some of the research that has been done on the psychology of so-called  naive physics  fits this general paradigm  as does some of the research on  garden paths   spontaneous but erroneous interpretations  in syntactic analysis of sentences. 
   we see that intuition  far from being a mysterious and inexplicable phenomenon  is a well known process: the process of recognizing something on the basis of previous experience with it  and as a result of that recognition  securing access in long-term memory to the things we know about it. what subjects can report about the origins of their intuitions  and what they can't report  are exactly what we would predict from a theory that explained the phenomena associated with recognition. as a matter of fact  we could simplify our vocabulary in psychology if we just abandoned the word  intuition   and used the term  recognition  instead. 
1 insight 
another process of thought that has sometimes been declared to be inexplicable by mechanical means is insight. my dictionary  this time  associates insight closely with intuition. in fact  its second definition of  intuition  is:  quick and ready insight   its explicit definition of  insight  is not much more helpful:  the power or act of seeing into a situation: understanding  penetration.  again  we gain an impression of suddenness  but in this case accompanied by depth. perhaps we shall want to regard any instance of insight as also an instance of intuition  in which case our work is already done  for we have just proposed a theory of intuition. let's see  however if there is an alternative some other phenomenon that needs explanation and to which we can attach the word  insight.  
   consider the  aha  phenomenon. someone is trying to solve a problem  without success. at some point  a new idea comes suddenly to mind - a new way of viewing the problem. with this new idea comes a conviction that the problem is solved  or will be solved almost immediately. moreover  the conviction is accompanied by an understanding of why the solution works. at this point we hear the  aha   soon followed by the solution -- or occasionally by a disappointed realization that the insight was illusory. in some cases  after a problem has been worked on for some time without progress  it is put out of mind for a while  and the  aha  comes unexpectedly  at a moment when the mind was presumably attending to something else. 
   in both scenarios  with and without the interruption  the phenomenon shares the characteristics of intuitive solution: suddenness of solution  or at least of the realization that the solution is on its way   and inability to account for its appearance. the process differs from intuition in that:  1  the insight is preceded by a period of unsuccessful work  often accompanied by frustration   1  what appears suddenly is not necessarily the solution  but the conviction of its imminence   1  the insight involves a new way of looking at the problem  the appearance of a new problem representation accompanied by a feeling of seeing how the problem works  and  1  sometimes  not always   the insight is preceded by a period of  incubation   during which the problem is not attended to consciously  and occurs at a moment when the mind has been otherwise occupied. 
the third of these features is the source of the feeling of 
 understanding  and  depth  that accompanies the experience of insight. again  these are the phenomena we use to identify instances of insight in human beings  ourselves or others . we can take the presence of these four features as our operational definition of insight  and using it  we now have some definite phenomena that we can study and seek to explain. 
1 	a theory of insight 
let me now describe a computer program that can experience insight  defined in the manner just indicated. i shall present this theory a little more tentatively than the theory of intuition proposed earlier because  while it demonstrates that a computer program can have insights  the evidence is a little less solid than for intuition that it matches all aspects of the human experience of insight. 
again  a program that combines the capabilities of 
epam and the general problem solver constitutes the core of the theory.  1  we suppose that a gps-like or soar-like problem solver is conducting  unsuccessfully so far  a 
simon 
heuristic  selective  search for a problem solution.  1  it holds in long-term memory some body of information about the problem and knowledge of methods for attacking it.  1  unfortunately  it is following a path that will not lead to a solution  although of course it is unaware of this .  1  we assume that the search is serial  its direction controlled by atttentional mechanisms that are represented by the flow of control in the program.  1  much of this control information  especially information about the local situation  is held in short-term memory  and is continually changing.  1  at the same time  some of the more permanent features of the problem situation are being noticed  learned  and stored in long-term memory  so that the information available for problem solution is changing  and usually improving.  1  the control structure includes an interrupt mechanism which will pause in search after some period without success or evidence of progress  and shift activity to another problem space where the search is not for the problem solution but for a different problem representation and/or a different search control structure.  1  when search is interrupted  the control information held in short-term memory will be lost  so that if search is later resumed  the direction of attention will be governed by the new representation and control structure  hence may lead the search in new directions.  1  as the non-local information that has been acquired in long-term memory through the previous search will participate in determine the search direction  the new direction is likely to be more productive than the previous one. 
1 	empirical tests of the theory 
now we have introduced nine assumptions to explain the insight that may occur when the search is resumed  which hardly looks like a parsimonious theory. but these assumptions were not introduced into the composite epamgps to solve this particular problem. all are integral properties of these systems  whose presence is revealed by many different kinds of evidence obtained in other tasks. 
   one body of evidence supporting this model of insight comes from an experimental investigation of the mutilated checkerboard problem that craig kaplan and i conducted a few years ago  kaplan and simon  1 . we begin with a chessboard  1 squares  and 1 dominos  each of which can cover exactly two squares. obviously  we can cover the chessboard with the dominos  with neither squares nor dominos left over. now  we mutilate the chessboard by removing the upper-left and lower-right corner squares  leaving a board of 1 squares. we ask subjects to cover it with 1 dominos or to prove it can't be done. 
   this is a difficult problem. most people fail to solve it even after several hours' effort. their usual approach is to attempt various coverings as systematically as possible. as there are tens of thousands of ways to try to cover the board  after some number of failures they become frustrated  their efforts flag and they begin to wonder whether a covering exists. increasingly they feel a need to look at the problem in a new way  but people seem not to have systematic methods for generating new problem representations. some subjects simplify by replacing the 1гд1 board with a 1гд1 board  but this does not help. 
   hints do help. although few subjects solve the problem without a hint  many do with a hint  usually in a few minutes after the hint is provided. for example  the experimenter may call attention to the fact that the two squares left uncovered after an unsuccessful attempt are always the same color  opposite to the color of the excised corner squares. attending to this fact  subjects begin to consider the number of squares of each color as relevant  and soon note that each domino covers a square of each color. this leads quickly to the inference that a set of dominos must always cover the same number of squares of each color  but that the mutilated board has more squares of the one color than of the other: therefore  a covering is impossible. 
   subjects who discover this solution  with or without a hint  exhibit behaviors that satisfy our definition of insight. the solution is preceded by unsuccessful work and frustration; it appears suddenly; it involves a new representration of the problem that makes the problem structure evident. the subjects come to the solution quite quickly once they attend to the critical property  equality of the numbers of squares of each color that are covered . this is also true of the few subjects who solve the problem without being given a hint. these subjects have their  aha!  when they attend to the fact that the uncovered squares are always the same color  and that the mutilated board has more squares of that color than of the other. aided by cues or not  successful subjects often  literally  say  aha!  at the moment of recognizing the relevance of the parity of squares of the two colors. 
   moreover  the mechanisms that bring about the solution are those postulated in our computer theory of insight  as can be seen by examining the list given above. steps 1 through 1 are the critical ones. in the case of hints  attention is directed to the crucial information by the hint  this information is stored in memory  and the search resumes from a new point and with a new direction of attention that makes the previous attempts to cover the board irrelevant. in the case of subjects who solve without a hint  the direction of attention to the invariant color of the uncovered squares may derive from a heuristic to attend to invariant properties of a situation - the properties that do not change  no matter what paths are searched in solution attempts. 
   there are probably several such heuristics  surprise is another one  that shift peoples' attention to particular aspects of a problem situation  thereby enabling the learning of key structural features and redirecting search. the evidence for such heuristics is not limited to laboratory situations; the role of the surprise heuristic in scientific discovery has been frequently noted. i shall return to it later. 
   the role of attention in insight receives further verification from a variant on the experiment. different groups of subjects are provided with different chessboards:  1  a standard board   1  a ruled 1-by-1 matrix without colors  and  1  an uncolored matrix with the words  bread  and  butter    pepper  and  salt  will do as well  printed on alternate squares. more subjects find the solution in condition 1 than in condition 1; and more in condition 1 than in condition 1. 
   
   the reason for the latter difference is obvious: presence of the alternating colors provides a cue to which a subject's attention may be directed. what is the reason for the superiority of  bread  and  butter  over red and black  subjects are familiar with standard chessboards and have no reason to think that the color has any relevance for this problem  hence don't attend to it. in the case of  bread  and  butter   the subjects' attention is attracted to this unusual feature of the situation; they wonder why  those crazy psychologists put those labels on the squares.  here we obtain direct support for the hypothesis that direction of attention to the key features of the situation provides the basis for solution. noticeability of a feature is essential  whether it is provided by an explicit clue or some other means. 
1 	incubation 
the checkerboard experiments do not say anything about incubation  or whether interruption of the solution process for a shorter or longer period may contribute to solution. here i can point to another set of experiments carried out by kaplan  1 . he defines incubation as  any positive effect of an interruption on problem solving performance   and lists seven explanations that have been offered for it:  unconscious work  conscious work that is later forgotten  recovery from fatigue  forgetting  priming  maturation and 
statistical regression  p. 1 .  
   kaplan then carries out experiments to show  or to confirm  that  1  interruption of certain kinds of tasks  socalled divergent-thinking tasks  improves subsequent performance  i.e.  incubation can be demonstrated experimentally    1  answers supplied after an interruption differ more from the just-previous answers than do successive answers supplied without interruption  i.e.  incubation can break  set     1  interruptions combined with a hint increase the effects of incubation  the hint shifts attention from continuing search to changing the representation    1  hints may work without subjects' conscious awareness of their connection with the unsolved problem  and  1  subjects underestimate the time they spend thinking about the problem during an interruption. details can be found in the original study. 
kaplan proposes a model  which he calls a generic 
memory model  to account for these phenomena. the model is compatible with the one we have already proposed  with the addition of so-called priming mechanisms of the kind that quillian  1  and anderson  1  incorporate in their models of semantic memory.1 the priming mechanisms increase the probability that subjects will attend to items that have been cued  at the same time rapidly decreasing attention to items in stm and slowly decreasing attention to items in ltm. the model accounts for the fact  as the previous model does not  that the length of the interruption is important. neither model needs to postulate unconscious work on the problem during interruption to account for incubation.1 forgetting in short-term memory of information that holds attention to an unproductive line of search  and redirection of attention from search in the original problem space to search for a new problem representation are the key mechanisms in both models that account for the bulk of the empirical findings. 
     on the basis of the evidence i have described and the models that have been offered to explain this evidence  i think it fair to claim that there exists a wholly reasonable theory of incubation  as it is observed in human discovery  that calls only on mechanisms that are already widely postulated as components of standard theories of cognition. the process of incubating ideas is as readily understandable as the process of incubating eggs. 
1 	inspiration 
the term  inspiration  is surrounded by an aura of the miraculous. interpreted literally  it refers to an idea that is not generated by the problem solver  but is breathed in from some external  perhaps heavenly  source. to inspire  says my faithful dictionary  is to  influence  move  or guide by divine or supernatural inspiration.  a bit circular  but quite explicit about the exogenous  non-material source. a greek phrase for it was more vivid: to be inspired  e.g.  at delphi  was to be  seized by the god.  
   the notion that creativity requires inspiration derives from puzzlement about how a mechanism  even a biological mechanism like the brain   if it proceeds in its lawful  mechanistic way  can ever produce novelty. the problem is at the center of plato's central question in the meno: how can an untutored slave boy be led through a geometric argument until he understands the proof  the answer plato provides  which hardly satisfies our modern ears  is that the boy knew it all the time; his new understanding was simply a recollection of a prior understanding buried deep in his memory  a recognition or intuition  . what bothers us about the answer is that plato does not explain where the buried knowledge came from. 
1 	generating novelty 
let's leave the meno  i have offered a solution for the puzzle elsewhere1  and in any event  we are talking science here  not philosophizing   and go directly to the question of how a mechanism creates novelty  for novelty is at the core of creativity. in fact  we shall define creativity operationally  in full accordance with general usage  as novelty that is regarded as having interest or value  economic  esthetic  moral  scientific or other value . 
   i shall start with an example. there are about 1 stable elements in nature  composed of protons and neutrons  and these  in turn  of component particles . there are 
   
innumerable molecules  chemical species  almost none of which existed just after the big bang or just after the 1 elements first appeared in the universe. 
   here is novelty on a mind-boggling scale; how did it come about  the answer is  combinatorics.  novelty can be created  and is created  by combinations and recombinations of existing primitive components. the 1 letters of the alphabet  or  if you prefer the 1-odd phonemes of english  provide the primitives out of which a denumerable infinity of words can be created. new numbers  new words  new molecules  new species  new theorems  new ideas all can be generated without limit by recursion from small finite sets of primitives. 
   the traditional name in ai for this basic noveltyproducing mechanism is generate and test. one uses a combinatorial process to generate new elements  then tests to see if they meet desired criteria. a good example of a generate-and-test system that can create novelty valuable for science is the bacon program  langley  simon  bradsaw and zytkow  1 . bacon takes as inputs uninterpreted numerical data and  when successful  produces as outpouts scientific laws  also uninterpreted  that fit the data .1 
1 	selective search as inspiration 
the law-generating process that bacon uses to find laws that describe data is not a random search process. the space of  possible functions  is not finite  and even if we limited search to some finite portion of it  any useful domain would be too large to yield often to random search. basically  bacon's law generator embodies three heuristics for searching selectively: first  it starts with simple functions  then goes on  by combinatorial means  to more complex ones. we don't have to pause long to define  simple  or  complex.  the simple functions are just those primitive functions that bacon starts with  in fact  the linear function ; the compound functions are formed by multiplying or dividing pairs of functions by each other. a functions is  simple  if it is generated early in the sequence   complex  if generated later. 
   second  bacon is guided by the data in choosing the next function to try. in particular  it notices if one variable increases or decreases monotonically with respect to another  testing whether ratios of the variables are invariant in the first case  products in the second  and shaping the next function it generates accordingly. this simple operation generates a wide class of algebraic functions  and by enlarging a bit the set of primitive functions  e.g.  adding the exponential  logarithmic and sine functions   the class of generatable functions could be greatly broadened. the main point is that bacon's choice of the next function to test 
буi hasten to add that bacon has discovered no new scientific laws  although other programs built in the same generate-andtest principle have ; but it has rediscovered  starting with only the same data that the original discoverer had  a number of the most important laws of 1th and 1th century physics and chemistry. 
depends on what kind of fit with the data the previously tried functions exhibited. 
   third  in problems involving data about more than two variables  bacon follows the venerable experimental procedure of changing one independent variable at a time. having found conditional dependencies among small sets of variables  it explores the effects of altering other variables. 
   that is essentially all there is to it. with these simple means  and provided with the actual data that the original discoverers used  bacon rediscovers kepler's third law  it finds p = d1 on the third or fourth try   ohm's law of current and resistance  black's law of temperature equilibrium for mixtures of liquids and a great many others. there are many other laws it doesn't discover  which is an essential fact if it is to be regarded as a valid theory of human performance. humans also don't discover laws more often than they discover them. 
   to validate bacon as a theory of human discovery;we would like to have as detailed historical data as possible on how the human discoveries were actually made  but sometimes the data are quite scanty. about all we know about kepler's discovery of his third law is that he initially made a mistake  declaring that the period of revolution of the planets varied as the square of their distance from the sun. some years later  he decided the fit of law to data was poor and went on to find the correct law. interestingly enough  bacon first arrives at kepler's erroneous square law  rejects it as not fitting the data well enough  and goes on to the correct law almost immediately. with a looser parameter to test whether a law fits the data  bacon would make kepler's mistake. 
   sometimes the processes of bacon can be tested directly against human processes. yulin qin and i  1  gave students the data  from the world almanac  on the periods and distances of the planets - labeling the variables simply x and v  without interpretation. in less than an hour  1 of 1 students found and fitted the 1-power law to the data. the students who succeeded used a function generator that responded to the nature of the misfits of the incorrect functions. the students who failed either were unable to generate more than linear functions or generated functions whose form was independent of previous fits and misfits. 
   1 spell out this example to show that theories of inspiration are constructed and tested in exactly the same manner as other scientific theories. once the phenomena have been defined  we can look for other phenomena that attend them and for mechanisms that exhibit the same behavior in the same situations. in historical cases more favorable than kepler's  we may have voluminous data on the steps toward discovery. in the case of both faraday and krebs  for example  laboratory notebooks are available  as well as the published articles and autobiographical accounts. in these cases  we have many data points for matching the scientist's behavior with the model's predictions. 
1 	discovery of new concepts 
1 	awards i have now cited a few pieces of evidence ~ many more exist -- that scientists do not have to be  seized by the god  to discover new laws; such laws  even laws of first magnitude  
can be arrived at by quite understandable and simulatable psychological processes. but what about new concepts  where do they come from  
   bacon is provided with one heuristic that i have not yet mentioned. when it discovers that there is an invariant relation in the interaction between two or more elements in a situation  it assigns a new property to the elements  measuring its magnitude by the relative strength of each element's action  one of the elements is assigned a unit value  becoming the standard . for example  bacon notices that when pairs of bodies collide  the ratio of accelerations of any given pair is always the same. bacon defines a new property  let's call it  obstinance    and assigns an obstinance of 1 to body a  and an obstinance to each other body inversely proportional to the magnitude of its acceleration in collisions with a. of course  we know that  obstinance  is what we usually call  inertial mass   and that bacon has reinvented that latter concept on the basis of this simple experiment. 
   this procedure turns out to be a quite general heuristic for discovering new concepts. bacon has used it to reinvent the concepts of specific heat  of refractive index  of voltage  of molecular weight and atomic weight  and to distinguish them  and others. here again  inspiration turns out to be a by-product of ordinary heuristic search. 
   all of these results are available in the psychological and cognitive science literature  langley  simon  bradshaw and 
zytkow  1 . they will not be improved by philosophical debate  but rather  by careful empirical study to determine the range of their validity and the goodness with which they approximate the observed phenomena. debate  philosophical or otherwise  is pointless without familiarity with the evidence. 
1 	other dimensions of discovery 
scientists do many things besides discovering laws and concepts. they plan and carry out experiments and interpret the findings  invent new instruments  find new problems  invent new problem representations. there are other dimensions to discovery  but these are perhaps the most important. i shall say no more about experiments  see kulkarni and simon  1  or instruments or problemfinding here. some processes for finding new representations have already been examined in our discussion of insight. there is still plenty of work to be done  but so far  no evidence of which i am aware that the explanation of the phenomena of intuition  insight and inspiration will require the introduction of mechanisms or processes unlike those that have been widely employed in simulating human thinking. that  of course  is an empirical claim -- actually  not so much a claim as an invitation to join in the exciting task of explaining how machines like people and computers can think  and sometimes think creatively. 
1 	physiological foundations 
it will not have passed without notice that i have said almost nothing today about the brain as a physiological organ. my silence should not be interpreted a doubt that the mind is in the brain  or a suggestion that processes beyond the physiological are required for its operation. the reason for my omission of the physiology of the brain is quite different. as i have pointed out in other contexts  sciences generally progress most effectively if they focus upon phenomena at particular levels in the scheme of things. hunters of the quark do not  fortunately  need to have theories about molecules  or vice versa. the phenomena of nature arrange themselves in levels  simon  1  and scientists specialize in explaining phenomena at each level  high energy physics  nuclear physics  analytic chemistry  biochemistry  molecular biology . . . . neurophysiology  symbolic information processing  and so on   and then  in showing  at least in principle  how the phenomena at each level can be explained  reduced  to the terms and mechanisms of the theory at the next level below. 
   at the present moment in cognitive science  our understanding of thinking at the information processing level has progressed far beyond our knowledge of the physiological mechanisms that implement the symbolic processes of thought.  fortunately  on the computer side  we know full well how the symbolic processes are implemented by electronic processes in silicon.  our ignorance of physiology is regrettable but not alarming for progress at the information-processing level  for this same sky-hook picture of science is visible in every scientific field during some period - usually a long period - in the course of its development. nineteenth century chemistry had little or no base in physics  and biology had only a little more in chemistry. 
   there is no reason why research in cognition should not continue to develop vigorously at both physiological and information processing levels  as it is now doing  watching carefully for the indications  of which there already are a few  that we can begin to build the links between them - starting perhaps with explanations of the nature of the physiological mechanisms  the  chips  and  integrated circuits   that constitute the basic repositories of symbolic memory in the brain. while we await this happy event  there is plenty of work for all of us  and no lack of knowledge of cognitive mechanisms at the symbolic level i have been considering in this paper. 
1 conclusion 
artificial intelligence is an empirical science with two major branches. one branch is concerned with building computer programs  and sometimes robots  to perform tasks that are regarded as requiring intelligence when they are performed by human beings. the other is concerned with building computer programs that simulate  and thereby serve as theories of  the thought processes of human beings engaged in these same tasks. i have directed my remarks to the outer edge of ai research belonging to the latter branch  where it is concerned with phenomena that are often regarded as ineffable  and not explainable by machine models. i have shown that  on the contrary  we have already had substantial success in constructing and implementating empirically tested information-processing theories that account for the phenomena of intuition  insight and inspiration. i have no immediate urge to predict how much further we shall go in the future or how fast. the continual progress on the 
	simon 	1 
journey over the past forty years has been speedy enough for me. 
   with the privilege that age carries  of being curmudgeonly  i have had some harsh things to say about philosophers and philosophy  perhaps no harsher than philosophers have had to say about ai . of course i am not really attacking a class of people called  philosophers'* but rather those people who think they can reach an understanding of the mind and of the philosophical questions surrounding it by methods other than those of empirical psychological science. traditional philosophy has much more to learn today from ai than ai has to learn from philosophy  for it is the human mind we must understand and understand as a physical symbol system - in order to advance our understanding of the classical questions that philosophers have labeled  epistemology  and  ontology  and the  mind-body problem   simon  1 . 
   moreover  it is not really the privilege of age i am claiming; rather  it is the privilege that comes from standing on a solid body of fact. i have mentioned a considerable number of these facts  drawn from papers in refereed journals or similarly credible sources. given the nature of this occasion  i may perhaps be pardoned for drawing a large portion of the facts i have cited from work in which i have been involved directly. i could have made an even stronger case if i had broadened the base  but i would have been familiar with fewer of the details. so if you want to calibrate my base of evidence  you can multiply it by a couple of orders of magnitude to take account of the work of all the other members of the ai and cognitive science communities who have been engaged in simulation of human thinking. 
in my account  i have tried carefully not to talk about 
 future hopes of understanding or modeling human thinking   but to confine myself to documented  easily replicable  present realities about our present capabilities for modeling and thereby explaining human thinking  even thinking of those kinds that require the processes we admiringly label  intuitive    insightful   and  inspired.  
   if i have been scornful of  some  philosophers  i hope i will not be thought scornful of human beings  or of our capacity to think. to explain a phenomenon is not to demean it. an astrophysical theory of the big bang or a three-dimensional chemical model of dna do not lessen the fascination of the heavens at night or the beauty of the unfolding of a flower. knowing how we think will not make us less admiring of good thinking. it may even make us better able to teach it. 
