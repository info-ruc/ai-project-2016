
kernel discriminant analysis  kda  is one of the most effective nonlinear techniques for dimensionality reduction and feature extraction. it can be applied to a wide range of applications involving highdimensional data  including images  gene expressions  and text data. this paper develops a new algorithm to further improve the overall performance of kda by effectively integrating the boosting and kda techniques. the proposed method  called boosting kernel discriminant analysis  bkda   possesses several appealing properties. first  like all kernel methods  it handles nonlinearity in a disciplined manner that is also computationally attractive; second  by introducing pairwise class discriminant information into the discriminant criterion and simultaneously employing boosting to robustly adjust the information  it further improves the classification accuracy; third  by calculating the significant discriminantinformationin the null space of the within-class scatter operator  it also effectively deals with the small sample size problem which is widely encountered in real-world applications for kda; fourth  by taking advantage of the boosting and kda techniques  it constitutes a strong ensemblebased kda framework. experimental results on gene expression data demonstratethe promising performance of the proposed methodology.
1 introduction
principal component analysis  pca  and linear discriminant analysis  lda  are two classical feature extraction and dimensionality reduction techniques broadly used in many tasks involving high-dimensional data. it is generally believed that for pattern classification problems such as face recognition and tissue classification of gene expression data  lda-based algorithms usually outperform pca-based ones. the reason is that the former optimizes the low-dimensional representation of the objects for classification by maximizing the ratio of the between-class scatter to the within-class scatter  while the latter simply optimizes object reconstruction without taking class information into consideration. many lda-based algorithms have been proposed. among them  as an attractive approach  some new lda algorithms  lu et al.  1a; masip and vitria`  1  which effectively integrate the boosting and lda techniques have been developed recently to further enhance the classification performance.
¡¡on the other hand  a major problem of linear subspace methods such as pca and lda is that they fail to extract nonlinear features representing higher-order statistics. in order to overcome this limitation  kernel dimensionality reduction techniques such as kernel principal component analysis  kpca   scho¡§lkopf et al.  1  and kernel discriminant analysis  kda   baudat and anouar  1; lu et al.  1b; dai and qian  1; xiong et al.  1; dai and yeung  1; zheng et al.  1  have been proposed recently to extend linear dimensionality reduction techniques to nonlinear versions by using the kernel trick  demonstrating better performance in many applications than that obtained using their linear counterparts. the basic idea is to first map each input data point x ¡Ê rn into the feature space f via a nonlinear mapping ¦Õ and then apply the corresponding linear dimensionality reduction algorithm in f. moreover  similar to their linear counterparts  kda-based methods are generally better than kpcabased methods for classification tasks. however  kda-based algorithms usually suffer from the small sample size problem  because the number of training examples available is usually smaller than the dimensionality of the feature space f especially for high-dimensional data. in order to overcome this problem  many approaches have been developed based on different criteria. recently  more effective solutions  dai and qian  1; zheng et al.  1  have been proposed to calculate the optimal discriminant vectors in the null space of the within-class scatter operator where the significant discriminant information exists. on the other hand  similar to conventional lda-based techniques  lotlikar and kothari  1; tang et al.  1; dai and yeung  1   the performance of kda degrades further due to the following deficiencies which are referred to as non-balanced problems in this paper:
1. for multi-class pattern classification problems  the optimal criterion based on the conventional between-class scatter is not directly related to classification accuracy. in particular  the corresponding dimensionality reduction procedure tends to overemphasizethe inter-class distances of well-separated outlier classes in the input space at the expense of classes that are close to each other  leading to significant overlap between them.
1. the expression of the average within-class scatter has animplicit assumption that all classes have the same weight
for the covariances. in fact  if the class with dominant covariance is simultaneously an outlier class in the input space  the within-class scatter will fail to estimate the correct value for improved classification  due to minimizing the spread of the outlier classes while neglecting the minimization of other covariances.
¡¡in this paper  we further improve the overall performance of kda by proposing a novel kda algorithm called boosting kda  bkda . the proposed approach effectively integrates the boosting technique with the most recently developed kda algorithms based on the pairwise class discriminant information. the bkda approach employs the boosting technique to robustly calculate the pairwise class discriminant information that is integrated into the scatter operators in order to solve the non-balanced problems of kda. although boosting has been applied to lda  so far it has not been studied much for kda. it is worthwhile to mention here several appealing properties of the proposed bkda method:
1. it handles nonlinearity in a disciplined manner that is alsocomputationally attractive like all kernel methods.
1. by introducing the pairwise class discriminant information into the discriminant criterion and simultaneously employing boosting to robustly adjust the information  it effectively overcomes the non-balanced problems in kda and further increases the classification accuracy.
1. it effectively boosts the significant discriminant information contained in the null space of the within-class scatter operator and simultaneously deals with the small sample size problem. however  the algorithm and analysis presented here can also be applied easily to boost other discriminant information such as that included in the orthogonalcomplementof the null space of the within-class scatter operator.
1. it constitutes a strong ensemble-based kernel kdaframework  taking advantage of both the boosting and kda techniques.
¡¡to demonstrate the effectiveness of the bkda method  we apply the proposed method to effectively extract discriminant features for the tissue classification of gene expression data. experimental results confirm that our bkda method is superior to several existing dimensionality reduction methods in terms of classification accuracy.
1 nonlinear feature extraction via kernel discriminant analysis
as a nonlinear extension of lda  kda essentially performs lda in the feature space f. note that f may be infinitedimensional and it should be regarded as a hilbert space. as such  the scatter matrices in the input space for lda correspond to operators in f for kda. for any operator a in f  we let a 1  denote the null space of a  i.e.  a 1  = {x|ax =

1}  and the orthogonal complement of a 1 .	thus 
a 1  ¨’ a 1  = f.
¡¡suppose x denotes a training set of n examples belonging to c classes  with each example being a vector in rn. let xi   x be the ith class containing ni examples  with xiio denoting the ioth example in it. in addition  each example xiio ¡Ê x has the corresponding label i ¡Ê {1 ... c}. by the implicit nonlinear mapping ¦Õ : rn ¡ú f  the n images in f can be represented by the set {¦Õ xiio }. the conventional between-class scatter operator s¦Õb   within-class scatter operator s¦Õw  and population scatter operator s¦Õt can be expressed as: s

s  where m and m.
we maximize the fisher criterion below to obtain the optimal projection directions w in f:
wts¦Õw
¡¡¡¡¡¡¦Õ	.	 1  wtsww
¡¡however  in many practical applications  one of the major problemswith kda is the so-called small sample size problem with respect to  1   because of the degeneracy of the withinclass scatter operator in f. in general  this problem is solved by applying techniques such as pseudoinverse  kernel pca  and qp decomposition. recently  some more effective methods  dai and qian  1; zheng et al.  1  have been developed to explore the significant discriminant information in the null space of the within-class scatter operator. in general  a simple and efficient approach of finding this significant discriminant information is to calculate the optimal discriminant vectors in the intersection subspace s with respect to the following modified criterion:
	.	 1 
furthermore  in many situations  nonlinear feature extraction based on the above criterion  1  indeed further enhances the overallperformancein terms of classification accuracyand numerical stability. in addition  it should be pointed out that  according to  chen et al.  1   for conventionallda  when the dimensionality n of the training examples is less than the total number of examples n  some useful discriminant information in the null space of the within-class scatter operator will be lost. this is especially the case when n is less than n   c because all this discriminant information will be fully discarded. in kda described above  this situation can be avoided by choosing the kernel function appropriately  with which the dimensionality of f nonlinearly mapped from the input space can become larger than the number of training examples.
1 boosting kernel discriminant analysis learner
1 boosting kernel discriminant analysis
similar to lda  kda also suffers from the non-balanced problems described in section 1. here  we will present the bkda algorithm that combines the strengths of the boosting and kda techniques and effectively solves the non-balanced problems of kda at the same time.
¡¡boosting is a general machine learning meta-algorithm for improving the accuracy of any given learning algorithm. one of the most effective boosting algorithms  referred to as adaboost  can be used in conjunction with many learning algorithms to improve their performance. boosting algorithms are adaptive in the sense that the classifiers built are tweaked in favor of those instances misclassified by previous classifiers  freund and schapire  1 . more specifically  the underlying idea of adaboost is based on the sample distribution  which  in essence  is a measure of how hard to classify an example. moreover  it should be notable that for multiclass problems  a version of adaboost called adaboost.m1 outperforms adaboost.m1 for many real-world applications. thus we prefer using adaboost.m1 in this paper. in order to effectively overcome the non-balanced problems of kda and simultaneously form a strong connection between kda and adaboost.m1  following  freund and schapire  1; lu et al.  1a   the pairwise class discriminant distribution is introduced on the basis of the mislabel distribution from adaboost.m1. further  at the tth iteration in adaboost.m1  any pairwise class discriminant distribution dti j between classes xi and xj can be calculated as follows:
	 	if;
otherwise.
 1 
 here  the mislabel distribution  measures the extent of the difficulty of discriminating the example  from the improper label   on the basis of the previous boosting results. obviously  a larger value of dti j intuitively indicates worse separability between two classes xi and xj  further embodying also that both are closer together in f.
¡¡to address the first non-balanced problem in kda  we replace the ordinary between-class scatter operator s¦Õb by a weighted between-class scatter operator s¦Õb:
	c	c
s
i=1 j=i+1
 where the weighting function is generally chosen to be a monotonically increasing function of . for simplicity  we let in this paper. obviously  based on the definitions of the pairwise class discriminant distribution and the weighting function  it can be seen that classes that are not well separated in f and thus can potentially impair the classification performance should be more heavily weighted in f.
¡¡in addition  a weighting scheme should also be employed to alleviate the second non-balanced problem  i.e.  the negative effect of outlier classes when estimating the within-class scatter operator. further  at the tth iteration  we replace the ordinary within-class scatter operator s¦Õw by the weighted withinclass scatter operator s¦Õw as follows:
	c	ni
	s	o	 
n
i=1 io=1
 where is the relevance-based weight for class is that for xiio. let us highlight some characteristics of s¦Õw here:
  by incorporating rit in  1   it ensures that the estimated
s¦Õw is only influenced slightly if class xi is an outlier class. this is reasonable since if one class is well separated from the other classes in f  then whether the within-class covariance operator of this class in the new space is compact or not will not have much effect on classification  tang et al.  1 .
  generally  a larger value of indicates a greater difficulty of classifying example xiio w.r.t. the previous boosting results. as such  qiit o emphasizes the difficult examples in the within-class covariance of class xi.
¡¡based on the adaboost.m1 algorithm and the definitions of s¦Õb and s¦Õw  we propose the bkda algorithm as detailed in fig.1 it should be pointed out that the kda involved in the bkda algorithm is to calculate the optimal discriminant vectors in the null space of the new s¦Õw in order to take advantage of the significant discriminant information in s¦Õw 1   and the detailed calculation procedure can be found in subsection 1 below. in addition  discriminant analysis is quite a strong feature extraction technique for classification. as a result  the boosting process cannot go forward due to the very small pseudo-loss t. in general  some sampling procedures are employedto artificially weaken the correspondingdiscriminant technique  and  in bkda  we choose some examples in each class based on to focus on the hardest examples in each class. for the features extracted by discriminant analysis  a simple nearest neighbor classifier is generally employed for classification. in order to be consistent with the adaboost algorithm  in bkda  the hypothesis between example  and class   can be built easily based on the normalized nearest neighbor classifier  while it gives results identical to the classical nearest neighbor classifier.
1 how to calculate the optimal discriminant vectors in the feature space f
in what follows  we will describe how to efficiently calculate the significant discriminant information in the null space of the variant s¦Õw in  1 . furthermore  on the basis of the variants s¦Õb and s¦Õw  we also represent the corresponding population scatter operator as s¦Õt = s¦Õb + s¦Õw. as described in section 1  the optimal discriminant vectors can be obtained in

s¦Õ	¦Õ	with respect to the following criterion:
	.	 1 
furthermore  based on the above criterion  we have to cal-

culate s	  which requires calculating s¦Õt 1  or

s¦Õw 1  first. however  the computation of s¦Õt 1  or s¦Õw 1  is quite intractable to some extent  due to the following reasons:

1  unlike s¦Õt   it is intractable to directly compute s¦Õt 1  by the eigenanalysis of s¦Õt  since s¦Õt cannot be explicitly expressed as s¦Õt = aat  where a is an operator with the explicit formulation; 1  the direct computation of s¦Õw 1  is very infeasible  since s¦Õw 1  is in general very large in f; 1  according to  cevikalp et al. can be indirectly calculated
by on the basis of first calculating s¦Õw 1   while  in f  high computational complexity is involved at the same time. in order to efficiently solve this problem  we provide the following theorem.
input:	a set of training examples x	= {xiio |xiio ¡Ê rn i = 1 ... c io = 1 ... ni}; a set of all mislabels m	=	{  i io  j |i j	¡Ê	{1 ... c} io	¡Ê ; the initial mislabel distribution on m: ; a small constant ¦Å.
 
for t = 1 ... tmax do
	  calculate the terms dti j by  1   and	in  1 .
  select s hardest examples per class based on qiit o to form a training subset st   x.
  apply kda in subsection 1 on st  and constitute the kda-based feature extraction technique  denoted by kda-t; apply kda-t on x to obtain {yiio t ¡Ê rr}; build the hypothesison the subset yt of {yiio t ¡Ê rr}  corresponding to st.
  calculate the pseudo-loss based on
.
  set	  then tmax = t   1 and break.
  update the mislabel distribution ¦£t: ¦£t+1 xiio j  =
¦£t xiio j ¦Ât 1+ht yiio t i  ht yiio t j  /1.
  normalize ¦£t+1: ¦£t+1 xiio j  =
i
.
end for
output: the final hypothesis: hf x  =
  where  for any
example x  yt ¡Ê rr is the corresponding nonlinear feature vector extracted by kda  kda-t  in subsection 1 over
st.figure 1: summary of bkda algorithm.
theorem 1. the subspace s
f} is equivalent to the subspace
1 x ¡Ê f}  where s¦Õt = s¦Õb +s¦Õw and s¦Õt is the conventional population scatter operator.
¡¡based on theorem 1  we thus propose to use the following steps to calculate the optimal discriminant vectors

for  1 :	1  calculate the orthonormal basis of s¦Õt  1 ; 1 

calculate the orthonormal basis of s¦Õw 1  in s¦Õt  1  to construct s ; 1  calculate the optimal discriminant vectors with respect to the discriminant criterion  1  in
s	.
¡¡in what follows  we will present the detailed computation procedure. from the discussions above  we first need to calcu-

late the orthonormal basis of s¦Õt  1  by applying kpca on s¦Õt . moreover  s¦Õt can be rewritten as:
c
	s 	 1 
i=1
where ¦µi =  ¦Õ x1i     m¦Õ ... ¦Õ xni i    m¦Õ  and ¦® = . since the dimensionality of s¦Õt in f is usually very high or even infinite-dimensional  kpca can be carried out by the eigenanalysis of ¦®t¦® instead  with size n¡Án. from the training set {¦Õ xiio }  an n¡Án matrix k can be defined as k =  kij i j=1 ... c where k
and. by the kernel trick  ¦®t¦® can be expressed as
	  	 1 
where 1 is an n ¡Á n matrix with all terms being one. let ¦Ël and el  l = 1 ... m  be the ith positive eigenvalue and the corresponding eigenvector of ¦®t¦®  respectively. then ¦Èl =
¦®el¦Ë l 1  l = 1 ... m  constitute the orthonormal basis of
  dai and qian  1; zheng et al.  1 . hence  any input x ¡Ê rn can be transformed into r in the low-dimensional space rm via kpca as follows:
	r 	 1 
where p =  ¦È1 ... ¦Èm   e =  e1¦Ë 1/1 ... em¦Ë m1  
i is	the	identity	matrix 	and	   with .
then  by  1   the training examples {xiio} in rn  or {¦Õ xiio } in f  can be mapped to the corresponding points {riio} in the low-dimensional space rm. the scatter matrices sb and sw in the low-dimensional space rm corresponding to s¦Õb and s¦Õw can be computed by either of the following two approaches  for simplicity  this paper adopts the first approach :
  by the corresponding definitions of s¦Õb and s¦Õw  sb and
sw can be reconstructed based on the training examples {riio} in rm;
  by the kernel trick  the scatter matrices can be directly computed using sb = pts¦Õbp and sw = pts¦Õwp  similar to kpca.
furthermore  we can directly calculate the null space of s¦Õw

in s¦Õt 1  by the eigenanalysis of sw  since sw is only an m ¡Á m matrix. more specifically  let v =  ¦Ã1 ... ¦Ãu  be the eigenvectors corresponding to the zero eigenvalues of sw  and then s can be spanned by pv.
as a result  the discriminant criterion  1  can be further transformed into the projection space of s by
jf z  = ztvtpts¦Õbpvz . let zl  l = 1 ... r  be the eigenvectors of vtsbv  sorted in descending order of the corresponding eigenvalues. according to  dai and qian  1; zheng et al.  1   it is clear that wl = pvzl  l = 1 ... r  constitute the optimal discriminant vectors with respect to the corresponding criterion
 1  in s . for an input pattern x  its corresponding nonlinear feature vector extracted by the kda procedure described above can be computed as y   where w =  w1 ... wr . this expression can be rewritten via the kernel trick as follows:
y.
 here  1 is an n ¡Á n matrix with all terms being one  i is the identity matrix  and k.
1 experimental results
gene expression data are usually high-dimensional involving a large number of genes but a small sample size. thus  how to effectively extract discriminant features plays a very important role in gene expression data classification  ye et al.  1 . to evaluate the performance of the proposed bkda algorithm  we conduct some gene expression classification experiments to compare bkda with some other dimensionality reduction methods.
¡¡since the non-balanced problems mentioned above only exist in multi-class classification problems  our experiments are performed on three different data sets involving more than two classes each in order to demonstrate the behavior of bkda:1
1. 1 tumors: 1 human tumor examples corresponding to 1 different cancer types;
1. a subset a of 1 tumors: contains all 1 human tumor examples corresponding to 1 different cancer types;
1. a subset b of 1 tumors: contains all examples corresponding to various human tumor and normal tissue types  each of which contains at least eight examples.
   each data set is randomly partitioned into disjoint training and test sets. for the training set x  each class xi contains l examples. since the first data set is less challenging than the second and third data sets  we set l = 1 for the first data set and l = 1 for the other two. moreover  in our experiments  no preprocessing such as gene selection is applied on the data sets. for each feature extraction method  we use a simple and efficient minimum mean distance rule with euclidean distance measure to assess the classification accuracy  and simultaneously build the corresponding normalized version for the hypothesis in bkda based on  lu et al.  1a . each experiment is repeated 1 times and the average classification rate is reported. for the kernelmethods  we use the rbf kernel and polynomial kernel  where ¦Ò = 1. in addition  it
should be noted that bkda can effectively boost the discrimination ability of the different features extracted by kda in bkda. to reduce the computational cost and simultaneously consider the space limitation in this paper  we simply fix the number of features r in bkda to c   1 for each data set  where c is the number of classes.
¡¡for comparison  the first set of experiments implements bkda and kda  zheng et al.  1  using both the polynomial and rbf kernels above  since kda  zheng et al.  1  can effectively calculate the significant discriminant information in the null space of the within-class scatter operator and the proposed method  ye et al.  1  for gene expression data classification is in essence a special case of kda in highdimensional spaces. furthermore  to explicitly show the effectiveness of bkda  in each comparison  kda offers two baselines: kda* denotes the maximum classification rate over the variant features; kda-r denotes the classification rate based on r fixed features. in addition  in bkda  the number of chosen examples per class is set to s = l 1  where l is the number of examples for each class in the training set. the experimental results shown in fig. 1 reveal that: as expected  bkda is capable of improving the overall performance for both kernel functions on all three data sets due to its advantages as discussed above; specially  bkda can effectively improve the performance of the corresponding kda-r on the same number of features. in addition  we find that when s is fixed at very small values  bkda fails to show its effectiveness when compared with kda* since the ensemble-based learner is too weak.

	 e 	 f 
figure 1: comparative performance of bkda and kda under different tmax values for bkda. bkda:training and bkda:test denote the results of bkda on the training set x and test set  respectively.  a  polynomial kernel on 1tumors;  b  rbf kernel on 1tumors;  c  polynomial kernel on subset a of 1tumors;  d  rbf kernel on subset a of 1tumors;  e  polynomial kernel on subset b of 1tumors;  f  rbf kernel on subset b of 1tumors.
¡¡the second experiment compares bkda with several effective linear dimensionality reduction methods  including bdlda  lu et al.  1a   lpp  he and niyogi  1   and npe  he et al.  1   and several other effective kernelbased nonlinear dimensionality reduction methods  including kpca  scho¡§lkopf et al.  1   gda  baudat and anouar  1   kdda  lu et al.  1b   wkda  dai and yeung  1   and kda/qr  xiong et al.  1 . table 1 reports the maximum classification rates of different methods on the three data sets. we can see that bkda is generally more effective than all other methods compared here.
¡¡notice that there has been extensive research based on classifiers for tissue classification of gene expression data  statnikov et al.  1   including k-nearest neighbor classifier table 1: maximum classification rates  %  of different methods on three data sets.
algorithm1 tumorssubset asubset bpoly.rbfpoly.rbfpoly.rbfkpca111111gda111111kdda111111wkda111111kda/qr111111our bkda111111bdlda111lpp111npe111 knn   decision tree  naive bayes classifier  bagging  boosting  support vector machine  svm   etc. however  our comparative experiments have not included these methods since we focus on the dimensionality reduction and feature extraction aspect in our study. in addition  over the past decade or so  dimensionality reduction techniques have been playing a very important role in face recognition. we have also applied bkda to face recognition based on the extended yaleb and pie databases  showing again that bkda outperforms all other dimensionality reduction methods compared. the results are not included in this paper due to space limitation.
1 conclusion
in this paper  a novel kda algorithm has been presented by incorporating the boosting technique into kda to give the bkda algorithm. this new algorithm effectively integrates the strengths of the boosting and kda techniques to give an ensemble-based kda framework with strong nonlinear feature extraction capability  and simultaneously overcomes the non-balanced and small sample size problems commonly encountered by kda-based methods. extensive empirical comparison of bkda with many other linear and nonlineardimensionality reduction methods on gene expression data classification shows that bkda is a very promising method.
1 acknowledgments
this research has been supported by competitive earmarked research grant 1 from the research grants council of the hong kong special administrative region  china.
