 
degrees of belief are formed using observed evidence and statistical background information. in this paper we examine the process of how prior degrees of belief derived from the evidence are combined with statistical data to form more specific degrees of belief. a statistical model for this process then is shown to vindicate the cross-entropy minimization principle as a rule for probabilistic default-inference. 
1 	introduction 
a knowledge based system incorporating reasoning with uncertain information gives rise to quantitative statements of two different kinds: statements expressing statistical information and statements of degrees of belief.  1% of applicants seeking employment at company x who are invited to an interview will get  a job there  is a statistical statement.  the likelihood that i will be invited for an interview if i apply for a job at company x is about 1'1 expresses a degree of belief. 
¡¡in this paper  both of these kinds of statements are regarded as probabilistic  i.e. 	the numbers appearing in these statements are assumed to obey the rules of probability theory. 	a degree of belief is viewed as a constraint on a set of possible  subjective  probability values. 
¡¡a very expressive extension of the language of firstorder logic for representing the two types of probabilistic information has been defined by halpern  and bacchus . for the purpose of the present paper  it will be sufficient to restrict attention to a much simpler language based on propositional logic. adapting the notation of halpern and bacchus  we will consider knowledge bases in an extension of the language l s  of propositional logic over the propositional variables s = { v 1   . . .   vn} incorporating expressions of the form 
		 i  
and 
 1  
where 
 1   and e is a new symbol not in s. 
¡¡the intended meaning of such a knowledge base is that formulas in the underlying propositional language express certain properties that a given object or  in a wider sense  a given event may or may not possess. formulas of the form  !  make a statement with what statistical probability property in the domain of discourse   holds given that property ' holds  and formulas of the form  1  are used to express that for the specific event it is believed that the probability of e having property   given that c has property   is 
¡¡using the propositional variables a     applicant   i  interview   and   the two example sen-
tences above may be symbolized by = 1 and prob l 1  |   t o which the definite knowledge job without an application and an interview   might be added.  the symbols - and  here used are definable abbreviations  not an extension of the syntax.  
¡¡semantics for this representation language are given by probability measures on l s : one probability measure interprets the statistical expressions  1   and one probability measure v  is needed for every event symbol e used in the knowledge base to interpret the degree of belief expressions  1 . 
¡¡a probability measure on l s  is fully determined by the probability values of the n :- 1  atoms of the language  i.e. the expressions in the set 
we denote by r. 
usually  the constraints  1  on the statistical measure 
;;   and constraints  1  on the belief measure v¡ê given in the knowledge base will only lead to reasonably narrow bounds for probabilities on a few formulas  while others remain largelv undetermined. in this situation we will be looking for a rule of probabilistic inference that selects from the multitude of probability measures consistent with the given constraints the ones that seem to be most reasonable or plausible. 
¡¡several types of inference rules here can be distinguished. first  there are those that only apply to sets of constraints on probabilities of a single kind. since in many applications only one kind of probabilistic information is represented in the knowledge base  this type 
	jaeger 	1 
¡¡
of inference is the one most frequently encountered. entropy maximization is the most common rule of this type. a second type of probabilistic inference is given by the random worlds formalism of bacchus et al . here constraints on statistical probabilities are used to derive degrees of belief. in  bacchus et ai  1  it is also shown how this method can be extended to make the resulting subjective probabilities also depend on given prior degrees of belief. it is this third kind of probabilistic inference  where information about both statistical and subjective probabilities are used to complete the set of degrees of belief  that we  too  are concerned with in this paper. 
¡¡that statistical information is relevant for the formation of degrees of belief appears to be obvious: given the statements of the introductory example - and no further information whether i'm either more or less likely than everybody else to get a job after an interview - i will conclude that my chances for finding employment at company x are approximately 1. 
¡¡this form of reasoning has been called default reasoning about probabilities in  jaeger  1a   because  just 
as in logical default reasoning  information about what  is generally true in the domain of discourse is used to derive conclusions about specific objects not strictly implied by the knowledge base. in this example the default inference only consisted of {a slightly generalized form of  direct inference   carnap  1    which is not applicable in more general cases. in  jaeger  1b  it was therefore proposed to use cross-entropy minimization as a generalization of direct inference that is applicable under much more general circumstances. this was motivated mainly by the logical properties that the resulting inference rule possesses - properties that are intuitively reasonable. in the present paper we are taking a more fundamental approach to the issue: first  it is undertaken to provide a precise epistemological analysis of the principles that underly default reasoning about probabilities. from this analysis a concrete statistical model for the interpretation of the probabilistic information will be derived. it then turns out that this model validates the cross-entropy minimization principle for default reasoning about probabilities. 
1 	the formation of beliefs: an interpretation 
in this section we will derive an analysis of the principles underlying default reasoning about probabilities. two examples will serve as a guide towards this analysis. 
e x a m p l e 1 i'm playing a game of dice with a friend who just has made the roll of the die that will decide the game: if she rolls a four or better  she wins; if a three or less turns up  i win. the die has come to rest out of my sight  but the outcome has been observed by my friend. by the somewhat satisfied expression on her face i gather that i will less likely have won than lost this game.  less likely  i'm here willing to quantify by a probability of 1  so that my degree of belief in the current toss t having the property p := r1 v r1 v r1 with r1 :=   i has turned up  is given by the subjective 
1 	reasoning under uncertainty 
probability 1. 
e x a m p l e 1 scanning channels on tv we tune in to a mystery film /. we just catch the last part of a spectacular car chase  apparently taking place in a european city. these two observations induce us to believe that the film has been an expensive production with probability   1  and is of american origin only with probability   1. 
¡¡the two uncertain events described in these examples are of a somewhat different nature: the first one is a product of what can only be understood as a random process. the uncertain event in the second example  however  is not random in the classical sense that a toss of a die  or the drawing of a card from a shuffled deck is random. the film  a fragment of which we have happened to see on tv  is not broadcast at that time as a result of being drawn from a gigantic urn containing all mystery films. given that we are partially ignorant of the deterministic chain of events that led to the screening of that particular film at that particular time  however  for us endows this events with all the features of randomness. some partial knowledge we may possess of the actual chain of events causing the given observation  and the ignorance about some other of it's parts  combines to the imperfect  perception of that chain of events as a random mechanism. 
¡¡interpreting an observed uncertain event r as a realization of some random mechanism  provides a means for defining one's degrees of belief: based on our model of a 
¡¡random mechanism  we can consider a long  hypothetical  sequence of events that are independent realizations of the same random mechanism. moreover  we can imagine all the elements of the sequence to provide us with the same evidence as c. such an imaginary sequence of events we call a thought experiment. our degree of belief  point- or multi-valued  that e has property ¦Õ now can be defined as a bound on the relative frequency with which we imagine events in the thought experiment to have 1. 
¡¡this interpretation of degrees of belief is the content of the following postulate. 
postulate 1: the degree of belief that an uncertain event c has property ¦Õ is the predicted hound on the relative frequency of ¦Õ in a long  imaginary  sequence of events  each of which  is a realization of the random mechanism modeling the chain of events that produced e  and each of which provides the same evidence that has been observed in e. 
¡¡speaking of a  long sequence of events  here is a somewhat sloppy terminology. in principle  a thought experiment must be considered as an infinite sequence of events; degrees of belief are defined by a prediction of the limiting relative frequencies in initial segments of increasing length from this sequence. 
¡¡by this postulate it is not claimed that we are always able to precisely specify a random mechanism corresponding to the observed event in the sense of reducing it  for example  to the random draw from a set of welldefined alternatives according to well-defined chances. 
our image of the random mechanism may well contain unknown parameters. 
¡¡for an illustration of this  consider a variation of example 1: suppose that i have a vague suspicion that my friend has a loaded die up her sleeve that enables her to roll a six at will  and that she occasionally will use this die instead of the fair one. finding myself in the same situation as described previously  i will now have to incorporate into my model of the random mechanism that produced the crucial toss of the die the possibility that in that toss the loaded die was in fact supplanted for the fair one. the result might be a model of a random mechanism consisting of first a random draw of one of either a fair or a loaded die  and a subsequent toss of that die. however  feeling unable to evaluate the likelihood for my friend to have cheated at the observed toss  i am unable to specify the respective probabilities for the two dice to be drawn. this makes my thought experiment  depend on an unknown parameter. depending on it's value  the predicted frequency of p will have any value between 1  always a loaded die is being tossed   and 1  only the fair die is being used . consequently  my degree of belief in p t  now will be the interval  1.1   
¡¡in our interpretation  then  the vagueness of a degree of belief in part is caused by an uncertainty about the parameters of the thought experiment. 
¡¡postulate 1 gives a semantic interpretation of the meaning of a degree of belief  but does not attempt to give a rule for their computation. particularly  the question of how to construct a random mechanism for the thought experiment  and how to translate the evidence into a predicted bias for the outcome of realizations of the random mechanism  are outside the scope of the statement made in that postulate. they  too  are outside the scope of this paper  where our immediate concern is with interpreting knowledge bases including statements of degrees of belief  but not containing the primary evidence which initiated these degrees of belief. 
¡¡the interpretation of degrees of belief here given  however  does provide guidance for finding a specific rule by which degrees of belief stated in the knowledge base should be combined with statistical information. 
¡¡example 1  continued : what  in the situation described previously  will be my degree of belief in the proposition r  t   i = 1 1   the observation i have made only provides evidence that bears on the probability of p t   but does allow me to discriminate between the three alternatives ri t   r1 t x r;s 1- however  i do have the information that the statistical probability of each of the r  in tosses of a fair die is 1. specifically  this means that each of the r  has an equal statistical probability. this statistical knowledge determines my prediction of the outcome of the thought experiment associated with the present event t: i will expect that here  too  each of the three alternatives r1 r1 r1 will appear with equal frequency 1/1 = 1. similarly  for i = 1 1  a degree of belief 1/1 will be assigned to r  t . 
¡¡example 1  continued : while1 a commercial break has stopped the flow of useful information  we have time to make up our mind whether we want to continue watching that mystery film. having a preference for films with a happy end  we first attempt to estimate the likelihood for this film to have one. none of the evidence provided in the short scene we have seen directly suggests either a happy or an unhappy ending. fortunately  however  we do have recourse to statistical information with what relative frequency happy endings have occurred in the great number of mystery films  distinguished by their having combinations of the properties a   american  and e   expensive   shown on television in the last few years. using our syntax for the representation of statistical probabilities  let this information consist  of 

here it is far from obvious what prediction for the relative frequency of happy endings in the thought experiment we should derive from these statistics and our prior predictions about the frequencies of a and e. it is easy  though  to obtain some bounds for the plausible values of this frequency. 
¡¡for an upper bound we may suppose that in the hypothetical sequence of mystery films the relative frequency 
of those of the four properties is maximal {within the given bounds that the relative frequency of property a is at most 1  and that of e at least 1  for which the conditional statistical probability  he | ¡ö  has the greatest values. this is achieved by assuming an outcome of the thought experiment in which both the relative frequency of are 1  i.e. every film in fact turns out to be expensive  and the number of american films is maximal. for such a sequence then a relative frequency 

of happy endings should be predicted. 
¡¡similarly  by considering an outcome of the thought experiment in which the number of expensive or american films is minimal  a lower bound of 1 is obtained for the expected frequency of he. 
¡¡what is the rationale for using statistical information  in the way described by these examples  for the prediction of the outcome of a thought experiment  clearly  here a close connection between the random mechanism  realizations of which constitute our thought experiment  and the statistical probability distribution  partially  described by the statistical data must be assumed: our understanding of the random mechanism producing the toss of the die in example 1 is characterized by the assumption that we observed an unmanipulated toss of a fair die. in the film-example the screening of that film at that time is perceived to be a random draw from the set of all screenings of mystery films by arbitrary networks at arbitrary times. 
¡¡thus  in both examples the random mechanism used as an explanation of the chain of events producing the observed event is equivalent to the statistical distribution 
	jaeger 	1 
- equivalent in the sense that when we consider an arbitrary series of realizations of the random mechanism  i.e. one in which it is not supposed that each realization supplies us with some specific evidence  then we would predict that the relative frequencies in this series agree with the statistical data. 
postulate 1: default reasoning about probabilities rests on the assumption that the observed event e is a realization of a random mechanism  equivalent to the statistical 
probability 	distribution. 
¡¡postulate 1 only describes a precondition that must be fulfilled in order to combine degrees of belief with statistical information. it gives no hint whatsoever by what operational rule this combination will actually be performed. 
¡¡a key observation that will be instrumental for a derivation of a specific analytical rule for this combination can be made by reconsidering the arguments used above in deriving bounds on r1 f  and he / : in both cases  the predictions for the relative frequencies of these properties in the thought experiments as  respectively  1 and  1 1  were obtained by only arguing from the prior beliefs derived from the evidence  and from the statistical data  but were completely independent of the evidence itself. 
¡¡when from a prior subjective probability of 1 for p t   and the statistical data available for tosses of fair dice  a degree of belief of 1 is derived for r1  t   this is done by simply considering a random sample of tosses of a die  in which the relative frequency of the property p happens to be 1. for this imaginary sample it is no longer necessary to assume that each of it's elements occurs in a setting analogous to the one; of the original toss. similarly in the film example; assume that the scene we have seen does not provide any more relevant information with respect to the actual film / having any of the properties a  e or he. then  in order to predict the relative frequency of he in the thought experiment associated with /  an arbitrary sample of mystery films with less than one half american and more than 1% expensive productions will be considered. if the original film happens to be black and white  and we have no statistical information referring to the property of being black and white  then we will not assume that every film in the random sample is black and white too  this property being recognized as irrelevant. 
¡¡to obtain a more precise notion of what it means that the given evidence does not provide any more relevant information  we say that a set  of degrees of belief exhausts the evidence with respect to l s  if  based on the evidence alone  and without any statistical information  we are unable to assign degrees of belief to properties definable in l s  any more specific than the ones in ty. the way in which statistical data is used to define degrees of belief now is described in a third postulate. 
postulate 1: //  is a set of degrees of belief exhausting the evidence obtained about an event c with respect to l s   then the predicted frequency of a property ¦Õ € l s  
1 	reasoning under uncertainty 
in the thought experiment associated with e is calculated as the expected relative frequency of the property ¦Õ in a large random sample of events  given that the relative frequencies of p r o p e r t i e s i n that sample is within the bounds prescribed 
¡¡as before in postulate 1  it was here preferred to use the imprecise term  large sample1'  when  in fact  we should more accurately speak about limiting frequencies as the sample size tends towards infinity. 
1 	the statistical model 
to implement the rule for the derivation of degrees of belief formulated in postulate 1 in a mathematical model  the concepts of a random sample and relative frequency of a property in such a sample  which  as yet  have only been used intuitively  must be formalized. 
¡¡the mathematical model for a random observation of a single event  is provided by a random variable: a function defined on some probability space equipped with a probability measure p  taking values in the set of possible events. since we distinguish different events only with respect to properties definable in l s   we may use the simpler model of a random variable taking values in t  this being the set. of equivalence classes of events with regard to these properties . such a random variable a' now is a model of a randomly sampled event  if it's distribution is equal to the statistical probability measure u on t  i.e. 

¡¡
¡¡theorem 1 provides a clear answer to what frequency of ¦Õ e l s  we should expect in the random samples described by postulate 1  provided is closed and convex  as is the case when is defined by a set of sentences  1 : when looking at sufficiently large samples  with a probability arbitrarily close to certainty  this relative frequency will be arbitrarily close to ¦× ¦Ì  ¦µ . 
¡¡for the die-example  the minimum cross-entropy solution for the given constraints and statistical distribution is   1 . 1   . . .   1  1/1 ...  1/1 . the upper bound 
of 1 derived for prob he /   in the film- example corresponds to the. minimum cross-entropy measure with respect to the statistical distribution with //. e  = 1 and // a | e  = 1. the lower bound derives from statistical measures ¦Ì with ¦Ì a  = 1 and ¦Ì e    1. the minimum cross-entropy measure ¦Ì for other statistical measures ¦Ì satisfying the statistical constraints of example 1 will yield values ¦Ì he  in between 1 and 1.  all these results are derivable from elementary properties of rross-entropy minimization  e.g. the axioms given in  shore and johnson  1 .  
¡¡by the epistemic analysis of section 1  we obtain a good insight under what conditions  an ideal agent's  default reasoning about probabilities  when reconstructed from information given in a formal knowledge base  is adequately modeled by cross-entropy minimization: first  we must make the assumption of postulate 1  i.e. that the agent who's degrees of belief are encoded in the knowledge base considers the random mechanism he or she associates with the event e to be equivalent to the statistical probabilities stated in the knowledge base. second  it must be assumed that the given degrees of belief exhaust the evidence  i.e. that the knowledge base reflects all the relevant information the agent has about e. observe that this second condition is a typical idealization that always has to be made to justify application of a non-monotonic inference rule  probabilistic or logical  to a knowledge base. 
	1 	comparison and conclusion 
traditionally  the meaning of degrees of belief often is defined in terms of preferences between acts  e.g. the acceptance of certain bets   the utility of which will depend on some uncertain proposition. by eliciting from a person suitable statements of preference  his or her degree of belief about the proposition can be defined by a unique  subjective  probability value. the most  influential presentation of this approach probably is  savage  1 . this view of degrees of belief is stronger than the one we used here  in the sense that they are always defined to be point-valued. nevertheless  the two definitions ere not incompatible: the thought experiment explanation focuses on how an agent arrives at a degree of belief without  trying to prescribe a method by which unique values will always result. the preferenceparadigm concentrates on the measurement of degrees of belief  which can very well be imagined to have been formed by a thought  experiment. 
¡¡shafer and tversky  speak of  mental experiments1' that are performed to obtain probability judgments. unlike the thought experiments described by 
	jaeger 	1 
¡¡
postulate 1  shafer and tversky's mental experiments are not an abstract epistemic model for the meaning of degrees of belief  but designate a variety of ways in which  in concrete situations  specific evidence can be compared to well-defined chances. thus  the  mental  drawing of a random sample of events according to some known statistics  as described in postulate 1  constitutes a mental experiment in the sense of shafer and tversky. 
¡¡paris and vencovska  have analyzed the problem of probabilistic inference from the same kind of knowledge bases as considered here. they base their approach on the semantic interpretation that a subjective probability represented by prob 1 e   in fact describes a statistical probability  ¦µ| s  : the statistical probability of ¦µ in the ideal reference class sr of elements that are  similar  to e. as a natural consequence of this view  there is little room for the distinction of different types of inference rules made in section 1: since essentially we are left with only one type of probabilities  there is only room for inference rules to be applied simultaneously to degrees of belief and statistics. 
¡¡paris and vencovska show that when entropymaximization is applied to their knowledge bases  which must also include a clause stating that  se  is small   then the effect of the general statistical information on the inferences made about the specific statistical terms  ¦µ | sc  is defined by cross-entropy minimization. together with a justification of the maximum entropy method   paris and vencovska  1    this provides a justification for minimum cross-entropy inferences. this derivation of the minimum cross-entropy principle  however  is of a completely different nature than the one presented here  because the justification of the maximumentropy method is based on logical arguments alone  just as in the well known work by shore and johnson  : it  is shown that if an inference process satisfies certain logical principles  i.e. behaves adequately when applied to knowledge bases of certain syntactic structures  then it will have to be entropy maximization. 
¡¡an argument of this kind can only be used to show that cross-entropy minimization is the adequate formalism for default reasoning about probabilities when it is taken for granted that at least one such formal process exists an assumption that in itself is not corroborated by an axiomatic derivation. it  might  very well be that there are other axioms that are intuitively reasonable for default reasoning about  probabilities  but are not satisfied by the minimum cross-entropy principle. in that case we would have to conclude that no completely adequate formal process exists. for this reason it has here been attempted to elucidate the process of the formation of degrees of belief based on statistical information in human reasoning by looking at its epistemic basis rather than by giving a normative  partial  description of its behaviour. it was then shown that with the unfolding interpretation of a degree of belief as a prediction of the outcome of a thought experiment  the reasoning process itself can be captured in a statistical model validating minimum cross-entropy reasoning. such a derivation of the minimum cross-entropy principle from a semantic model provides valuable evidence that it does  in fact  
1 	reasoning under uncertainty 
not have counterintuitive logical properties  since these would have to correspond to flaws in the semantic  model. 
