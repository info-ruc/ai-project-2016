 
the faim-1 is an ultra-concurrent symbolic multiprocessor which attempts to significantly improve the performance of ai systems. the system includes a language in which concurrent ai application programs can be written  a machine which provides direct hardware support for the language  and a resource allocation mechanism which maps programs onto the machine in order to exploit the program's concurrency in an efficient manner at run-time. the paper provides a brief synopsis of the nature of the language and resource allocation mechanism  but is primarily concerned with the description of the physical architecture of the machine. the architecture is consistent with high performance vlsi implementation and packaging technology  and is easily extended to include arbitrary numbers of processors. 
	i 	introduction 
   the goal of the faim-1 machine is to provide a high performance symbolic multiprocessor  one hundred or more times faster than current machines in common use  e.g.  a symbolics 
1  to meet the voracious computational demands of future artificial intelligence applications. this implies a real machine - one that works  is affordable and that people can program. such a machine should entice researchers into the area of distributed ai problem solving and encourage its widespread use in the research community. it is hoped that such usage will facilitate development of the necessary expertise to make sophisticated  cost-effective machine intelligence applications practical. the faim-1 is also an architecture which is conveniently extensible  both in terms of scale  number of processors  and for future improvements to incorporate the benefits of new technology and systems ideas. 
   the system's functionality is primarily motivated from the top by the needs of ai symbolic computation  but the system structure is also restricted by the need to produce a high-performance  cost-effective system in an available technology. we feel that it is necessary to provide a consistent system which is designed from first principles to meet the needs of ai applications rather than adopting an ad hoc combination of systems ideas and components that were developed for sequential  primarily numeric applications. such a system must also be complete enough to permit viable use and evaluation. the faim-1 system therefore includes a language  programming environment  architecture  a hardware prototype is under development   and a resource allocation mechanism. the focus of this paper is to describe the physical architecture of the faim-1 system. a brief synopsis of the language and resource allocation strategy will be presented in order to provide some perspective for the architecture in the context of the overall system. 
   in order to achieve our goals for an appreciably higher performance generation of intelligent machine systems based on concurrent multiprocessing  it is necessary to make a significant break with conventional architectural principles. some of the traditional mechanisms simply are not viable in a highly concurrent environment. on the other hand  a dramatic shift of computational base from sequential to concurrent processing will be difficult after 1 years of highly refined experience with uniprocessing. in practice programmers are not going to readily make the difficult shift if the new systems require a significant change in the style in which they solve problems  or if the speed of the target machine is too slow to motivate the effort. our approach in satisfying these conflicting constraints is to provide a reasonably small shift in thinking at the programming language level in order to incorporate concurrency  while making major changes in the structure of both the system software and hardware architecture in order to achieve the necessary level of system performance. 
   the design of the faim-1 system is intended to exploit concurrency at all levels of the system  and to pursue technological performance mechanisms in the implementation of the prototype hardware. 
	ii 	language and resource allocation 
a. 	the o i l language 
   oil  our intermediate language  can be viewed both as the kernel of a high level  concurrent  ai symbolic programming language and as the machine language for the faim-1 multiprocessing system. the design of oil was primarily influenced by current ai programming practices. commonly used are languages for knowledge representation  logic programming  object-oriented programming  production rules  procedural code  etc. future complex ai applications may require several  or all  of these programming styles. emulating one programming style within another is inefficient  and therefore a need exists for a better linguistic mechanism that efficiently supports many of the major styles. oil can be viewed as a blend of object-oriented  logic  and procedural programming semantics into a single and internally consistent linguistic framework. effort has been made to retain as much as possible from existing and familiar ai programming languages. some modifications to familiar mechanisms have been incorporated into oil which were primarily induced by the need to provide concurrent semantics where possible. 
   an oil program is a collection of objects that communicate by sending messages. each object is viewed semantically as an independent and therefore potentially concurrent program module. the communication structure explicitly indicates the level of concurrency represented by the program. an oil object consists of some local state information  local variables and data structures  and several porta through which messages are sent and received in fifo order. a behavior  associated with each port  
   
describes what the object does in response to a message. the behavior is a program  that may modify the local state and/or send messages to other objects. atomic oil objects are of two distinct types: logical and procedural. 
   logical behaviors are written in a declarative style that is essentially a parallel version of prolog  clocksin and mellish  1 . numerous parallel logic programming semantics are being investigated within the research community. oil logical behaviors are semantically or-parallel  restricted and-parallel  degroot  1  logic programs. 
   procedural behaviors are written imperatively in a parallel lexically scoped dialect of lisp based on the language t  rees et a/.  1 . behaviors can be nested heterogeneously to form other objects to permit control to pass between declarative and imperative behaviors. 
   the programmer may also annotate the oil code with pragmas which are used to provide information about the dynamic aspects of how the code may behave at run time. these programmer supplied hints are used by the static resource allocator in an attempt to properly partition the concurrency extant in the program onto the physical resources of the machine. the goal of this activity is to maximise the benefit of concurrent operation at run time. this pragma information gives the programmer optional control over some aspects of the allocation strategy. pragmas  as the name implies  are purely pragmatic information and are therefore orthogonal to the functional correctness of the program itself. if no pragma information is supplied  the program will still run although perhaps not efficiently. 
b. 	resource allocation 
   given a program that is a collection of concurrent tasks and a machine composed of a number of processors on which to run the program  there is an inherent problem of how to allocate the tasks onto the physical resources in an efficient manner. in systems such as the cosmic cube  seitz  1  the burden of task allocation is left for the programmer; for certain highly regular cases this is neither a complex nor difficult task. in general however  it is important that the task structure reflect the programmer's organization of the solution and be independent of the machine architecture. efficient task allocation is a critical problem that must be solved if advanced  highly concurrent machine systems are to mature and be truly useful. several mechanisms have been studied  but generally three main options exist: 
1. p r o g r a m m e d resource allocation relies on a smart programmer to write specific load modules for each individual processing element  pe . the disadvantage is that the task may be complex and the solution non-intuitive. an additional problem is that such code does not port to new machine structures  and therefore effectively returns to the dark ages of machine dependent programming. the advantage is that in many cases the programmer knows the optimal allocation better than any automatic mechanism. 
1. d y n a m i c resource allocation employs a smart operating system to observe how load is distributed in the system. if an inefficient allocation exists  then the operating system redistributes some of the load from busy processors to lightly loaded or idle processors. the advantage of this mechanism is that if the load changes rapidly  then neither the programmed nor the static mechanisms can adapt properly. the drawback is that the overhead of dynamic allocation must be paid at runtime and therefore can decrease system performance. 
1. static resource allocation relies on a smart compiler to analyse the program text and to partition the resulting object 
	a. davis and s. robison 	1 
code into a set of cooperating  concurrent sub tasks that conform to the grain size of the pe's and their interconnection topology. the primary advantage of this mechanism is that it does not directly increase the programmer's burden and also does not diminish the run-time performance of the system. the primary disadvantages are that the compiler based resource allocator can be very complex  and if the program exhibits highly dynamic behavior  then the result may be far from optimal. 
   since the major goal of the faim-1 system is very high performance  the focus for resource allocation strategies is on static methods. aspects of both the programmed and dynamic methods exist in that pragmas are both a way in which the programmer can influence allocation decisions  and a mechanism by which predictions about dynamic run-time behavior can be specified. due to the highly dynamic nature of many ai application programs  some form of additional dynamic load balancing support will also need to be provided by the run-time kernel. however  our present interest is to experiment with how far we can push static methods. the faim-1 static resource allocation decisions are made in a post-compiler process called the allocator. 
   the intermediate compiled code and the associated pragmas are passed to the allocator by the oil compiler. the allocator then performs a dataflow analysis on the procedural code  a communication connectivity analysis on the program objects  and an inference connectivity analysis on the logical behaviors to produce a directed-program graph. this graph is then manipulated into a more abstract graph form which encapsulates much of the program graph detail. this intermediate graph form is then embedded into the machine graph representing the physical machine resources via a simulated annealing process  kirkpatrick et al.  1 . while optimal graph embedding is in general an np hard problem  the method used here fortunately is not concerned with an optimal allocation and runs in polynomial time in order to find an adequate allocation partition. 
	ill 	faim-1 system architecture 
   the primary purpose of the faim-1 architecture is the efficient  high-performance execution of oil programs. the ob-
jective is accomplished by supporting  directly in hardware  the computational model on which oil is based. 
   the faim-1 is a multiprocessor system consisting of a number of identical processing elements  called hectogons  the result of an inside joke with no other interesting meaning   interconnected by a communication network. each hectogon is a complete  selftimed computer capable of sequentially executing any compiled oil program that can be stored in its local memory. hectogons communicate with each other via messages which are sent through communication ports. note the similarity between this model and the organization of an oil program. each hectogon has 1 ports that may potentially be concurrently active. communication lines run between ports on different hectogons; the exact configuration of connections is called the communication topology. 
   a hectogon is composed of 1 self-timed  seitz  1  subsystems - named the frisc  ism  cxam  spun  sram  and post office. three of these subsystems  ism  cxam  sram  are specialised memory systems that provide intelligent storage  the post office supports inter-hectogon communication  the frisc element is the processor  and unification support is provided by the spun element. for the initial 1 processor faim-1 prototype  four of the subsystems are being implemented as custom cmos vlsi components and the other two  spun and sram  
   
1 a. davis and s. robison 
subsystems are being constructed from commercially available components. 
   one of the key features of this design is to experiment with the utility of specialised storage subsystems to relieve the processor from much of the storage management duties which are typically very time consuming in ai applications. these intelligent memory components permit a higher level of processor memory interaction  which inherently alleviates the classic memory bottleneck of conventional von neumann architectures. 
   each of the 1 subsystems can be be viewed as concurrent system modules  and as an ensemble they support a considerable level of concurrent activity within the confines of a single processing element. larger grain concurrency  corresponding to parallel program tasks  is exploited by distributing the tasks across multiple processing elements. 
	a. 	communication topology 
   the faim-1 communication topology is divided into two levels. at the b o t t o m level  hectogon elements are wired together to form a processing surface. at the top level  an arbitrary number of processing surfaces are connected together to produce a multiple surface instance of a faim-1 system. a processing surface is a planar hexagonal mesh structure and hectogons which are on the edges of the plane are called peripheral. when a connection leaves the surface from a peripheral hectogon's ports  it is routed to a simple 1-ported switch. one of the remaining ports of the switch is used for connection to an adjacent surface while the other is wrapped around to a switch on the opposite edge of the same hexagonal surface. figure 1 shows a 1 processor surface  with the switches and wrap lines on one axis only. the switches and wrap lines have been omitted on the other two axes for clarity. this wrapped  hexagonal mesh is a 1-axis variant of a twisted toroidal topology  martin  1 . this particular variant has a communication diameter of n-1 where n is the number of processors on an edge of the surface and each wrap is twisted n-l increments on each axis. the n-l twist creates a provably optimal switching diameter and can be supported by a rather simple routing algorithm which scales to permit multiple surfaces to be interconnected as well. this n-l twist topology can also be viewed as a uniform hexagonal mesh of processors which covers the surface of a sphere. 
hex display with one wrapped axis 

figure 1: an e-1 processing surface 
the wrap lines reduce the maximum communication distance between elements on the surface. it is hoped that the resource allocation strategy will be able to allocate tasks onto this surface such that a high degree of communication locality will be achieved. however  it is unlikely that strict locality can be efficiently achieved for highly dynamic ai programs. therefore reducing worst-case communication times between non-local processing elements  by reducing communication distance  is also important. 
   multiple faim-1 surfaces can be arbitrarily tessellated in a planar array  by connecting the adjacent hectogon off-surface switch wires. figure 1 illustrates this multi-surface interconnection plan. in this figure  hectogons on the same surface have similar textures. the switches and wrap lines have been omitted for clarity. the tiling of a plane with several processor surfaces produces a large number of peripheral ports that can be attached to non-hectogon devices. this is useful for i/o purposes  providing a large number of connections to secondary storage units or a host processor. by varying the surface size and the number of surfaces that are connected together it is possible to produce a system containing any desired number of processors. 

figure 1: the tiling of multiple surfaces 
   the primary advantage in using a hex communication topology is that it is easily extensible. the periphery of a processing surface forms a regular hexagon. in the 1 processor instance  each peripheral edge contains 1 processing elements. this particular configuration is therefore called an ¡ê-1 surface. an interesting artifact of this surface configuration  is that up to an ¡ê-1 surface  the number of processing elements on a surface is a prime number. this can be exploited during surface initialization to concurrently determine individual hectogon status and identity. 
   a secondary advantage of the surface topology is that the onsurface wiring scheme is strictly planar and therefore amenable to wafer-scale packaging  either as a hybrid or a full wafer-scale integration design. we do not anticipate using wafer-scale integration in the near term due to the inherent problems associated with yield induced component failures. however  immersion cooled  hybrid wafer-scale packaging  stopper  1  is an attractive option. in general it will be important for future highperformance architectures to be amenable to fabrication in modern circuit and packaging technology. namely it does not make sense for multiprocessor architectures which exploit concurrency for increased performance to sacrifice an order of magnitude in speed due to a poor choice of implementation technology. 
fault tolerance is an important aspect of any highly replicated 
   
multiprocessor architecture  since the probability of at least one processor being down increases with the number of processors in the system. homogeneous multiprocessor architectures intrinsically contain redundant elements which could be used to support fault tolerant behavior. unfortunately most of these architectures to date have not utilised this feature  and have by default become fault intolerant. koren  gordon et a/.  1  has shown that hexagonal mesh structures are particularly attractive fault tolerant topologies. in addition  communication fault tolerance is enhanced in a hex mesh topology because each element has a number of paths by which it may send messages to any particular destination. 
b. 	pott office 
   hectogons communicate with each other by sending messages. the post office subsystem is a highly concurrent  autonomous communications controller. it is responsible for the physical delivery of messages which are sent between communicating hectogons. initially messages are created by the frisc and linked into the deliverable-messages list stored in the sram. the post ofbce can independently access this structure and route individual messages to their intended destinations. 
   messages are variable length structures and contain two subfields: a list of destinations  and the message body. a destination is a relative address which indicates the physical offset of the receiving hectogon in the hexagonal mesh. physically  faim-1 post offices form an independent packet routing network on the hexagonal mesh topology. messages are therefore decomposed by the pobt office into a series of fixed length packets. each packet contains a destination and a packet body. routing of individual packets is done separately and recomposition of a message from a collection of packets is done by the receiving post office. this implies that packet arrival at the destination post office is inherently unordered  since several physical routing paths may be used for member packets of a particular message. while this increases the level of responsibility in the receiving post office  it permits congestion delays to be avoided dynamically in the switching topology. 
   routing decisions are made algorithmically in the post office by a finite state machine as follows: 
  if the destination is one of the neighboring hectogons  then the packet is sent out on the appropriate port if it is available. if the port is not available  then the post office waits until it is. this model assumes that ports do not fail and can be easily generalised to permit a more lenient failure model. 
  if the destination is not a local neighbor then the packet is sent out on any port which reduces the manhattan distance to the destination. if no such port is available after a time specified by a critical-time parameter  then the packet is sent out to the first available port. this randomised routing after a critical time permits messages to be routed around areas of congestion. simulation results demonstrate that this mechanism permits congested areas to be gradually dissipated by spreading the communication load over a wider spectrum of the physical communication resources. 
   upon receipt of a packet  the receiving post office determines if the packet has arrived at its destination. if it has  the packet is stored into the appropriate location in the sram. if this packet completes the receipt of a message then the frisc is interrupted by the post office to handle the received message. if the local hectogon is not the final destination then the packet is forwarded using the routing algorithm described above. the final word of 
	a. davis and s. robison 	1 
each packet contains a crc which is monitored by the receiving post office to validate packet integrity during transmission. if the crc check indicates that a transmission error has occurred  the receiving post office signals the sender to retransmit the last packet. 
   physically the post office is composed of the following components: 
  the packet buffer pool is a multiported storage module consisting of a set of packet buffers. 
  the external controller is a finite state machine which makes routing decisions  manages the packet buffer pool  and connects port controllers to packet buffers. 
  the port controllers  there are six - one for each hecto-gon port   are responsible for receiving a packet and placing it in the assigned packet buffer  vice versa for output . they also perform bus master nomination duties and control the asynchronous handshaking of the bus wires used to asynchronously control word transmissions during packet transfer. the port controllers are also responsible for checking the crc codes and signalling retransmission when necessary. 
  the internal controller performs dma accesses between the sram and the packet buffer pool. it is responsible for packetizing and depacketizing messages. the internal controller interrupts the frisc when a complete message has been received and has been composed in the sram. the internal controller contains a mail-table which is used to store target addresses in the sram for incoming messages  and to indicate which packets of a particular message have been received. a similar table is maintained for stream communications which require that the order in which messages are sent will be the order in which they are handled by the receiving program object. 
   potentially the post office can keep all 1 external hectogon ports active concurrently  freeing the frisc element from using its cycles on communication overhead. this is a significant improvement over designs such as the transputer  inmos  1  which steals processor cycles to drive the ports and where only one of the four ports can be active at any particular time. in addition the hardware support for the routing algorithm is integrated into the architecture and does not have to be part of the run-time kernel. 
c . 	i n s t r u c t i o n s t r e a m m e m o r y 
   the instruction stream memory  ism  is a specialized high speed instruction delivery subsystem. in the faim-1  the locus of control for instruction delivery resides solely in the ism. the ism provides storage for the frisc object code  and provides the control to decide what instruction should be executed next and delivers it to the frisc. 
   the instruction stream can be viewed as a sequence of instructions broken by calls or jumps  both of which may be conditional. modern programming practices tend to produce relatively short instruction sequences that correspond to small procedures. the role of either a jump or call is to select a next instruction that is not in lexical order. since the jump and call instructions are seen by the ism before they are evaluated  the ism can predict when one will appear and plan for it in advance. the use of specialised hardware to enhance instruction delivery is certainly not new; branch prediction and instruction prefetch have been used to improve performance in many conventional architectures. most of these systems  for example the scoreboards in the i b m system 1  bell and newell  1   translation look aside buffers  
1 	a. davis and s. robison 
and instruction caches  smith  1   increase speed by interposing a piece of specialised hardware between the memory and the processor. 
   the ism takes this approach a step further. it provides a specialised instruction memory rather than merely placing a specialised interface in front of a conventional memory. the obvious disadvantage is that the ism is only useful for storing instructions  and therefore the ism cannot serve multiple storage roles as in conventional systems. the advantages  however are numerous: 
  the ism can be tuned for its sole function of high-speed instruction delivery. 
  the processor complexity can be reduced. 
  code density can be increased since jumps and calls are removed from the code stream. 
  a separate parallel data path for instruction delivery can be used. 
  operational concurrency is increased  since the ism and frisc can operate as concurrently cooperating partners. 
  a more flexible interrupt and trap structure is permitted. 
   the code density issue is an important one since the power of the faim-1 machine is derived primarily from the high level of replication of processing elements that the architecture supports  and not from the power of the individual processor design. in order to permit high replication levels it is necessary that the size of the individual processor be small. therefore the amount of storage available to each hectogon is much less than that typically associated with conventional main frame architectures. the reduced storage sise provides significant motivation to any mechanism which can improve code density  hence the removal of branch instructions and the use of a stack based micro-architecture for the frisc element are steps in the right direction. 
   the current ism design attempts to improve performance by capitalising on the fact that instruction access patterns are not random. hence the use of r a m memory for instruction storage is both slow and unnecessarily complex. the ism essentially performs instruction prefetch of all instruction paths that can be taken with the exception of interrupt and trap sequences  and has them ready for delivery at the time they are needed. 
   physically the ism organises its storage into a set of tracks  each of which contains 1 instruction packets. an individual packet is 1 bits long. the frisc instruction format permits two types of instruction lengths. short instructions are a single packet and long instructions are formed as a pair of packets. the ism decodes the instructions and sends properly formatted instructions over the 1 bit wide instruction bus. hence it dynamically delivers a code stream of the right length instructions from a packed array of stored instruction packets. this reduces the amount of dead storage caused by fragmentation that might be incurred by permitting multiple instruction lengths. a track is viewed as a linear sequence of instructions which terminate by a jump or call instruction  or by the physical end of the track. if sequential object code segments are longer than one track  they are continued on the next track. if they terminate by a call or jump then each track header contains a tag indicating where the call or jump target is located. this tag replaces what would normally be jump and call opcodes in the instruction stream. since 1-way conditional call/jump structures are supported  two possible targets exist. one will always be located on the next track and the other will point to what is termed the remote track. 
   three track buffers are connected to the instruction output bus  they are the current  remote  and next track buffers. instructions are delivered from the current track buffer. concurrent with the delivery of the first instruction  the current track buffer tag is examined to determine the address of the appropriate remote track. the next and remote track buffers are then loaded. since the machine is a completely asynchronous system  there is no way to determine the exact synchronization of these activities. however  in normal operation  no trap or interrupt occurs   two instructions are delivered from the current track buffer while the remote and next track buffers are loaded. this corresponds to full prefetch of both possible branch targets. if the branch does not occur in the first two instruction times of the current track then no delay will occur on either a call or a 
jump. 
   when a conditional branch is taken  due to the two stage pipeline of the processor  the condition line will not be valid until two stage times after the branch instruction. the strategy used is similar to the delayed branch technique used in the mips machine  prsybylski tt a/.  1   i.e. to insert non-condition modifying instructions in the code stream in order to prevent dead-time in the evaluation pipeline. since most of the frisc instructions do not modify the condition flags  this is typically fairly easy. 
   the communication between the frisc and ism units is selftimed  and controlled by a 1 cycle request/acknowledge protocol. typically the ism is ready with the next instruction long before the frisc has completed evaluation of the previous one. the ism also fields interrupts and traps and provides the proper instruction streams in these cases. in the case of a trap or interrupt  a 1 stage delay occurs since this is the only case when the ism cannot predict where the desired instruction will be in advance and prefetching is therefore not possible. 
   in cases where a context switch between user processes is necessary the ism can predict this case and prefetch the desired code as well as preset the appropriate status bits in the frisc to create the context switch. it is noteworthy that not only does the ism execute certain control instructions such as branch and call  but it also makes up instructions that are not in the code stream in order to initialise the processor status word when context switching is performed. for testability  it is possible for the frisc to read or write any packet in the ism. 
   while implementation details have been suppressed here for brevity  it is interesting to note that the ism is a significant performance enhancement over traditional instruction delivery mechanisms. it also uses instruction bus bandwidth more efficiently  and is a design that would not be economical if built from commercially available components. the ism is being fabricated as a 1 pin custom cmos component  where each part contains storage for 1k instruction packets. these parts can be cascaded with no performance penalty into a group of up to 1 ism chips to form the instruction storage and delivery subsystem for a particular hectogon. the maximum amount of code storage per hectogon is therefore 1k short instructions. 
d . 	t h e f r i s c a n d s r a m 
   the frisc  for fanatically reduced instruction set computer  is a specialised processor which supports the operations of oil and coordinates the other hectogon subsystems. it is also designed to be an efficient multitasking evaluation engine. internally the frisc contains a 1 bit wide data-path  1 tag bits  and 1 data bits   and is essentially a stack machine. parallel tag hardware permits tag based traps to be used in order to optimise main line instruction streams for the common case. this is similar to the mechanism used in other high speed symbolic uniprocessor architectures such as the symbolics 1  symbolics  1 . this approach makes complex microprogramming support for the most general case unnecessary  and removes a level of control indirection which improves the overall performance of the frisc element  patterson and sequin  1 . 
   the instruction set is tailored to support oil user programs. altogether  the frisc supports 1 basic instructions  corresponding to oil functions such as unify  car  etc. integer arithmetic is supported by the alu  but multiplication and division are performed iteratively using multiply-step and divide-step instructions. it is also interesting to examine the functions that are not in the frisc instruction set. jump and 
call opcodes are absent since they are handled directly by the 
ism. also missing are complex string search instructions since they are supported directly by the cxam. 
   the frisc is actually composed of two processors  the evaluation or e processor and the switching or s processor. the s processor is responsible for manipulating the run-list and loading/unloading shadow registers to permit rapid context switching. this considerably increases the physical complexity of the frisc element but provides high performance direct hardware support for multitasking. the goal is to keep as much of the faim-l's physical resources active as the process structure warrants. for example if a task initiates a cxam search it will block and another task can be executed in overlap fashion pending arrival of the answers from the cxam. the s processor is actually a small finite state machine with a couple of index registers which permit it to drive the data paths in the e processor and access the sram. 
   in the e processor  all of the primary registers in the data-path exist in triplicate  one register for each of the three contexts that the hardware supports: usero  user1  and kernel. the status word indicates which set of shadow registers are active for the current process. the primary registers are the index register  frame pointer  stack pointer  stack top  stack next  and the stack buffer. the stack buffer acts as a stack cache  and is actually 1 words deep. the stack buffer has its own memory controller which attempts to keep the buffer half full. this implies that a sequence of pops or pushes will not typically need immediate access to the sram memory bus. non-shadowed registers include the q register used during multiply and divide step loops  and the memory data and address registers. the alu takes sources from two internal busses  one of which can be shifted prior to the alu. results from the alu pass through a barrel shifter and are posted on a result bus which is used to deliver the result to the selected target. this shift arrangement facilitates the arithmetic and bit field manipulation instructions of the frisc instruction set. 
   the frisc views most data structures as objects; a conventional paged memory with a small finite-state machine attached to each page  collectively called the sram  provides an objectoriented memory system for the frisc. the sram  structure ram  stores all procedural data structures  as well as logical variable bindings and the bodies of logical rules in list form. the sram's atomic addressable entity is a word  which is composed of two portions: a 1-bit tag and a 1-bit data field. multipleword objects  e.g. simple vectors  bignums  or continuations  are represented as a structure containing one or more header words followed by indexable data fields. the tag bits support the common dynamic data types allowed in many ai languages. using the data tag bits  the sram can  concurrently with other 
frisc computation  follow a pointer chain to retrieve an object requested by the frisc. 
   the close connectivity between the frisc and its small sram removes the usual performance gap between registers and primary memory. in our case  registers have at most a 1 speed advantage over memory  so the complexity of a general register architecture is not easily justified. stack architectures are a more natural fit as a compiler target  providing improved instruction 
	a. davis and s. robison 	1 
code density  reduced data path complexity  and faster context switches. the resulting simple data path and simple instruction set is a candidate for straightforward control implementation. 
e . 	c o n t e x t 	a d d r e s s a b l e 	m e m o r y 
   the context addressable memory  cxam  is a highly parallel associative storage subsystem capable of searching for and retrieving structured data. pattern matching and associative storage accesses are common operations in many ai applications. the cxam provides direct hardware support for this important activity. rule headers for logical oil behaviors  and procedural data structures which are associatively accessed are stored in the cxam. 
   in previous systems  a variety of hashing schemes have been used in lieu of c a m components. this choice makes sense in traditional architectures where the  smart processor with big  dumb memory  partition is cast in concrete. the typical c a m does not provide sufficient associative support for ai match functions  since they match either tag bits or single word contents. in fa1m-1  the cxam can match structures as well as slot contents. 
the structure of both entries and queries in the cxam is a 
lisp s-expression. each slot therefore can either be a structure or an atom. atoms can be symbols  numbers  variables  or don't cares. semantically  variables are treated as don't cares by the cxam. the inclusion of variables as atom types for the cxam is based on faim-l's support of logic programming. the retrieval and setting of logical variable bindings is supported elsewhere in the hectogon. 
	the cxam responds to four commands: 	find match  	give 
match  delete structure  and add structure. the find and give functions are optimized for speed  while the delete and add functions are implemented with more concern for minimizing circuit area than performance. the frequency of find and give is much higher during program execution than that for delete and add. the cxam also manages its own free space and removes garbage automatically  thereby freeing the frisc element to process user instructions rather than manage storage. 
   physically the cxam consists of a storage area and a number of parallel search engines which share a multiported query buffer which contains the pattern to be matched. each search engine concurrently searches a subset of the storage area. a complete and very detailed exposition of this device  implemented as a custom cmos component  can be found in  brunvand  1 . 
f . 	s t r e a m e d p i p e l i n e u n i f i e r 
   the streamed pipelined unifier  spun  provides direct hardware support for unification of logical oil behaviors. the cxam can be used to find the next rule or set of rules to be tried  but the cxam does not perform full unification since its match function does not consider variable bindings. the spun unit takes the query and the streamed set of matched structures from the cxam  detects which variable bindings still need to be matched  fetches bindings in the current context from the sram  and completes the unification. this may entail binding a variable  in which case the spun unit must post this binding back in the sram. it may also entail starting another subgoal unification  in which case the present state must be stacked  and a new query must be presented to the cxam. 
	iv 	conclusions 
   in this paper  we have presented an architectural overview for the design of a highly parallel symbolic processor known as 
1 	a. davis and s. robison 
faim-1. in general there have been two approaches taken in the design of similar systems. the first is to build concurrent processing ensembles out of conventional processor and memory components as has been done for the cosmic cube  seits  1   butterfly gurwits  1   and dado  stolfo  1  systems. in general we feel that to truly achieve a new generation of viable symbolic processors which are a major performance improvement over existing systems  it will be necessary to significantly reallocate the transistor budget to support tasks which are specific to the domain of symbolic processing. this is not possible by merely assembling old components in new ways. the other approach is to experiment with radical new models of computation which are inherently highly parallel as is the case with the connection  hillis  1  and boltsmann hinton et al.  1  machines. the problem with this approach is that the ways in which we solve problems must change radically as well  and the incorporation of 1 years of expertise in programming is all but impossible in the short term. we feel that both approaches are viable: the first in the short term and the second in the long term. the faim-1 design attempts to fill the gap by providing a rather different but specialised architecture for performance  while requiring only a small change in programming practice to incorporate concurrency. 
	v 	acknowledgments 
   the faim-1 system is the result of a wide variety of ideas from a number of project members  and does not represent the sole effort of the authors. key contributions have also been made by: judy anderson  allan schiffman  shimon cohen  ken stevens  
ian robinson  mike deering  marty tenenbaum  dick lyon  erik brunvand  bill coates  bill athas  dan carnese  barak pearlmutter  bob hon  john conery  alvin despain  and gary lindstrom. 
