
smoothing approaches to the simultaneous localization and mapping  slam  problem in robotics are superior to the more common filtering approaches in being exact  better equipped to deal with non-linearities  and computing the entire robot trajectory. however  while filtering algorithms that perform map updates in constant time exist  no analogous smoothing method is available. we aim to rectify this situation by presenting a smoothingbased solution to slam using loopy belief propagation  lbp  that can perform the trajectory and map updates in constant time except when a loop is closed in the environment. the slam problem is represented as a gaussian markov random field  gmrf  over which lbp is performed. we prove that lbp  in this case  is equivalent to gauss-seidel relaxation of a linear system. the inability to compute marginal covariances efficiently in a smoothing algorithm has previously been a stumbling block to their widespread use. lbp enables the efficient recovery of the marginal covariances  albeit approximately  of landmarks and poses. while the final covariances are overconfident  the ones obtained from a spanning tree of the gmrf are conservative  making them useful for data association. experiments in simulation and using real data are presented.
1 introduction
the problem of simultaneous localization and mapping  slam  is a core competency of robotics which has attracted a large amount of attention. the classical solution to the slam problem uses an extended kalman filter  ekf   smith and cheeseman  1  that maintains the joint distribution on the current robot pose and the landmarks in the map in the form of a gaussian distribution. since updating the covariance matrix of this distribution requires o n1  time  the ekf algorithm does not scale to large maps. recent work on the slam problem has focussed on improving the time complexity of the solutions. one commonly used technique is to update the information form of the gaussian distribution. though the information matrix becomes dense through

figure 1: the gaussian markov random field  gmrf  corresponding to an illustrative smoothing and mapping  sam  problem with 1 poses in the robot trajectory. poses in the graph are shown as circles and landmarks as squares.
repeated marginalization of poses  recently introduced techniques such as the sparse extended information filter  seif   thrun et al.  1  and the thin junction tree filter  tjtf   paskin  1  present approximations to keep the information matrix sparse. the sparsity of the representation allows for a constant time update of the filter.
모in contrast to the above filtering approaches which only compute the current robot pose with the map  a smoothing approach recovers the complete robot trajectory and the map. the smoothing information matrix naturally stays sparse over time without the need for any approximations. further  smoothing can incorporate new information about past robot poses as opposed to filtering approaches that cannot update a pose after it has been marginalized out. even though the number of variables increases continuously for smoothing  in many real-world scenarios  especially those involving exploration  this larger matrix still contains far less entries than in filter-based methods  dellaert  1   eustice et al.  1 . filters are more efficient then smoothing only when a limited number of structure points is observed repeatedly. however  even in this case the inability to correct previous poses may lead to poor results.
모the crucial limitation of all the above approaches based on the information form of the gaussian is that recovering an estimate of the map and robot pose involves a matrix inversion  and hence  is computationally infeasible for large systems. indeed  in the case of the filtering approaches such as the seif  obtaining even the mean involves a matrix inversion.
the solution is obtained in these cases through an iterative process that solves a linear system wherein only a constant number of iterations is performed at each step so as to maintain the runtime bounds of the algorithm. this approximation is based on the assumption that the slam solution evolves only slowly over time so that a relinearization of the problem at each time step is not required. the iterative method  however  still does not yield the covariance  which is needed for maximum-likelihood data association and also as an estimate of the uncertainty in the landmark locations and pose.
모this paper deals with the use of loopy belief propagation  lbp   weiss and freeman  1  to obtain an approximate solution to the slam problem  including the marginal covariances on the map and pose. we consider the case wherein the map consists of a set of features  and build on the smoothing and mapping  sam  approach as introduced in  dellaert  1   which updates the joint distribution on the robot trajectory and the map using the square root information form of the gaussian distribution. our approach is based on the fact that the sam problem can be treated from the graphical model viewpoint as a gaussian markov random field  gmrf   winkler  1  on which inference can be performed using lbp.
모we provide a constant time update algorithm  which we call the wildfire algorithm  for estimating the map and robot trajectory  including the marginal covariances. while a naive implementation of lbp requires o n  time to update the marginal distributions  where n is the number of nodes in the graph  the wildfire algorithm discards negligibly small messages between nodes. subsequently  only those nodes which have either received or sent significant messages get updated. we show that the number of nodes involved in a significant message exchange is o 1  unless a significant event  such as loop closing  occurs when the complete graph may get updated  increasing the time bound to o n .
모relinearization of the gmrf graph  which is required since the sam problem is non-linear in general  is performed in constant time using a technique similar to the wildfire algorithm in that only nodes that have changed are relinearized. moreover  it need only be done periodically when the difference between the current estimate and the linearization point exceeds a specified threshold. as before  relinearization may take linear time after a significant update of the gmrf graph. we prove that lbp on a gmrf is equivalent to gaussseidel relaxation and provides the exact map estimates of the nodes. the covariances are  however  overconfident due to the approximate nature of the inference. as a conservative approximation  the covariances obtained by restricting inference to a spanning tree of the gmrf graph can be used to perform data association.
모in terms of related work  closest are the relaxation based approaches given in  duckett et al.  1; frese and duckett  1 . however  these are unable to compute the marginal covariances on the poses and landmarks. graphical slam  folkesson and christensen  1  is another method that builds a graphical model of the smoothing problem. our approach shares many common features with graphical slam  including the ability to modify data association on the fly  and increase efficiency by eliminating variables judiciously.
in addition  our algorithm is incremental and largely runs in o 1  time  which are advantages over graphical slam.
1 smoothing and mapping
we begin by reviewing the sam framework as given in  dellaert  1 . in the smoothing framework  our aim is to recover the maximuma posteriori  map  estimate for the entire
붟
trajectory x = {xi : i 뫍 1...m} and landmark locations
붟
 the map  l = {lj : j 뫍 1...n}  given the landmark mea-
	붟	붟
surements z = {zk : k 뫍 1...k} and odometry u = {ui}. this can be computed by maximizing the joint posterior on the trajectory and landmark locations p x l|z 
	붣  =붟 argmax p x l|z u 	 1 
붣
붟
where 붣 = {x l} is the set of unknown variables  i.e the robot trajectory and landmark locations. the posterior can be factorized using bayes law as

here p xi|xi 1 ui  is the motion model  parameterized by the odometry ui  and p zk|xik ljk  is the landmark measurement model  assuming that the correspondences  ik jk  are known. the prior on the initial pose p x1  is taken to be a constant  usually the origin  while the priors on the remaining poses and landmark locations are assumed to be uniform.
모we assume gaussian motion and measurement models  as is standard in the slam literature. the motion model is given as xi = fi xi 1 ui +wi  where wi is zero-meangaussian noise with covariance matrix qi. similarly  the measurement equation is given as zk = hk xik ljk  + vk where vk is normally distributed zero-mean measurement noise with covariance rk.
모in practice  only linearized versionsof the motion and measurement models are considered  for example for use in the extended kalman filter  ekf  approach to slam  smith et al.  1   with multiple iterations being used to obtain convergence to the minimum. in the following  we will assume that either a good linearization point is available or that we are working on one iteration of a non-linearoptimization method. it can be shown  dellaert  1  that this results in a sparse linear least squares problem in 붻붣
붻붣  =	argmin
		 1 
where fii 1 is the sparse jacobian of fi .  at the linearization point and is the odometry prediction error. analogously   and  are respectively the sparse jacobians of hk .  with respect to a change in xik and ljk  evaluated at the linearization point   and is the measurement prediction error. ||.|| is the mahalanobis distance with respect to the covariance matrix  and we use the matrix gii =  id뫄d   d being the dimension of xi  to avoid treating 붻xi specially.
모the solution to the least squares problem  1  is given by the linear system a붻붣   b = 1 where a = jtj is the
hessian matrix  j being the system jacobian matrix obtained by assembling the individual measurement jacobians  and b = jte where e is the vector of all the measurement errors.
sam as a markov random field
we now show that inference on the posterior  1  can be performed by placing the sam problem in a markov random field  mrf  framework. the graph of an mrf is undirected and its adjacency structure indicates which variables are linked by a common factor  a measurement or constraint .
모a pairwise mrf is described by a factored probabilitydensity given as
		 1 
where the second product is over the pairwise cliques {i j}  counted once. the posterior  1  can be modeled as a pairwise mrf where the singletoncliques correspondto marginal prior distributions or singleton measurements  such as gps  on the unknown variables and the edge cliques correspond to the motion and measurement likelihoods respectively.
모for the linear case considered in the previous section  these equations translate to

where yi 뫍 {x l}. this gives us a gaussian markov random field  gmrf  corresponding to the linearized sam problem. note that in the context of the sam problem  the singleton clique factors are most often set to unity except for the first pose  which is clamped to the origin.
1 loopy belief propagation for sam
we use loopy belief propagation  lbp  on the sam gmrf to solve the linear system  1  and obtain not only the map values of the unknown variables but also the covariances. for ease of computation  we use the information form of the gaussian to specify the single and pairwise clique potentials in the gmrf given by  1   wherein these are expressed using the information vector 붾 and the information matrix 붦  and we define the information form as
		 1 
where c is an additive constant. if 붦 is full-rank  equation  1  corresponds to a gaussian density as
	n x;붦 1붾 붦 1  = n x;뷃 p 	 1 
where k is a constant of proportionality.
모using this formulation  we can write the edge potentials from  1  as 뷍 xi lj  뫚 n 1 yij;붾ij 붦ij    where yij is the vector  xi lj  t  and we have droppedthe indexk from the corresponding pair {ik jk} for convenience. the distribution parameters are then given as and the potentials on the edges linking two poses are defined analogously.
모the gmrf potentials on the singleton cliques are similarly defined as 뷋 yi  = n 1 yi;붾i 붦i  where yi 뫍 {x l}  and 붾i = 붦i뷃i  붦i = pi 1 follows from  1  and  1 . the potentials 뷋 yi  represent the beliefs on the unknowns and if only a uniform prior exists on an mrf node yi  both 붾i and 붦i can be set to zero. note that setting 붦i to zero is equivalent to having a gaussian prior with infinite covariance.
belief propagation
the goal of belief propagation in our case is to find the marginal probability  or the belief  p yi  of a node yi 뫍 붣 = {x l} in the sam graph. a complete characterization of the belief propagation algorithm for gmrfs is beyond the scope of this paper  but we provide the high-level equations and data flow below. details can be found in  weiss and freeman  1 .
모the beliefs are computed using a message passing algorithm wherein each node passes messages to its neighbors and in turn uses its incoming messages to compute its belief. while belief propagationis guaranteed to compute the correct beliefs only if the graphical model does not contain loops  it has been proven that it finds the correct map solution for gaussian graphical models even in the presence of loops  i.e the means of the beliefs are correct while the covariances are incorrect in general  weiss and freeman  1 .
모lbp works by repeatedly computing the messages and beliefs for every node in the graph starting with constant messages  weiss and freeman  1   either in synchronous or asynchronous fashion. this sweep over the graph is iterated until the beliefs converge. denoting the current iteration by a discrete time superscript t  the belief parameters  the information vector and matrix  for node yi are given as
		 1 
		 1 
and the parameters of the message mij yj  from node yi to node yj as

where use has been made of the definitions in  1  and  1 .
algorithm 1 the wildfire algorithm for lbp

1. push the most recently added nodes and their neighbors in thesam graph g v e  onto the queue q
1. do until q is empty
 a  pop node y from q and compute messages mtyk for all k 뫍ny using  1 
 b  if  push k onto q
 c  update the belief b y  using  1 

figure 1: the wildfire algorithm only updates nodes with significant message deviations  starting from the most recently added node. in normal operation  left  only the frontier of the graph is updated. however  during loop closings  right  most of the graph is updated. updated poses are shown in brown  dark  while the unchanged nodes are in yellow  light . the robot is moving counterclockwise starting at the bottom.
1 constant time loopy sam
each iteration of lbp as defined above is o n  where n is the number of nodes in the graph. this is too slow to be useful  especially since the trajectory is included in the node count. we can  however  improve this bound to o 1  for incremental operation  i.e. when new nodes or edges are added to the gmrf.
모the key observation to getting a constant time algorithm is that at each step only a few of the nodes get updated  i.e. experience an actual change in their values  and these nodes are the ones that have been recently added to the graph. if we commence the belief updation sweep in every iteration of lbp from the most recently added node  the new messages are initially seen to deviate from their previous values by large amounts. however  these deviations become smaller as we move further back in the graph. once the message deviations become negligible  smaller than some pre-specified significance threshold   the beliefs no longer change and the remaining nodes need not be updated.
모the wildfire algorithm  so called because the message deviations from the previous iteration spread like wildfire from their point of origin and gradually become negligible  is illustrated in figure 1. the algorithm implementation features a queue onto which nodes with significant message deviations on incoming edges are pushed. a summary of the algorithm is given in algorithm 1.
모the wildfire algorithm has o 1  running time since in most situations the number of edges with large message deviations is o 1 . however  this is not true when special events such as loop closures occur as shown in figure 1. in such instances  the complete graph is updated resulting in o n  complexity. note that no special clause is required to handle these instances - the algorithm automatically updates all the nodes in the graph as the message deviations do not die down.
모relinearization of the sam graph  which involves computing the clique potentials at new linearization points  can also be performed using the wildfire algorithm  the only difference being that the deviations of the node beliefs from their respective linearization points are now used instead of the message deviations. in this case  however  two variants are possible. one technique is to delay relinearization until the belief deviations are greater than a certain threshold for some given proportion of nodes in the graph. as this proportion approaches unity  relinearization becomes o n  but is performed only very rarely. a second strategy is to perform relinearization at regular time intervals so that it remains o 1 . the choice between these strategies depends on the type of application  the environment  and size of the sam graph.
모we now have all the components to describe the loopy sam algorithm. at each step  a new pose node  new measurement links and possibly new landmark nodes  are added to the sam gmrf. a fixed number of lbp iterations are then performed using the wildfire algorithm to update the beliefs on the nodes. periodically  relinearization is performed  again using a variant of the wildfire algorithm. the means of the pose and landmark nodes in the sam gmrf  obtained from the belief information vector and matrix at each node  correspond to the map solutions for the trajectory and the map respectively.
1 marginal covariances
we now show how to recover the approximate marginal covariances of the nodes from the lbp beliefs. an important use of the covariances is to bound the search area for possible correspondences in data association. this is usually done using the projection of the combined pose and landmark uncertainty into the measurement space  the computation of which requires knowledge of the marginal covariances of the poses and landmarks.
모the inverse of the belief information matrix at each node in the sam gmrf gives the marginal covariance. while in general the belief obtained from lbp can be overconfident or underconfident  it has been proven to be always overconfident for gaussian mrfs with pairwise cliques  weiss and freeman  1 . overconfident covariances cannot be used for data association since this results in many valid correspondences being rejected. a first conservative approximation would be to use the inverse of the diagonal blocks of the system hessian matrix as the marginalcovariances. however  this results in overly conservative estimates.
모a better approximation can be obtained by restricting message passing to a spanning tree of the sam gmrf. since the spanning tree is obtained by deleting edges from the gmrf and inference on it is exact  we get a conservative estimate of the true marginal covariances that is also better than our first

figure 1:  top  a spanning tree of the gmrf graph  with edges colored blue  dark   for a linear simulation.  bottom  covariances for the same simulation - ground truth is shown in blue  thin dark   overconfident lbp covariances are shown in green  light   and conservative covariance estimates obtained by restricting bp to the spanning tree are shown in red  thick dark .
order approximation. maintenance of the spanning tree and message passing on it require extra work. however  the spanning tree can be extended at each step in o 1  so that  with the use of the wildfire algorithm  the overall scheme still remains constant time. figure 1 illustrates the nature of this approximation.
1 loopy sam and gauss-seidel relaxation
we prove in this section that computing the map solution using lbp for a gmrf is equivalent to a modified form of gauss-seidel relaxation  which is commonly used to solve linear systems. relaxation has previously been used in the context of slam  duckett et al.  1  but is not capable of recoveringthe marginalcovariances a short-comingthat lbp overcomes. the equivalence of the map portion of lbp and relaxation is an interesting result that connects our technique with a well-understood method from linear algebra.
모relaxation solves the linear system a붻붣 b = 1   encountered in section 1  by iterating over a set of equations  each of which updates the value of exactly one unknown using the current estimate of all the others
		 1 
where the sum is over all the variables except yi  and yi 뫍 붣 = {x l} as before.
모we begin our proof by noting that the message from yj to yi given by  1  can be re-written as
	mtji	=	jkit ek   aijyj i	 1 
where yj i is the estimate of yj without considering the node yi  k is the index of the measurement linking yi and yj in the

1 1 1 1 1
steps
figure 1: runtime for a simulation consisting of 1 steps. the times for the message and belief computation  and the total update step are shown. all times were obtained on a 1ghz linux machine using an implementation of the algorithm in ocaml. note the two spikes  one at the very end  corresponding to loop closures.
system jacobian j  and e is the corresponding measurement error. this result is true since we have

where m j i is the estimate of mj without considering node yj  and similarly mjt   mijt 1 = mj i. since mj iyj i = m j i holds by the definition of the information form of the gaussian  substituting these identities and the definitions of 붾 and 붦 from  1  into the message equation  1  gives us the result  1 .
using  1  in the belief equation  1   we get
		 1 
which closely corresponds to the relaxation equation  1 . hence  the map computation in lbp is simply a modified version of gauss-seidel relaxation.
1 results
linear gaussian simulations
we first present the results of applying our approach in a controlled simulation devoid of non-linearities. the simulation consisted of an environmentwith randomlyplaced  uniformly distributed landmarks. the robot followed a large 1-shaped figure in this environment and the run included two loop closures at the  waist  of the 1-figure.
모figure 1 gives the results for a linear simulation consisting of 1 steps  i.e. a trajectory of length 1. the total number of nodes in the sam gmrf was 1. it can be verified that the algorithm is o 1   and the two spikes in the graph corresponding to the loop closures are also clearly visible. the spikes occur since the lbp update makes a sweep through the whole graph in these instances.

figure 1: trajectory and map computed from the victoria park dataset overlaid on satellite imagery of the park.
1 victoria park dataset
we applied our algorithm to the sydney victoria park dataset  available at http://www.acfr.usyd.edu. au/homepages/academic/enebot   a popular test dataset in the slam community. the trajectory consists of 1 poses along an approximately 1 kilometer long trajectory. landmarks were obtained by running a simple treedetection algorithm on the laser range data  yielding 1 treelike features and a total of 1 measurements.
모figure 1 shows the resulting computed trajectory and map  where the landmarks are shown using crosses  overlaid on an image of the park. the gps trajectory  which was not used in the estimation  is also shown for reference. this demonstrates the applicability of our technique to real-world data.
1 conclusion
we presented an algorithm to perform smoothing-based slam using loopy belief propagation  lbp . we provide a flavor of lbp that makes map and trajectory updates constant time except when closing loops in the environment. in this case  lbp is equivalent to a modified version of gauss-seidel relaxation  a result that we proved here.
모while our technique has advantages over existing state-ofthe-art in that we can recover the marginal covariances in o 1  time  these are a heuristic approximation to the true covariances. it is our experience that the lbp covariances are of good quality when the robot is exploring new areas in the environment but become highly overconfident when it revisits previously visited areas. in general  the more loopy the gmrf  the more overconfidentthe covariancesfrom lbp. this observation can be intuitively explained by appealing to the fact that the covariances become incorrect due to multiple use of the same evidence when it is propagated around a loop in the graph.
모it is future work to define rigorous bounds on the covariance obtained from lbp  and also investigate avenues to recover the exact covariance using related techniques  an example being  sudderth et al.  1 .
acknowledgements
the authors were partly supported during this work by the national science foundation award iis - 1  career: markov chain monte carlo methods for large scale correspondenceproblems in computer vision and robotics 
