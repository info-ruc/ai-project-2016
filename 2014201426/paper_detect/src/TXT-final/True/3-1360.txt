
the particle filter has emerged as a useful tool for problems requiring dynamic state estimation. the efficiency and accuracy of the filter depend mostly on the number of particles used in the estimation and on the propagation function used to re-allocate these particles at each iteration. both features are specified beforehand and are kept fixed in the regular implementation of the filter. in practice this may be highly inappropriate since it ignores errors in the models and the varying dynamics of the processes. this work presents a self adaptive version of the particle filter that uses statistical methods to adapt the number of particles and the propagation function at each iteration. furthermore  our method presents similar computational load than the standard particle filter. we show the advantages of the self adaptive filter by applying it to a synthetic example and to the visual tracking of targets in a real video sequence.
1 introduction
the particle filter is a useful tool to perform dynamic state estimation via bayesian inference. it provides great efficiency and extreme flexibility to approximate any functional nonlinearity. the key idea is to use samples  also called particles  to represent the posterior distribution of the state given a sequence of sensor measurements. as new information arrives  these particles are constantly re-allocated to update the estimation of the state of the system.
모the efficiency and accuracy of the particle filter depend mainly on two key factors: the number of particles used to estimate the posterior distribution and the propagation function used to re-allocate these particles at each iteration. the standard implementation of the filter specifies both factors beforehand and keeps them fixed during the entire operation of the filter. in this paper  we present a self adaptive particle filter that uses statistical methods to select an appropriate number of particles and a suitable propagation function at each iteration.
모this paper is organized as follows. section 1 provides background information about the standard particle filter.
section 1 presents our method to adaptively estimate the number of particles. section 1 presents our method to adaptively improve the propagation function. section 1 shows the results of applying the self adaptive filter to a visual tracking task. finally  section 1 presents the main conclusions of this work.
1 particle filter
in bayesian terms  the posterior distribution of the state can be expressed as:
	p xt/~yt  = 붹 p yt/xt p xt/~yt 1 	 1 
where 붹 is a normalization factor; xt represents the state of the system at time t; and ~yt represents all the information collected until time t. equation  1  assumes that xt totally explains the current observation yt.
모the particle filter provides an estimation of the posterior in equation  1  in 1 main steps: sampling  weighting  and re-sampling. the sampling step consists of taking samples  particles  from the so-called dynamic prior distribution  p xt/~yt 1 . next  in the weighting step  the resulting particles are weighted by the likelihood term p yt/xt . finally  a re-sampling step is usually applied to avoid the degeneracy of the particle set. the key point that explains the efficiency of the filter comes from using a markovian assumption and expressing the dynamic prior by: z p xt/~yt 1  =	p xt/xt 1 p xt 1/~yt 1 dxt 1.	 1 
 this expression provides a recursive implementation of the filter that allows it to use the last estimation p xt 1/~yt 1  to select the particles in the next iteration. these particles are then propagated by the dynamics of the process  to complete the sampling step.
모at each iteration the operation of the particle filter can be seen as an importance sampling process  tanner  1 . importance sampling provides an efficient way to obtain samples from a density p x   in cases where this function can be evaluated  but it is not affordable or possible to sample from it directly. the basic idea in importance sampling is to use a proposal distribution q x   also called importance function  to obtain the samples xi  and then weigh each sample using a compensatory term given by p xi /q xi . it is possible to show  tanner  1  that under mild assumptions the set of weighted-samples resembles the target distribution p x .
모the sampling and weighting steps of the particle filter correspond to the basic steps of an importance sampling process. in this case  given that the true posterior p xt/~yt  is not known  the samples are drawn from an importance function that corresponds to the dynamic prior p xt/~yt 1 . using this importance function  the compensatory terms are exactly the non normalized weights used in the weighting step of the particle filter. the methods presented in this paper use results from the theory of importance sampling to provide a self adaptive version of the particle filter.
1 adaptive selection of the number of particles
the selection of the number of particles is a key factor in the efficiency and accuracy of the particle filter. the computational load and the convergence of the filter depend on this number. most applications select a fixed number of particles in advance  using ad hoc criteria  or statistical methods such as monte carlo simulations or some standard statistical bound  boers  1 . unfortunately  the use of a fixed number of particles is often inefficient. the dynamics of most processes usually produces great variability in the complexity of the posterior distribution1. as a consequence  the initial estimation of the number of particles can be much larger than the real number of particles needed to perform a good estimation of the posterior distribution or  worse  at some point  the selected number of particles can be too small causing the filter to diverge.
모the effect of the number of particles on the accuracy of the filter is determined by two factors: the complexity of the true density and how closely the proposal density mimics the true density. both factors are very intuitive; the estimation of a more complex pdf requires a greater number of samples to correctly represent the less predictable shape of the function. also  a greater mismatch between the proposal and the true densities produces many wasted samples located in irrelevant parts of the true distribution. previous works to adaptively determine an adequate number of particles have failed to consider these two factors together  fox et al.  1; koeller and fratkina  1; fox  1 .
모here  we propose two methods based in the theory of statistics that can be used to adaptively estimate the number of particles to represent the target posterior distribution without adding a significant load to the normal operation of the filter. at each cycle of the particle filter  these techniques estimate the number of particles that  with a certain level of confidence  limits the maximum error in the approximation.
1 kld-sampling revised
the kld-sampling algorithm  fox  1  is a method to adaptively estimate the number of samples needed to bound the error of the particle filter. the error is measured by the kullback-leibler divergence  kl-divergence  between the true posterior distribution and the empirical distribution  which is a well known nonparametric maximum likelihood estimate. kld-sampling is based on the assumption that the true posterior can be represented by a discrete piecewise constant distribution consisting of a set of multidimensional bins. this assumption allows the use of the 뷌1 asymptotic convergence of the likelihood ratio statistic to find a bound for the number of particles n:
		 1 
where 1 is the upper bound for the error given by the kldivergence   1   붻  is the quantile of the 뷌1 distribution with k   1 degrees of freedom  and k is given by the number of bins with support.
모the problem with kld-sampling is the derivation of the bound using the empirical distribution  which has the implicit assumption that the samples comes from the true distribution. this is not the case for particle filters where the samples come from an importance function. furthermore  the quality of the match between this function and the true distribution is one of the main elements that determines the accuracy of the filter  hence the suitable number of particles. the bound given by kld-sampling only uses information about the complexity of the true posterior  but it ignores any mismatch between the true and the proposal distribution.
모to fix the problem of kld-sampling we need a way to quantify the degradation in the estimation using samples from the importance function. the goal is to find the equivalent number of samples from the importance and the true densities that capture the same amount of information about the latter.
모in the context of monte carlo  mc  integration   geweke  1  introduced the concept of relative numerical efficiency  rne   which provides an index to quantify the influence of sampling from an importance function. the idea behind rne is to compare the relative accuracy of solving an integral using samples coming from both the true and the proposal density. accuracy is measured according to the variance of the estimator of the integral.
모if we use mc integration to estimate the mean value of the state  emc x    the variance of the estimator is given by  doucet et al.  1  1:
	v ar emcn  x   = v arp x /n	 1 
where n is the number of samples coming from the true distribution p x   and the subscript p expresses that the variance involved is computed using the target distribution.
모when the samples come from an importance function q x   the variance of the estimator corresponds to the variance of importance sampling  is   which is given by  geweke  1 :
v ar eisn  x   = eq  x ep x  1 w x 1 /nis = 횰s1 /nis 
 1 
where w x  corresponds to p x /q x  the weights of is and nis is the number of samples coming from the importance function.
모to achieve similar levels of accuracy  the variance of both estimators should be equal. this allow us to find a relation that quantifies the equivalence between samples from the true and the proposal density 
		 1 
모replacing  1  in  1  allows us to correct the bound given by kld-sampling when the samples do not come from the true distribution but from an importance function:
	.	 1 
모using mc integration  v arp x  and 횰s1 can be estimated by: and

모with. equation  1  shows that using appropriate accumulators  it is possible to calculate the bound incrementally keeping the o n  complexity of the filter.
1 asymptotic normal approximation
usually  the particle filter keeps track of the posterior density with the goal of estimating the mean or higher order moments of the state. this suggests an alternative error metric to determine the number of particles. instead of checking the accuracy of the estimation of the posterior  it is possible to check the accuracy of a particle filter in the estimation of a moment of this density.
모under weak assumptions and using the strong law of large numbers  it is possible to show that at each iteration the estimation of the mean given by the particle filter is asymptotically unbiased  degroot  1 . furthermore  if the variance of this estimator is finite  the central limit theorem justifies an asymptotic normal approximation for it  degroot  1   which is given by:
           epf x  몲 n ep x  횰s1 /nis   1  where n 뷃 1  denotes the normal distribution with mean 뷃 and standard deviation .
모using this approximation  it is possible to build a one sided confidence interval for the number of particles that limits the error in the estimation of the mean:
		 1 
where | 몫 | denotes absolute value; ep x  is the true mean value of the state; 1 corresponds to the desired error; and  1  붸  corresponds to the confidence level.
모following the usual derivation of confidence intervals  equation  1  produces the following bound for the number of particles:
		 1 
1 testing the bounds figure 1 shows the distributions used to test the bounds. the true distribution corresponds to p x  = 1n 1  +
1n 1  and the importance function to q x  = 1n 1  + 1n 1 . in all the trials  the desired error was set to 1 and the confidence level to 1%.

figure 1: inefficient allocation of samples due to a mismatch between p x  and q x .
모figure 1a shows the number of particles predicted by the different bounds. the predicted number of particles is highly consistent. also  the revised versions of kld-sampling consistently require a larger number of particles than the original algorithm. this is expected because of the clear mismatch between the proposal and the true densities.
모figure 1b shows the resulting kl-divergence for each case. it is interesting to note how the practical results match closely the theoretical ones. using the original version of kld-sampling  the error in the estimation is significantly greater than the one specified. however  when the same predicted number of particles is sampled from the true distribution  solid-line   the resulting error matches closely the one specified. this shows clearly that constraining the sampling to the right assumption  the original bound predicted by kld-sampling is correct. in the case of the revised versions of kld-sampling the resulting error using equation  1  matches closely the one specified. in the same way  the error provided by the bound in equation  1  also matches closely the level specified.
1 adaptive propagation of the particles
the regular implementation of the particle filter propagates the particles using the dynamic prior p xt/~yt 1 . this strategy has the limitation of propagating the samples without considering the most recently available observation  yt. importance sampling suggests that one can use alternative propagation functions that can provide a better allocation of the samples  for example  a suitable functional of yt. unfortunately  the use of an arbitrary importance function significantly increases the computational load of the particle filter. in this case  as opposed to the standard particle filter  the estimation of each weight requires the evaluation of the dynamic prior. this section shows a method to build an importance function that takes into account the most recent observation yt without increasing the computational complexity of the filter.

figure 1: a  number of particles given by each bound. b  resulting kl-divergence.1 previous work
in the literature about particle filters and importance sampling  it is possible to find several techniques that help to allocate the samples in areas of high likelihood under the target distribution. the most basic technique is rejection sampling  tanner  1 . the idea of rejection sampling is to accept only the samples with an importance weight above a suitable value. the drawback is efficiency: there is a high rejection rate in cases where the proposal density does not match closely the target distribution. in  west  1   west presents a kernel-based approximation to build a suitable importance function. however  the computational complexity of the method is unaffordable. in the context of mobile robot localization  thrun et al.  thrun et al.  1  propose to sample directly from the likelihood function  but in many applications this is not feasible or prohibitive.
모pitt and shephard propose the auxiliary particle filter  pitt and shephard  1 . they augment the state representation with an auxiliary variable. to sample from the resulting joint density  they describe a generic scheme with computational complexity of o n . the disadvantage is the additional complexity of finding a convenient importance function. pitt and sheppard provide just general intuitions about the form of a this function. in this paper we improve on this point by presenting a method to find a suitable importance function.
1 adaptive propagation of the samples
sampling from the dynamic prior in equation  1  is equivalent to sample from the following mixture distribution:
모모모모모모xn p xt/~yt 1  뫘	붹k p xt/xkt 1 	 1 
k=1
where the mixture coefficients 붹k are proportional to p xt 1/~yt 1 . the key observation is that under this scheme the selection of each propagation density depends on the mixture coefficients 붹k's  which do not incorporate the most recent observation yt. from an mc perspective  it is possible to achieve a more efficient allocation of the samples by including yt in the generation of the coefficients. the intuition is that the incorporation of yt increases the number of samples drawn from mixture components associated with areas of high probability under the likelihood function.
모under the importance sampling approach  it is possible to generate a new set of coefficients 붹k  that takes into account yt by sampling from the importance function p xt 1/~yt . in this way  the set of samples xit from the dynamic prior p xt/~yt 1  is generated by sampling from the mixture 
		 1 
and then adding to each particle xit a compensatory weight given by 
	 	with 	 1 
the resulting set of weighted samples {xit wti}ni=1 still comes from the dynamic prior  so the computational complexity of the resulting filter is still o n . the extra complexity of this operation comes from the need to evaluate and to draw samples from the importance function p xit 1/~yt . fortunately  the calculation of this function can be obtained directly from the operation of the regular particle filter. to see this clearly  consider the following:
p xt xt 1/~yt 뫚p yt/xt xt 1 ~yt 1 p xt xt 1/~yt 1 뫚p yt/xt p xt/xt 1 ~yt 1 p xt 1/~yt 1 뫚p yt/xt p xt/xt 1 p xt 1/~yt 1   1 equation  1  shows that  indeed  the regular steps of the particle filter generate an approximation of the joint density p xt xt 1/~yt . after re-sampling from p xt 1/~yt 1   propagating these samples with p xt/xt 1   and calculating the weights p yt/xt   the set of resulting sample pairs  xit  xit 1  with correcting weights p yt/xit  forms a valid set of samples from the joint density p xt xt 1/~yt . considering that p xt 1/~yt  is just a marginal of this joint distribution  the set of weighted-samples xit 1 are valid samples from it.
모the previous description provides an adaptive algorithm that allows the particle filter to use yt in the allocation of the samples. first  n particles are used to generate the importance function p xt 1/~yt . then  starting from this importance function  another n particles are used to generate
the desired posterior p xt/~yt . the relevant compensatory weights are calculated according to equation  1  and the likelihood term p yt/xt . the resulting filter has a computational complexity of o 1n .
모in the previous algorithm the overlapping between a regular iteration of the regular particle filter and the process of generating the importance function provides a convenient way to perform an online evaluation of the benefits of updating the dynamic prior with information from the last observation. while in cases of a poor match between the dynamic prior and the posterior distribution the updating of the dynamic prior can be beneficial  in cases where these distributions agree  the updating does not offer a real advantage  and the extra processing should be avoided. to our current knowledge  this issue has not been addressed before.
모the basic idea is to quantify at each iteration of the particle filter the trade-off between continuing drawing samples from a known but potentially inefficient importance function p xt 1/~yt 1  versus incurring in the cost of building a new importance function p xt 1/~yt  that provides a better allocation of the samples under the likelihood function. the important observation is that  once the regular particle filter reaches an adequate estimate  it can be used to estimate both the posterior distribution p xt/~yt  and the updated importance function p xt 1/~yt .
모the last step of the algorithm is to find a metric that provides a way to quantify the efficiency in the allocation of the samples. considering that the efficiency in the allocation of the samples depends on how well the dynamic prior resembles the posterior distribution  an estimation of the distance between these two distributions is a suitable index to quantify the effectiveness of the propagation step. we found a convenient way to estimate the kullback-leibler divergence  kl-divergence  between these distributions  and in general between a target distribution p x  and an importance function q x :
	kl p x  q x   뫘 log n    h w i .	 1 
모equation  1  states that for a large number of particles  the kl-divergence between the dynamic prior and the posterior distribution can be estimated by calculating how far the entropy of the distribution of the weights  h w i   is from the entropy of a uniform distribution  log n  . this is an intuitive result because in the ideal case of importance sampling  where p x  = q x   all the weights are equal. in consequence the entropy of the weights is a suitable value to quantify the efficiency in the allocation of the samples.
1 application
to illustrate the advantages of the self adaptive particle filter  we use a set of frames of a video sequence consisting of two children playing with a ball. the goal is to keep track of the positions of the ball and the left side child. each hypothesis about the position of a target is given by a bounding box defined by its height  width  and the coordinates of its center. the motion model used for the implementation of the particle filter corresponds to a gaussian function of zero mean and diagonal covariance matrix with standard deviations of 1 for the center of each hypothesis and 1 for the width and height.
모figure 1 shows the results of tracking the targets using the self adaptive particle filter. the bounding boxes correspond to the most probable hypotheses in the sample set used to estimate the posterior distributions of the states. in the estimation of the number of particles  we just consider the x and y coordinates of the center of the bounding boxes  assuming independence to facilitate the use of equation  1 . we set the desired error to 1 and the confidence level to 1%. a minimum number of 1 samples is always used to ensure that convergence has been achieved. in the adaptation of the propagation function we set the threshold for the entropy of the weights in 1.
모figure 1-left shows the number of particles needed to estimate the posterior distribution of the ball at each frame without adapting the propagation function. figure 1-right shows the number of particles in the case of adapting the importance function. the tracking engine decides to adapt the importance function at all the frames where the ball travels from one child to the other  frames 1 .
모in the case of tracking the child  the result shows that there is not a major difference between the self adaptive particle filter and the regular filter. the self adaptive filter needs a roughly constant number of particles during the entire sequence without needing to adapt the importance function. this is expected because the child has only a small and slow motion around a center position during the entire sequence. therefore the stationary gaussian motion model is highly accurate and there is not a real advantage of adapting the number of particles or the propagation function.
모in the case of the ball  the situation is different. during the period that the ball travels from one child to the other  frames 1 to 1   it has a large and fast motion  therefore the gaussian motion model is a poor approximation of the real motion. as a consequence there is a large mismatch between the dynamic prior and the posterior distribution. this produces an inefficient allocation of the samples and the estimate without adapting the importance function needs a larger set of samples to populate the relevant parts of the posterior. in contrast  when adapting the importance function during frames 1 to 1 it is possible to observe a significant reduction in the number of samples due to a better allocation of them.
1 conclusions
in this paper we present a self adaptive version of the particle filter that uses statistical techniques to estimate a suitable number of particles and to improve the propagation function. in terms of the estimation of the number of particles  the validation of the bounds using a synthetic example shows that the empirical results match closely the theoretical predictions. in particular  the results indicate that by considering the complexity of the true density and how closely the proposal density mimics the true density  the new bounds show a clear improvement over previous techniques such as kld-sampling.
모the mechanisms used by the self adaptive filter to adapt the importance function and to identify when the adaptation of the importance function may be beneficial proved to be highly relevant. using these mechanisms to track targets in a real video sequence  the self adaptive filter was able to ef-

figure 1: tracking results for the ball and the left side child for frame 1  1  and 1. the bounding boxes correspond to the most probable hypotheses in the sample set used to estimate the posterior distributions.

figure 1: number of particles used at each iteration to track the ball. left: without adapting the importance function. right:adapting the importance function.
ficiently track targets with different motions using a general gaussian motion model. furthermore  by avoiding the need of overestimating the number of particles and by allocating these particles in areas of high likelihood  the self adaptive filter proved to operate with a similar computational complexity to the regular particle filter.
