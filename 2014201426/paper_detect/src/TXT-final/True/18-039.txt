 
   in this paper we apply some recent work of angluin  1  to the induction of the english auxiliary verb system. in general  the induction of finite automata is computationally intractable. however  angluin shows that restricted finite automata  the kreversible automata  can he learned by efficient  polynomial time  
algorithms. we present an explicit computer model demonstrating that the english auxiliary verb system can in fact be learned as a 1-reversible automaton  and hence in a computationally feasible amount of time. the entire system can be acquired by looking at only half the possible auxiliary verb sequences  and the pattern of generalization seems compatible with what is known about human acquisition of auxiliaries. we conclude that certain linguistic subsystems may well be learnable by inductive inference methods of this kind  and suggest an extension to context-free languages. 
	i 	introduction 
   formal inductive inference methods have rarely been applied to actual natural language systems. linguists generally suppose that languages are easy to learn because grammars are highly constrained; no  general purpose  inductive inference methods are required. this assumption has generally led to fruitful insights on the nature of grammars. yet it remains to determine whether all of a language is learned in a grammar-specific manner. in this 
paper we show how to successfully apply one computationally efficient inductive inference algorithm to the acquisition of a domain of english syntax. our results suggest that particular language subsystems can be learned by general induction procedures  given certain general constraints. 
   the problem is that these methods are in general computationally intractable. even for regular languages induction can be exponentially different  gold  1 . this suggests that there may be general constraints on the design of certain linguistic subsystems to make them easy to learn by general inductive inference methods. we propose the constraint of k-reversibilily as one such restriction. this constraint guarantees polynomial time inference  angluin  1 . in the remainder of this paper  we also show  by an explicit computer model  that the english auxiliary verb system meets this constraint  and so is easily inferred from a corpus. the theory gives one precise characterization of just where we may expect general inductive inference methods to be of value in language acquisition. 
'this paper describes research done at the artificial intelligence laboratory of the massachusetts institute of technology. support for the laboratory's artificial intelligence research is provided in part by the advanced research projects agency of the department of defense under the office of naval research contract n1-c-1. 
ii learning k-reversible languages from examples 
   the question we address is. if a learner presumes that a natural language domain is systematic in some way  can the learner intelligently infer the complete system from only a subset of sample sentences  let us develop an example to formally describe what we mean by  systematic in some way.  and how such a systematic domain allows the inference of a complete system from examples. if you were told that mary bakes cakes  john bakes cakes  and mary eats pies are legal strings in some language  you might guess that. john eats pies is also in that language. strings in the language seem to follow a recognizable pattern  so you expect other strings that follow the same pattern to be in the language also. 
   in this particular case  you are presuming that the to-belearned language is a zero-reversible regular language. angluin  1  has defined and explored the formal properties of reversible regular languages. we here translate some of her formal definitions into less technical terms. 
   a regular language is any language that can be generated from a formula called a regular expression. for example the strings mentioned above might have come from the language that the following regular expression generates: 
 mary|john   bakes eats    very* delicious   cakesipies l 
   a complete natural language  is too complex to be generated by some concise regular expression  but some .simple subsets of a natural language can lit this kind of pattern. 
   to formally define when a regular language is reversible  let us first define a prefix as any substring  possibly zero-length  that can be found at the very beginning of some legal string in a language  and a sulfix as any substring  again  possibly zerolength  that  can be found at the very end of some legal string in a language. in our case the strings are sequences of words  and the language is the set of all legal sentences in our simplified subset of english. also  in any legal string say that the suffix that immediately follows a prefix is a tail for that prefix. then a regular language is zero-reversible if whenever two prefixes in the language have a tail in common  then the two prefixes have all tails in common. 
   in the above example  prefixes mary and john have the tail bakes cakes in common. if we presume that the language these two strings come from is zero-reversible  then mary and john must have all tails in common. in particular  the third string shows that mary has eats pies as a tail  so john must also have eats pies as a 
   tail. our current hypothesis after having seen these three strings is that they come not from the three string language expressed by  mary john  bakes cukc.s | mary cuts pies  which is not zeroreversible  but. lather from the four-string language  mary john   bakes cakes   cats pies   which is zero-reversible. notice that we have enlarged the corpus just enough to make the language zero-reversible. 
   a regular language is k-reversible  where k is a non-negative integer  if whenever two prefixes whose last k words match have a tail in common  then the two prefixes have all tails in common. a higher value of k gives a more conservative condition for 
1
inference. for example  if we presume that the aforementioned strings come from a i-reversible language  then instead of presuming that whatever mary does john does  we would presume only that whatever mary bakes. john bakes. in this case the third string fails to yield any inference  but  if we were later told that. mary bakes pies is in the language  we could infer that john bakes pies is also in the language. further adding the sentence mary bakes would allow 1-reversible inference to also induce john bakes  
resulting in the seven-string 1-reversible language expressed by  mary john  bakes  cakes pies  | mary eats pies. 
   with these examples zero-reversible inference would have generated  mary john   bakes eats   cakes pics * by now  which overgeneralizes an optional direct object into zero or more direct objects. on the other hand  two-reversible inference would have inferred no additional strings yet. for a particular language we hope to find a k that is small enough to yield some inference but not so small that we overgeneralize and start inferring strings that are in fact  not in the true language wo are trying to learn. 
	iii 	an inference algorithm 
in addition to formally characterizing k-reversible languages  
angluin also developed an algorithm for inferring a k- reversible language from a finite set of positive examples  as well as a method for discovering an appropriate k when negative examples  strings known not to be in the language  are also presented. she also presented an algorithm for determining  given some a-reversiblc regular language  a minimal set of shortest possible examples  a  characteristic  or  covering  sample  sufficient for inducing the language. we have implemented these procedures on a computer in mac-lisp and have applied them to all of the artificial languages in augluins paper as well as to all of the natural language examples in this paper. 
   to describe the inference algorithm  we make use of the fact that every regular language can be associated with a corresponding deterministic finite-state automaton  dfa  which accepts or generates exactly that  language. 
   given a sample of strings taken from the full corpus  we first generate a pre fix-tree automaton which accepts or generates exactly those strings and no others. we now want to infer additional strings so as to induce a k-reversible language  for some chosen k. let  us say that  when accepting a siring  the last  k symbols encountered before arriving at a state is a k leader of that state. then to generalize the language  we recursively merge any two states where any of the following is true: 
  another state arcs to both states on the same word.  this enforces determinism.  
 doth states have a common a: leader and either 
-both states are accepting states or 
-both states arc to a common state on the same word. 
when near of these conditions obtains any longer  the resulting 
dfa accepts or generates the smallest /c-reversible language that includes the original sample of strings.  the term  reversible  is used because a k-rcvcrsible dfa is still deterministic with lookahead k when its sets of initial and final states are swapped and all of its arcs are reversed.  
   this procedure works incrementally. each new string may be added to the dfa in prefix-tree fashion and the state-merging algorithm repeated. the resulting language induced is independent of the order of presentation of sample strings. 
r. berwick and s. pilato 1 
   if an appropriate k is not known a priori  but. some negative as well as positive examples are presented  then one can try increasing values of k until the induced language contains none of the negative examples. 
iv 	inference qf the english auxiliary system 
we have chosen to test the english auxiliary system under 
k-reversible inference because english verb sequences are highly regular  yet they have some degree of complexity and admit to some exceptions. we represent the english auxiliary system as a corpus of 1 variants of a declarative statement in third person singular. the variants cover all standard legal permutations of tense  aspect  and voice  including do support and nine modals. we simply use the surface forms  which are strings of words with no additional information such as syntactic category or root-byinflection breakdown. for instance  the present  simple  active example is judy gives bread. one modal  perfective  passive vari ant is judy would have been given bread. 
   we have explored the k-revcrsible properties of this natural language subsystem in two main steps. first we determined for what values of k the corpus is in fact a-reversiblc. given a finite corpus  we could be sure the language is k-reversiblc for all k at or above some value.  to do this we treated the lull corpus as a set of sample strings and tried successively larger values of a: until finding one where k-reversible inference applied to the corpus generates no additional strings. we could then be sure that any a: of that value or greater could be used to infer an accurate model of the english auxiliary system without overgeneralizing. 
   after finding the range of values of k- to work with  we were interested in determining which  if any  of those values of a; would yield some power to infer the full corpus from a proper subset of examples. to do this we took the dfa which represents the full corpus and computed  for a trial a:  a set of sample strings that would be minimally sufficient to induce the full corpus. if any such values of k: exist  then we can say that  in a nontrivial way  the english auxiliary system is learnable as a /c-reversible language from examples. 
   we found that the english auxiliary system can be faithfully modeled as a k-reversible regular language for k   1. only zeroreversible inference overgeneralizes the full corpus as well as the active and passive corpora treated as separate languages. for the active corpus  zero-reversible inference groups the forms of do with the other modals. the dfas for the passive and full corpora also contain loops and thereby generate infinite numbers of illegal variants. 
   does treating the english auxiliary system as a 1-or-morereversible language yield any inferential power  the english auxiliary system as a 1-reversible language can in fact be inferred from a cover of only 1 examples out of the 1 variants in the corpus. the active corpus treated separately requires 1 examples out of 
1 and the passive corpus requires 1 out of 1. treating the full corpus as a 1-reversible language requires 1 examples  and a1+reversible model cannot infer the corpus from any proper subset whatsoever. 
   for irreversible inference  1 of the verb sequences of length three or shorter will yield the remaining nine such strings and none longer. verb sequences of length four or five can be divided into two patterns   modal  have been yiv iny cn  and ... be en  being given. adding any one  length-four  string from the first pattern will yield the remaining 1 strings of that pattern. further adding two length-four strings from the awkward second pattern will yield the remaining 1 strings of that pattern  nine of which are of length five. this completes the corpus. 
1 r. berwick and s. pilato 
v discussion 
   the auxiliary system has often been regarded as an acid test for a theory of language acquisition  given this  we are encouraged that it is in fact learnable via a computationally efficient general method. it is significant that  at least in this domain we have found a k  of i  that is low enough to generate a good amount of inference from examples yet high enough to avoid overgeneralization. even more conservative 1-reversibility generates a little inference. 
   this inductive power derives from the systematic sequential structure of the english auxiliary system. in an idealized form  ignoring tense and inflections  the regular expression 
      do |   modal    have   be |  bepassive  give generates all english verb sequence patterns in our corpus. 
   zero-reversible inference basically attempts to simplify any partial  disjunctive permutation like  a b x   ay into an exhaustive  combinatorial permutation like  a b  x y . since the active corpus  excluding be-passivc from the idealized regular expression  in fact has such a simple form except for the do disjunction  zero-reversible inference productively completes the three-place permutation but also destroys the disjunction  by overgcneralizing what patterns can follow both do and  model . onereversible inference requires that disjuncts share some final word to be mergeable  so that do cannot merge with any auxiliary triplet  yet the permutation of  modal  have by  be  is still productive. similar considerations obtain in the passive case  as well as for the joint corpus. 
   in complex environments  rather than reduce the inferential power by raising k one could instead embed this algorithm within a larger system. for example  a more realistic model of processing english verb sequences would have an external  more linguistically motivated mechanism force the separate treatment of active versus passive forms. then if  say on considerations of frequency of occurrence  do exceptions were externally handled and the infrequent ... be being ... cases were similarly excluded from the immature learner  then one could apply the more powerful zeroreversible inference to the remaining active and passive forms without overgeneralizing. in such a case the active system can be induced from 1 examples out of 1 variants and the passive system from 1 out of 1. the entire active system is learnablc once examples of each form of each verb and each modal have been seen  plus one example to fix the relative order of have vs. be  and one example each to fix the order of modal vs. have or be. 
   though a more complex model must ultimately represent a domain like the english auxiliary system  the way k-rcvcrsible inference in itself handles a complex territory satisfies some conditions of psychological lidelity. especially zero-reversibility is a rather simple form of generalization of sequential patterns with which we believe humans readily identify. in general the longer  more complex cases can be inferred from simpler cases. also  there is a reasonable degree of play in the composition of the covering sample  and the order of presentation does not affect the language learned. 
   children evidently never make mistakes on the relative order of auxiliaries  which is consistent with the reversibility model  but they do mistakenly combine do with tensed verb forms  pinker  1 .  liven that the appearance of do in declarative sentences is also fairly rare  one might prefer the aforementioned zeroreversible system that handles do support as an exception  rather than opt for a 1-reversible inference which is flawless but a slower learner. 
   the ... be being ... cases are systematically related to the rest  but also have a natural boundary: 1-reversible inference from simpler cases doesn't intrude into that territory  yet only a few such examples allow one to infer the remainder. very rare sequences like could havt; been being given will be successfully acquired even if they are not seen. this seems consistent with human judgments that such phrasing is awkward but apparently legal. 
¡¡k-reversibility is essentially a model of simplicity  not of complexity. as such  it induces not linguistic structure but the substitution classes that linguistic structures typically work with  building these by analogy from examples. in the linguistic structure for which k-reversibility is defined regular grammars - it functions to induce the classes that fill  slots'' in a regular expression  based on the similarity of tail sets. increasing the value of k is a way of requiring a higher degree of similarity before calling a match.  see gonzalez and thomason  1  for other approaches to 'c-tail inference that are not so efficient.  
   the same principle can apply to the induction of substitution classes in other linguistic domains including morphological  syntactic  and semantic systems. for a particularly direct example  consider the right-hand sides of context-free rewrite rules. any subset of such rules having the same left-hand side constitutes a regular language over the set of terminal and nonterminal symbols  and is therefore a candidate for induction. one might thus infer new rewrite rules from the pattern of existing ones  thereby not only concluding that words are members of certain simple syntactic classes  but also simplifying a disjunctive set of rules into a more concise set that exhibits systematic properties. berwick's lparstfal system  1  is an example of this kind of extension. 
   we believe that k-revcrsibility illustrates a psychologically plausible pattern induction process for natural language learning that in its simplest form has an efficient computational algorithm 
associated with it. the basic principle behind k-reversible inference shows some promise as a flexible tool within more complex models of language acquisition. it is encouraging that  at least in a simple case  computational linguistic models can suggest formal learnability constraints that are natural enough to be useful in the learning of human languages. 
