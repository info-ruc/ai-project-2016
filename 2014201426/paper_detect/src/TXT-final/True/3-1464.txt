
it is now well-known that the feasibility of inductive learning is ruled by statistical properties linking the empirical risk minimization principle and the  capacity  of the hypothesis space. the discovery  a few years ago  of a phase transition phenomenon in inductive logic programming proves that other fundamental characteristics of the learning problems may similarly affect the very possibility of learning under very general conditions.
our work examines the case of grammatical inference. we show that while there is no phase transition when considering the whole hypothesis space  there is a much more severe  gap  phenomenon affecting the effective search space of standard grammatical induction algorithms for deterministic finite automata  dfa . focusing on the search heuristics of the rpni and red-blue algorithms  we show that they overcome this problem to some extent  but that they are subject to overgeneralization. the paper last suggests some directions for new generalization operators  suited to this phase transition phenomenon.
1 introduction
it is now well-known that the feasibility of inductive learning is ruled by statistical properties linking the empirical risk minimization principle and the  capacity  of the hypothesis space  vapnik  1 . while this powerful framework leads to a much deeper understanding of machine learning and to many theoretical and applicative breakthroughs  it basically involves only statistical information on the learning search space  e.g. the so-called vc-dimension. the dynamics of the learning search is not considered.
모independently  a new combinatoric paradigm has been studied in the constraint satisfaction community since the early 1s  motivated by computational complexity concerns  cheeseman et al.  1 : where are the really hard problems  indeed  the worst case complexity analysis poorly accounts for the fact that  despite an exponential worst-case complexity  empirically  the complexity is low for most csp instances. these remarks led to developing the so-called phase transition framework  pt   hogg et al.  1   which considers the satisfiability and the resolution complexity of csp instances as random variables depending on order parameters of the problem instance  e.g. constraint density and tightness . this framework unveiled an interesting structure of the csp landscape. specifically  the landscape is divided into three regions: the yes region  corresponding to underconstrained problems  where the satisfiability probability is close to 1 and the average complexity is low; the no region  corresponding to overconstrained problems  where the satisfiability probability is close to 1 and the average complexity is low too; last  a narrow region separating the yes and no regions  referred to as phase transition region  where the satisfiability probability abruptly drops from 1 to 1 and which concentrates on average the computationally heaviest csp instances.
모the phase transition paradigm has been transported to relational machine learning and inductive logic programming  ilp  by  giordana and saitta  1   motivated by the fact that the covering test most used in ilp  muggleton and raedt  1  is equivalent to a csp. as anticipated  a phase transition phenomenon appears in the framework of ilp: a wide yes  respectively no  region includes all hypotheses which cover  resp. reject  all examples  and the hypotheses that can discriminate the examples lie in the narrow pt  where the average computational complexity of the covering test reaches its maximum.
모besides computational complexity  the pt phenomenon has far-reaching effects on the success of relational learning  botta et al.  1 . for instance  a wide failure region is observed: for all target concepts/training sets in this region  no learning algorithms among the prominent ilp ones could find hypotheses better than random guessing  botta et al.  1 .
모these negative results lead to a better understanding of the intrinsic limits of the existing ilp algorithms and search biases. formally  consider a greedy specialization  top-down  search strategy: starting its exploration in the yes region  the system is almost bound to make random specialization choices  for all hypotheses in this region cover every example on average. the yes region constitutes a rugged plateau from a search perspective  and there is little chance that the algorithm ends in the right part of the pt region  where good hypotheses lie. a similar reasoning goes for algorithms that follow a greedy generalization strategy.
모the phase transition paradigm thus provides another perspective on the pitfalls facing machine learning  focusing on the combinatoric search aspects while statistical learning focuses on the statistical aspects.
모the main question studied in this paper is whether the pt phenomenon is limited to relational learning  or threatens the feasibility and tractability of other learning settings as well.
모a learning setting with intermediate complexity between full relational learning and propositional learning is thus considered  that of grammatical inference  gi   pitt  1; sakakibara  1 . only the case of finite-state automata  fsa  section 1   will be considered through the paper. specifically  the phase transition phenomenon will be investigated with respect to three distributions on the fsa space  incorporating gradually increasing knowledge on the syntactical and search biases of gi algorithms.
모the first distribution incorporates no information on the algorithm and considers the whole space of fsa. using a set of order parameters  the average coverage of automata is studied analytically and empirically.
모the second one reflects the bias introduced by the generalization relations defined on the fsa space and exploited by gi algorithms. the vast majority of these algorithms first construct a least general generalization of the positive examples  or prefix tree acceptor  pta   and restrict the search to the generalizations of the pta  or generalization cone1.
모the third distribution takes into account the heuristics used by gi algorithms  guiding the search trajectory in the generalization cone. due to space limitations  the study is restricted to two prominent gi algorithms  namely rpni  oncina and garcia  1  and red-blue  lang et al.  1 .
모this paper is organized as follows. section 1 briefly introduces the domain of grammatical inference  the principles of the inference algorithms and defines the order parameters used in the rest of the paper. section 1 investigates the existence and potential implications of phase transition phenomenons in the whole fsa space  section 1  and in the generalization cone  section 1 . section 1 focuses on the actual landscape explored by gi algorithms  specifically considering the search trajectories of rpni and red-blue. section 1 discusses the scope of the presented study and lays out some perspectives for future research.
1 grammatical inference
after introducing general notations and definitions  this section briefly discusses the state of the art and introduces the order parameters used in the rest of the paper.
1 notations and definitions
grammatical inference is concerned with inferring grammars from positive  and possibly negative  examples. only regular grammars are considered in this paper. they form the bottom class of the hierarchy of formal grammars as defined by chomsky  yet are sufficiently rich to express many interesting sequential structures. their identification in the limit from positive examples only is known to be impossible  while it is feasible with a complete set of examples  gold  1 .
모it is known that any regular language can be produced by a finite-state automaton  fsa   and that any fsa generates a regular language. in the remaining of the paper  we will mostly use the terminology of finite-state automata. a fsa is a 1-tuple a = h q q1 f 붻i where  is a finite alphabet  q is a finite set of states  q1   q is the set of initial states  f   q is the set of final states  붻 is the transition function defined from q 뫄  to 1q.
모a positive example of a fsa is a string on   produced by following any path in the graph linking one initial state q1 to any accepting state.
모a finite state-automaton  fsa  is deterministic  dfa  if q1 contains exactly one element q1 and if  q 뫍 q  x 뫍
 card 붻 q x   뫞 1. otherwise it is non-deterministic  nfa . every nfa can be translated into an equivalent dfa  but at the price of being possibly exponentially more complex in terms of number of states. given any fsa a'  there exists a minimum state dfa  also called canonical dfa  a such that l a  = l a1   where l a  denotes the set of strings accepted by a . without loss of generality  it can be assumed that the target automaton being learned is a canonical dfa.
모a set s+ is said to be structurally complete with respect to a dfa a if s+ covers each transition of a and uses every element of the set of final states of a as an accepting state. clearly  l pta s+   = s+.
모given a fsa a and a partition 뷇 on the set of states q of a  the quotient automaton is obtained by merging the states of a that belong to the same block in partition 뷇  see  dupont et al.  1  for more details . note that a quotient automaton of a dfa might be a nfa and vice versa. the set of all quotient automata obtained by systematically merging the states of a dfa a represents a lattice of fsas. this lattice is ordered by the grammar cover relation . the transitive closure of  is denoted by . we say that iff
l a뷇i    l a뷇j . given a canonical dfa a and a set s+ that is structurally complete with respect to a  the lattice derived from pta s+  is guaranteed to contain a.
모from these assumptions  follows the paradigmatic approach of most grammatical inference algorithms  see  e.g.   coste  1; dupont et al.  1; pitt  1; sakakibara  1    which equates generalization with state merging operations starting from the pta.
1 learning biases in grammatical inference
the core task of gi algorithms is thus to select iteratively a pair of states to be merged. the differences among algorithms is related to the choice of:  i  the search criterion  which merge is the best one ;  ii  the search strategy  how is the search space explored ; and  iii  the stopping criterion.
we shall consider here the setting of learning fsas from
positive and negative examples  and describe the algorithms studied in section 1. in this setting  the stopping criterion is determined from the negative examples: generalization proceeds as long as the candidate solutions remain correct  not covering any negative example1
모the rpni algorithm  oncina and garcia  1  uses a depth first search strategy with some backtracking ability  favoring the pair of states which is closest to the start state  such that their generalization  fsa obtained by merging the two states and subsequently applying the determinisation operator  does not cover any negative example.
모the red-blue algorithm  also known as blue-fringe   lang et al.  1  uses a beam search from a candidate list  selecting the pair of states after the evidence-driven state merging  edsm  criterion  i.e. such that their generalization involves a minimal number of final states. red-blue thus also performs a search with limited backtracking  based on a more complex criterion and a wider search width than rpni.
1 order parameters
following the methodology introduced in  giordana and saitta  1   the pt phenomenon is investigated along socalled order parameters chosen in accordance with the parameters used in the abbaddingo challenge  lang et al.  1 :
  the number q of states in the dfa.
  the number b of output edges on each state.
  the number l of letters on each edge.
  the fraction a of accepting states  taken in  1 .
  the size |쑢 of the alphabet considered.
  the length ` of the test examples. also the maximal length ` of the learning examples in s+  as explained below .
모the study first focuses on the intrinsic properties of the search space  section 1  using a a random sampling strategy  all ` letters in the string being independently and uniformly drawn in  . in section 1  we examine the capacity of the studied learning algorithms to approximate a target automaton  based on positive sampling  where each training string is produced by following a path in the graph  randomly selecting an output edge in each step1.
1 phase transitions: the fsa space and the generalization cone
this section investigates the percentage of coverage of deterministic and non-deterministic finite-state automata  either uniformly selected  section 1   or selected in the subspace actually investigated by grammatical inference algorithms  that is  the generalization cone  section 1 .
1 phase transition in the whole fsa space
the sampling mechanism on the whole deterministic fsa space  dfa  is defined as follows. given the order parameter values  q b l a |쑢 :
  for every state q   i  b output edges  q q1  are created  where q1 is uniformly selected with no replacement among the q states;  ii  l 뫄 b distinct letters are uniformly selected in ; and  iii  these letters are evenly distributed among the b edges above.
  every state q is turned into an accepting state with probability a.
the sampling mechanism for nfa differs from the above in a single respect: two edges with same origin state are not required to carry distinct letters.
모for each setting of the order parameters  1 independent problem instances are constructed. for each considered fsa  the sampling mechanisms are detailed below   the coverage rate is measured as the percentage of covered examples among 1 examples  strings of length `  uniformly sampled.
모fig. 1 shows the average coverage in the  a b  plane  for |쑢 = 1  l = 1 and ` = 1  where the accepting rate a varies in  1  and the branching factor b varies in {1}. each point reports the average coverage of a sample string s by a fsa  averaged over 1 fsa drawn with accepting rate a and branching factor b  tested on 1 strings s of length ` .
모these empirical results are analytically explained from the simple equations below  giving the probability that a string of length ` be accepted by a fsa defined on an alphabet of size |쑢  with a branching factor b and l letters on each edge  in the dfa and nfa cases  the number of states q is irrelevant here .
 
		for a dfa
p accept  = for a nfa
모the coverage of the fsa decreases as a and b decrease. the slope is more abrupt in the dfa case than in the nfa case; still  there is clearly no phase transition here.

figure 1: coverage landscapes for deterministic and nondeterministic fsa  for |쑢=1  l=1 and `=1. the density of accepting states a and the branching factor b respectively vary in  1  and {1}.
1 pt in the generalization cone
the coverage landscape displayed in fig. 1 might suggest that grammatical inference takes place in a well-behaved search space. however  grammatical inference algorithms do not explore the whole fsa space. rather  as stated in section 1  the search is restricted to the generalization cone  the set of generalizations of the pta formed from the set s+ of the positive examples. the next step is thus to consider the search space actually explored by gi algorithms.
모a new sampling mechanism is defined to explore the dfa generalization cone:
1. |s+|  = 1 in the experiments  examples of length ` are uniformly and independently sampled within the space of all strings of length   `  and the corresponding pta is constructed;
1. n  = 1 in the experiments  ptas are constructed in that way.
1. k  = 1 in the experiments  generalization paths  leading from each pta to the most general fsa or universal acceptor  ua   are constructed;
in each generalization path  a1 = pta a1 ... at = ua   the i-th fsa ai is constructed from ai 1 by merging two uniformly selected states in ai 1  and subsequently applying the determinisation operator.
1. the generalization cone sample is made of all the fsasin all generalization paths  circa 1 fsas in the experiments .
모the sampling mechanism on the non-deterministic generalisation cone differs from the above in a single respect: the determinisation operator is never applied.
모fig. 1 shows the behaviour of the coverage in the dfa generalisation cone for |쑢 = 1 and ` = 1. each dfa a is depicted as a point with coordinates  q c   where q is the number of states of a and c is its coverage  measured as in section 1 . the coverage rate for each fsa in the sample is evaluated from the coverage rate on 1 test strings of length
`.
모fig. 1 similarly shows the behaviour of the coverage in the nfa generalisation cone  with |쑢 = 1 and ` = 1.
모fig 1  typical of all experimental results in the range of observation  |쑢 = 1 1  and ` = 1 1 1   shows a clear-cut phase transition. specifically  here  the coverage abruptly jumps from circa 1% to 1%; and this jump coincides with a gap in the number of states of the dfas in the generalization cone: no dfa with a number of states in  1  was found. the gap is even more dramatic as the length of the training and test sequences ` is increased.
모interestingly  a much smoother picture appears in the nondeterministic case  fig. 1 ; although the coverage rapidly increases when the number of states decreases from 1 to 1  no gap can be seen  neither in the number of states nor in the coverage rate itself1.
in the following  we focus on the induction of dfas.
1 phase transition and search trajectories
the coverage landscape  for the dfas  shows a hole in the generalization cone  with a density of hypotheses of coverage

figure 1: coverage landscape in the dfa generalization cone  |쑢 = 1  ` = 1 . at the far right stand the 1 pta sampled  with circa 1 states each. the generalization cone of each pta includes 1 generalization paths  leading from the pta to the universal acceptor. each point reports the coverage of a dfa  evaluated over a sample of 1 strings. this graph shows the existence of a large gap regarding both the number of states and the coverage of the dfas that can be reached by generalization.

figure 1: coverage landscape in the nfa generalization cone  with same order parameters as in fig. 1.
in between a large interval  typically between less than 1% to approximately 1%  falling abruptly. therefore  a random exploration of the generalization cone would face severe difficulties in finding a hypothesis in this region and would likely return hypotheses of poor performance if the target concept had a coverage rate in this  no man's land  interval.
모it is consequently of utmost importance to examine the search heuristics that are used in the classical grammatical inference systems. first  are they able to thwart the a priori very low density of hypotheses in the gap  second  are they able to guide the search toward hypotheses of appropriate coverage rate  specially if this coverage falls in the gap 
모the study will thus focus on two standard algorithms in grammatical inference  namely the rpni and the red-blue algorithms  oncina and garcia  1; lang et al.  1 .
1 experimental setting
previous experiments considered training sets made of positive randomly drawn strings sequences only. however  in order to assess the performance of learning algorithms  the hypothesis learned must now be compared to the target automaton. therefore  another experimental setting is used in this section  with the sampling of target automata  and the construction of training and test sets. these data sets include positive and negative examples as most gi algorithms  and specifically rpni and red-blue  use negative examples in order to stop the generalization process.
모in our first experiments  we tested whether heuristically guided inference algorithms can find good approximations of the target automata considering target automata with approximately  i  1% coverage rate  as considered in the influential abbadingo challenge  and in the middle of the  gap    and  ii  1% coverage rate.
모for each target coverage rate  we used the experimental setting described in  lang et al.  1  in order to retain a certain number of target automata with a mean size of q states  q = 1  in our experiments . for each automaton then  we generated n  =1  training sets of size |s|  = 1  labeled according to the target automaton  with an equal number of positive and negative instances  |s+| = |s | = 1  of length ` = 1. the coverage rate was computed as before on 1 uniformly drawn strings  with no intersection with the training set .
모in a second set of experiments  we analyzed the learning performances of the algorithms with respect to test errors  both false positive and false negative.
모in these experiments  we chose the type of target automata by setting the number of states q and some predetermined structural properties1.
1 the heuristically guided search space
due to space limitation  only the graph obtained for the rpni algorithm is reported  see figure 1   with three typical learning trajectories. similar results were obtained with the redblue algorithm.
모one immediate result is that both the rpni and the edsm heuristics manage to densely probe the  gap . this can explain why the gap phenomenon was not discovered before  and why the red-blue algorithm for instance could solve some cases of the abbadingo challenge where the target concepts have a coverage rate of approximately 1%. however  where rpni tends to overspecialize the target automaton  red-blue tends to overgeneralize it by 1% to 1%.
모in order to test the capacity of the algorithms to return automata with a coverage rate close to the target coverage  we repeated these experiments with target automata of coverage rate of approximately 1%. the results  figure 1  shows that  in this case  rpni ends up with automata of coverage 1 to 1 times greater than the target coverage. the effect is even more pronounced with red-blue which returns automata of average coverage rate around 1%!

figure 1: three rpni learning trajectories for a target concept of coverage=1%. their extremity is outlined in the oval on the left. the doted horizontal line corresponds to the coverage of the target concept. the cloud of points corresponds to random trajectories.

figure 1: same as in figure 1  except for the coverage of the target concept  here 1%.
1 generalization error
table 1  obtained for different sizes of the target automata and for training sets of structural completeness above 1%  confirms that both rpni and red-blue return overgeneralized hypotheses. on one hand  their average coverage is vastly greater than the coverage of the target automata  on the other hand  they tend to cover only part of the positive test instances  while they cover a large proportion of the negative test instances. this shows that the heuristics used in both rpni and red-blue may be inadequate for target concepts of low coverage.
1 conclusion
this research has extended the phase transition-based methodology  botta et al.  1  to the grammatical inference framework. ample empirical evidence shows that the search landscape presents significant differences depending on the search operators that are considered.
algo.qcucovcqfucovfpcovfncovfrb1.1.1.1.1.1rb1.1.1.1.1.1rb1.1.1.1.1.1rb1.1.1.1.1.1rpni1.1.1.1.1.1rpni1.1.1.1.1.1rpni1.1.1.1.1.1rpni1.1.1.1.1.1table 1: performances of red-blue  rb  and rpni for target dfa of sizes q = 1  1  1 and 1 states. qf  ucovf  pcovf and ncovf respectively denote the average size of the learned automata  their average coverage  the true positive and the false positive rates.
모a first result is that random search appears to be more difficult in the dfa generalization cone than in the whole search space: a large gap was found  in terms of hypothesis coverage and size. this remark explains why sophisticated search biases are needed for grammatical inference algorithms in the problem range corresponding with the hole in the generalization cone.
모a second finding regards the limitations of the search operators in rpni and red-blue  especially outside the region of the abbadingo target concepts. experiments with artificial learning problems built from target concepts with coverage less than 1% reveal that rpni and red-blue alike tend to learn overly general hypotheses; with respect to both the size  estimated by the number of states  and the coverage of the hypotheses  often larger by an order of magnitude than that of the target concept. what is even more worrying  is that this overgeneralization does not imply that the found hypotheses are complete: quite the contrary  the coverage of the positive examples remains below 1%  in all but one setting.
모the presented study opens several perspectives for further research. first  it suggests that the learning search  and especially the stopping criterion  could be controlled using a hyper-parameter: the coverage rate of the target concept  possibly supplied by the expert  or estimated e.g. by crossvalidation . in other words  the stopping criterion of the algorithms might be reconsidered. secondly  more conservative generalisation operators will be investigated. preliminary experiments done with e.g. reverted generalisation  same operator as in rpni  applied on the reverted example strings  show that such operators can delay the determinisation cascade  and offer a finer control of the final coverage rate of the hypotheses.
모finally  the main claim of the paper is that the phase transition framework can be used to deliver indications regarding when and where specific search biases might fail   hopefully leading to understand and ultimately alleviate their limitations.
acknowledgments
the last two authors are partially supported by the pascal network of excellence ist-1 1.
