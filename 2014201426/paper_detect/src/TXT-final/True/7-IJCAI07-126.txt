
the definition of object  e.g.  data point  similarity is critical to the performance of many machine learning algorithms  both in terms of accuracy and computational efficiency. however  it is often the case that a similarity function is unknown or chosen by hand. this paper introduces a formulation that given relative similarity comparisons among triples of points of the form object i is more like object j than object k  it constructs a kernel function that preserves the given relationships. our approach is based on learning a kernel that is a combination of functions taken from a set of base functions  these could be kernels as well . the formulation is based on defining an optimization problem that can be solved using linear programming instead of a semidefinite program usually required for kernel learning. we show how to construct a convex problem from the given set of similarity comparisons and then arrive to a linear programming formulation by employing a subset of the positive definite matrices. we extend this formulation to consider representation/evaluation efficiency based on formulating a novel form of feature selection using kernels  that is not much more expensive to solve . using publicly available data  we experimentally demonstrate how the formulation introduced in this paper shows excellent performance in practice by comparing it with a baseline method and a related state-of-the art approach  in addition of being much more efficient computationally.
1 introduction and related work
the definition of a distance or a similarity function over the input or data space is fundamental in the performance of machine learning algorithms  both in terms of accuracy and efficiency. this can be easily verified by looking at the role that the distance or similarity plays in common algorithms
 e.g.  k-means  nearest neighbors  support vector machines or any other kernel method  etc. . however  since the concept of similarity depends on the task of interest  objects can be similar or dissimilar in many ways depending on the application   a similarity function that is appropriate for one task may not be appropriate for another. this task dependency has been noted in a number of approaches that seek to build these functions for a particular classification or regression model given some training data  e.g.   cohen et al.  1; xing et al.  1; schultz and joachims  1; lanckriet et al.  1; athitsos et al.  1  .
¡¡in this paper  we are interested in automatically finding good similarity functions after some basic information about the task of interest has been provided  e.g.  by an expert user . unlike approaches that rely on class labels  e.g.   lanckriet et al.  1   or on building sets of points that are globally similar  or dissimilar   xing et al.  1; wagstaff et al.  1   here we explore a simpler-to-specify form of user supervision: that provided by statements like object i is more similar to object j than to object k1. we remark that we are not only interested on building functions that are consistent with this information provided by the user  but we are also interested in obtaining functions that are efficient to compute. thus  in addition we focus on functions that can be evaluated by looking only at part of the features or input dimensions; therefore  implying some form of feature selection.
   more formally  let us represent the objects of interest as points xk in a d-dimensional space d  with k = {1 ... n}. we would like to obtain a function k :  that satisfies the above similarity comparisons and that in addition can be well approximated by which only uses d   d components of xk. for this we will rely on  mercer  kernel representations  cristianini and shawe-taylor  1  and define k  similarly for k   as a kernel where; thus  k is a mixture of kernel functions1. throughoutthis paper  we will work with this representation of similarity functions  define convex problems for obtaining k  and later extend this idea to focus on k . some of the motivation for the framework in this paper is related to earlier work in metric learning  rosales and fung  1 ; however  here the focus is on a different problem  kernel design.
¡¡in this paper  we first show how relative constraints  involving triples of points of the form  k xi xj    k xi xk  can be used to learn k. one can think of this first formulation as parallel to other approaches for learning k that rely on supervised learning with class labels  and  unlike the one presented here  are classification-algorithm dependent . these formulations involve solving a semidefinite programming problem  sdp   graepel  1; lanckriet et al.  1   but we show how a different linear programming formulation  introduced in this paper  is also sufficient in practice and much more efficient computationally  e.g.  it can be use to solve much larger problems  faster . in addition  we extend this formulation to consider the issue of representation/evaluation efficiency based on a form of feature selection. this formulation leads to a linear programming approach for solving the kernel design problem that also optimizes for succinct feature representations.
1 kernel design
in recent years the idea of kernel design for learning algorithms has received considerable attention in the machine learning community. traditionally  an appropriate kernel is found by choosing a parametric family of well-known kernels  e.g.  gaussian or polynomial  and then learning a generally sub-optimal set of parameters by a tuning procedure. this procedure  although effective in most cases  suffers from important drawbacks:  1  calculating kernel matrices for a large range of parameters may become computationally prohibitive  especially when the training data is large and  1  very little  to none  prior knowledge about the desired similarity can be easily incorporated into the kernel learning procedure. more recently  several authors  bennett et al.  1; lanckriet et al.  1; fung et al.  1  have considered the use of a linear combination of kernels that belong to a family or superset of different kernel functions and parameters; this transforms the problem of choosing a kernel model into one of finding an optimal linear combination of the members of the kernel family. using this approach there is no need to predefine a kernel; instead  a final kernel is constructed according to the specific classification problem to be solved. in this paper we also use a mixture model representation1; however  our approach does not depend on and is not attached to any specific machine learning algorithm  for classification  regression  inference  etc . instead  inspired by the fact that most kernels can be seen as similarity functions  we rely on explicit conditions on similarity values among subset of points in the original training dataset; this is  our formulation allows the user to explicitly ask for conditions that have to be satisfied or reflected in the desired kernel function to be learned.
1 relative similarity constraints
as explained above  we concentrate on examples of proximity comparisonsamong triples of objects of the type object i is more like object j than object k. in choosing this type of relative relationships  we were inspired primarily by  athitsos et al.  1; cohen et al.  1; schultz and joachims  1; rosales and fung  1   where these relationships are defined with respect to distances or rankings. the use of relative similarity constraints of this form for kernel design offers different challenges. note that we are not interested in preserving absolute similarities  which are in general much more difficult to obtain  absolute similarities imply relative similarities  but the converse is not true .
¡¡one important observation for kernel design is that the kernel conditions implied by these relationships are linear relationships among individual kernel entries in the kernel matrix  defined over the points of interest  and can be expressed as linear constraints. in addition to this  unlike any of the methods referred to above  by restricting the kernel family to have only kernels that depend in one original feature at the time  we are able to learn kernels that depend on a minimal set of the original features. this can be seen as implicitly performing feature selection with respect to the original data space. in combination with the benefits of a new linear programming approach  the formulation proposed in this paper has a unique set of attributes with significant advantages over recent approaches for kernel design.
1 notation and background
in the following  vectors will be assumed to be column vectors unless transposed to a row vector by a superscript . the scalar  inner  product of two vectors x and y in the ddimensional real space d will be denoted by xy. the 1norm and 1-norm of x will be denoted by  and 
respectively. a column vector of ones of arbitrary dimension will be denoted bye  and one of zeros will be denoted by1. a kernel matrix for a given set of points will be denoted by k  individual components by kij  and kernel functions by k   or simply k.
1 a linear programming formulation for kernel design
1 using a linear combination of kernels
let us say we are given a set of points x with k = {1 ... n} for which an appropriatesimilarity function is unknown or expensive to compute. in addition we are given information about a few relative similarity comparisons. formally  we are given a set t = { i j k |k xi xj    k xi xk } for some kernel function k. the kernel k is not known explicitly  instead a user may only be able to provide these similarity relationships sparsely or by example. we are interested in finding a kernel function k   that satisfies these relationships.
¡¡for the rest of the paper  let us suppose that instead of the kernel k being defined by a single kernel mapping  e.g.  gaussian  polynomial  etc.   the kernel k is instead composed of a linear combination of kernel functions kj j = {1 ... k}  as below:
	.	 1 
¡¡as pointed out in  lanckriet et al.  1   the set ¦¸ = {k1 x y  ... kk x y } can be seen as a predefined set of initial guesses of the kernel matrix. note that the set ¦¸ could contain very different kernel matrix models  all with different parameter values. the goal is then to find a kernel function k¦Á that satisfies relative kernel value constraints  specified by the user as a set t  . a general formulation for achieving this is given by:

k 
where t are slacks variables  t indexes t   the function h k¦Á  is a regularizer on the kernel matrix k¦Á  or alternatively the kernel function k¦Á  for capacity control  and the parameter ¦Ã ¡Ý 1 controls the trade-off between constraint satisfaction and regularization strength  usually obtained by tuning . note that this formulation seems sufficient  we can optimize the set of values ¦Ái in order to obtain a positive semidefinite  psd  linear combination k¦Á x y  =  suitable for the specific task at hand. this formulation  however requires solving a relatively expensive semidefinite program  sdp . enforcing ¦Ái ¡Ý 1  i   fung et al.  1; lanckriet et al.  1  and moreover restricting all the kernels in the set ¦¸ to be psd results in k¦Á being psd. however this strongly limits the space of attainable kernel functions k¦Á.
¡¡instead of using these restrictions  we explore an alternative  general definition that allows us to consider any kernel function in ¦¸ and anywithout compromisingcomputational efficiency  e.g.  without requiring us to solve a sdp . for this we will explicitly focus on a well characterized subfamily of the psd matrices: the set of diagonal dominant matrices. in order to provide a better understanding of the motivation for our next formulation we present the following theorem as stated in  golub and van loan  1 :
theorem 1 diagonal dominance theorem suppose that  is symmetric and that for each i = 1 ... n   we have:
 
then m is positive semi-definite  psd . furthermore  if the inequalities above are all strict  then m is positive definite.
based on the diagonal dominance theorem  for matrices with positive diagonal elements  above  by simplifying the notation letting kij¦Á ¡Ô k¦Á xi xj   we arrive to the following alternative formulation:

s.t

 1 
where and s is a set of training points  e.g.  those points used to define the triples in t  .
finally  defining auxiliary variables  and ¦Ã1 ¡Ý 1  formulation  1  can be rewritten as a linear programming problem in the following way:

s.t

 1 
where  same dimensionality as ¦Á  and sj ¡Ý 1. note that to simplify notation we have overloaded the variables i j k  but their meaning should be clear from the context . in order to better understand the motivation for formulation  1  it is important to note that:
 i  minimizing 	is	equivalent	to	minimizing  since.
 ii  since we are implicitly minimizing  at the optimal solution to problem  1   we have that:
 iii  combining     and  ii  we obtain:

which implies that k¦Á  is diagonal dominant and hence positive semidefinite.
1 learning kernels that depend on fewer input features  k  
next  we will modify formulation  1  in order to learn kernels that depend in a small subset of the original input features. as far as we know  all of the existing direct objective optimization methods for feature selection with kernels perform feature selection in the implicit feature space induced by the kernel and not with respect to the original feature space. doing this using traditional kernels  leads to nonlinear  nonconvex optimization problemsthat are difficult to solve due to the nonlinearity relations among the original features introduced by the kernel mappings.
¡¡in orderto overcomethis difficultywe propose a simple but effective idea. instead of using kernels that depend on all the features we will only consider a set ¦¸¡¥ comprised of weak kernels that depend on only one feature at a time. for example  if xi =  x1i  ... xfi  ... xdi   and xj =  xj1 ... xfj  ... xdj   are two vectors on d  then a weak gaussian kernel only depending on feature f is defined by:
		 1 
let us denote by if the set of indices i of kernels ki ¡Ê ¦¸¡¥ that only depend on feature f for f ¡Ê {1 ... d}. then  any linear combination of weak kernels in ¦¸¡¥ can be written as:
d
		 1 
note that if ¦Áp = 1   p ¡Ê if for a given f  this implies that kij¦Á does not depend on the original feature f. this motivates our next formulation for feature selection that uses weak onedimensional kernels:

s.t

 1 
where now s is indexed by the feature number f rather than the kernel number alone. it is interesting to note that for each feature f  formulation  1  is minimizing mf = max{|¦Áp|/p ¡Ê if}. this is appropriate since:
	p	f
	 	 i j kij¦Á does not depend on featuref.
 1 
1 numerical evaluation
for our experimental evaluation we used a collection of nine publicly available datasets  part of the uci repository1. a summary of these datasets is shown in table 1. these datasets are commonly used in machine learning as a benchmark for performance evaluation. our choice of datasets is motivated primarily by their use in evaluating a competing approach  xing et al.  1  aimed to learn distance functions.
¡¡we evaluate our approach against single kernels by comparing against the standard gaussian  at various widths   linear  and polynomial kernels. these kernels are also the ones used as the basis for our mixture kernel design; thus it is a reasonable baseline comparison. we compareboth of our formulations. one formulation attempts to perform implicit feature selection by defining weak kernels  while the other uses full kernel matrices  that depend on all input dimensions .
¡¡we have also chosen to compare our formulation against that proposed in  xing et al.  1 . this obeys various reasons. in addition to being a state-of-the-art method  the primary reason for our choice is that it uses a similar  but not identical  type of supervision  as explained below. unlike other related approaches  computer code and data has been made public for this method1. in addition  this method outperformed a constrained version of k-means  wagstaff et al.  1  in the task of finding good clusterings.
table 1: benchmark datasets
	name	pts  n 	dims  d 	classes

1 housing-boston11 ionosphere11 iris11 wine11 balance scale11 breast-cancer wisc.11 soybean small11 protein11 pima diabetes11.1 evaluation settings
the datasets employed in these experiments are generally used for classification since class labels are available. however  the various methods to be compared here do not require explicit class labels. the method introduced in this paper requires relative similarity information among a subset of points  clearly class labels provide more information . we use the available class labels to generate a set of triples with similarity comparisons that respect the classes. more explicitly  given a randomly chosen set of three points  from the training set   if two of these belong to the same class and a third belongs to a different class  then we place this triple in our set t  i.e.  i and j are the points in the same class  k is the remaining point . for the case of  xing et al.  1   the supervision is in the form of two sets  one called a similar set and the other a dissimilar set. in order to identify these sets  we can again use the class labels  now to build a similar set of pairs  likewise for a dissimilar set of pairs . given this level of supervision  this method attempts to find an optimal mahalanobis distance matrix to have same-class points closer to each other than different-class points  see  xing et al.  1  for details .
¡¡for every triple  i j k  ¡Ê t used in our approach for learning  we use  i j  ¡Ê s and  i k  ¡Ê d for learning in  xing et al.  1 ; where s and d are the similar and dissimilar sets. we believe this provides a fair level of supervision for both algorithms since roughly the same information is provided. it is possible to obtain a superset of t from s and d  and by construction s and d can be obtained from t .
¡¡in order to evaluate performance for the various methods  we use a 1/1 split of the data into training and testing  for the methods where training is required . from the training portion  we generate 1 triples  as explained above  for actual training. this information is provided  in the appropriate representation  to both algorithms. for testing  we repeatedly choose three points at random  and if their class labels imply that any two points are more similar to each other than to a third  i.e.  again if two points have the same class and a
third has a differentclass label   then we check that the correct relationships were learned. that is  whether the two points in the same class are more similar  or closer  to each other than any of these points  chosen at random  to the third point. this same measure is used for all algorithms. thus  we define the percentage correct simply as the proportion of points from the test set  sampled at random  that respect the class-implied similarity or distance relationship.
¡¡our method requires setting two balancing parameters. we set them by using cross validation by splitting the training set in two halves. the values tested  for both parameters  were {1 1 1 1 1}. these parameters have an effect on the number of dimensions employed since a higher value for ¦Ã1 favors using fewer kernels  or dimensions . likewise  larger values for ¦Ã1 favors kernel matrices with a smaller trace.
1 discussion and results
fig. 1 shows the performance of our approach compared against various gaussian and polynomial kernels. a linear kernel performed almost identically to the polynomial kernel of degree two and it is omitted in the graph. the mean and standard deviation  of the performance measure  for each individual kernel was computed from 1 random samplings of the dataset. in order to be consistent across approaches  the number of samples used for testing was set to 1 triples.
¡¡one could expect an optimal mixture of kernels to provide a higher performancein both cases  full and weak kernel mixtures  when compared with single kernels. this is generally the case. it can be seen in the figure that in most cases single predetermined kernels are suboptimal. in the case of weak kernels  the performance is always better than single kernels. however  for the case of mixtures of full kernels there are cases where a single kernel provides a higher performance in average. this can be at least due to two reasons:  1  simply unlucky selection  sampling  of test points. in datasets 1  and 1  the standard deviation is large enough to justify this possibility. a much more interesting reason is  1  the imposed restrictions on the solution space of the mixture kernel  e.g.  dataset 1 . recall that we concentrated our solution on a subspace of the psd matrices  that of diagonal dominant matrices. if the full cone of psd matrices were to be incorporated as solution space  the kernel mixture could perform as good as any base kernel  since a mixture of a single kernel is a valid solution . note however that  although this is possible  one would be required to pay much higher computational costs  e.g.  for large datasets or number of constraints .
¡¡interestingly  the performance for the mixture of weak kernels is superior. this can be due to the larger number of degrees of freedom  the number of ¦Á mixing parameters is larger  or also a feature selection effect on overfitting. however  even though this is the case for the datasets considered in this paper  and results suggest that this may often be the case   we remark that this result is not theoretically guaranteed since non-linear interactions among multiple dimensions or features may not be representable using a linear combination of single features.
¡¡fig. 1 shows the average optimal number of dimensions found by this process in a 1-fold cross validation experiment and the correspondingone-standard-deviationerror bars. the number of dimensions was identified by counting the number of ¦Á's larger than 1. this automatic choice of dimensionality was done using cross-validation as explained above and is a valuable property of the method presented. note that the

figure 1: performance of single kernels and our approachs in the nine uci datasets. we show both instances of our approach  using full kernel matrices and weak kernels . bars show performance results on 1 random splits of training/test points  training only applicable to kernel mixtures . performance is measured in terms of the percentage of randomly chosen points  1  from test set whose distance relationship respect the class labels. the number of triples used for training for all runs was 1. error bars show one standard deviation.
method effectively reduces the number of features for all but one dataset.
¡¡fig. 1 shows the comparison of the present formulation  using weak kernels  against  xing et al.  1 . we show percentage correct averaged over 1 random splits of the data along with one-standard-deviation bars. for each of the 1 splits  1 triples from the test set are randomly chosen. when comparing the performance of both methods  we note that  except for dataset 1  our method clearly outperforms the competing approach. interestingly  this dataset was the same for which the optimal number of dimensions was determined to always be equal to the original dimensionality.
¡¡in addition to the implicit non-linear representations implied by the kernels employed  we believe that a key reason for the superior performanceof the mixture of weak kernels is the automatic identification of relevant dimensions. this reduction in dimensionality appears to provide an important advantage at the time of generalization. it is generally accepted that a simpler representation is preferable  e.g.   blumer et al.  1   and it can reduce overfitting in practice.
¡¡from a computational efficiency perspective at test time  being able to represent the original data more succinctly is especially advantageous. in particular  when similarities can be calculated directly using a low-dimensional representation  computationaltime savings can be significant for on-line applications. the projection step in this approach can be precomputed off-line. in retrieval applications  e.g.  query-byexample   the objects can be stored in their low-dimensional representation. from a conceptual point of view  this formulation also has the advantage of providing a more effective tool for understanding the data since it can identify whether variables  dimensions  are of high or low relevance for a task of interest.

figure 1: dimensionality reduction. total number of dimensions and average number of dimensions  along with one-standarddeviation error bars  found by our algorithm for each dataset using the optimal parameters ¦Ã1 ¦Ã1. averages are computed over 1 random splits of training/test points  and 1 triples per run.

figure 1: performance comparison between competing approach and our weak kernel approach in the nine uci datasets. bars show performance results on 1 random splits of training/test points. performance is measured in terms of the percentage of randomly chosen points  1  from test set whose distance relationship respect the class labels. the number of triples used for training for all runs was 1. error bars show one standard deviation.
1 conclusions and future work
we presented a novel formulation for learning kernel functions fromrelative similarity information. our generalformulation consisted of a convex optimization problem requiring semidefinite programming to be solved. we designed a practical approximate problem  requiring linear programming instead. in particular  we showed how the diagonal dominance constraint on the kernel matrix leads to a general problem that can be solved very efficiently. in addition to this  we have shown how to extend the formulation to allow for implicit feature selection using a combination of kernels. experiments indicated that our weak kernel formulation outperforms a state-of-the-art approach that uses similar information to learn a distance  rather than a similarity function .
¡¡although relative similarity constraints are used in this paper  other constraints could potentially be imposed. for example  we could have sign or magnitude restrictions on certain kernel components  we can impose local isometry constraints  similar to  weinberger et al.  1    or we can request the kernel to preserve local distances.
¡¡we believe this paper has potential implications in other problems. our results suggest that the sdp problem could  in some cases  be replaced by a linear programming problem. we suggested one way to achieve this goal; this has the potential to be applied to other problems whose solution involves sdp.
