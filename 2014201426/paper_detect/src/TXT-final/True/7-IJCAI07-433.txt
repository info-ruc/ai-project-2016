
the internet is full of information sources providing various types of data from weather forecasts to travel deals. these sources can be accessed via web-forms  web services or rss feeds. in order to make automated use of these sources  one needs to first model them semantically. writing semantic descriptions for web sources is both tedious and error prone. in this paper we investigate the problem of automatically generating such models. we introduce a framework for learning datalog definitions for web sources  in which we actively invoke sources and compare the data they produce with that of known sources of information. we perform an inductive search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. the paper includes an empirical evaluation demonstrating the effectiveness of our approach on real-world web sources.
1 introduction
we are interested in making use of the vast amounts of information available as services on the internet. in order to make this information available for structured querying  we must first model the sources providing it. writing source descriptions by hand is a laborious process. given that different services often provide similar or overlapping data  it should be possible to use knowledge of previously modeled services to learn descriptions for newly discovered ones.
모when presented with a new source of information  such as a web service   the first step in the process of modeling the source is to determine what type of data it requires as input and what type of data it produces as output. in previous work  he  and kushmerick  1; lerman et al.  1   researchers have addressed the problem of classifying the attributes of a service into semantic types  such as zipcode . once the semantic types for the inputs are known  we can invoke the service  but are still not able to make use of the data it returns. to do that  we need also to know how the output attributes relate to the input. for example  a weather service may return a temperature value when queried with a zipcode. the service is not very useful  until we know whether the temperature being returned is the current temperature  the predicted high temperature for tomorrow  or the average temperature for this time of year. these three possibilities can be described by datalog rules as follows:  note that the $symbol is used to distinguish the input attributes of a source. 
1 source $zip temp  :- currenttemp zip  temp . 1 source $zip temp  :- forecast zip temp .
1 source $zip temp  :- averagetemp zip  temp .
the expressions state that the input zipcode is related to the output temperature according to domain relation called currenttemp  forecast  and averagetemp respectively  each of which is defined in some domain ontology. in this paper we describe a system capable of inducing such definitions automatically. the system leverages what it knows about the domain  namely the ontology and a set of known sources  to learn a definition for a newly discovered source.
1 an example
we introduce the problem of inducing definitions for online sources by way of an example. in the example we have four semantic types  namely: zipcode  distance  latitude and longitude. we also have three known sources of information  each of which has a definition in datalog. the first source  aptly named source1  takes in a zipcode and returns the latitude and longitude coordinates of its centroid. the second calculates the great circle distance between two pairs of coordinates  while the third converts a distance from kilometers into miles. definitions for the sources are as follows:
source1 $zip  lat long :- centroid zip lat long .
source1 $lat1  $long1 $lat1 $long1 dist :greatcircledist lat1  long1 lat1 long1 dist . source1 $dist1  dist1 :- convertkm1mi dist1  dist1 .
the goal in this example is to learn a definition for a new service  called source1  that has just been discovered on the internet. this new service takes in two zipcodes as input and returns a distance value as output:1source1 $zip  $zip distance 
the system described in this paper takes this type signature as well as the definitions for the known sources and searches for an appropriate definition for the new source. the definition discovered in this case would be the following conjunction of calls to the known sources:
source1 $zip1  $zip1 dist :source1 zip1 lat1 long1   source1 zip1 lat1 long1   source1 lat1 long1 lat1 long1 dist1   source1 dist1 dist .
this definition states that the output distance can be calculated from the input zipcodes  by first giving those zipcodes to source1  calculating the distance between the resulting coordinates using source1  and then converting the distance into miles using source1. to test whether this source definition is correct the system must invoke the new source and the definition to see if the values generated agree with each other. the following table shows such a test:
$zip1$zip1dist  actual dist  predicted 111111111in the table  the input zipcodes have been selected randomly from a set of examples  and the output from the source and the definition are shown side by side. since the output values are quite similar  once the system has seen a sufficient number of examples  it can be confident that it has found the correct semantic definition for the new source.
모the definition above was written in terms of the source predicates  but could just as easily have been written in terms of the domain relations. to do so  one needs to replace each source predicate by its definition as follows:
source1 $zip1  $zip1 dist :centroid zip1 lat1 long1   centroid zip1 lat1 long1   greatcircledist lat1  long1 lat1 long1 dist1   convertkm1mi dist1  dist1 .
written in this way  the new definition for source1 makes sense at an intuitive level: the source is simply calculating the distance in miles between the centroids of the zipcodes.
1 problem formulation
we are interested in learning definitions for sources by invoking them and comparing the output they produce with that of known sources of information. we formulatethe problemas a tuple  where t is a set of semantic data-types  r is a set of domain relations  s is a set of known sources  and s  is the new source. each of the semantic types comes with a set of example values and a function for checking equality between values of the type. the set of relations r may include interpreted predicates  such as 뫞. each source s 뫍 s is associated with a type signature  a binding constraint  that distinguishes input from output  and a view definition  that is a conjunctive query over the relations in r. the new source to be modeled s   is described in the same way  except that its view definition is unknown. the solution to the source definition induction problem is a definition for this source.
모by describing sources using the powerful language of conjunctive queries  we are able to model most information sources on the internet  as sequential compositions of simple functionality . we do not deal with languages involving more complicated constructs such as aggregation  union or negation because the resulting search space would be prohibitively large. finally  we assume an open-world semantics  meaning that sources may be incomplete with respect to their definitions  they may not return all the tuples implied by their definition . this fact complicates the induction problem and is addressed in section 1.
1 algorithm
the algorithmused to searchfor and test candidatedefinitions takes as input a type signature for the new source  also called the target predicate . the space of candidate definitions is then enumerated in a best-first manner  in a similar way to top-down inductive logic programming  ilp  systems like foil  cameron-jones and quinlan  1 . each candidate produced is tested to see if the data it returns is in some way similar to the target:
1 invoke target with set of random inputs;
1 add empty clause to queue;
1 while v 뫹 best definition from queue;
algorithm 1: best-first search algorithm
1 invoking the source
the first step in our algorithm is to generate a set of tuples that will represent the target predicate during the induction process. in other words  we try to invoke the new source to sample some data. doing this without biasing the induction process is not trivial. the system first tries to invoke the source with random combinations of input values taken from the examples of each type. many sources have implicit restrictions on the combination of input values. for example  a geocoding service which takes a number  street  and zipcode as input  may only return an output if the address actually exists. in such cases  randomly combining values to form input
tuples is unlikely to result in any successful invocations. after failing to invoke the source a number of times  the system will try to generate examples from other sources whose output contains the required combination of attribute types. frequency distributions can also be associated with the example values of each semantic type  such that common constants  like ford  can be chosen more frequently than less common ones  like ferrari .
1 generating candidates
once the system has assembled a representative set of tuples for the new source  it starts generating candidate definitions by performing a top-down best-first search through the space of conjunctions of source predicates. in other words  it begins with a very simple source definition and builds ever more complicated definitions by adding one literal  source predicate  at a time to the end of the best definition found so far. it keeps doing this until the data produced by the definition matches that produced by the source being modeled. for example  consider a newly discovered source that takes in a zipcode and a distance  and returns all the zipcodes that lie within that radius  along with their respective distances . the target predicate representing the source is:
           source1 $zip1  $dist1 zip1 dist1  now assume we have one known source  namely source1 from the previous example:
source1 $zip1  $zip1 dist 
and we also have the interpreted predicate:
뫞 $dist1 $dist1 
the search for a definition for source1 might then proceed as follows. the first definition generated is the empty clause:
source1 $  $      .
the null character    represents a don't care variable  which means that none of the inputs or outputs have any restrictions placed on their values. literals  source predicates  are then added one at a time to refine this definition.1 doing so produces the following candidate definitions  among others:
source1 $zip1  $dist1      :- source1 zip1   dist1 . source1 $zip1  $ zip1   	:- source1 zip1 zip1   . source1 $ $dist1   dist1  :- 뫞 dist1 dist1 .
note that the semantic types in the signature of the target predicate limit greatly the number of candidate definitions produced. the system checks each of these candidates in turn  selecting the best one for further expansion. assuming that the first of the three scores the highest  it would be expanded to form more complicated candidates  such as:
source1 $zip1  $dist1   dist1  :-
source1 zip1   dist1   뫞 dist1 dist1 .
the size of the search space is highly dependent on the arity of the sources. sources with multiple attributes of the same type make for an exponential number of possible definitions at each expansion step. to limit the search in such cases  we first generate candidates with a minimal number of join variables in the final literal and progressively constrain the best performing definitions  by further equating variables .
모the rationale for performing search over the source predicates rather than the domain predicates is that if the search were performed over the latter an additional query reformulation step would be required each time a definition is tested.
in practice  the fact that definitions for the known sources may contain multiple literals means that many different conjunctions of domain predicates will reformulate to the same conjunction of source predicates  resulting in a much larger search space. for this reason  we perform the search over the source predicates and rely on post-processing to remove redundantliterals from the unfoldingof the definition produced.
1 limiting the search space
the search space generated by this top-down search algorithm may be very large even for a small number of sources. as the number of sources available increases  the search space becomes so large that techniques for limiting it must be used. we employ some standard  and other not so standard  ilp techniques for limiting this space:
1. maximum clause length
1. maximum predicate repetition
1. maximum existential quantification level
1. definitions must be executable
1. no repetition of variables allowed within a literal
such limitations are often referred to as inductive search bias or language bias  ne뫣dellec et al.  1 . the first restriction limits the length of the definitions produced  while the second limits the number of times the same source predicate can appear in a given candidate. the third restricts the complexity of the definitions by reducing the number of literals that do not contain variables from the head of the clause.1 the fourth
requires that source definitions can be executed from left to right  i.e.  that the inputs of each source appear in the head of the clause or in one of the literals to the left of that literal. finally  we disallow definitions in which the same variable appears multiple times in the same literal  in the body of the clause . for example  the following definition which returns the distance between a zipcode and itself  would not be generated  because zip1 appears twice in the last literal:
source1 $zip1  $   dist1  :- source1 zip1  zip1 dist1 .
such definitions occur rarely in practice  thus it makes sense to exclude them  thereby greatly reducing the search space.
1 comparing candidates
we proceed to the problem of evaluating candidate definitions. the basic idea is to compare the output produced by the source with that produced by the definition on the same input. the more similar the tuples produced  the higher the score for the candidate. we then average the score over different input tuples to see how well the candidate describes the source overall. in the motivating example  a single output tuple  distance value  was produced for every input tuple  pair of zipcodes . in general  multiple output tuples may be produced by a source  as was the case for source1 . thus the system needs to compare the set of output tuples produced by the target with those produced by the definition to see if any of the tuples are the same. since both the new source and the known sources can be incomplete  the two sets may simply overlap  even if the candidate definition correctly describes the new source. assuming that we can count the number of tuples that are the same  we can use the jaccard similarity to measure how well the candidate hypothesis describes the data returned by the new source:

here i denotes the set of input tuples used to test the new source s. os i  denotes the set of tuples returned by the source when invoked with input tuple i. ov i  is the corresponding set returned by the candidate definition v. if we view this hypothesis testing as an information retrieval task  we can consider recall to be the number of common tuples divided by the number of tuples produced by the source  and precision to be the common tuples divided by the tuples produced by the definition. the jaccard similarity takes both precision and recall into account in a single score.
모the table below provides examples of the score for different output tuples. the first three rows of the table show inputs for which the predicted and actual output tuples overlap. in the fourth row  the definition produced a tuple  while the source didn't  so the definition was penalised. in the last row  the definition correctly predicted that no tuples would be output from the source. our score function is undefined at this point. from a certain perspective the definition should score well here because it has correctly predicted that no tuples would be returned for that input  but giving a high score to a definition when it produces no tuples can be dangerous. doing so may cause overly constrained definitions that can generate very few output tuples to score well  while less constrained definitions that are better at predicting the output tuples on average can score poorly. to prevent this from happening  we simply ignore inputs for which the definition correctly predicts zero tuples.  this is the same as setting the score for this case to be the average for the other cases.  after ignoring the last row  the overall score for this definition is calculated to be 1.
inputactualpredictedjaccardtuple ioutput os i output ov i similarity111 1  #undef!1 approximate matches between constants
when deciding whether the two tuples produced by the target and the definition are the same  we must allow for some flexibility in the values they contain. in the motivating example for instance  the distance values returned did not match exactly  but were  sufficiently similar  to be accepted as the same. for certain nominal types  like zipcode  it makes sense to check equality using exact string matches. for numeric types like temperature  an error bound  like 1 c  or a percentage error  such as 1%  may be more reasonable. for strings like company name  edit distances such as the jarowinkler score do a better job at distinguishing strings representing the same entity from those representing different ones.  see  bilenko et al.  1  for a discussion of string matching techniques.  in other cases a simple procedure might be available to check equality for a given type  so that values like  monday  and  mon  are equated. the actual equality procedure used will depend on the semantic type and we assume in this work that such a procedure is given in the problem definition. we note that the procedure need not be 1% accurate  but only provide a sufficient level of accuracy to guide the system toward the correct definition. indeed  equality rules could even be generated offline by training a machine learning classifier.
1 scoring partial definitions
as the search proceeds toward the correct definition  many semi-complete  unsafe  definitions will be generated. these definitions do not producevalues for all attributes of the target predicate but only a subset of them. for example  the following definition produces only one of the two output attributes returned by the source:
source1 $zip1  $dist1 zip1    :-
source1 zip1  zip1 dist1 .
this presents a problem  because our score is only defined over sets of tuples containing all of the output attributes of the new source. one solution might be to wait until the definitions become sufficiently long as to produce all outputs before comparing them to see which one best describes the new source. there are two reasons why we wouldn't want to do that: firstly  the space of complete  safe  definitions is too large to enumerate  and thus we need to compare partial definitions so as to guide the search toward the correct definition. secondly  the best definition that the system can generate may well be a partial one  as the set of known sources may not be sufficient to completely model the source.
모we can compute the score over the projection of the source tuples on the attributes produced by the definition  but then we are giving an unfair advantage to definitions that do not produce all of the source's outputs. that is because it is far easier to correctly produce a subset of the output attributes than to produce all of them. so we need to penalise such definitions accordingly. we do this by first calculating the size of the domain of each of the missing attributes. in the example above  the missing attribute is a distance value. since distance is a continuous variable  we approximate the size of its domain using  max   min /accuracy  where accuracy is the error-bound on distance values.  this cardinality calculation may be specific to each semantic type.  armed with the domain size  we penalise the score by scaling the number of tuples returned by the definition according to the size of the domains of all output attributes not generated by it. in essence  we are saying that all possible values for these extra attributes have been  allowed  by this definition.  this technique is similar to that used for learning without explicit negative examples in  zelle et al.  1 . 
1 experiments
we tested the system on 1 different problems  target predicates  corresponding to real services from five domains. the methodologyfor choosing services was simply to use any service that was publicly available  free of charge  worked  and didn't require website wrapping software. the domain model used in the experiments was the same for each problem and included 1 semantic types  ranging from common ones like zipcode to more specific types such as stock ticker symbols. it also contained 1 relations that were used to model 1 different publicly available services. these known sources provided some of the same functionality as the targets.
모in order to induce definitions for each problem  the new source  and each candidate  was invoked at least 1 times using random inputs. to ensure that the search terminated  the number of iterations of the algorithm was limited to 1  and a search time limit of 1 minutes was imposed. the inductive search bias used during the experiments was: {max. clause length: 1  predicate repetition limit: 1  max. existential quantification level: 1  candidate must be executable  max. variable occurrence per literal: 1}. an accuracy bound of 1% was used to determine equality between distance  speed  temperature and price values  while an error bound of 1 degrees was used for latitude and longitude. the jarowinkler score with a threshold of 1 was used for strings such as company  hotel and airport names. a hand-written procedure was used for matching dates.
1 results
overall the system performed very well and was able to learn the intended definition  albeit missing certain attributes  in 1 of the 1 problems. some of the more interesting definitions learnt by the system are shown below:
1 getdistancebetweenzipcodes $zip1  $zip1 dis1 :-
getcentroid zip1  lat1 lon1  
getcentroid zip1  lat1 lon1  
getdistance lat1  lon1 lat1 lon1 dis1   convertkm1mi dis1  dis1 .
1 usgselevation $lat1  $lon1 dis1 :convertft1m dis1  dis1  altitude lat1  lon1 dis1 . 1 getquote $tic1  pri1 dat1 tim1 pri1 pri1 pri1 pri1  cou1   pri1     pri1   com1  :-
yahoofinance tic1  pri1 dat1 tim1 pri1 pri1 pri1  pri1 cou1  
getcompanyname tic1  com1       add pri1 pri1 pri1  add pri1 pri1 pri1 .
1 yahooweather $zip1  cit1 sta1   lat1 lon1 day1 dat1  tem1 tem1 sky1  :-
weatherforecast cit1  sta1   lat1 lon1   day1 dat1  tem1 tem1     sky1         getcitystate zip1  cit1 sta1 .
1 yahoohotel $zip1  $ hot1 str1 cit1 sta1            :hotelsbyzip zip1  hot1 str1 cit1 sta1   .
1 yahooautos $zip1  $mak1 dat1 yea1 mod1     pri1    :-
googlebasecars zip1  mak1   mod1 pri1     yea1   converttime dat1   dat1       getcurrenttime    dat1   .
모the first definition calculates the distance in miles between two zipcodes and is the same as in our original example  source1 . the second source provided usgs elevation data in feet  which was found to be sufficiently similar to known altitude data in meters. the third source provided stock quote information  and a definition was learnt involving a similar service from yahoo. for this source  the system discovered that the current price was the sum of the previous day's close and today's change. the fourth definition is for a weather forecast service  and a definition was learnt in terms of another forecast service.  the system distinguished high from low and forecast from current temperatures.  the fifth source provided information about nearby hotels. certain attributes of this source  like the hotel's url and phone number  could not be learnt  because none of the known sources provided them. nonetheless  the definition learnt is useful as is. the last source was a classified used-car listing from yahoo that took a zipcode and car manufactureras input. the system discovered that there was some overlap between the cars  make  model and price  listed on that source and those listed on another site provided by google.
problemscandidatesdomain#  #attr. #	 #lit. precis.recallgeospatial1  1 1  1 1%1%financial1  1 1  1 1%1%weather1  1 1  1 1%1%hotels1  1 1  1 1%1%cars1  1 1  1 1%1%모the table above shows for each domain  the number of problems tested  the average number of attributes per problem  in parentheses   the average number of candidates generated prior to the winning definition  and the average number of literals per definition found  in parentheses . the last two columns give the average precision and recall values  where precision is the ratio of correctly generated attributes  of the new source  to all of the attributes generated  and recall is the ratio of correctly generated attributes  to all of the attributes that should have been generated. these values indicate the quality of the definitions produced. ideally  we would like to have 1% precision  no errors in the definitions  and high recall  most of the attributes being generated . that was the case for the 1 geospatial problems. one reason for the particularly good performance on this domain was the low number of attributes per problem  resulting in smaller search spaces. as would be expected  the number of candidates generated was higher for problems with many attributes  financial and weather domains . in general  precision was very high  except for a small number of problems  in the financial and cars domains . overall the system performed extremely well  generating definitions with a precision of 1% and recall of 1%.
1 related work
early work on the problem of learning semantic definitions for internet sources was performed by  perkowitz and etzioni  1   who defined the category translation problem. that problem can be seen as a simplification of the source induction problem  where the known sources have no binding constraints or definitions and provide data that does not change over time. furthermore  they assume that the new source takes a single value as input and returns a single tuple as output. to find solutions to this problem  the authors too used a form of inductive search based on an extension of the foil algorithm  cameron-jones and quinlan  1 .
모more recently  there has been some work on classifying web services into different domains  he  and kushmerick  1  and on clustering similar services together  dong et al.  1 . this work is closely related  but at a more abstract level. using these techniques one can state that a new service is probably a weather service because it is similar to other weather services. this knowledge is very useful for service discovery  but not sufficient for automating service integration. in our work we learn more expressive descriptions of web services  namely view definitions that describe how the attributes of a service relate to one another.
모the schema integration system clio  yan et al.  1  helps users build queries that map data from a source to a target schema. if we view this source schema as the set of known sources  and the target schema as a new source  then our problems are similar. in clio  the integration rules are generated semi-automatically with some help from the user.
모the imap system  dhamanka et al.  1  tries to discover complex  many-to-one mappings between attributes of a source and target schema. it uses a set of special purpose searchers to find different types of mappings. our system uses a general ilp-based framework to search for many-tomany mappings. since our system can perform a similar task to imap  we tested it on the hardest problem used to evaluate imap. the problem involved aligning data from two online cricket databases. our system  despite being designed to handle a more general task  was able to achieve 1% precision and 1% recall  which is comparable to the performance  accuracy  range of 1% reported for imap on complex matches with overlapping data.
모finally  the semantic web community have developed standards  martin et al.  1; roman et al.  1  for annotating sources with semantic information. our work complements theirs by providing a way to automatically generate semantic information rather than relying on service providers to create it manually. the datalog-based representation used in this paper  and widely adopted in information integration systems  levy  1   can be converted to the description logic-based representations used in the semantic web.
1 discussion
in this paper we presented a completely automatic approach to learning definitions for online information sources. this approach exploits definitions of sources that have either been given to the system or learned previously. the resulting framework is a significant advance over prior approaches that have focused on learning only the input and outputs of services. one of the most important applications of this work is to learn semantic definitions for data integration systems  levy  1 . such systems require an accurate definition in order to exploit and integrate available sources of data.
모our results demonstratethat we are able to learn definitions with a moderate size domain model and set of known sources. we intend to scale the techniqueto much larger problems. we note that the system is already applicable for many domainspecific problems. for example  in the area of geospatial data integration where the domain model is naturally limited  the technique can be applied as is.
모there are a number of future directions for this work that will allow the system to be applied more broadly. these include  1  introducing constants into the modeling language   1  developing additional heuristics to direct the search toward the best definition   1  developing a robust termination condition forhalting the search   1 introducinghierarchyinto the semantic types  and  1  introducing functional and inclusion dependencies into the definition of the domain relations.
acknowledgements: we thank jose뫣 luis ambite  kristina lerman  snehal thakkar  and matt michelson for useful discussions regarding this work.
