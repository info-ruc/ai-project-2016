
modelling data in structured domains requires establishing the relations among patterns at multiple scales. when these patterns arise from sequential data  the multiscale structure also contains a dynamic component that must be modelled  particularly  as is often the case  if the data is unsegmented. probabilistic graphical models are the predominant framework for labelling unsegmented sequential data in structured domains. their use requires a certain degree of a priori knowledge about the relations among patterns and about the patterns themselves. this paper presents a hierarchical system  based on the connectionist temporal classification algorithm  for labelling unsegmented sequential data at multiple scales with recurrent neural networks only. experiments on the recognition of sequences of spoken digits show that the system outperforms hidden markov models  while making fewer assumptions about the domain.
1 introduction
assigning a sequence of labels to an unsegmented stream of data is the goal of a number of practical tasks such as speech recognition and handwriting recognition. in these domains  the structure at multiple scales is often captured with hierarchical models to assist with the process of sequence labelling.
모probabilistic graphical models  such as hidden markov models  rabiner  1  hmms  and conditional random fields  lafferty et al.  1  crfs   are the predominant framework for sequence labelling. recently  a novel algorithm called connectionist temporal classification  graves et al.  1  ctc  has been developed to label unsegmented sequential data with recurrent neural networks  rnns  only. like crfs  and in contrast with hmms  ctc is a discriminant algorithm. in contrast with both crfs and hmms  ctc is a general algorithm for sequence labelling  in the sense that ctc does not require explicit assumptions about the statistical properties of the data or explicit models of the patterns and the relations among them.
모hierarchical architectures are often used with hmms. there exist efficient algorithms for estimating the parameters in such hierarchies in a global way  i.e. a way that maximises the performance at the top of the hierarchy. nonetheless  hmms do not efficiently represent information at different scales  indeed they do not attempt to abstract information in hierarchical form  nevill-manning and witten  1 .
모the only paper known to us that describes a hierarchy of crfs has been presented recently by kumar and hebert  for classifying objects in images. the hierarchy is not trained globally  but sequentially: i.e. estimates of the parameters in the first layer are found and  then  with these parameters fixed  those in the second layer  and transition matrices  are estimated.
모this paper uses a hierarchical approach to extend the applicability of ctc to sequence labelling in structured domains. hierarchical ctc  hctc  consists of successive levels of ctc networks. every level predicts a sequence of labels and feeds it forward to the next level. labels at the lower levels represent the structure of the data at a lower scale. the error signal at every level is back-propagated through all the lower levels  and the network is trained with gradient descent. the relative weight of the back-propagated error and the prediction error at every level can be adjusted if necessary  e.g. depending on the degree of uncertainty about the target label sequence at that level  which can depend on the variability in the data. in the extreme case in which only the error at the top level is used for training  the network can  potentially  discover structure in the data at intermediate levels that results in accurate final predictions.
모the next section briefly introduces the ctc algorithm. section 1 describes the architecture of hctc. section 1 compares the performance of hierarchical hmms and hctc on a speech recognition task. section 1 offers a discussion on several aspects of the algorithm and guidelines for future work. final conclusions are given in section 1.
1 connectionist temporal classification
ctc is an algorithm for labelling unsegmented sequential data with rnns only  graves et al.  1 . the basic idea behind ctc is to interpret the network outputs as a probability distribution over all possible label sequences  conditioned on the input data sequence. given this distribution  an objective function can be derived that directly maximises the probabilities of the correct labellings. since the objective function is differentiable  the network can then be trained with standard backpropagation through time  werbos  1 .
the algorithm requires that the network outputs at different times are conditionally independent given the internal state of the network. this requirement is met as long as there are no feedback connections from the output layer to itself or the network.
모a ctc network has a softmax output layer  bridle  1  with one more unit than the number of labels required for the task. the activation of the extra unit is interpreted as the probability of observing a  blank   or no label at a given time step. the activations of the other units are interpreted as the probabilities of observing the corresponding labels. the blank unit allows the same label to appear more than once consecutively in the output label sequence. a trained ctc network produces  typically  a series of spikes separated by periods of blanks. the location of the spikes usually corresponds to the position of the patterns in the input data; however  the algorithm is not guaranteed to find a precise alignment.
1 classification
for an input sequence x of length t we require a label sequence l 뫍 l뫞t  where l뫞t is the set of sequences of length 뫞 t on the alphabet l of labels. we begin by choosing a label  or blank  at every timestep according to the probability given by the network outputs. this defines a probability distribution over the set  sequences on the extended alphabet of labels with blank included. to disambiguate the elements of from label sequences  we refer to them as paths.
모the conditional probability of a particular path is given by:
		 1 
where ykt is the activation of output unit k at time t.
모paths can be mapped into label sequences by first removing the repeated labels  and then removing the blanks. for example  the path  a - a b -   and the path  - a a - - a b b  would both map onto the labelling  a a b . the conditional probability of a given labelling l 뫍 l뫞t is the sum of the probabilities of all the paths corresponding to it:
	p lx 	 1 
where is the many-to-one map implied by the above process.
모finally  the output of the classifier h x  is the most probable labelling for the input sequence:
	h x  = argmaxp l|x .	 1 
l
in general  finding this maximum is intractable  but there are several effective approximate methods  graves et al.  1 . the one used in this paper assumes that the most probable path will correspond to the most probable labelling:
모모h x  뫘 b 뷇  	 1  where 뷇  = argmaxp 뷇|x 
뷇
and 뷇  is just the concatenation of the most active outputs at every time-step.
1 training
the objective function for ctc is derived from the principle of maximum likelihood. that is  it attempts to maximise the log probability of correctly labelling the entire training set. let s be such a training set  consisting of pairs of input and target sequences  x z   where the length of sequence z is less than or equal to the length of the input sequence x. we can express the objective function to be minimised as:
	x .	 1 
모the network can be trained with gradient descent by differentiating equation  1  with respect to the network outputs. this can be achieved by calculating the conditional probabilities p z|x  with a dynamic programming algorithm similar to the forward-backward recursions used for hmms  graves et al.  1 .
모we define 붸t s  as the probability of having passed through sequence l1...s and being at symbol s at time t:
		 1 
and 붹t s  as the probability of passing through the rest of the label sequence  ls...|l|  given that symbol s has been reached at time t:
		 1 
모to allow for blanks in the output paths  for every label sequence l 뫍 l뫞t we consider a modified label sequence l  with blanks added to the beginning and the end and inserted between every pair of labels. the length of l is therefore
             . in calculating 붸t s  and 붹t s   we allow all transitions between blank and non-blank labels  and also those between any pair of distinct non-blank labels. the sequences can start with either blank or the first symbol in l  and can end with either blank or the last symbol in l.
from equations  1    1    1  and  1   we find that for any t:
	.	 1 
noting that the same label  or blank  may be repeated several times in a single labelling l  we define the set of positions where label k occurs as  which may be empty  and differentiate equation  1  with respect to the network outputs:
	.	 1 
모setting l = z and differentiating the objective function  we obtain the error signal received by the network during

figure 1: schematic representation of an hctc network labelling an unsegmented input data stream. outputs at each level correspond to the probability of having detected a particular label  or emitting the blank label  at a particular time. training:

 1 
where are the unnormalised and normalised out-
puts of the softmax layer  respectively  and
		 1 
is a normalization factor.
1 hierarchical connectionist temporal classification
a hierarchy of ctc networks is a series g =  g1 g1 ... gi  of ctc networks  with network g1 receiving as input the external signal and all other networks gi receiving as input the output of network gi 1. this architecture is illustrated in figure 1.
모note that every network gi in the hierarchy has a softmax output layer. this forces the hierarchical system to make decisions at every level and use them in the upper levels to achieve a higher degree of abstraction. the analysis of the structure of the data is facilitated by having output probabilities at every level.
모because of the modular design  the flexibility of the system allows using other architectures  such as feeding the external inputs to several levels in the hierarchy  as long as the mathematical requirements of the ctc algorithm are met  see section 1 .
1 training
in general  hctc requires target sequences for every level in the hierarchy  given a particular input sequence in the training set. each example in the training set s consists of a

figure 1: schematic representation of the error signal flow on an hctc network. the network is trained globally with gradient descent. external error signals injected at intermediate levels are considered optional. if applied  their contribution to the total error signal can be adjusted.
pair  x z   where x =  x1 x1 ... xt  is the external input sequence  and z =  z1 z1 ... zi  is the set of target sequences  where |zi| 뫞 t  i.
모the system is trained globally with gradient descent. figure 1 illustrates the error signal flow for an hctc network. the error signal due to the target sequence zi  received by network gi  is given by equation  1 . to simplify the notation  we define:
		 1 
where the input sequence vi is the external input sequence for network g1 and the output of network gi 1 for all levels
.
모the error signal 붟targeti is back-propagated into the network gi  and then into all lower levels j : j   i. the contribution of this term to the error signal at the unnormalised activation uti k of the kth output unit of networkis:

where m is the set of units in level i+1 which are connected to the set of softmax output units  k  in level i; 붻m is the error signal back-propagated from unit m 뫍 m; wmk is the weight associated with the connection between units m 뫍 m and are the output activations of the softmax layer at level i.
모finally  the total error signal 붟i received by network gi is the sum of the contributions in equations  1  and  1 . in general  the two contributions can be weighted depending on the problem at hand. this is important if  for example  the target sequences at some intermediate levels are uncertain or not known at all.
if i = i 
	뷂i붟targeti	+ 붟backpropi	otherwise 	 1 
digitphonemeszeroz - ii - r - owonew - ax - ntwot - oothreeth - r - iifourf - ow - rfivef - ay - vsixs - i - k - ssevens - eh - v - e - neightey - tninen - ay - nohowtable 1: phonetic labels used to model the digits in the experiments.
with 1 뫞 뷂i 뫞 1. in the extreme case where 뷂i = 1  no target sequence is provided for the network gi  which is free to make any decision that minimises the error 붟backpropi received from levels higher up in the hierarchy. because training is done globally  the network can  potentially  discover structure in the data at intermediate levels that results in accurate predictions at higher levels of the hierarchy.
1 experiments
in order to validate the hctc algorithm we chose a speech recognition task because this is a problem known to contain structure at multiple scales. hmms remain state-of-the-art in speech recognition  and therefore hctc performance is compared with that of hmms.
모the task was to find the sequence of digits spoken in an standard set of utterances using  at an intermediate level  the sequence of phonemes associated to every word.
1 materials
the speaker-independent connected-digit database  tidig-
its  leonard and doddington  1   consists of more than 1 thousand digit sequences spoken by over 1 men  women and children. the database was recorded in the u.s. and it is dialectically balanced. the utterances were distributed into a test and a training set. we randomly selected five percent of the training set to use as a validation set. this left 1 utterances in the training set  1 in the validation  and 1 in the test set.
모the following eleven digits are present in the database:  zero    one    two    three   ...   nine  and  oh . utterances consist of a sequence of one to seven digits. the representation of the digits at the phonetic level used in the experiments can be seen in table 1. nineteen phonetic categories were used  nine of which are common to two or more digits.
모samples were digitized at 1khz with a quantization range of 1bits. the acoustic signal was transformed into mel frequency cepstral coefficients  mfcc  with the htk toolkit  young et al.  1 . spectral analysis was carried out with a 1 channel mel filter bank from 1hz to 1khz. a pre-emphasis coefficient of 1 was used to correct spectral tilt. twelve mfcc plus the 1th order coefficient were computed on hamming windows 1ms long  every 1ms.
delta and acceleration coefficients were added giving a vector of 1 coefficients in total. for the network  the coefficients were normalised to have mean zero and standard deviation one over the training set.
1 setup
the	hmm	system	was	implemented	with	the	htk
toolkit  young et al.  1 . three-states left-to-right models were used for each one of the nineteen phonetic categories. a silence model and a  short pause  model  allowed at the end of a digit  were also estimated. observation probabilities were modelled by a mixture of gaussians. the grammar model allowed any sequence  preceded and followed by silence  of one or more digits. neither linguistic information nor probabilities of partial phone sequences were included in the system.
모the number of gaussians and the insertion penalty that optimised performance on the validation set was selected. using the training set  the number of gaussians was increased in steps of two until performance on the validation set stabilised or  as in our case  decreased slightly  1 gaussians . every time the number of gaussians was increased  the parameter estimation algorithm  herest  was applied twice and results were collected on the validation set with insertion penalties varying from 1 to -1 in steps of -1. the best performance on the validation set was obtained with 1 gaussians and an insertion penalty of -1. the total number of parameters for the hmm is 1.
모a 1-level hctc was used. the top  second level  had as target the sequence of digits corresponding to the input stream. the bottom  first level  used as target the sequence of phonemes corresponding to the target sequence of digits for the top level. each ctc network uses the bi-directional lstm recurrent neural network  graves and schmidhuber  1; graves et al.  1   primarily because on a phoneme recognition task better results than for other rnns have been reported  graves et al.  1 . also  the ctc formalism is best realised with a bi-directional recurrent neural network  schuster and paliwal  1  because the network output at a particular time depends on both past and future events in the input sequence.
모within each level  the input layer was fully connected to the hidden layer and the hidden layer was fully connected to itself and the output layer. in the first level  the input layer was size 1  the forward and backward layers had 1 blocks each  and the output layer was size 1  1 phonetic categories plus blank . in the second level  the input layer was size 1 also  the forward and backward layers had 1 blocks each and the output layer was size 1  eleven digits plus the blank label . the input and output cell activation functions were a hyperbolic tangent. the gates used a logistic sigmoid function in the range  1 . the total number of weights in the hctc network is 1.
모training of the hctc network was done by gradient descent with weight updates after every training example. in all cases  the learning rate was 1  momentum was 1  weights were initialized randomly in the range   1 1  and  during training  gaussian noise with a standard deviation of 1 was added to the inputs to improve generalisation.
systemlerhmm1%hctc  뷂1 =1 
hctc  뷂1 =1 1  1%
1%table 1: label error rate  ler  on tidigits. results for hctc  뷂1 = 1  are means over 1 runs with different random weights   standard error. this gives a 1% confidence interval of  1;1   with a t-value of 1 for 1 degrees of freedom and a two-tailed test. for hctc  뷂1 = 1  the best result obtained is shown; this coincides with the best result obtained with 뷂1 = 1.
모performance was measured as the normalised edit distance  label error rate; ler  between the target label sequence and the output label sequence given by the system.
1 results
performance rates for the systems tested can be seen in table 1. the best hmm-based system achieved an error rate of 1% in continuous digit recognition. the label error rate for hctc was on average 1%  1% confidence interval of  1;1  . this is an improvement of more than 1% with respect to the hmm and with approximately half the number of parameters.
모the best results without injecting the error signal associated to the phoneme labels  뷂1 = 1  was 1%  which is as good as the best performance achieved with 뷂1 = 1. in this case  however  the pattern of activations of seven output units  instead of twenty  was enough to encode the dynamic structure of the data at the lower level in order to make accurate predictions at the top level of the sequence of digits in the utterance  see figure 1 . some of these seven output units become active/inactive at specific points in time for each digit pattern. and some of them are active for different digits. nonetheless  they cannot be associated directly to phonetic categories and might encode another type of information in the signal.
1 discussion and future work
inserting levels in the hierarchy without a corresponding target label sequence is interesting if the target labels are not known  or  for instance  if the phonetic labels used are suspected to be invalid due to  e.g.  the presence of dialectal variations in the dataset. if the sources of variability in the data cannot be specified accurately  a system with less constraints might be capable of making more effective decisions.
모experimentally  this means that the system will be more difficult to train for a particular goal. hctc with and without target label sequences at the phonetic level achieved similar performance  albeit by different means. nevertheless  hctc with 뷂1 = 1 suffered to a larger extent from the problem of local minima.
모assuming that reasonable target label sequences can be specified a priori for intermediate levels  a possible solution is to train hctc until the error has decreased significantly  and then remove the contribution to the error signal of the target label sequences for intermediate levels. this will free a partially trained network to explore alternatives that maximise performance at the top level of the hierarchy.
모in the future  we would also like to explore ways of improving system scalability. for example  large vocabulary speech recognition requires systems that work with many thousands of words. using output layers of that size for hctc is not currently practical. instead of assigning a unique output unit to every possible label  other methods can be explored such as assigning labels to specific activation patterns in a group of output units. this will require modifying ctc's training algorithm.
모another aspect we would like to investigate is the potential of hctc for word spotting tasks. as a discriminant method  hctc may improve detection rates due to its capability of discriminating between keywords and non-speech or other speech events. besides this  hctc provides estimates of a posteriori probabilities that can help to directly assess the level of confidence in the predictions. this is in contrast to generative approaches  such as hmms  which use unnormalized likelihoods.
1 conclusion
this paper has presented the hctc algorithm  which uses a hierarchy of recurrent neural networks to label sequences of unsegmented data in structured domains. hctc is capable of finding structure at multiple levels and using it to achieve accurate predictions further up in the hierarchy. the use of neural networks offers a flexible way of modelling the domain while at the same time allowing the system to be trained globally. experimental results demonstrated that this approach outperformed hidden markov models on a speech recognition task.
acknowledgments
this research was funded by snf grant 1/1.
