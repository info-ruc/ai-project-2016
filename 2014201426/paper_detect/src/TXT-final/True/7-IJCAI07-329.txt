
we consider the problem of learning heuristics for controlling forward state-space beam search in ai planning domains. we draw on a recent framework for  structured output classification   e.g. syntactic parsing  known as learning as search optimization  laso . the laso approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. however  the search problems that arise in ai planning tend to be qualitatively very different from those considered in structured classification  which raises a number of potential difficulties in directly applying laso to planning. in this paper  we discuss these issues and describe a laso-based approach for discriminative learning of beam-search heuristics in ai planning domains. we give convergence results for this approach and present experiments in several benchmark domains. the results show that the discriminatively trained heuristic can outperform the one used by the planner ff and another recent non-discriminative learning approach.
1 introduction
a number of state-of-the-art planners are based on the old idea of forward state-space heuristic search  bonet and geffner  1; hoffmann and nebel  1; nguyen et al.  1 . the success is due to the recent progress in defining domain-independentheuristic functions that work well across a range of domains. however  there remain many domains where these heuristics are deficient  leading to planning failure. one way to improve the applicability and robustness of such planningsystems is to develop learningmechanisms that automatically tune the heuristic to a particular domain based on prior planning experience. in this work  we consider the applicability of recent developments in machine learning to this problem. in particular  given a set of solved planning problems from a target domain  we consider using discriminative learning techniques for acquiring a domain-specific heuristic for controlling beam search.
모despite the potential benefits of learning to improve forward state-space planning heuristics  there have been few reported successes. while there has been a substantial body of work on learning heuristics or value functions to control search  e.g.  boyan and moore  1; zhang and dietterich  1; buro  1   virtually all such work has focused on search optimization problems. these problems involve finding  least cost  configurations of combinatorial objects and have a much different flavor than the types of domains encountered in benchmarks from ai planning. to our knowledge  no such previous system has been demonstrated on benchmark domains from ai planning.
모recent work  yoon et al.  1  has made progress toward learning heuristics for planning domains. the work focused on improvingthe heuristic used by the state-of-the-artplanner ff  hoffmann and nebel  1 . in particular  the approach used linear regression to learn an approximation of the difference between ff's heuristic and the observed distances-togoal of states in the training plans. the primary contribution of the work was to define a generic knowledge representation for features and a features-search procedure that allowed learning of good regression functions across a range of planning domains. while the approach showed promising results  the learning mechanism has a number of potential shortcomings. most importantly  the mechanism does not consider the actual search performance of the heuristic during learning. that is  learning is based purely on approximating the observed distances-to-goal in the training data. even if the learned heuristic performs poorly when used for search  the learner makes no attempt to correct the heuristic in response.
모in this paper  we consider a learning approach that tightly couples learning with the actual search procedure  iteratively updating the heuristic in response to observed search errors. this approach is discriminative in the sense that it only attempts to learn a heuristic that discriminates between  good  and  bad  states well enough to find the goal  rather than attempting to precisely model the distance-to-goal. in many areas of machine learning  such discriminative methods have been observed to outperform their non-discriminative counterparts. a main goal of this work is to demonstrate such benefits in the context of planning.
모our learning approach is based on the recent framework of learning as search optimization  laso   daume iii and marcu  1   which was developed to solve  structured output classification  problems. such problems involve mapping structured inputs  e.g. sentences  to structured outputs  e.g. syntactic parses  and classification can be posed as performing a search over candidate outputs guided by a heuristic. laso provides an approach for discriminative learning of such heuristics and has demonstrated good performance across several structured classification problems. however  the search problems corresponding to structured classification are qualitatively very different from those typical of ai planning domains. for example  in structured classification the search problems typically have a single or small number of solution paths  whereas in ai planning there are often a very large number of equally good solutions. given these differences  the utility of laso in our context is not clear.
모the main contributions of this paper are to describe a laso-inspired algorithm for learning beam-search heuristics  to prove the convergence of the algorithm  and to provide an empirical evaluation in a number of ai planning domains. our empirical results show that the approach is able to learn heuristics that improve beam-search compared to using the heuristic from the planner ff. in addition  the results show that discriminativelearning appearsto havean advantageover the existing non-discriminative approach.
모in what follows  we first give our problem setup for learning planning heuristics. next  we give an overview of the laso framework for structured classification  followed by a description of our laso variant and convergenceanalysis. finally  we present experiments and conclude.
1 learning planning heuristics
planning domains. a planning domain d defines a set of possible actions a and a set of states s in terms of a set of predicate symbols p  action types y   and constants c. a state fact is the application of a predicate to the appropriate number of constants  with a state being a set of state facts. each action a 뫍 a consists of: 1  an action name  which is an action type applied to the appropriate number of constants  1  a set of precondition state facts pre a   1  two sets of state facts add a  and del a  representing the add and delete effects respectively. as usual  an action a is applicable to a state s iff pre a    s  and the application of an  applicable  action a to s results in the new state.
모given a planning domain  a planning problem is a tuple  s a g   where a   a is a set of actions  s 뫍 s is the initial state  and g is a set of state facts representing the goal. a solution plan for a planning problem is a sequence of actions  a1 ... al   where the sequential application of the sequence starting in state s leads to a goal state s where. in this paper  we will view planning problems as directed graphswhere the vertices representstates and the edges represent possible state transitions. planning then reduces to graph search for a path from the initial state to goal.
모learning to plan. we focus on learning heuristics in the simple  but highly successful  framework of forward statespace search planning. our goal is to learn heuristics that can quickly solve problems using breadth-first beam search with a small beam width. given a representative training set of problems from a planning domain  our approach first solves the problems using potentially expensive search and then uses the solutions to learn a heuristic that can guide a small width beam search to the same solutions. the hope is that the learned heuristic will then quickly solve new problems that could not be practically solved prior to learning.
모heuristic representation. we consider learning heuristic functions that are represented as linear combinations of features  i.e. h n  =쑐wi몫fi n  where n is a search node  fi is a feature of search nodes  and wi is the weight of feature fi. one of the challenges with this approach is defining a generic feature space from which features are selected and weights are learned. the space must be rich enough to capture important properties of a wide range of domains  but also be amenable to searching for those properties. for this purpose we will draw on prior work  yoon et al.  1  that defined such a feature space  based on properties of relaxed plans  and described a search approach for finding useful features. in this investigation  we will use the features from that work in addition to using the relaxed-plan length heuristic.
모the approach of  yoon et al.  1  used a simple weight learning method  where weights were tuned by linear regression to predict the distance-to-goal of search nodes in the training set. while this approach showed promise  it is oblivious to the actual performance of the heuristic when used for search. in particular  even if the heuristic provides poor guidance for search on the training problems no further learning will occur. the main objective of this work is to improve performance by investigating a more sophisticated weight learning mechanism that is tightly integrated with the search process  iteratively adapting the heuristic in response to observed search errors. below we first describe prior work from structured classification upon which our approach is based  and then describe its adaptation to our setting.
1 learning heuristics for structured classification
structured classification is the problem of learning a mapping fromstructured inputs to structuredoutputs. an example problem is part-of-speech tagging where the goal is to learn a mapping from word sequences  i.e. sentences  to sequences of part-of-speech tags. recent progress in structured classification includes methods based on condition random fields  lafferty et al.  1   perceptron updates  collins  1   and margin optimization  taskar et al.  1 .
모a recent alternative approach  daume iii and marcu  1  views structured classification as a search problem and learns a heuristic for that problem based on training data. in particular  given a structured input x  the problem of labeling x by a structured output y is treated as searching through an exponentially large set of candidate outputs. for example  in part-of-speech tagging where x is a sequence of words and y is a sequence of word tags  each node in the search space is a pair  where y is a partial labeling of the words in x. learning corresponds to inducing a heuristic that quickly directs search to the search node  x y  where y is the desired output. this framework  known as learningassearch optimization laso   has demonstrated state-of-the-art performance on a number of structured classification problems and serves as the basis of our work.
laso	assumes	a	feature-vector	function f n 	=  that maps search nodes to descriptive
features. for example  in part-of-speech tagging  the features may be indicators that detect when particular words are labeled by particular tags  or counts of the number of times an article-tag was followed by a noun-tag in a partial labeling y. the heuristic is a linear combination of these features h n  = f n  몫 w  where w is a weight vector. laso attempts to select a w that guides search to the target solution by directly integrating learning into the search process. for each training example  laso conducts a search guided by the heuristic given by the current weights. whenever a  search error  is made  the weights are updated so as to avoid the same type of error in the future. the process repeats until convergence or a stopping conditions. convergence results have been stated  daume iii and marcu  1  for certain types of weight updates.
1 learning heuristics for planning
given the success of laso in structured classification  it is interesting to consider its applications to a wider range of search problems. here we focus on search in ai planning. recall that our  learning to plan  training set contains planning problems with target solutions. this problem can be viewed as structured classification with a training set { xi yi }  where each xi =  s1 g  is a planning problem and each yi =  s1 s1 ... st  is a sequence of states along a solution plan for xi. we can now consider applying laso to learn a heuristic that guides a forward state-space search to find the solution yi for each xi.
모while in concept it is straightforward to map planning to the laso framework  it is not so obvious that the approach will work well. this is because the search problems arising in ai planning have very different characteristics compared to those tackled by laso so far. most notably  there are typically a large number of good  even optimal  solutions to any given planning problem. these solutions may take very different paths to the goal or may result by simply reordering the steps of a particular plan. for example  in the blocks world  in any particular state  there are generally many possible good next actions as it does not matter which order the various goal towers are constructed. despite the possibility of many good solutions  laso will attempt to learn a heuristic that strictly prefers the training-set solutions over other equally good solutions that are not in the training set. this raises the potential for the learning problem to be impossible to solve or very difficult since many of the other good solutions to xi may be inherently identical to yi. in such cases  it is simply not clear whether the weights will converge to a good solution or not.
모one approach to overcoming this difficulty might be to include many or all possible solutions in the training set. in general  this is not practical due to the enormous number of possible good plans  though studying methods for computing compact representations of such plan sets and using them in laso is of interest. rather  in this work we continue to use a single target solutions and evaluate an algorithm very much like the original laso  noting the potential practical problems that might arise due to multiple solutions. interestingly  below we are able to derive a convergence result for this algorithm under certain assumptions about the structure of the multiple good solutions relative to the target solution.
모below we describe a variant of laso used in our planning experiments. our variant is based on the use of breadth-first beam search  which is not captured by the original laso and that we found to be more useful in the context of planning. we will refer to the modified procedure as laso .
모beam search. in breadth-first beam search  a beam b of beam width b is generated at each search step resulting in a beam of b nodes. at each step  all of the nodes on the current beam are expanded and the top b children  as scored by the heuristic  are taken to be the next beam. this process continues until a goal node appears on the beam  at which point a solution plan has been found. when the beam width is small  many nodes in the search space are pruned away  often resulting in the inability to find a solution or finding very sub-optimal solutions. when the beam width increases  the quality of the solutions tend to improve  however  both the time and space complexity increases linearly with the beam width  leading to practical limitations. the goal of our work is to learn a domain-specific heuristic that allows for beam search with small b to replicate the result of using a large b. this can be viewed as a form of speedup learning.
모discriminative learning. the input to our learner is a set { xi yi } of pairs  where xi =  s1 g  is a training problem from the target planning domain and yi =  s1 s1 ... st  is a state sequence corresponding to a solution plan for xi. our training procedure will attempt to find weights such that for each problem the j'th state in the solution is contained in the j'th beam of the search. a search error is said to occur whenever this is not the case. figure 1 gives pseudo-code for the overall learning approach. the top-level procedure repeatedly cycles through the training set passing each example to laso  to arrive at updated weights. the procedure terminates when the weights remain unchanged after cycling through all examples or a user defined stopping condition.
모given a training example  xi yi   laso  conducts a beam search starting with the initial beam { xi  s1  }  i.e. a single search node with an empty plan. after generating beam j of the search  if n  =  xi  s1 s1 ... sj   is not on the beam then we have a search error. in this case  we update the weights in a way that makes n  more preferred by the heuristic  ideally enough to remain on the beam next time through the search. we use a weight updating rule  similar to the perceptron update proposed in  daume iii and marcu  1 

 where 1   붸 뫞 1 is a learning rate parameter  f n  is the feature vector of search node n and b is the current beam. intuitively this update rule moves the weights in a direction that decreases the heuristic value  increase the preference  of the desired search node n  and increases the heuristic value for the nodes in the beam. after the weight update  the beam is replaced by the single search node n  and the search continues. note that each call to laso  is guaranteed to terminate in t search steps  generating training examples as necessary.
heuristiclearn  { xi yi } b  w 뫹 1
repeat until w is unchanged or a large number of iterations for every  xi yi 
laso   xi yi  w b 
return wlaso    x y  w b 
// x is a planning problem  s1 g 
// y is a solution trajectory  s1 s1 ... st 
// w is the weight vector
b 뫹 { x  s1  } // initial beam for
n  뫹  x  s1 ... sj+1   // desired node
	if n  뫍/ b then	 
w 뫹 update w b n  
b 뫹 {n }beamexpand  b w b  candidates 뫹 {}b for everycandidatesn 뫍 뫹 candidates뫋 successors n 
returnfor everyhb nodes in candidates with lowest heuristic valuenn 뫍뫹candidatesw 몫 f n  // compute heuristic score of nfigure 1: the discriminative learning algorithm.
1 convergence of laso 
we now prove that under certain assumptions laso  is guaranteed to converge in a finite number of iterations to a set of weights that solves all of the training examples. in particular  we extend the convergenceresults of the original laso to the case of  multiple good solutions . the proof is a simple generalization of the one used to prove convergence of perceptron updates for structured classification  collins  1 .
모consider a set of training problems  xi yi   where xi =  s1 g  and yi =  s1 s1 ... st . for each  xi yi  we denote by n ij =  xi  s1 ... sj   the node on the desired search path at depth j for example i. also let dij be the set of all nodes that can be reached in j search steps from n i1. that is  dij is the set of all possible nodes that could be in the beam after j beam updates. in our result  we will let r be a constant such thatwhere f n  is the feature vector of node denotes 1-norm.
모our results will be stated in terms of the existence of a weight vector that achieves a certain margin on the training set. here we use a notion of margin that is suited to our beam search framework and that is meaningful when there is no weight vector that ranks the target solutions as strictly best  i.e. there can be other solutions that look just as good or better. as defined below a beammargin is a triple where b is a non-negative integer  and 붻1 붻1 뫟 1.
definition 1  beam margin . a weight vector w has beam margin  on a training set { xi yi } if for each i j there is a set of size at most b such that
 and 

weight vector w has beam margin  if at each search depth it ranks the target node n ij better than most other nodes by a margin of at least 붻1  and ranks at most b nodes better than n ij by a margin no greater than 붻1. whenever this condition is satisfied we are guaranteed that a beam search with width using weights w will solve all of the training problems. the case where  corresponds to the more typical definition of margin  also used by the original laso   where the target is required to be ranked higher than all other nodes. by considering the case where we can show convergence in cases where no such  dominating  weight vector exists  yet there are weight vectors that allow search to correctly solve the training problems. the following theorem shows that if laso  uses a large enough beam width relative to the beam margin  then it is guaranteed to converge after a finite number of mistakes.
theorem 1. if there exists a weight vector w  such that  and w has beam margin  on the training set  then for any beam width  the number of mistakes made by laso  is bounded by.
proof.  sketch  let wk be the weights before the kth mistake is made. then w1 = 1. suppose the kth mistake is made when the beam b at depth j does not contain the target node n  = n ij. using the fact that for n 뫍 b  wk 몫 f n     wk 몫 f n   one can derive that
  which by induction implies that
. next  using the definition of beam margin one can derive that  which implies that . combining these inequalities and noting that we get that
  implying the theorem.	
모notice that when  i.e. there is a dominating weight vector  the mistake bound reduces to  which does not depend on the beam width and matches the result stated in
 daume iii and marcu  1 . this is also the behavior when
. in the case when 붻1 = 붻1 and we use the minimum beam width allowed by the theorem  the bound is   which is a factor of  larger than when. thus  this result points to a trade-off between the mistake bound and computational complexity of laso . that is  the computational complexity of each iteration increases linearly with the beam width  but the mistake bound decreases as the beam width becomes large. this agrees with the intuition that the more computation time we are willing to put into search at planning time  the less we need to learn.
1 experimental results
we present experiments in five strips domains: blocks world  pipesworld  pipesworld-with-tankage  psr and philosopher. we set a time cut-off of 1 cpu minutes and considered a problem to be unsolved if a solution is not found within the cut-off. given a set of training problems we generated solution trajectories by runningboth ff and beam search with different beam widths and then taking the best solution found as the training trajectory. for blocks world  we used a set of features learned in previous work  yoon et al.  1; fern et al.  1; yoon  1  and for the other domains we used the those learned in  yoon et al.  1; yoon  1 . in all cases  we include ff's heuristic as a feature.
모we used laso  to learn weights with a learning rate of 1. for philosopher  laso  was run for 1 iterations with a learning beam width of 1. for the other domains  laso  was run for 1 or 1 iterations with a learning beam width of 1  this beam width did not work well for philosopher . the learning times varied across domains  depending on the number of predicates and actions  and the length of solution trajectories. the average time for processing a single problem in a single iteration was about 1 seconds for psr  1 seconds for pipesworld-with-tankage  and less than 1 seconds for the other domains.
모domain details. blocks world problems were generated by the bwstates generator  slaney and thie뫣baux  1 . thirty problems with 1 or 1 blocks were used as training data  and 1 problems with 1  1  or 1 blocks were used for testing. there are 1 features in this domain including ff's relax-plan-length heuristic. the other four domains are taken from the fourth international planning computation  ipc1 . each domain included 1 or 1 problems  roughly ordered by difficulty. we used the first 1 problems for training and the remaining problems for testing. including ff's relaxedplan-length heuristic  there were 1 features in pipesworld  1 features in pipesworld-with-tankage  1 features in psr and 1 features in philosopher.
모performance across beam sizes. figure 1 gives the performance of beam search in each domain for various beam widths. the columns correspond to four algorithms: len - beam search using ff's relaxed-plan-length heuristic  u beam search using a heuristic with uniform weights for all features  laso  - beam search using the heuristic learned using laso   with learning beam width specified above   and lr - beam search using the heuristic learned from linear regression as was done in  yoon et al.  1 . each row corresponds to a beam width and shows the number of solved test problems and the average plan length of thesolvedproblems.
모in general  for all algorithms we see that as the beam width increases the number of solved problems increases and solution lengths improve. however  after some point the number of solved problems typically decreases. this behavior is typical for beam search  since as the beam width increases there is a greater chance of not pruning a solution trajectory  but the computational time and memory demands increase. thus  for a fixed time cut-off we expect a decrease in performance.
모laso  versus no learning. compared to len  laso  tended to significantly improve the performance of beam search  especially for small beam widths-e.g. in blocks world with beam width 1 laso  solves twice as many problems as len. the average plan length has also been reduced significantly for small beam widths. as the beam width increases the gap between laso  and len decreases but laso  still solves more problems with comparable solution quality. in pipesworld  laso  has the best performance with beam width 1  solving 1 more problems than len. as the beam width increases  again the performance gap decreases  but laso  consistently solves more problems than len. the trends are similar for the other domains  except that in psr  len solves slightly more than laso  for large beam widths.
blocks worldproblems solvedaverage plan lengthblenulaso lrlenulaso lr111-1111-1111-1111-1111-1111-1111-1111-1pipesworldproblems solvedaverage plan lengthblenulaso lrlenulaso lr111111111111111111111111111111111111pipesworld-with-tankageproblems solvedaverage plan lengthblenulaso lrlenulaso lr111111111111111111111111111111111111psrproblems solvedaverage plan lengthblenulaso lrlenulaso lr111----111-11111111111111111111111111111philosopherproblems solvedaverage plan lengthblenulaso lrlenulaso lr111-1-111-1111-1111-1111-1111-1111-1111-11figure 1: experimental results for five planning domains.
모laso  significantly improves over u in blocks world  pipesworld and pipesworld-with-tankage. especially in blocks world  where u does not solve any problem. for psr  laso  only improves over u at beam width 1 and is always worse in philosopher  see discussion below .
모the results show that laso  is able to improve on the state-of-the-art heuristic len and that in the majority of our domains learning is beneficial compared to uniform weights. in general  the best performance for laso  was achieved for small beam widths close to those used for training.
모comparing laso  with linear regression. to compare with prior non-discriminativeheuristic learning work we learned weights using linear regression as done in  yoon et al.  1  utilizing the weka linear regression tool. the results for the resulting learned linear-regression heuristics are shown in the columns labeled lr.
for blocks world  lr solves fewer problems than laso  with beam widths smaller than 1 but solves more problems than laso  with beam widths larger than 1. for pipesworld and pipesworld-with-tankage  laso  always solves more problems than lr. in psr  laso  is better than lr with beam width 1  but becomes slightly worse as the beam width increases. in philosopher  lr outperforms laso   solving all problems with small beam widths.
모the results indicate that laso  can significantly improve over non-discriminative learning  here regression  and that there appears to be utility in integrating learning directly in to search. the results also indicate that laso  can fail to converge to a good solution in some domains where regression happens to work well  particularly in philosopher. in this domain  since action sequences can be almost arbitrarily permuted  there is a huge set of inherently identical optimal/good solutions. laso  tries to make the single training solution look better than all others  which appears problematic here. more technically  the large set of inherently identical solutions means that the beam-width threshold required by theorem 1  i.e.   is extremely large  suggesting poor convergence properties for reasonably beam widths.
모plan length. laso  can significantly improve success rate at small beam widths  which is one of our main goals. however  the plan lengths at small widths are quite suboptimal  which is typical behavior of beam search. ideally we would like to obtain these success rates without paying a price in plan length. we are currently investigating ways to improvelaso  in this direction. also we note that typically one of the primary difficulties of ai planning is to simply find a path to the goal. after finding such a path  if it is significantly sub-optimal  incomplete plan analysis or plan rewriting rules can be used to significantly prune the plan  e.g. see  ambite et al.  1 . thus  we can use the current laso  to quickly find goals followed by fast plan length optimization.
1 summary and future work
we discussed the potential difficulties of applying laso to ai planning given the qualitative differences between search problems in ai planning and those in structured classification. nevertheless  our preliminary investigation shows that in several planning domains our laso variant is able to significantly improve over the heuristic of ff plan and over regression-based learning  yoon et al.  1 . we conclude that the approach has good promise as a way of learning heuristics to control forward state-space search planners. our results also demonstrated failures of the discriminative approach  where it performed significantly worse than linear regression  which suggest future directions for improvement.
모in future work we plan to extend our approach to automatically induce new features. another important direction is to investigate the sensitivity of the laso approach to the particular solutions provided in the training data. in addition  understanding more general conditions under which the approach is guaranteed to converge is of interest. currently  we have shown a sufficient condition for convergence but not necessary. we are also interested in determiningthe computational complexity of learning linear heuristics for controlling beam search. also of interest is to investigate the use of plan analysis in laso to convert the totally ordered training plans to partially-order plans  which would help deal with the problem of  many inherently identical solutions  experienced in domains such as philosopher. finally  we plan to consider other search spaces and settings such as partial-order planning  temporal-metric planning  and probabilistic planning.
acknowledgments
this work was supported by nsf grant iis-1 and darpa contract fa1-1.
