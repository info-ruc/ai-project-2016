
when not enough time is available to fully explore a search tree  different algorithms will visit different leaves. depth-first search and depth-bounded discrepancy search  for example  make opposite assumptions about the distribution of good leaves. unfortunately  it is rarely clear a priori which algorithm will be most appropriate for a particular problem. rather than fixing strong assumptions in advance  we propose an approach in which an algorithm attempts to adjust to the distribution of leaf costs in the tree while exploring it. by sacrificing completeness  such flexible algorithms can exploit information gathered during the search using only weak assumptions. as an example  we show how a simple depth-based additive cost model of the tree can be learned on-line. empirical analysis using a generic tree search problem shows that adaptive probing is competitive with systematic algorithms on a variety of hard trees and outperforms them when the node-orderingheuristic makes many mistakes. results on boolean satisfiability and two different representations of number partitioning confirm these observations. adaptive probing combines the flexibility and robustness of local search with the ability to take advantage of constructive heuristics.
1	introduction
consider the problem of searching a finite-depth tree to find the best leaf. many search trees arising in practical applications are too large to be explored completely. given a limited amount of time  one can only hope to search the tree in such a way that leaves with a greater chance of being optimal are encountered sooner. for instance  when a node-ordering heuristic is available  a depth-first search can expand the children of a node in the order in which they are preferred by the heuristic. however  the backtracking order of depth-first search will visit the second-ranked child of the last internal branching node before reconsidering the choice at the next to last branching node. each decision at which a non-preferred child is chosen is called a discrepancy  harvey and ginsberg  1 . depth-first search will visit the leaf whose path from the root has all discrepancies below depth i before visiting the leaf with a single discrepancy at depth i. this corresponds to an implicit assumption that a single discrepancy at depth i will lead to a worse leaf than taking discrepancies at every deeper depth.
모limited discrepancy search  harvey and ginsberg  1; korf  1  was designed with a different assumption in mind. it assumes that discrepancies at any depth are equally disadvantageous and so visits all leaves with k discrepancies anywhere in their paths before visiting any leaf with k + 1 discrepancies. depth-bounded discrepancy search  walsh  1  uses a still different assumption: a single discrepancy at depth i is worse than taking discrepancies at all depths shallower than i. motivated by the idea that node-orderingheuristics are typically more accurate in the later stages of problemsolving  when local information better reflects the remaining subproblem  this assumption is directly opposed to the one embodied by depth-first search.
모when faced with a new search problem  it is often not obvious which algorithm's assumptions most accurately reflect the distribution of leaf costs in the tree or even if any of them are particularly appropriate. in this paper  we investigate an adaptive approach to tree search in which we use the costs of the leaves we have seen to estimate the cost of a discrepancy at each level. simultaneously  we use these estimates to guide search in the tree. starting with no preconceptions about the relative advantage of choosing a preferred or non-preferred child node  we randomly probe from the root to a leaf. by sharpening our estimates based on the leaf costs we observe and choosing children with the probability that they lead to solutions with lower cost  we focus the probing on areas of the tree that seem to contain good leaves.
모this stochastic approach is incomplete and cannot be used to prove the absence of a goal leaf. in addition  it generates the full path from the root to every leaf it visits  incurring overhead proportional to the depth of the tree when compared to depth-first search  which generates roughly one internal node per leaf. however  the problem-specific search order of adaptive probing has the potential to lead to better leaves much faster. since an inappropriate search order can trap a systematic algorithm into exploring vast numbers of poor leaves  adaptive probing would be useful even if it only avoided such pathological performance on a significant fraction of problems.

모after describing the details of an algorithm based on this adaptive approach  we investigate the algorithm's performance using the abstract tree model of harvey and ginsberg . we find that adaptive probing outperforms systematic methods on large trees when the node-ordering heuristic is moderately inaccurate  and exhibits better worst-case performance whenever the heuristic is not perfect at the bottom of the tree. to confirm these observations  we also test the algorithm on two different representations of the combinatorial optimization problem of number partitioning and on the goalsearch problem of boolean satisfiability. it performs well on satisfiability and the naive formulation of number partitioning  but is competitive only for long run-times when using the powerful karmarkar-karp heuristic.
1	an adaptive probing algorithm
within the general approach outlined above  there are many ways to extract information from observed leaf costs. in this paper  we will evaluate one simple model. we will assume that the cost of every leaf is the sum of the costs of the actions taken to reach it from the root. each position in the ordered list of children counts as a distinct action and actions at different levels of the tree are modeled separately. so a tree of depth d and branching factor b requires db parameters  one for each action at each level. the model assumes  for instance  that the effects of choosing the second-most-preferred child at level 1 is the same for all nodes at level 1. this is just a generalization of the assumption used by discrepancy search algorithms. in addition  we will estimate the variance of the action costs by assuming that each estimated cost is the mean of a normal distribution  with all actions having the same variance.
모this model is easy to learn during the search. each probe from the root corresponds to a sequence of actions and results in an observed leaf cost. if aj i  is the cost of taking action i at depth j and lk is the cost of the kth leaf seen  probing three times in a binary tree of depth three might give the following information:
a1 	+ a1 	+	a1  = l1 a1 	+	a1 + a1 	= l1 a1 + a1 	+ a1 	= l1
we can then estimate the aj i  using a least squares regression algorithm. in the experiments reported below  a perceptron was used to estimate the parameters  cesa-bianchi et al.  1 . this simple gradient descent method updates each cost according to the error between a prediction of the total leaf cost using the current action estimates   lk  and the actual leaf cost  lk. if d actions were taken  we update each of their estimates by

where 붾 controls the learning rate  or gradient step-size . all results reported below use 붾 = 1  although similar values also worked well.  values of 1 and 1 resulted in reduced performance.  this update requires little additional memory  takes only linear time  adjusts d parameters with every leaf  and often performed as well as an impractical o d1  singular value decomposition estimator. it should also be able to track changes in costs as the probing becomes more focussed  if necessary.
모because we assume that it is equal for all actions  the variance is straightforward to estimate. if we assume that the costs of actions at one level are independent from those at another  then the variance we observe in the leaf costs must be the sum of the variances of the costs selected at each level. the onlycomplicationis that the variancecontributedby each level is influenced by the mean costs of the actions at that level-if the costs are very different  then we will see variance even if each action has none. more formally  if x and y
are independent and normally distributed with common variance if w takes its value accordingto x with probability p and y with probability 1  p  then
횽1 = =e w1   뷃1w
p 뷃1x + 횾y1  + 1  p  뷃1y + 횾y1   
1
 p뷃x + 1  p 뷃y  =since we can easily compute p by recording the number of times each action at a particular level is taken  and since the action costs are estimates of the 뷃i  we can use this formula to subtract away the effects of the different means. following our assumption  we can then divide the remaining observed variance by d to distribute it equally among all levels.
모using the model during tree probing is also straightforward. if we are trying to minimize the leaf cost  then for each decision  we want to select the action with the lower expected cost  i.e.  the lower mean . as our estimates may be quite inaccurate if they are based on few samples  we don't always want to select the node with the lower estimated cost. rather  we merely wish to select each action with the probability that it is truly best. given that we have estimates of the means and variance of the action costs and we know how many times we have tried each action  we can compute the probability that one mean is lower than another using a standard test for the difference of two sample means. we then choose each action according to the probability that its mean cost is lower. to eliminate any chance of the algorithm converging to a single path  the probability of choosing any action is clamped at
1/d for a depth d tree  which ensures at least one deviation on 1% of probes.
모now we have a complete adaptive tree probing algorithm. it assumes the search tree was drawn from a simple model of additive discrepancy costs and it learns the parameters of the tree efficiently on-line. exploitation of this information is balanced with exploration according to the variance in the costs and the number of times each action has been tried. the method extends to trees with large and non-uniform branching factors and depths. the underlying model should be able to express assumptions similar to those built into algorithms as diverse as depth-first search and depth-bounded discrepancy search  as well as many other weightings not captured by current systematic methods.
1	empirical evaluation
we first investigate the performance of this adaptive probing algorithm using an abstract model of heuristic search. this gives us precise control over the density of good nodes and the accuracy of the heuristic. to ensure that our conclusions apply to more complex domains  we will also evaluate the algorithm using two np-complete search problems: the combinatorial optimization problem of number partitioning and the goal-search problem of boolean satisfiability.
1	an abstract tree model
in this model  introduced by harvey and ginsberg  for the analysis of limited discrepancy search  one searches for goal nodes in a binary tree of uniform depth. goals are distributed according to two parameters: m  which controls goal density  and p  which controls the accuracy of the heuristic. each node either has a goal below it  in which case it is good  or does not  in which case it is bad. clearly  the root is good and bad nodes only have bad children. the probabilities of the other configurations of parent and children are:
p good 뫸 good good  = 1m
p good 뫸 bad good 	= 1  p
p good 뫸 good bad 	= 1m   1  p 
the expected number of goal nodes is  1m d  where d is the depth of the tree.
모following walsh's  analysis of depth-bounded discrepancy search  we will estimate the number of leaves that each algorithm must examine before finding a goal using empirical measurements over lazily  but deterministically  generated random trees. to provide a leaf cost measure for adaptive probing  we continue the analogy with constraint satisfaction problems that motivated the model and define the leaf cost to be the number of bad nodes in the path from the root.  if we were able to detect failures before reaching a leaf  this would be the depth remaining below the prune.  the results presented below are for trees of depth 1 in which m =1. the probability that a random leaf is a goal is 1. by investigating different values of p  we can shift the locations of these goals relative to the paths preferred by the heuristic.
모figure 1 shows the performance of depth-first search  dfs   korf's  improved version of limited discrepancy search  ilds   depth-bounded discrepancy search  dds   and adaptive probing on 1 trees. a heuristicbiased probing algorithm is also shown. this algorithm selects the preferred child with the largest probability that would be allowed during adaptive probing. following walsh  we raise the accuracy of the heuristic as depth increases. at the root  p = 1 which makes the heuristic random  while at the leaves p =1 for 1% accuracy. ilds was modified to incorporate this knowledge and take its discrepancies at the top of the tree first.
모adaptive probing quickly learns to search these trees  performing much better than the other algorithms. even though dds was designed for this kind of tree  its assumptions are too strong and it always branches at the very top of the tree. ilds wastes time by branching equally often at the bottom where the heuristic is more accurate. the ad hoc biased probing algorithm  which branches at all levels  is competitive

figure 1: probability of finding a goal in trees of depth 1 with m = 1 and p linearly varying between 1 at the root and 1 at the leaves.

figure 1: performance on trees of depth 1  m = 1  and p varying from 1 at the root to 1 at the leaves.
with ilds  and will actually surpass it  given more time  but fails to exploit the structure in the search space. dfs vainly branches at the bottom of the tree  ignorant of the fatal mistake higher in the tree  and solves almost no problems within 1 leaves.
모dds does better when the heuristic is more accurate  since its steadfast devotion to the preferred child in the middle and bottom of the tree is more often correct. figure 1 shows the algorithms' performance on similar trees in which the heuristic is accurate 1% of the time at the leaves. dds has better median performance  although adaptive probing exhibits more robust behavior  solving all 1 problems within 1 leaves. dds had not solved 1% of these problems after 1 leaves and did not complete the last one until it had visited almost 1 leaves. in this sense  dds has a heavier tail in its cost distribution than adaptive probing. similar results were obtained in trees with uniform high p. adaptive probing avoids entrapment in poor parts of the tree at the expense of an initial adjustment period.
the objective in a number partitioning problem is to divide a given set of numbers into two disjoint groups such that theas possible. it was used by johnson et al. to evaluate simulated annealing   korf to evaluate his improvement to limited discrepancy search   and walsh to evaluate depth-bounded discrepancy search . to encourage difficult search trees by reducing the chance of encountering a perfectly even partitioning  karmarkar et al.  1   we used instances with 1-digit numbers or 1-digit numbers.1 common lisp  which provides arbitrary precision integer arithmetic  was used to implement the algorithms.  results were normalized as if the original numbers had been between 1 and 1. to better approximate a normal distribution  the logarithm of the partition difference was used as the leaf cost.
the greedy representation
we present results using two different representations of the problem. the first is a straightforward greedy encoding in which the numbers are sorted in descending order and then each decision places the largest remaining number in a partition  preferring the partition with the currently smaller sum. figure 1 compares the performance of adaptive tree probing with depth-first search  dfs   improved limited discrepancy search  ilds   depth-bounded discrepancy search  dds  
	1	1	1	1	1 1
nodes generated
figure 1: performance on the greedy representation of number partitioning as a function of nodes generated.
and completely random tree probing. to provide a comparison of the algorithms'search orders  the horizontalaxis represents the number of leaves seen. adaptive probing starts off poorly  like random sampling  but surpasses all other algorithms after seeing about 1 leaves. it successfully learns an informative model of the tree and explores the leaves in a more productive order than the systematic algorithms.
모however  recall that adaptive tree probing suffers the maximum possible overhead per leaf  as it generates each probe from the root.  this implementation did not attempt to reuse initial nodes from the previous probe.  the number of nodes  both internal and leaves  generated by each algorithm should correlate well with running time in problems in which the leaf cost is computed incrementally or in which the nodeordering heuristic is expensive. figure 1 compares the algorithms on the basis of generated search nodes.  to clarify the plot  dfs and ilds were permitted to visit many more leaves than the other algorithms.  in a demonstration of the importance of overhead  dfs dominates all the other algorithms in figure 1: searching the ckk representation of number partitioning. each instance had 1-digit numbers.
this view  and ilds performs comparably to adaptive probing. dfs reuses almost all of the internal nodes on each leaf's path  generating only those just above the leaves. since ilds needs to explore discrepancies at every level of the tree  it will usually need to generate a significant fraction of the path down to each leaf. dds  which limits its discrepancies to the upper levels of the tree  incurs overhead similar to that of adaptive probing because it never reuses internal nodes in the middle of the tree.
모on instances using 1 numbers  adaptive probing again dominated dds  but was clearly surpassed by ilds.  it performed on par with a version of ilds that visited discrepancies at the top of the tree before those at the bottom.  this suggests that  in these search trees  the advantage of adaptive probing over ilds and dds increases with problem size.
the ckk representation
a more sophisticated representation for number partitioning was suggested by korf   based on the heuristic of karmarkar and karp . the essential idea is to postpone the assignment of numbers to particular partitions and merely constrain pairs of number to lie in either different bins or the same bin. numbers are considered in decreasing order and constrained sets are reinserted in the list according to the remainingdifferencethey represent. this representationcreates a very different search space from the greedy heuristic.
모figure 1 shows the performance of the algorithms as a function of leaves seen. dds has a slight advantage over ilds  although adaptive probing is eventually able to learn an equally effective search order. dfs and random sampling too often go against the powerful heuristic. as in the greedy representation  however  interior node overhead is an important consideration. figure 1 shows that dds and adaptive probing are not able to make up their overhead  and results using 1 numbers suggest that these difficulties increase on larger problems. bedrax-weiss  argues that the kk heuristic is extraordinarily effective at capturing relevant information and that little structure remains in the space. these results are consistent with that conclusion  as the uniform and limited discrepancies of ilds appear best.
figure 1: performance on the ckk representation of number partitioning as a function of nodes generated.

figure 1: fraction of random1-satisfiability problems solved. error bars indicate 1% confidence intervals around the mean over 1 instances  each with 1 variables and 1 clauses per variable.  the dfs and dds means are lower bounds. 
1	boolean satisfiability
we also tested on instances of boolean satisfiability. following walsh   we generated problems according to the random 1-sat model with 1 clauses per variable and filtered out any unsatisfiable problems. all algorithms used unit propagation  selected the variable occurring in the most clauses of minimum size  and preferred the value whose unit propagation left the most variables unassigned. the cost of a leaf was computed as the number of variables unassigned when the empty clause was encountered.
모figure 1 shows the percentage of 1-variable problems solved as a function of the number of nodes generated. although walsh used these problems to argue for the suitability of dds  we see that both ilds and purely random sampling perform significantly better.  crawford and baker  similarly found random sampling effective on scheduling problems that had been converted to satisfiability problems.  dfs performs very poorly. adaptive probing performs slightly better than random sampling  this is most noticeable at the extremes of the distribution . although slight  this advantage persisted at all problem sizes we examined  1  1  1  and 1 variables .
모to summarize: in each search space we examined  the systematic search algorithms ranked differently in performance. this makes it difficult to select which algorithm to use for a new problem. even when taking its overhead into account  adaptive probing seemed to perform respectably in every search space. the only space in which it was not the best or near the best was the ckk space for number partitioning  in which the node-ordering heuristic is very accurate. of course  further work is needed to assess its performance in very different domains  such as those with a high branching factor  and against additional methods  such as interleaved depth-first search  meseguer  1 .
1	related work
abramson  used random sampling in two-player game trees to estimate the expected outcome of selecting a given move. he also discussed learning a model off-line to predict outcome from static features of a node. in an optimization context  juille뫣 and pollack  used random tree probing as a value choice heuristic during beam search  although no learning was used.
모bresina  used stochastic probing for scheduling  introducing a fixed ad hoc bias favoring children preferred by the node-orderingheuristic. adaptive probingprovides a way to estimate that bias on-line  rather than having to specify it beforehand  presumably using trial and error. by removing this burden from the user  it also becomes feasible to use a more flexible model.
모the dts system of othar and hansson  uses learning during search to help allocate effort. their method learns a function from the value of a heuristic function at a node to the node's probability of being a goal and the expected effort required to explore the node's subtree. it then explores nodes with the greatest expected payoff per unit of effort. in a similar vein  bedrax-weiss  proposed weighted discrepancysearch  which uses a training set of similar problems to estimate the probability that a node has a goal beneath it  and uses the distribution of these values to derive an optimal searching policy. adaptive probing is less ambitious and merely estimates action costs rather than goal probability.
모squeaky-wheel optimization  joslin and clements  1  adapts during tree search  although it learns a variable ordering for use with a greedy constructive algorithm  rather than learning about the single tree that results from using an ordinary variable choice heuristic. the relative benefits of adapting the variable ordering as opposed to the value ordering seem unclear at present. adaptive probing is slightly more general  as the squeaky-wheel method requires the user to specify a domain-specific analysis function for identifying variables that should receive increased priority during the next probe.
모adaptive tree probing is similar in spirit to iterative improvement algorithms such as adaptive multi-start  boese et al.  1   pbil  baluja  1   and comit  baluja and davies  1  which explicitly try to represent promising regions in the search space and generatenew solutions fromthat representation. for some problems  however  tree search is more natural and heuristic guidance is more easily expressed over extensions of a partial solution in a constructive algorithm than over changes to a complete solution. adaptive probing gives one the freedom to pursue incomplete heuristic search in whichever space is most suitable for the problem. it is a promising area of future research to see how the two types of heuristic information might be combined.
모the greedy random adaptive search procedure grasp  of feo and resende  is  in essence  heuristic-biased stochastic probing with improvement search on each leaf. adaptive probing provides a principled  relatively parameterfree  way to perform the probing step. similarly  aspects of ant colony optimization algorithms  dorigo and gambardella  1   in which 'pheremone' accumulates to represent the informationgathered by multiple search trials  can be seen as an approximation of adaptive probing.
모adaptive probing is also related to stage  boyan and moore  1   which attempts to predict promising starting points for hill-climbing given the values of user-specified problem-specific features. the discrepancy cost model requires less of the user  however  since the usual node-ordering function is used as the only problem-specificfeature. the tree structure itself can be used to give the geometry for the search space model.
모althoughadaptive tree probingseems superficially like traditional reinforcement learning  since we are trying to find good actions to yield the best reward  important details differ. here  we always start in the same state  choose several actions at once  and transition deterministically to a state we have probably never seen before to receive a reward. rather than learning about sequences of actions through multiple states  our emphasis is on representing the possible action sets compactly to facilitate generalization about the reward of various sets of actions. we assume independence of actions  which collapses the breadthof the tree  andadditivityof action costs  which allows learning from leaves. in essence  we generalize over both states and actions.
1	possible extensions
the particular adaptive probing algorithm we have evaluated is only one possible way to pursue this general approach. it would be interesting to try more restricted models  perhaps forcing action costs to be a smooth function of depth  for example. it may be worthwhile to distribute variance unequally among depths. additional features besides depth might be helpful  perhaps characterizing the path taken so far.
모the algorithm we have investigated here takes no prior experience into account. an initial bias in favor of the heuristic may be beneficial. furthermore  it may also be possible to reuse the learned models across multiple problems in the same domain.
모adaptive probing can be used for goal search  as we saw with boolean satisfiability  as long as a maximum depth and a measure of progress are available. if a measure of leaf quality is not available  it may be possible to fit the model using many small instances of similar problems  or small versions of the current problem  that can be quickly solved and then to scale up the model to guide probing on the original problem.
1	conclusions
it is a widely held intuition that tree search is only appropriate for complete searches  while local improvement search dominates in hard or poorly understood domains. adaptive probing can overcome the strong assumptions that are built into systematic tree search procedures. by learning a model of the tree on-line and simultaneously using it to guide search  we have seen how incomplete heuristic search can be effective in a tree-structured search space. when the node-ordering heuristic is very accurate  a systematic discrepancy search algorithm may be more effective. but for problems with unknown characteror domains that are less well-understood the robustness of adaptive probing makes it superior. its flexibility raises the possibility that  for difficult and messy problems  incomplete tree search may even be a viable alternative to local improvement algorithms.
acknowledgments
thanks to stuart shieber  avi pfeffer  irvin schick  rocco servedio  jeff enos  and the harvard ai research group for their many helpful suggestions and comments. this work was supported in part by nsf grants cda-1and iri1.
references
 abramson  1  bruce abramson. the expected-outcome model of two-player games. pitman  1.
 baluja and davies  1  shumeet baluja and scott davies. fast probabilistic modeling for combinatorial optimization. in proceedings of aaai-1  1.
 baluja  1  shumeet baluja. genetic algorithms and explicit search statistics. in michael c. mozer  michael i. jordan  and thomas petsche  editors  advances in neural information processing systems 1  1.
 bedrax-weiss  1  tania bedrax-weiss. optimal search protocols. phd thesis  university of oregon  eugene  or  august 1.
 boese et al.  1  kenneth d. boese  andrew b. kahng  and sudhakar muddu. a new adaptive multi-start technique for combinatorial global optimizations. operations research letters  1-1  1.
 boyan and moore  1  justin a. boyan and andrew w. moore. learning evaluation functions for global optimization and boolean satisfiability. in proceedings of aaai-1  1.
 bresina  1  john l. bresina. heuristic-biased stochastic sampling. in proceedings of aaai-1  pp 1  1.
 cesa-bianchi et al.  1  nicolo` cesa-bianchi  philip m. long  and manfred k. warmuth. worst-case quadratic loss bounds for on-line prediction of linear functions by gradient descent. ieee transactions on neural networks  1 :1  1.
 crawford and baker  1  james m. crawford and andrew b. baker. experimental results on the application of satisfiability algorithms to scheduling problems. in proceedings of aaai-1  pages 1  1.
 dorigo and gambardella  1  marco dorigo and luca maria gambardella. ant colony system: a cooperative learning approach to the traveling salesman problem. ieee transactions on evolutionary computation  1 :1  1.
 feo and resende  1  t. a. feo and m. g. c. resende. greedy randomized adaptive search procedures. journal of global optimization  1-1  1.
 gent and walsh  1  ian p. gent and toby walsh. phase transitions and annealed theories: number partitioning as a case study. in proceedings of ecai-1  1.
 hansson and mayer  1  othar	hansson	and	andrew
mayer. dts: a decision-theoretic scheduler for space telescope applications. in monte zweben and mark s. fox  editors  intelligent scheduling chapter 1  pages 1. morgan kaufmann  san francisco  1.
 harvey and ginsberg  1  william d. harvey and matthew l. ginsberg. limited discrepancy search. in proceedings of ijcai-1  pages 1  1.
 johnson et al.  1  david s. johnson  cecilia r. aragon  lyle a. mcgeoch  and catherine schevon. optimization by simulated annealing: an experimental evaluation; part ii  graph coloring and number partitioning. operations research  1 :1  may-june 1.
 joslin and clements  1  david e. joslin and david p. clements.  squeaky wheel  optimization. in proceedings of aaai-1  pages 1. mit press  1.
 juille뫣 and pollack  1  hughes juille뫣 and jordan b. pollack. a sampling-based heuristic for tree search applied to grammar induction. in proceedings of aaai-1  pages 1. mit press  1.
 karmarkar and karp  1  narenda karmarkar and richard m. karp. the differencing method of set partitioning. technical report 1  berkeley  1.
 karmarkar et al.  1  narenda karmarkar  richard m. karp  george s. lueker  and andrew m. odlyzko. probabilistic analysis of optimum partitioning. journal of applied probability  1-1  1.
 korf  1  richard e. korf. from approximate to optimal solutions: a case study of number partitioning. in proceedings of ijcai-1  1.
 korf  1  richard e. korf. improved limited discrepancy search. in proceedings of aaai-1  pp 1  1.
 meseguer  1  pedro meseguer. interleaved depth-first search. in proceedings of ijcai-1  pp 1  1.
 walsh  1  toby walsh. depth-bounded discrepancy search. in proceedings of ijcai-1  1.
heuristic search in infinite state spaces guided by lyapunov analysis
theodore j. perkins and andrew g. barto
department of computer science
university of massachusetts amherst
amherst  ma 1  usa
perkins barto  cs.umass.eduabstract
in infinite state spaces  many standard heuristic search algorithms do not terminate if the problem is unsolvable. under some conditions  they can fail to terminate even when there are solutions. we show how techniques from control theory  in particular lyapunov stability analysis  can be employed to prove the existence of solution paths and provide guarantees that search algorithms will find those solutions. we study both optimal search algorithms  such as a*  and suboptimal/real-time search methods. a lyapunovframework is useful for analyzing infinite-state search problems  and provides guidance for formulating search problems so that they become tractable for heuristic search. we illustrate these ideas with experiments using a simulated robot arm.
1	introduction
as the boundaries become less distinct between artificial intelligence and fields more reliant on continuous mathematics  such as control engineering  it is being recognized that heuristic search methods can play useful roles when applied to problems with infinite state spaces  e.g.  boone   davies et al.  . however  the theoretical properties of heuristic search algorithms differ greatly depending on whether the state space is finite or infinite.
모for finite state space problems  a variety of wellunderstood algorithms are available to suit different needs. for example  the a* algorithm finds optimal solutions when they exist  and is also  optimally efficient -no other heuristic search algorithm has better worst-case complexity. variants  such as ida*  allow more memory-efficient search at the cost of greater time complexity. conversely  suboptimal search methods  such as depth-first search or best-first search with an inadmissible heuristic  can often produce some solution  typically suboptimal  more quickly than a* can find an optimal solution  pearl  1; russell and norvig  1 . the rta* algorithm is able to choose actions in real-time  while still guaranteeing eventual arrival at a goal state  korf  1 . however  if the state space is infinite  none of these algorithms is guaranteed to have the same properties. a* is complete only if additional conditions hold on the costs of search operators  and it does not terminate if the problem admits no solution  pearl  1 . suboptimal search methods may not terminate  even if a closed list is maintained. rta* is not guaranteed to construct a path to a goal state  korf  1 .
모a further difficulty when infinite state spaces are considered is that the most natural problem formulations often include infinite action spaces. to apply heuristic search  one must select a finite subset of these actions to be explored in any given state. in doing so  the possibility arises that an otherwise reachable goal set becomes unreachable.
모some of these difficulties are unavoidable in general. with an infinite number of possible states  a search problem may encode the workings of a turing machine  including its memory tape. thus  the question of whether or not an infinitestate search problem has a solution is in general undecidable. however  it is possible to address these difficulties for useful subclasses of problems. in this paper we examine how lyapunov analysis methods can help address these difficulties when it is applicable. a lyapunov analysis of a search problem relies on domain knowledge taking the form of a lyapunov function. the existence of a lyapunov function guarantees that some solution to the search problem exists. further  it can be used to prove that various search algorithms will succeed in finding a solution. we study two search algorithms in detail: a* and a simple iterative  real-time method that incrementally constructs a solution path. our main goal is to show how lyapunov methods can help one analyze and/or formulate infinite-state search problems so that standard heuristic search algorithms are applicable and can find solutions.
모the paper is organized as follows. in section 1 we define heuristic search problems. section 1 covers some basics of lyapunov theory. sections 1 and 1 describe how lyapunov domain knowledge can be applied to prove that a search algorithm will find a solution path. the relationship between lyapunov functions and heuristic evaluation functions is also discussed. section 1 contains a demonstration of these ideas on a control problem for a simulated robot arm. section 1 concludes.
1	state space search problems
definition 1 a state space search problem  ssp  is a tuple
s g s1 o1	ok	  where:
s is the state set. we allow this to be an arbitrary set.
g	s is the set of goal states. s1	g is the initial  or start  state.
 o1 ok is a set of search operators. some search operators may not be applicable in some states. when a search operator oj is applied to a state s g  it results in a new state succj s and incurs a cost cj s 1
모a solution to an ssp is a sequence of search operators that  when applied starting at s1  results in some state in g. an optimal solution to an ssp is a solution for which the total  summed  cost of the search operators is no greater than the total cost of any other solution.
모for infinite state spaces  the infimal cost over all solution paths may not be attained by any path. thus  solutions may exist without there being any optimal solutions. this possibility is ruled out if there is a universal lower bound clow 1 on the cost incurred by any search operator in any non-goal state  pearl  1 . we will use the phrase  the ssp's costs are bounded above zero  to refer to this property.
모note that our definition of an ssp includes a finite set of search operators o1 ok   some of which may be unavailable in some states. we have made this choice purely for reasons of notational convenience. the theory we present generalizes immediately to the case in which there is an infinite number of search operators  but the number of operators applicable to any particular non-goal state is finite.
1	control lyapunov functions
lyapunov methods originated in the study of the stability of systems of differential equations. these methods were borrowed and extended by control theorists. the techniques of lyapunov analysis are now a fundamental component of control theory and are widely used for the analysis and design of control systems  e.g.  vincent and grantham 1 .
모lyapunov methods are tools for trying to identify a lyapunov function for a control problem. in search terms  a lyapunov function is most easily understood as a descent function  the opposite of a hill-climbing function  with no local minima except at goal states.
definition 1 given an ssp  a control lyapunov function  clf  is a function l : s   with the following properties:
1. l s	1 for all s	s
1. there exists 붻 1 such that for all s g there is some search operator oj such that l s l succj s 붻
모the second property asserts that at any non-goal state some search operator leads to a state at least 붻 down on the clf. since a clf is non-negative a descent procedure cannot continue indefinitely. eventually it must reach a state where a 붻 step down on l is impossible; such a state can only be a goal state.
모a clf is a strong form of domain knowledge  but the benefits of knowing a clf for a search problem are correspondingly strong. the existence of solutions is guaranteed  and  suboptimal  solutions can be constructed trivially. numerous sources discuss methods for finding lyapunov functions  e.g.  vincent and grantham   krstic뫣 et al.  . for many important problems and classes of problems  standard clfs have already been developed. for example  linear and feedback linearizable systems are easily analyzed by lyapunov methods. these systems include almost all modern industrial robots  vincent and grantham  1 . path planning problems similarly yield to lyapunovmethods  connolly and grupen  1 . many other stabilization and control problems  less typically studied in ai  have also been addressed by lyapunov means  including: attitude control of ships  airplanes  and spacecraft; regulation of electrical circuits and engines; magnetic levitation; stability of networks or queueing systems; and chemical process control  see levine  for references . lyapunov methods are relevant to many important and interesting applications.
모the definition of a clf requires that at least one search operator leads down by 붻 in any non-goal state. a stronger condition is that all search operators descend on the clf:
definition 1 given an ssp  the set of search operators descends on a clf l if for any s g there exists at least one applicable search operator  and every applicable search operator oj satisfies l s l succj s 붻 for some fixed 붻 1
1	clfs and a*
in this section we establish two sets of conditions under which a* is guaranteed to find an optimal solution path in an ssp with an infinite state space. we then discuss several other search algorithms  and the relationship of clfs to heuristic evaluation functions.
theorem 1 if there exists a clf l for a given ssp  and either of the following conditions are true:
 1. the set of search operators descends on l  or
 1. the ssp's costs are bounded above zero  then a* search will terminate  finding an optimal solution path from s1 to g
모proof: under condition 1  there are only a finite number of non-goal states reachable from a given start state s1 the only way for an infinite number of states to be reachable is for them to occur at arbitrarily large depths in the search tree. but since every search operator results in a state at least 붻 lower on l and l 1 everywhere  no state can be at a depth greater than l s1 붻 since the clf implies the existence of at least one solution and there are only a finite number of reachable states  it follows  e.g.  from pearl   section 1   that a* must terminate and return an optimal solution path.
모alternatively  suppose conditions 1 holds. the clf ensures the existence of at least one solution path. let this path have cost c. since each search operator incurs at least clow cost  the f-value of a node at depth d is at least d clow. a* will not expand a node with f-value higher than c. thus  no node at depth d c clow will ever be expanded. this means a* must terminate and return an optimal solution. qed.
모these results extend immediately to variants of a* such as uniform cost search or ida*. under condition 1  not only is the number of reachable states finite  but these states form an acyclic directed graph and all  leaves  of the graph are goal states. thus  a great many search algorithms can safely be applied in this case. under condition 1  depth-first branchand-bound search is also guaranteed to terminate and return an optimal solution  if it is initialized with the bounding cost c this follows from nearly the same reasoning as the proof for a*.
모how do clfs relate to heuristic evaluation functions  in general  a clf seems a good candidate for a heuristic function. it has the very nice property that it can be strictly monotonically decreased as one approaches the goal. contrast this  for example  to the manhattan distance heuristic for the 1puzzle  russell and norvig  1 . an 1-puzzle solution path typically goes up and down on the manhattan distance heuristic a number of times  even though the ultimate effect is to get the heuristic down to a value of zero.
모however  a clf might easily fail to be admissible  overestimating the optimal cost-to-goal from some non-goal state. notice that the definition of a clf does not depend on the costs of search operators of an ssp. it depends only on s g and the successor function. clfs capture information about the connectivity of the state space more than the costs of search operators.
모a possible  fix  to the inadmissibility of a clf is to scale it. if a clf is multiplied by any positive scalar  defining a new function l 붸l  then l is also a clf. if a given clf overestimates the cost-to-goal for an ssp  then it is possible that a scaled-down clf would not overestimate. we will not elaborate on the possibility of scaling clfs to create admissible heuristics in this paper. we will  however  use the scalability of a clf in the next section  where we discuss real-time search.
1	real-time search
lyapunovdomain knowledge is especially well suited to realtime search applications. as mentioned before  one can rapidly generate a solution path by a simple descent procedure. a generalization of this would be to construct a path by performing a depth-limited search at each step to select the best search operator to apply next.  best  would mean the search operator leading to a leaf for which the cost-so-far plus a heuristic evaluation is lowest. we will call this algorithm  repeated fixed-depth search  or rfds.
모korf  described this procedure in a paper on realtime search and  rightly  dismissed it because in general it is not guaranteed to produce a path to a goal state  even in finite state spaces. however  the procedure is appealing in its simplicity and appropriate for real-time search in that the search depth can be set to respond to deadlines for the selection of search operators. in this section we examine conditions under which this procedure can be guaranteed to construct a path to a goal state.
theorem 1 given an ssp and a clf  l  for that ssp  if the set of search operators descends on l then rfds with any depth limit d 1 and any heuristic evaluation function will terminate  constructing a complete path from s1 to g
모proof: at each step  any search operator that is appended to the growing solution will cause a step down on l of at least 붻  which inevitably leads to g qed.
모a more interesting and difficult case is when some of the operators may not be descending  so that the solution path may travel up and down on the clf.
theorem 1 given an ssp whose costs are bounded above zero and a clf  l. suppose that l is used as a heuristic evaluation of non-goal leaves  and that l s l succ1 s c1 s for all s g. then rfds with any depth limit d 1
will terminate  constructing a complete path from s1 to g
모note that the theorem assumes that o1 has the special property l s l succ1 s c1 s . more generally  in each state there must be some search operator that satisfies this property. it is only for notational convenience that we have assumed that o1 satisfies the property everywhere.
모to prove theorem 1  we require some definitions. suppose rfds generates a path s1 s1 sn where sn may or may not be a goal state but the other states are definitely non-goal.
at the tth step  t 1 n rfds generates a search tree to evaluate the best operator to select next. let gt be the cost of the search operators leading up to state st; let lt be the leafstate with the best  lowest  evaluation in the tth search tree; let ct 1 ct n t be the costs of the search operators from st to lt; and let ht be the heuristic evaluation of lt-zero if the leaf corresponds to a goal state  and l lt otherwise. define ft gt 뫉in t1 ct i ht
lemma 1 under the conditions of theorem 1  ft ft 1 for t 1 n 1
모proof: ft gt 뫉in t1 ct i ht gt 1 뫉in t1 ct i ht because st 1 is one step along the path to the best leaf of iteration t. thus  the inequality we need to establish is 뫉in t1 ct i ht  ct 1 i ht 1 the right side of this inequality is simply the cost of some path from st 1 to lt 1 plus a heuristic evaluation. the left side corresponds to a path from st 1 to lt  plus a heuristic evaluation.
모first  suppose lt is a goal state. the path from st 1 to lt will be included among those over which rfds minimizes at the tth step. thus  in this case the inequality holds.
모if lt is not a goal state  then the search at iteration t 1 expands lt. the evaluation of the path from st 1 to succ1 lt
is뫉in t1 ct ic1 lth succ1 lt뫉in t1 ct ic1 ltl succ1 lt뫉in t1 ct il lt뫉in t1 ct iht모thus  this path's cost meets the inequality  and since rfds minimizes over that and other paths  the inequality must hold for the best path of iteration t 1 qed.
모proof of theorem 1: suppose rfds runs forever without constructing a path to a goal state. from the previous lemma  ft f1 for all t 1. since the ssp's costs are bounded above zero  ft gt t clow for sufficiently large t  this contradicts ft f1 thus rfds does construct a path to goal. qed.
모theorem 1 requires a relationship between the decrease in a clf caused by application of o1 and the cost incurred by o1. a given clf may not satisfy this property  or one may not know whether the clf satisfies the property or not. we propose two methods for generating a clf guaranteed to have the property  assuming that applying o1 to any non-goal state causes a decrease in the original clf of at least 붻  for some fixed 붻 1
모the first method is scaling  which is applicable when there is a maximum cost chigh that operator o1 may incur when applied to any non-goal state. in this case  consider the new
chigh
clf l 붻 l. for any s g l s l succ1 s chigh 붻 l s l succ1 s chigh 붻 붻 chigh c1 s
thus  l is a clf satisfying the conditions of theorem 1.
모a problem with this scheme is that one or both of the constants chigh and 붻1 may be unknown. an overestimate of the
모모모모모chigh constant 붻1 will produce a new clf satisfying the theorem. if an overestimate cannot be made directly  then a scaling factor can be determined on-line  as rfds runs. suppose one defines l 붸l for any initial guess 붸. one can than perform rfds  checking the condition l s l succ1 s c1 s every time o1 is applied to some state. if the condition is violated  update the scaling factor  e.g.  as 붸
c1 s 붼 for some fixed 붼 1 this update rule l s l succ1 s
ensures that the guess 붸 is consistent with all the data ob-
모모모모모모모모모모모모모모모모모모모모모모chigh served so far and will meet or exceed 붻1 in some finite number of updates. if rfds does not construct a path to a goal state by the time that a sufficient number of updates are performed  then theorem 1 guarantees the completion of the path afterward.
모a second method for generating a clf that meets the conditions of theorem 1 is to perform roll-outs  tesauro and galperin  1; bertsekas et al.  1 . in the present context  this means defining l s  the cost of the solution path generated by repeated application of o1 starting from s and until it reaches g . the function l is evaluated by actually constructing the path. the reader may verify that l meets the definition of a clf and satisfies the requirements of theorem 1. performing roll-outs can be an expensive method for evaluating leaves because an entire path to a goal state needs to be constructed for each evaluation. however  roll-outs have been found to be quite effective in both game playing and sequential control  tesauro and galperin  1; bertsekas et al.  1 .
1	robot arm example
we briefly illustrate the theory presented above by applying it to a problem requiring the control of the simulated 1-link robot arm  depicted in figure 1. the state space of the arm is  1  corresponding to three angular joint positions and three angular joint velocities. we denote the joint position and joint velocity column vectors by 붿 and 붿빡. the set g of goal states is s s : s 1   which is a small hyper-rectangle of states in which the arm is nearly stationary in the straight-out horizontal configuration.
모the dynamics of mechanical systems such as a robot arm are most naturally described in continuous time. a standard

figure 1: diagram of the 1-link robot arm
model for a robot arm is  craig  1 :
	d	d	붿	붿빡
	dts	dt 붿빡	h 1 붿 붿빡 뷉 v 붿 붿빡	g 붿
where g represents gravitational forces  v represents coriolis and other velocity-dependent forces  h is the inertia matrix  and 뷉 is the actuator torques applied at the joints.
모we develop a set of continuous time controllers-rules for choosing 뷉--based on feedback linearization and linearquadratic regulator  lqr  methods  e.g. 	vincent and grantham  1 .	these controllers form the basis of the search operators described in the next section. feedback linearization amounts to reparameterizing the control torque in terms of a vector u  where 뷉 hu	v	g this puts the robot dynamics into the particularly simple linear form d	붿	붿빡

	dt	붿빡	u
to which lqr control design is applicable. an lqr controller is a rule for choosing u  and thus 뷉  based on 붿 and 붿빡. lqr design yields two main benefits: 1  a simple controller that can asymptotically bring the arm to any specified target state  1  a lyapunov function that can be used as a clf for the problem of getting to a goal state. we define five basic controllers:
c1: an lqr controller with 붿 1 1 as the target configuration. this configuration is within g  so c1 alone is able to bring the arm into g from any initial position. we use the associated lyapunov function as a clf for the ssp we are constructing: larm stqs  where q is a symmetric positive definite matrix produced during the standard computations for the lqr design.
c1: chooses 뷉 as an lqr controller with target configuration 붿 1 1 except that u is multiplied by 1. this tends to cause higher control torques that bring the arm towards the target configuration faster.
c1: similar to c1  but u is multiplied by 1 instead of 1.
c1: an lqr controller that does not apply any torque to the first joint  but regulates the second two joints to configuration 붿1 붿1 뷇 1 뷇 1 . this tends to fold the arm in towards the center  so it can swing around the first joint more easily.
c1: similar toc1  but regulating the second two joints to the configuration 붿1 붿1 뷇 1 뷇
1	search operators
we construct two sets of five search operators. in the first set of search operators  ops1  each operator corresponds to one of the five controllers above. succj s is defined as the state that would result if the arm started in state s andcj controlled the arm for   1 time interval.
모we define the cost of applying a search operator to be the continuous time integral of the quantity 붿 t 1 뷉 t 뷉1 where 붿 t are the angles the arm goes through during the  -time interval  뷉 t are the joint torques applied  and 뷉1 is the torque needed to hold the arm steady  against gravity  at configuration 붿 1 1 this kind of performance metric is standard in control engineering and robotics. the term 붿 t 1 reflects the desire to move the arm to g; the term 뷉 t 뷉1 penalizes the use of control torques to the extent that they differ from 뷉1. defined this way  the costs of all search operators are bounded above zero.
모the second set of search operators  ops1  is also based on controllers c1 through c1  but the continuous-time execution is somewhat different. when a controller cj j 1
is being applied for a  -time interval  the time derivative of larm  l빡arm  is constantly computed. if l빡arm is greater than 1  the torque choice of c1 is supplied instead. under c1  l빡arm 붻1 for some unknown 붻1 in this way 
l빡arm is bounded below zero for all the ops1 search operators  and thus they all step down larm by at least the  unknown  amount 붻   min 1 붻1 with each application. the costs for applying these search operators is defined in the same way as for ops1.
1	experiments and results
we tested a* and rfds on the arm with both sets of search operators  averaging results over a set of nine initial configurations 붿t1 x y y for x 뷇 1뷇 1 뷇 1 and y 뷇 1 뷇 1 theorem 1 guaranteed that a* would find an optimal solution within either set of search operators. we ran rfds at a variety of depths and with three different heuristic leaf evaluation functions: zero  rfds-z   roll-outs  rfds-r   and a scaled version of larm  rfds-s . because we did not know an appropriate scaling factor for larm  for each starting configuration we initialized the scaling factor to zero and updated it as discussed in section 1  using 붼 1 all of the rfds runs under ops1 were guaranteed to generate solutions by theorem 1. for the rfds-r and rfds-s runs with ops1  theorem 1 provided the guarantee of producing solutions.
모the results are shown in figures 1 and 1  which report average solution cost  nodes expanded by the search  and the amount of virtual time for which the robot arm dynamics were simulated during the search. simulated time is closely related to the number of nodes expanded  except that the rfds-r figures also account for the time spent simulating the roll-out path to the goal. the actual cpu times used were more than an order of magnitude smaller than the simulated times  and our code has not been optimized for speed. the  c1 only  row gives the average solution cost produced by controlling the arm using c1 from every starting point. achieving this level of performance requires no search at all. a* with either set of search operators found significantly lower cost solutions than those produced by c1. the a* solution qualities with ops1 and ops1 are very close. we anticipated that ops1 results might not be as good  because the paths are constrained to always descend on larm  whereas ops1 allows the freedom of ascending. for this problem  the effect is small. a* with ops1 involved significantly less search effort than with ops1  in terms of both the number of expanded nodes and the amount of arm simulation time required.
모the rfds-z - ops1 combination is the only one not guaranteed to produce a solution  and indeed  it did not. under ops1  rfds-z is guaranteed to produce a path to a goal for any search depth. at low search depths  the solutions it produced were worse than those produced by c1. however  at greater depth  it found significantly better solutions  and with much less search effort than required by a*.
모rfds-r with either set of search operators produced excellent solutions at depth one. this result is quite surprising. apparently the roll-out yields excellent information for distinguishing good search operators from bad ones. another unexpected result is that greater search depths did not improve over the depth-one performance. solution quality is slightly worse and search effort increased dramatically. at the higher search depths  rfds-r required more arm simulation time than did a*. this primarily resulted from the heuristic leaf evaluations  each of which required the simulation of an entire trajectory to g.
모rfds-s produced good solutions with very little search effort  but greater search effort did not yield as good solutions as other algorithms were able to produce. rfds-s seems to be the most appropriate algorithm for real-time search if very fast responses are needed. it produced solutions that significantly improved over those produced by c1  using search effort that was less than one tenth of that of the shallowest rollout runs  and less than one hundredth of the effort required by a*.
1	discussion
the arm simulation results reflect some of the basic theory and expectations developed earlier in the paper. the algorithms that were guaranteed to find solutions did  and those that were not guaranteed to find solutions did not. these results depend on  among other things  our choice of the  duration  of a search operator   . we ran the same set of experiments for four other choices of  : 1  1  1  and 1. many of the qualitative results were similar  including the excellent performance of roll-outs with a depth-one search.
모one difference was that for higher    a* became more competitive with rfds in terms of search effort. con-
algorithmsol'nnodessim.costexpandedtimec1 only11a*111rfds-z depths 1rfds-r depth 1.1.1.1rfds-r depth 1.1.1.1rfds-r depth 1.1.1.1rfds-r depth 1.1.1.1rfds-r depth 1.1.1.1rfds-s depth 1.1.1.1rfds-s depth 1.1.1.1rfds-s depth 1.1.1.1rfds-s depth 1.1.1.1rfds-s depth 1.1.1.1figure 1: ops1 results
algorithmsol'nnodessim.costexpandedtimec1 only11a*111rfds-z depth 1.1.1.1rfds-z depth 1.1.1.1rfds-z depth 1.1.1.1rfds-z depth 1.1.1.1rfds-z depth 1.1.1.1rfds-r depth 1.1.1.1rfds-r depth 1.1.1.1rfds-r depth 1.1.1.1rfds-r depth 1.1.1.1rfds-r depth 1.1.1.1rfds-s depth 1.1.1.1rfds-s depth 1.1.1.1rfds-s depth 1.1.1.1rfds-s depth 1.1.1.1rfds-s depth 1.1.1.1figure 1: ops1 results
versely  at   1  a* exhausted the computer's memory and crashed. the complexity of a* generally grows exponentially with the length of the optimal solution. for rfds  which iteratively constructs a solution  effort is linear in the length of the solution it produces. the complexity of rfds is more governed by the search depth  which can be chosen independently. another difference observed at   1 is that rfds-z succeeded in producing solutions from all starting positions when the search depth was four or higher.
모for all algorithms  solution quality was lower for higher  . this is simply because lower   allows finer-grained control over the trajectory that the arm takes. the freedom to choose   suggests an alternative means for performing realtime search: one could perform a sequence of a* searches  initially with a high   and then decrease   to find solutions at finer temporal resolutions.
1	conclusion
we demonstrated how domain knowledge in the form of a lyapunov function can help one analyze and formulate infinite-state search problems. lyapunov methods guarantee the existence of solutions to a problem  and they can be used to show that both optimal and suboptimal/real-time search procedures will find those solutions. these results provide a theoretical basis for extending the range of problems to which heuristic search methods can be applied with a guarantee of results.
acknowledgments
this work was funded by the national science foundation under grant no. ecs-1. any opinions  findings  and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation. theodore perkins is also supported by a graduate fellowship from the university of massachusetts amherst.
references
 bertsekas et al.  1  d. p. bertsekas  j. n. tsitsiklis  and c. wu. rollout algorithms for combinatorial optimization. journal of heuristics  1.
 boone  1  g. boone. minimum-time control of the acrobot. in 1 international conference on robotics and automation  pages 1  1.
 connolly and grupen  1  c. i. connolly and r. a. grupen. the applications of harmonic functions to robotics. journal of robotics systems  1 :1  1.
 craig  1  j. j. craig. introduction to robotics: mechanics and control. addison-wesley  1.
 davies et al.  1  s. davies  a. ng  and a. moore. applying online search techniques to continuous-state reinforcement learning. in proceedings of the fifteenth national conference on artificial intelligence  aaai-1   pages 1  1.
 korf  1  r. e. korf. real-time heuristic search. artificial intelligence   1 :1  1.
 krstic뫣 et al.  1  m. krstic뫣  i. kanellakopoulos  and p. kokotovic뫣. nonlinear and adaptive control design. john wiley & sons  inc.  new york  1.
 levine  1  w. s. levine  editor. the control handbook. crc press  inc.  1.
 pearl  1  j. pearl. heuristics. addison wesley publishing company  inc.  reading  massachusetts  1.
 russell and norvig  1  s. j. russell and p. norvig. artificial intelligence: a modern approach. prentice-hall  inc.  englewood cliffs  new jersey 1  1.
 tesauro and galperin  1  g. tesauro and g. r. galperin. on-line policy improvement using monte-carlo search. in advances in neural information processing: proceedings of the ninth conference. mit press  1.
 vincent and grantham  1  t. l. vincent and w. j. grantham. nonlinear and optimal control systems. john wiley & sons  inc.  new york  1.
a backbone-search heuristic for efficient solving of hard 1-sat formulae
	olivier dubois	and gilles dequen
lip1  c.n.r.s.-universite뫣 paris 1  1 place jussieu  1 paris cedex 1  france.
laria  universite뫣 de picardie  jules verne  c.u.r.i.  1 rue du moulin neuf  1 amiens  france. e-mail : olivier.dubois lip1.fr  dequen laria.u-picardie.frabstract
of late  new insight into the study of random -sat formulae has been gained from the introduction of a concept inspired by models of physics  the 'backbone' of a sat formula which corresponds to the variables having a fixed truth value in all assignments satisfying the maximum number of clauses. in the present paper  we show that this concept  already invaluable from a theoretical viewpoint in the study of the satisfiability transition  can also play an important role in the design of efficient dpl-type algorithms for solving hard random -sat formulae and more specifically -sat formulae. we define a heuristic search for variables belonging to the backbone of a -sat formula which are chosen as branch nodes for the tree developed by a dpl-type procedure. we give in addition a simple technique to magnify the effect of the heuristic. implementation yields dpl-type algorithms with a significant performance improvement over the best current algorithms  making it possible to handle unsatisfiable hard 1-sat formulae up to 1 variables.
1	introduction
the satisfiability problem  as one of the basic np-complete problems  garey and johnson  1   has appeared to possess a property of great interest with respect to computational complexity. random cnf formulae in which each clause has a fixed number of literals  known as -sat formulae  exhibit with respect to satisfiability  a so-called phase transition phenomenon such that the sat formulae at the transition appear in probability the most difficult to solve. this property  initially described in  mitchell et al.  1  and since studied in numerous papers  crawford and auton  1; gent and walsh  1  has the following practical manifestation. when in an experiment clauses with exactly literals      ...  limited to manageable sizes of formulae   over a set of boolean variables  are chosen at random with a fixed ratio   the probability of satisfiability falls abruptly from near to near as passes some critical value called the threshold. moreover  leaving aside 1-sat

this work was supported by advanced micro devices inc.
formulae which are well known to be solvable in polynomial time   at the threshold  for a peak of difficulty of solving is observed and this peak grows exponentially as a function of . this intriguing property correlating the satisfiability threshold with the hardness of -sat formulae  has stimulated many theoretical as well as experimental studies  eliciting a better understanding of the phase transition phenomenon and progress in solving hard sat formulae.
recently  the satisfiability phase transition has attracted much attention from physicists. sophisticated models of statistical physics have been evolved over the past few decades to predict and analyze certain phase transitions  and these can be compared  from a theoretical viewpoint  with the satisfiability transition. this has shed new light on the question. in particular in 1  an insightful concept was introduced  the 'backbone' of a sat formula  monasson et al.  1 . the backbone corresponds to the set of variables having a fixed truth value in all assignments satisfying the maximum number of clauses of a -sat formula  cf maxsat . the relevance of this concept is connected to the fact that the number of solutions  i.e. satisfying assignments  of a satisfiable random sat formula has been proved to be almost surely exponential as a function of up to and at the threshold  boufkhad and dubois  1   a result corroborated in its maxsat version by the physics model known as 'replica symmetric ansatz'. this concept of backbone of a -sat formula has turned out to play an important role in theoretical studies  allowing for example  the scaling window of the 1-sat transition to be determined  bolloba뫣s et al.  1 . we show in this paper that the concept of backbone can also play an important role in more efficiently solving hard -sat formulae. we will focus particularly on 1-sat formulae which are the easiest -sat formulae to handle  for   and therefore at present the most studied in the literature. the hard 1-sat formulae are generated randomly with a ratio around which appears experimentally to be close to the threshold value. in the last ten years  the best-performing complete solving algorithms  i.e.  those definitely determining whether a solution exists or not  have been based on the classical dpl procedure  davis et al.  1 . important progress was achieved  making it possible at present  for example  to solve -sat hard formulae with 1 variables  and therefore with 1 clauses  in roughly the same computing time as formulae with 1 variables  and 1 clauses  ten years ago under equivalent computing conditions.
the dpl-type procedures which have led to this progress were all designed according to a single viewpoint  that of dealing with the clauses which remain not satisfied at successive nodes of the solving tree  trying mainly to reduce them as much as possible in number and in size. clauses already satisfied in the course of solving were not considered. by using the concept of backbone we change this viewpoint  focusing on the clauses which can be satisfied in the course of solving  and how. thus  we first explain in this paper how with this new viewpoint  the size of trees developed by a dpl-type procedure can be efficiently reduced. then we give practically a simple branching heuristic based on the search for variables belonging to the backbone of a formula  and we show that it brings immediately an improvement in performance compared with the best current heuristic  this improvement increasing with the number of variables. we further present a technique to magnify the effect of the heuristic. combining this heuristic and this technique in a dpl-type procedure  we obtain significant performance improvements over existing sat solvers in the literature  solving unsatisfiable hard random 1-sat formulae with 1 to 1 variables from 1 to about 1 times faster than by the current best suitable algorithms  particularly satz1  li  1 . moreover  hard 1-sat formulae up to 1 variables are shown to be solvable by our procedure in about 1 days of computing time  which begins to be relatively acceptable. these experimental results reported in the present paper bring an up beat view in contrast to the misgivings expressed in  li and gerard  1  about the possibility to obtain further significant progress with dpl-type algorithms  which went as far as to suggest that the best current algorithms could be close to their limitations.
1	an approach using the concept of backbone for designing efficient dpl-type procedures.
in this section  we need to generalize the notion of backbone of a sat formula . given a set of clauses   no longer necessarily of maximum size  we define the backbone associated to as the set of literals of having the truth value true in all assignments satisfying the clauses of .
since the overall performance of a complete algorithm on random sat formulae depends essentially on how it handles unsatisfiable formulae  in the remainder of this section we consider only unsatisfiable 1-sat formulae. let  then  be an unsatisfiable 1-satformula with clauses built on a set of literals  itself derived from a set of boolean variables. let be the so-called refutation tree of   as developed by a dpl-type procedure. at each leaf of   an inconsistency is detected. let be the subset of clauses of satisfied by the literals set to true along the path from the root of to the leaf . call the set of assignments satisfying . suppose there is a backbone associated to
     . from the above-mentioned theoretical study  this can be expected in practice often to be the case at the leaves of . the inconsistency detected at means that assignments are refuted as possible solutions of . we have the inequality: . equality obtains precisely when the set of the	literals constitutes the backbone	of
     . furthermore  the assignments refuted as possible solutions of at different leaves of are distinct  form disjoint sets   and their total number equals . hence  in lowering the number of leaves of two factors intervene  namely:  i  the size of the sets must be as large as possible  and  ii  the equality must hold most of the time  at least approximately.
the above considerations extend to all nodes of . in this case  at any node   represents the potential maximum number of assignments that can be refuted as possible solutions of at the node . according to the above viewpoint  the global condition in order best to reduce the size of is that  from the root to a nearest node   literals be chosen which constitute a backbone relative to some subset of clauses of . indeed  the nearer the node is to the root  the smaller the set of satisfied clauses will tend to be. now  the crucial point is that any assignment giving the value false to at least one literal of the backbone  can be refuted on the basis of the subset of clauses associated to . if the set is of small size  it is then to be expected that any refutation derived from be short  i.e. that the refutation tree for all such assignments be of small size. let us illustrate this with the following very simple example. suppose that among all the clauses of there are 1 of the form:

			. the
backbone of these 1 clauses reduces to the literal . according to the above principle  this literal is a preferential choice for a branch stemming from the root. in the branch of the literal   refutations are obtained by branching just twice  applying the same principle of bringing forward a backbone at the nearest possible node  thus either or  will be chosen indifferently. note that in this example the backbone search principle includes in its general form the notion of resolvent without using it. the literal could of course be inferred by applying classical resolution.
we next detail a heuristic and a technique which embody the foregoing  if only partially.
1	a backbone-variables search heuristic
consider a generic node of the refutation tree of   and let be the reduced formula at node . the heuristic  figure 1  described below aims at selecting literals that may belong to a hypothetical backbone associated to a subset of clauses of of the smallest possible size. the simple idea sustaining this heuristic is to estimate the number of 'possibilities' for a given literal of to be constrained to true in the clauses of where it appears. e.g.  if contains the clause   one possibility for to be true is
               . if in addition	and	appear	and times  respectively  in clauses of	  the number of possibilities for	to be true can be estimated as	at this second level by setting to false the necessary literals in the clauses where	and	appear. if however	and	appear in both binary and ternary clauses  we must consider the possibility to be greater when a binary rather than a ternary clause is involved  since a single literal is required to have the value false instead of two. to evaluate a 'possibility'  we therefore weight the relevant binary and ternary clauses. but for set	and let max be an integer.
proc
compute
	if	max then
	return	
else
모return		end if
figure 1: backbone search heuristic h
the heuristic evaluation  only the ratio of the weightings matters. as a first apporximation  we take the ratio of probabilities in a random assignment  i.e. simply a ratio of 1. within the heuristic function  an evaluation level is defined for   as exemplified by the above allusion to a second-level evaluation of . to be precise  the evaluation level is the number of ternary clauses which must successively be brought into play in order to reach the conclusion that is true  following the aforementioned principle. fixing an evaluation level thus limits the number of ternary clauses to be used before concluding that is true. on the other hand  there is no need to limit the number of binary clauses that can be used  with or without ternary ones. since binary clauses  due to the fact that a single literal has to be false  do not give rise to combinations  their use does not increase the computational complexity of the heuristic. at a given node of the refutation tree  all literals must be evaluated to the same level so as to guarantee comparable evaluations. the heuristic function in figure 1 embodies the simple principles just stated. the estimation level equals the number of recursive calls to   namely the constant max augmented by 1. for the formal definition of the heuristic  there remains to introduce two types of sets of clauses. denotes the set of binary or unary clauses derived by eliminating   from the clauses of where appears. let and be the number of unary and binary clauses respectively in . is the set of all binary subclauses derived from ternary clauses of such that each of these binary subclauses set to false  implies true either directly or by virtue of certain unary clauses having the value false. let us take an example showing how to compute the heuristic h. consider the following set of clauses:



.
we evaluate	on the above set of clauses with max set to
1. we have   . to carry out the evaluation  the sets  must be determined for each literal appearing in the clauses of   in order to know the values  and

	 . we thus have:	 	 

	and	. with regard to	  we
have	because of the unary clause	  and

owing to . proceeding thus for all variables in the clauses of   the value of is :

.
we assessed the performance of the heuristc in a pure dpl procedure. at each node of the tree developed by the procedure  the product  was computed for every as yet unassigned variable . the chosen branching variable was that corresponding to the maximum value of  at the node under consideration. the product  favors opposite literals which may each belong to a different backbone relative to a separate subset of clauses. the performance of
this heuristic was compared to that of a heuristic of the type used in recent algorithms. we implemented the heuristic of the solver  satz1   li  1   appearing currently to be the fastest for solving random 1-sat formulae. we solved samples of 1 random 1-sat formulae having from 1 to 1 variables in steps of 1  and a clauses-to-variables ratio of 1. for every considered number of variables  the mean ratio of the computation time with the satz1 heuristic compared to the computation time with the heuristic described above   h  heuristic   for a level limited to 1  has been plotted on figure 1. the smooth line connecting the consecutive points shows the evolution of this mean ratio from 1 to 1 variables. this ratio is seen to be markedly above 1  it increases from 1 to 1 with accelerated growth. table 1

size of hard random unsatisfiable problem  in #variables 
figure 1: mean time ratio of satz1 heuristic to  h  heuristic on hard random 1-sat unsatisfiable formulae.
gives the mean size of the tree developed with either heuristic. these results immediately demonstrate the advantage of the approach based on the concept of backbone.
size of formulae    heuristicsatz1 heuristicin #varsmean #nodesmean #nodes11111111111table 1: average size of tree built by satz1 heuristic and  h  heuristic on hard random 1-sat unsatisfiable formulae.
1	picking backbone variables
the technique described in this section aims at improving the efficiency of the previous heuristic by detecting either literals which must belong to some backbone relative to some set of clauses  or clauses that cannot be associated to literals belonging to a backbone. only the principles of the technique are given in this section  together with illustrative examples. a literal has to belong to a backbone relative to some set of clauses  if among all possibilities leading to being true  at least one will necessarily hold. take the simple example of the following clauses: example 1:


consider the literal . at the first level  the possibilities that it be true are  and . at the
second level  these possibilities become simply and  . necessarily one or the other will obtain.
therefore 	belongs to the backbone associated to the given
set of clauses.
conversely  a literal in some clause cannot belong to a backbone  if setting to true  it leads to an inconsistency. this principle generalizes the classical local treatment of inconsistency detection at each node of a solving tree  such as implemented in csat  posit  and satz1. as an example  take the following clauses: example 1:


according to the criteria usually adopted by algorithms in the literature  only the literal  or at most the 1 literals and  in the clauses of example 1 will be selected for a local treatment of inconsistency detection. setting to true leads to an inconsistency by unit propagation. from the backbone viewpoint  cannot  therefore  belong to a backbone. as mentioned  this case is classically handled by existing algorithms. now suppose that  instead of the sixth clause  we have the clause  . unit propagation no longer detects an inconsistency. on the other hand  following our backbone viewpoint and without changing the classical selection criteria for the local treatment of inconsistencies  which are of recognized efficiency   it is possible to detect that the first clause containing     cannot contribute to a set of clauses such that would belong to a backbone relative to this set. indeed  the only possibility that be true is that be false. an inconsistency now follows  again by unit propagation. as a consequence  is set to true  and the clause cannot be associated to a backbone including . the technique inspired by the principle just stated is called pickback.
we briefly mention two other examples which show the general nature of this principle. example 1:




setting to true does not lead to an inconsistency. yet  setting to false by 'pickback' leads  via unit propagation  to an inconsistency. we may therefore add this clause    which subsumes	.
example 1:


setting to true does not lead to an inconsistency. yet setting alone to false in allows to deduct the value false for   thus validating a stronger pickback than in the previous example.
1	experimental results
the  h  heuristic and the technique just described in section 1 were implemented in a dpl procedure. this implementation is denoted cnfs  short for cnf solver . in this section comparative performance results are given  pitting cnfs against 1 solvers which currently appear to be the best performers on -sat or -sat random formulae. these are: tableau in its version ntab  crawford and auton  1   posit  freeman  1   csat  dubois et al.  1   and satz1  li  1 . in addition we consider 1 strong solvers sato  zhang  1   relsat  bayardo and schrag  1  which are more devoted to structured formulae than to random formulae. we carried out our experimentations on random 1-sat formulae generated as per the classical generators in the literature  i.e. each clause drawn independently and built by randomly drawing 1 among a fixed number of variables  then negating each with probability  . in order that the impact of the backbone approach used in cnfs be shown with as much precision as possible  our experiments covered a wide range of formula sizes  going from 1 to 1 variables and drawing large sam-
ples. the hardware consisted of amd athlon-equipped1 pcs running at 1 ghz under a linux operating system. the com-
parative results thus obtained are now to be given in detail.
1	performance comparison results
comparative results from 1 to 1 variables
table 1 gives the mean sizes of the solving trees developed by the 1 programs under scrutiny  sato  relsat  ntab  csat  posit  satz1 and cnfs  on samples of 1 formulae with 1 and 1 variables  and on samples of 1 formulae with 1 variables. mean tree sizes are expressed as numbers of branching nodes  and are listed separately for satisfiable and unsatisfiable formulae. they are found to be from 1 to about 1 times smaller for cnfs than for the 1 solvers ntab  csat  posit and satz1 in the case of unsatisfiable formulae  and from 1 to 1 times smaller for satisfiable formulae; these ratios depend on both solver and formula size. it is important to observe that the compute time ratio of cnfs with respect to each of the other 1 solvers increases with formula size. table 1 shows how improved tree sizes obtained with cnfs translate into computation times. compute time ratios of cnfs with respect to ntab  csat  posit and satz1 are seen to vary from 1 to 1 in the unsatisfiable  and from 1 to 1 in the satisfiable case. tree size improvements thus carry over into equivalent computation time gains. these first results show  therefore  that cnfs provides  in the range of formulae from
sat
solversunsat  n 
& 1 v 1 c  1 n   1 y  1 v 1 c  1 n   1 y  1 v 1 c  1 n   1 y sat  y mean #nodes
 std dev. mean #nodes
 std dev. mean #nodes in
millions  std dev. satounsat1  1 1  1 -sat1  1 1  1 -rel satunsat1  1 1  1 1  1 sat1  1 1  1 1  1 ntabunsat1  1 1  1 1  1 sat1  1 1  1 1  1 csatunsat1  1 1  1 1 1 sat1  1 1  1 1 1 positunsat1  1 1  1 1 1 sat1 1 1  1 1 1 satz1unsat1 1 1  1 1 1 sat1 1 1  1 1 1 cnfsunsat1 1 1  1 1 1 sat1 1 1  1 1 1 table 1: average size of tree on hard random 1-sat formulae from 1 to 1 variables for sato  relsat  tableau  csat  posit  satz1 & cnfs solvers
1 to 1 variables  significant performance improvements that increase with formula size  in terms of tree size as well as computation time.
further performance comparisons with satz1
pursuing this line of experimentation  we now offer performance comparison results on formulae from 1 to 1 variables with the satz1 solver  which performs best of the 1 against which cnfs was measured up to now. we first solved samples of 1 formulae from 1 to 1 variables in increments of 1.
figure 1 shows the mean computation time curves  plot-

size of hard random unsatisfiable problem  in #variables 
figure 1: mean computation time of satz1 & cnfs on 1-sat hard random unsatisfiable formulae from 1 to 1 variables ted in the same way as for the figure 1  of satz1 and cnfs for the unsatisfiable formulae within the solved samples. indeed  the most significant performance figure for a complete algorithm is on unsatisfiable formulae. the gain previously observed between 1 and 1 variables is seen further to increase from 1 to 1. cnfs is 1 times faster than satz1 for 1 variables  and 1 times faster for 1.
sat
solversunsat  n 
& 1 v 1 c  1 n   1 y  1 v 1 c  1 n   1 y  1 v 1 c  1 n   1 y sat  y mean time
 std dev. mean time
 std dev. mean time
 std dev. satounsat1s  1s 1h 1m  1h -sat1s  1s 1h  1h 1m -rel satunsat1s  1s 1s  1s 1h 1m  1h 1m sat1s  1s 1s  1s 1h 1m  1h 1m ntabunsat1s  1s 1s  1s 1m 1s  1m 1s sat1s  1s 1s  1s 1m 1s  1m 1s csatunsat1s  1s 1s  1s 1m 1s  1m 1s sat1s  1s 1s  1s 1m 1s  1m 1s positunsat1s  1s 1s  1s 1m 1s  1m 1s sat1s  1s 1s  1s 1m 1s  1m 1s satz1unsat1s  1s 1s  1s 1m 1s  1m 1s sat1s  1s 1s  1s 1m 1s  1m 1s cnfsunsat1s  1s 1s  1s 1m 1s  1m 1s sat1s  1s 1s  1s 1m 1s  1m 1s table 1: mean computation time on hard random 1-sat formulae from 1 to 1 variables for sato  relsat  tableau  csat  posit  satz1 & cnfs solvers
table 1 contains precise compared mean values of tree size
#vars
#clausesunsat  n 
sat  y mean #nodes in
millions  std dev. mean time  std dev. cnfssatz1cnfssatz1 vn1  1 1  1 1m 1s  1s 1m 1s  1s 1 cy1  1 1  1 1m 1s  1s 1m 1s  1s 1 vn1  1 1  1 1h 1m  1h 1h 1m  1h 1 cy1  1 1  1 1h 1m  1h 1h 1m  1h table 1: average size of tree and mean computation time on hard random 1-sat formulae from 1 and 1 variables and computation time on formulae with 1 and 1 variables  listing the satisfiable and unsatisfiable cases separately. tree size gain increase of cnfs versus satz1 may be noticed not to carry over entirely into computation time gain increase. indeed  the gain factor for cnfs on unsatisfiable formulae goes from 1 for 1 variables to 1 for 1. this probably reflects the high cost of the backbone-variable search heuristic. one can therefore hope that technical improvements of the latter will lead to yet greater computation time performance gains. finally  figure 1 sums up  on the whole range 1 to 1 variables  the evolution of the gain ratio of cnfs vs satz1 in computation time and tree size. these curves clearly demonstrate a complexity function gain of cnfs over sat1 on random 1-sat formulae.
solving hard 1-sat formulae up to 1 variables
for the reader's information  table 1 shows how cnfs performs in computation time and number of nodes on two samples of 1 formulae with 1 and 1 variables  respectively. formulae with 1 variables  regarded as quite large for complete solving methods  selman et al.  1  are now within reach  and this in approximately 1 machine hours on a single-processor 'domestic' pc. let us also indicate that for random formulae with 1 variables  irrespective of their satisfiability  the mean solving time with cnfs is about 1 hours.

size of hard random unsatisfiable problem  in #variables 
figure 1: progression of and on unsatisfiable formulae from 1 to 1 variables
#varsunsat  n mean #nodes in#clausessat  y millions  std dev. mean time  std dev. 1 vn1  1 1 days 1h  1h 1 cy1  1 1 day 1h  1h 1 vn1  1 1 days 1h  1h 1 cy1  1 1 days 1h  1h table 1: mean computation time and average size of tree of cnfs on large hard 1-sat formulaes up to 1 variables
1	conclusion
in the course of the last decade  algorithms based on the dpl procedure for solving propositional formulae have seen a performance improvement that was quite real  if much less spectacular than in the case of stochastic algorithms. it was recently suggested that the performance of dpl-type algorithms might be close to their limitations  giving rise to a fear that subsequent progress might be very difficult to achieve and that large unsatisfiable formulae  e.g.  1 variables  might remain beyond reach. we have presented a dpl-type algorithm incorporating mainly a new and simple heuristic using the backbone concept recently introduced from models of physics. this concept has changed the viewpoint from which classical heuristics were developed. we were thus able to improve the current best solving performance for hard 1sat formulae by a ratio increasing with formula size  equal to 1 for 1 variables   and we have shown that solving unsatisfiable formulae with 1 variables was feasible. an important lesson can be drawn from our results. in order to improve the performance of dpl-type algorithms significantly and to enhance the state of the art in complete solving  it appears that a deep understanding of the structure of the solutions of a sat formula is paramount. this is why experimental  as well as theoretical studies aiming to further such comprehension are essential.
references
 bayardo and schrag  1  r. bayardo and r. schrag. using csp look-back techniques to solve real-world sat instances. in proc. of the 1th nat. conf. on artificial intelligence  pages 1. aaai  1.
 bolloba뫣s et al.  1  b. bolloba뫣s  c. borgs  j. t. chayes  j. h. kim  and d. b. wilson. the scaling window of the 1sat transition. random structures and algorithms  1.
 boufkhad and dubois  1  y. boufkhad and o. dubois. length of prime implicants and number of solutions of random formulae. theoretical computer science  1  1 :1  1.
 crawford and auton  1  j. m. crawford and l. d. auton. experimental results on the crossover point in satisfiability problems. in proc. of the 1th nat. conf. on artificial intelligence  pages 1. aaai  1.
 davis et al.  1  m. davis  g. logemann  and d. loveland. a machine program for theorem-proving. comm. assoc. for comput. mach.   1 :1  1.
 dubois et al.  1  o. dubois  p. andre  y. boufkhad  and j. carlier. sat versus unsat. dimacs series in discrete math. and theoret. comp. sc.  pages 1  1.
 freeman  1  j. w. freeman. hard random 1-sat problems and the davis-putnam procedure. artificial intelligence  1-1 :1  1.
 garey and johnson  1  m. r. garey and d. s. johnson. computers and intractability / a guide to the theory of np-completeness. w.h. freeman & company  san francisco  1.
 gent and walsh  1  i. p. gent and t. walsh. the sat phase transition. in proc. of the 1th european conf. on artificial intelligence  pages 1. ecai  1.
 li and gerard  1  c. m. li and s. gerard. on the limit of branching rules for hard random unsatisfiable 1-sat. in proc. of european conf. on artificial intelligence  pages 1. ecai  1.
 li  1  c. m. li. a constraint-based approach to narrow search trees for satisfiability. information processing letters  1 :1  1.
 mitchell et al.  1  d. mitchell  b. selman  and h. j. levesque. hard and easy distribution of sat problems. in proc. 1th nat. conf. on artificial intelligence. aaai  1.
 monasson et al.  1  r. monasson  r. zecchina  s. kirkpatrick  b. selman  and l. troyansky. determining computational complexity from characteristic 'phase transitions'. nature 1  pages 1  1.
 selman et al.  1  b. selman  h. kautz  and d. mcallester. ten challenges in propositional reasoning and search. in proc. of ijcai-1  1.
 zhang  1  h. zhang. sato. an efficient propositional prover. in proc. of the 1th internat. conf. on automated deduction  pages 1. cade-1  lncs  1.
backbones in optimization and approximation
john slaney
automated reasoning project
australian national university
canberra
australia
jks arp.anu.edu.autoby walsh
department of computer science
university of york
york
england tw cs.york.ac.ukabstract
we study the impact of backbones in optimization and approximation problems. we show that some optimization problems like graph coloring resemble decision problems  with problem hardness positively correlated with backbone size. for other optimization problems like blocks world planning and traveling salesperson problems  problem hardness is weakly and negatively correlated with backbone size  while the cost of finding optimal and approximate solutions is positively correlated with backbone size. a third class of optimization problems like number partitioning have regions of both types of behavior. we find that to observe the impact of backbone size on problem hardness  it is necessary to eliminate some symmetries  perform trivial reductions and factor out the effective problem size.
1	introduction
what makes a problem hard  recent research has correlated problem hardness with rapid transitions in the solubility of decision problems  cheeseman et al.  1; mitchell et al.  1 . the picture is  however  much less clear for optimization and approximation problems. computational complexity provides a wealth of  largely negative  worst-case results for decision  optimization and approximation. empirical studies like those carried out here add important detail to such theory. one interesting notion  borrowed from statistical physics  is that of the backbone. a percolation lattice  which can be used as a model of fluid flow or forest fires  undergoes a rapid transition in the cluster size at a critical threshold in connectivity. the backbone of such a lattice consists of those lattice points that will transport fluid from one side to the other if a pressure gradient applied. the backbone is therefore the whole cluster minus any dead ends. the size and structure of this backbone has a significant impact on the properties of the lattice. in decision problems like propositional satisfiability  an analogous notion of  backbone  variables has been introduced and shown to influence problem hardness  monasson et al.  1 . here  we extend this notion to optimization and approximation and study its impact on the cost of finding optimal and approximate solutions.
1	backbones
in the satisfiability  sat  decision problem  the backbone of a formula is the set of literals which are true in every model  monasson et al.  1 . the size of the backbone and its fragility to change have been correlated with the hardness of sat decision problems  parkes  1; monasson et al.  1; singer et al.  1a; achlioptas et al.  1 . a variable in the backbone is one to which it is possible to assign a value which is absolutely wrong - such that no solution can result no matter what is done with the other variables. a large backbone therefore means many opportunities to make mistakes and to waste time searching empty subspaces before correcting the bad assignments. put another way  problems with large backbones have solutions which are clustered  making them hard to find both for local search methods like gsat and walksat and for systematic ones like davis-putnam.
모the notion of backbone has been generalized to the decision problem of coloring a graph with a fixed number of colors   culberson and gent  1 . as we can always permute the colors  a pair of nodes in a -colorable graph is defined to be frozen iff each has the same color in every possible -coloring. no edge can occur between a frozen pair. the backbone is then simply the set of frozen pairs.
모to generalize the idea of a backbone to optimization problems  we consider a general framework of assigning values to variables. the backbone is defined to be the set of frozen decisions: those with fixed outcomes for all optimal solutions. in some cases   decision  just amounts to  assignment : for example  in max-sat  the backbone is simply the set of assignments of values to variables which are the same in every possible optimal solution. in general  however  the relevant notion of decision is obtained by abstraction over isomorphism classes of assignments. in graph coloring  for example  the decision to color two nodes the same is a candidate for being in the backbone whereas the actual assignment of  blue  to them is not because a trivial permutation of colors could assign  red  instead.
1	graph coloring
we first consider the optimization problem of finding the minimal number of colors needed to color a graph. a pair of nodes in a graph coloring optimization problem is frozen iff each has the same color in every possible optimal coloring. no edge can occur between a frozen pair without increasing the chromatic number of the graph. the backbone is again the set of frozen pairs. in a graph of nodes and edges  we normalize the size of the backbone by   the maximum possible backbone size. as with graph coloring decision problems  culberson and gent  1   we investigate the  frozen development  by taking a random list of edges and adding them to the graph one by one  measuring the backbone size of the resulting graph. we study the frozen development in single instances since  as with graph coloring decision problems  culberson and gent  1   averaging out over an ensemble of graphs obscures the very rapid changes in backbone size.

1
1 1 1 1 1 e/n
figure 1: frozen development in a single 1 node graph. backbone size  y-axis  is plotted against . the number of edges is varied from to in steps of 1. backbone size is normalized by its maximum value. the chromatic number  which increases from 1 to 1  is plotted on the same axes.

1
1 1 1 1 1 e/n
figure 1: cost to color graph optimally for the same graphs as figure 1. nodes visited  y-axis  is plotted against .
in figure 1  we plot the frozen development for a typical
1 node graph. just before the chromatic number increases  there are distinct peaks in backbone size. when the chromatic number increases  the backbone size immediately collapses. in figure 1  we plot the search cost to find the optimal coloring for the same 1 node graph. to find optimal colorings  we use an algorithm due to mike trick which is based upon brelaz's dsatur algorithm  brelaz  1 . search cost peaks with the increases in chromatic number and the peaks in the backbone size. optimization here closely resembles decision since it is usually not hard to prove that a coloring is optimal. there is thus a strong correlation between backbone size and both optimization and decision cost.
1	traveling salesperson problem
we next consider the traveling salesperson problem. a leg in a traveling salesperson  tsp  optimization problem is frozen iff it occurs in every possible optimal tour. the tsp backbone is simply the set of frozen legs. we say that the tsp backbone is complete iff it is of size . in such a situation  the optimal tour is unique. note that it is impossible to have a backbone of size . it is  however  possible to have a backbone of any size or less. computing the tsp backbone highlights a connection with sensitivity analysis. a leg occurs in the backbone iff adding some distance  to the corresponding entry in the inter-city distance matrix increases the length of the optimal tour. a tsp problem with a large backbone is therefore more sensitive to the values in its inter-city distance matrix than a tsp problem with a small backbone.
모to explore the development of the backbone in tsp optimization problems  we generated 1-d integer euclidean problems with cities randomly placed in a square of length . we varied from 1 to 1  generating 1 problems at each integer value of   and found the backbone and the optimal tour using a branch and bound algorithm based on the hungarian heuristic. the cost of computing the backbone limited the experiment to . the backbone quickly becomes complete as is increased. figure 1 is a scatter plot of backbone size against the cost to find and prove the tour optimal.
모the pearson correlation coefficient  between the normalized backbone size and the log of the number of nodes visited to find and prove the tour optimal is just -1. this suggests that there is a slight negative correlation between backbone size and tsp optimization cost. we took the log of the number of nodes visited as it varies over 1 orders of magnitude. this conclusion is supported by the spearman rank correlation coefficient  which is a distribution free test for determining whether there is a monotonic relation between two variables. the data has a spearman rank correlation of just -1.
모to explore the difference between optimization and decision cost  in figure 1 we plot the  decision  cost for finding the optimal tour. the pearson correlation coefficient  between the normalized backbone size and the log of the number of nodes visited to find the optimal tour is 1. this suggests that there is a positive correlation between backbone size and tsp decision cost. this conclusion is supported by the spearman rank correlation coefficient  which is 1.

1.1.1.1.1 normalized backbone size
figure 1: cost to find and prove the tour optimal  y-axis  logscale  against normalized backbone size  x-axis  for 1-d integer euclidean tsp problems with cities placed on a square of length . nodes visited by a branch and bound algorithm  y-axis  logscale  is plotted against normalized backbone size  x-axis . 1 random problems are generated at each integer value of from 1 to 1. the straight line gives the least squares fit to the data.
모tsp is unlike graph coloring in that optimization appears significantly different from decision. we conjecture this is a result of there usually being no easy proofs of tour optimality. indeed  the cost of proving tours optimal is negatively correlated with backbone size. this roughly cancels out the positive correlation between the  decision  cost of finding the optimal tour and backbone size. but why does the cost of proving tours optimal negatively correlated with the backbone size  if we have a small backbone  then there are many optimal and near-optimal tours. an algorithm like branch and bound will therefore have to explore many parts of the search space before we are sure that none of the tours is any smaller.
1	number partitioning
we have seen that whether optimization problems resemble decision problems appears to depend on whether there are cheap proofs of optimality. number partitioning provides a domain with regions where proofs of optimality are cheap  and there is a positive correlation between optimization cost and backbone size   and regions where proofs of optimality are typically not cheap  and there is a weak negative correlation between optimization cost and backbone size .
모one difficulty in defining the backbone of a number partitioning problem is that different partitioning algorithms make different types of decisions. for example  korf's ckk algorithm decides whether a pair of numbers go in the same bin as each other or in opposite bins  korf  1 . one definition of backbone is thus those pairs of numbers that cannot be placed in the same bin or that cannot be placed in opposite bins. by comparison  the traditional greedy algorithm for number partitioning decides into which bin to place each number. an-

1.1.1.1.1 normalized backbone size
figure 1: cost to find optimal tour  y-axis  logscale  against normalized backbone size  x-axis  for the 1 city problems from figure 1. the straight line again gives the least squares fit to the data.
other definition of backbone is thus those numbers that must be placed in a particular bin. we can break a symmetry by irrevocably placing the largest number in the first bin. fortunately  the choice of definition does not appear to be critical as we observe very similar behavior in normalized backbone size using either definition. in what follows  we therefore use just the second definition.

figure 1: frozen development averaged over 1 problems. average backbone size  y-axis  against  x-axis .
problems contain random numbers in the interval   and is varied from to in steps of 1. backbone size is
normalized by its maximum value.
모in figure 1  we plot the frozen development averaged over 1 problems. the frozen development in a single problem is very similar. as in  gent and walsh  1   we partition random numbers uniformly distributed in . we generate 1 problem instances at each and   and then prune numbers to the first bits using mod arithmetic. the size of the optimal partition is therefore monotonically increasing with . we see characteristic phase transition behaviour in the backbone size. there is a very sharp increase in backbone size in the region where even the best heuristics like kk fail to find backtrack free solutions. by the decision phase boundary at
 gent and walsh  1   the backbone tends to be complete and the optimal solution is therefore unique. this rapid transition in average backbone size should be compared to graph coloring where  culberson and gent  1  typically had to look at single instances to see large jumps in backbone size.

1.1.1.1.1 normalized backbone size
figure 1: optimization cost  y-axis  logscale  against normalized backbone size  x-axis  for the number partitioning problems from figure 1. the straight line gives the least squares fit to those problems whose backbone is neither complete nor empty.
모in figure 1  we give a scatter plot for the optimization cost for korf's ckk algorithm against backbone size. the data falls into two regions. in the first  optimization problems have backbones less than 1% complete. optimization in this region is similar to decision as proofs of optimality are typically easy  and optimization cost is positively correlated with backbone size. data from this region with non-empty backbones has a pearson correlation coefficient  of 1  and a spearman rank correlation coefficient  of 1. in the second region  optimization problems have complete backbones. the cost of proving optimality is now typically greater than the cost of finding the optimal solution. due to the rapid transition in backbone size witnessed in figure 1  we observed no problems with backbones between 1% and 1% complete.
1	blocks world planning
our fourth example taken from the field of planning raises interesting issues about the definition of a backbone. it also highlights the importance of considering the  effective  problem size and of eliminating trivial aspects of a problem.
모we might consider a solution to a blocks world planning problem to be the plan and the backbone to be those moves present in all optimal  minimal length  plans. however  since most moves simply put blocks into their goal positions and are therefore trivially present in all plans  almost all of a plan is backbone. a more informative definition results from considering  deadlocks . a deadlock is a cycle of blocks  each of which has to be moved before its successor can be put into the goal position. each deadlock has to be broken  usually by putting one of the blocks on the table. once the set of deadlock-breaking moves has been decided  generating the plan is an easy  linear time  problem  slaney and thie뫣baux  1 . a better definition of solution then is the set of deadlock-breaking moves. however  this is not ideal as many deadlocks contain only one block. these singleton deadlocks give forced moves which inflate the backbone  yet are detectable in low-order polynomial time and can quickly be removed from consideration. we therefore define a solution as the set of deadlock-breaking moves in a plan  excluding those which break singleton deadlocks. the backbone is the set of such moves that are in every optimal solution.
모we considered uniformly distributed random blocks world problems of 1 blocks  with both initial and goal states completely specified. to obtain optimal solutions  we used the domain-specific solver reported in  slaney and thie뫣baux  1  and measured hardness as the number of branches in the search tree. as in  slaney and thie뫣baux  1   we observed a cost peak as the number of towers in the initial and goal states reaches a critical value of 1 towers. we therefore plotted backbone size against the number of towers  and found that this peaks around the same point  see figure 1 .

figure 1: mean backbone size for 1 block optimization problems against number of towers in initial and goal state.
모although problem hardness is in a sense correlated with backbone size  this result must be interpreted carefully because solution size also peaks at the same point. with more than about 1 towers  few deadlocks exist so solution size  as we measure it  is small. with a smaller number of towers  the problem instance is dominated by singleton deadlocks  so again the solution is small. the size of the backbone as a proportion of the solution size shows almost no dependence on the number of towers.
모another important feature of the blocks world is that the number of blocks in an instance is only a crude measure of problem  size . at the heart of a blocks world planning problem is the sub-problem of generating a hitting set for a collection of deadlocks. the effective size of an instance is therefore the number of blocks that have to be considered for inclusion in this hitting set. this effective size dominates the solution cost  overshadowing any effect of backbone size. in our next experiment  therefore  we filtered out all but those instances of effective size 1. we obtain similar results restricting to other sizes. of 1 random problems  1 were of effective size 1. for each of those  we measured the hardness of solving two decision problems: whether there exists a plan of length  the optimal plan length   and whether there exists a plan of length . these can be regarded as measuring the cost of finding an optimal solution and of proving optimality respectively.

1
1.1.1.1.1 normalized backbone size
figure 1: cost of finding an optimal solution     and of proving optimality     against backbone size as a proportion of solution size  for 1 block problems of effective size 1. the straight lines give the least squares fits to the data.
모figure 1 shows the results are similar to tsp problems. finding an optimal solution tends to be harder if the backbone is larger  for the familiar reason that if solutions are clustered  most of the search space is empty. this data has a pearson correlation coefficient  of 1 and a spearman rank correlation coefficient  of 1. proving optimality  on the other hand  tends to be slightly easier with a larger backbone. this data has and .
1	approximation and -backbones
our definition of backbone ignores those solutions which are close to optimal. in many real-world situations  we are willing to accept an approximate solution that is close to optimal. we therefore introduce the notion of the -backbone: the set of frozen decisions in all solutions within a factor of optimal. for   this gives the previous definition of backbone. for   the -backbone is by definition empty. for example  the tsp -backbone consists of those legs which occur in all tours of length less than or equal to .
모in figure 1  we give a scatter plot of the size of the backbone for number partitioning problems against the cost of finding an approximate solution within a factor 1 of optimal. similar plots are seen for other values of . as with
       the data falls into two regions. in the first  problems have 1-backbones less than 1% complete. the cost of approximation in this region is positively correlated with backbone size. however  the correlation is less strong than that between backbone size and optimization cost. data from this region with non-empty backbones has a pearson correlation coefficient  of 1  and a spearman rank correlation coefficient  of 1. in the second region  problems have complete 1-backbones. the cost of finding an approximate solutions in this region is now typically as hard as that for the hardest problems with incomplete 1-backbones.

1.1.1.1.1 normalized 1-backbone size
figure 1: cost of finding approximate solution within factor 1 of optimal  y-axis  logscale  against normalized backbone size  x-axis  for the number partitioning problems from figure 1. the straight line gives the least squares fit to those problems whose backbone is neither complete nor empty.
1	related work
first moment methods can be used to show that  at the satisfiability phase transition  the expected number of solutions for a problem is exponentially large. kamath et al. proved that  whilst most of these problems have few or no solutions  a few have a very large number of clustered solutions  kamath et al.  1 . this was verified empirically by parkes who showed that many variables are frozen although some are almost free  parkes  1 . he argued that such problems are hard for local search algorithms to solve as solutions are clustered and not distributed uniformly over the search space.
모monasson et al. introduced the 1+ -sat problem class to study computational complexity in np-complete decision problems  monasson et al.  1 . for   random 1+ -sat behaves like the polynomial random 1-sat problem  whilst for   random 1+ -sat behaves like the np-complete random 1-sat problem  monasson et al.  1; singer et al.  1b . the rapid change in backbone size is continuous  second order  for   and discontinuous  first order  for . this transition may explain the onset of problem hardness and could be exploited in search.
모backbones have also been studied in tsp  approximation  problems  kirkpatrick and toulouse  1; boese  1 . for example  boese shows that optimal and near-optimal tours for the well known att 1-city problem tended are highly clustered  boese  1 . heuristic optimization methods for the tsp problem have been developed to identify and eliminate such backbones  schneider et al.  1 .
모a related notion to the backbone in satisfiability is the spine  bollobas et al.  1 . a literal is in the spine of a set of clauses iff there is a satisfiable subset in all of whose models the literal is false. for satisfiable problems  the definitions of backbone and spine coincide. unlike the backbone  the spine is monotone as adding clauses only ever enlarges it.
1	conclusions
we have studied backbones in optimization and approximation problems. we have shown that some optimization problems like graph coloring resemble decision problems  with problem hardness positively correlated with backbone size and proofs of optimality that are usually easy. with other optimization problems like blocks world and tsp problems  problem hardness is weakly and negatively correlated with backbone size  and proofs of optimality that are usually very hard. the cost of finding optimal and approximate solutions tends  however  to be positively correlated with backbone size. a third class of optimization problem like number partitioning have regions of both types of behavior.
모what general lessons can be learnt from this study  first  backbones are often an important indicator of hardness in optimization and approximation as well as in decision problems. second   heuristic  methods for identifying backbone variables may reduce problem difficulty. methods like randomization and rapid restarts  gomes et al.  1  may also be effective on problems with large backbones. third  it is essential to eliminate trivial aspects of a problem  like symmetries and decisions which are trivially forced  before considering its hardness. finally  this and other studies have shown that there exist an number of useful parallels between computation and statistical physics. it may therefore pay to map over other ideas from areas like spin glasses and percolation.
acknowledgements
the second author is an epsrc advanced research fellow. we wish to thank the members of the apes research group.
references
 achlioptas et al.  1  d. achlioptas  c. gomes  h. kautz  and b. selman. generating satisfiable problem instances. in proc. of 1th nat. conf. on ai. 1.
 boese  1  k.d. boese. cost versus distance in the traveling salesman problem. technical report csd-1  ucla computer science department  1.
 bollobas et al.  1  b. bollobas  c. borgs  j. chayes  j.h. kim  and d.b. wilson. the scaling window of the 1-sat transition. random structures and algorithms 	1.	to appear. available from dimacs.rutgers.edu/ dbwilson/1sat/.
 brelaz  1  d. brelaz. new methods to color the vertices of a graph. communications of acm  1-1  1.
 cheeseman et al.  1  p. cheeseman  b. kanefsky  and w.m. taylor. where the really hard problems are. in proc. of the 1th ijcai  pages 1. 1.
 culberson and gent  1  j. culberson and i.p. gent. frozen development in graph coloring. theoretical computer science  1. to appear. available from www.apes.cs.strath.ac.uk/apesreports.html.
 gent and walsh  1  i.p. gent and t. walsh. analysis of heuristics for number partitioning. computational intelligence  1 :1  1.
 gomes et al.  1  c. gomes  b. selman  k. mcaloon  and c. tretkoff. randomization in backtrack search: exploiting heavy-tailed profiles for solving hard scheduling problems. in the 1th int. conf. on artificial intelligence planning systems  aips'1   1.
 kamath et al.  1  a. kamath  r. motwani  k. palem  and p. spirakis. tail bounds for occupancy and the satisfiability threshold conjecture. randomized structure and algorithms  1-1  1.
 kirkpatrick and toulouse  1  s. kirkpatrick and g. toulouse. configuration space analysis of the traveling salesman problem. j. de physique  1-1  1.
 korf  1  r. korf. from approximate to optimal solutions: a case study of number partitioning. in proc. of the 1th ijcai. 1.
 mitchell et al.  1  d. mitchell  b. selman  and h. levesque. hard and easy distributions of sat problems. in proc. of the 1th nat. conf. on ai  pages 1. 1.
 monasson et al.  1  r. monasson  r. zecchina  s. kirkpatrick  b. selman  and l. troyansky. determining computational complexity for characteristic 'phase transitions'. nature  1-1  1.
 parkes  1  a. parkes. clustering at the phase transition. in proc. of the 1th nat. conf. on ai  pages 1. 1.
 schneider et al.  1  j. schneider  c. froschhammer  i. morgernstern  t. husslein  and j.m. singer. searching for backbones - an efficient parallel algorithm for the traveling salesman problem. comput. phys. commun.  1-1  1.
 singer et al.  1a  j. singer  i.p. gent  and a. smaill. backbone fragility and the local search cost peak. journal of artificial intelligence research  1-1  1.
 singer et al.  1b  j. singer  i.p. gent  and a. smaill. local search on random 1+p-sat. in proc. of the 1th ecai. 1.
 slaney and thie뫣baux  1  j. slaney and s. thie뫣baux. linear time near-optimal planning in the blocks world. in proc. of 1th nat. conf. on ai  pages 1. 1.
 slaney and thie뫣baux  1  j. slaney and s. thie뫣baux. on the hardness of decision and optimisation problems. in proc. of the 1th ecai  pages 1. 1.
cooperative search and nogood recording
cyril terrioux
lsis - equipe inca  lim  1  rue joliot-curie
f-1 marseille cedex 1  france  e-mail: terrioux lim.univ-mrs.frabstract
within the framework of constraint satisfaction problem  we propose a new scheme of cooperative parallel search. the cooperation is realized by exchanging nogoods  instantiations which can't be extended to a solution . we associate a process with each solver and we introduce a manager of nogoods  in order to regulate exchanges of nogoods. each solver runs the algorithm forward-checking with nogood recording. we add to algorithm a phase of interpretation  which limits the size of the search tree according to the received nogoods. solvers differ from each other in ordering variables and/or values by using different heuristics. the interest of our approach is shown experimentally. in particular  we obtain linear or superlinear speed-up for consistent problems  like for inconsistent ones  up to about ten solvers.
1	introduction
in constraint satisfaction problems  one of main tasks consists in determining whether there exists a solution  i.e. an instantiation of all variables which satisfies all constraints. this task is a np-complete problem. in order to speed up the resolution of problems  parallel searches are used. a basic one is independent parallel search which consists in running several solvers  each one using a different heuristic  instead of a single solver. the aim is that at least one of the solvers gets an heuristic suitable for the problem which we solve. tested on the graph coloring problem   hogg and williams  1    this approach has better results than a classical resolution with a single solver  but the gains seem limited. hogg and williams recommend then the use of a cooperative parallel search. a cooperative parallel search is based on the same ideas as the independent search with in addition an exchange of informations between solvers  in order to guide solvers to a solution  and then  to speed up the resolution. experimental results on cryptarithmetic problems   clearwater et al.  1; hogg and huberman  1   and on graph coloring problem   hogg and huberman  1; hogg and williams  1   show a significant gain in time with respect to an independent search. in both cases  the exchanged informations correspond to partial consistent instantiations.
in  martinez and verfaillie  1   a cooperation based on exchanging nogoods  i.e. instantiations which can't be extended to a solution  is proposed. exchanged nogoods permit solvers to prune their own search tree. so  one can expect to find more quickly a solution. solvers run the algorithm forward checking with nogood recording  noted fc-nr  schiex and verfaillie  1  . the realized implementation gathers all solvers in a single process which simulates the parallelism. it is turned to a monoprocessor system. experimentations on random csps show that cooperative search is better than independent one. however  the weak gain with report to a single solver gives a doubt about efficiency of a such search.
모from the idea of martinez and verfaillie  we define a new scheme of cooperation with exchange of nogoods  turned to systems with one or several processors. we associate a process with each solver. each solver runs fc-nr. in order to avoid problems raised by the cost of communications  we introduce a manager of nogoods whose role is to regulate exchange of nogoods. in addition to sending and receipt of messages  we add a phase of interpretation to fc-nr  in order to limit the size of the search tree according to received nogoods.
our second main aim is to answer an open question   martinez and verfaillie  1   about efficiency of a cooperative parallel search with exchange of nogoods. we show experimentally the interest of our approach.
모the plan is as follows. in section 1  we give basic notions about csps  nogoods and fc-nr. then  in section 1  we present our scheme by describing the manager of nogoods and the phase of interpretation. finally  after providing experimental results in section 1  we conclude in section 1.
1	definitions
1	definitions about csps
a constraint satisfaction problem  csp  is defined by a quadruplet . is a set of vari-
ables. each variable takes its values in the domain from . variables are subject to constraints from . each constraint involves a set of variables.
a relation  from   is associated with each constraint such that represents the set of allowed -uplets over
.
a csp is called binary if each constraint involves two variables. let and be two variables  we note the corresponding constraint. afterwards  we consider only binary csps. however  our ideas can be extended to n-ary csps.
모given such that   an instantiation of variables from is a -uplet from
. it is called consistent if
	  inconsistent otherwise.	we
use indifferently the term assignment instead of instantiation. we note the instantiation in the more meaningful form . a solution is a consistent instantiation of all variables. given an instance
                 determine whether has a solution is a np-complete problem.
	given a csp	and an instantiation
모모모모모모모모모모  is the csp induced by	from	with
a forward checking filter such that:
-
-
-	. is said fc-consistent if .
1	nogoods: definitions and properties
in this part  we give the main definitions and properties about nogoods and fc-nr   schiex and verfaillie  1  .
a nogood corresponds to an assignment which can't be extended to a solution. more formally   schiex and verfaillie  1    given an instantiation and a subset of constraints
     is a nogood if the csp doesn't have any solution which contains . is called the nogood's justification  we note the variables subject to constraints from  . the arity of the nogood is the number of assigned variables in . we note the restriction of to variables which are both in and in .
for instance  every inconsistent assignment corresponds to a nogood. the converse doesn't hold.
모to calculate justifications of nogoods  we use the notion of  value-killer   introduced in  schiex and verfaillie  1   and we extend it in order to exploit it in our scheme. given an assignment   the csp induced by   and the set of nogoods found by other solvers  a constraint
    is a value-killer of value from for if one of the following conditions holds:
1. is a value-killer of	for
1. and	and
1.
if a unique solver is used   which corresponds to the definition presented in  schiex and verfaillie  1  .
모assume that an inconsistency is detected because a domain becomes empty. the reasons of failure  i.e. justifications  correspond to the union of value-killers of	. the following theorem formalizes the creation of nogoods from dead-ends.
theorem 1 let be an assignment and be an unassigned variable. let be the set of value-killers of . if it doesn't remain any value in   then is a nogood.
the two next theorems make it possible to create new nogoods from existing nogoods. the first theorem builds a new nogood from a single existing nogood.
theorem 1  projection  schiex and verfaillie  1  
if	is a nogood  then	is a nogood.
in other words  we keep from instantiation the variables which are involved in the inconsistency. thus  we produce a new nogood whose arity is limited to its strict minimum. theorem 1  we build a new nogood from a set of nogoods:
theorem 1 let be an instantiation  be an unassigned variable. let be the set of value-killers of . let be the extension of by assigning the value to  
	 . if	  ... 	are nogoods  then
is a nogood.
모a nogood can be used either to backjump or to add a new constraint or to tighten an existing constraint. in both cases  it follows from the use of nogoods a pruning of the search tree. fc-nr explores the search tree like forward checking  during the search  it takes advantage of dead-ends to create and record nogoods. these nogoods are then used as described above to prune the search tree. the main drawback of fc-nr is that the number of nogoods is potentially exponential. so  we limit the number of nogoods by recording nogoods whose arity is at most 1  i.e. unary or binary nogoods   according to the proposition of schiex and verfaillie   schiex and verfaillie  1  . nevertheless  the ideas we present can be easily extended to n-ary nogoods.
1	description of our multiple solver
our multiple solver consists of sequential solvers which run independently fc-nr on the same csp. each solver differs from another one in ordering variables and/or values with different heuristics. thus  each one has a different search tree. the cooperation consists in exchanging nogoods. a solver can use nogoods produced by other solvers in order to prune a part of its search tree  which should speed up the resolution. during the search  solvers produce nogoods which are communicated to other solvers. therefore  when a solver finds a nogood  it must send messages to inform its partners. although the number of nogoods is bounded  the cost of communications can become very important  prohibitive even. so  we add a process called  manager of nogoods   whose role is to inform solvers of the existence of nogoods. accordingly  when a solver finds a nogood  it informs the manager  which communicates at once this new information to a part of other solvers. in this way  a solver sends only one message and gets back more quickly to the resolution of the problem.
the next paragraph is devoted to the role and the contribution of manager in our scheme.
1	the manager of nogoods
role of the manager
the manager's task is to update the base of nogoods and to communicate new nogoods to solvers. update the base of nogoods consists in adding constraints to initial problem or in tightening the existing constraints. to a unary  respectively binary  nogood corresponds a unary  resp. binary  constraint. each nogood communicated to manager is added to the base.
모in order to limit the cost of communications  the manager must inform only solvers for which nogoods may be useful. a nogood is said useful for a solver if it allows this solver to limit the size of its search tree.
the next theorem characterizes the usefulness of a nogood according to its arity and the current instantiation.
theorem 1  characterization of the usefulness 
 a  a unary nogood is always useful 
 b  a binary nogood is useful if  in the current instantiation  and are assigned respectively to and  
 c  a binary nogood is useful if  in the current instantiation   resp.   is assigned to  resp.     resp.   isn't assigned and  resp.   isn't removed yet.
proof: see  terrioux  1 .
from this theorem  we explicite what solvers receive some nogoods  according to their usefulness :
 a  every unary nogood is communicated to all solvers  except the solver which finds it  
 b  binary nogood is communicated to each solver  except the solver which finds it  whose instantiation contains or .
in case  b   we can't certify that the nogood is useful  because the solver may have backtracked between the sending of the nogood by the manager and its receipt by the solver. with a view to limit the cost of communications  only the instantiation of nogood is conveyed. communicate the justification isn't necessary because this nogood is added to the problem in the form of a constraint . thanks to received information  solvers can forbid with justification .
contribution of the manager
in this part  we show the contribution of the manager of nogoods to our scheme with respect to a version without manager. the comparison is based on the total number of messages which are exchanged during all search. let be the total number of nogoods which are exchanged by all solvers. we count unary nogoods and binary ones. note that  among these nogoods  doubles may exist. in effect  two solvers can find independently a same nogood. in a scheme without manager  each solver communicates the nogoods it finds to other solvers. so  messages are sent for unary nogoods and for binary ones. but  in a scheme with manager  nogoods are first sent to manager by solvers. during all search  solvers convey to manager messages for unary nogoods and for binary ones. then  the manager sends only unary nogoods to solvers. these nogoods correspond to nogoods minus the doubles. likewise  for binary nogoods  doubles aren't communicated. furthermore  for the remaining binary nogoods  the manager restricts the number of recipients. let be the number of messages sent by manager for binary nogoods. in our scheme  we exchange messages for unary nogoods and messages for binary ones.
모in the worst case  the scheme with manager produces up to additional messages in comparison with the scheme without manager. but  in general  and are little enough so that the scheme with manager produces fewer messages.
1	phase of interpretation
the method which we are going to describe is applied whenever a nogood is received. solvers check whether a message is received after developing a node and before filtering. in the phase of interpretation  solvers analyze received nogoods in order to limit the size of their search tree by stopping branch which can't lead to solution or by enforcing additional filtering. for unary nogoods  this phase corresponds to a permanent deletion of a value and to a possible backjump. method 1 details the phase for such nogoods.
method 1  phase of interpretation for unary nogoods  let be the current instantiation. let be the received nogood.
we delete	from	.
 a  if	is assigned to the value	  then we backjump to	.
if is empty  we record the nogood   with the set of value-killers of .
 b  if	is assigned to	 	   we do nothing.
 c  if	isn't assigned  we check whether	is empty.
if is empty  we record the nogood   with the set of value-killers of .
theorem 1 the method 1 is correct.
proof: see  terrioux  1 .
for binary nogoods  the phase corresponds to enforce an additional filtering and to a possible backjump. method 1 describes the actions done during this phase for binary nogoods.
method 1  phase of interpretation for binary nogoods  let be the current instantiation and be the received nogood.
 a  if and are assigned in to and respectively  then we backjump to the deepest variable among and . if  resp.   is this variable  we delete by filtering  resp.   from  resp.  .
 b  if  resp.   is assigned to  resp.   and  resp.   isn't be assigned  we delete by filtering  resp.   from  resp.  .
if  resp.   becomes empty  we record the nogood with the set of value-killers of  resp.  .
theorem 1 the method 1 is correct.
proof: see  terrioux  1 .
unlike the phase of interpretation for unary nogoods  here  the deletion isn't permanent.
whereas the phase of interpretation is correct  its addition to fc-nr may  in some cases  compromise a basic property of fc-nr. the next paragraph is devoted to this problem.
1	maintening fc-consistency
we remind first a basic property of fc-nr  from fc :
property 1 every instantiation built by fc-nr is fcconsistent.
모after a backtrack to the variable   fc-nr  like fc  cancels the filtering which follows the assignment of . so  it restores each domain in its previous state. in particular  if a domain is wiped out after filtering  it isn't empty after restoring. it ensues the preserve of the property 1.
with the addition of communications and of the phase of interpretation  this property may be compromised. for example  we consider the search tree explored by a solver which cooperates with other ones. let . this solver assignes first to   then to . enforce fc-consistency after assigning removes from . the filtering which follows the assignment of to deletes and from .
the solver assignes to   and then  it visits the corresponding subtree. assume that this subtree contains only failures and that the solver receives unary nogoods which forbid assigning values and to . so the solver backtracks and records a unary nogood which forbids for . it backtracks again  to   and assignes to   which raises a problem  namely is empty  due to permanent removals of   and from by unary nogoods . so  the current instantiation
isn't fc-consistent and the property 1 doesn't hold. the next theorem characterizes the problem:
theorem 1
let	be a fcconsistent instantiation. we consider the exploration by fc-
nr of subtree rooted in . let be the set of values of which remain forbidden by nogoods at the end of the exploration such that these nogoods are recorded or received during this exploration and none of them doesn't involve . if all values of are removed during the exploration  no value is restored in after cancelling the filtering
following the assignment of	to	if and only if
.
proof: see  terrioux  1 .
it ensues that  in some cases  the union of receipts and creations of nogoods with the filtering induces the existence of empty domains  and thus property 1 doesn't hold.
모a solution consists in checking whether a domain is wiped out after cancelling the last filtering and  if there exists such domain  in backtracking until the domain isn't empty. it isn't necessary to check the emptiness of every domain  thanks to the following lemma which determines potential origins of this problem.
lemma 1 only recording or receipt of a nogood may induce the loss of property 1.
proof: see  terrioux  1 .
this lemma enables to check only the emptiness of domains which become empty after recording or receiving a nogood. if emptiness is induced by receiving a nogood  we introduce an additional phase of backjump. this phase follows every detection of empty domain after receiving a nogood and makes it possible to keep on with the search from a fcconsistent instantiation.
method 1  backjump's phase 
if	is empty due to a received nogood  we backjump:
- until isn't empty or until the current instantiation is empty  if the nogood is unary 
- until	isn't empty  if the nogood is binary.
note that we backtrack to empty instantiation only for inconsistent problems.
theorem 1 the method 1 is correct.
proof: see  terrioux  1 .
모if emptiness is induced by recording a nogood  a solution consists in recording only nogoods which don't wipe out a domain. however  we communicate all nogoods to manager. this solution is easy to implement  but it doesn't take advantage of all found nogoods.
1	experimental results
1	experimental protocol
we work on random instances produced by random generator written by d. frost  c. bessie`re  r. dechter and j.-c. re뫣gin. this generator takes 1 parameters     and . it builds a csp of class with variables which have domains of size and binary constraints     in which tuples are forbidden    .
모experimental results we give afterwards concern classes with which varies between 1 and 1. considered classes are near to the satisfiability's threshold which corresponds to . however  for fc-nr  the difficulty's shape is observed for . every problem we consider has a connected graph of constraints.
모given results are the averages of results obtained on 1 problems per class. each problem is solved 1 times in order to reduce the impact of non-determinism of solvers on results. results of a problem are then the averages of results of 1 resolutions. for a given resolution  the results we consider are ones of the solver which solves the problem first.
모experimentations are realized on a linux-based pc with an intel pentium iii 1 mhz processor and 1 mb of memory.
1	heuristics
in order to guarantee distinct search trees  each solver orders variables and/or values with different heuristics. as there exist few efficient heuristics  from an efficient heuristic for choosing variables  we produce several different orders by choosing differently the first variable and then applying . we use the heuristic   bessie`re and re뫣gin  1    for which the next variable to assign is one which minimizes the ratio   where is the current domain of and is the set of variables which are connected to by a binary constraint . this heuristic is considered better  in general  than other classical heuristics. that's why we choose it.
in our implementation  only the size of domains varies for each instantiation. the degree is updated when a new constraint is added thanks to a nogood.
as regards the choice of next value to assign  we consider values in appearance order or in reverse order. in following results  unless otherwise specified   half solvers use the appearance order to order domains  other half reverse order.
# consistent problems1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111	table 1: efficiency  in %  for consistent and inconsistent problems for classes	.1	results
efficiency
in this paragraph  we assess the speed-up and the efficiency of our method. let be the run-time of a single solver for the resolution of a serie of problems and be the run-time for solvers which are run in parallel. we define speed-up as the ratio  and efficiency as the ratio . the speed-up is called linear with report to the number of solvers if it equals to   superlinear if it is greater than   sublinear otherwise.
table 1 presents efficiency  in %  obtained for classes with which varies between 1 and 1. in table 1  up to 1 solvers  we obtain linear or superlinear speed-up for all classes  except 1 classes . above 1 solvers  some classes have linear or superlinear speed-up  but most classes have sublinear speed-up. we note also a decrease of efficiency with the increase of number of solvers.
모according to consistency of problems  we observe a better speed-up for consistent problems  which remains linear or superlinear . we note the same decrease of efficiency with the number of solvers. but  efficiency for inconsistent problems is less important  whereas it is greater than 1 up to 1 solvers. it follows the appearance of sublinear speedup above 1 solvers. this lack of efficiency for inconsistent problems infers a decrease of efficiency for overall problems.
explanations of obtained results
first  we take an interest in explaining observed gains. possible origns are multiple orders of variables and cooperation. we define an independent version of our scheme  i.e. without exchange of nogoods . we compare the two versions by calculating the ratio of the run-time for the independent version over one for cooperativeversion. figure 1 presents results obtained for the class  1 1  with a number of solvers between 1 and 1. we observe similar results for other classes. we note first that cooperative version is always better than independent one. then  we observe that the ratio is near 1 for consistent problems  solid line . that means that the good quality of results  for these problems  results mostly from multiple orders of variables. however  the ratio remains

figure 1: ratio independent search / cooperative search.
greater than 1. so  exchange of nogoods participates in obtained gains too.
finally  for inconsistent problems  dashed line   ratio is more important than for consistent ones. it increases with the number of solvers. in other words  obtained results for these problems are mostly due to cooperation and the contribution of cooperation increases with the number of solvers.
for inconsistent problems  we must underline the predominant role of values heuristics. for each solver  except one if the number of solvers is odd   there exists a solver which uses the same variables heuristic as and the values heuristic which is reverse with report to one of . without exchanging nogoods  these two solvers visit similar search trees. with exchanging nogoods  each one explores only a part of its search tree thanks to received nogoods. it's the same for consistent problems  but this effect is less important because the search stops as soon as a solution is found.
모we focus then on possible reasons of efficiency's decrease. with a scheme like our  an usual reason of efficiency's lack is the importance of cost of communications. our method doesn't make exception. but  in our case  there is another reason which explains the decrease of performances. we compare multiple solvers and . both have 1 solvers. for ordering values  half solvers of use the appearance order  other half the reverse order. all solvers of use the appearance order. we realise that has a better efficiency than . the number of messages for is greater than for . but  above all  it's the same for the number of nodes. so explores more important trees. and differ in used heuristics for ordering values and variables. the heuristics we use for ordering variables are near each other. using two different orders of values adds diversity to resolution. thus  is more various than . this difference of diversity per-
mits to explain the gap of efficiency between and . the lack of diversity is the main reason  with the increase of number of communications  of the efficiency's decrease.
number of messages and real contribution of manager in order to measure the real contribution of manager  we compare the costs of communications in a scheme with manager and one without manager. in presented results  we consider that the cost of a message is independent of presence or not of the manager  and that the communication of a binary nogood is twice as expensive as one of a unary nogood  because a binary nogood consists of two pairs variable-value  against a single pair for a unary nogood . figure 1 presents the ratio of cost of communications for a scheme without manager over one for a scheme with manager. considered problems  consistent and inconsistent  belong to class  1 1 . for information  we observe similar results for other classes.

figure 1: ratio between schemes with and without manager.
모first  we note an increase of manager's contribution with . thus  we can hope that the number of solvers above of which the cost of communications penalizes efficiency is greater in a scheme with manager than one without manager.
모more precisely  we note that communications of unary nogoods is more expensive in the scheme with manager  when is less important. this result was foreseeable. indeed  in case of unary nogoods  the manager removes only doubles. as the probability that two solvers find simultaneously the same nogoods is all smaller since there are few solvers. we explain thus that the scheme with manager becomes better than one without manager when the number of solvers increases. regarding the cost of communications for binary nogoods  the scheme with manager is significantly cheaper and this economy increases with . this result is explained by the fact that binary nogoods are  in general  useful for few solvers.
on overall communications  the scheme with manager is the best  due essentially to the number of binary nogoods which is significantly greater than one of unary nogoods  with a factor between 1 and 1 .
모in conclusion  the manager does its job by limiting the number of exchanged messages. it avoids solvers a lack of time due to management of messages  in particular  receipt of useless nogoods .
1	conclusions and future works
in this paper  from an idea of martinez and verfaillie  we define a new scheme of cooperative parallel search by exchanging nogoods and we assess experimentally its efficiency. we observe then linear or superlinear speed-up up to 1 solvers for inconsistent problems and up to 1 solvers for consistent ones. so  exchange of nogoods is an efficient form of cooperation. we note a decrease of efficiency with the number of solvers  due to the increasing number of communications and to a lack of diversity of solvers.
모a first extension of this work consists in finding several efficient and diverse heuristics in order to improve efficiency as well as increase the number of solvers. then  we can extend our scheme by applying any algorithm which maintains some level of consistency  by using different algorithms  which would permit to combine complete search methods and incomplete ones like in  hogg and williams  1  and to improve the diversity of solvers   or by generalizing it to another form of cooperation with exchange of informations.
references
 bessie`re and re뫣gin  1  c. bessie`re and j.-c. re뫣gin. mac and combined heuristics : two reasons to forsake fc  and cbj   on hard problems. in proc. of cp 1  pages 1  1.
 clearwater et al.  1  s. clearwater  b. huberman  and t. hogg. cooperative solution of constraint satisfaction problems. science  1-1  nov. 1.
 hogg and huberman  1  t. hogg and b. a. huberman. better than the best: the power of cooperation. in l. nadel and d. stein  editors  1 lectures in complex systems  volume v of sfi studies in the sciences of complexity  pages 1. addison-wesley  1.
 hogg and williams  1  t. hogg and c.p. williams. solving the really hard problems with cooperative search. in proc. of aaai 1  pages 1  1.
 hogg and williams  1  t. hogg and c.p. williams. expected gains from parallelizing constraint solving for hard problems. in proc. of aaai 1  pages 1  seattle  wa  1.
 martinez and verfaillie  1  d. martinez and g. verfaillie. echange de nogoods pour la re뫣solution coope뫣rative de proble`mes de satisfaction de contraintes. in proc. of cnpc 1  pages 1  dijon  france  1. in french.
 schiex and verfaillie  1  t. schiex and g. verfaillie. nogood recording for static and dynamic constraint satisfaction problems. in proc. of the 1 ieee ictai  1.
 terrioux  1  c. terrioux. cooperative search and nogood recording. research report  lim  1.
search on high degree graphs
toby walsh
department of computer science
university of york
york
england
tw cs.york.ac.ukabstract
we show that nodes of high degree tend to occur infrequently in random graphs but frequently in a wide variety of graphs associated with real world search problems. we then study some alternative models for randomly generating graphs which have been proposed to give more realistic topologies. for example  we show that watts and strogatz's small world model has a narrow distribution of node degree. on the other hand  baraba뫣si and albert's power law model  gives graphs with both nodes of high degree and a small world topology. these graphs may therefore be useful for benchmarking. we then measure the impact of nodes of high degree and a small world topology on the cost of coloring graphs. the long tail in search costs observed with small world graphs disappears when these graphs are also constructed to contain nodes of high degree. we conjecture that this is a result of the small size of their  backbone   pairs of edges that are frozen to be the same color.
1	introduction
how does the topology of graphs met in practice differ from uniform random graphs  this is an important question since common topological structures may have a large impact on problem hardness and may be exploitable. baraba뫣si and albert have shown that graphs derived from areas as diverse as the world wide web  and electricity distribution contain more nodes of high degree than are likely in random graphs of the same size and edge density  baraba뫣si and albert  1 . as a second example  redner has shown that the citation graph of papers in the isi catalog contains a few nodes of very high degree  render  1 . whilst 1 out of the 1 papers receive less than 1 citations  1 are cited more than 1 times  and one received 1 citations. the presence of nodes with high degree may have a significant impact on search problems. for instance  if the constraint graph of a scheduling problem has several nodes with high degree  then it may be difficult to solve as some resources are scarce. as a second example  if the adjacency graph in a hamiltonian circuit problem has many nodes of high degree  then the problem may be easy since there are many paths into and out of these nodes  and it is hard to get stuck at a  dead-end  node. search heuristics like brelaz's graph coloring heuristic  brelaz  1  are designed to exploit such variation in node degree.
모this paper is structured as follows. we first show that nodes of high degree tend to occur infrequently in random graphs but frequently in a wide variety of real world search problems. as test cases  we use exactly the same problems studied in  walsh  1 . we then study some alternative models for randomly generating graphs which give more nonuniform graphs  specifically baraba뫣si and albert's power law model  watts and strogatz's small world model  and hogg's ultrametric model . finally  we explore the impact of nodes of high degree on search and in particular  on graph coloring algorithms.
1	random graphs
two types of random graphs are commonly used  the
and the	models.	in the	model  graphs with
nodes and edges are generated by sampling uniformly from the possible edges. in the model  graphs with nodes and an expected number of edges are generated by including each of the possible edges with fixed probability . the two models have very similar properties  including similar distributions in the degree of nodes. in a random graph  the probability that a node is directly connected to exactly others  follows a poisson distribution. more precisely 
where is the number of nodes  is the probability that any pair of nodes are connected  and is   the expected node degree. as the poisson distribution decays exponentially  nodes of high degree are unlikely.
모in this paper  we focus on the cumulative probability  which is the probability of a node being directly connected to or less nodes:
whilst is smoothly varying for random graphs  it can behave more erratically on real world graphs. the cumulative probability  which is by definition monotonically increasing  tends to give a clearer picture. figure 1 shows that the cumu-


figure 1: cumulative probability  y-axis  against the normalized node degree  x-axis  for random graphs with
.
lative probability against the normalized degree for random graphs rapidly approaches a step function as increases. the degree of nodes therefore becomes tightly clustered around the average degree.
1	real world graphs
we next studied the distribution in the degree of nodes found in the real world graphs studied in  walsh  1 .
1	graph coloring
we looked at some real world graph coloring problems from the dimacs benchmark library. we focused on the register allocation problems as these are based on real program code. figure 1 demonstrates that these problems have a very

figure 1: cummulative probability  y-axis  against the normalized node degree  x-axis .  register data  is the zeronin.i.1 register allocation problem which is converted into a graph coloring problem with 1 nodes and 1 edges.  random graph  is a random graph of the same size and edge density. other problems in the dimacs graph coloring benchmark gave similar results.
skewed distribution in the degree of their nodes. other problems from the dimacs benchmark library gave very similar cumulative probability distributions for the degree of their nodes. compared to random graphs of the same size and edge density  these register allocation problems have a number of nodes that are of much higher and lower degree than the average. for example  the node of maximum degree in figure 1 is directly connected to 1% of the nodes in the graph. this is more than twice the average degree  and there is less than a 1 in 1 million chance that a node in a random graph of the same size and edge density has degree as large as this. on the other hand  the node of least degree has less than half the average degree  and there is less than a 1 in 1 million chance that a node in a random graph of the same size and edge density has degree as small as this. the plateau region in the middle of the graph indicates that there are very few nodes with the average degree. most nodes have either higher or lower degrees. by comparison  the degrees of nodes in a random graph are tightly clustered around the average. a similar plateau region around the average degree is seen in most of the register allocation problems in the dimacs benchmark library.
1	time-tabling
time-tabling problems can be naturally modelled as graph coloring problems  with classes represented by nodes and time-slots by colors. we therefore tested some real world time-tabling problems from the industrial engineering archive at the university of toronto. figure 1 demonstrates that problems in this dataset also have a skewed distribution in the degree of their nodes. other benchmark problems from this library gave very similar curves. compared to random graphs with the same number of nodes and edges  these timetabling problems have a number of nodes that have much higher and lower degree than the average. for example  the node of maximum degree in figure 1 is directly connected to 1% of the nodes in the graph. this is nearly three times the average degree  and there is less than a 1 in chance that a node in a random graph of the same size and edge density has degree as large as this. on the other hand  the node of least degree has approximately one tenth of the average degree  and there is less than a 1 in chance that a node in a random graph of the same size and edge density has degree as small as this.  walsh  1  suggests that sparse problems in this dataset have more clustering of nodes than the dense problems. however  there was no obvious indication of this in the distribution of node degrees.
1	quasigroups
a quasigroup is a latin square  a by multiplication table in which each entry appears just once in each row or column. quasigroups can model a variety of practical problems like sports tournament scheduling and the design of factorial experiments. a number of open questions in finite mathematics about the existence  or non-existence  of quasigroups with particular properties have been answered using model finding and constraint satisfaction programs  fujita et al.  1 . recently  a class of quasigroup problems have been proposed as a benchmark for generating hard and satisfiable problem instances for local search methods  achlioptas et al.  1 .
모an order quasigroup problem can be represented as a binary constraint satisfaction problem with variables  each

figure 1: cummulative probability  y-axis  against the normalized node degree  x-axis .  time-tabling data  is the earl haig collegiate time-tabling problem which is converted into a graph coloring problem with 1 nodes and 1 edges.  random graph  is a random graph of the same size and edge density. other problems from the industrial engineering archive at the university of toronto gave similar results.
with a domain of size . the constraint graph for such a problem consists of cliques  one for each row and column  with each clique being of size . each node in the constraint graph is connected to other nodes. hence  if and 1 otherwise  and the cumulative probability is a step function at .
as all nodes in the constraint graph of a quasigroup have the same degree  quasigroups may suffer from limitations as a benchmark. for example  the brelaz heuristic  brelaz  1   which tries to exploit variations in the degree of nodes in the constraint graph  may perform less well on quasigroup problems than on more realistic benchmarks in which there is a variability in the degree of nodes.
1	non-uniform random models
as the and models tend to give graphs with a narrow distribution in the degree of nodes  are there any better models for randomly generating graphs  in this section  we look at three different random models  all proposed by their authors to give more realistic graphs.
1	small world model
watts and strogatz showed that graphs that occur in many biological  social and man-made systems are often neither completely regular nor completely random  but have instead a  small world  topology in which nodes are highly clustered yet the path length between them is small  watts and strogatz  1 . such graphs tend to occur frequently in real world search problems  walsh  1 . to generate graphs with a small world topology  we randomly rewire a regular graph like a ring lattice  watts and strogatz  1; gent et al.  1 . the ring lattice provides nodes that are highly clustering  whilst the random rewiring introduces short cuts which rapidly reduces the average path length. unfortunately  graphs constructed in this manner tend not to have a wide distribution in the degree of nodes  and in particular are unlikely to contain any nodes of high degree. for small amounts of rewiring  peaks around the lattice degree  and converges on the poisson distribution found in random graphs for more extensive rewiring.

figure 1: cummulative probability  y-axis  against the normalized node degree  x-axis .  small world  is a graph with a small world topology generated by randomly rewiring a ring lattice of 1 nodes  each with 1 neighbors with a rewiring probability  .  random graph  is a random graph of the same size and edge density.
모in figure 1  we plot the cumulative probability for the node degrees of graphs generated to have a small world topology by randomly rewiring a ring lattice. small world graphs have a distribution of node degrees that is narrower than that for random graphs with the same number of nodes and edges. due to the lack of variability in the degree of nodes  these small world graphs may have limitations as a model of real world graphs. the absence of nodes of high degree is likely to impact on search performance. for instance  heuristics like brelaz which try to exploit variations in node degree are likely to find these graphs harder to color than graphs with a wider variability in node degree. can we find a model with a variability in the node degree that is similar to that seen in the real world graphs studied in the previous section 
1	ultrametric model
to generate graphs with more realistic structures  hogg has proposed a model based on grouping the nodes into a tree-like structure  hogg  1 . in this model  an ultrametric distance between the nodes is defined by grouping them into a binary tree and measuring the distance up this tree to a common ancestor. a pair of nodes at ultrametric distance is joined by an edge with relative probability . if   graphs are purely random. if   graphs have a hierarchical clustering as edges are more likely between nearby nodes. figure 1 gives the cumulative probability distribution for the node degrees in a graph generated with an ultrametric distance using the model from  hogg  1 . there is a definite broadening of the distribution in node degrees compared to random graphs. nodes of degree higher and lower than the average occur more frequently in these ultrametric graphs than in random graphs. for example  one node in the ultrametric graph

figure 1: cummulative probability  y-axis  against the normalized node degree  x-axis .  ultrametric  is a graph with a ultrametric world topology generated with 1 nodes  1 edges  to give an average degree of   and .  random graph  is a random graph of the same size and edge density.
is connected to all the other nodes. this node has more than twice the average degree  and there is less than a 1 in 1 million chance that a node in a random graph of the same size and edge density has degree as large as this. on the other hand  the node of least degree has just over half the average degree  and there is less than a 1 in 1 chance that a node in a random graph of the same size and edge density has degree as small as this. ultrametric graphs thus provide a better model of the distribution of node degrees. however  they lack a small world topology as nodes are not highly clustered  walsh  1 . can we find a model which has both a small world topology  which has shown to be common in real world graphs  and a large variability in the node degree  which has also been shown to be common  
1	power law model
baraba뫣si and albert have shown that real world graphs containing nodes of high degree often follow a power law in which the probability that a node is connected to others is proportional to where is some constant  typically around 1   baraba뫣si and albert  1 . redner has shown that highly cited papers tend to follow a zipf power law with exponent approximately -1  render  1 . it follows from this result that the degree of nodes in the citation graph for highly cited papers follows a power law with proportional to . such power law decay compares to the exponential decay in seen in random graphs.
모to generate power law graphs  baraba뫣si and albert propose a model in which  starting with a small number of nodes      they repeatedly add new nodes with     edges.
these edges are preferentially attached to nodes with high degree. they suggest a linear model in which the probability that an edge is attached to a node is where is the degree of node . using a mean-field theory  baraba뫣si et al.  1   they show that such a graph with	nodes has:

that is  is proportional to where . note that is also proportional to   the square of the average degree of the graph. in the limit of large   . the presence of non-linear terms in the preferential attachment probability will change the nature of this power law scaling and may be a route to power laws in which the scaling exponent is different to 1.

figure 1: cummulative probability  y-axis  against the normalized node degree  x-axis  for graphs generated to have a simple power law scaling in their node degree. note the logscale used for the x-axis.  power law  is a graph constructed by the modified baraba뫣si and albert's model with
모모모  and ;  random  is a random graph of the same size and edge density.
모we propose a minor modification to this model to tackle the problem that the average degree is bounded by the size of the initial graph . this will hinder the construction of high density graphs  which were not uncommon in the previous section . we suggest connecting an edge to a node with probability . each new node is then connected to the graph by approximately edges on average.
this modification is similar to moving from the to the model of random graphs.
모in figure 1  we plot the cumulative probability for the degree of nodes in graphs generated by this modified model. as with the ultrametric graphs  we observe a definite broadening of the distribution in node degrees compared to random graphs. nodes of degree higher and lower than the average occur more frequently in these power law graphs than in random graphs. for example  the node of maximum degree is directly connected to 1% of the nodes in the graph. this is more than three times the average degree  and there is less than a 1 in chance that a node in a random graph of the same size and edge density has degree as large as this. on the other hand  the node of least degree has nearly one fifth of the average degree  and there is less than a 1 in chance that a node in a random graph of the same size and edge density

has degree as small as this. unlike random graphs in which the distribution sharpens as we increase the size of graphs  we see a similar spread in the distribution of node degrees as these graphs are increased in size.
모ideally  we want like a method for generating graphs that gives graphs with both nodes of high degree and a small world topology. the nodes of high degree generated by the  modified  baraba뫣si and albert model are likely to keep the average path length short. but are the nodes likely to be tightly clustered  table 1 demonstrates that these graphs tend to have a small world topology as the graph size is increased.
1.1.1.1.1.1111111.1.1.1.1.1111111.1.1.1.1.111111table 1: average path lengths     and clustering coefficients     for graphs constructed to display a simple power law in the node degree. the clustering coefficient is the average fraction of neighbors directly connected to each other and is a measure of  cliqueness . graphs have nodes and are generated by the modified baraba뫣si and albert model using and . for comparison  the characteristic path lengths     and clustering coefficients     for random graphs of the same size and edge density are also given. the last column is the proximity ratio      the normalized ratio of the clustering coefficient and the characteristic path length  i.e.  . graphs with a proximity ratio  have a small world topology.
1	search
graphs generated by the modified baraba뫣si and albert model have both a broad distribution in degree of their nodes and a small world topology. these are both features which are common in real world graphs but rare in random graphs. these graphs may therefore be good benchmarks for testing graph coloring algorithms. they may also be useful for benchmarking other search problems involving graphs  e.g. for generating the constraint graph in constraint satisfaction problems  the adjacency graph in hamiltonian circuit problems  ... 
모unfortunately coloring graphs generated by the  modified  baraba뫣si and albert model is typically easy. most heuristics based on node degree can quickly  in many cases  immediately  find a -coloring. in addition  a -clique can be quickly found within the nodes of high degree showing that a -coloring is optimal. a simple fix to this problem is to start with an initial graph which is not a clique. this initial graph could be a ring lattice as in  watts and strogatz  1; walsh  1   or the inter-linking constraint graph of a quasigroup as in  gent et al.  1 . in both cases  we observe similar results. the choice of the initial graph has little effect on the evolution of nodes of high degree. in addition  starting from a ring lattice or the constraint graph of a quasigroup promotes the appearance of a small world topology. as in  achlioptas et al.  1; gent et al.  1   we generate problems with a mixture of regular structure  from the initial graph  and randomness  from the addition of new nodes .

figure 1: number of search nodes  x-axis  against probability of visiting this many search nodes  y-axis  when coloring graphs generated to have either a power law scaling in their node degree  a small world topology or a purely random topology. note the logscale used for the x-axis.  power law  is a 1 node graph constructed by the modified baraba뫣si and albert's model  starting from the constraint graph of an order 1 quasigroup  adding additional nodes into the graph with 1 edges each on average;  random  is a random graph of the same size and edge density;  small world  is a graph formed by randomly rewiring a 1 node ring lattice  each node starting with 1 neighbours  and each edge being rewired with probability 1. other instances of power law  random and small world graphs generated with the same parameters gave similar search cost distributions.
모in figure 1  we plot the distribution in search costs for coloring graphs with either a power law scaling in their node degree  a small world topology or a purely random topology. to find optimal colorings  we use an algorithm due to mike trick which is based upon brelaz's dsatur algorithm  brelaz  1 . unlike small world graphs  power law graphs do not display a long tail in the distribution of search costs. whilst power law graphs are easier to color than random graphs  there is a larger spread in search costs for power law graphs than for random graphs. the absence of a long tail means that there are less benefits with these power law graphs for a randomization and rapid restart strategy  gomes et al.  1; 1  compared to small world graphs  walsh  1 .
1	backbones
recent efforts to understand the hardness of satisfiability problems has focused on  backbone  variables that are frozen to a particular value in all solutions  monasson et al.  1 . it has been shown  for example  that hard random 1-sat problems from the phase transition have a very large backbone  parkes  1 . backbone variables may lead to thrashing behaviour since search algorithms can branch incorrectly on them. if these branching mistakes occur high in the search tree  they can be very costly to undo. the idea of backbone variable has been generalized to graph coloring  culberson and gent  1 . since any permutation of the colors is also a valid coloring  we cannot look at nodes which must take a given color. instead  we look at nodes that cannot be colored differently. as in  culberson and gent  1   two nodes are frozen in a -colorable graph if they have the same color in all valid -colorings. no edge can occur between two nodes that are frozen. the backbone is simply the set of frozen pairs.
모the power law graphs generated by the modified baraba뫣si and albert model in figure 1 had very small backbones. indeed  in many cases  there are only one or two pairs of nodes in the backbone. at the start of search  it is therefore hard to color incorrectly any of the nodes in one of these power law graphs. this helps explain the lack of a long tail in the distribution of search costs. by comparison  the small world graphs had backbones with between fifty and one hundred pairs of nodes in them. at the start of search  it is therefore easy to color incorrectly one of nodes. this gives rise to a long tail in the distribution of search costs for backtracking algorithms like brelaz's dsatur algorithm.
1	conclusions
we have shown that nodes of high degree tend to occur infrequently in random graphs but frequently in a wide variety of real world search problems. as test cases  we used exactly the problem studied in  walsh  1 . we then studied some alternative models for randomly generating nonuniform graphs. watts and strogatz's small world model gives graphs with a very narrow distribution in node degree  whilst hogg's ultrametric model gives graphs containing nodes of high degree but lacks a small world topology. baraba뫣si and albert's power law model combines the best of both models  giving graphs with nodes of high degree and with a small world topology. such graphs may be useful for benchmarking graph coloring  constraint satisfaction and other search problems involving graphs. we measured the impact of both nodes of high degree and a small world topology on a graph coloring algorithm. the long tail in search costs observed with small world graphs disappears when these graphs are also constructed to contain nodes of high degree. this may be connected to the small size of their  backbone   pairs of edges frozen with the same color.
모what general lessons can be learnt from this research  first  search problems met in practice may be neither completely structured nor completely random. since algorithms optimized for purely random problems may perform poorly on problems that contain both structure and randomness  it may be useful to benchmark with problem generators that introduce both structure and randomness. second  in addition to a small world topology  many real world graphs display a wide variation in the degree of their nodes. in particular  nodes of high degree occur much more frequently than in purely random graphs. third  these simple topological features can have a major impact on the cost of solving search problems. we conjecture that graph coloring heuristics like brelaz are often able to exploit the distribution in node degree  preventing much of thrashing behaviour seen in more uniform graphs.
acknowledgements
the author is an epsrc advanced research fellow and wishes to thank the other members of the apes research group.
references
 achlioptas et al.  1  d. achlioptas  c. gomes  h. kautz  and b. selman. generating satisfiable problem instances. in proc. of 1th nat. conf. on ai. 1.
 baraba뫣si and albert  1  a-l. baraba뫣si and r. albert. emergence of scaling in random networks. science  1-1  1.
 baraba뫣si et al.  1  a-l. baraba뫣si  r. albert  and h. jeong. mean-field theory for scale-free random networks. physica a  1-1  1.
 brelaz  1  d. brelaz. new methods to color the vertices of a graph. communications of acm  1-1  1.
 culberson and gent  1  j. culberson and i.p. gent. frozen development in graph coloring. theoretical computer science  1. to appear.
 fujita et al.  1  masayuki fujita  john slaney  and frank bennett. automatic generation of some results in finite algebra. in proc. of the 1th ijcai  pages 1. 1.
 gent et al.  1  i.p. gent  h. hoos  p. prosser  and t. walsh. morphing: combining structure and randomness. in proc. of the 1th nat. conf. on ai. 1.
 gomes et al.  1  c. gomes  b. selman  and n. crato. heavy-tailed distributions in combinatorial search. in g. smolka  editor  proc. of 1rd int. conf. on principles and practice of constraint programming  cp1   pages 1. springer  1.
 gomes et al.  1  c. gomes  b. selman  k. mcaloon  and c. tretkoff. randomization in backtrack search: exploiting heavy-tailed profiles for solving hard scheduling problems. in the 1th international conference on artificial intelligence planning systems  aips'1   1.
 hogg  1  t. hogg. refining the phase transition in combinatorial search. artificial intelligence  1-1 :1  1.
 monasson et al.  1  r. monasson  r. zecchina  s. kirkpatrick  b. selman  and l. troyansky. determining computational complexity for characteristic 'phase transitions'. nature  1-1  1.
 parkes  1  a. parkes. clustering at the phase transition. in proc. of the 1th nat. conf. on ai  pages 1. 1.
 render  1  s. render. how popular is your paper  an empirical study of the citation distribution. european physical journal b  1-1  1.
 walsh  1  t. walsh. search in a small world. in proceedings of 1th ijcai. artificial intelligence  1.
 watts and strogatz  1  d.j. watts and s.h. strogatz. collective dynamics of 'small-world' networks. nature  1-1  1.

search  satisfiability 
and constraint
satisfaction problems
satisfiability

backjumping for quantified boolean logic satisfiability
enrico giunchiglia and massimo narizzano and armando tacchella
dist - universita` di genova
viale causa 1  1 genova  italy
{enrico mox tac} mrg.dist.unige.it

abstract
the implementation of effective reasoning tools for deciding the satisfiability of quantified boolean formulas  qbfs  is an important research issue in artificial intelligence. many decision procedures have been proposed in the last few years  most of them based on the davis  logemann loveland procedure  dll  for propositional satisfiability  sat . in this paper we show how it is possible to extend the conflict-directed backjumping schema for sat to qbf: when applicable  it allows to jump over existentially quantified literals while backtracking. we introducesolution-directed backjumping  which allows the same for universally quantified literals. then  we show how it is possible to incorporate both conflict-directed and solution-directed backjumping in a dll-based decision procedurefor qbf satisfiability. we also implement and test the procedure: the experimental analysis shows that  because of backjumping  significant speed-ups can be obtained. while there have been several proposals for backjumping in sat  this is the first time -as far as we know- this idea has been proposed  implemented and experimented for qbfs.
1	introduction
the implementation of effective reasoning tools for deciding the satisfiability of quantified boolean formulas  qbfs  is an important research issue in artificial intelligence. many reasoning tasks involving abduction  reasoning about knowledge  non monotonic reasoning  are pspace-complete reasoning problems and are reducible in polynomial time to the problem of determining the satisfaction of a qbf. more important  since qbf reasoning is the prototypical pspace problem  many of these reductions are readily available. for these reasons  we have seen in the last few years the presentation of several implemented decision procedures for qbfs  like qkn  kleine-bu몮ning  h. and karpinski  m. and flo몮gel  a.  1   evaluate  cadoli et al.  1  cadoli et al.  1   decide  rintanen  1b   quip  egly et al.  1   qsolve  feldmann et al.  1 . most of the above decision procedures are based on the davis  logemann  loveland procedure  dll  for propositional satisfiability  davis et al.  1   sat . this is because it is rather easy to extend dll to deal with qbfs  and also because dll is at the core of many state-of-the-art deciders for sat.
모in this paper we show how it is possible to extend the conflict-directed backjumping schema for sat  see  e.g.   prosser  1  bayardo  jr. and schrag  1   to qbf: when applicable  it allows to jump over existentially quantified literals while backtracking. we introduce solutiondirected backjumping  which allows the same for universally quantified literals. then  we show how it is possible to incorporate both conflict-directed and solution-directedbackjumping in a dll-based decision procedure for qbf satisfiability. we also implement and test the procedure: the experimental analysis shows that  because of backjumping  significant speed-ups can be obtained. while there have been several proposals for backjumping in sat  this is the first time -as far as we know- this idea has been proposed  implemented and experimented for qbfs.
모the paper is structured as follows. in section 1 we introduce some formal preliminaries necessary for the rest of the paper. in section 1 we present qube  a dll based decision procedure for qbfs. section 1 is devoted to the presentation of the theoretical results at the basis of the backjumping procedure presented in section 1. the experimental analysis is reported in section 1. we end the paper with the conclusions and future work in section 1. proofs are omitted for lack of space.
1	formal preliminaries
consider a set p of propositional letters. an atomis an element of p. a literalis an atom or the negation of an atom. in the following  for any literal   is the atom occurring in ; and

	is	if	is an atom  and is	otherwise.
모a clause is an -ary disjunction of literals such that  for any two distinct disjuncts in   it is not the case that . a propositionalformula is a -ary
conjunction of clauses. as customary in sat  we represent a clause as a set of literals  and a propositional formula as a set of clauses. with this notation  e.g. 
the clause is the emptyclause and stands for the empty disjunction  the propositional formula is the emptysetofclauses and stands for the empty conjunction  the propositional formula stands for the set of clauses whose only element is the empty clause.
a qbfis an expression of the form
 1 
where
every	is a quantifier  either existential or universal	  are pairwise distinct atoms in p  and is a propositional formula in the atoms	.
             is the prefixand is the matrixof  1 .1 for example  the expression
 1 
is a qbf with 1 clauses.
consider a qbf  1 . a literal	is existentialif	belongs to the prefix of  1   and is universalotherwise.
	unitin  1  if	is existential  and  for some	 
- a clause	belongs to	  and
- each expression     occurs at the right of in the prefix of  1 .

monotone if either is existential  occurs in   and does not occur in ; or is universal  does not occur in   and occurs in .
a clause	is contradictoryif no existential literal belongs to . for example  the empty clause is contradictory.
모the semantics of a qbf can be defined recursively as follows. if contains a contradictoryclause then is false. if the matrix of is the empty set of clauses then is true. if is  respectively    is true if and only if or
 respectively and  are true. if is a qbf and is a literal  is the qbf obtained from by deleting the

clauses in which occurs  and removing from the others. it is easy to see that if is a qbf without universal quantifiers  the problem of deciding satisfiability reduces to sat.
1	qube
qube is implemented in c on top of sim  an efficient decider for sat developed by our group  giunchiglia et al.  1a . a high-level description of qube is presented in
figure 1 in figure 1  is a global variable initially set to the input qbf.
1 := the input qbf ; stack:= the empty stack ;
1 function simplify  
1 do
1 :=	;
1 if   a contradictory clause is in	  return false;
1 if   the matrix of	is empty   return true;
1 if  	is unit in	 
1 := unit; extend   ;
1 if  	is monotone in	 
1 := pure; extend   ;
1 while  	 ;
1 return undef;
1 function backtrack res 
1 while   stack is not empty   1	:= retract  ;
1 if  	= l-split  and
1   res = false and =   or 1  res = true and =   

1 := r-split; return ;
1 return null;
1 function qubesolver  
1 do
1 res := simplify  ;
1 if  res = undef 	:= chooseliteral  ;
1 else	:= backtrack res ;
1 if  	null  extend   ;
1 while  	null ;
1 return res;
figure 1: the algorithm of qube.
stack is a global variable storing the search stack  and is initially empty.
false  true  undef  null  unit  pure  l-split 
r-split are pairwise distinct constants. for each atom	in the input qbf 
- is a property of whose possible values are unit  pure  l-split  r-split  and have the obvious meaning  and
- is	if	is existential  and	otherwise.
extend    deletes the clauses of in which occurs  and removes from the others. additionally before performing the above operations  pushes and in the stack.
retract   pops the literal and corresponding qbf that are on top of the stack: the literal is returned  while the qbf is assigned to .  intuitively  retract is the  inverse  operation of extend .
simplify   simplifies till a contradictory clause is generated  line 1   or the matrix of is empty  line 1  

pspace complete. 1
모모we use the following pseudocode conventions. indentation indicates block structure. two instructions on the same line belong to the same block. :=  is the assignment operator. the constructs while cond block   do block while cond   if cond block else block have the same interpretation as in the c language.
or no simplification is possible  lines 1  1 . the simplifications performed in lines 1 and 1 correspond to lemmas 1  1 respectively of  cadoli et al.  1  cadoli et al.  1 .
chooseliteral   returns a literal occurring in such that for each atom occurring to the left of in the prefix of the input qbf 
- and	do not occur in	  or
- is existential iff	is existential.
	chooseliteral   also sets	to l-split.
backtrack res : pops all the literals and corresponding qbfs  line 1  from the stack  till a literal is reached such that is l-split  line 1   and either
- is existential and res = false  line 1 ; or -	is universal and res = true  line 1 .
	if such a literal	exists 	is set to r-split  and

is returned  line 1 . if no such literal exists  null is
returned  line 1 .
it is easy to see that qube is a generalizationof dll: qube and dll have the same behavior on qbfs without universal quantifiers.
모to understand qube behavior  consider the qbf  1 . for simplicity  assume that chooseliteral returns the negation of the first atom in the prefix which occurs in the matrix of the qbf under consideration. then  the tree searched by qube when is  1  is represented in figure 1. in figure 1  each node shows
the sequence of literals assigned by qube before a branch takes place  first line : for each literal in the sequence  we also show the value of ; and
the matrix of the resulting qbf  prefixed by a label  second line .
as the result of the computation  qube would correctly return false  i.e.   1  is unsatisfiable.
1	backjumping
let	be a qbf  1 . consider	.
in the following  for any finite sequence of literals  we write
	assign	as an abbreviation for
	assign	assign
where  if is a set of clauses  assign is the set of clauses obtained from by deleting the clauses in which

 occurs  and removing	from the others. as an abbreviation for the expression obtained from	by removing	whenever	or	is in	.
as an abbreviation for the qbf
assign
intuitively  if is a sequence of literals representing an assignment  is the qbf resulting after the literals in are assigned. for example  considering figure 1  if is  1   is
 	   and	is	  then	is	.
모as in constraint satisfaction  see  e.g.   dechter  1  prosser  1  bayardo  jr. and schrag  1   it may be the case that only a subset of the literals in the current assignment is responsible for the result  either true or false  of satisfiability. then  assuming that it is possible to effectively determine such a subset   we could avoid doing a right branch on a literal   if is not in . to make these notions precise we need the following definitions.
	a finite sequence	of literals is an
assignmentfor if for each literal	in
	is unit  or monotone in	; or
	occurs in	and for each atom	occurring to the
	left of	in the prefix of the input qbf 
	and	do not occur in	  or
	is existential iff	is existential.
	consider an assignment	for	.
a set of literals is a reasonfor resultif   and
for any assignmentfor	such that	-	is in 	-	is in	is in	 is satisfiable iff
for example  if	is  1  is satisfiable.	if	is	  thenis a reason forresult: foreach assignment fiable  and is unsatis-	if	is	  thenis a reason forresult:for each assignment satisfiable.	 	isintuitively 
when given an assignment such that either the matrix is empty or it contains a contradictoryclause  we first
compute a reason for result  and while backtracking  we dynamically modify the reason for the current result. furthermore  we use in order to avoid useless branches: in particular  we avoid doing a right branch on a literal if is not in .
the theorems in the next subsections -in which we show how to compute the reason- are an easy consequence of the following proposition.
let	be a reason for	result. we say that is a reasonfor	satisfiabilityif	is satisfiable  and is a reasonfor	unsatisfiability  otherwise.
proposition 1 let be a qbf  1 . let be an assignment for . let be a sequence obtained by removing some of the literals in . let be the set of literals in	.

	l	r

l  u  	u	r	l	u	p	u	r	p	pu
	l  	u
figure 1: qube computation tree for  1 . u  p  l  r stand for unit  pure  l-split  r-split respectively. the prefix is.
	is a reason for	unsatisfiability iff the qbf
assign
is unsatisfiable. is a reason for	satisfiability iff the qbf
assign
is satisfiable.
1	conflict-directed backjumping
the following theorem allows us to compute the reason for result when the matrix of contains a contradictory clause. given a sequence of literals   we say that a literal is false-irrelevant in if -assuming is the sequence obtained by deleting in - contains a contradictory clause.
theorem 1 let be a qbf. let be an assignment for such that contains a contradictory clause. let be a sequence obtained by recursively eliminating a falseirrelevant literal in . let be the set of literals in . then is a reason for unsatisfiability.
	if	is  1   this theorem allows us to conclude  e.g.  that
-with reference to figure 1-
if is   then is a reason for unsatisfiability  and
if is   then is a reason for unsatisfiability.
모our next step is to show how it is possible to compute reasons for	unsatisfiability while backtracking.
theorem 1 let be a qbf. let be a literal. let be an assignment for . let be a reason for unsatisfiability.
1. if	is not in	  then	is a reason for	unsatisfiability.
1. if	  and	is universal  then	is a reason for unsatisfiability.
1. if   is existential and monotone in   then is a reason for unsatisfiability.

1. if	 	 	is a reason for	 unsatisfiability 

and	is existential  then	is a reason for unsatisfiability.
	if	is  1   considering theorem 1 and figure 1:
given what we said in the paragraph below theorem 1  the 1th statement allows us to conclude that is a reason for unsatisfiability when is
 
from the above  the 1nd statement allows us to conclude that is a reason for unsatisfiability when is
 
from the above  the 1st statement allows us to conclude that is a reason for unsatisfiability when is in
.
from the last item  it follows that lookingfor assignments satisfying when begins with   is useless. given this  our  backjumping  procedure would have avoided the generation of the branch leading to     in figure 1.
1	solution-directed backjumping
the following theorem allows us to compute the reason for result when the matrix of	is empty. given a sequence of literals	  we say that a literal	is true-irrelevantin	if -assuming	is the sequence obtained by deleting	in	- the matrix of	is empty.
theorem 1 let be a qbf. let be an assignment for such that the matrix of is empty. let be a sequence obtained by recursively eliminating a true-irrelevant literal in . let be the set of literals in . then is a reason for satisfiability.
with reference to figure 1  the above theorem allows us to conclude that  e.g.  is a reason for satisfiability  if is and is  1 .
theorem 1 let be a qbf. let be a literal. let be an assignment for . let be a reason for satisfiability.
1. if	is not in	  then	is a reason for	satisfiability.
1. if	  and	is existential  then	is a reason for satisfiability.
1. if	 	is universal and monotone in	  then is a reason for	satisfiability.

1. if	 	 	is a reason for	 satisfiability  and

 is universal  then	is a reason for satisfiability.
	if	is  1   considering theorem 1 and figure 1:
1 function backjump res 
1 := initwr res ;
1 while   stack is not empty  
1 := retract  ;
1 if  	 
1 if  res = false and	=	  or
1  res = true and	=	 
1 if  	= unit  or  	= r-split 

1 :=
1 if 	= l-split 
1 := r-split;
1 :=	;

1 return ;
1 else	:=	;
1 return null;
figure 1: a procedure for conflict-directed and solutiondirected backjumping.
given what we said in the paragraph below theorem 1  the 1nd statement allows us to conclude that
is a reason for satisfiability when is   from the above  the 1nd statement allows us to conclude that is a reason for satisfiability when is
 
from the above  the 1st statement allows us to conclude that is a reason for satisfiability when is
.
from the last item  it follows that looking for assignments falsifying when begins with   is useless. given this  our  backjumping  procedure would have avoided the generation of the branch leading to     in figure 1.
1	implementation in qube
a procedure incorporating both conflict-directed and solution-directed backjumping has been implemented in qube. a high-level description of this procedure is presented in figure 1. consider figure 1. assume that is the input qbf and that is the assignment for corresponding to the sequence of literals stored in the stack. then 
initwr res  initializes the variable   storing the reason for result. in our implementation:
	if res	false 	contains a contradictory clause. let

모be a clause in such that  for each literal in   is in or is universal in . then initwr res  returns the set of literals in such that is in  see theorem 1 . for example  if is  1   and is   then initwr res  returns .
	if res	true  the matrix of	is empty.	then
initwr res  returns the set of literals in the sequence obtained from by recursively eliminating a universal literal such that  for each clause in   if then there is another literal in the sequence with  see theorem 1 . for example  if is
	 1   and	is	  then initwr res  returns
.
if is a literal in   is the property of that  if set  stores the reason for result.
the function qubesolver in figure 1 needs to be modified in two ways. first  if is a unit in   then  contains a contradictory clause. let be a clause in such that  for

each literal	in	 	is in	or	is universal in	. then 

the set of literals in such that is in is a reason for  unsatisfiability. if unitsetreason    is invoked when
is a unit in	  and assuming this function returns a set
defined as above  the instruction
:= unitsetreason   ;
has to be added in line 1. for example  if is  1   and is   then is a unit in   contains a contradictory clause  and is stored in
         . second  the procedure backjump res has to be invoked in place of backtrack res at line 1.
모considering the procedure backjump res  in figure 1 - once the working reason is initialized  line 1  according to the above- backjump res  pops all the literals and corresponding qbfs  line 1  from the stack  till a literal is reached such that belongs to the working reason  line 1   is
l-split  line 1   and either is existential and res=false  line 1 ; or is universal and res=true  line 1 .
if such a literal	exists 	is set to r-split line 1  

the working reason is stored in  line 1   and is returned  line 1 . if no such literal exists  null is returned  line 1 .
모notice that if is not in   we can safely retract despite the other conditions  see statement 1 in theorems 1  1 : assigning would not change the result of the computation.
모if is in   one of the conditions in line 1 or line 1 is satisfied  but is unit or r-split  line 1   we can use the results in statement 1 of theorems 1  1 to compute the new working reason  line 1 .
모if is in but neither the condition in line 1 nor the condition in line 1 is satisfied  then we can retract and remove from  line 1 . see the second statement of theorems1  1.
모finally  given the reasons returned by our implementation of initwr res   it is easy to see that neither a universal  monotone literal can belong to the reason of an universal literal when res=true  nor an existential  monotone literal can belong to the reason of an existential literal when res=false. thus  the statement 1 in theorems 1  1  included for theoretical completeness  is never applied in our implementation.
모our implementation of qube with backjump is a generalization of the conflict-directed backjumping procedure implemented  e.g.  in relsat: assuming that our procedure and relsat -without learning- perform the same nondeterministic choices  the two systems have the same behavior on qbfs without universal quantifiers.
1	experimental analysis
1
figure 1: qube-bt and qube-bj median cpu time  left  and number of branching nodes  right . 1 samples/point.to evaluate the benefits deriving from backjumping  we compare qube with backtracking  that we call qube-bt   and qube with backjumping  that we call qube-bj . for both background: satisfiability percentage.
systems  we use the branching heuristics described in  feldmann et al.  1  . all the tests have been run on a pentium iii  1mhz  1mbram.
모we first consider sets of randomly generated qbfs. the generation model that we use is model a by gent and walsh . in this model  each qbf has the following 1 properties:
1. the prefix consists of sequences  each sequence has quantifiers  and each two quantifiers in a same sequence  are of the same type 
1. the rightmost quantifier is	 
1. the matrix consists of	clauses 
1. each clause consists of	literals of which at least 1 are existential.
figure 1 shows the median of the cpu times  left  and number of branching nodes  right  of qube-bt and qube-bj when       and  on the -axis  is varied in such a way to empirically cover the  1% satisfiable - 1% unsatisfiable  transition  shown in the background . notice the logarithmicscale on the -axis. consider figure 1left. as it can be observed  qube-bj is faster  up-to two orders of magnitude  than qube-bt. qube-bj better performances are due to its minor number of branching nodes  as shown in figure 1-right.
qube-btqube-bjtest filetimebr. nodestimebr. nodesb*-1ii.11-11b*-1ii.11-11b*-1iii.1-11t*-1.iv.1.111t*-1.iv.1.111t*-1.iv.1.111t*-1.iv.1.111c*-111.1table 1: qube-bt and qube-bj on rintanen's benchmarks. names have been abbreviated to fit in the table.
we also test qube-bt and qube-bj on the structured rintanen's benchmarks.1 these problems are translations from planning problems to qbf  see  rintanen  1a  . for lack of space  table 1 shows qube-bt and qube-bj performances on only 1 out of the 1 benchmarks available. on 1 of the other 1 problems  qube-bt and qube-bj perform the same number of branching nodes  and on the remaining 1  none of the two systems is able to solve the problem in the time limit of 1 seconds. as before  qube-bj never performs more branching nodes than qube-bt  and is sometimes much faster. the last line of the table shows that when the number of branching nodes performed by qubebt and qube-bj is the same  the computational overhead paid by qube-bj is not dramatic.
1	conclusions and future work
in the paper we have shown that it is possible to generalize the conflict-directed backjumping schema for sat to qbfs. we have introduced solution-directed backjumping. our implementation in qube shows that these forms of backjumping can produce significant speed ups. as far as we know  this is the first time a backjumping schema has been proposed  implemented and experimented for qbfs. it is worth remarking that the logics of conflict-directed  section 1  and solutiondirected  section 1  backjumping are symmetrical: indeed  it would have been easy to provide a uniform treatment accounting for both forms of backjumping. we decided not to do it in order to improve the readability of the paper  at the same time showing the tight relations with the conflictdirected backjumping schema for sat.
beside the aboveresults  we have also comparativelytested
qube and some of the other qbf solvers mentioned in the introduction. our results show that qube compares well with respect to the other deciders  even without backjumping. for example  of the 1 rintanen's structured problems  decide  qube-bj  qube-bt  qsolve  evaluate are able to solve 1  1  1  1  1 samples respectively in less than 1s. in this regard  we point out that decide features  inversion of quantifiers  and  sampling  mechanisms which seem particularly effective on these benchmarks.
모qube  the experimental results  and the test sets used are available at qube web page: www.mrg.dist.unige.it/star/qube.
qube  besides the backjumping procedure above described  features six different branching heuristics  plus an adaptation of trivial truth  see  cadoli et al.  1  cadoli et al.  1  .  see  giunchiglia et al.  1b  for a description of qube's available options.  about trivial truth  backjumping and their interactions  we have conducted an experimental evaluation on this. the result is that neither trivial truth is always better than backjumping  nor the other way around. on the other hand  the overhead of each of these techniques  on the test sets we have tried  is not dramatic  and thus it seems a good idea to use both of them. these and other results are reported in  giunchiglia et al.  1c .
모about the future work  we are currently investigating the effects of using different branching heuristics. then  we plan to extend qube in order to do some form of  size  or  relevance  learning as it has been done in sat  see  e.g.   bayardo  jr. and schrag  1  .
acknowledgments
thanks to davide zambonin for the many fruitful discussion about implementation. thanks to an anonymous reviewer for suggesting the name  solution-directed backjumping . thanks to marco cadoli  rainer feldmann  theodor lettman  jussi rintanen  marco schaerf and stefan schamberger for providing us with their systems and helping us to figure them out during our experimental analisys. this work has been partially supported by asi and murst.
references
 bayardo  jr. and schrag  1  r. j. bayardo  jr. and r. c. schrag. using csp look-back techniques to solve realworld sat instances. in proc. aaai  pages 1  1.
 cadoli et al.  1  m. cadoli  a. giovanardi  and m. schaerf. an algorithm to evaluate quantified boolean formulae. in proc. aaai  1.
 cadoli et al.  1  m. cadoli  m. schaerf  a. giovanardi  and m. giovanardi. an algorithm to evaluate quantified boolean formulae and its experimental evaluation. journal of automated reasoning  1. to appear. reprinted in  gent et al.  1 .
 davis et al.  1  m. davis  g. logemann  and d. loveland. a machine program for theorem proving. journal of the acm  1   1.
 dechter  1  rina dechter. enhancement schemes for constraint processing: backjumping  learning  and cutset decomposition. artificial intelligence  1 :1  january 1.
 egly et al.  1  u. egly  t. eiter  h. tompits  and s. woltran. solving advanced reasoning tasks using quantified boolean formulas. in proc. aaai  1.
 feldmann et al.  1  r. feldmann  b. monien  and s. schamberger. a distributed algorithm to evaluate quantified boolean formulae. in proc. aaai  1.
 gent and walsh  1  ian gent and toby walsh. beyond np: the qsat phase transition. in proc. aaai  pages 1- 1  1.
 gent et al.  1  ian p. gent  hans van maaren  and toby walsh  editors. sat1. highlights of satisfiability research in the year 1. ios press  1.
 giunchiglia et al.  1a  enrico giunchiglia  marco maratea  armando tacchella  and davide zambonin. evaluating search heuristics and optimization techniques in propositional satisfiability. in proc. of the international joint conference on automated reasoning  ijcar'1   1.
 giunchiglia et al.  1b  enrico giunchiglia  massimo narizzano  and armando tacchella. qube: a system for deciding quantified boolean formulas satisfiability. in proc. of the international joint conference on automated reasoning  ijcar'1   1.
 giunchiglia et al.  1c  enrico giunchiglia  massimo narizzano  and armando tacchella. on the effectiveness of backjumping and trivial truth in quantified boolean formulas satisfiability  1. submitted. available at www.mrg.dist.unige.it/star/qube.
 kleine-bu몮ning  h. and karpinski  m. and fl몮ogel  a.  1  kleine-bu몮ning  h. and karpinski  m. and fl몮ogel  a. resolution for quantified boolean formulas. information and computation  1 :1  1.
 prosser  1  patrick prosser. hybrid algorithms for the constraint satisfaction problem. computational intelligence  1 :1  1.
 rintanen  1a  jussi rintanen. constructing conditional plans by a theorem prover. journal of artificial intelligence research  1-1  1.
 rintanen  1b  jussi t. rintanen. improvements to the evaluation of quantified boolean formulae. in proc. ijcai  pages 1  1.
solving non-boolean satisfiability problems with stochastic local search
alan m. frisch and timothy j. peugniez
artificial intelligence group
department of computer science university of york
york yo1dd
united kingdomabstract
much excitement has been generated by the recent success of stochastic local search procedures at finding satisfying assignments to large formulas. many of the problems on which these methods have been effective are non-boolean in that they are most naturally formulated in terms of variables with domain sizes greater than two. to tackle such a problem with a boolean procedure the problem is first reformulated as an equivalent boolean problem. this paper introduces and studies the alternative of extending a boolean stochastic local search procedure to operate directly on non-boolean problems. it then compares the non-boolean representation to three boolean representations and presents experimental evidence that the non-boolean method is often superior for problems with large domain sizes.
1	introduction
much excitement has been generated by the recent success of stochastic local search  sls  procedures at finding satisfying truth assignments to large formulas of propositional logic. these procedures stochasticly search a space of all assignments for one that satisfies the given formula. many of the problems on which these methods have been effective are non-boolean in that they are most naturally formulated in terms of variables with domain sizes greater than two. to tackle a non-boolean problem with a boolean procedure  the problem is first reformulated as an equivalent boolean problem in which multiple boolean variables are used in place of each non-boolean variable.
모this encode-and-solve approach often results in comparable  if not superior  performance to solving the problem directly. because boolean satisfiability is conceptually simple  algorithms for it are often easier to design  implement and evaluate. and because sls algorithms for boolean satisfiability have been studied intensively for more than a decade  highly-optimised implementations are publicly available.
모this paper proposes and studies a new approach to solving non-boolean satisfaction problems: that of generalising a boolean sls procedure to operate directly on a non-boolean formula by searching through a space of assignments to nonboolean variables. in particular  we have generalised walksat  selman et al.  1   a highly-successful sls procedure for boolean satisfiability problems  to a new procedure  nbwalksat  peugniez  1; frisch and peugniez  1   that works on formulas whose variables have domains of any fi-
nite size.1
모in this way we are able to apply highly-refined sls technology directly to non-boolean problems without having to encode non-boolean variables as boolean variables.
모the main question addressed by this paper is how the performance of the direct approach compares to that of the transformational  or encode and solve  approach. in particular we compare one direct method  nb-walksat  and three transformational methods by empirically testing their ability to solve large graph colouring problems and large random nonboolean formulas. our three transformation methods consist of applying walksat to the results of three transforms.
모boolean variables are merely a special case of non-boolean variables  and  intuitively  the difference between the nonboolean and boolean variables grows as the domain size of the non-boolean variable increases. consequently  one would expect that in a comparison of encodings for non-boolean problems that domain size would be the most important parameter to consider and that one would find that any difference in performance between the encodings would increase when domain size is increased. ours is the first study to consider this.
모our experimental results show nb-walksat to be highly effective  demonstrating that the effectiveness of the walksat strategies can be transferred from the boolean case to the non-boolean case. on problems with large domain sizes our direct method is often superior to the transformation methods  which  in some cases  are ineffective.
모in the course of studying this new method of solving nonboolean problems with sls  we make several other new contributions. most naturally  we have had to generalise the notion of a boolean formula to that of a non-boolean formula. of the three transformations we use  one is new and one is an enhanced version of a transformation used by others. in order to test the effect of domain size on problem solving performance we want a method for generating random formulas that vary in domain size but are similar in other respects. we propose such a method and use it in our experiments.
1	non-boolean formulas
non-boolean formulas are constructed from propositional variables  each of which is associated with a finite  non-empty domain. a non-boolean assignment maps every variable to a member of its domain. atomic non-boolean formulas  or nb-atoms  are of the form x/d  where x is a variable and d is a member of its domain. this formula is assigned true by an assignment if the assignment maps x to d; otherwise it is assigned false. molecular non-boolean formulas are constructed from atomic non-boolean formulas and truthfunctional connectives with the usual syntax and boolean semantics. observe that though non-boolean variables can take on values from a domain of arbitrary size  the semantics is still two-valued in that every formula is assigned either true or false. as usual  we say that an assignment satisfies a nonboolean formula if it maps it to true.
모walksat  and many other boolean sls procedures  operate on boolean formulas in conjunctive normal form  cnf   and nb-walksat  our generalisation of walksat  operates on non-boolean formulas in cnf. a formula  boolean or nonboolean  is in cnf if it is a conjunction of disjunctions of literals. a literal is either an atomic formula  called a positive literal  or its negation  called a negative literal . we say that a cnf formula is positive if all its literals are positive and negative if all its literals are negative.
모non-boolean formulas generalise boolean formulas since a boolean formula can be transformed to a non-boolean formula by replacing every atom p with p1/true  where p1 is a variable whose domain is {true  false}.
모we sometimes use terms such as  nb-atom  or  nbformula  to emphasise that these syntactic objects are part of the non-boolean language. similar use is made of terms such as  b-atom  and  b-formula .
1	nb-walksat
walksat is a highly successful sls procedure for finding satisfying assignments to boolean formulas in cnf. we have generalised walksat to a new system  nb-walksat  that operates similarly on non-boolean formulas. indeed when handling a boolean formula the two procedures perform the same search.1 this paper is concerned with version 1 of nbwalksat  which we derived from walksat version 1. this section describes the operation of nb-walksat and  since on boolean formulas nb-walksat and walksat perform the same search  this section implicitly describes the operation of walksat.
모the simplest way to understand the operation of nbwalksat is to consider it as working on positive cnf nbformulas. this can be achieved by considering nb-walksat's first step to be the replacement of every negative literal  x/di with x/d1 뫈 몫몫몫 뫈 x/di 1 뫈 x/di+1 뫈 몫몫몫 뫈 x/dn  where x is a variable with domain {d1 ... dn}.
모nb-walksat operates by choosing a random assignment and then  until a satisfying assignment is found  repeatedly selecting a literal from an unsatisfied clause and modifying the assignment so as to satisfy that literal. since the selected literal  x/d  occurs in an unsatisfied clause  the present assignment must map x to a value other than d. the present assignment is modified so that it maps x to d  and its mapping of all other variable is unmodified. we say that the literal x/d has been flipped.
모what distinguishes nb-walksat and walksat from other procedures is the heuristic employed for selecting which literal to flip. though walksat provides a range of such heuristics  the greedy heuristic is generally the most effective  selman et al.  1  and many reported experiments have used it. consequently  it is the greedy version of walksat that forms the basis for nb-walksat and only the greedy heuristic is considered in this paper.
모nb-walksat with the greedy heuristic chooses a literal to flip by first randomly selecting a clause with uniform distribution from among all the clauses that are not satisfied by the current assignment. we say that flipping a literal breaks a clause if the clause is satisfied by the assignment before the flip but not after the flip. if the selected clause contains a literal such that flipping it would break no clauses  then the literal to flip is chosen randomly with uniform distribution from among all such literals. if the selected clause contains no such literals  then a literal is chosen either  i  randomly with uniform distribution from the set of all literals in the clause or  ii  randomly with uniform distribution from among the literals in that clause that if flipped would break the fewest clauses. the decision to do  i  or  ii  is made randomly; with a user-specified probability  pnoise  the  noisy  choice  i  is taken.
1	transforming non-boolean formulas
to transform nb-satisfaction problems to b-satisfaction problems we map each nb-formula to a b-formula such that the satisfying assignments of the two formulas correspond. this paper presents three such transforms  called the unary/unary  unary/binary and binary transforms. each operates on an arbitrary formula  though our experiments only apply the transforms to cnf formulas. each transform operates by replacing each atom in the nb-formula with a b-formula that  in a sense  encodes the nb-atom it replaces. the resulting formula is known as the kernel of the transformation. the transforms employ two ways of producing a kernel; we call these two encodings  unary  and  binary .
모if the unary encoding of the kernel is used  the transform also needs to conjoin two additional formulas to the kernel  known as the at-least-one formula  or alo formula  and the at-most-one formula  or amo formula . as with the kernel  two encodings can be used for the alo and amo formulas: unary and binary. the three transforms we use in this paper are binary  which uses a binary encoding for the kernel and no alo or amo formula   unary/unary  which uses unary encodings for the kernel and for the alo and amo formulas  and unary/binary  which uses a unary encoding for the kernel and a binary encoding for the alo and amo formulas .
모the longer version of this paper analyses the sizes of the formulas produced by all the encodings.
the unary/unary transform the unary/unary transform produces a kernel by transforming each nb-atom x/d to a distinct propositional variable  which we shall call x:d. the idea is that a boolean assignment maps x:d to true if and only if the corresponding non-boolean assignment maps x to d. thus  the role of an nb-variable with domain {d1 ...dn} is played by n b-variables.
모furthermore  one must generally add additional formulas to the boolean encoding to represent the constraint that a satisfying assignment must satisfy exactly one of x:d1 ... x:dn. this constraint is expressed as a conjunction of an alo formula  asserting that at least one of the variables is true  and an amo formula  asserting that at most one of the variables is true . to state that at least one of x:d1 ... x:dn must be satisfied we simply use the clause x:d1 뫈 몫몫몫 뫈 x:dn. the entire alo formula is a conjunction of such clauses  one clause for each nb-variable. to say that at most one of x:d1 ... x:dn must be satisfied we add  x:di 뫈  x:dj  for all i and j such that 1 뫞 i   j 뫞 n.
the entire amo formula is a conjunction of all such clauses produced by all nb-variables.
모notice that these alo and amo formulas are in cnf. and since the transform produces a kernel whose form is identical to that of the original formula  the entire b-formula produced by the unary/unary transform is in cnf if and only if the original nb-formula is.
the binary transform the unary/unary transform uses d b-variables to encode a single nb-variable of domain size d and  hence  uses a base 1 encoding. by using a base 1 encoding  the binary transformation requires only dlog1 de bvariables to encode the same nb-variable.1 if x is a variable of domain size d then the binary transform maps an nb-literal of the form x/di by taking the binary representation of i   1 and encoding this in dlog1 de boolean variables. for example  if x has domain {d1 d1 d1 d1} then
x/d1 is mapped to  x1 뫇  x1  
x/d1 is mapped to  x1 뫇 x1  
x/d1 is mapped to x1 뫇  x1 and x/d1 is mapped to x1 뫇 x1 .
모to see what happens when the domain size is not a power of two  reconsider x to have the domain {d1 d1 d1}. if we map x/d1  x/d1 and x/d1 as above then there is a problem in that the boolean assignment that satisfies x1 뫇 x1 does not correspond to an assignment of a domain value to x. one solution to this problem  which has been employed by hoos   is to add an alo formula to ensure that the extraneous binary combination x1 뫇 x1 cannot be satisfied in any solution.
모here we introduce and use a new and better solution to this problem in which no extraneous combinations are produced and therefore no alo formula is required. in this example the extraneous combination is eliminated if
x/d1 is mapped to  x1 
x/d1 is mapped to x1 뫇  x1 and x/d1 is mapped to x1 뫇 x1.
here  the transform of x/d1 covers two binary combinations:   x1 뫇 x1  and   x1 뫇  x1 .
모to see what happens in general let x be a variable with domain {d1 ... dn} and let k be 1dlog1ne   n. then x/d1 ... x/dk are each mapped to cover two binary combinations and x/dk+1 ... x/dn are each mapped to cover a single binary combination.
모notice that this transform generates no extraneous binary combinations. also notice that  as a special case  if n is a power of two then each x/di  1 뫞 i 뫞 n  is mapped to cover a single binary combination. finally  to confirm that this binary transform requires no amo formula and no alo formula  observe that every boolean assignment must satisfy the binary transform of exactly one of x/d1 ... x/dn.
모notice that the binary transformation of a cnf formula is not necessarily in cnf. however  the binary transformation of a negative cnf formula is almost in cnf; it is a conjunction of disjunctions of negated conjunctions of literals. by using demorgan's law  the negations can be moved inside of the innermost conjunctions  resulting in a cnf formula of the same size. at the other extreme  the binary transformation of a positive cnf formula is a conjunction of disjunctions of conjunctions of literals. one way of transforming this to cnf is to distribute the disjunctions over the conjunctions. unfortunately applying this distribution process to the binary transform of a positive nb-clause produces a cnf formula that can be exponentially larger than the original clause.
the unary/binary transform the unary/binary transform produces the same kernel as the unary/unary transform. the alo and amo formulas it produces achieve their effect by introducing the binary encodings of nb-atoms and linking each to the unary encoding of the same nb-atom. since the binary encoding requires no alo or amo formulas  the unary/binary encoding requires no alo or amo formulas beyond these linking formulas.
모the links provided by the amo formula state that for each nb-atom  x/di  its unary encoding  x:di  implies its binary encoding. for example  if the nb-variable x has domain {d1 d1 d1 d1} then the amo linking formula for x is
 x:d1 뫸   x1 뫇  x1  	뫇	 x:d1 뫸   x1 뫇 x1  	뫇  x:d1 뫸  x1 뫇  x1  	뫇	 x:d1 뫸  x1 뫇 x1  .
the entire amo formula is a conjunction of such linking formulas  one for each nb-variable.
모the links provided by the alo formula state that for each nb-atom  x/di  its unary encoding x:di is implied by its binary encoding. for example  if the nb-variable x has domain {d1 d1 d1 d1} then the alo-linking formula for x is
   x1 뫇  x1  뫸 x:d1 	뫇	   x1 뫇 x1  뫸 x:d1 	뫇   x1 뫇  x1  뫸 x:d1 	뫇	  x1 뫇 x1  뫸 x:d1 .
the entire alo formula is a conjunction of linking formulas  one linking formula for each nb-variable. observe that both the amo and alo linking formulas can be put easily into cnf with at most linear expansion.
1	experimental results
this section presents experiments on graph colouring problems and random nb-formulas that compare the performance of the four methods  which we shall refer to as nb  nonboolean encoding   bb  binary encoding  uu  unary/unary encoding  and ub  unary/binary encoding . in all experiments  walksat version 1 was used to solve the boolean encodings and nb-walksat version 1 was used to solve the non-boolean encodings  even in cases where the domain size is 1. both programs provide the user the option of either compiling the program with fixed size data structures or allocating the data structures when the formula is input at runtime; the latter option was used in all experiments. all experiments were run on a 1 mhz pentium iii with 1 megabytes of memory.
모considerable care must be taken in setting the pnoise parameter for the experiments. much work in this area has been reported without giving the value used for pnoise  and thus is irreproducible. setting the parameter to any fixed value over all formulas is not acceptable; we have observed that a parameter setting that is optimal for one formula can be suboptimal by several orders of magnitude for another formula. the best option is to report performance at the optimal setting for pnoise  which-in the absence of any known method to determine this a priori-we have determined experimentally. this is also the route followed by hoos  in his extremely careful work.
모in using sls procedures it is often considered advantageous to restart the search at a new  randomly-selected assignment if the procedure has not found a solution after a prescribed number of flips. this issue is not relevant for the current study. since the optimal restart point and the amount gained by restarting are functions of the procedure's performance without restarts  this study need only be concerned with the performance without restarts.
graph colouring our experiments used 1 instances of the graph colouring problem that are publicly available at the rutgers univ. dimacs ftp site.1
모each problem instance was encoded as a cnf nb-formula. for each node in the graph the formula uses a distinct variable whose domain is the set of colours. the formula itself is a conjunction of all clauses of the form  x/c 뫈  y/c  such that x and y are nodes connected by an arc and c is a colour.
모three boolean cnf representations of each problem instance were produced by applying the three transforms of section 1 to the non-boolean representation. the longer version of this paper proves that a negative cnf nb-formula can be transformed to a b-formula without the amo formulas. so the two unary encodings used here contain only kernels and alo clauses. also because the nb-clauses representing instances of the graph colouring problem are negative  the binary transform maps each nb-clause to a single b-clause  as discussed in section 1 .
모figure 1 shows the results obtained for solving each problem instance 1 times with each of the four methods. the formula size column shows the size  in terms of atom occurrences  of the formula input to walksat or nb-walksat. all experiments were run with pnoise set to its optimal value  which is recored in the pnoise column. the flip rate column gives the mean flip rate over all 1 runs. the median flips and median time column gives the sample median of the number of flips and amount of cpu time taken to solve the problem.
모examination of these results reveals that over the range of problems nb and uu have roughly comparable  within a factor of 1 to 1  solution times.1 with increasing domain size  number of colours  the nb fliprate drops sharply but the advantage that nb has in terms of number of flips increases. these two opposing factors cancel each other  resulting in the roughly comparable solution times.
모on the two problems with small domain size  1 colours  bb and ub produce solution times that are competitive with the other methods. however  on three problems with larger domain size-le1a with 1 colours and dsjc1.col with 1 and 1 colours-bb and ub do not compete well with the other methods. on le1a  a very easy problem with a domain size of 1  bb has competitive performance but ub is totally ineffective. we conjecture that ub cannot solve this very easy problem because the method is highly sensitive to domain size.
random non-boolean cnf formulas since we can control certain parameters in the generation of random cnf nbformulas  they provide a good testbed. in particular  since this paper is a study of solving problems with domain sizes greater than two  we would like to know how problem solving performance varies with domain size. to measure this we need to select problem instances that have different domain sizes but are otherwise similar. formulating a notion of  otherwise similar  has been one of the most challenging problems faced by this research.
모we have developed a program that generates random  positive cnf nb-formulas using five parameters: n  d  c  v and l. each generated formula consists of exactly c clauses. using a fixed set of n variables each with a domain size of d  each clause is generated by randomly choosing v distinct variables and then  for each variable  randomly choosing l distinct values from its domain. each of the chosen variables is coupled with each of its l chosen values to form l몫v positive literals  which are disjoined together to form a clause.
모the simplest conjecture on how to study varying domain size is to fix the values of n  c  v   and l and then to systematically vary d. one can easily see that performance on this task would just exhibit typical phase-transition behaviour  mitchell et al.  1 : small values of d would produce underconstrained problems  which would become criticallyconstrained and then over-constrained as d increases. the
problem instancemethodpnoise settingformula sizeflip rate
 flips/sec median flipsmedian time  secs le1a
1 colours
1 nodes
1 arcsnb.1 1 1 1.1bb.1 1 1 1.1uu.1 1 1 1.1ub.1 1 1 11le1d
1 colours
1 nodes
1 arcsnb.1 1 1 1.1bb.1 1 1 1.1uu.1 1 1 1.1ub.1 1 1 1.1le1a
1 colours
1 nodes
1 arcsnb.1 1 1 1.1bb1uu.1 1 1 11ub1dsjc1.col
1 colours
1 nodes
1 arcsnb.1 1 1 1.1bb1uu.1 1 1 11ub1dsjc1.col
1 colours
1 nodes
1 arcsnb.1 1 1 1.1bb.1 1 1 11uu.1 1 1 1.1ub.1 1 1 11le1a
1 colours
1 nodes
1 arcsnb.1 1 1.1bb.1 1111uu.1 1 1 1.1ub1figure 1: results  given to three significant figures  for 1 sample runs on each graph colouring problem. on those rows missing entries  1 attempts  each of 1 1 flips  were made at each of four pnoise settings  .1  .1  .1 and .1 ; no solutions were found.problem instances generated by this method would not be similar in terms of their location relative the phase transition.
모our solution to this shortcoming is to vary d and to adjust the other four parameters so as to put the problem class at the phase transition-that is  at the point where half the instances in the class are satisfiable. but for any given value of d many combinations of values for n  c  v and l put the problem class at the phase transition. we determine the values of these parameters by keeping dv   the size of the search space  at a fixed value for all problem instances and then requiring all clauses to have the same  constrainedness   as measured by  l/d v . mimicking boolean 1cnf  we set v to three and aim to keep  l/d v at 1  which is achieved by setting l to .1d. this follows the constant length model advocated by mitchell  selman and levesque . finally c is set to whatever value puts the problem class at the phase transition  which we determined experimentally. thus  while varying d  the parameters that we are holding constant are search space size  the number of variables constrained by each clause and the amount of constraint placed on each  and the proportion of instances with that parameter setting that are solvable.
모our experiments were conducted on domain sizes of 1  1  1  1 and 1. following the model above we kept dv at a constant value  1. then for each domain size  we experimentally located the point of the phase transition. the leftmost column of figure 1 shows the value of c  number of clauses  at which these experiments located the phase transitions.
모with the parameters of the random formulas determined  at each domain size we generated a series of random formulas according to the above method and  using a non-boolean version of the davis-putnam procedure  kept the first 1 satisfiable ones.1 at each domain size 1 attempts were made to solve the suite of 1 formulas with the nb  uu and ub methods. the longer version of this paper proves that a positive cnf nb-formula can be transformed to a b-formula without the alo formulas. since our randomly-generated formulas are positive  the two unary encodings that are used here contain only kernels and amo formulas.
모the bb method was not attempted since the binary transformation of these random nb-formulas is unreasonably long. as discussed at the end of section 1  if the binary transform is used to produce a cnf b-formula from a positive cnf nbformula  the size of the resulting b-formula is exponential in the clause length of the original nb-formula-which in this case is .
모the results of these experiments  shown in figure 1  consistently follow a clear pattern. in terms of both time and number of flips  all methods show a decline in performance as domain size grows. however  the decline of uu and ub is so sharp that ub is ineffective on domain size 1 and uu is two orders of magnitude slower than nb on domain size 1. on these random formulas  as on the graph colouring problems  the fliprates of all methods decrease as the domain size increases. however  on the random problems  unlike the colouring problems  nb's fliprate suffers the least from increasing domain size. indeed  by the time domain size reaches 1  nb has a higher fliprate than uu  and this advantage grows as domain size increases. the sharp decline in the uu fliprate results from the rapid increase in the size of the uu formulas  which occurs because the size of their amo component grows quadratically with domain size.
1	discussion and conclusions
problemmethodpnoise settingformula sizeflip rate
 flips/sec median flipsmedian time  ms .1111 varsuu.1111 clausesub.1111.1 1 11 varsuu.1 1 1 11 clausesub.1 1 1 1.1 1 1 11 varsuu.1 1 1 11 clausesub.1 1 1 1.1 1 1 11 varsuu.1 1 1 11 clausesub1.1 1 1 11 vars
1 clausesuu.1 1 1 1ub1figure 1: results  to three significant figures  for a suite of 1 satisfiable non-boolean cnf formulas chosen at random. the values recorded in the flips and time column are the flips and time required to solve a single formula  not the entire suite. on those rows missing entries  the median number of flips to solution of 1 runs at noise levels .1  .1 and .1 all exceeded 1 1.the literature reports a number of cases where the transformation approach has been used to attack a non-boolean problem with boolean sls. such problems include the n-queens problem  selman et al.  1   graph colouring  selman et al.  1; hoos  1  chp. 1   the all-interval-series problem  hoos  1  chp. 1   the hamiltonian circuit problem  hoos  1; 1  chp. 1   random instances of the  finite  discrete  constraint satisfaction problem  hoos  1; 1  chp. 1   and strips-style planning  kautz and selman  1; kautz et al.  1; kautz and selman  1; ernst et al.  1; hoos  1  chp. 1 . yet  in spite of this widespread use of the transformation approach  only two previous studies have compared the performance of sls on different encodings of non-boolean variables: ernst  millstein and weld  compare unary/unary and binary encodings of strips-style planing problems and hoos  1; 1  chapter 1  compares the unary/unary and binary encodings of random constraint satisfaction problems and of hamiltonian circuit problems. other than these two comparative studies  we know of no work that uses boolean encodings other than unary/unary.
모although hoos' experiments use a clever methodology designed to provide insight into the structure of sls search spaces  his study examines only problem instances whose non-boolean formulation involves variables with domain size 1. and though the work of ernst  millstein and weld is also enlightening in many respects  one cannot determine the domain sizes of the non-boolean variables in their encodings since they do not report the identity of their suite of planning instances. in contrast to all previous studies  ours considers the effect of varying domain size and hence is able to confirm the expectation that domain size is generally a critical parameter. in particular  our study suggests that hoos' conclusion-which also appears to be the dominant view in the community- that uu is a viable alternative to nb is generally true only for problems with smaller domain sizes. on problems that require amo formulas and have large domain sizes our study shows that nb tends to be the best choice and is sometimes the only feasible choice.
모in almost all work  when non-boolean problems are encoded as boolean formulas the encoding is done in a single stage  directly mapping the problem to a boolean formula. in some cases this is done manually and in others it is done automatically. in contrast  we advocate a two-stage approach in which the problem is first mapped  manually or automatically  to a non-boolean formula which is then systematically  and  in our case  automatically  transformed to a boolean formula. we conjecture that all boolean encodings that have so far been produced by the single stage approach could be produced by the two stage approach.
모it has recently come to our attention that work by bejar뫣 and manya`  in the area of multi-valued logic has independently led to the development of another generalisation of walksat  called regular-wsat  that appears to be almost identical to nb-walksat. regular-wsat operates on a special class of signed clauses called regular signed clauses. a signed clause is a syntactic variant of an nb-clause except that every variable must be associated with the same domain. to define regular signed clauses  the domain values first must be totally ordered.  in certain other settings  the order is only required to be a lattice.  a signed clause is said to be regular if  in effect  it is positive and for each literal x/d it contains it also contains either every literal x/d1 such that d1   d or every literal x/d1 such that d   d1.
모though regular-wsat is presented as a procedure for regular signed clauses  it appears that it would work on any signed clause and thus is akin to nb-walksat. nor does there appear to be anything inherent in the presentation of regularwsat that prevents it from handling variables with different domain sizes. however  when operating on a clause containing variables with differing domain sizes it appears that regular-wsat and nb-walksat use different distributions in choosing which literal to flip. regular-wsat first chooses a variable from among those in the clause and then chooses one of the values that appear with that variable  whereas nbwalksat simply chooses from among all the literals in the clause.
모though regular signed clauses are a special class of nbclauses  they can be used effectively to encode a range of problems including graph colouring problems and roundrobin scheduling problems  bejar and many뫣 a  1`  . the extra restrictions placed on regular signed clauses can be exploited to translate them to boolean clauses more compactly than can nb-clauses  bejar뫣 et al.  1 . however  we are not aware of any attempt to exploit these translations to solve problems encoded as regular signed clauses.
모it would be worthwhile to extend the comparisons made in this paper to include additional solution methods employing sls. as just suggested  the comparisons could include boolean translations of regular signed clauses. it would be especially worthwhile to consider sls methods  such as minconflicts  minton et al.  1   that operate directly on the original problem or on a constraint satisfaction encoding of the problem.1
모of the many questions remaining to be addressed by future work the biggest challenge facing the study of problem encodings-including all encoding issues  not just the handling of non-boolean variables-is the quest for generality. what can we say about about encoding issues that can guide us in producing effective encodings of new problems  this challenge must be decomposed if progress is to be made. this paper's biggest contribution towards this end is separating out the issue of non-boolean variables and identifying domain size as a critical parameter.
모finally we claim that  in addition to developing new problem encodings  the applicability of sls technology can be extended by enriching the language on which the sls operates. this claim is supported by recent results on pseudoboolean constraints  walser  1  and non-cnf formulas  sebastiani  1 . our success with nb-walksat adds further weight to the claim.
acknowledgements
we are grateful to henry kautz and bram cohen for providing their walksat code  from which the nb-walksat implementation was developed. we also thank peter stock  who used his non-boolean davis-putnam program to filter out the unsatisfiable instances from our suite of randomly-generated nb-formulas. thanks to toby walsh for many illuminating discussions and to steve minton  bart selman and the anonymous referees for useful suggestions.
references
 bejar and many뫣 a  1`   ramon bejar and felip many뫣 a.` solving combinatorial problems with regular local search algorithms. in harald ganzinger and david mcallester  editors  proc. 1th int. conference on logic for programming and automated reasoning  lpar  tbilisi  georgia  volume 1 of lecture notes in artificial intelligence  pages 1. springer-verlag  1.
 bejar뫣 et al.  1  ramon bejar  reiner h뫣 ahnle  and felip몮 manya.뫣 a modular reduction of regular logic to classical logic. in proc. 1st international symposium on multiple-valued logics  warsaw  poland. ieee cs press  los alamitos  may 1.
 brafman and hoos  1  ronen i. brafman and holger h. hoos. to encode or not to encode - i: linear planning. in proc. of the sixteenth int. joint conf. on artificial intelligence  pages 1. morgan kaufman  1.
 ernst et al.  1  michael d. ernst  todd d. millstein  and daniel s. weld. automatic sat-compilation of planning problems. in proc. of the fifteenth int. joint conf. on artificial intelligence  pages 1  1.
 frisch and peugniez  1  alan m. frisch and timothy j. peugniez. solving non-boolean satisfiability problems with local search. in fifth workshop on automated reasoning: bridging the gap between theory and practice  st. andrews  scotland  april 1.
 hoos  1  holger h. hoos. stochastic local search- methods  models  applications. phd thesis  technical university of darmstadt  1.
 hoos  1  holger h. hoos. sat-encodings  search space structure  and local search performance. in proc. of the sixteenth int. joint conf. on artificial intelligence  pages 1. morgan kaufman  1.
 kautz and selman  1  henry a. kautz and bart selman. planning as satisfiability. in bernd neumann  editor  proc. of the tenth european conf. on artificial intelligence  pages 1  vienna  austria  august 1. john wiley & sons.
 kautz and selman  1  henry a. kautz and bart selman. pushing the envelope: planning  propositional logic  and stochastic search. in proc. of the thirteenth national conf. on artificial intelligence  pages 1  portland  oregon  usa  1. aaai press / the mit press.
 kautz et al.  1  henry a. kautz  david mcallester  and bart selman. encoding plans in propositional logic. in luigia carlucci aiello  jon doyle  and stuart shapiro  editors  principles of knowledge representation and reasoning: proc. of the fifth int. conf.  pages 1  san francisco  1. morgan kaufmann.
 minton et al.  1  steven minton  mark d. johnson  andrew b. philips  and philip laird. minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems. artificial intelligence  1-1 :1- 1  1.
 mitchell et al.  1  david mitchell  bart selman  and hector j. levesque. hard and easy distributions of sat problems. in proc. of the tenth national conf. on artificial intelligence  pages 1  san jose  ca  1. mit press.
 peugniez  1  timothy j. peugniez. solving non-boolean satisfiability problems with local search. bsc thesis  department of computer science  university of york  march 1.
 sebastiani  1  roberto sebastiani. applying gsat to non-clausal formulas. journal of artificial intelligence research  1-1  1.
 selman et al.  1  bart selman  hector j. levesque  and david mitchell. a new method for solving hard satisfiability problems. in proc. of the tenth national conf. on artificial intelligence  pages 1. aaai press  1.
 selman et al.  1  bart selman  henry a. kautz  and bram cohen. noise strategies for improving local search. in proc. of the twelfth national conf. on artificial intelligence  pages 1  menlo park  ca  usa  1. aaai press.
 walser  1  joachim p. walser. solving linear pseudoboolean constraint problems with local search. in proc. of the fourteenth national conf. on artificial intelligence  pages 1. aaai press  1.

search  satisfiability 
and constraint
satisfaction problems
constraint satisfaction problems

backtracking through biconnected components of a constraint graph

jean-franc ois baget lirmm
1  rue ada
1 montpellier  cedex 1 france
e-mail: baget lirmm.fr yannic s. tognetti lirmm
1  rue ada
1 montpellier  cedex 1 france
e-mail: tognetti lirmm.fr

abstract
the algorithm presented here  bcc  is an enhancement of the well known backtrack used to solve constraint satisfaction problems. though most backtrack improvements rely on propagation of local informations  bcc uses global knowledge of the constraint graph structure  and in particular its biconnected components  to reduce search space  permanently removing values and compiling partial solutions during exploration. this algorithm performs well by itself  without any filtering  when the biconnected components are small  achieving optimal time complexity in case of a tree. otherwise  it remains compatible with most existing techniques  adding only a negligible overhead cost.
1	introduction
constraint satisfaction problems  csps   since their introduction in the early 1's  have flourished in many branches of artificial intelligence  and are now used in many  real-life  applications. since the satisfiability of a csp is a np-complete problem  much effort have been devoted to propose faster algorithms and heuristics. backtrack can be seen as the backbone for those improvements: this algorithm first extends a partial solution by assigning a value to a variable  and undo a previous assignment if no such value can be found.
모backtracking heuristics try to restrict search space  their goal can be to maximize the probability of a  good guess  for the next assignment  variable and value orderings   or to recover more efficiently from a dead-end  backjumping and its variants . filtering techniques use propagation of some local consistency property ahead of the current partial solution  effectively reducing variable domains; they can be used either in a preprocessing phase  or dynamically during exploration. structure-driven algorithms emerged from the identification of tractable classes of csp  such as trees  mackworth and freuder  1 ; they often transform the csp into one that can be solved in polynomial time  gottlob et al.  1 .
모the algorithm presented  bcc  does not fit easily in this classification. it accepts any preprocessed ordering of the variables  as long as it respects some properties related to the biconnected components of the constraint graph. then it exploits the underlying tree structure to reduce thrashing during the backtrack  storing partial solutions and removing permanently some values. though no kind of local consistency property is used  and no look-ahead is performed  bcc performs well when the graph contains small biconnected components  achieving optimal time complexity in case of a tree.
모after basic definitions about csps  we present in sect. 1 the version of backtrack used as the kernel of bcc. sect. 1 is devoted to definitions of the variable orderings compatible with bcc. the algorithm is presented in sect. 1  along with a proof of its soundness and completeness and an evaluation of its worst-case complexity. in sect. 1  we compare bcc with popular backjumping or filtering techniques  pointing out the orthogonality of these approaches and showing that a mixed algorithm is possible. finally  we discuss limitations of our algorithm  and suggest a method to overcome them.
1	definitions and notations
a binary constraint network over a set of symbols consists of a set of variables  denoted    each associated to a finite value domain   and of a set of constraints. a constraint between two variables and is a subset of . a variable is called instantiated when it is assigned a value from its domain. a constraint between two instantiated variables is said satisfied if value value . this test is called a consistency check between and . a consistent instantiation of a subset is an instantiation of all variables in that satisfies every constraint between variables of .
the constraint satisfaction problem  csp   given a constraint network   asks whether there exists a consistent instantiation of   called a solution of .
모without loss of generality  we can consider that constraints are always defined over two different variables  and that there is at most one constraint  either or   between two variables. so the constraint graph of a network  associating each variable to a node and connecting two nodes whose variables appear in the same constraint  is a non oriented simple graph without loops. in this paper  we will consider some total order on   inducing an orientation of this graph: edges can be seen as oriented from the smallest to the greatest of their ends. according to this orientation  we can define the predecessors and the successors of a node . note that is the width of  freuder  1 .
1	back to backtrack
as the backtrack algorithm  bt  is the kernel of bcc  we considered as vital to make it as efficient as possible  according to its usual specifications: 1  variables are examined in a fixed  arbitrary ordering; and 1  no information on the csp is known or propagated ahead of the current variable.
1	algorithm kernel

algorithm 1: backtrack 	 

	data	: a non empty network	.
	result	: true if	admits a solution  false otherwise.
computeorder    ; level 1;
failure false;
	while   level	1  and  level	   do
 level ;
if  hasmorevalues  currentvariable   then failure false; level nextlevel  currentvariable ;
else level previouslevel  currentvariable ;
if  getfirstvalue  currentvariable   then
level nextlevel  currentvariable ;
failure true; level previouslevel  currentvariable ;
	return  level =	 ;

모as in  prosser  1   bt is presented in its derecursived version: we will later easily control the variable to study after a successful or failed instantiation  without unstacking recursive calls. for the sake of clarity  it is written to solve a decision problem  though a solution could be  read in the variables . this algorithm should consider any variable ordering  so computeorder only builds the predecessor and successor sets according to the implicit order on the variables  and sorts the predecessors sets  for soundness of partial history . to respect our specifications  if is the th variable according to   previouslevel    must return and nextlevel    . according to this restriction  our only freedom to improve bt is in the implementation of the functions getfirstvalue and hasmorevalues.
1	computing candidates
suppose	bt	has	built	a	consistent	instantiation	of
모모모모모모. values that  assigned to   would make the instantiation of consistent  are the candidates for . then bt is sound and complete if getfirstvalue indicates the first candidate  returning false when missing  a leaf dead-end at    dechter and frost  1    and successive calls to hasmorevalues iterate through them  returning false when they have all been enumerated  internal dead-end . testing whether a value is a candidate for is done in at most width    consistency checks.
the iterative method: the most natural method to implement getfirstvalue  resp. hasmorevalues  is to iterate through the domain of   halting as soon as a value pass a consistency check with every in   starting at the first domain value  resp. after the last successful value . both return false when the whole domain has been checked. though in the worst case  the cost of is width   values are checked only when needed: this method is good when the csp is underconstrained or when width    is small.
the partition refinement method:	suppose
모모모모모  and note the domain of . then compute such that contains the values of
satisfying the constraint . then is the set of candidates for . though worst case complexity  when constraints are  full   is the same  it is efficient in practice: if is the constraint tightness of a random network  smith  1   then the expected size of is ; so the expected time to compute is . though some computed candidates may not be used  this method performs well when constraints tightness and variables width are high. the method hasmorevalues only iterates through this result.
storing refinement history: a partial history is a trace of the refinements computed in the current branch of the backtrack tree: so having computed and stored these sets as well as the values of used to obtain them  if we backtrack up to and come back again later to   we only have to refine domains from to   replacing the previous values. this mechanism is implemented in alg. 1.

algorithm 1: getfirstvalue   

data : a variable   belonging to a network where bt has computed a consistent instantiation of all variables preceding . we also suppose that has at least one ancestor.
result : stores refinement history  and returns true unless the computed set of candidates is empty.
last 1; w	width   ;
	y	;
while  last w and	 last 	and usedval last  = y.value  do y	 last ; last++;
	while   last w  and  	 last-1 	   do
 last-1 ;
	usedval last 	y.value;
;
      last-1   do if   y.value  v  constraint y  x   then
	 last 	 last 	v ;
	success	 last = w+1  and  	 last-1 	 ;
if  success  then value	 last-1 ; return success;

모each variable is assigned a field last  denoted  in an oop style  .last or simply last when there is no ambiguity   a field value pointing to the current value of   a vector containing the sorted variables in   and two vectors of size width x +1: contains the successive refinements  and usedval contains the values of the ancestors used for successive refinements of the . note that  k  contains all values of  k-1  consistent with the assignment of usedval k  to the variable ancestors k-1 .  is initialized with the domain of . the partial history mechanism adds the same benefit as backmarking  bm   gaschnig  1 . though more memory expensive  width space  for each variable   than the usual version of bm  it is more resistant to the jumps forward and backward that will be explicited further in this paper.
1	variables orderings compatible with bcc
let us now define the particular variable orderings that will be compatible with bcc. let be ordered subsets of such that  their intersection is not necessarily empty   then predset   = . we call an ordering of compatible with this decomposition if  for every subset   for every variable   if predset   then is greater than every variable of predset . given such an ordering  the accessor of is its smallest element.
1	connected components
a non empty graph is connected if  for every pair of nodes  there is a walk from one node to the other. a connected component of is a maximum connected subset of .
now suppose a network whose constraint graph admits connected components  namely . we call a
cc-compatible ordering of the variables of an ordering of compatible with some arbitrary ordering of these connected components. should we launch bt on this network  and should computeorder store such an ordering  a wellknown kind of thrashing can occur: if bt fails on component
모 i.e. it registers a dead-end at the accessor of    then it will keep on generating every consistent instantiation of the variables in to repeat the same failure on .
모since the represent independent subproblems  a global knowledge on the graph   the usual method is to launch bt on each connected component  stopping the whole process whenever some admits no solution. it could also be implemented in a simple way with a slight modification of the function previouslevel: if is the accessor of a connected component  previouslevel    must return 1.
모though very simple  this example illustrates well the method we use later: introduce in the variables ordering some  global  properties of the graph  then use them during backtrack. with a cc-compatible ordering  we benefit from the independence of the sub-problems associated to the connected components; with bcc-compatible orderings  we will benefit from  restricted dependence  between these sub-problems.
1	biconnected components
a graph is -connected if the removal of any different nodes does not disconnect it. a -connected component of is a maximum -connected subgraph of . a biconnected component  or bicomponent   is a 1-connected component. note we consider the complete graph on two vertices biconnected. the graph of bicomponents  obtained by representing each bicomponent as a node  then adding an edge between two components if they share a node  is a forest called the bcc tree of . it is computed in linear time  using two depth-first search  dfs  traversals of  tarjan  1 . fig. 1 represents a constraint graph and its associated bcc tree.
모if we consider the bcc tree as a rooted forest  a natural ordering of its nodes  representing bicomponents  is a total order such that children are greater than their parent. for the bcc tree in fig. 1  choosing and as roots  a dfs could give the order   and a bfs  breadth-first search  . both dfs and bfs traversals result in a natural ordering.

figure 1: a constraint graph and its bcc tree.

figure 1: a bcc dfs  bfs  ordering.
모a bcc-compatible ordering of the variables of a network is an ordering compatible with a natural ordering of its bcc tree  it is also cc-compatible . such an ordering  if computed by a dfs traversal of the bcc tree  and by a bfs traversal inside the components  would be denoted by a bcc dfs  bfs  ordering. it can be computed in linear time.
모nodes of the graph in fig. 1 have been ordered by a bcc dfs  bfs  ordering. accessors of a bicomponent have been represented in white. the second variable of a bicomponent is the smallest variable in this component  its accessor excepted. by example  in fig. 1  1 is the second variable of the component . given a bcc-compatible ordering  the accessor of a variable is defined as the accessor of the smallest  according to the natural ordering  bicomponent in which it belongs. a second variable is always chosen in the neighborhood of its accessor. tab. 1 gives the accessors determined by the bcc dfs  bfs  ordering illustrated in fig. 1.
variable11111	1	1	1	1accessor11111	1	1	1	1table 1: accessors determined by the ordering of fig. 1.
모suppose a natural ordering of a bcc tree nodes. a leaf variable is the greatest variable appearing in a leaf of the bcc tree. in fig. 1  leaf variables are coloured in grey. we say that a bicomponent covers a leaf variable if is the greatest node appearing in the subtree rooted by  including itself . now  we define the compilers of a leaf variable as the accessors of the bicomponents covering . for the ordering in fig. 1  compilers are: cmp   cmp   cmp and cmp .
1	the algorithm bcc
the kernel of bcc is bt  as described in sect. 1. the call to computeorder orders the variables in a bcc-compatible way  and stores all information on accessors  leaf variables and compilers. this preprocessing takes linear time. we suppose bt is running on a network .
1	theoretical foundations
theorem 1 if is the second variable of some bicomponent  any call to previouslevel    can safely  and optimally  return the index of its accessor .
모moreover  no solution of	contains the variable	instantiated with its current value.
모a call to previouslevel    means that we have detected some dead-end  either internal or leaf  at variable . the first part of this theorem asserts that we can backjump right to the accessor of   without missing any possible solution  and that it could be dangerous to backjump any further  formal definitions about safe and optimal backjumps can be found  by example  in  dechter and frost  1  . a dead-end found at variable 1 in fig. 1 would result in a backjump to variable 1. the second part of the theorem means that we can permanently remove the current value of from its domain.
proof of theorem 1: the first part is proved by showing that previouslevel    performs a particular case of graphbased backjumping  gbbj   dechter  1 . the accessor of a second variable is the only variable in . gbbj performs a safe backjump to the greatest predecessor of   we have no other choice than . it also adds to a  temporary neighborhood  containing all other predecessors of  sect. 1 . this set being empty  not maintaining it is safe.
모second part relies on the ordering. the following lemma locates the leaf dead-ends responsible for the dead-end at .
lemma 1 let be the set of variables that registered a dead-end after the last successful instantiation following . then variables in belong to the subtree of bicomponents rooted by the smallest component containing  the bcc subtree of   the accessor of is also called the accessor of  .
sketch of proof: a failure is  propagated upward  in the same bicomponent until a second variable  since variables in the same component  the accessor apart  are numbered consecutively by the compatible ordering . at this point  failure is propagated to the accessor  the backjump in the first part of the theorem   and this accessor belongs to a parent component  a consequence of the natural ordering .
모now suppose there is a solution such that is assigned the value   and we prove it is absurd. let us now consider the subnetwork of   containing all the variables in the the bcc subtree of   ordered in the same way as in . let us reduce the domain of in to . then admits a solution  and bt will find the first one for the given ordering. let us now examine the first leaf dead-end  at variable   that caused to bypass this solution in the bt
on  thanks to lem. 1  it is a variable of  . since we had just before a part of the solution of   the domain of has been emptied by a variable outside  or it would not have been a solution of  . so there is a constraint between and some variable in a biconnected component outside of
: this is absurd  since there would be an elementary cycle
	where	appears in some an-
cestor of both the component of and the one of . then and would be in the same bicomponent.
모since no variable smaller than is responsible for the deadend registered at   the backjump is optimal.
theorem 1 we suppose previouslevel has been implemented to match specifications of th. 1. if is a leaf variable numbered   any call to nextlevel     before returning   can mark  for every variable being a compiler of
  that the current value of has been compiled and that is the forward jump  fj  for that value  eventually eras-
ing previous fj for that value .
모thereafter  every time a call to nextlevel should return the index of a second variable   if the current value of its accessor has been compiled  nextlevel can safely  and optimally  return instead the forward jump for .
sketch of proof: this is the dual of th. 1 second part: we had shown that when some failure propagates to the accessor of a bcc subtree   then no modification outside can give a consistent instantiation of   while keeping current value. now  when we have found a consistent instantiation of   then no modification outside can make all instantiations of inconsistent  unless modifying the current value of .
모a call to nextlevel on a leaf means that a whole bcc subtree has been successfully instantiated. in fig. 1  a call to nextlevel 1  means we have a consistent instanciation of the bcc subtree rooted by the bicomponent covering 1. before returning 1  this call will store in the accessors of
and their current values and   and mark that the fj for these compiled values is 1. now we go to 1 and begin to instantiate the subtree containing 1  already instantiated  and even compiled  and 1  ...  1. if there is a dead-end at 1  we should backjump to 1  see th. 1   and permanently remove its current value  we must also remove it from its compiled values . otherwise  it means we can successfully instantiate this subtree until leaf 1  then make a call to nextlevel. this call updates all information for compilers of 1  and in particular in 1: the fj for the compiled value is now 1. now suppose we have some dead-end at 1. according to th. 1  we can backjump to 1. now suppose further failures make us backtrack to 1  that we successfully instantiate  and that we also instantiate 1 with its compiled value. should a call to nextlevel return 1  it would mean that we found a consistent instantiation of the whole component . then we know there is a consistent instantiation of 1 ...  1   and we can safely jump forward to 1.
1	implementation
functions previouslevel and nextlevel  alg. 1  1  naturally encode the above theorems. nextindex and previousindex assume the previous role of nextlevel and previouslevel  returning  .

algorithm 1: previouslevel   

	data	: a variable	.
	result	: the index of the variable to be examined by bcc after a dead-end.
if  accessor    = x  then return 1; if  issecondvariable       then
accessor accessor   ; accessor.value ;
for  i = 1; i width accessor ; i++  do accessor.  i  accessor.  i  val ;
   return getindex  accessor    ; return previousindex    ;

algorithm 1: nextlevel   

	data	: a variable	.
	result	: the index of the variable to be examined by bcc after a success.
index nextindex   ; if  isleaf      then compilers    do
	.compiledvalues	.compiledvalues	.value ;
addkey/value  .fj    .value  index  ;
	if  index	  then
             v index ; if  issecondvariable  nextvariable   then
accessor accessor nextvariable ; if  accessor.value accessor.compiledvalues then
모모모모index getkey/value accessor.fj  accessor.value ; return index;

1	worst-case complexity
lemma 1 using the bcc algorithm  any bcc subtree is entered at most once for each value in its accessor's domain.
proof: a consequence of th. 1 and 1. should we enter a
bcc subtree	  either we find a consistent instantiation of
   and the current value for its accessor is compiled  so every time nextlevel should return the second variable for the smallest bicomponent containing   it jumps instead to the next subtree ; or we have registered some dead-end at  thanks to lem. 1  this failure came from    and the current value for the accessor is permanently removed.
corollary 1 when is a tree  bcc runs at most in   where and is the greatest domain size.
proof: bicomponents are complete graphs with two nodes  accessor and second variable  . they are entered on   and there will be at most consistency checks on . there is bicomponents  each one entered at most time  lem. 1 .
모this result is the same as the one given in  freuder  1   obtained by first achieving arc consistency      then by a backtrack-free search. preprocessing required for bcc is linear  and all possible dead-ends can be encountered  but bcc benefits as much from these failures as from successes. this complexity is optimal  dechter and pearl  1 . th. 1 extends this result to any constraint graph.
theorem 1 let be the number of bicomponents  be the size of the largest one  and be the size of the largest domain  then bcc runs in the worst case in .
모though this worst-case complexity is the same as  freuder  1   bcc should be more efficient in practice: it backtracks along bicomponents instead of generating all their solutions.
1	bcc and other algorithms
though bcc is efficient when the constraint graph contains numerous bicomponents  it is nothing more than a bt inside these components. we study in this section how to improve bcc with some well-known bt add-ons  and point out that bcc reduces the search tree in an original way.
1	backjumping schemes
to be sound  bcc must perform a very limited form of backjumping  bj   see th. 1 . the natural question is: what if we use more performant backjumps 
gaschnig's backjumping  gbj   gaschnig  1  is only
triggered in case of a leaf dead-end. it jumps back to the first predecessor of that emptied its domain. this backjump is safe and optimal in case of a leaf dead-end.
graph-based backjumping  gbbj   dechter  1  is triggered on any dead-end   and returns its greatest predecessor . safety is ensured by adding to a temporary set  jumpback set  of predecessors consisting of all the predecessors of . it is optimal when only graph information is used.
conflict directed backjumping  cdbj   prosser  1  integrates gbj and gbbj. it backjumps at any dead-end to the greatest variable that removed a value in the domain of . the jumpback set contains only the predecessors of that removed some value in . cdbj is safe and optimal.
theorem 1 let previous    be a function implementing a safe backjump scheme xbj  always returning the index of a variable belonging to a bicomponent containing . then if is the accessor of   and if no further backjump from will return in the component of   the current value of can be permanently removed.
모the obtained algorithm bcc+xbj is sound and complete. idea of proof: restrictions enable to paste proof of th. 1.
corollary 1 the algorithms bcc+gbj  bcc+gbbj and bcc+cdbj are sound and complete.
sketch of proof: see that bcc+gbbj and bcc+cdbj respect the specifications in th. 1  and that a backjump on an accessor makes us remove a value only if the jumpback set of only contains its predecessors. note also that gbj does not always backjump in the same component  a second variable can backjump to the last element of the parent component  not necessarily the accessor   so bcc specific backjump must be kept to make bcc+gbj sound and complete.
모finally  we note that these bj schemes recover more quickly from a failure than bt  but nothing ensures that this failure will not be repeated over and over. bcc's behaviour cannot be obtained by such a bj mechanism.
1	filtering techniques
different levels of arc consistency can be used in algorithms to prune values that become inconsistent ahead of the last instantiated variable. a constraint is said arc-consistent if 
	 	  and conversely.
maintaining arc-consistency  mac   sabin and freuder  1  is believed to be the most efficient general csp algorithm. this family of algorithms rely on maintaining some kind of arc-consistency ahead of the current variable.
theorem 1 let be the variable currently studied by mac  and be a variable whose domain was emptied by the ac phase. let be the bicomponents belonging to the branch of the bcc tree  between the component of and the component of . let be their acessors. then all values from their domains that were not temporarily removed by the ac phase can be definitively removed by bcc.
sketch of proof: consider the accessor of the component containing . if ac propagation has emptied the domain of   consider the filtered domain of . this failure means that  should we extend our current instantiation of all variables until with any assignment of to   this instantiation does not extend to a solution. thanks to th. 1  no instantiation of with the value extends to a solution.
모finally  having proven that bcc and mac combine their effort to produce a good result  we point out that mac alone cannot replace bcc's efficiency: perhaps mac will never consider a value that had to be remove by bcc  but should it consider it  nothing will restrain mac to fail again on it.
1	conclusion and future works
we have presented here a variation on bt called bcc  whose efficiency relies on the number of bicomponents of the constraint graphs. early tests have shown that  the number of components increasing  bcc alone quickly compensates its inefficiency inside a component and its results are more than a match for other bt improvements. moreover  we have shown that bcc itself could be added some well-known bt improvements  such as bj  fc or mac  effectively combining the best of two worlds. so an improved bcc could be the perfect candidate for networks whose constraint graph contains many bicomponents.
모however  we must confess that such constraint graphs are not so common... so  apart from an original theoretical result expressing that a network whose constraint graph is a tree does not need an ac phase nor a backtrack-free search to be solved in   what can bcc be used for 
let be a binary constraint network of size   and and be two of its variables. then we can transform into an
equivalent network of variables  by fusioning and into a single variable . the domain of is composed of the pairs of compatible values in the constraint  it is
   when there is no such constraint . any constraint is then transformed in the following way: if   then all pairs such that belongs to the domain of are possible values for the constraint between and . the same construction is performed for constraints incident to . the obtained network is equivalent  but has now variables  and its maximum domain can have now size  incident constraints can have size  . we can iteratively fusion different nodes  obtaining a network of size   but where a variable domain can have size .
모if a constraint graph admits a -separator  a set of nodes whose removal disconnects the graph   then  by fusioning these variables as indicated above  we create at least two bicomponents sharing the obtained variable. suppose that we can find in some way separators of size k  the fusion of these separators creating bicomponents of size . then bcc  without any improvement   will run in time . this decomposition may be quite efficient when is small and the separators cut the graph in a non degenerate way  keeping the size of components comparable. this decomposition method  where polynomial cases are obtained when and are both bounded by a constant  is still to be compared to the ones studied in  gottlob et al.  1 .
모to implement and test this decomposition method  we are currently looking for an heuristic that  given an undirected simple graph with weighted edges  find a  small  separator of the graph such that:
 1  removal of the separator creates a graph whose greatest connected component is as small as possible;
 1  product of the weight on edges that belong to the separator is as small as possible.
the weight on edges will be initialized by the constraint tightness. we believe that networks designed by a human being  who decomposes a problem into subproblems slightly related to each other  will be good candidates for this heuristic.
acknowledgements
we would like to thanks christian bessie`re for his bibliographical suggestions  michel chein and michel habib who helped making the concept of  semi-independant subgraphs  evolve into the much more formal one of -connected components  as well as the anonymous referees  for their precious comments and advices.
references
 dechter and frost  1  r. dechter and d. frost. backtracking algorithms for constraint satisfaction problems. ics technical report  university of california  1.
 dechter and pearl  1  r. dechter and j. pearl. networkbased heuristics for constraint satisfaction problems. artficial intelligence  1 :1  1.
 dechter  1  r. dechter. enhancement schemes for constraint processing: backjumping  learning  and cutset decomposition. artificial intelligence  1 :1  january 1.
 freuder  1  e. c. freuder. a sufficient condition for backtrack-free search. journal of the acm  1 :1  january 1.
 gaschnig  1  j. gaschnig. performance measurement and analysis of certain search algorithms. research report cmu-cs-1  carnegie-mellon university  1.
 gottlob et al.  1  g. gottlob  n. leone  and f. scarcello. a comparison of structural csp decomposition methods. in proc. of ijcai'1  pages 1  1.
 mackworth and freuder  1  a.k. mackworth and e. c. freuder. the complexity of some polynomial network consistency algorithms for constraint satisfaction problems. artificial intelligence  1  1.
 prosser  1  p. prosser. hybrid algorithms for the constraint satisfaction problem. computational intelligence  1 :1  1.
 sabin and freuder  1  d. sabin and e. c. freuder. contradicting conventional wisdom in constraint satisfaction. in proc. of ecai'1  pages 1  1.
 smith  1  b. smith. locating the phase transition in binary constraint satisfaction problems. artificial intelligence  1-1  1.
 tarjan  1  r. e. tarjan. depth-first search and linear graph algorithms. siam j. of computing  1-1  1.
a constraint satisfaction approach to parametric differential equations
	micha janssen	pascal van hentenryck	yves deville
	ingi-ucl	brown university	ingi-ucl
louvain-la-neuve  belgium	providence  ri	louvain-la-neuve  belgiumabstract
initial value problems for parametric ordinary differential equations arise in many areas of science and engineering. since some of the data is uncertain and given by intervals  traditional numerical methods do not apply. interval methods provide a way to approach these problems but they often suffer from a loss in precision and high computation costs. this paper presents a constraint satisfaction approach that enhances interval methods with a pruning step based on a global relaxation of the problem. theoretical and experimental evaluations show that the approach produces significant improvements in accuracy and/or efficiency over the best interval methods.
1	introduction
initial value problems for ordinarydifferential equations arise naturally in many applications in science and engineering  including chemistry  physics  molecular biology  and mechanics to name only a few. an ordinary differential equation
 ode 	is a system of the form
often denoted in vector notation by	or
모모모모모. in addition  in practice  it is often the case that the parameters and/or the initial values are not known with certainty but are givenas intervals. hence traditional methods do not apply to the resulting parametric ordinary differential equations since they would have to solve infinitely many systems. interval methods  pioneered by moore  moo1   provide an approach to tackle parametric odes as they return reliable enclosures of the exact solution at different points in time. interval methods typically apply a one-step taylor interval method and make extensive use of automatic differentiation to obtain the taylor coefficients  moo1 . their major problem however is the explosion of the size of the boxes at successive points as they often accumulate errors from point to point and lose accuracy by enclosing the solution by a box  this is called the wrapping effect . lohner's awa system  loh1  was an important step in interval methods which features coordinate transformations to tackle the wrapping effect. more recently  nedialkov and jackson's iho method  nj1  improved on awa by extending a hermiteobreschkoff's approach  which can be viewed as a generalized taylor method  to intervals. note that interval methods inherently accommodate uncertain data. hence  in this paper  we talk about odes to denote both traditional and parametric odes.
모this research takes a constraint satisfaction approach to odes. its basic idea  djvh1; jdvh1  is to view the solving of odes as the iteration of two processes:  1  a forward process that computes initial enclosures at given times from enclosures at previous times and bounding boxes and  1  a pruning process that reduces the initial enclosures without removing solutions. the real novelty in our approach is the pruning component. pruning in odes however generates significant challenges since odes contain unknown functions.
모the main contribution of this paper is to show that an effective pruning technique can be derived from a relaxation of the ode  importing a fundamental principle from constraint satisfaction into the field of differential equations. three main steps are necessary to derive an effective pruning algorithm. the first step consists in obtaining a relaxation of the ode by safely approximatingits solution using  e.g.  generalized hermite interpolation polynomials. the second step consists in using the mean-value form of this relaxation to solve the relaxation accurately and efficiently. unfortunately  these two steps  which were skeched in  jdvh1   are not sufficient and the resulting pruning algorithm still suffers from traditional problems of interval methods. the third fundamental step consists in globalizing the pruning by considering several successive relaxations together. this idea of generating a global constraint from a set of more primitive constraints is also at the heart of constraint satisfaction. it makes it possible  in this new context  to address the problem of dependencies  and hence the accumulation of errors  and the wrapping effect simultaneously.1
모theoretical and experimental results show the benefits of the approach. the theoretical results show that the pruning step can always be implemented to make our approach faster than existing methods. in addition  it shows that our approach should be significantly faster when the function is very complex. experimental results confirm the theory. they show that the approach often produces many orders of magnitude improvements in accuracy over existing methods while not increasing computation times. alternatively  at similar accuracy  other approaches are significantly slower. of particular interest is the fact that our approach is not dependent on high-order taylor coefficients contrary to other methods.
모the rest of the paper is organized as follows. section 1 introducesthe main definitions and notations. sections 1  1  and 1 describe the pruning component. sections 1 and 1 present the theoretical and experimental analyses.
1	background and definitions
basic notational conventions small letters denote real values  vectors and functions of real values. capital letters denote real matrices  sets  intervals  vectors and functions of intervals. denotes the set of all closed intervals
whose bounds are floating-point numbers. a vector of intervals is called a box. if   then  denotes the smallest interval such that . if   then
 . in this paper  we often use	instead of for simplicity. if	  then	denotes the smallest box such that	. we also assume that	  and	are reals 	are in	  and	are in	. finally  we use	to denote	 	to denote and	to denote	.
모we restrict attention to odes that have a unique solution for a given initial value. techniques to verify this hypothesis numerically are well-known  moo1; djvh1 . moreover  in practice  the objective is to produce  an approximation of  the values of the solution of at different points
           . this motivates the following definition of solutions and its generalization to multistep solutions.
definition 1  solution of an ode  the solution of an ode is the function	such that satisfies	for an initial condition	.
definition 1  multistep solution of an ode  the multistep solution of an ode is the partial function
if
where	is the solution of	and is undefined otherwise.
since multistep solutions are partial functions  we generalize the definition of interval extensions to partial functions.
definition 1  interval extension of a partial function 
the interval function is an interval extension of the partial function if
where	.
finally  we generalize the concept of bounding boxes  a fundamental concept in interval methods for odes  to multistep methods. intuitively  a bounding box encloses all solutions of an ode going through certain boxes at given times.

figure 1: successive integration steps
definition 1  bounding box  let	be an ode system 
be the multistep solution of   and . a box is a bounding box of over wrt       if  for all  	.
1	the constraint satisfaction approach
the constraint satisfaction approach followed in this paper was first presented in  djvh1 . it consists of a generic algorithm for odes that iterates two processes:  1  a forward process that computes initial enclosures at given times from enclosures at previous times and bounding boxes and  1  a pruning process that reduces the initial enclosures without removing solutions. the forward process also provides numerical proofs of existence and uniqueness of the solution. the intuition of the successive integration steps is illustrated in figure 1. forward components are standard in interval methods for odes. this paper thus focuses on the pruning process  the main novelty of the approach. our pruning component is based on relaxations of the ode as described in the next section. to our knowledge  no other approach uses relaxations of the ode to derive pruning operators and the only other approach using a pruning component  nj1; rih1  was developed independently.
1	pruning
the pruningcomponentuses safe approximationsof the ode to shrink the boxes produced by the forward process. to understand this idea  it is useful to contrast the constraint satisfaction to nonlinear programming  vhmd1  and to ordinary differential equations. in nonlinear programming a constraint can be used almost directly for pruning the search space  i.e.  the cartesian products of the intervals associated with the variables  . it suffices to take an interval extension of the constraint. now if does not hold  it follows  by definition of interval extensions  that no solution of lies in .
the interval extension can be seen as a filter that can be used for pruning the search space in many ways. for instance  numerica uses box   -consistency on these interval constraints  vhmd1 . ordinary differential equations raise new challenges. in an ode functions u and u' are  of course  unknown. hence it is not obvious how to obtain a filter to prune boxes.
u

figure 1: geometric intuition of the multistep filter
모one of the main contributions of our approach is to show how to derive effective pruning operators for parametric odes. the first step consists in rewriting the ode in terms of its multistep solution ms to obtain

let us denote this formula this rewriting may not appear useful since ms is still an unknown function. however it suggests a way to approximate the ode. indeed  we show in section 1 how to obtain interval extensions of ms and  by using polynomial interpolations together with their error terms. this simply requires a bounding box for the considered time interval and safe approximations of ms at successive times  both of which are available from the forward process. once these interval extensions are available  it is possible to obtain an interval formula of the form
which approximates the original ode. the above formula is still not ready to be used as a filter because is universally quantified. the solution here is simpler and consists of restricting attention to a finite set of times to obtain the relation
which produces a computable filter. indeed  if the relation does not hold for a time   it follows that no solution of can go through boxes
at times . the following definition and proposition capture these concepts more formally.
definition 1  multistep filters  let	be an ode and s its
solution. a multistep filter for	is an interval relation satisfying
proposition 1  soundness of multistep filters  let be an ode and let be a multistep filter for . if does not hold for some   then there exists
no solution of	going through	at times	.
모how can we use this filter to obtain tighter enclosures of the solution  a simple technique consists of pruning the last box produced by the forward process. assume that is a box enclosing the solution at time     and that we are interested in pruning the last box	. a subbox can be pruned away if the condition
does not hold for some evaluation point . let us explain briefly the geometric intuition behind this formula by considering what we call natural filters. given interval extensions ms and dms of ms and   it is possible to approximate the
ode	by the formula
in this formula  the left-hand side of the equation represents the approximation of the slope of while the right-hand represents the slope of the approximation of . since the approximations are conservative  these two sides must intersect on boxes containing a solution. hence an empty intersection means that the boxes used in the formula do not contain the solution to the ode system. figure 1 illustrates the intuition. it is generated from an actual ordinary differential equation  considers only points instead of intervals  and ignores error terms for simplicity. it illustrates how this technique can prune away a value as a potential solution at a given time. in the figure  we consider the solution to the equation that evaluates to and at and respectively. two possible points and are then considered as possible values at . the curve marked ko describes an interpolation polynomial going through at times . to determine if is the value of the solution at time   the idea is to test if the equation is satisfied at time .  we will say more about how to choose later in this paper . as can be seen easily  the slope of the interpolation polynomial is different from the slope specified by at time and hence cannot be the value of the solution at . the curve marked ok describes an interpolation polynomial going through at times . in this case  the equation is satisfied at time   which means that cannot be pruned away. the filter proposed earlier generalizes this intuition to boxes. both the left- and the right-hand sides represent sets of slopes and the filter fails when their intersection is empty. traditional consistency techniques and algorithms based on this filter can now be applied. for instance  one may be interested in updating the last box producedby the forward process using the operator
the following definition is a novel notion of consistency for odes to capture pruning of the last boxes.
definition 1  forward consistency of multistep filters 
let fl be a multistep filter. fl is forward   -consistent wrt
	 	and	if
the algorithm used in our computational results enforces forward consistency of the filters we now describe.
1	filters
filters rely on interval extensions of the multistep solution and its derivative wrt . these extensions are  in general  based on decomposing the  unknown  multistep solution into the sum of a computable approximation and an  unknown  error term   i.e. 
there exist standard techniques to build	 and to bound
모. section 1 reviews how they can be derived from generalized hermite interpolation polynomials. here we simply assume that they are available and we show how to use them to build filters.
모in the previous section  we mention how natural multistep filters can be obtained by simply replacing the multistep solution and its derivative wrt by their interval extensions to obtain
it is not easy however to enforce forward consistency on a natural filter since the variables may occur in complex nonlinear expressions. in addition  in odes  the boxes are often small which makes mean-valueforms particularlyinteresting.
1	mean-value filters
the mean-value theorem for a function	states that
a mean-value interval extension	of	in interval	can then be defined as
for some in   where is an interval extension of the derivativeof . in order to obtain a mean-valuefilter  consider the ode
where the multistep solution has been expanded to make the approximations and the error terms explicit. we are thus interested in finding a mean-value interval extension wrt of the function defined as
however  as mentioned earlier  is not a computable function and we cannot compute its derivative wrt . fortunately  it is possible to construct a mean-value filter by considering the error terms as constants and taking a mean-value interval
extension of the function	defined as
if is a mean-value interval extension of   then the ode is approximated by an equation of the form
where and are interval extensions of the error term and of its derivative wrt . in the following  we call the
resulting filter an implicit mean value filter.
모the implicit mean-value filter for a time produces an interval constraint of the form
where each and are vectors of elements and each is an matrix. assuming that we are interested in pruning the last box   we obtain an operator as
 the implicit mean-value filter can be turned into explicit form by inverting the matrix .

figure 1: intuition of the global filter
definition 1  explicit mean-value filter  let be an ode and rewrite an implicit mean-value filter for as
where	is an	matrix and	a vector of	elements.
an explicit mean-value filter for	is given by
it is simple to make an explicit mean-value filter forward 1 consistent  since the variables in have been isolated.
1	global filters
the explicit mean-value filter produces significant pruning of the boxes but it still suffers from a dependency problem typical of interval methods. consider the application of the filter at two successive time steps of the method. we obtain expressions of the form
note that the second expression considers all tuples of the form	to compute . but only a subset of these tuples satisfy the first equation for a given . the key idea to remove this dependency problem is to use a global filter which clusters a sequence of mean-value filters together. figure 1 gives the intuition for . a global filter is then obtained by predicting and pruning boxes simultaneously. more precisely  the idea is to cluster the successive filters
this filter can be turned into an explicit form by substituting in the second equation  and in the third equation
and so on. the resulting system can be written as a system
where	is a square	matrix and	is a vector of
elements. it is important to mention that such a global filter is forward   -consistent wrt and   since the variables have been isolated. the dependency problem has thus been reduced to the well-known wrapping effect which has standard solutions in interval methods.
1	the wrapping effect
approximating the set of solutions to an equation by a box may induce a significant loss of precision. this problem is known as the wrapping effect and its standard solution is to choose an appropriate coordinate system to represent the solutions compactly. let us review briefly how to integrate this idea with our proposals. consider a global filter that we write as . the idea is to introduce a coordinate transformation to obtain a system where is diagonally dominant  i.e.  the non-diagonalelements are very small compared to the diagonal. if is diagonally dominant  then the wrapping effect induces no or little loss of precision in computing . our pruning step can easily accommodate this idea. iteration defines and it sends   and to iteration of the main algorithm. iteration
will have to solve the system
a coordinate transformation is applied once again to obtain
so that is diagonally dominant. the matrices are computed using qr-factorizations  loh1 .
1	hermite filters
we now show how to build interval extensions of       by using hermite interpolation polynomials sb1 . informally  a hermite interpolation polynomial approximates a continuously differentiable function and is specified by imposing that its values and the values of its successive derivatives at various points be equal to the values of and of its derivatives at the same points. note that the number of conditions at the different points may vary.
definition 1  hermite    interpolation polynomial  let and
                     . consider the ode	. let	. the
hermite    interpolation polynomial wrt and   is the unique polynomial of degree satisfying
it is easy to take interval extensions of a hermite interpolation polynomial and of its derivatives. the only remaining issue is to bound the error terms. the following standard theorem  e.g.   sb1    atk1   provides the necessary theoretical basis.
theorem 1  hermite error term  let	be	the
hermite    interpolation polynomial wrt	and	. let
	 	 
and	. then  for	 
		;
		.
모how to use this theorem to bound the error terms  it suffices to take interval extensions of the formula given in the theorem and to replace by and by a bounding box for the ode over . in the following  we call hermite    filters  filters based on hermite    interpolation and we denote a global hermite    filter by ghf   . the following novel result is fundamental and specifies the speed of convergence of hermite filters. it
cost-1cost-1ihoghfghf-1ghf-1table 1: complexity analysis.
shows that the order of natural and mean-value hermite    filters is the sum of the elements in   i.e.  the number of conditions imposed on the interpolation.
proposition 1  order  let	be a natural or mean-value
hermite    filter for ode	. let	 
	 	and
. then  under
some weak assumptions on	and	 
1	theoretical cost analysis
we now analyse the cost of our method and compare it to nedialkov's iho    method  nj1   the best interval method we know of. we use the following assumptions. at each step  the forward process uses moore's taylor method and the pruning component applies a global hermite filter together with coordinate transformations  using lohner's qrfactorizationtechnique . for simplicity of the analysis  we assume that  the natural encoding of  function contains only arithmetic operations. we denote by the number of operations in   by the number of operations  and by the sum . we also assume that the cost of evaluating is times the cost of evaluating . we define as max  	.
we also report separately interval arithmetic operations involved in  1  products of a real and an interval matrix  cost1  and  1  the generation of jacobians  cost-1 . table 1 reports the main cost of a step in the iho    method  iho in the table  and our method ghf     ghf in the table . it also shows the complexity of two particular cases of ghf   . the first case  ghf-1  corresponds to a polynomial with only two interpolation points      while the second case corresponds to a polynomial imposing two conditions on every interpolation points    . note that the methods are of the same order in all cases.
모the first main result is that ghf-1 is always cheaper than iho  which means that our method can always be made to run faster by choosing only two interpolation points.  the
next section will show that substantial improvement in accuracy is also obtained in this case . ghf-1 is more expensive than ghf-1 and iho when is simple. however  when contains many operations  which is often the case in practical applications   ghf-1 can become substantially faster because cost-1 in ghf-1 is independent of and cost-1 is substantially smaller in ghf-1 than in ghf-1 and iho. it also shows the versatility of the approach that can be taylored to the application at hand.
ivpghfihoprecisiontimeihoghfratioihoghfhilb 1 1 11e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-111brus 1 1 11e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-111bio1 1 1 11e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-1111bp 1 11e-1.1e-1.1e-1111e-1.1e-1.1e-111 1 11e-1.1e-1.1e-1111e-1.1e-1.1e-1111bp 1 1 11e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-111 1 1 11e-1.1e-1.1e-1111e-1.1e-1.1e-111lor 1 11e-1.1e+1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-1111e-1.1e-1.1e-111bio1 1 11e-1.1e-1.1e-1.1.1.11e-1.1e-1.1e-1.1.1.11e-1.1e-1.1e-111table 1: experimental results.
1	experimental analysis
we now report experimental results of our c++ implementation on some standard benchmarks  hnw1; loh1  and two molecular biology problems  which are real-life parametric odes given to us by a biologist:
hilbert quadratic problem  hilb : 1  1.1 full brusselator  brus : 1 1  1 two-body problem  1bp : 1 1  1 three-body problem 1bp : 1 1 -1  1.1 lorentz system  lor : 1 1  1 molecular biology problem  bio1 : 1 1 1  1 molecular biology problem  bio1 : 1.1.1.1  1 table 1 compares ghf    and iho    methods of the same order. it reports the precision at the last step and execution time  in seconds  of both methods for the same  constant  stepsize. the experimental results follow the same assumptions as in the theoretical analysis. the forward process uses moore's taylor method of order  same order as the predictor used in iho     and a taylor series method of order to compute a bounding box  except for 1bp and 1bp where we use a series of order 1. the choice of the evaluation time involved in ghf    has not been discussed yet. so far we have no theoretical result about the optimal choice of . we use a simple binary search algorithm to determine a good value for at the beginning of or during the integration. in our experiments  we chose between the last two interpolation points  keeping the distance constant throughout the integration. our results could be further improved by using a variable distance.
모the results indicate that our method produces orders of magnitude improvements in accuracy and runs faster than the best known method. the gain in precision is particularly significant for lower orders. the theoretical results are also confirmed by the experiments. when contains many operations  e.g. in 1bp   using many interpolation points is particularly effective. for very complex functions  the gain in computation time could become substantial. when is simple  using few interpolation points becomes more interesting.
모as a consequence  we believe that a constraint satisfaction approach to parametric ordinary differential equations is a very promising avenue that complements well traditional approaches. the main open issue at this point is the choice of an optimal evaluation point. this will be the topic of our research in the coming months.
acknowledgment
this research is partially supported by the actions de recherche concertee뫣 arc/1-1and an nsf nyi award. special thanks to philippe delsarte for interesting discussions and for his detailed comments on this paper.
references
 atk1 k. e. atkinson an introductionto numerical analysis. john wiley & sons  new york  1. cb1 j. cruz  and p. barahona. an interval constraint approach to handle parametric ordinary differential equations for decision support. proceed-ings of ekbd-1  1nd international workshop on extractionof knowledge from data bases  1  1.
 djvh1  y. deville  m. janssen  and p. van hentenryck. consistency techniques in ordinary differential equations. in cp'1  pisa  italy  october 1.
 hnw1  e. hairer  s.p. n rsett  g. wanner. solving ordinary differential equations i. springer-verlag  berlin  1.
 jdvh1  m. janssen  y. deville  and p. van hentenryck. multistep filtering operators for ordinary differential equations. in cp'1  alexandria  va  october 1.
 loh1 lohner r. j. enclosing the solutions of ordinary initial and boundary value problems. in computer arithmetic: scientific computation and programming languages  wiley  1. moo1 r.e. moore. methods and applications of interval analysis. siam publ.  1. nj1 n.s. nedialkov and k.r. jackson. an interval hermite-obreschkoff method for computing rigorous bounds on the solution of an initial value problem for an ode  developments in reliable computing  kluwer  1. rih1 r. rihm. implicit methods for enclosingsolutions of odes. j. of universal computer science  1   1. sb1 j. stoer and r. bulirsch. introduction to numerical analysis. springer-verlag  new york  1. vhmd1  p. van hentenryck  l. michel  and y. deville. numerica: a modeling language for global optimization. the mit press  cambridge  mass.  1.
improved bounds on the complexity of kb-consistency
	lucas bordeaux  eric monfroy  and fred뫣 eric뫣	benhamou
모모모irin  universite뫣 de nantes  france  bordeaux  monfroy  benhamou  irin.univ-nantes.frabstract
   -consistencies form the class of strong consistencies used in interval constraint programming. we survey  prove  and give theoretical motivations to some technical improvements to a naive consistency algorithm. our contribution is twofold: on the one hand  we introduce an optimal consistency algorithm whose time-complexity of improves the known bound by a factor   is the number of constraints  is the number of variables  and is the maximal size of the intervals of the box . on the other hand  we prove that improved bounds on time complexity can effectively be reached for higher values of . these results are obtained with very affordable overheads in terms of space complexity.
모keywords: strong consistency techniques  interval constraint propagation.
1	introduction
consistency algorithms are polynomial techniques which aim at struggling against the combinatorial explosion of bruteforce search by reducing the search space. among them  strong consistencies  freuder  1  denote generic algorithms which can be tuned to enforce arbitrary intermediate levels of consistency. one of the most  if not the most  promising strong consistency techniques is the class of consistencies  where may vary from to a maximum value corresponding to global consistency.
     -consistencywas introduced in  lhomme  1 as a response to the problem called early quiescence  davis  1 : when applied to non-trivial  real-valued problems  filtering techniques adapted from arc-consistency do not reduce the search space enough in practice  and stronger consistencies are thus needed to limit the size of the search tree. the basic idea is that given a solver of level   each possible value of the variables can be enumerated; if the solver proves the inconsistency of the problem obtained when variable is instantiated to value   we know that is not a possible value for . this method can be applied to both domainbased solvers  by suppressing inconsistent values from domains  and interval-based solvers  by tightening bounds .
모the domain-consistency obtained  called singletonconsistency has been emphasized as one of the most efficient techniques for hard finite-domain problems  debruyne and bessie`re  1 . this paper deals with the interval-consistency obtained  called -consistency.
   -consistency has been successfully applied to difficult nonlinear problems  such as a challenging transistor design application  puget and van hentenryck  1 . the important advantage of over path-consistency  montanari  1  and its variants  dechter and van beek  1; sam-haroud and faltings  1  is that it does not require the use of the costly multi-dimensional representations of the constraints needed in these methods.
모let be the number of elements in the widest initial interval  be the number of variables  and be the number of constraints; the known bound of time-complexity for computing -consistencyis : is computed in and each increase in the factor has a cost of . our aim is to show that this complexity is overestimated and that the expected optimum is actually . we show that this bound can be approached in the general case and that it can be exactly reached in the especially interesting case of .
모this result is obtained by a cautious formulation of the algorithm  in which an optimization pointed out by lhomme and lebbah  lebbah  1  is integrated. informally speaking  each time an interval is reduced  the solver performs an incomplete computation. the optimization is to store the result of this computation  so that successive calls to the solver do not need to be restarted from scratch. a dynamic dependency condition is used to determine that some operations can be avoided. we then study the cost of the cumulation of all these incomplete computations to find improved bounds on time complexity.
모the outline of this paper is the following: next section introduces the required material and terminology on constraints and solvers. section 1 introduces interval consistency techniques and in particular -consistency. we then define and prove two improvements to a naive algorithm for computing it  section 1 . a theoretical analysis of the resulting algorithm allows us to state the new complexity results in section 1  while the end of the paper is concerned with conclusions and perspectives.
1	constraints and solvers
we note the domain of computation. is supposed to be finite and totally ordered: it is thus possible to compute the predecessor and the successor of any element . this assumption may seem very restrictive  but it corresponds to the usual representation of numbers with computers: in practice  is either the set of the canonical floating-point intervals or the subset of the integers encoded on a given number of bits.
1	basic terminology
let be a finite set of variables. a point is defined by a coordinate on each variable   i.e.  points are mappings from to . a constraint is a set of tuples. for readers more familiar with database-theoretic concepts  dechter and van beek  1   variables may also be thought of as field names  points as tuples  and constraints as relations or tables. points may also be seen as logical valuations.
모a csp is a set of constraints. the set of solutions to is the set of the points which belong to all its constraints  i.e.   . the problem of constraint satisfaction is to find  one/all  solution s  to a csp. it is a npcomplete problem in its decisional form.
모the cut of a set of points is the set containing the points of whose coordinate on is :
모모모. it is also referred to as the selection or instantiation operator. among other properties  this operator is monotonic:
.
모though these definitions are fully general and do not depend on the way constraints are expressed  some points in this paper are concerned with the special case of systems of equations and inequations over . as an example of such systems  defines the csp where and
모. a solution is . the cut defines the set	.
모notations: throughout the text  the considered csp will be fixed and called ; consequently we shall omit this parameter in most definitions.
1	interval-consistency
we call search space     the set of points among which solutions are searched for. in the interval-consistency framework  benhamou and older  1   the search space is the cartesian product of intervals associated to every variable. the success of this representation comes from the fact that it is a compact and expressive way to approximate the solutions. we note and the lower and upper bounds of the interval of possible values for variable . the search space is thus the set containing the points whose coordinates are inside the bounds    . it is called a box  due to the obvious geometric intuition. when we refer to a bound without further detail  it can be indifferently the lower or
the upper bound of	.
모an intuitive way to reason on bounds is by relating them to the facets of the box. formally  a facet is the set of points of the box whose coordinate on a given variable is fixed to a bound: it can be written for some variable and some bound of .
모notations: given a box   we note the property   which states that contains a solution.
1	solvers
we define  constraint  solvers as functions which verify the following properties:
definition 1.  solver  a solver is a function on boxes such that:
 contractance 
  correctness   monotonicity 
 idempotence 
모note that we deliberately exclude the case of nonidempotent solvers  which are not relevant here. solvers are intended to reduce the search space; the key-point is that they are usually integrated within a branch and prune algorithm  which is guaranteed to find the solutions only if the solver does not remove any of them  hence we call this essential property correctness. the opposite property to correctness is completeness  which states the implication
. complete solvers are able
to detect all inconsistencies  but are usually too costly to be considered practically.
1	kb-consistency
the solvers we now consider can be seen as functions which tighten the intervals by modifying each bound in turn; we often refer to this process as facet reduction  meaning that facets are  pushed down . solvers are defined via local consistency properties  which characterize the facets obtained when the reduction stops. we now define -consistency inductively  starting with a basic consistency technique for the case .
1	1b-consistency
   -consistency is also called hull-consistency by some authors  benhamou and older  1 . on finite domains  it is the core of some of the main constraint logic programming solvers  van hentenryck et al.  1; codognet and diaz  1   and it is often referred to as partial lookahead.
definition 1.  1b-consistency  a box is 1b-consistent if for every bound we have:
the -consistency solver is the function which maps a box to the greatest -consistent box included in .
모this definition means that bounds are reduced until each facet of the box intersects every constraint. it can be proved that is a valid function: the computed greatest box exists and is unique; this result comes from more general properties of confluence of fixpoint computations  apt  1 . furthermore  is a valid solver: correctness holds because a value is suppressed from a variable if   so that we have   which provesthat the solver only removes facets which do not contain solutions; idempotence is straightforward and monotonicity comes from the monotonicity of the selection operator.
     -consistency can only be computed for some special constraints called primitive  which correspond to the basic arithmetic relations defined by the language. solvers like prolog iv decompose arbitrary constraints  e.g.    into such primitive constraints    .
1	kb-consistency
the condition that every facet intersects every constraint is a very rough approximation of  global  consistency. consistency relies on the observation that ideally  a facet should be reduced until it contains a solution; since deciding whether a facet contains a solution is itself a constraint satisfaction problem  a solver can do for this task.
definition 1.  kb-consistency  a box is -consistent  for   if for every bound we have:
the consistency solver is the function such that the box is the greatest -consistent box included in .
모informally speaking  bounds are reduced until no facet is rejected by the solver . this definition is recursive: consistency uses the -consistency solver and can in turn be used to define -consistency  and so on. properties of confluence  idempotence and monotonicity hold for . its correctness is a consequence of the correctness of : since implies   the removed
facets do not contain any solution. it is easy to prove that
is  stronger  than   i.e.  that  this is due to the monotonicity of  .
모note that the idea of -consistency can be customized and that any consistency technique can be used instead of consistency. some authors have considered the use of boxconsistency  puget and van hentenryck  1  or lazy forms of arc-consistency  debruyne and bessie`re  1 .
1	algorithms
we now present the algorithms for computing consistency. as far as -consistency is concerned  we only sketch the notions required to understand the subsequent proofs  and the reader is referred to  benhamou and older  1; lhomme  1  for further explanations.
모we then give an incremental description of the algorithm for -consistency in the general case: starting from a naive algorithm  we successively describe two optimizations used by the experts of this technique and suggested in  lebbah  1   see also  delobel  1  . the theoretical interest of these improvements shall be studied in the next section.
1	computing 1b-consistency
the algorithm for enforcing -consistency is a variant of the seminal propagation algorithm - ac1. it is a  constraintoriented  algorithm  which handles a set of constraints to be considered  generally stored as a queue . it relies on the fact that every constraint relates a given number of variables we say that depends on this variable   or alternatively that constrains . figure 1 describes the algorithm.
two points are essential to the complexity study:
1 :=
1 while	do
1 :=	% arbitrary choice of
1 for each variable constrained by do
1 reduce	while
1 reduce	while
1 if	has been modified then
1 :=	depends on
	figure 1: the algorithm for	-consistency
in the case of numeric constraints  each constraint depends on  at most  variables;
lines 1 and 1 of the algorithm can be performed in bounded time using interval computations.
1	naive algorithm for kb-consistency
the filtering of a box by -consistency is computed by reducing each of its facets in turn. in the following algorithm  figure 1  we use the notation reduce while to represent the computation of the new bound. this notation actually represents a loop: while the specified condition does not hold  the bound is iteratively reduced to the next  when reducing a lower bound  or previous  upper bound  element. this loop may indeed be optimized  for instance by trying to remove several values at once  or even by using recursive splitting. unfortunately  these heuristics do not improve the worst-case complexity: at worst  every value has to be considered in turn to be effectively reduced.
1 do
1 for each bound	do
1 reduce	while
1 while is changed
	figure 1: naive algorithm for	-consistency
모the correctness of this algorithm is straightforward since when the loop stops  all the bounds are -consistent.
1	storing information
the most obvious drawback of the preceding algorithm is that each time a facet is reconsidered  the solver
is restarted from scratch to reduce it. a more clever approach is to store the result of this reduction  in order to reuse it in the next steps  figure 1 . the notation memorize is a simple assignment  but it is intended to express the fact that the memorized box has already been computed in the previous line.
1 for each bound	do:=1 do
1 for each bound	do
1 :=
1 if	then
1 reduce	while
1 memorize
1 while is changedinto	figure 1:	algorithm with storage
모we use a vector 1 which associates to every bound the last non-empty box computed while reducing it. when a bound is reconsidered  it is possible to restart the solver from the box  cf. line 1   because we have:
proof. note that the intersection	is a way to rewrite	since	contains only points with value	on	.	since for some	  a direct consequence of monotonicity is that	. we also have	 contractance   and thus
. using mono-
tonicity and idempotence we prove that
. the
other inclusion	is straightforward.	
1	avoiding computation
the second optimization relies on the observation that it is not worth recomputing a bound if   because is then equal to which is already a fixpoint of the solver . in the algorithm of figure 1  this condition is checked in a naive fashion before each bound reduction:
1 for each bound	do:=1 do
1 for each bound	do
1 if	then
1 :=
1 if	then
1 reduce	while
1 memorize
1 while is changedintofigure 1: algorithm with dynamic dependency checkings
모figure 1 is an illustration of the preceding considerations: in this figure  bound is reduced but the global box still contains  = for some box
모모모 . the re-computation of bound can therefore be avoided.
1	complexity analysis
this section is concerned with the theoretical study of the algorithms  starting with the special case of -consistency. we then study the complexity of the algorithms of section 1  and point out little improvements needed to compute in near-optimal time. as a comparison  we first recall the original complexity results given in  lhomme  1 . as usual in the literature  mackworth and freuder  1   the complexity of local consistency algorithms is expressed as a function of the following parameters:

figure 1: an illustration of the algorithms
the number of variables 
the number of constraints 
the number of elements in the greatest interval of the
box.
모in general  it is possible to artificially define as many constraints as is wished on a given number of variables. on the contrary  it is not possible to have  for ternary constraints  unless the absurd case of disconnected constraint graphs is accepted  thus .
1	complexity of 1b-consistency
the complexity of -consistency has been determined by  lhomme  1  using an argument inspired from  mackworth and freuder  1 . we recall this result and give a simpler proof:
complexity 1. the time-complexity for the algorithm computing -consistency  see figure 1  is .
proof. between two insertions of a constraint into the set   the reduction of one of the intervals on which depends must have occurred. depends on a bounded number of variables  1 in our case   and each interval may be reduced at most times. the overall number of insertions of a given constraint into the queue is hence bounded by   and the number of insertions of the constraints is . since lines 1 and 1 of the algorithm require constant time  this gives the complexity.	
모we also recall the best-case running time: when no reduction is obtained  the box is already -consistent   every constraint is dequeued in time .
1	complexity of the original algorithm
the complexity of the original algorithm for -consistency presented in  lhomme  1  is the same as the one of the naive algorithm presented in figure 1. to show that this algorithm runs in time   we just prove the following result:
complexity 1. the number of calls to the solver in the naive -consistency algorithm is .
proof. the worst case is obtained when re-computing all facets  line 1 of the algorithm of figure 1  results in a minimal reduction  removal of one element  of only one of the bounds: consequently  applications of are needed to remove one element. since one interval is associated to each of the variables  elements may be suppressed one after the other in the worst case  hence the overall number of applications of the solver is . 
모note that this complexity does not seem to be optimal: since we use the solver to test each element of every variable  we could expect an optimal algorithm to perform only calls to . we shall see how this presumably optimal bound can be reached in the case of -consistency.
1	1b-consistency
we now have all the theoretical elements at hand for a critical study of the algorithm of figure 1. we first restrict ourselves to the case of -consistency  whose expected optimal complexity is . the algorithm of figure 1 is not optimal yet for a simple reason: to suppress some value on a given interval  it may be needed to apply several -consistencies  each time with cost . the problem is that is run on all constraints in a systematic way  while only the ones which depend on a bound which has been reduced since the last run should be enqueued.
모this situation may be avoided with a finer storage of the queues used by -consistency  figure 1 : a queue is attached to each facet   and each time the reduction of a facet stops  the corresponding queue is empty. only the constraints which effectively have to be reconsidered are enqueued  line 1 . in the following the expression using means that is run with as an initial queue instead of the queue containing all the constraints used in figure 1:
1 for each bound	do	:=
1 :=
1 do
1 for each bound	do
1 :=	using
1 if	then
1 reduce	while
1 memorize	into
1 for each bound	do
1 if	then
1 :=
1 :=	depends on
1 while is changed
figure 1: optimized handling of the queues in
모note that this algorithm is essentially the same as the one of figure 1 where the test for inclusion in line 1 and the intersection in line 1 are applied in an optimized way. these two operations can be especially easily computed since only the interval for the variable which has effectively been changed has to be checked or reduced.
complexity 1. the	algorithm	of	figure	1	is	optimal
 	 .
proof. several calls to the solver may be required to suppress a value from a given variable. throughout these calls  each constraint is taken into account only if one of the variables on which it depends has been reduced  either during the current computation of -consistency or between the previous computation and the current one . the cumulated number of constraint insertions into the queue for all the calls related to value is then bounded by . since such values may be suppressed  the global cost is . 
모this bound is correct because the number of tests of interval inclusions has been optimized: in the algorithm  interval inclusions are checked every time one of the
elements is effectively removed  and the cost of these tests is then . since is   this complexity is less than
.
1	complexity of kb-consistency
we refine the analysis of the complexity of -consistency. figure 1 shows the savings obtained by the memorization presented before. is a facet of some box. it is reduced by
       -consistency in order to show that the corresponding value is inconsistent. the values suppressed by consistency appear as tiles on the figure. the second part of the figure shows a case where the search space has been reduced and the re-computation of -consistency is needed.

	figure 1: successive reductions of a facet	.
모to determine the complexity of -consistency  we bound the number of filterings by -consistency  i.e. the tiles . the keypoint is that the algorithm stores the suppressions obtained by   and avoids computing them several times. we have the following result:
complexity 1. when computing -consistency  the number of computations of -consistency is .
proof. consider the computation needed to suppress some value associated to a variable   may be thought of as the bound of the box . such values exist. due to the memorization  it is needed to re-compute the consistency on the facet only in the case where one of the values of the corresponding memorized box     has been removed. in the worst case  it may happen that no computation of -consistency reduces the memory; up to such computations can thus occur  since suppressions of a single value of the memory may occur. each time  the number of -consistencies that are checked is  one is run on each facet of  .
the overall number of	-consistencies is bounded by
	.	
모since the original bound on the number of consistencies is   we have shown an improvement of the worst-case behaviour. we summarize the results for     and stronger consistencies    :
  b  b1	conclusion
investigating several improvements to a basic algorithm for computing -consistency  we have designed an optimal algorithm for -consistency and shown that improvedbounds are reached for higher values of . note that the algorithms presented here are very affordable in terms of space complexity: the storage of the boxes needed by the algorithms of figures 1 and 1 requires space   space is needed to store a box for each facet at each level and no two applications of some solver for the same level of may coexist . the handling of the queues in the algorithm of figure 1 requires additional space.
모in terms of algorithms  our main contribution was to overview and justify theoretically some improvements based on some memorization of computation. though our purpose was not to discuss the practical relevance of this technique  it actually proves to be effective on our implementation  since it allows saving some percents of the computations of facets. typical benchmarks show that 1% to 1% of the facet reduction may be avoided while computing -consistency. it might be possible to generalize these memorization techniques to each level of -consistency  and hence to improve the theoretical time-complexity - to the detriment of spacecomplexity.
모our study of the complexity of interval consistency techniques may be regarded as complementary to the qualitative results which compare the filtering obtained by the algorithms. for instance  it is well-known that -consistency allows to compute more precise intervals than box-consistency  collavizza et al.  1   but that it is also a more costly technique. since the efficiency of constraint solving depends on the compromise between the pruning obtained and the time required to achieve it  an interesting perspective is to study the difference between the two algorithms in terms of theoretical costs.
모note that our study was restricted to the original definition of -consistency  and that the alternative definition based on box-consistency introduced in  puget and van hentenryck  1  has not been considered. since box-consistency is computed in a very similar way to   it seems however that little customization of our algorithm is needed to handle this case. a more challenging task would be to determine whether similar optimizations to the ones presented in this paper may be helpful in the combinatorial case  where domains are considered instead of intervals.
모acknowledgements we express our gratitude to olivier lhomme for its encouragements and advice.
references
 apt  1  k. apt. the essence of constraint propagation. theoretical computer science  1-1   1.
 benhamou and older  1  f. benhamou and w. older. applying interval arithmetic to real  integer and boolean constraints. journal of logic programming  1   1.
 codognet and diaz  1  p. codognet and d. diaz. compiling constraints in clp fd . journal of logic programming  1   1.
 collavizza et al.  1  h. collavizza  f. delobel  and m. rueher. comparing partial consistencies. reliable computing  1   1.
 davis  1  e. davis. constraint propagation with interval labels. artificial intelligence  1  1.
 debruyne and bessie`re  1  r. debruyne and c. bessie`re. some practicable filtering techniques for the constraint satisfaction problem. in proceedings of the 1th ijcai  nagoya  japan  1.
 dechter and van beek  1  r. dechter and p. van beek. local and global relational consistency. theoretical computer science  1   1.
 delobel  1  f. delobel. resolution뫣 de contraintes reelles뫣 non-lineaires뫣 . phd thesis  universite뫣 de nice - sophia antipolis  1.
 freuder  1  e.c. freuder. synthesising constraint expressions. communications of the acm  1  1.
 lebbah  1  y. lebbah. contribution a` la resolution뫣 de contraintes par consistance forte  in french . phd thesis  ensti des mines de nantes  1.
 lhomme  1  o. lhomme. consistency techniques for numeric csps. in proceedings of the 1th ijcai  pages 1  chambe뫣ry  france  1.
 mackworth and freuder  1  a. mackworth and e. freuder. the complexity of some polynomial network consistency algorithms for constraint satisfaction problems. artificial intelligence  1   1.
 montanari  1  u. montanari. networks of constraints: fundamental properties and applications to picture processing. information science  1   1.
 puget and van hentenryck  1  j.-f. puget and p. van hentenryck. a constraint satisfaction approach to a circuit design problem. journal of global optimization  1  1.
 sam-haroud and faltings  1  j. sam-haroud and b. v. faltings. consistency techniques for continuous constraints. constraints  1  1.
 van hentenryck et al.  1  p. van hentenryck  v. saraswat  and y. deville. design  implementation and evaluation of the constraint language cc fd . journal of logic programming  1-1   1.
refining the basic constraint propagation algorithmchristian bessi뺟re
lirmm-cnrs  umr 1 
1 rue ada
1 montpellier cedex 1  france bessiere lirmm.fr jean-charles r뺝gin ilog
1  route des dolines 1 valbonne  france regin ilog.fr

abstract
propagating constraints is the main feature of any constraint solver. this is thus of prime importance to manage constraint propagation as efficiently as possible  justifying the use of the best algorithms. but the ease of integration is also one of the concerns when implementing an algorithm in a constraint solver. this paper focuses on ac-1  which is the simplest arc consistency algorithm known so far. we propose two refinements that preserve as much as possible the ease of integration into a solver  no heavy data structure to be maintained during search   while giving some noticeable improvements in efficiency. one of the proposed refinements is analytically compared to ac-1  showing interesting properties  such as optimality of its worst-case time complexity.
1	introduction
constraint propagation is the basic operation in constraint programming. it is now well-recognized that its extensive use is necessary when we want to efficiently solve hard constraint satisfaction problems. all the constraint solvers use it as a basic step. thus  each improvement that can be incorporated in a constraint propagation algorithm has an immediate effect on the behavior of the constraint solving engine. in practical applications  many constraints are of well-known types for which specific algorithms are available. these algorithms generally receive a set of removed values for one of the variables involved in the constraint  and propagate these deletions according to the constraint. they are usually as cheap as one can expect in cpu time. this state of things implies that most of the existing solving engines are based on a constraint-oriented or variable-oriented propagation scheme  ilog solver  choco  etc. . and ac-1  with its natural both constraint-oriented  mackworth  1  and variable-oriented  mcgregor  1  propagation of the constraints  is the generic constraint propagation algorithm

1
모모this work has been partially financed by ilog under a research collaboration contract ilog/cnrs/university of montpellier ii. 1
member of the coconut group.
which fits the best this propagation scheme. its successors  ac-1  ac-1  and ac-1  indeed  were written with a valueoriented propagation. this is one of the reasons why ac-1 is the algorithm which is usually used to propagate those constraints for which nothing special is known about the semantics  and then for which no specific algorithm is available . this algorithm has a second strong advantage when compared to ac-1  ac-1 or ac-1  namely  its independence with regard to specific data structure which should be maintained if used during a search procedure.  and following that  a greater easiness to implement it.  on the contrary  its successors  while more efficient when applied to networks where much propagationoccurs   bessi뺟re et al.  1; bessi뺟re et al.  1    need to maintain some additional data structures.
모in this paper  our purpose is to present two new algorithms  ac1 and ac1  which  like ac-1  accept variableoriented and constraint-oriented propagation  and which improve ac-1 in efficiency  both in terms of constraint checks and cpu time . ac1  like ac-1  is free of any data structure to be maintained during search. ac1  at the price of a slight extra data structure  just an integer for each valueconstraint pair  reaches an optimal worst-case time complexity.1 it leads to substantial gains  which are shown both on randomly generated and real-world instances of problems. a comparison with ac-1 shows interesting theoretical properties. regarding the human cost of their implementation  ac1 needs a few lines more than the classical ac-1  and ac1 needs the management of its additional data structure.
1	preliminaries
constraint network. a finite binary constraint network is defined as a set of variables
모모모모모모  a set of domains	  where is the finite set of possible values for variable   and a set of binary constraints between pairs of variables. a constraint on the ordered set of variables is a subset of the cartesian product
that specifies the allowed combinations of values for the variables and .  for each constraint   a constraint is defined between   allowing the same pairs of values

1
a related paper by zhang and yap appears in these proceedings.
in the reverse order.  verifying whether a given pair is allowed by or not is called a constraint check. a solution of a constraint network is an instantiation of the variables such that all the constraints are satisfied.
arc consistency. let be a constraint network  and a constraint in . a value is consistent with iff such that .
  is then called a support for on .  a value is viable iff it has support in all such that . is arc consistent iff all the values in all the domains are viable. we achieve arc consistency in by removing every value which is not viable.
1	a first stab at improving ac-1
1	background on ac-1
before presentingthe algorithmwe proposein this section  let us briefly recall the ac-1 algorithm. we present it with the structure proposed by mcgregor  mcgregor  1   which is built on a variable-based propagation scheme. this will be recommended for the algorithm presented in the next subsection. the main algorithm  see algorithm 1  is very close to the original ac-1  with an initialization phase  lines 1 to 1   a propagation phase  line 1   and with the use of the function revise1 that removes from the values without support in  line 1 . but instead of handling a queue of the constraints to be propagated  it uses a queue1 of the variables that have seen their domain modified. when a variable is picked from the queue  line 1 of propagation1 in algorithm 1   all the constraints involving are propagated with revise1. this is the only change w.r.t. the mackworth's version of ac-1  this algorithm has been presented in  chmeiss and j뺝gou  1  under the name ac-1. 

algorithm 1: main algorithm

functionac  in	: set : boolean
1 ;
	for each	do
	for each	such that	do
1 if revise-x	false1 then if	then return false ;
1 ;
1 return propagation-x   ;

1	the algorithm ac1
if we closely examine the behavior of ac-1  we see that removing a single value from a domain  inside function revise1  is enough to put in the propagation queue algorithm 1: subprocedures for ac-1

functionpropagation1  in	: set : boolean
1 while	do
1 pick	from	;
1 for each	such that	do
1 if revise-x	then
	if	then return false ;
1 ; return true ;
functionrevise1  in	: variable : boolean
	change	false;
	for each	do
if	then remove	from	;
       change	true; return change ;

 line 1 of ac in algorithm 1 and line 1 of propagation1 in algorithm 1   and to provoke a call to revise1 for every constraint involving  lines 1 and 1 of propagation1 . revise1 will look for a support for every value in whereas for some of them was perhaps not even a support.  as a simple example  we can take the constraint   where
     . removing value 1 from leads to a call to revise1   which will look for a support for every value in   for a total cost of
   constraint checks  whereas only had lost support.  we exploit this remark in ac1. instead of looking blindly for a support for a value each time is modified  we do that only under some conditions. in addition to the queue of variables modified  we use a second data structure   1 which for each variable
contains the values removed since the last propagation of
   . when a call to revise1 is performed  instead of systematically looking whether a value still has support in   we first check that really lost a support  namely one of its supports is in  line 1 of revise1 in algorithm 1 . the larger is  the more expensive that process is  and the greater the probability to actually find a support to in this set is. so  we perform this  lazymode  only if is sufficiently smaller than
. we use a parameter ratio to decide that.  see line
1 of propagation1 in algorithm 1.  the boolean lazymode is set to true when the ratio is not reached. otherwise  lazymode is false  and revise1 performs exactly as revise1  going directly to lines 1 to 1 of revise1 without testing the second part of line 1.
if we run ac1 on our previous example  we have
             . if ratio   then for each   we check whether contains a support of before looking for a support for . this requires 1 constraint checks. the only value for which support is effectively sought is . that requires 1 additional algorithm 1: subprocedures for ac1

functionpropagation1  in	: set : boolean while	do
	pick	from	;
1 lazymode	ratio	; for each	such that	do
if revise1 then if then return false ;
;
1 reset	; return true ;
functionrevise1  in	: variable;
in lazymode: boolean  : boolean
	change	false;
	for each	do
1 if	or	then
1 if	then remove	from	; add	to	;
1 change	true; return change ;

constraint checks. we save 1 constraint checks compared to ac-1.
analysis
let us first briefly prove ac1 correctness. assuming ac-1 is correct  we just have to prove that the lazy mode does not let arc-inconsistent values in the domain. the only way the search for support for a value in is skipped  is when we could not find a support for in  line 1 of revise1 . since contains all the values deleted from since its last propagation  line 1 of propagation1   this means that has exactly the same set of supports as before on . thus  looking again for a support for is useless. it remains consistent with . the space complexity of ac1 is bounded above by the
sizes of	and	.	is in	and	is in	  where
is the size of the largest domain. this gives a overall complexity. in this space complexity  it is assumed that we built ac1with the variable-orientedpropagationscheme  as recommended earlier. if we implement ac1 with the constraint-oriented propagation scheme of the original ac-1  we need to attach a to each constraint put in the queue. this implies a space complexity.
모the organization of ac1 is the same as in ac-1. the main change is in function revise1  where and are examined instead of only . their total size
is bounded above by	. this leads to a worst-case where
checks are performed as in revise1. thus  the overall time complexity is in since revise1 can be called times per constraint. this is as in ac-1.
1 ac1
in section 1  we proposed an algorithm  which  like ac-1  does not need special data structures to be maintained during search.  except the current domains  which have to be maintained by any search algorithm performing some lookahead.  during a call to revise1  for each in   we have to look whether has a support in   to know whether it lost supports or not. in this last case  we have to look again for a support for on in the whole
set. if we could remember what was the support found for in the last time we revised   the gain would be twofold: first  we would just have to check whether this last support has been removed from or not  instead of exploring the set . second  when a support was effectively removed from   we would just have to explore the values in that are  after  that last support since  predecessors  have already been checked before. adding a very light extra data structure to remember the last support of a value on a constraint leads to the algorithm ac1 that we present in this section.
모let us store in the value that has been found as a support for at the last call to revise1 . the function revise1 will always run in the lazy mode since the cost of checking whether the support on of a value has been removed from is not dependent on the number of values removed from . a second change w.r.t. ac1 is that the structure is no longer necessary since the test     can replace the test    . the consequence is that ac1 can equally be used with a constraintbased or variable-based propagation. we only present the function revise1  which simply replaces the function revise1 in ac-1. the propagation procedure is that of ac-1  and the structure has to be initialized to nil at the beginning. in line 1 of revise1  see algorithm 1  we check whether still belongs to .
if it is not the case  we look for a new support for	in
모모모; otherwise nothing is done since is still in . in line 1 of revise1 we can see the second advantage of storing : if the supports are checked in in a given ordering      we know that there isn't any support for before in
모모모. thus  we can look for a new support only on the values greater than .

algorithm 1: subprocedure for ac1
functionrevise1  in
change	false;
for each	do: variable : boolean1	ifthenif
then;else
removefrom;	add	to;       change return change ;true;1
모on the example of section 1  when is removed  ac1 simply checks for each that still belongs to   and finds that has been removed. looking for a new sup-
ac-1ac1ac1ac-1#cckstime	#ccks	time#cckstimetime   under-constrained 1111111   over-constrained 1111111   phase transition 1 1.1 11111   phase transition 1 1.1 111 1.1.1scen#1 arc inconsistent 1 1.1 111 1.1.1table 1: arc consistency results in mean number of constraint checks  #ccks  and mean cpu time in seconds  time  on a pc
pentium ii 1mhz  1 instances generated for each random class .  *  the number of constraint checks performed by ac-1is similar to that of ac1  as discussed in section 1.
port for 1 does not need any constraint check since
does not contain any value greater than   which was equal to 1. it saves 1 constraint checks compared to ac-1.
analysis
proving correctness of ac1 can be done very quickly since the framework of the algorithm is very close to ac-1. they have exactly the same initialization phase except that
ac1 stores   the support found for each on each .  in line 1 it is assumed that nil does not belong to .  during the propagation  they diverge in the way they revise an arc. as opposed to ac-1  revise1 goes into a search for support for a value in only if does not be-
long to	. we see that checking that
still belongs to is sufficient to ensure that still has a support in . and if a search for a new support has to be done  limiting this search to the values of
greater than w.r.t. to the ordering used to visit is sufficient. indeed  the previous call to revise1 stopped as soon as the value was found. it was then the smallest support for in w.r.t. .
모the space complexity of ac1 is bounded above by the size of   and . is in or   depending on the propagationscheme that is used  variable-basedor constraintbased . is in since each value has a pointer for each constraint involving . this gives a overall complexity.
모as in ac-1 and ac1  the function revise1 can be called times per constraint in ac1. but  at each call to revise1   for each value
모모모  there will be a test on the   and a search for support only on the values of greater than . thus  the total work that can be performed for a value over the possible calls to revise1 on a pair is bounded above by tests on and constraint checks. the overall time complexity is then bounded above by   which is in . this is optimal  mohr and henderson 
1 . ac1 is the first optimal arc consistency algorithm proposed in the literature that is free of any lists of supported values. indeed  the other optimal algorithms  ac-1  ac-1  ac-1  and ac-inference all use these lists.
1	experiments
in the sections above  we presented two refinements of ac-1  namely ac1 and ac1. it remains to see whether they are effective in saving constraint checks and/or cpu time when compared to ac-1. as we said previously  the goal is not to compete with ac-1/ac-1  which have very subtle data structure for the propagation phase. an improvement  even small  w.r.t. ac-1 would fulfill our expectations. however  we give ac-1 performances  just as a marker.
1	arc consistency as a preprocessing
the first set of experiments we performed should permit to see the effect of our refinements when arc consistency is used as a preprocessing  without search . in this case  the chance to have some propagations is very small on real instances. we have to fall in the phase transition of arc consistency  see  gent et al.  1  . so  we present results for randomly generated instances  those presented in  bessi뺟re et al.  1    and for only one real-world instance. for the random instances  we used a model b generator  prosser  1 . the parameters are   where is the number of variables  the size of the domains  the number of constraints  their density    and the number of forbiddentuples  their tightness  . the real-world instance  scen#1  is taken from the fullrlfap archive 1 which contains instances of radio link frequency assignment problems  rlfaps . they are described in  cabon et al.  1 . the parameter ratio used in ac1 is set to 1. table 1 presents the results for four classes of random instances plus the real-world one.
	the	upper	two	are	under-constrained
   1   and over-constrained    1   problems. they represent cases where there is little or no propagation to reach the arc consistent or arc inconsistent state. this is the best case for ac-1  which performs poorly during propagation. we can see that ac1 and ac1 do not suffer from this.
모the third and fourth experiments are at the phase transition of arc consistency for sparse
   1    and dense    1    problems. we can assume there is much propagation on these problems before reaching the arc consistent state. this has a significant impact on the respective efficiencies of the algorithms. the smarter
mac-1mac1mac1mac1#cckstime#cckstime#cckstimetimescen#1 111 1.1 111scen#1 111 1.1 111graph#1 111 1.1 111graph#1 111 1.1 111graph#1 111 1.1 111table 1: results for search of the first solution with a mac algorithm in mean number of constraint checks  #ccks  and meancpu time in seconds  time  on a pc pentium ii 1mhz.
the algorithm is  the lower the number of constraint checks is. ac1 dominates ac1  which itself dominates ac-1. and the cpu time follows this trend.
모the lower experiment reports the results for the scen#1. this is one of the instances in fullrlfap for which arc consistency is sufficient to detect inconsistency.
1	maintaining arc consistency during search
the second set of experiments we present in this section shows the effect of our refinements when arc consistency is maintained during search  mac algorithm  sabin and freuder  1   to find the first solution. we present results for all the instances contained in the fullrlfap archive for which more than 1 seconds were necessary to find a solution or prove that none exists. we took again 1 for the ratio in ac1. it has to be noticed that the original question in these instances is not satisfiability but the search of the  best  solution  following some criteria. it is of course out of the scope of this paper.
모from these instances we can see a slight gain for ac1 on ac-1. on scen#1  it can be noticed that with a smaller ratio  ac1 slightly improves its performances.  ratio = 1 seems to be the best.  a more significant gain can be seen for ac1  with up to 1 times less constraint checks and twice less cpu time on scen#1. as for the experiments performed on random instances at the phase transition of arc consistency  this tends to show that the trick of storing the data structure significantly pays off. however  we have to keep in mind that we are only comparing algorithms with simple data structures. this prevents them from reaching the efficiency of algorithms using lists of supported values when the amount of propagation is high  namely on hard problems.  e.g.  a mac algorithm using ac-1 for enforcing arc consistency needs only 1 seconds to solve the scen#1 instance. 
1 ac1 vs ac-1
in the previous sections  we proposed two algorithms based on ac-1 to achieve arc consistency on a binary constraint network. ac1 is close to ac-1  from which it inherits its time complexity and its space complexity.
ac1  thanks to its additional data structure  has an optimal worst-case time complexity  and an space complexity. these are the same characteristics as ac-1 so  we can ask the question:  what are the differences between
ac1 and ac-1  .
모let us first briefly recall the ac-1 behavior  bessi뺟re  1 . ac-1 looks for one support  the first one or smallest one with respect to the ordering   for each value on each constraint to prove that is currently viable. when is found as the smallest support for on   is added to   the list of val-
ues currently having	as smallest support. if
is removed from   it is added to the   which is the stream driving propagations in ac-1. when is picked from the   ac-1 looks for the next support  i.e.  greater than   in for each value in . notice that the corresponds to in ac1  namely the set of values removed but not yet propagated.
모to allow a closer comparison  we will suppose in the following that the lists of ac-1 are split on each constraint involving   leading to a structure   as in ac-1.
property 1 let be a constraint network. if we suppose ac1 and ac-1 follow the same ordering of variables and values when looking for supports and propagating deletions  then  enforcing arc consistency on with ac1requires the same constraint checks as with ac-1.
proof. since they follow the same ordering  both algorithms performthe same constraint checks in the initialization phase: they stop search for support for a value	on	as soon as the first	in	compatible with	is found  or when	is exhausted  then removing	 . during the propagation phase  both algorithms look for a new support for a value	on	only when	has lost its current support	in	 i.e. 	for ac-1  and for ac1 . both algorithms start the search for a new support for	at the value in	immediately greater than	w.r.t. the	ordering. thus  they will find the same new support for	on	  or will remove	  at the same time  and with the same constraint checks. and so on.
모from property 1  we see that the difference between ac1 and ac-1 cannot be characterized by the number of constraint checks they perform. we will then focus on the way they find which values should look for a new support. for that  both algorithms handle their specific data structures. let us characterize the number of times each of them checks

 namely  the fact that	 .
its own data structure when a set	of deletions is propagated on a given constraint	.
property 1 let	be a constraint in a network
모모모모. let be a set of values removed from that have to be propagated on . if 
 
  and
= # checks performed on	to propagate	  then 	and	represent the number of operations
ac-1 and ac1 will respectively perform to propagate on	.
proof. from property 1 we know that ac-1 and ac1 perform the same constraint checks. the difference is in the process leading to them. ac-1 traverses the
list for each	 i.e. 	operations   and ac1 checks whether	belongs to	for every in	 i.e. 	operations .

figure 1: the constraint example
모we illustrate this on the extreme case presented in figure 1. in that example  the three values of are all compatiblewith the first value of . in addition  is compatible with all the values of from to   and with all the values of from to . imagine that for some reason  the value has been removed from  i.e. 
모모모모모모 . this leads to     and   which is a case in which propagating with ac-1 is much better than with ac1  even if none of them needs any constraint check. indeed  ac-1 just checks that is empty 1 and stops. ac1 takes one by one the 1 values	of	to check that their	is not
in	. imagine now that the values	to	of
have been removed  i.e. 	 . now 
 	  and	. this means that ac1 will clearly outperform ac-1. indeed  ac-1 will check for all the
1 values in that is empty 1 while ac1 just checks that is not in for the 1 values in .
discussion
thanks to property 1  we have characterized the amount of effort necessary to ac-1 and ac1 to propagate a set of removed values on a constraint . gives us information on which algorithm is the best to propagate on . we can then easily imagine a new algorithm  which would start with an ac-1 behavior on all the constraints  and would switch to ac1 on a constraint as soon as would be positive on this constraint  and then forget the lists on  . switching from ac1 to ac-1 is no longer possible on this constraint because we can deduce in constant time that when belongs to   but we cannot obtain cheaply from the structure. a more elaborated version would maintain the lists even in the ac1 behavior  putting in each time is found as being the  . this would permit to switch from ac-1 to ac1 or the reverse at any time on any constraint in the process of achieving arc consistency. these algorithms are of course far from our initial purpose of emphasizing easiness of implementation since they require the
	and	structures to be maintained during search.
1	non-binary versions
both ac1 and ac1 can be extended to deal with non-binary constraints. a support is now a tuple instead of a value. tuples in a constraint are ordered w.r.t. the ordering of the domains  combined with the ordering of  or any order used when searching for support . once this ordering is defined  a call to
revise1 -because of a set of values removed from - simply checks for each whether there exists a support of on the constraint for which  the value of in the tuple  belongs to . if yes  it looks for a new support for on . revise1 checks for each whether   which is a tuple  still belongs to
모모모모모모모모모before looking for a new support for on	.
모this extension to non-binary constraints is very simple to implement. however  it has to be handled with care when the variable-oriented propagation is used  as recommended for ac1.  with a constraint-based propagation  a set is duplicated for each constraint put in the queue to propagate it.  variable-based propagation is indeed less precise in the way it drives propagation than constraint-based propagation. take the constraint	as an example. if and	are modified consecutively 	and
are put in the queue	consecutively. picking	from
implies the calls to revise and revise   and picking implies the calls to revise and revise . we see that revise is called twice while once was enough. to overcome this weakness  we need to be more precise in the way we propagate deletions. the solution  while being technically simple  is more or less dependent on the architecture of the solver in which it is used. standard techniques are described in  ilog  1; laburthe  1 .
1	conclusion
we presented ac1 and ac1  two refinements in ac-1. the first one improves slightly ac-1 in efficiency  number of constraint checks and cpu time  although it does not need any new data structure to be maintained during search. the second  ac1  needs an additional data structure  the supports  which should be maintained during search. this data structure permits a significant improvement on ac-1  and decreases the worst-case time complexity to the optimal . ac1 is the first algorithm in the literature achieving optimally arc consistency while being free of any lists of supported values. its behavior is compared to that of ac-1  making a contribution to the understanding of the different ac algorithms  and opening an opportunity of improvement. this is in the same vein as the work on ac-1 vs ac-1  wallace  1   which was leading up to ac-1.
acknowledgements
we would like to thank philippe charman who pointed out to us the negative side of value-oriented propagation. the first author also wants to thank all the members of the ocre team for the discussions we had about the specification of the choco language.
references
 bessi뺟re et al.  1  c. bessi뺟re  e. c. freuder  and j. c. r뺝gin. using inferenceto reducearc consistencycomputation. in proceedings ijcai'1  pages 1  montr뺝al  canada  1.
 bessi뺟re et al.  1  c. bessi뺟re  e.c. freuder  and j.c. r뺝gin. using constraint metaknowledge to reduce arc consistency computation. artificial intelligence  1- 1  1.
 bessi뺟re  1  c. bessi뺟re. arc-consistency and arcconsistency again. artificial intelligence  1-1  1.
 cabon et al.  1  c. cabon  s. de givry  l. lobjois  t. schiex  and j.p. warners. radio link frequency assignment. constraints  1-1  1.
 chmeiss and j뺝gou  1  a. chmeiss and p. j뺝gou. efficient path-consistency propagation. international journal on artificial intelligence tools  1 :1  1.
 gent et al.  1  i.p. gent  e. macintyre  p. prosser  p. shaw  and t. walsh. the constrainedness of arc consistency. in proceedings cp'1  pages 1  linz  austria  1.
 ilog  1  ilog. user's manual. ilog solver  1 edition  1.
 laburthe  1  f. laburthe. user's manual. choco  1 edition  1.
 mackworth  1  a.k. mackworth. consistency in networks of relations. artificial intelligence  1-1  1.
 mcgregor  1  j.j. mcgregor. relational consistency algorithms and their application in finding subgraph and graph isomorphism. information science  1-1  1.
 mohr and henderson  1  r. mohr and t.c. henderson. arc and path consistency revisited. artificial intelligence  1-1  1.
 prosser  1  p. prosser. an empirical study of phase transition in binary constraint satisfaction problems. artificial intelligence  1-1  1.
 sabin and freuder  1  d. sabin and e.c. freuder. contradicting conventional wisdom in constraint satisfaction. in proceedings ppcp'1  seattle wa  1.
 van hentenryck et al.  1  p. van hentenryck  y. deville  and c.m. teng. a generic arc-consistency algorithm and its specializations. artificial intelligence  1-1  1.
 wallace  1  r.j. wallace. why ac-1 is almost always better than ac-1 for establishing arc consistency in csps. in proceedings ijcai'1  pages 1  chamb뺝ry  france  1.
making ac-1 an optimal algorithm
yuanlin zhang and roland h.c. yap1
school of computing  national university of singapore
lower kent ridge road  1  singapore
{zhangyl  ryap} comp.nus.edu.sgabstract
the ac-1 algorithm is a basic and widely used arc consistency enforcing algorithm in constraint satisfaction problems  csp . its strength lies in that it is simple  empirically efficient and extensible. however its worst case time complexity was not considered optimal since the first complexity result for ac-1  mackworth and freuder  1  with the bound o ed1   where e is the number of constraints and d the size of the largest domain. in this paper  we show suprisingly that ac-1 achieves the optimal worst case time complexity with o ed1 . the result is applied to obtain a path consistency algorithm which has the same time and space complexity as the best known theoretical results. our experimental results show that the new approach to ac-1 is comparable to the traditional ac-1 implementation for simpler problems where ac-1 is more efficient than other algorithms and significantly faster on hard instances.
1	introduction
arc consistency is a basic technique for solving constraint satisfaction problems  csp  and variations of arc consistency are used in many ai and constraint applications. there have been many algorithms developed for arc consistency such as ac-1  mackworth  1   ac-1  mohr and henderson  1   ac-1  bessiere  1  and ac-1  bessiere et al.  1 . the ac-1 algorithm was proposed in 1  mackworth  1 . the first worst case analysis of ac-1  mackworth and freuder  1  gives a complexity of o ed1   where e is the number of constraints and d the size of largest domain. this result is deeply rooted in the csp literature  eg.  wallace  1; bessiere et al.  1   and thus ac-1 is typically considered to be non-optimal. other algorithms such as ac-1  ac-1  ac-1 are considered theoretically optimal  with time complexity o ed1 . as far as we are aware  there has not been any effort to improve the theoretical bound of ac-1 to be optimal. here  we re-examine ac-1 for a number of reasons. firstly  ac-1 is one of the simplest ac algorithms and is known to be practically efficient  wallace  1 . the simplicity of arc revision in ac-1 makes it convenient for implementation and amenable to various extensions for many constraint systems. thus while ac-1 is considered as being sub-optimal  it often is the algorithm of choice and can outperform other theoretically optimal algorithms.
모in this paper  we show that ac-1 achieves worst case optimal time complexity of o ed1 . this result is surprising since ac-1 being a coarse grained  arc revision  algorithm  mackworth  1   is considered to be non-optimal. the known results for optimal algorithms are all on fine grained  value revision  algorithms. preliminary experiments show that the new ac-1 is comparable to the traditional implementations on easy csp instances where ac-1 is known to be substantially better than the optimal fine grained algorithms. in the hard problem instances such as those from the phase transition  the new ac-1 is significantly better and is comparable to the best known algorithms such as ac-1. we also show that the results for ac-1 can be applied immediately to obtain a path consistency algorithm which has the same time and space complexity as the best known theoretical results. 1
1	background
in this section we give some background material and notation used herein. the definitions for general csp follow  montanari  1; mackworth  1 .
definition 1 a constraint satisfaction problem  n  d  c  consists of a finite set of variables n = {1 몫몫몫 n}  a set of domains d = {d 몫몫몫 d }  where i 뫍 di  and a set of constraints c 뫍 n}  where each constraint cij is a binary relation between variables i and j. for the problem of interest here  we require that  x y x 뫍 di y 뫍 dj  x y  뫍 cij if and only if  y x  뫍 cji.
모for simplicity  in the above definition we consider only binary constraints  omitting the unary constraint on any variable  mackworth  1 . without loss of generality we assume there is only one constraint between each pair of variables.
definition 1 the constraint graph of a csp  n  d  c  is the graph g =  v e  where v = n and e = { i j  |  cij 뫍 c or  cji 뫍 c}.
the arcs in csp refer to the directed edges in g. throughout this paper  n denotes the number of variables  d the size of the largest domain  and e the number of constraints in c.
definition 1 given a csp  n d c   an arc  i j  of its constraint graph is arc consistent if and only if  x 뫍 di  there exists y 뫍 dj such that cij x y  holds. a csp  n d c  is arc consistent if and only if each arc in its constraint graph is arc consistent.
모the ac-1 algorithm for enforcing arc consistency on a csp is given in figure 1. the presentation follows  mackworth  1; mackworth and freuder  1  with a slight change in notation and node consistency removed.

procedure revise  i j   begin
delete 뫹 false for each x 뫍 di do
1.	if there is no y 뫍 dj such that cij x y  then delete x from di;
모모delete 뫹 true endif
return delete
end

figure 1: procedure revise for ac-1

algorithm ac-1
begin
1. q
select and delete any arc  k m  from q;
1. if revise  k m   then
1. 뫹	|	뫍	m endwhile
end

figure 1: the ac-1 algorithm
모the task of revise  i j   in fig 1 is to remove those invalid values not related to any other value with respect to arc  i j . we will show in section 1 that different implementations of line 1 lead to different worst case complexities. as such  we argue that it is more useful to think of ac-1 as a framework rather than a specific algorithm. in ac-1  a csp is modeled by its constraint graph g  and what ac-1 does is to revise all arcs    i  = { k i | k i  뫍 e g }  line 1 in
fig 1  except some special arc if the domain of variable i is modified by revise  i j  . a queue q is used to hold all arcs that need to be revised.the traditional understanding of ac-1 is given by the following theorem whose proof from  mackworth and freuder  1  is modified in order to facilitate the presentation in section 1.
theorem 1  mackworth and freuder  1  given a csp  n  d  c   the time complexity of ac-1 is o ed1 .
모proof consider the times of revision of each arc  i j .  i j  is revised if and only if it enters q. the observation is that arc  i j  enters q if and only if some value of j is deleted  line 1 in fig 1 . so  arc  i j  enters q at most d times and thus is revised d times. given that the number of arcs are 1e  revise i j  is executed o ed  times. the complexity of revise  i j   in fig 1 is at most 
모the reader is referred to  mackworth  1; mackworth and freuder  1  for more details and motivations concerning arc consistency.
1	a new view of ac-1
the traditional view of ac-1 with the worst case time complexity of o ed1   described by theorem 1  is based on a naive implementation of line 1 in fig 1 that y is always searched from scratch. hereafter  for ease of presentation  we call the classical implementation ac-1. the new approach to ac-1 in this paper  called ac-1  is based on the observation that y in line 1 of fig 1 needn't be searched from scratch even though the same arc  i j  may enter q many times  the search is simply resumed from the point where it stopped in the previous revision of  i j . this idea is implemented by procedure existy  i x  j  in fig 1.
모assume without loss of generality that each domain di is associated with a total ordering. resumepoint  i x  j  remembers the first value y 뫍 dj such that cij x y  holds in the previous revision of  i j . the succ y dj1  function  where dj1 denotes the domain of j before arc consistency enforcing  returns the successor of y in the ordering of dj1 or nil  if no such element exists. nil is a value not belonging to any domain and precedes all values in any domain.

procedure existy  i x  j  begin
y 뫹 resumepoint  i x  j ;
1:	if y 뫍 dj then % y is still in the domain return true;
else
1:	while 
if y 뫍 dj and cij x y  then resumepoint  i x  j  뫹 y; return true
endif;
return false
endif
end

figure 1: procedure for searching y in revise i j 
theorem 1 the worst case time complexity of ac-1 can be achieved in o ed1 .
모proof here it is helpful to regard the execution of ac-1 on a csp instance as a sequence of calls to existy  i x  j . consider the time spent on x 뫍 di with respect to  i j . as in theorem 1  an arc  i j  enters q at most d times. so  with respect to  i j   any value x 뫍 di will be passed to existy  i x  j  at most d times . let the complexity of each execution of existy  i x  j  be tl  1 뫞 l 뫞 d . tl can be considered as 1 if y 뫍 dj  see line 1 in fig 1  and otherwise it is sl which is simply the number of elements in dj skipped before next y is found  the while loop in line 1 .
furthermore  the total time spent on x 뫍 di with respect to  where sl = 1 if tl = 1.
observe that in existy  i x  j  the while loop  line 1  will skip an element in dj at most once with respect to x 뫍 di.
therefore . this gives for each arc
 i j   we have to check at most d values in di and thus at most o d1  time will be spent on checking arc  i j . thus  the complexity of the new implementation of ac-1 is o ed1  because the number of arcs in constraint graph of the csp is

모the space complexity of ac-1 is not as good as the traditional implementation of ac-1. ac-1 needs additional space to remember the resumption point of any value with respect to any related constraint. it can be shown that the extra space required is o ed   which is the same as ac-1.
모the same idea behind ac-1 applies to path consistency enforcing algorithms. if one pair  x y  뫍 ckj is removed  we need to recheck all pairs  x    뫍 cij with respect to ckj   cik  the composition of cik and ckj   and    y  뫍 clk with respect to cjk   clj. the resumption point z 뫍 dk is remembered for any pair  x y  of any constraint cij with respect to any intermediate variable k such that cik x z  ckj z y  both hold. resumepoint  i x   j y  k  is employed to achieve the above idea in the algorithm in fig 1 which is partially motivated by the algorithm in  chmeiss and jegou  1 .

algorithm pc
begin initialize q ; while q not empty do
select and delete any   i x  j  from q;
모모revise pc  i x  j q   endwhile
end
procedure initialize q  begin
for any i j k 뫍 n do
for any x 뫍 di y 뫍 dj such that cij x y  do
if there is no z 뫍 dk such that cik x z  뫇 ckj z y  then
cij x y  뫹 false;
else resumepoint  i x   j y  k  뫹 z
end

figure 1: algorithm of path consistency enforcing
모by using a similar analysis to the proof of theorem 1  we have the following result.
theorem 1 the time complexity of the algorithm pc is o n1  with space complexity o n1 .
모the time complexity and space complexity of the pc algorithm here are the same as the best known theoretical results  singh  1 .

begin
for anydo
for any y 뫍 dj such that cij x y  do z 뫹 resumepoint  i x   j y  k ; while not 
모do	z 뫹 succ z dk1 ; if
else resumepoint  i x   j y  k   뫹 z
endfor
end

figure 1: revision procedure for pc algorithm
1	preliminary experimental results
in this paper  we present some preliminary experimental results on the efficiency of ac-1. while arc consistency can be applied in the context of search  such as  bessiere and regin  1    we focus on the performance statistics of the arc consistency algorithms alone.
모the experiments are designed to compare the empirical performance of the new ac-1 algorithm with both the classical ac-1 algorithm and a state-of-the-art algorithm on a range of csp instances with different properties.
모there have been many experimental studies on the performance of general arc consistency algorithms  wallace  1; bessiere  1; bessiere et al.  1 . here  we adopt the choice of problems used in  bessiere et al.  1   namely some random csps  radio link frequency assignment problems  rlfaps  and the zebra problem. the zebra problem is discarded as it is too small for benchmarking. given the experimental results of  bessiere et al.  1   ac-1 is chosen as a representative of a state-of-the-art algorithm because of its good timing performance over the problems of concern. in addition  an artificial problem domino is designed to study the worst case performance of ac-1.
모randomly generated problems: as in  frost et al.  1   a random csp instance is characterized by n d e and the tightness of each constraint. the tightness of a constraint cij is defined to be |di 뫄 dj|   |cij|  the number of pairs not permitted by cij. a randomly generated csp in our experiments is represented by a tuple  n  d  e  tightness . we use the first 1 instances of each of the following random problems generated using the initial seed 1  as in  bessiere et al.  1  :  i  p1: under constrained csps  1 1  where all generated instances are already arc consistent;  ii  p1: over constrained csps  1 1  where all generated instances are inconsistent in the sense that some domain becomes empty in the process of arc consistency enforcing; and  iii  problems in the phase transition  gent et al.  1  p1:  1 1  and p1:  1 1 . the p1 and p1 problems are further separated into the arc consistent instances  labeled as ac  which can be made arc consistent at the end of arc consistency enforcing; and inconsistent instances labeled as inc. more details on the choices for p1 to p1 can be found in  bessiere et al.  1 .
모rlfap: the rlfap  cabon et al.  1  is to assign frequencies to communication links to avoid interference. we use the celar instances of rlfap which are real-life problems available at
ftp://ftp.cs.unh.edu/pub/csp/archive/code/benchmarks.
모domino: informally the domino problem is an undirected constraint graph with a cycle and a trigger constraint. the domains are di = {1 몫몫몫 d}. the constraints are c = {ci i+1  | i   n} 뫋 {c1n} where c1n = { d d } 뫋 { x x + 1  | x   d} is called the trigger constraint and the other constraints in c are identity relations. a domino problem instance is characterized by two parameters n and d. the trigger constraint will make one value invalid during arc consistency and that value will trigger the domino effect on the values of all domains until each domain has only one value d left. so  each revision of an arc in ac-1 algorithms can only remove one value while ac-1 only does the necessary work. this problem is used to illustrate the differences between ac-1 like algorithms and ac-1. the results explain why arc revision oriented algorithms may not be so bad in the worst case as one might imagine.
ac-1ac-1ac-1p1#ccks111time 1 111p1#ccks111time 1 111p1 ac #ccks1 1 1 1time 1 111p1 inc #ccks1 1 1 1time 1 111p1 ac #ccks1 1 11 1time 1 111p1 inc #ccks1 1 11 1time 1 111table 1: randomly generated problems
rflapac-1ac-1ac-1#1#ccks111time 1 111#1#ccks1 1 11 1time 1 111#1#ccks1 1 11 1time 1 111#1#ccks111time 1 111table 1: celar rlfaps
모some details of our implementation of ac-1 and ac-1 are as follows. we implement domain and related operations by employing a double-linked list. the q in ac-1 is implemented as a queue of nodes on which arcs incident will be revised  chmeiss and jegou  1  . a new node will be put
dac-1ac-1ac-1#ccks1 1 11time 1 1111#ccks1 1 11 1time 1 1111#ccks1 1 11 1time 1 111table 1: domino problems
at the end of the queue. constraints in the queue are revised in a fifo order. the code is written in c++ with g++. the experiments are run on a pentium iii 1 processor with linux. for ac-1  we note that in our experiments  using a single currently supported list of a values is faster than using multiple lists with respect to related constraints proposed in  bessiere et al.  1 . this may be one reason why ac-1 is slower than ac-1 in  bessiere et al.  1 . our implementation of ac-1 adopts a single currently supported list.
모the performance of arc consistency algorithms here is measured along two dimensions: running time and number of constraint checks  #ccks . a raw constraint check tests if a pair  x y  where x 뫍 di and y 뫍 dj satisfies constraint cij. in this experiment we assume constraint check is cheap and thus the raw constraint and additional checks  e.g. line 1 in figs 1  in both ac-1 and ac-1 are counted. in the tabulated experiment results  #ccks represents the average number of checks on tested instances  and time x  the time in seconds on x instances.
모the results for randomly generated problems are listed in table 1. for the under constrained problems p1  ac-1 and ac-1 have similar running time. no particular slowdown for ac-1 is observed. in the over constrained problems p1  the performance of ac-1 is close to ac-1 but some constraint checks are saved. in the hard phase transition problems p1 and p1  ac-1 shows significant improvement in terms of both the number of constraint checks and the running time  and is better than or close to ac-1 in timing.
모the results for celar rlfap are given in table 1. in simple problems  rlfap#1 and rlfap#1  which are already arc consistent before the execution of any ac algorithm  no significant slowdown of ac-1 over ac-1 is observed. for rlfap#1 and rlfap#1  ac-1 is faster than both ac-1 and ac-1 in terms of timing.
모the reason why ac-1 takes more time while making less checks can be explained as follows. the main contribution to the slowdown of ac-1 is the maintenance of the currently supported list of each value of all domains. in order to achieve space complexity of o ed   when a value in the currently supported list is checked  the space occupied in the list by that value has to be released. our experiment shows that the overhead of maintaining the list doesn't compensate for the savings from less checks under the assumption that constraint checking is cheap.
모the domino problem is designed to show the gap between ac-1 implementations and ac-1. results in table 1 show that ac-1 is about half the speed of ac-1. this can be explained by a variation of the proof in section 1  in ac1 the time spent on justifying the validity of a value with respect to a constraint is at most 1d while in ac-1 it is at most d. the domino problem also shows that ac-1 is at least an order of magnitude slower in time with more constraint checks over ac-1 and ac-1.
모in summary  our experiments on randomly generated problems and rlfaps show the new approach to ac-1 has a satisfactory performance on both simple problems and hard problems compared with the traditional view of ac-1 and stateof-the-art algorithms.
1	related work and discussion
some related work is the development of general purpose arc consistency algorithms ac-1  ac-1  ac-1   ac-1 and the work of  wallace  1 . we summarize previous algorithms before discussing how this paper gives an insight into ac-1 as compared with the other algorithms.
모an arc consistency algorithm can be classified by its method of propagation. so far  two approaches are employed in known efficient algorithms: arc oriented and value oriented. arc oriented propagation originates from ac-1 and its underlying computation model is the constraint graph. value oriented propagation originates from ac-1 and its underlying computation model is the value based constraint graph.
definition 1 the value based constraint graph of a csp n   and
thus a more rigorous name for the traditional constraint graph may be variable based constraint graph. the key idea of value oriented propagation is that once a value is removed only those values related to it will be checked. thus it is more fine grained than arc oriented propagation. algorithms working with variable and value based constraint graph are also called coarse grained algorithms and fine grained algorithms respectively. an immediate observation is that compared with variable based constraint graph  time complexity analysis in value based constraint graph is straightforward.
모given a computation model of propagation  the algorithms differ in the implementation details. for variable based constraint graph  ac-1  mackworth  1  is an  open implementation . the approach in  mackworth and freuder  1  can be regarded as a realized implementation. the new view of ac-1 presented in this paper can be thought of as another implementation with optimal worst case complexity. our new approach simply remembers the result obtained in previous revision of an arc while in the old one  the choice is to be lazy  forgetting previous computation. other approaches to improving the space complexity of this model is  chmeiss and jegou  1 . for value based constraint graph  ac-1 is the first implementation and ac-1 is a lazy version of ac-1. ac-1 is based on ac-1 and it exploits the bidirectional property that given cij cji and x 뫍 di y 뫍 dj  cij x y  if and only if cji y x .
모another aspect is the general properties or knowledge of a csp which can be isolated from a specific arc consistency enforcing algorithm. examples are ac-1 and ac-inference. we note that the idea of metaknowledge  bessiere et al.  1  can be applied to algorithms of both computing models. for example  in terms of the number of raw constraint checks  the bidirectionality can be employed in coarse grained algorithm  eg. in  gaschnig  1   however it may not be fully exploited under the variable based constraint graph model. other propagation heuristics  wallace  1  such as propagating deletion first  bessiere et al.  1  are also applicable to algorithms of both models. this is another reason why we did not include ac-1 in our experimental comparison.
모we have now a clear picture on the relationship between the new approach to ac-1 and other algorithms. ac-1 and ac-1 are methodologically different. from a technical perspective  the time complexity analysis of the new ac-1 is different from that of ac-1 where the worst case time complexity analysis is straightforward. the point of commonality between the new ac-1 and ac-1 is that they face the same problem: the domain may shrink in the process of arc consistency enforcing and thus the recorded information may not be always correct. this makes some portions of the new implementation of the ac-1 similar to ac-1. we remark that the proof technique in the traditional view of ac-1 does not directly lead to the new ac-1 and its complexity results.
모the number of raw constraint checks is also used to evaluate practical efficiency of csp algorithms. in theory  applying bidirectionality to all algorithms will result in a decrease of raw constraint checks. however  if the cost of raw constraint check is cheap  the overhead of using bidirectionality may not be compensated by its savings as demonstrated by  bessiere et al.  1 .
모it can also be shown that if the same ordering of variables and values are processed  ac-1 and the classical ac-1 have the same number of raw constraint checks. ac-1 and ac1 will make no less raw constraint checks than ac-1 and ac-1 respectively.
모ac-1 does not perform well in practice  wallace  1; bessiere et al.  1  because it reaches the worst case complexity both theoretically and in actual problem instances when constructing the value based constraint graph for the instance. other algorithms like ac-1 and ac-1 can take advantage of some instances being simpler where the worst case doesn't occur. in practice  both artificial and real life problems rarely make algorithms behave in the worst case except for ac-1. however  the value based constraint graph induced from ac-1 provides a convenient and accurate tool for studying arc consistency.
모given that both variable and value based constraint graph can lead to worst case optimal algorithms  we consider their strength on some special constraints: functional  monotonic and anti-functional. for more details  see  van hentenryck et al.  1  and  zhang and yap  1 .
모for coarse grained algorithms  it can be shown that for monotonic and anti-monotonic constraints arc consistency can be done with complexity of o ed   eg. using our new view of ac-1 . with fine grained algorithms  both ac-1 and ac-1 can deal with functional constraints. we remark that the particular distance constraints in rlfap can be enforced to be arc consistent in o ed  by using a coarse grained algorithm. it is difficult for coarse grained algorithm to deal with functional constraints and tricky for fine grained algorithm to monotonic constraints.
모in summary  there are coarse grained and fine grained algorithms which are competitive given their optimal worst case complexity and good empirical performance under varying conditions. in order to further improve the efficiency of arc consistency enforcing  more properties  both general like bidirectionality and special like monotonicity  of constraints and heuristics are desirable.
모 wallace  1  gives detailed experiments comparing the efficiency of ac-1 and ac-1. our work complements this in the sense that with the new implementation  ac-1 now has optimal worst case time complexity.
1	conclusion
this paper presents a natural implementation of ac-1 whose complexity is better than the traditional understanding. ac1 was not previously known to have worst case optimal time complexity even though it is known to be efficient. our new implementation brings ac-1 to o ed1  on par with the other optimal worst case time complexity algorithms. techniques in the new implementation can also be used with path consistency algorithms.
모while worst case time complexity gives us the upper bound on the time complexity  in practice  the running time and number of constraint checks for various csp instances are the prime consideration. our preliminary experiments show that the new implementation significantly reduces the number of constraint checks and the running time of the traditional one on hard arc consistency problems. furthermore  the running time of ac-1 is competitive with the known best algorithms based on the benchmarks from the experiment results in  bessiere et al.  1 . further experiments are planed to have a better comparison with typical algorithms. we believe that based on the celar instances  the new approach to ac1 leads to a more robust ac algorithm for real world problems than other algorithms.
모we also show how the new ac-1 leads to a new algorithm for path consistency. we conjecture from the results of  chmeiss and jegou  1  that this algorithm can give a practical implementation for path consistency.
모for future work  we want to examine the new ac-1 in maintaining arc consistency during search.
1	acknowledgment
we are grateful to christian bessiere for providing benchmarks and discussion. we acknowledge the generosity of the french centre d'electronique de l'armement for providing the celar benchmarks.
references
 bessiere  1  c. bessiere 1. arc-consistency and arcconsistency again  art. int.1  1  1.
 bessiere et al.  1  c. bessiere  e. c. freuder and j. regin 1. using constraint metaknowledge to reduce arc consistency computation  art. int.1  1  1.
 bessiere and regin  1  c. bessiere and j. regin 1. mac and combined heuristics: two reasons to forsake fc and cbj   on hard problems  proc. of principles and practice of constraint programming  cambridge  ma. pp. 1.
 cabon et al.  1  b. cabon  s. de givry  l. lobjois  t. schiex and j.p. warners 1. radio link frequency assignment  constraints 1   1  1.
 chmeiss and jegou  1  a. chmeiss and p. jegou 1. path-consistency: when space misses time  proc. of aaai-1  usa: aaai press.
 frost et al.  1  d. frost  c. bessiere  r. dechter and j. c. regin 1. random uniform csp generators  http://www.lirmm.fr/몲bessiere/generator.html.
 gaschnig  1  j. gaschnig 1. experimental case studies of backtrack vs. waltz-type vs. new algorithms for satisfying assignment problems  proc. of ccscsi-1  1.
 gent et al.  1  j. p. gent  e. maclntyre  p. prosser  p. shar and t. walsh 1. the constrainedness of arc consistency  proc. of principles and practice of constraint programming 1 cambridge  ma  1  pp. 1.
 van hentenryck et al.  1  p. van hentenryck  y. deville  and c. m. teng 1. a generic arc-consistency algorithm and its specializations  art. int.1  1  1- 1.
 mackworth  1  a. k. mackworth 1. consistency in networks of relations   art. int.1   1  1.
 mackworth and freuder  1  a. k. mackworth and e. c. freuder 1. the complexity of some polynomial network consistency algorithms for constraint satisfaction problems   art. int.1  1  1.
 mohr and henderson  1  r. mohr and t. c. henderson 1. arc and path consistency revisited  art. int.1  1  1.
 montanari  1  u. montanari 1. networks of constraints: fundamental properties and applications  information science 1   1  1.
 singh  1  m. singh 1. path consistency revisited  int. journal on art. intelligence tools 1&1   1  1.
 wallace  1  richard j. wallace 1. why ac-1 is almost always better than ac-1 for establishing arc consistency in csps   proc. of ijcai-1 chambery  france  1.
 wallace  1  r. j. wallace and e. freud 1. ordering heuristics for arc consistency algorithms   proc  of canadian conference on ai vancouver  bc  1  pp. 1.
 zhang and yap  1  y. zhang  r. h. c. yap 1. arc consistency on n-ary monotonic and linear constraints  proc. of principles and practice of constraint programming singapore  1  pp. 1 .
temporal constraint reasoning with preferences
lina khatib	paul morris robert morris	francesca rossi
	1. kestrel technology	dipartimento di matematica pura ed applicata1. computational sciences division
nasa ames research center  ms 1
moffett field  ca 1
universita' di padova via belzoni 1  1 padova  italy

abstract
a number of reasoning problems involving the manipulation of temporal information can be viewed as implicitly inducing an ordering of decisions involving time  associated with durations or orderings of events  on the basis of preferences. for example  a pair of events might be constrained to occur in a certain order  and  in addition  it might be preferable that the delay between them be as large  or as small  as possible. this paper explores problems in which a set of temporal constraints is specified  each with preference criteria for making local decisions about the events involved in the constraint. a reasoner must infer a complete solution to the problem such that  to the extent possible  these local preferences are met in the best way. constraint-based temporal reasoning is generalized to allow for reasoning about temporal preferences  and the complexity of the resulting formalism is examined. while in general such problems are np-complete  some restrictions on the shape of the preference functions  and on the structure of the set of preference values  can be enforced to achieve tractability. in these cases  a generalization of a single-source shortest path algorithm can be used to compute a globally preferred solution in polynomial time.
1	introduction and motivation
some real world temporal reasoning problems can naturally be viewed as involving preferences associated with decisions such as how long a single activity should last  when it should occur  or how it should be ordered with respect to other activities. for example  an antenna on an earth orbiting satellite such as landsat 1 must be slewed so that it is pointing at a ground station in order for recorded science data to be downlinked to earth. assume that as part of the daily landsat 1 scheduling activity a window is identified within which a slewing activity to one of the ground stations for one of the antennae can begin  and thus there are choices for assigning the start time for this activity. antenna slewing on landsat 1 has been shown to cause a vibration to the satellite  which in turn affects the quality of the observation taken by the imaginginstrument if the instrumentis in use duringslewing. consequently  it is preferable for the slewing activity not to overlap any scanning activity  although because the detrimental effect on image quality occurs only intermittently  this disjointness is best not expressed as a hard constraint. rather  the constraint is better expressed as follows: if there are any start times within such that no scanning activity occurs during the slewing activity starting at   then is to be preferred. of course  the cascading effects of the decision to assign on the sequencing of other satellite activities must be taken into account as well. for example  the selection of   rather than some earlier start time within   might result in a smaller overall contact period between the ground station and satellite  which in turn might limit the amount of data that can be downlinked during this period. this may conflict with the preference for maintaining maximal contact times with ground stations.
모reasoning simultaneously with hard temporal constraints and preferences  as illustrated in the example just given  is the subject of this paper. the overall objective is to develop a system that will generate solutions to temporal reasoning problems that are globally preferred in the sense that the solutions simultaneously meet  to the best extent possible  all the local preference criteria expressed in the problem.
모in what follows a formalism is described for reasoning about temporal preferences. this formalism is based on a generalization of the temporal constraint satisfaction problem  tcsp  framework  dechter et al  1   with the addition of a mechanism for specifying preferences  based on the semiring-based soft constraint formalism  bistarelli et. al.  1  . the result is a framework for defining problems involving soft temporal constraints. the resulting formulation  called temporal constraint satisfaction problems with preferences  tcspps  is introduced in section 1. a sub-class of tcspps in which each constraint involves only a single interval  called simple temporal problems with preferences  stpps   is also defined. in section 1  we demonstrate the hardness of solving general tcspps and stpps  and pinpoint one source of the hardness to preference functions whose  better  values may form a non-convex set. restricting the class of admissible preference functions to those with convex intervals of  better  values is consequently shown to result in a tractable framework for solving stpps. in section 1  an algorithm is introduced  based on a simple generalization of the single source shortest path algorithm  for finding globally best solutions to stpps with restricted preference functions. in section 1  the work presented here is compared to other approaches and results.
1	temporal constraint problems with preferences
the proposed framework is based on a simple merger of two existing formalisms: temporal constraint satisfaction problems  tcsps   dechter et. al.  1   and soft constraints based on semirings  bistarelli et. al.  1  1. the result of the merger is a class of problems called temporal constraint satisfaction problems with preferences  tcspps . in a tcspp  a soft temporal constraint is represented by a pair consisting of a set of disjoint intervals and a preference function:
	  where	  and	is
a set of preference values.
examples of preference functions involving time are:
min-delay: any function in which smaller distances are preferred  that is  the delay of the second event w.r.t. the first one is minimized.
max-delay: assigning higher preference values to larger distances; close to k: assign higher values to distances which are closer to ; in this way  we specify that the distance between the two events must be as close as possible to .
모as with classical tcsps  the interval component of a soft temporal constraint depicts restrictions either on the start times of events  in which case they are unary   or on the distance between pairs of distinct events  in which case they are binary . for example  a unary constraint over a variable representing an event  restricts the domain of   representing its possible times of occurrence; then the interval constraint is shorthand for	.
a binary constraint over and   restricts the values of the distance   in which case the constraint can be expressed as	. a uniform  binary representation of all the constraints results from introducing a variable for the beginning of time  and recasting unary constraints as binary constraints involving the distance	.
모an interesting special case occurs when each constraint of a tcspp contains a single interval. we call such problems simple temporal problems with preferences  stpps   due to the fact that they generalize stps  dechter et. al.  1 . this case is interesting because stps are polynomially solvable  while general tcsps are np-complete  and the effect of adding preferences to stps is not immediately obvious. the next section discusses these issues in more depth.
모a solution to a tcspp is a complete assignment to all the variables that satisfies the distance constraints. each solution has a global preference value  obtained by combining the local preference values found in the constraints. to formalize the process of combining local preferences into a global preference  and comparing solutions  we impose a semiring structure onto the tcspp framework.
a semiring is a tuple	such that is a set and	;
모  the additive operation  is commutative  associative and is its unit element;
모  the multiplicativeoperation is associative  distributes over   is its unit element and is its absorbing element.
a c-semiring is a semiring in which	is idempotent  i.e. 
모모모모모모모모  	is its absorbing element  and	is commutative.
모c-semirings allow for a partial order relation over to be defined as iff . informally  gives us a way to compare tuples of values and constraints  and
can be read b is better than a. moreover: and are monotone on ; is its minimum and its maximum; is a complete lattice and  for all  
 where lub=least upper bound . if is idempotent  then is a complete distributive lattice and is its greatest lower bound  glb . in our main results  we will assume is idempotent and also restrict to be a total order on the elements of . in this case and
.
모given a choice of semiring with a set of values   each preference function associated with a soft constraint takes an element from and returns an element of . the semiring operations allow for complete solutions to be evaluated in terms of the preference values assigned locally. more precisely  given a solution in a tcspp with associated semiring   let be a soft constraint over variables and be the projection of over the values assigned to variables and  abbreviated as  . then  the corresponding preference value given by is   where . finally  where is a set  and is the multiplicative operator on the semiring  let abbreviate
모모모모모. then the global preference value of     is defined to be	.
모the optimal solutions of a tcspp are those solutions which have the best preference value  where  best  is determined by the ordering of the values in the semiring. for example  consider the semiring
                     used for fuzzy constraint solving  schiex  1 . the preference value of a solution will be the minimum of all the preference values associated with the distances selected by this solution in all constraints  and the best solutions will be those with the maximal value. another example is the semiring
  which is related to solv-
ing classical constraint problems  mackworth  1 . here
there are only two preference values: true and false  the preference value of a complete solution will be determined by the logical and of all the local preferences  and the best solutions will be those with preference value true  since true is better than false in the order induced by logical or . this semiring thus recasts the classical tcsp framework into a tcspp.
모given a constraint network  it is often useful to find the corresponding minimal network in which the constraints are as explicit as possible. this task is normally performed by enforcing various levels of local consistency. for tcspps  in particular  we can define a notion of path consistency. given two soft constraints  and   and a semiring   we define:
	the intersection of two soft constraints	and
모  written   as the soft constraint   where
- returns the pairwise intersection of intervals in and   and
- for all	;
the composition of two soft constraints
	and	  written	  is the soft constraint
  where
- if and only if there exists a value and such that   and
-
모모모모모  where is the generalization of over sets.
	a path-induced constraint on variables	and	is
모  i.e.  the result of performing on each way of composing paths of size two between and	. a constraint	is path-consistent if and only if
모모모모모  i.e.  is at least as strict as . a tcspp is path-consistent if and only if all its constraints are pathconsistent.
모if the multiplicative operation of the semiring is idempotent  then it is easy to prove that applying the operation to any constraint of a tcspp
returns an equivalent tcspp. moreover  under the same condition  applying this operation to a set of constraints returns a final tcspp which is always the same independently of the order of application1. thus any tcspp can be transformed into an equivalent path-consistent tcspp by applying the operation to all constraints until no change occurs in any constraint. this algorithm  which we call path  is proven to be polynomial for tcsps  that is  tcspps with the semiring  : its complexity is   where is the number of variables and is the range of the constraints  dechter et. al.  1 .
모general tcspps over the semiring are np-complete; thus applying path is insufficient to solve them. on the other hand  with stpps over the same semiring that coincide with stps  applying path is sufficient to solve them. in the remaining sections  we prove complexity results for both general tcspps and stpps  and also of some subclasses of problems identified by specific semirings  or preference functions with a certain shape.
1	solving tcspps and stpps is np-complete
as noted above  solving tcsps is np-complete. since the addition of preference functions can only make the problem of finding the optimal solutions more complex  it is obvious that tcspps are at least np-complete as well.
모we turn our attention to the complexity of general stpps. we recall that stps are polynomially solvable  thus one might speculate that the same is true for stpps. however  it is possible to show that in general  stpps fall into the class of np-complete problems.
theorem 1  complexity of stpps  general stpps are npcomplete problems.
proof:
모first  we prove that stpps belong to np. given an instance of the feasibility version of the problem  in which we wish to determine whether there is a solution to the sttp with global preference value   for some   we use as a certificate the set of times assigned to each event. the verificationalgorithm  chops  the set of preference values of each local preference function at . the result of a chop  for each constraint  is a set of intervals of temporal values whose preference values are greater than . the remainder of the verification process reduces to the problem of verifying general temporal csps  tcsp   which is done by non-deterministically choosing an interval on each edge of the tcsp  and solving the resulting stp  which can be done in polynomial time. therefore  sttps belong to np.
모to prove hardness we reduce an arbitrary tcsp to an stpp. thus  consider any tcsp  and take any of its constraints  say	. we will now obtain a corresponding soft temporal constraint containing just one interval  thus belonging to an stpp . the semiring that we will use for the resulting stpp is the classical one:
. thus the only
two allowed preference values are false and true  or 1 and
1 . assuming that the intervals in are ordered such that for   the interval of the soft constraint is just . the preference function will give value 1 to values in and 1 to the others. thus we have obtained an stpp whose set of solutions with value  which are the optimal solutions  since in the chosen semiring  coincides with the set of solutions of the given tcsp. since finding the set of solutions of a tcsp is np-hard  it follows that the problem of finding the set of optimal solutions to an stpp is np-hard.
1	linear and semi-convex preference functions
the hardness result for stpps derives either from the nature of the semiring or the shape of the preference functions. in this section  we identifyclasses of preferencefunctionswhich define tractable subclasses of stpps.
모when the preference functions of an stpp are linear  and the semiring chosen is such that its two operations maintain such linearity when applied to the initial preferencefunctions  the given stpp can be written as a linear programming problem  solving which is tractable  cormen et. al.  1 . thus 

figure 1: examples of semi-convex functions  a - f  and non-semiconvex functions  g - i 
consider any given tcspp. for any pair of variables	and
  take each interval for the constraint over	and	  say
모모  with associated linear preference function . the information given by each of such intervals can be represented by the following inequalities and equation:  
모모모모모모  and	. then if we choose the fuzzy semiring   the global preference value will satisfy the inequality for each preference function defined in the problem  and the objective is . if instead we choose the semiring   where the objective is to minimize the sum of the preference levels  we have and the objective is 1. in both cases the resulting set of formulas constitutes a linear programming problem.
모linear preference functions are expressive enough for many cases  but there are also several situations in which we need preference functions which are not linear. a typical example arises when we want to state that the distance between two variables must be as close as possible to a single value. unless this value is one of the extremes of the interval  the preference function is convex  but not linear. another case is one in which preferred values are as close as possible to a single distance value  but in which there are some subintervals where all values have the same preference. in this case  the preferencecriteria define a step function  which is not convex.
모a class of functions which includes linear  convex  and also some step functions will be called semi-convex functions. semi-convex functions have the property that if one draws a horizontal line anywhere in the cartesian plane defined by the function  the set of such that is not below the line forms an interval. figure 1 shows examples of semi-convex and non-semi-convex functions.
모more formally  a semi-convex function is one such that  for all   the set such that forms an interval. it is easy to see that semi-convex functions include linear ones  as well as convex and some step functions. for example  the close to k criteria cannot be coded into a linear preference function  but it can be specified by a semi-convex preference function  which could be for and for	.
모semi-convex functions are closed under the operations of intersection and composition defined in section 1  when certain semirings are chosen. for example  this happens with the fuzzy semiring  where the intersection performs the min  and composition performs the max operation. the closure proofs follow.
theorem 1  closure under intersection  the property of functions being semi-convex is preserved under intersection. that is  given a totally-ordered semiring with an idempotent multiplicative operation and binary additive operation  or over an arbitrary set of elements   let and be semi-convex functions which return values over the semiring.
let be definedas   where is the multiplicative operation of the semiring. then is a semi-convex function as well.
proof: from the definition of semi-convex functions  it suffices to prove that  for any given   the set identifies an interval. if is empty  then it identifies the empty interval. in the following we assume to be not empty.
 	is a lower bound operator since it is assumed to be idempotent 
 since each of	and	is semi-convex 
theorem 1  closure under composition  the property of functions being semi-convex is preserved under composition. that is  given a totally-ordered semiring with an idempotent multiplicative operation and binary additive operation  or over an arbitrary set of elements   let and be semi-convex functions which return values over the semiring.
define as	. then is a semi-convex function as well.
proof: again  from the definition of semi-convex functions  it suffices to prove that  for any given   the set identifies an interval. if is empty  then it identifies the empty interval. in the following we assume to be not empty.
 since	is an upper bound operator 
	for some	and
such that
               for some	and such that
 	is a lower bound operator since it is assumed to be idempotent 
for some
	for some	and some
 since each of	and	is semi-convex 
모that closure of the set of semi-convex functions requires a total order and idempotence of the operator is demonstrated by the following example. in what follows we assume monotonicity of the operator. let a and b be preference values with       and .
suppose	and	are real numbers with	. define for	and	otherwise. also define for	and	otherwise. clearly 
and	are semi-convex functions. define	. note
that	for	 	for
and for . since includes all values except where   is not semi-convex.
모now consider the situation where the partial order is not total. then there are distinct incomparablevalues a and b that satisfy the condition of the example. we conclude the order must be total. next consider the case in which idempotence is not satisfied. then there is a preference value c such that
모. it follows that . in this case  setting satisfies the condition of the example. we conclude
that idempotence is also required.
모the results in this section imply that applying the path algorithm to an stpp with only semi-convex preference functions  and whose underlying semiring contains a multiplicative operation that is idempotent  and whose values are totally ordered will result in a network whose induced soft constraints also contain semi-convexpreferencefunctions. these results will be applied in the next section.
1	solving stpps with semi-convex functions is tractable
we will now prove that stpps with semi-convex preference functions and an underlying semiring with an idempotent multiplicative operation can be solved tractably.
모first  we describe a way of transforming an arbitrary stpp with semi-convex preference functions into a stp. given an stpp and an underlying semiring with the set of preference values  let and be a soft constraint defined on variables in the stpp  where is semi-convex.
consider the interval defined by
 because is semi-convex  this set defines an interval for any choice of  . let this interval define a constraint on the same pair . performing this transformation on each soft constraint in the original stpp results in an stp  which we refer to as .  notice that not every choice of will yield an stp that is solvable.  let be the highest preference value  in the ordering induced by the semiring  such that has a solution. we will now prove that the solutions
of	are the optimal solutions of the given stpp.
theorem 1 consider any stpp with semi-convex preference functions over a totally-ordered semiring with idempotent.
take as the highest such that has a solution. then the solutions of are the optimal solutions of the stpp.
proof: first we provethat every solution of is an optimal solution of stpp. take any solution of   say .
this instantiation   in the original stpp  has value
	  where	is the distance	for an as-
signment to the variables	 	  and
is the preference function associated with the soft constraint
	  with	.
모now assume for the purpose of contradiction that is not optimal in stpp. that is  there is another instantiation such that . since	  by monotonicity of the   we can have only if each of the is greater than the corresponding . but this means that we can take the smallest such value   call it   and construct . it is easy to see that has at least one solution    therefore is not the highest value of   contradicting our assumption.
모next we prove that every optimal solution of the stpp is a solution of . take any optimal for stpp  and assume it is not a solution of . this means that  for some constraint  . therefore  if we compute in stpp  we have that . then take any solution of  there are some  by construction of  . if we compute in stpp  since = glb  we assume idempotent   we have that   thus was not optimal as initially assumed.
모this result implies that finding an optimal solution of the givenstpp with semi-convexpreferencefunctionsreducesto a two-step search process consisting of iteratively choosing a
   then solving   until is found. under certain conditions  both phases can be performedin polynomial time  and hence the entire process can be tractable.
모the first phase can be conducted naively by trying every possible  chop  point and checking whether has a solution. a binary search is also possible. under certain conditions  it is possible to see that the number of chop points is also polynomial  namely:
if the semiring has a finite number of elements  which is at most exponential in the number of variables of the given stpp  then a polynomial number of checks is enough using binary search.
if the semiring has a countably infinite number of elements  and the preference functions never go to infinity  then let be the highest preference level given by the functions. if the number of values not above is at most exponential in   then again we can find in a polynomial number of steps.
모the second phase  solving the induced   can be performed by transforming the graph associated with this stp into a distance graph  then solving two single-source shortest path problems on the distance graph  dechter et. al.  1 . if the problem has a solution  then for each event it is possible to arbitrarily pick a time within its time bounds  and find corresponding times for the other events such that the set of times for all the events satisfy the interval constraints. the
	imaging	b	c
slewing
ad
preference
function
figure 1: non-semi-convex preference function for the landsat problem
complexity of this phase is  using the bellman-ford algorithm  cormen et. al.  1  .
모the main result of this discussion is that  while generaltcspps are np-complete  there are sub-classes of tcspp problems which are polynomially solvable. important sources of tractability include the shape of the temporal preference functions  and the choice of the underlyingsemiring for constructing and comparing preference values.
모despite this encouraging theoretical result  the extent to which real world preferences conform to the conditions necessary to utilize the result is not clear. to illustrate this  consider again the motivating example at the outset. as illustrated in figure 1  suppose an imaging event is constrained to occur during   and that the interval is the interval during which a slewing event can start to occur. assuming the soft constraint that prefers no overlap between the two occurrences  the preference values for the slewing can be visualized as the function pictured below the interval  a function that is not semi-convex. a semi-convex preference function would result by squeezing one or the other of the endpoints of the possible slewing times far enough that the interval would no longer contain the imaging time. for example  removing the initial segment from the interval of slewing times would result in a semi-convex preference function. dealing with the general case in which preference functions are not semi-convex is a topic of future work.
1	related work
the merging of temporal csps with soft constraints was first proposed in  morris and khatib  1   where it was used within a framework for reasoning about recurring events. the framework proposed in  rabideau et. al.  1  contains a representation of local preferences that is similar to the one proposed here  but uses local search  rather than constraint propagation as the primarymechanismforfinding goodcomplete solutions  and no guarantee of optimality can be demonstrated.
모finally  the property that characterizes semi-convex preference functions  viz.  the convexity of the interval above any horizontal line drawn in the cartesian plane around the function  is reminiscent of the notion of row-convexity  used in characterizing constraint networks whose global consistency  and hence tractability in solving  can be determined by applying local  path  consistency  van beek and dechter  1 . there are a number of ways to view this connection. one way is to note that the row convexconditionfor the 1 matrix representation of binary constraints prohibits a row in which a sequence of ones is interrupted by one or more zeros. replacing the ones in the matrix by the preference value for that pair of domain elements  one can generalize the definition of row convexityto prohibit rows in which the preference values decrease then increase. this is the intuitive idea underlying the behavior of semi-convex preference functions.
1	summary
we have defined a formalism for characterizing problems involving temporal constraints over the distances and duration of certain events  as well as preferences over such distances. this formalism merges two existing frameworks  temporal csps and soft constraints  and inherits from them their generality  and also allows for a rigorous examination of computational properties that result from the merger.
acknowledgments
thanks to david mcallister  rina dechter and alessandro sperduti for helpful comments in earlier stages of the development of this work. this work was partially funded by the research institute for advanced computer science  nasa ames research center  and by the ec tmr network getgrats  general theory of graph transformation systems .
references
 bistarelli et al.  1  s. bistarelli  u. montanari  and f. rossi. semiring-based constraint solving and optimization. journal of the acm  1 :1  march 1.
 cormen et al.  1  t.h. cormen  c.e. leiserson  and r.l. rivest. introduction to algorithms. mit press  cambridge  ma  1.
 dechter et al.  1  r. dechter  i. meiri  and j. pearl. temporal constraint networks. artificial intelligence  1  1-1  1.
 freuder and wallace   1  e. c. freuder and r. j. wallace. partial constraint satisfaction. artificial intelligence  1  1.
 mackworth   1  a.k. mackworth. consistency in networks of relations. artificial intelligence  1 :1  1.
 schiex et al.  1  t. schiex  h. fargier  and g. verfaille. valued constraint satisfaction problems: hard and easy problems. in proceedings of ijcai1  pages 1- 1. morgan kaufmann  1.
 morris and khatib   1  r. morris  and l. khatib. reasoning about recurring events: satisfaction and optimization. in computational intelligence  1   1.
 rabideau et al.  1  g. rabideau  b. engelhardt and s. chien. using generic preferences to incrementally improve plan quality. in proceedings of the 1nd nasa internationalworkshop on planningand schedulingfor space  1  march  1.
 van beek and dechter   1  p. van beek  and r. dechter. on the minimality and global consistency of rowconvex constraint networks. in journal of the acm  1  1  1.
a hybrid approach for the 1 multidimensional knapsack problem
michel vasquez 1  and jin-kao hao 1 
1  ema-eerie  parc scientifique g. besse  f-1 nimes cedex 1  vasquez site-eerie.ema.fr
1  universite뫣 d'angers  1 bd lavoisier  f-1 angers cedex 1  hao info.univ-angers.frabstract
we present a hybrid approach for the 1 multidimensional knapsack problem. the proposed approach combines linear programming and tabu search. the resulting algorithm improves significantly on the best known results of a set of more than 1 benchmark instances.
1	introduction
the np-hard 1 multidimensional knapsack problem  mkp1  consists in selecting a subset of given objects  or items  in such a way that the total profit of the selected objects is maximized while a set of knapsack constraints are satisfied. more formally  the mkp1 can be stated as follows.
         maximize	subject to mkp1
and
where in   in and in . the binary components of are decision variables: if the object is selected  otherwise. is the profit associated to . each of the constraints is called a knapsack constraint.
모the special case of the mkp1 with is the classical knapsack problem  kp1   whose usual statement is the following. given a knapsack of capacity and objects  each being associated a profit  gain  and a volume occupation  one wants to select   and not fixed  objects such that the total profit is maximized and the capacity of the knapsack is not exceeded. it is well known that the kp1 is not strongly np-hard because there are polynomial approximation algorithms to solve it. this is not the case for the general mkp1. the mkp1 can formulate many practical problems such as capital budgeting where project has profit and consume units of resource . the goal is to determine a subset of the projects such that the total profit is maximized and all resource constraints are satisfied. other important applications include cargo loading  shih  1   cutting stock problems  and processor allocation in distributed systems  gavish and pirkul  1 . notice that the mkp1 can be considered as a general 1 integer programming problem with non-negative coefficients.
모given the practical and theoretical importance of the mkp1  it is not surprising to find a large number of studies in the literature; we give a brief review of these studies in the next section.
1	state of the art
like for many np-hard combinatorial optimizationproblems  both exact and heuristic algorithms have been developed for the mkp1. existing exact algorithms are essentially based on the branch and bound method  shih  1 . these algorithms are different one from another according to the way the upper bounds are obtained. for instance  in  shih  1   shih solves  exactly  each of the single constrained  relaxed knapsack problems and select the minimum of the objective function values as the upper bound. better algorithms have been proposed by using tighter upper bounds  obtained with other mkp1 relaxation techniques such as lagrangean  surrogate and composite relaxations  gavish and pirkul  1 . due to their exponential time complexity  exact algorithms are limited to small size instances   and	 .
모heuristic algorithms are designed to produce near-optimal solutions for larger problem instances. the first heuristic approach for the mkp1 concerns for a large part greedy methods. these algorithms construct a solution by adding  according to a greedy criterion  one object each time into a current solution without constraint violation. the second heuristic approach is based on linear programming by solving various relaxations of the mkp1. a first possibility is to relaxe the integrality constraints and solve optimally the relaxed problem with simplex. other possibilities include surrogate and composite relaxations  osorio et al.  1 .
모more recently  several algorithms based on metaheuristics have been developed  including simulated annealing  drexl  1   tabu search  glover and kochenberger  1; hanafi and fre뫣ville  1  and genetic algorithms  chu and beasley  1 . the metaheuristic approachhas allowed to obtain very competitive results on large instances compared with other methods   and  .
모most of the above heuristics use the so-called pseudo- utility criterion for selecting the objects to be added into a solution. for the single constraint case  the kp1   this criterion corresponds to the profit/resource ratio. for the general mkp1  the pseudo-utility is usually defined as
where is a multiplier for the column vector . notice that it is impossible to obtain  optimal  multipliers. thus the pseudo-utility criterion may mislead the search.
모in this paper  we propose an alternative way to explore the search space. we use fractional optimal solutions given by linear programming to guide a neighborhood search  tabu search or ts  algorithm. the main idea is to exploit around a fractional optimal solution with additional constraints. we experiment this approach on a very large variety of mkp1 benchmark instances and compare our results with the best known ones. we show that this hybrid approach outperforms previously best algorithms for the set of tested instances.
모the paper is organized as follows. next section presents the general scope of our approach. section 1 describes the algorithm used to determine promising sub-space of the whole search space from where we run a ts algorithm. this algorithm is presented in section 1. finally we give computational results on a large set of mkp1 instances in section 1.
1	a hybrid approach for the mkp1
the basic idea of our approach is to search around the fractional optimum of some relaxed mkp. our hypothesis is that the search space around should contain high quality solutions1.
모this general principle can be put into practice via a two phase approach. the first phase consists in solving exactly a relaxation mkp of the initial mkp1 to find a fractional optimal solution . the second phase consists in exploring carefully and efficiently some areas around this fractional optimal point.
모clearly  there are many possible ways to implement this general approach. for this work  we have done the following choices. the first phase is carried out by using simplex to solve a relaxed mkp. the second phase is ensured by a tabu search algorithm.
모let us notice first that relaxing the integrality constraints alone may not be sufficient since its optimal solution may be far away from an optimal binary solution. to be convinced  let us consider the following small example with five objects and one knapsack constraint:
this relaxed problem leads to the fractional optimal solution  with an optimal cost value while the optimal binary solution is with an optimal cost value .
모however  the above relaxation can be enforced to give more precise information regarding the discrete aspect of the problem. to do this  let us notice that all solutions of mkp1 verify the property: in with an integer.
now if we add this constraint to the relaxed mkp  we obtain a series of problems:
	maximize	s.t.
mkp   	and	-	and in
where is the sum of the components of   defining a hyperplane. we have thus several  promising  points around which a careful search will be carried out.
to show the interest of this extra constraint  take again the
previous example with problems:  leading to three relaxedmkp   etmkp   etmkp   etwhere gives directly an optimal binary solution without further search.
모in general cases  search is necessary to explorearoundeach fractional optimum and this is carried out by an effective ts algorithm  section 1 . in order not to go far away from each fractional optimum   we constraint ts to explore only the points such that   the distance between and is no greater than a given threshold 1.
모to sum up  the proposed approach is composed of three steps:
1. determine interesting hyperplanes	;
1. run simplex to obtain the corresponding fractional optimum ;
1. run tabu search around each	  limited to a sphere of fixed radius.
1	determine hyperplanes	and
simplex phase
given an size instance of mkp1  it is obviousthat the values do not have the same interest regarding the optimum. furthermore exploring all the hyperplanes would be too much time consuming. we propose  in this
section  a simple way to compute good values of	.
모starting from a 1 lower bound	 i.e. a feasible solution obtained by a previous heuristic   the method consists in solving  with the simplex algorithm  the two following problems: minimize	s.t.
	mkp	   	and	-	and
let	be the optimal value of this problem. a knapsack that holds fewer items than	respects no longer the constraint	. maximize	s.t.
	mkp	   	and	-	and
let be the optimal value of this problem. it is not possible to hold more items than without violate one of the constraints .
모consequently  local search will run only in the hyperplanes for which is bounded by and . hence we compute  using once again the simplex algorithm  the relaxed mkp which give us the con-
tinuous optima	which are used by the ts phase.
1	tabu search phase
a comprehensive introduction of tabu search may be found in  glover and laguna  1 . we give briefly below some notations necessary for the understandingof our ts algorithm
.
1	notations
the following notations are specifically oriented to the mkp1:
a configuration is a binary vector with components; the unconstrained search space is defined to equal the set   including both feasible and unfeasible con-
figurations;
a move consists in changing a small set of components of giving and is denoted by ;
in this binary context  the flipped variables of a move can be identified by their indexes in the vector: these indexes are the attributes of the move; the neighborhood of     is the subset of configurations reachable from in one move. in this binary context  a move from to can be identified without ambiguity by the attribute if is obtained by flipping the element of . more generally  we use to denote the move where
. such a move is called a
		.
1	search space reduction
regarding the knapsack constraints      is the completely relaxed search space. it is also the largest possible search space. we specify in this section a reduced search space which will be explored by our tabu search algorithm. to define this reduced space  we take the ideas discussed previously  section 1 :
1. limitation of to a sphere of fixed radius around the point   the optimal solution of the relaxed mkp .
	that is:	;
1. limitation of the number of objects taken in the configurations   that are visited by the tabu search algorithm  to the constant  intersection of and the hyperplane
 .
모for the first point  we use to define the distance where et may be binary or continuous. we use the following heuristic to estimate the maximal distance authorized from the starting point . let be the elements of the vec-
tor sorted in decreasing order. are fractional components of and we have:	.
let u be the number of the components having the value of 1 in . in the worst case  we select items rather than the components. with that gives
. if	  it follows	that corresponds to the case where	is a binary vector. depending on the
mkp1 instances  we choose . hence  each tabu process running around has its own search space :
note that all are disjoint  this is a good feature for a distributed implementation of our approach. finally  in order to further limit ts to interesting areas of   we add a qualitative constraint on visited configurations:
where is the best value of a feasible configuration found so far.
1	neighborhood
a neighborhoodwhich satisfies the constraint is the classical add/drop neighborhood: we remove an object from the current configuration and add another object to it at the same time. this neighborhood is used in this study. the set of neighboring configurations of a configuration is thus defined by the formula:
it is easy to see that this neighborhood is a special ase of
 	 cf. sect. 1 . we use in the following sections indifferently	or	with	and
           . as ts evaluates the whole neighborhood for a move  we have a time complexity of for each iteration of .
1	tabu list management
the reverse elimination method      introduced by fred glover  glover  1   leads to an exact tabu status  i.e. tabu has been visited . it consists in storing in a running list the attributes  pair of components  of all completed moves. telling if a move is forbidden or not needs to trace back the running list. doing so  one builds another list  the so-called residual cancellation sequence     in which attributes are either added  if they are not yet in the
모모  or dropped otherwise. the condition corresponds to a move leading to a yet visited configuration. for more details on this method see  dammeyer and vo   1;
glover  1 . has a time complexity   is the current value of the move counter .
모for our specific  move  we need only one trace of the running list. each time we meet   the move with and attributes is said tabu. let be the move counter. the following algorithm updates the tabu status of the whole neighborhood of a configuration and corresponds to the equivalence
algorithm 1: updatetabu
모모모모% end of running list repeat
running list then
else then
until
this algorithm traces back the running list table from its entry to its entry.
1	evaluation function
we use a two components evaluation function to assess the configurations :	and
모모. to make a move from   we take among the configuration defined by the following relation:
	such that	or
and
random choice is used to break ties.
	each time	the running list is reset and
is updated. hence the tabu algorithm explores areas of the search space where trying to reach feasibility.
1	tabu algorithm
based on the above considerations  our ts algorithm for mkp1     works on a series of problems like:
	find	such that
where is a strictly increasing sequence. let be the running list size. considering the last feature of our algorithm given just above  sect. 1   is twice the maximum number of iterations without improvement of the cost function. the starting configuration is built with the following simple greedy heuristic: choose the items   which have the largest value.
1	computational results
all the procedures of have been coded in c language. our algorithm has been executed on different kind of cpu  up to 1 distributed computers like pii1  piii1 
ultrasparc1 and 1 . for all the instances solved below  we run with 1 random seeds  1..1  of the standard srand   c function.
모we have first tested our approach on the 1 classical problems used in  aboudi and jo몮rnsten  1; balas and martin  1; chu and beasley  1; dammeyer and vo   1; drexl  1; fre뫣ville and plateau  1; glover and kochenberger  1; shih  1; toyoda  1 . the size of these problems varies from =1 to 1 items and from =1 to 1 constraints. these instances are easy to solve for state-of-theart algorithms. indeed  our approach finds the optimal value
algorithm 1:
% move counter
	if	then
	; else
		;
repeat
do
then
%	evaluation then
then
 
% restore old values
then
% complete move
then
%reset the running list
running list
until
 known for all these instances  in an average time of 1 second  details are thus omitted . the running list size     is fixed to 1.
모for the next two sets of problems  the running list size     is fixed to 1. hence the maximum number of moves without improvement is 1.
모the second set of tested instances is constituted of the last seven  also the largest ones with 1 to 1 items  1 to 1 constraints  of 1 benchmarks proposed by glover and kochenberger  glover and kochenberger  1 . these instances are known to be particularly difficult to solve for branch & bound algorithms. table 1 shows a comparison between our results  columns 1 to 1  and the best known ones reported in  hanafi and fre뫣ville  1   column  .

table 1:comparativeresultsonthe1largestgkpb.
모column shows the number of items of the best solution with cost found by . from the table  we observe that all the results are improved. columns and give the number of moves and the time elapsed to reach . we know that the algorithm runs 1 moves after . the search process takes thus an average of 1 seconds for the test problems gk1...gk1  1 seconds for gk1 and 1 seconds for gk1. last column gives the hyperplane's values visited by .
the third set of test problems concerns the 1 largest benchmarks  	items 	constraints  of or-
library1  proposed recently by chu and beasley  chu and beasley  1 .
cbtable 1:comparativeresultsonthe1largestcbpb.
모table 1 compares our results with those reported in  chu and beasley  1       which are among the best results for these instances. from the table  we see that our approach improves significantly on all these results. the average time of the 1 last iterations is equal to 1 seconds for these instances.
모column     shows that an average of seven hyperplanes are scanned by the procedure for a given instance.
these results can be further improved by giving more
cpu time  iterations  to our algorithm. for example  with
                	finds	after 1 seconds for the instance cb1.1.
모the chu and beasley benchmark contains 1 instances with 1 variables: 1 instances with	=1 constraints  1 with	and 1 with	 results detailed just above . each set of 1 instances is divided into 1 series with and	. table
1 compares  for each subset of 1 instances  the averages of the best results obtained by   those obtained more recently by osorio  glover and hammer  osorio et al.  1   columns 1 and 1  and those by  column 1 . the new algorithm of  osorio et al.  1  uses advanced techniques such as cutting and surrogate constraint analysis  see column fix+cuts for results . we reproduce also from  osorio et al.  1   in column   the best values obtained by the mip solver cplex v1.1 alone.
fix+cutscplextable 1: averageperformanceoverthe1largestcb pb.
모the column indicates the average gap values in percentage between the continuousrelaxed optimumand the best cost value found: . and algorithms were stopped after 1 hours of computing or when the tree size memory of 1m bytes was exceeded. our best results were obtained with and the algorithm never requires more than 1m bytes of memory. except for the instances with and our approachoutperforms all the other algorithms.
모to finish the presentation on chu and beasley benchmarks  table 1 and 1 show the best values obtained by our algorithm on the 1 cb1 and the 1 cb1 instances.
xxxxxx111111111111111111111111111111111111111111111table 1: bestvaluesforcb1.xx
xxxxxx111111111111111111111111111111111111111111111table 1: bestvaluesforcb1.xx
모to conclude this section  we present our results on the 1 latest instances proposed very recently by glover and kochenberger  available at: http://hces.bus. olemiss.edu/tools.html.  these benchmarks contain up to =1 items and =1 constraints  thus are very large. table 1 compares our results  columns 1 and 1  and the best known results taken from the above web site. once again  our approach gives improved solutions for 1 out of 1 instances. let us mention that the experimentation on these instances showed some limits of our approach regarding the computing time for solving some very large instances    . indeed  given the size of our neighborhood     see section 1   up to 1 days were needed to get the reported values.

table 1: comparisononthelastest1mkgk pb.
1	conclusion
in this paper  we have presented a hybrid approach for tackling the np-hard 1 multidimensional knapsack problem  mkp1 . the proposed approach combines linear progamming and tabu search. the basic idea consists in obtaining  promising  continuous optima and then using ts to explore carefully and efficiently binary areas close to these continuous optima. this is carried out by introducing several additional constraints:
1. hyperplane constraint:in;1. geometrical constraint:;1. qualitative constraint:.모our tabu search algorithm integrates also some advanced features such as an efficient implementation of the reverse elimination method for a dynamic tabu list management in the context of a special 1-change move.
모this hybrid approach has been tested on more than 1 state-of-the-art benchmark instances  and has led to improved solutions for most of the tested instances.
모one inconvenience of the proposed approach is its computing time for very large instances   items . this is partially due to the time complexity of the neighborhood used. despite of this  this study constitutes a promising starting point for designing more efficient algorithms for the mkp1. some possibilities are: 1  to develop a partial evaluation of relaxed knapsack constraints; 1  to study more precisely the relationship between and for a given instance; 1  to find a good crossover operator and a cooperative distributed implementation of the algorithm.
모finally  we hope the basic idea behind the proposed approach may be explored to tackle other np-hard problems.
acknowledgement: we would like to thank the reviewers of the paper for their useful comments.
references
 aboudi and jo몮rnsten  1  r. aboudi and k. jo몮rnsten. tabu search for general zero-one integer programs using the pivot and complement heuristic. orsa journal of computing  1 :1  1.
 balas and martin  1  e. balas and c.h. martin. pivot and a complement heuristic for 1 programming. management science  1-1  1.
 chu and beasley  1  p.c. chu and j.e. beasley. a genetic algorithm for the multidimensional knapsack problem. journal of heuristic  1-1  1.
 dammeyer and vo   1  f. dammeyer and s. vo . dynamic tabu list management using the reverse elimination method. annals of operations research  1-1  1.
 drexl  1  a. drexl. a simulated annealing approach to the multiconstraint zero-one knapsack problem. computing  1-1  1.
 fre뫣ville and plateau  1  a. fre뫣ville and g. plateau. sac a` dos multidimensionnel en variable 1 : encadrement de la somme des variables a` l'optimum. recherche
	operationnelle뫣	  1 :1  1.
 gavish and pirkul  1  b. gavish and h. pirkul. allocation of data bases and processors in a distributed computting system. management of distributed data processing  1-1  1.
 gavish and pirkul  1  b. gavish and h. pirkul. efficient algorithms for solving multiconstraint zero-one knapsack problems to optimality. mathematical programming  1-1  1.
 glover and kochenberger  1  f. glover and g.a. kochenberger. critical event tabu search for multidimensional knapsack problems. in i.h. osman j.p. kelly  editor  metaheuristics: the theory and applications  pages 1. kluwer academic publishers  1.
 glover and laguna  1  f. glover and m. laguna. tabu search. kluwer academic publishers  1.
 glover  1  f. glover. tabu search. orsa journal of computing  1-1  1.
 hanafi and fre뫣ville  1  s. hanafi and a. fre뫣ville. an efficient tabu search approach for the 1 multidimensional knapsack problem. european journal of operational research  1-1  1.
 osorio et al.  1  m.a. osorio  f. glover  and peter hammer. cutting and surrogate constraint analysis for improved multidimensional knapsack solutions. technical report  hearin center for enterprise science. report hces-1  1.
 shih  1  w. shih. a branch & bound method for the multiconstraint zero-oneknapsack problem. journal of the operational research society  1-1  1.
 toyoda  1  y. toyoda. a simplified algorithm for obtaining approximate solutions to zero-one programming problem. management science  1 :1 1.
the exponentiated subgradient algorithm for heuristic boolean programming
dale schuurmans and finnegan southey	robert c. holtedepartment of computer science
   university of waterloo dale fdjsouth  cs.uwaterloo.ca department of computing science university of alberta holte cs.ualberta.ca

abstract
boolean linear programs  blps  are ubiquitous in ai. satisfiability testing  planning with resource constraints  and winner determination in combinatorial auctions are all examples of this type of problem. althoughincreasingly well-informed by work in or  current ai research has tended to focus on specialized algorithms for each type of blp task and has only loosely patterned new algorithms on effective methodsfrom other tasks. in thispaper we introducea singlegeneral-purposelocal search procedure that can be simultaneouslyapplied tothe entire range of blp problems  without modification. although one might suspect that a general-purpose algorithm might not perform as well as specialized algorithms  we find that this is not the case here. our experiments show that our generic algorithm simultaneously achieves performance comparable with the state of the art in satisfiability search and winner determination in combinatorial auctions- two very different blp problems. our algorithm is simple  and combines an old idea from or with recent ideas from ai.
1	introduction
a boolean linear program is a constrained optimizationproblem where one must choose a set of binary assignments to variables to satisfy a given set of linear inequalities while simultaneously optimizing a linear side objective . specifically  we consider blp problems of the canonical form
	subject to	 1 
clearly this is just a special case of integer linear programming  ilp  commonly referred to as 1 integer programming.1 many important problems in ai can be naturally expressed as blp problems; for example: finding a satisfying truth assignment for cnf propositional formulae  sat 

 work performed while at the university of ottawa. 1
although one could alternatively restrict the choice of values to we often find it more convenient to use .
 kautz and selman  1   winner determination in combinatorial auctions  ca   sandholm  1; fujishima et al.  1   planning with resource constraints  kautz and walser  1; vossen et al.  1   and scheduling and configuration problems  walser  1; lau et al.  1 . the two specific problems we will investigate in detail below are sat and ca.
모in general  blp problems are hard in the worst case  np-hard as optimization problems  np-complete as decision problems . nevertheless  they remain important problems and extensive research has been invested in improving the exponential running times of algorithms that guarantee optimal answers  developing heuristic methods that approximate optimal answers  and identifying tractable subcases and efficient algorithms for these subcases. there is a fundamental distinctionbetween complete methods  which are guaranteed to terminate and produce an optimal solution to the problem  and incomplete methods  which make no such guarantees but nevertheless often produce good answers reasonably quickly. most complete methods conduct a systematic search through the variable assignment space and use pruning rules and ordering heuristics to reduce the number of assignments visited while preserving correctness. incomplete methods on the other hand typicallyemploy local search strategies that investigate a small neighborhood of a current variable assignment  by flipping one or a small number of assignments  and take greedy steps by evaluating neighbors under a heuristic evaluation function. although complete methods might appear to be more principled  incomplete search methods have proven to be surprisingly effective in many domains  and have led to significant progress in some fields of research  most notably on sat problems  kautz and selman  1  but more recently on ca problems as well  hoos and boutilier  1  .
모in this paper we investigate incomplete local search methods for general blp problems. although impressive  most of the recent breakthroughs in incomplete local search have been achieved by tailoringmethods to a reduced class of blp problems. a good illustration of this point is the ca system of hoos and boutilier   casanova  which is based on the sat system of  hoos  1; mcallester et al.  1   novelty+   but is not the same algorithm  neither of these algorithms can be directly applied to the opposing problem .
another example is the wsat oip  system of walser  which is an extension of wsat  selman et al.  1  and achieves impressive results on general ilp problems  but nevertheless requires soft constraints to be manually created to replace a given optimization objective.
모our main contribution is the following: although a variety of research communities in ai have investigated blp problems and developed effective methods for certain cases  the current level of specialization might not be yielding the benefits that one might presume. to support this observation we introduce a generic local search algorithm that when applied to specialized forms of blp  specifically sat and ca  achieves performance that competes with the state of the art on these problems. our algorithm is based on combining a simple idea from or  subgradient optimization for lagrangian relaxation  with a recent idea from ai  specifically from machine learning theory  multiplicative updates and exponentiated gradients . the conclusion we draw is that research on general purpose methods might still prove fruitful  and that known ideas in or might still yield additional benefits in ai problems beyond those already acknowledged.
1	background
research on constrained optimization in ai has become increasingly well informed by extensive foundational work in or  gomes  1 . or has investigated the specific task of ilp in greater depth than ai  and has tended to place more emphasis on developing general purpose methods applicable across the entire range of ilp problems. complete methods in or typically employ techniques such as branch and bound  using linear programming or lagrangian relaxation   cutting planes  and branch and cut to prune away large portions of the assignment space in a systematic search  martin  1 . this research has yielded sophisticated and highly optimized ilp solvers  such as the commercial cplex system  which although general purpose  performs very well on the specialized problems investigated in ai. in fact  it is still often unclear whether specialized ai algorithms perform better  andersson et al.  1 .
모perhaps less well known to ai researchers is that beyond systematic search with branch and bound  the or literature also contains a significant body of work on incomplete  heuristic  local search methods for approximately solving ilp problems; see for example  magazine and oguz  1; larsson et al.  1 . the simplest and most widespread of these ideas is to use subgradient optimization  which we describe below .
모our algorithm arises from the observation that the most recent strategies developed for the sat problem have begun to use an analog of subgradient optimizationas their core search strategy; in particular the dlm system of  wu and wah  1  and the sdf system of  schuurmans and southey  1   see also  thornton and sattar  1; frank  1; davenport et al.  1  . interestingly  these are among the most effective methods for finding satisfying assignments for cnf formulae  and yet they appear tobe recapitulatinga forty year old idea in or going back to  everett  1 . below we show that  indeed  a straightforwardsubgradient optimization approach  withjust three basic improvements from ai  yields a state of the art sat search strategy that competes withdlm  arguably the fastest sat search procedure currently known .
모interestingly  the method we develop is not specialized to sat problems in any way. in fact  the algorithm is a general purpose blp search technique that can in principle be applied to any problem in this class. to demonstrate this point we apply the method without modification  beyond parameter setting  to ca problems and find that the method still performs well relative to the state of the art  although somewhat less impressively than on sat problems . in this case we also find that the commercial cplex system performs very well and is perhaps the best of the methods that we investigated. our results give us renewed interest in investigating general purpose algorithms for blp that combine well understood methods from or with recent ideas from ai.
1	the exponentiated subgradient algorithm
to explain our approach we first need to recap some basic concepts from constrained optimization theory. consider the canonical blp  1 . for any constrainedoptimizationproblem of this form the lagrangian is defined to be
 1 
where is the th row vector of the constraint matrix and is the real valued lagrange multiplier associated with constraint . one can think of the multipliers simply as weights on the constraint violations .
thus the lagrangian can be thought of as a penalized objective function where for a given vector of constraint violation weights one could imagine minimizing the penalized objective . in this way  the lagrangian turns a constrained optimization problem into an unconstrained optimization problem. in fact  the solutions of these unconstrained optimizations are used to define the dual function
 1 
the dual problem for  1  is then defined to be
	subject to	 1 
let denote the maximum value of  1  and let denote the minimum value of  1 . note that these are all just definitions. the reason that  1  can be considered to be a dual to
 1  is given by the weak duality theorem  which asserts that  bertsekas  1; martin  1   see appendix a .
therefore  solving for any lagrange multiplier vector gives a lower bound on the optimum value of the original problem  1 . the best achievable lower boundis achieved by solvingthe dual problem of maximizing subject to   yielding the optimal value . the difference is called the duality gap. a fundamental theorem asserts that there is no duality gap for linear  or convex  programs with real valued variables  bertsekas  1   so in principle these problems can be solved by dual methods alone. however  this is no longer the case once the variables are constrained to be integer or valued.
모althoughthe dual problem cannot be used to directly solve  1  because of the existence of a possible duality gap  obtaining lower bounds on the optimal achievable value can still

figure 1: subgradients generalize gradients to nondifferentiable functions. note that the subgradient is not uniquely defined at a nondifferentiable point.
prove very useful in various search strategies  ranging from branch and bound to local search. obviously there is a natural desire to maximize by searching through lagrange multiplier vectors for larger values. a natural way to proceed would be to start with a vector   solve the unconstrained minimization problem  1   and then update to obtain a better weight vector   and so on. the issue is knowing how to update the weight vector . this question is complicated by the fact that is typically nondifferentiable  which leads to the last technical development we will consider: subgradient optimization.
모since is always known to be concave  bertsekas  1   a subgradient of  at a point   is given by any direction vector such that	for all .  intuitively  a subgradient vector gives the increasing direction of a plane that sits above but touches at   and hence can serve as a plausible search direction if one wishes to increase ; see figure 1.  therefore  to update   all we need to do is find a subgradient direction. here we can exploit an extremely useful fact: after minimizing to obtain	the resulting vector of residual constraint violation values is always a subgradient of at  bertsekas  1; martin  1   see appendix b . so  in the end  despite the overbearing terminology  subgradient optimization is a fundamentally simple procedure:
subgradient optimization: to improve the lower bound on the optimal solution of the original problem  take a given vector of lagrange multipliers   solve for a primal vector that minimizes the lagrangian
모모모  and update by adding a proportionalamount of the residual constraint violations to  maintaining  ; i.e. use the rule . note that this has the intuitive effect of increasing the weights on the violated constraints while decreasing weights on satisfied constraints. although this update is not guaranteed to increase the value of at every iteration  it is guaranteed to move closer to for sufficiently small step-sizes
 bertsekas  1 .
모these ideas go back at least to  everett  1   and have been applied with great success by held and karp  and many since. typically  subgradient optimization has been
used as a technique for generating good lower bounds for branch and bound methods  where it is known as lagrangian relaxation  fisher  1  . however  subgradient optimization can also be used as a heuristic search method for approximatelysolving 1  ina very straightforwardway: at each iteration simply check if is feasible  i.e. satisfies    and  if so  report as a feasible objective value  keeping it if it is the best value reported so far ; see for example  magazine and oguz  1; larsson et al.  1 .
모interestingly  in the last few years this procedure has been inadvertentlyrediscovered inthe ailiterature. specifically  in the field of incomplete local search methods for sat  clause weighting schemes turn out to be using a form of subgradient optimization as their main control loop. these procedures work by fixing a profile of clause weights  lagrange multipliers   greedily searching through variable assignment space for an assignment that minimizes a weighted score of clause violations  the lagrangian   and then updating the clause weights by increasing them on unsatisfied clauses-all of which comprises a single subgradient optimization step. however  there are subtle differences between recent sat procedures and the basic subgradient optimization approach outlinedabove. the dlm system of  wu and wah  1  explicitlyuses a lagrangian  but the multiplierupdates follow a complex system of ad hoc calculations.1 the sdf system of  schuurmans and southey  1  is simpler  albeit slower   but includes several details of uncertain significance. nevertheless  the simplicity of this latter approach offers useful hypotheses for our current investigation.
모given the clear connection between recent sat procedures and the traditional subgradient optimizationtechnique in or  we conduct a study of the deviations from the basic or method so that we can identify the deviations which are truly beneficial. our intent is to validate  or refute  the significance of some of the most recent ideas in the ai sat literature.
모linear versus nonlinear penalties: one difference between recent sat methods and subgradient optimization is that the sat methods only penalize constraints with positive violations  by increasing the weight on these constraints   whereas standard subgradient optimization also rewards satisfied constraints by reducing their weight proportionallyto the degree of negative violation . to express this difference  consider an augmented lagrangian which extends  1  by introducing a penalty function on constraint violations
the penalty functions we consider are the traditional linear penalty and the  hinge  penalty
	if 	if
 note that is an integer.  dlm and sdf can be interpreted as implicitly using a hinge penalty for dual updates on sat problems.  however  sdf uses a different penalty for its primal search.  intuitively  the hinge penalty has advantages because it does not favor increasing the satisfaction level of satisfied constraints above reducing the violations of unsatisfied constraints. below we observe that the traditional linear penalty leads to poor performance on constraint satisfaction tasks and the ai methods have an advantage in this respect.
모unfortunately  the consequence of choosing a nonlinear penalty functionis that findinga variable assignment which minimizes is no longer tractable. to cope with this difficultyai sat solvers replace the global optimizationprocess with a greedy local search  augmented with randomization . therefore  they only follow a local subgradient direction in at each update. however  despite the locally optimal nature of the dual updates  the hinge penalty appears to retain an advantage over the linear penalty.
모multiplicativeversus additiveupdates: the sdf procedure updates multiplicatively rather than additively  in an analogy to the work on multiplicative updates in machine learning theory  kivinen and warmuth  1 . a multiplicative update is naturally interpreted as following an exponentiated version of the subgradient; that is  instead of using the traditional additive update one uses  
       given the vector of penalized violation values . below we compare bothtypes ofupdate and find that the multiplicative approach typically works better.
모weight smoothing:  schuurmans and southey  1  also introduce the idea of weight smoothing: after each update the constraint weights are pulled back toward the population average  using the rule

	 	. this has the effect
of increasing the weights of frequently satisfied constraints without requiring them to be explicitly violated  which led to a noticeable improvement in sdf's performance .
exponentiated subgradient optimization  esg : the final procedure we propose follows a standard subgradient optimization search with three main augmentations:  1  we use the augmented lagrangian with a hinge penalty rather than a linear penalty   1  we use multiplicative rather than additive updates  and  1  we use weight smoothing to uniformize weights without requiring constraints to be explicitly violated. the parameters of the final procedure are     and a noise parameter  see figure 1 .1 below we compare four variants of the basic method: esg   esg  multiplicative updates with hinge and linear penalties respectively   and asg   asg  additive updates with each penalty .1
모the final esg procedure differs from the sdf system of  schuurmans and southey  1  in many respects. the most important difference is that esg can be applied to arbitrary blp tasks  whereas sdf is applicable only to sat problems. however  even if a generalized form of sdf could be devised  there would still remain minor differences: first  esg uses a constant multiplier for dual updates  whereas sdf uses an adaptive multiplier that obtains a minimum difference in for some primal search direction. second  sdf uses a different penalty in its primal and dual phases  exponential esg	procedure
initialize	and	random
while solution not found and restart not reached primal search: use greedy local search from	to find that locally minimizes	 iff stuck  with probability	take a random move and continue ;	.

	dual step:	;	.
	update:	;
figure 1: esg procedure
versus hinge  whereas esg uses the same hinge penalty in both phases. third  esg smooths all of the weights  whereas sdf only smooths weights on satisfied constraints. finally  esg includes a small probabilityof steppingout of local minima during its primal search  whereas sdf employs a deterministic primal search. overall  these simplifications allow for a more streamlined implementation for esg that yields a significant reduction in cpu overhead per step  while simultaneously allowing application to more general tasks.
1	sat experiments
we consider applying the blp techniques to sat problems. an instance of sat is specified by a set of clauses
모모모  where each clause is a disjunction of literals  and each literal denotes whether an assignment of  true  or  false  is required of a variable . the goal is to find an assignment of truth values to variables such that every clause is satisfied on at least one literal. in our framework  an instance of sat can be equivalently represented by the blp
	subject to	 1 
where	and	are vectors of zeros and twos respectively  and
if	appears in a negative literal in if	appears in a positive literal in
otherwise
an assignment of to variable denotes  true   and an assignment of denotes  false . the idea behind this representation is that a clause is encoded by a row vector in which has nonzero entries corresponding to the variables occurring in   along with their signs. a constraint is violatedonly when the assignment to agrees with the coefficients in on every nonzero entry  yielding a row sum of . if a single sign is flipped then the row sum drops by 1 and the constraint becomes satisfied.
모note that the zero vector means that the minimizationcomponentof thisproblemis trivialized. nevertheless it remains a hard constraint satisfactionproblem. the trivializedobjective causes no undue difficulty to the methods introduced above  and therefore the blp formulation allows us to accommodate both constraint satisfaction and constrained optimization problems in a common framework  albeit in a more restricted way than  hooker et al.  1  .
모for this problem we compared the subgradient optimization techniques esg/asg against state of the art local search methods: dlm  sdf  novelty+  novelty  wsat  gsat and hsat. however  for reasons of space we report results only
avg.est. opt.	failopt.steps	steps	%secuf1 1 problems cplex  1 probs 1na1.1asg1na1naesg111.1asg11.1.1esg11.1uf1 1 problems cplex  1 probs 1	na	1.1asg1	1	11esg1	1.1uf1 1 problems cplex  1 probs 1	na	1.1asg1	1	1.1esg1	1	1.1table 1: comparison of general blp methods on sat
for dlm  sdf  and novelty+  which represent the best of the last two generations of local search methods for sat.1 we ran these systems on a collection of  satisfiable  benchmark problems from the satlib repository.1 here the sat solvers were runon theirstandard cnf inputrepresentations whereas the blp solvers were run on blp transformed versions of the sat problems  as given above.  although alternative encodings of sat problems could affect the performance of blp solvers  beacham et al.  1  we find that the simple transformation described above yields reasonable results. 
모to measure the performance of these systems we recorded wall clock time on a piii 1mhz processor  average number of primal search steps   flips   needed to reach a solution  and the proportion of problems not solved within 1k steps  1m on bw large.c . to avoid the need to tune every method for a restart threshold we employed the strategy of  schuurmans and southey  1  and ran each method 1 times on every problem torecord the distributionof solutiontimes  and used this to estimate the minimum expected number of search steps required under an optimal restart scheme for the distribution  parkes and walser  1 . the remaining parameters of each method were manually tuned for every problem set. the parameters considered were: novelty+ walk  noise  
dlm 1 tunable parameters   sdf   esg and asg	.1

1
   implementations of these methods are available at ai.uwaterloo.ca/ dale  www.satlib.org  and manip.crhc.uiuc.edu. 1
   satlib is at www.satlib.org. we used three classes of problems in our experiments. the uf problems are randomly generated 1cnf formulae that are generated at the phasetransition ratio of 1 clauses to variables.  these formulae have roughly a 1% chanceof being satisfiable  but uf contains only verified satisfiable instances.  the flat problems are sat encoded instances of hard random graph coloring problems. the remaining problems are derived from ai planning problems and the  all interval series  problem. 1
   for the large random collections  tables 1 and 1: flat1- uf1  the methods were tuned by running on 1 arbitrary problems. performance was then measured on the complete collection. novelty+ parameters were tuned from the publishedvalues of  hoos and stu몮tzle  1 . dlm was tuned by running each of the 1 default parameter sets available at manip.crhc.uiuc.edu and choosing the best.

avg.est. opt.failopt.stepssteps%secais1 1 problem novelty+111.1sdf111.1dlm pars1 1 1.1esg111.1ais1 1 problem novelty+111.1sdf111.1dlm pars1 1 1.1esg111.1ais1 1 problem novelty+111.1sdf111.1dlm pars1 1 11esg111.1bw large.a 1 problem novelty+111.1sdf111.1dlm pars1 1 1.1esg111.1bw large.b 1 problem novelty+111.1sdf111.1dlm pars1 1 1.1esg111.1bw large.c 1 problem novelty+1 1 11sdf1 1 11dlm pars1 11 11esg1 1 11logistics.c 1 problem novelty+111.1sdf111.1dlm pars1 1 1.1esg111.1
flat1 1 problems novelty+	1	1.1.1sdf	1	11.1dlm pars1	1	11.1esg	1	11.1flat1 1 problems novelty+	1	11.1sdf	1	11.1dlm pars1	1	11.1esg	1	11.1uf1 1 problems novelty+	1	11.1sdf	1	11.1dlm pars1	1	11.1esg	1	11.1uf1 1 problems novelty+	1	11.1sdf	1	1.1.1dlm pars1	1	1.1.1esg	1	1.1.1uf1 1 problems novelty+11	1.1sdf11	.1.1dlm pars1 1 1	.1.1esg11	.1.1table 1: comparison of esg	to specialized sat methods

모table 1 compares the different blp procedures on random problems from satlib. the most salient feature of these results is that the linear penalty performs poorly  as anticipated . here  the traditional asg method was unable to solve any problems in the allotted steps. table 1 also shows that multiplicative updates  esg  were generally superior to additive updates  asg  regardless of the penalty function used  at least on random sat problems . although details are omitted  we have also found that some smoothing     usually improves the performance of esg but is generally less beneficial for asg. cplex generally performs poorly in these experiments.
모table 1 shows the results of a larger scale comparison between esg and state of the art local search methods forsat. these resultsshow that esg tends tofinds solutionsinfewer primal search steps  flips  than other procedures. however  esg 's time per flip is about 1 to 1 times higher than dlm and about 1 to 1 times higher than novelty+  which means that its overall run time is only comparable to these methods. nevertheless  esg   dlm and sdf obtain a large reduction in search steps over novelty+ on the structured ais and planning problems  with the notable exception of bw large.c . as a result  dlm and esg both tend to demonstrate better run times than novelty+ on these problems. all three methods  novelty+  dlm and esg   demonstrate similar run times on the random uf and flat problems.
모note that none of these sat procedures  other than esg/asg and cplex  can be applied to general blp problems without modification.
1	ca experiments
the second problem we considered is optimal winner determination in combinatorial auctions  ca . this problem introduces a nontrivial optimization objective that is not present in sat. however  the subgradient optimization approach remains applicable  and we can apply the same esg/asg methods to this task without modifying them in any way.  nevertheless  we did conduct some implementation specializations to gain improvements in cpu times on sat problems.  the ca problem has been much less studied in the ai literature  but interest in the problem is growing rapidly.
모an instance of the optimal winner determination problem in combinatorial auctions  ca  is given by a set of items
       with available quantities   and a set of bids which offer amounts for a specified subset of items. we can represent the bid requests in a constraint matrix where
if bid	requests item otherwise
the problem then is to find a set of bids that maximizes the total revenue subject to the constraint that none of the item quantities are exceeded. if this problem can be expressed as the blp
	subject to	 1 
however it is not in our canonical form. to transform it to the canonical form we use the substitutions
모모모  and . the minimum solution to the transformed version of this problem can then be converted back to a maximum solution of the original ca.
모for this task we compared the esg/asg algorithms to cplex and casanova  hoos and boutilier  1   a local search method loosely based on novelty+.1 the problems we tested on were generated by the cats suite of ca problem generators  leyton-brown et al.  1   which are intended to model realistic problems. however  our results indicate that these tend to be systematically easy problems for all the methods we tested  so we also tested on earlier artificial problem generators from the literature  sandholm  1; fujishimaet al.  1 . some of these earlier generators were shownto be vulnerableto trivial algorithms andersson et al.  1  but some still appear to generate hard problems. however  to push the envelope of difficulty further  we also encoded several hard sat problems as combinatorial auctions by using an efficient polynomial  quadratic  reduction from sat to ca. unfortunately  space limitations preclude a detailed description of this reduction  but our results show that these converted sat problems are by far the most difficult available at a given problem size.
모to measure the performance of the various methods  we first solved all of the problems using cplex and then ran the local search methods until they found an optimal solution or timed out. although we acknowledge that this method of reportingignores the anytime performance of the various methods  it seems sufficient for our purposes. to give some indication of anytime behavior  we recorded the fraction of optimality achieved by the local search methods in cases where they failed to solve the problem within the allotted time.
모table 1 shows a comparison of the different subgradient optimization procedures on one of the cats problem classes. these results show that the linear penalty is once again weaker than the hinge  but to a lesser extent than on sat problems . interestingly  additive updates appear to work as well as multiplicative updates on these problems  although the performance of asg begins to weaken on harder constraint systems such as those encountered in table 1 .
모table 1 shows the results of a larger comparison between esg/asg  casanova and cplex on the cats problems and an artificial problem. these results show that there is very little reason not to use cplex to solve these problems in practice.1 cplex  of course  also proves that its answers are optimal  unlike local search. nevertheless  the results show that esg and asg are roughlycompetitive withcasanova  which is a specialized local search technique for this task.
note that the large score proportions obtained by all methods when they fail to find the optimal solution follows from the fact that even simple hill-climbing strategies tend to find near optimal solutions in a single pass  holte  1 . it appears that most of the difficulty in solving these problems is in gaining the last percentage point of optimality.
모finally  table 1 shows that casanova demonstrates inferior performance on the more difficult sat ca encoded problems. nevertheless all methods appear to be challenged by these problems. interestingly  cplex slows down by a factor of 1 when transforming from the direct sat encoding of section 1 to the sat ca encoding used here. by comparison the esg method slows down by a factor of 1k.
1	conclusion
although we do not claim that the methods we have introduced exhibit the absolute best performance on sat or ca problems  they nevertheless compete very well with the state of the art techniques on both types of problem. therefore  we feel that the simple lagrangian approach introduced here might offer a useful starting point for future investigation beyond the myriad walksat variants currently being pursued in ai. among several directions for future work are to investigate other general search ideas from the or literature  such as tabu search  glover  1   and compare to other local search methods for ilp problems  resende and feo  1; voudouris and tsang  1   which do not appear to be competitive on sat problems  but still offer interesting perspectives on ilp problems in general .
a	weak duality
it is insightful to see why for all . in fact  it is almost immediately obvious: note that for any we have
for all
	for all	such that	 since	 
and hence . from this one can see that the constraint is imposed precisely to ensure a lower bound on .
b	subgradient direction
it is also easy to see that the vector of constraint violation values
모모모모모모모 where	  must yield a subgradient of	at	:
 since
acknowledgments
research supported by nserc and cito. thanks to holger hoos and craig boutilierfor helpful comments and providing access to casanova. thanks also to peter van beek and the anonymous referees for many helpful suggestions. the assistance of istva뫣n hern뫣advo몮lgyi is also warmly appreciated.
avg.avg.	fail%secsteps	%opt.cats-regions 1 problems esg11.1.1asg11.1.1esg111asg111table 1: comparison of subgradient optimization methods
avg.avg.	fail	%secsteps	%	opt.cats-regions 1 problems cplex11	1casanova11.1.1esg11.1.1asg11.1.1cats-arbitrary 1 problems cplex1	1casanova1	1	1esg1	1	1asg1	1	1cats-matching 1 problems cplex1	11casanova.1	11esg.1	11asg.1	1cats-paths 1 problems cplex1	11casanova1	11esg1	1	1asg1	1	1cats-scheduling 1 problems cplex1	1casanova1	1	1esg1	1.1asg1	1	1decay-1-.1 1 problems cplex11	1casanova11	1esg11.1.1asg11.1.1table 1: results on cats and synthetic problems
avg.	avg.	fail%sec	steps	%opt.sat uf1 ca 1 problems cplex1	11casanova1.11esg1.11asg1.11sat uf1 ca 1 problems cplex11casanova1.11esg1.11asg1.11table 1: results on hard sat	ca encoded problems
references
 andersson et al.  1  a. andersson  m. tenhunen  and f. ygge. integer programming for combinatorial auction winner determination. in proceedings icmas-1  1.
 beacham et al.  1  a. beacham  x. chen  j. sillito  and p. van beek. constraintprogramminglessons learned from crossword puzzles. in proc. canadian ai conf.  1.
 bertsekas  1  d. bertsekas. nonlinear optimization. athena scientific  1.
 davenport et al.  1  a. davenport  e. tsang  c. wang  and k. zhu. genet: a connectionist architecture for solving constraint satisfaction problems. in proceedings aaai-1  pages 1  1.
 everett  1  h. everett. generalized lagrange multiplier method for solving problems of the optimal allocation of resources. operations res.  1-1  1.
 fisher  1  m. fisher. the lagrangian relaxation method for solving integer programming problems. management sci.  1-1  1.
 frank  1  j. frank. learning short-tem weights for gsat. in proceedings ijcai-1  pages 1  1.
 fujishima et al.  1  y. fujishima  k. leyton-brown  and y. shoham. taming the computational complexity of combinatorial auctions: optimal and approximate approaches. in proceedings ijcai-1  pages 1  1.
 glover  1  f. glover. tabu search part 1. orsa journal on computing  1 :1 1.
 gomes  1  c. gomes. structure  duality  and randomization: common themes in ai and or. in proceedings aaai-1  pages 1  1.
 held and karp  1  m. held and r. karp. the travelling salesman problem and minimum spanning trees. operations res.  1-1  1.
 holte  1  r. holte. combinatorial auctions  knapsack problems  and hill-climbingsearch. in proceedings canadian ai conference  1.
 hooker et al.  1  j. hooker  g. ottosson  e. thorsteinsson  and h.-k. kim. on integrating constraint propagation and linear programming for combinatorial optimization. proceedings aaai-1  pages 1  1.
 hoos and boutilier  1  h. hoos and c. boutilier. solving combinatorial auctions using stochastic local search. in proceedings aaai-1  pages 1  1.
 hoos and stu몮tzle  1  h. hoos and t. stu몮tzle. local search algorithms for sat: an empirical evaluation. j. automat. reas.  1-1  1.
 hoos  1  h. hoos. on the run-time behavior of stochastic local search algorithms for sat. in proceedings aaai1  pages 1  1.
 kautz and selman  1  h. kautz and b. selman. pushing the envelope: planning  propositionallogic  and stochastic search. in proceedings aaai-1  pages 1  1.  kautz and walser  1  h. kautz and j. walser. statespace planning by integer optimization. proceedings aaai-1  pages 1  1.
 kivinen and warmuth  1  j. kivinen and m. warmuth. exponentiated gradient versus gradient descent for linear predictors. infor. comput.  1-1  1.
 larsson et al.  1  t. larsson  m. patriksson  and a.-b. stromberg. conditionalsubgradient optimization-theory and applications. euro. j. oper. res.  1-1  1.
 lau et al.  1  h. lau  a. lim  and q. liu. solving a supply chain optimization problem collaboratively. proceedings aaai-1  pages 1  1.
 leyton-brown et al.  1  k. leyton-brown  m. pearson  and y. shoham. towards a universal test suite for combinatorial auction algorithms. in proc. ec-1  1.
 magazine and oguz  1  m. magazine and o. oguz. a heuristic algorithm for the multidimensional knapsack problem. euro. j. oper. res.  1-1  1.
 martin  1  r. martin. large scale linear and integer optimization. kluwer  1.
 mcallester et al.  1  d. mcallester  b. selman  and h. kautz. evidence for invariants in local search. in proceedings aaai-1  pages 1  1.
 parkes and walser  1  a. parkes and j. walser. tuning local search for satisfiabilitytesting. in proceedings aaai1  pages 1  1.
 resende and feo  1  m. resende and t. feo. a grasp for satisfiability. in cliques  coloring  and satisfiability  dimacs series v.1  pages 1. ams  1.
 sandholm  1  t. sandholm. an algorithm for optimal winner determination in combinatorial auctions. in proceedings ijcai-1  pages 1  1.
 schuurmans and southey  1  d. schuurmans and f. southey. local search characteristics of incomplete sat procedures. in proc. aaai-1  pages 1  1.
 selman et al.  1  b. selman  h. kautz  and b. cohen. noise strategies for improving local search. in proceedings aaai-1  pages 1  1.
 thornton and sattar  1  j. thornton and a. sattar. on the behavior and application of constraint weighting. in proceedings cp-1  pages 1  1.
 vossen et al.  1  t. vossen  m. ball  a. lotem  and d. nau. on the use of integer programming models in ai planning. proceedings ijcai-1  pages 1  1.
 voudouris and tsang  1  c. voudouris and e. tsang. partial constraint satisfaction problems and guided local search. in proceedings pact-1  pages 1  1.
 walser  1  j. walser. integer optimization by local search. springer-verlag  1.
 wu and wah  1  z. wu and w. wah. an efficient globalsearch strategy in discrete lagrangian methods for solving hard satisfiability problems. in proceedings aaai-1  pages 1  1.
a new method for the three dimensional container packing problem
andrew lim and wang ying
department of computer science  national university of singapore
science drive 1  singapore 1abstract
a new algorithm for solving the three dimensional container packing problem is proposed in this paper. this new algorithm deviates from the traditional approach of wall building and layering. it uses the concept of  building growing  from multiple sides of the container. we tested our method using all 1 test cases from the or-library. experimental results indicate that the new algorithm is able to achieve an average packing utilization of more than 1%. this is better than the results reported in the literature.
1	introduction
packing boxes into containers is a common problem faced by the logistics and transportation industries. the three dimensional  1d  packing problem is a difficult extension of the two dimensional  1d  stock cutting problem and the one dimensional  1d  bin packing problem. even the 1d bin packing problem is known to be np-complete. for a survey of packing problems  please refer to  k.a. and w.b.  1 . while pseudo-polynomial time algorithms have been derived for the 1d packing problem  no such results have been derived for the 1d packing problem.
모the 1d packing problem can be divided into two main variants. the first variant is: given a list of boxes of various sizes  pack a subset of these boxes into one container to maximize the volume utilization of the container. the second variant is: given a list of boxes of various sizes  pack all the boxes into as few containers as possible. in addition  constraints  such as orientation restriction  weight distribution  locality restriction  may be imposed on the boxes.
모in this paper  we study the packing problem that maximizes the volume utility of a single container with orientation restrictions on the boxes.
모many different heuristics have been developed to  optimize  volume utilization in a single container.  j.a and b.f.  1  proposed a primitive  wall-building  method.  gehring h. and m.  1  proposed an improved  wallbuilding  heuristic.  t.h. and nee  1  proposed a layering approach.  han c.p. and j.p.  1  provided an algorithm for packing identical boxes  which is a combination of  wallbuilding  and layering.  b.k. and k.  1  proposed a data structure for representing the boxes and free spaces  which is used in their heuristic.  e.e. and m.d.  1  presented a comparative evaluation of different heuristic rules for freight container loading with the objective of minimizing the container length needed to accommodate a given cargo.  w.b.  1  examined a number of heuristic rules for the problem. the emphasis on both wall building and layering is to create walls or layers of boxes as flat as possible. at all times  the methods attempt to retain a flat forward packing face. the existing methods most mirror the hand-packing process  which is a natural way to solve the problem. however  they put many restrictions on the location of the boxes which may not be feasible to the actual application. another problem of the existing approaches is that they may leave a thin layer of nonuseful empty spaces near the entrance or the ceiling of the container  which results in space underutilization. also  the papers in the existing literature used data that are not easily accessible  hence  it is relatively hard to compare the various methods quantitatively.
모in this paper  we propose a new method. boxes are packed into container in  building-growing  fashion instead of  wall building  manner. furthermore  each wall of the container can be treated as base to locate the boxes  which provides more flexibility. we tested our program on all the 1 test cases in  or-lib   . our experimental results indicate that we are able to achieve an average packing utilization more than 1%.
1	problem specification
we are given a set of rectangular boxes and a rectangular container . let   and represent the three dimensions of the box   and     and represent the three dimensions of the container.
모the objective of the problem is to select a subset and assign a position to each box    in the 1d space of container such that is maximized subjected to the constraints that all boxes must be totally contained in   no two boxes intersect in 1d space and every edge of the packed box must be parallel or orthogonal to the container walls.
1	multi-directional building-growing algo
the idea of this 1d packing algorithm  wang  1  comes from the process of constructing a building. when erecting a building  we construct the basement on the ground first  then the building will  rise up  floor by floor. here  the box will play the role of the floor  and the wall of the container will act as the ground. boxes are placed on the wall of the container first as it builds the basement on the ground. after which  other boxes will be placed on top of the basement boxes. following this process  boxes will be placed one on top of another  just like the construction of a new building. every wall of the container can be treated as the ground for boxes to stack on. however  the users reserve the right to choose the walls to act as the ground. note that  this algorithm does not take into consideration whether packed boxes form a flat layer or not  which is the main idea of the  wall building  approach. figure 1 illustrates the main idea of this algorithm.

figure 1: multi-directional building-growing approach
1	formulation of the algorithm
overall packing process
after getting the option from the user for the acting-asground walls  this algorithm reads in the container and box information  including length  width  height  orientation and quality information  from the data file. the algorithm then orders the boxes according to certain criteria. subsequently  actual packing will be performed based on the best matching between base area of boxes and empty spaces. the following pseudo code presents the overall algorithm.
main
1. get wall option  
1. read in the container information  length width height 
1. for every box of type b 
1.read in b's information length width height orientation quantity 
1.bn=create box nodes box b 
1.insert box node box node bn 
1. pack  
getwall option  
users are free to choose which of the walls of a container to be used as ground/base. basically  all the empty spaces created from one ground wall  form an empty space list. there are at most six empty space lists as shown in figure 1.

figure 1: empty space list and empty space object data
structure
모an empty space in each empty space list has its own origin and coordinate system as shown in figure 1. here the lower coordinate system is the  normal  coordinate system.
upper right and far empty spaces	z	y treat this point as the origin of the
	corresponding coordinate system                    	lowerupper
x
z
leftrightx
z
lower left and near emptyy spaces treat this point as
	the origin of the correspondingcoordinate system	nearfar
	y	x
figure 1: empty space coordinate system representation
모the initial value of each empty space list head is the whole container. if the user does not choose wall as the ground  then empty space list 's head should be pointing to null. note that each empty space list only takes care of all the empty spaces using wall as base  without knowing the existence of the other empty space lists. because of the exact and accurate representation  empty spaces may overlap with each other.
create box nodes box b 
every box has three attributes namely  length  width and height. length is measured along the x-axis  so is width along the y-axis and height along the z-axis. and the area surrounded by length and width is called the base area of a box. there is a unique id assigned to each box  yet  there are six orientations associated with it  see figure 1. note that a number is used to denote each edge of the box. .

figure 1: six orientations associated with one box
모for each type of box  same type of box means boxes with same length  width and height   at most six box nodes will
box1box1box1box1box1box1id111length111width111height111table 1: id and attributes associated with box in figure 1
be created for them but with the same id. each box node represents a possible orientation for one type of box. if a certain orientation is not allowed  this algorithm will simply omit the box node of that orientation.
insert boxnode box node bn 
whenever a box information is obtained from the data file  a new box node will be created and inserted to the box node list
based on the function below. given a newly created box node
     this algorithm goes through the box node list  and for each existing box node   calculates the priority number for and respectively as follows:

where parameters vol per and dim per denote the volume priority percentage and the biggest dimension priority percentage respectively; routines and return the
volume and the biggest dimension of box	inside box node bn respectively. after getting	and	  the algorithm compares these two numbers:
if	  insert to the front of .
otherwise  continue to compare with the priority number of the box node behind .
according to the above  a box node nearer to the head possesses either larger volume or one bigger dimension  hence  has higher priority to be packed into the container first. note that boxes with the same id but with different orientations are located adjacent to each other  and they have the same priority to be packed into the container  because they are actually the same box.
pack  
in every attempt to pack one box into the container  the algorithm finds a most suitable empty space for each of the first box nodes with the sane priority number in terms of the percentage of the base area of that box occupies the base area of the empty space. inside all these pairs of box node and empty space  the algorithm finds one pair     with the greatest base area occupation percentage   remove   if . pack   if  .
	decide to pack or remove	carefully  if

the following pseudo code illustrates the packing process of the algorithm.
while box node head!=null 
for box node	with same priority as box node head 
for every empty space oe in the 1 empty space lists  percentage   =
find one pair  bn oe  with greatest p=percentage bn  if 	 
//bn cannot be packed into any empty space remove
	//because	is too big to be packed into any empty space now
//and empty space will become smaller and smaller  there will //be no chance for bn to be packed into any empty space later
if 		 
1.create a packed box according to oe's coordinate system  then convert it to the normal coordinate system and insert it into packed box list
1.delete one box id from box node
1.update emptyspace empty space oe packed box pb 
if 1	packing per 
	//in this case  we cannot find a good fit between	and oe 
//so we need to decide whether we can drop the first	box //nodes with the same priority number if after removing the first	box nodes and
	moving per 
	remove the first	box nodes
//because without these few difficult-to-be-packed box nodes  //we may still get good utilization
else
1.create a packed box according to oe's coordinate system then convert it to the normal cooridnate system and insert it into packed box list
1.delete one box id from box node
1.update empty space empty space oe packed box pb 
updateempty space empty space oe packed box pb  after a box is packed into the container  the original set of empty spaces will be modified by the newly packed box . moreover  will not only affect the empty space it resides in  but also the empty spaces nearby   because  in order to represent the empty spaces effectively  the empty spaces overlap. the responsibility of this routine is to update the empty spaces after one box is placed into the container. this process can be performed using the following two steps:
step 1:	change	to the coordinate system which
lies in and absorb the new empty spaces out of	 figure 1 .
updating	:
1. remove	from its empty space list
1. if   are not inside the empty space list resides in 

figure 1: update empty space oe which provides empty
space for packed box pb
	insert it them  into the empty space list	resides in
step 1: for every empty space	in the six empty space lists 
1. convert	to the coordinate system which	is in
1. test whether there is overlap between and   and update accordingly
all the possible overlap cases between and will be disscussed below and the update method will be given by the end. case 1: pb intersects one corner of e.
e.g. pb intersects the lower left near corner of e. case 1: pb intersects one entire edge of e.
e.g. pb intersects the left edge of upper plane of e. case 1: pb intersects the inner part of one edge of e. e.g. pb intersects part of lower left edge of e.
case 1: pb gets into e from one plane of it without touching any corner point of e.
e.g. pb gets into e from the left plane of e. case 1: pb cuts away one entire plane of e.
e.g. pb cuts away the whole left plane of e.
case 1: pb crosses one pair of parallel edges of e without touching any corner point of e
e.g. pb crosses the upper horizontal pair of edges of e.
case 1: pb passes through one pair of parallel planes of e without touching any corner point of e
e.g. pb passes through the near-far planes of e. case 1: pb cuts e into two new empty space.
e.g. pb cuts e into two new empty space from vertical direction. case 1: the whole pb resides inside e. case 1: pb covers the whole e.
updating	in each case above:
1. remove	from its empty space list
1. if  newly created empty spaces are not inside the empty space list resides in  insert it them  into the empty space list resides in
1	experimental results
this algorithm was successfully implemented using the java programming language. all the experiments were run on an alpha 1mhz machine running the digital unix operating system. the or-library  or-lib    has a collection of test data for a variety of or problems. there are nine sets of test data with 1 test cases for the 1d packing problem. all the test data in  or-lib    have been tested.
모the algorithm was tuned using various parameter settings. the packing result for test file 1 is omitted in all result presentations  because of the fact that we can pack every box into the container in most cases and the size of the container is much larger than the total size of the boxes to be packed into the container.
모the best packing result in terms of container volume utilization is shown in figure 1. the parameter and wall combination used for the best packing result is shown in table 1.
parameterwall combinationpacking vol=1left wallmoving per=1right wallvol per=1far walldim per=1lower walltable 1: best parameter setting and wall combination
in the experimental results shown in figure 1  the x-axis represents the test case index and the y-axis represents the container volume utilization.

figure 1: best experimental results for test file 1 1 using multi-directional building-growing approach
모using this parameter setting and wall combination  the average container volume utilization for all the available test cases in the or-library  except dest file 1  can reach as high as 1. this utilization is comparable to the published result of the existing literature. moreover  the special advantage of this algorithm is its speed as it needs about two seconds to pack one container.
모on the other hand  the container volume utilization can still be improved for the price of efficiency. in the next section  an improvement phase is introduced.
1	incorporating a look-ahead mechanism
in the packing process of multi-directional buildinggrowing  mdbg  approach  the algorithm only chooses one pair of best fit box and empty space in terms of base area occupation percentage among the most difficult-to-be-packed boxes. mdbg algorithm only takes care of the local information  and the decision made by this algorithm doesn't contain any knowledge for the future. the look-ahead mechanism not only considers the current pairing  but also looks at its potential effect.
1	look-ahead improvement strategy
the strategy will choose pairs of boxes and empty spaces with the greatest base area occupation percentage. each one of these pairs of boxes and empty spaces will be assumed to be the chosen pair for this attempt and be temporarily packed into the container  and the packing process is continued using the mdbg approach from this attempt onwards until all the boxes have been packed into the container or no more boxes can be packed into it. then based on the final container volume utilization  the improvement strategy will choose one pair of the box and empty space from the initial pairs  which gives best final container volume utilization. this chosen pair of box and empty space will then be used as the actual packing for this attempt. this pair of box and empty space satisfies both best fit currently and best potential effect in the future. figure 1 illustrates the look-ahead idea.

choose one pair from these k candidate 
pairs with greatest final container volume utilization to perform actual packing for the current attempt.
figure 1: illustration of the look-ahead improvement phase
the following is the pseudo-code for the look-ahead improvement strategy.
newpack  
while there's box in the box node list 
//attempt to pack one box into the container //1.get k boxes with greatest base area occupation percentage for every box b in the box node list 
for every empty space in the 1 empty space lists 
find a best fit empty space e for b with greatest base area occupation percentage p
	if 	 
//b cannot be packed into any empty space remove b
	else if top k base area occupation percentages	p	1 
replace e and b into the top k best fit pairs
//1. using mdbg approach pack the k candidate pairs for i=1..k 
	1.temporarily pack	to empty space	at this attempt
1.continue packing using pack   in section 1
1.record down the final container volume utilization at
//1.get one box with the greatest final utilization from the k
// candidates  and use this box in the actual packing of this
// attempt
find one box b from	and one empty space	from	with greatest final container volume utilization pack b into e permanently
1	experimental results
in the last section  using mdbg approach  we have experimented with all the test data using all possible wall combinations for many possible parameter settings. the best packing result comes from the wall combination: left wall  right wall  far wall  lower wall; and the parameter setting:	 
모. based on this experience  the lookahead mechanism is tested on all data files using this parameter setting and wall combination  and let go from 1 to 1. the results are summarized in the tables below:
k=1k=1k=1k=1k=1k=1file1.1.1.1.1.1.1file1.1.1.1.1.1.1file1.1.1.1.1.1.1file1.1.1.1.1.1.1file1.1.1.1.1.1.1file1.1.1.1.1.1.1file1.1.1.1.1.1.1file1.1.1.1.1.1.1average111111table 1: packing result with look-ahead improvement phase
모for the look-ahead method the best packing result comes from the wall combination: left wall  right wall  far wall  upper wall and the parameter setting:
			.
the average container volume utilization using this best parameter setting and wall combination can be increased to 1. the detailed packing result is listed in figure 1.
모note that in the experimental results shown in figure 1  the x-axis represents the test case index and the y-axis represents the container volume utilization  and the average packing utilization and packing time are indicated on the top of each figure.
1	comparison of results
to show the effectiveness of the proposed packing algorithm and its coupled improvement strategy  we compare our results run on the test problems from the  or-lib    with

figure 1: best experimental result for test file 1 1 using look-ahead improvement phase
results in  e.e and ratcliff  1  which is the only available work in the existing literature containing packing results on 1 test problems from the or-library.
scheme a scheme b scheme c mdbg algolamfile1.1%1%1%1%1%file1.1%1%1%1%1%file1.1%1%1%1%1%file1.1%1%1%1%1%file1.1%1%1%1%1%file1.1%1%1%1%1%file1.1%1%1%1%1%table 1: performance comparison between the existing algorithms and the proposed algorithms
모in table 1  the scheme a   scheme b   scheme c  are the respective scheme a  b and c in  e.e and ratcliff  1 . mdbg algo refers to the multi-directional buildinggrowing algorithm  and lam refers to look-ahead mechanism. from the experiment results  it is clear that the container volume utilization using the mdbg algorithm is slightly better than that of scheme a and b in  e.e and ratcliff  1 . but the mdbg algorithm coupled with the lam produces a much higher container volume utilization than scheme c in  e.e and ratcliff  1 . the experimental results provide a strong evidence that the multi-directional building-growing algorithm when coupled with the lookahead mechanism gives an significant improvement to the container volume utilization for 1d container packing problem.
1	conclusion
in this paper  we presented a new approach to solving the 1d container packing problem. our paper is the first paper to benchmark our results across all the or-benchmarks. our packing utilization of 1% compares very favorably against past works in this area.
references
 b.k. and k.  1  ngoi b.k. and whybrew k. a fast spatial representation method. journal of international advance manufacturing technology  1   1.
 e.e. and m.d.  1  bischoff e.e. and marriott m.d. a comparative evaluation of heuristics for container loading. european journal of operational research  1-1  1.
 e.e and ratcliff  1  bischoff e.e and msw ratcliff. issues in the development of approaches to container loading. omega. int. j. mgmt sci.  1 :1  1.
 gehring h. and m.  1  menschner k. gehring h. and meryer m. a compute-based heuristic for packing pooled shipment containers. european journal of operational research  1 :1  1.
 han c.p. and j.p.  1  knott k. han c.p. and egbelu j.p. a heuristic approach to the three-dimensional cargoloading problem. int. j. prod. res.  1 :1  1.
 j.a and b.f.  1  george j.a and robinson b.f. a heuristic for packing boxes into a container. computer and operational research  1-1  1.
 k.a. and w.b.  1  dowsland k.a. and dowsland w.b. packing problems. european journal of operational research  1-1  1.
 or-lib    or-lib. http://www.ms.ic.ac.uk/library/.
 t.h. and nee  1  loh. t.h. and a.y.c. nee. a packing algorithm for hexahedral boxes. in proceedings of the conference of industrial automation  singapore  pages 1  1.
 wang  1  ying wang. 1d container packing. honours thesis  national university of singapore  1.
 w.b.  1  dowsland w.b. three dimensional packingsolution approaches and heuristic development. international journal of operational research  1-1  1.

search  satisfiability 
and constraint
satisfaction problems
satisfiability

balance and filtering in structured satisfiable problems

henry kautz
yongshao ruan
dept. comp. sci. & engr.
univ. washingtondimitris achlioptas
microsoft research redmond  wa 1 optas microsoft.com   seattle  wa 1 kautz cs.washington.edu ruan cs.washington.edu
carla gomes
bart selman
dept. of comp. sci.
cornell univ.
ithaca  ny 1mark stickel
artificial intelligence center
sri international
menlo park  california 1 stickel ai.sri.comgomes cs.cornell.edu selman cs.cornell.edu

abstract
new methods to generate hard random problem instances have driven progress on algorithms for deduction and constraint satisfaction. recently achlioptas et al.  aaai 1  introduced a new generator based on latin squares that creates only satisfiable problems  and so can be used to accurately test incomplete  one sided  solvers. we investigate how this and other generators are biased away from the uniform distribution of satisfiable problems and show how they can be improved by imposing a balance condition. more generally  we show that the generator is one member of a family of related models that generate distributions ranging from ones that are everywhere tractable to ones that exhibit a sharp hardness threshold. we also discuss the critical role of the problem encoding in the performanceof bothsystematic and local search solvers.
1	introduction
the discovery of methods to generate hard random problem instances has driven progress on algorithms for propositional deduction and satisfiability testing. gomes & selman  1  introduced a generation model based on the quasigroup  or latin square  completion problem  qcp . the task is to determine if a partially colored square can be completed so that no color is repeated in any row or any column. qcp is an np-complete problem  and random instances exhibit a peak in problem hardness in the area of the phase transition in the percentage of satisfiable instances generated as the ratio of the number of uncolored cells to the total number of cells is varied. the structure implicit in a qcp problem is similar to that found in real-world domains: indeed  many problems in scheduling and experimental design take the form of a qcp. thus  qcp complements earlier simpler generation models  such as random -cnf  mitchell et al. 1 . like them qcp generates a mix of satisfiable and unsatisfiable instances.
모in order to measure the performance of incomplete solvers  it is necessary to have benchmark instances that are known to be satisfiable. this requirement is problematic in domains where incomplete methods can solve larger instances than complete methods: it is not possible to use a complete method to filter out the unsatisfiable instances. it has proven difficult to create generators for satisfiable -sat. achlioptas et al.  1  described a generation model for satisfiable quasigroup completion problems called  quasigroups with holes   qwh . the qwh generation procedure basically inverts the completion task: it begins with a randomly-generated completed latin square  and then erases colors or  pokes holes . the backbone of a satisfiable problem is the set of variables that receive the same value in all solutions to that problem. achlioptas et al.  1  showed that the hardest qwh problems arise in the vicinity of a threshold in the average size of the backbone.

figure 1: comparison of the qwh and filtered qcp models for order 1 using walksat. the x-axis is the percentage of holes  re-parameterized and the y-axis is the number of flips.
modelwalksatsatzsatoflipsbacktracksbranchesqwh1filtered qcp1balanced qwh1table 1: peak median hardness for different generation models for order 1 problems.
모despite the similarities between the filtered qcp and qwh models the problem distributions they generate are quite different. as noted by achlioptas et al.  aaai 1   the threshold for qcp is at a higher ratio of holes than is the threshold for qwh. but even more significantly  we discovered that the hardestproblemsobtainedusing the filtered qcp model are about an order of magnitude more difficult to solve than the hardest one obtained using the qwh model. figure 1 illustrates the difference for the incomplete local search solver walksat  selman et al. 1 .  the critical parameter for a fixed order is the percentage of holes  uncolored squares  in the problem. in all of the graphs in this paper the axis is re-parameterized as the number of holes divided by . achlioptas et al.  1  demonstrates that this re-parameterization adjusts for different problem sizes.  the first two lines of table 1 compare the peak hardness of qwh and filtered qcp for walksat and two complete systematic solvers  satz  li & anbulagan1  and sato  zhang 1 . these solvers are running on boolean cnf encodings of the problem instances. the performance of such modern sat solvers on these problems is competitive with or superior to the performance of optimized csp solvers running on the direct csp encoding of the instances  achlioptas et al. 1 .  note that the number of backtracks performed by satz is usually much lower than the number performed by other systematic algorithms because of its use of lookahead.  we begin by explaining this difference in hardness by showing how each generation model is biased away from the uniform distribution of all satisfiable quasigroup completion problems. next  we introduce a new satisfiable problem model  balanced qwh  which creates problems that are even harder than those foundby filtered qcp  as summarized in the last line of table 1. this new model provides a tool for creating hard structured instances for testing incomplete solvers. in addition  its simplicity and elegance suggests that it will provide a good model to use for theoretical analysis of structured problems.
모in the final section of the paper we turn to the issue of how we encode the quasigroup problems as boolean satisfiability problems. for systematic methods the best encoding is based on a view of a quasigroupas a 1-dimensionalcube  ratherthan as a 1-dimensional object. as shown below  the 1-d encodings are almost impossible to solve by systematic methods for larger orders. this difference is not unexpected  in that the 1d encodings include more redundant constraints  which are known to help backtracking algorithms. for walksat  however  a more complex picture emerges: the hardest instances can be solved more quickly under the 1-d encoding  while under-constrained instances are easier under the 1-d encoding.
1	the qwh and filtered qcp models
in order to better understand the qwh and filtered qcp models we begin by considering the problem of choosing a member uniformly at random from the set of all satisfiable quasigroup completion problems of order with uncoloredcells. it is easy to define a generator for the uniform model:  1  color cells of an order square randomly;  1  apply a complete solver to test if the square can be completed; if not  return to step  1 . this generator for the uniform model is  however  impractical for all but largest values of . the problem is that the probability of creating a satisfiable instance in step  1  is vanishingly small  and so the generator will loop for an exponentially long time. in other words  the set of satisfiable quasigroup completion problems  although large  is small relative to the set of all completion problems.
모because the instances generated by placing down random colors are almost certainly unsatisfiable  the quasigroup completion generator from gomes & selman  1  tries to filter out as many unsatisfiable configurations while incrementally partially coloring a square. the formulation is in terms of a csp  with a variable for each cell  where the domain of the variable is initialized to the set of colors. the generator repeatedly selects an uncolored cell at random  and assigns it a value from its domain. then  forward-checking is performed to reduce the domains of cells that share a row or column with it. if forward-checking reaches an inconsistency  an empty domain  the entire square is erased and the process begins anew. otherwise  once a sufficient number of cells have been colored arc consistency is checked; if the square passes this test  then it is output. the hardest mix of sat and unsat instances is generated when the number of holes is such that about 1% of the resulting squares are satisfiable. because of it's popularity in the literature  we simply call the distribution generated by the process the qcp model. adding a final complete solver to check satisfiability and eliminating those square which are unsatisfiable results in the filtered qcp model.1
모the hardest hole ratio for the filtered qcp model is shifted toward the more constrained side. we discovered that hardest satisfiable instances occur when the filtering step eliminates about 1% of the partial latin squares.
rggr grgrrrgrgfigure 1: top: the domains of each variable of an order 1 latin square after the top left cell is colored r. bottom:
the four completion problems  generated with probabilities respectively.
모while the use of incremental forward checking successfully biases qcp away from unsatisfiable problems  it also has the unintended effect of introducing a bias between different satisfiable instances. for a simple example  consider generating order 1 problems with 1 holes  where the colors are r and g. without loss of generality  suppose the first step of the algorithm is to color the top left square r. as shown in figure 1  forward-checking reduces the domains of two of the remaining cells. the next step is to pick one of the three remaining cells at random  and then choose a color in its domain at random. as a result  one of the four partial latin squares shown in the figure is created  but with different probabilities: the first two with probability each  and the last two with probability each. the fourth square is immediately eliminated by the final forward checking step. because our choice of initial square and color was arbitrary  we see that each of the partial latin squares where the two colored cells are in the same row or column is twice as like to be generated as one in which the two colored cells appear on a diagonal.
모the qwh model introduced by achlioptas et al.  1  works as follows:  1  a random complete latin square is cre-
rrrgrbfigure 1: two partial latin squares with same patternof holes but a different number of solutions  illustrating the bias of the
qwh model.
ated by a markov-chain process;  1  random cells are uncolored. does this generate the uniform model  again the answer is no. because all patterns of holes are equally likely to be generated in step  1   we can without loss of generality consider both the number and pattern of holes to be fixed. the probability of creating a particular completion problem is proportional to the number of different complete latin squares that yield that problem under the fixed pattern of holes. therefore  we can show that qwh does not generate the uniform model by simply presenting two partial latin squares with the same pattern of holes but a different number of solutions.
모such a counterexample appears in figure 1: the square on the left has two solutions while the one on the right has only a single solution. in other words  the left square is twice as likely to be generated as the right square. in general  the qwh model is biased towards problems that have many solutions.
1	patterns and problem hardness
we now consider different models of qcp and qwh  corresponding to different patterns. as we will see  the complexity of the models is dependent not only on the number of uncolored cells but also on the underlying pattern.
1	rectangular and aligned models
in order to establish a baseline  we first consider two tractable models for qcp and qwh  which we refer to as rectangular and aligned models. figure 1  left and middle  illustrates such instances. in the rectangular qwh model  a set of columns  or rows  is selected and all the cells in these columns are uncolored. moreover  one additional column  row  can be selected and be partially uncolored. the aligned qwh model is a generalization of the rectangular model in that we can pick both a set of rows and a set of columns and treat them as the chosen rows/columns in the rectangular model. put differently  in the aligned model  the rows and columns can be permuted so that all cells in are colored  some cells
of	are colored and all other cells are uncolored. we note that one could also naturally generate rectangular and aligned qcp instances.
모in order to show that both the rectangular and aligned models are tractable  let us consider a latin rectangle r on symbols . let r i  denote the number of occurrences of the symbol in r  we first introduce the following theorem from combinatorics.
모theorem:  ryser 1  an	x	latin rectangle r on symbols	can be embedded in a latin square of side	if and only if for all	.
theorem: completing a rectangular qcp or qwh is in p.
모proof: we can complete the rectangular qcp or qwh instance column by column  or row by row   starting with the column  or row  partially uncolored.  if there is no partially uncolored columns or rows  consider an arbitrary fully uncolored column or row.  we construct a bipartite graph   with parts	  and
in the following way: represents the uncolored cells of the column  row  that needs to be colors; represents the colors not used in that column  row . an edge       denotes that node can be colored with color . to complete the first column  row  we find a perfect matching in . if there is no such matching we know that the instance is unsatisfiable. after completing the first column  row   we can complete the remaining columns  rows  in the same way.
theorem: completing an aligned qcp or qwh is in p.
모proof: we start by noting that an aligned instance can be trivially rearranged into an x rectangle    by permutating rows and columns  with possibly a row or column partially uncolored . so  if we show that we can complete an x rectangle into an x rectangle  we have proved our theorem  since we obtain an instance of the rectangular model. to complete an x rectangle into an x   again we complete a column  or row  at a time  until we obtain a rectangle of side . for the completion of each column  or row   again  we solve a matching on the bipartite graph. the only difference resides in the way we build the graph: only contains colors for which the condition in ryser's theorem is satisfied.
1	balance
while the rectangular and aligned models cluster holes  we now turn to a model that attempts to increase problem hardness by minimizing clustering. let us start by reviewing the role of balance in the two most studied combinatorial optimization problems over random structures: random satisfiability and random graph coloring. it is particularly interesting to note the features shared by algorithms performing well on these problems.
모random -sat. a random formula is formed by selecting uniformly and independently clauses from the set of all -clauses on a given set of variables. the first algorithm to be analyzed on random -sat employs the pure literal heuristic repeatedly: a literal is satisfied only if does not appear in the formula. thus  a pure variable has all its occurrences appear with the same sign - a rather dramatic form of sign imbalance. the next key idea is unit-clause propagation  i.e.  immediately satisfying all clauses of length 1. more generally  dealing with shortest clauses first has turned out to be very useful. subsequent improvements also come from exploiting imbalances in the formula: using degree information to determine which variable to set next and considering the number of positive and negative occurrences to determine value assignment.

figure 1: left: an example of the rectangular model; middle: an example of the aligned model; right: a balanced model. holes모bayardo & schrag  1  gave experimental results on the role of balance  by considering random -sat formulas in which all literals occur in the same numberof clauses. in such formulas  an algorithm has to first set a non-trivial fraction of all variables  essentially blindly  before any of the ideas mentioned above can start being of use. this suggests the potential of performing many more backtracks and indeed  are in white.
balanced random -sat formulas are an order of magnitude harder than standard ones.
모random graph coloring. a random graph on vertices is formed by including each of the edges with probability
 . for graph coloring the most interesting range is   where is a constant. one can also consider list-coloring  where each vertex has a prescribed list of available colors. analogously to the pure literal heuristic  a first idea is to exploit the advantage offered by low degree vertices. in particular  if we are using colors then we can safely remove from the graph all vertices having degree smaller than : if we can color the remaining graph then we can certainly complete the coloring on these vertices since each one of them will have at least one available color  as it has at most neighbors . upon potentially reaching a -core  where every vertex has degree at least   it becomes useful to consider vertices having fewest available colors remaining and  among them  those of highest degree. conversely  random graphs that are degree-regular and with all lists having the same size tend to be harder to color.
1	balance in random quasigroups
in the standard qwh model we pick a random quasigroup and then randomly turn a number of its entries to holes. fixing the quasigroup choice and the number of holes  let us consider the effect that different hole-patterns have on the hardness of the resulting completion problem.  for brevity  we only refer to rows below but naturally all our comments apply to columns just as well. 
모two extreme cases are rows with just one hole and rows with holes. in the first case  the row can be immediately completed  while in the second case it turns out that given any consistent completion of the remaining squares one can always complete the quasigroup. more generally  it seems like a good idea for any algorithm to attempt to complete rows having a smallest number of holes first  thus minimizing the branching factor in the search tree. equivalently  having an equal number of holes in each row and column should tend to make things harder for algorithms.
모even upon deciding to have an equal number of holes in each row and column  the exact placement of the holes remains highly influential. consider for example a pair of rows having holes only in columns and further assume that there are no other holes in columns . then  by permuting rows and columns it is clear that we can move these four holes to  say  the top left corner of the matrix. note now that in this new  isomorphic  problem the choices we make for these four holes are independent of all other choices we make in solving the problem  giving us a natural partition to two independent subproblems.
모more generally  given the 1 matrix where zeros correspond to colored entries and ones correspond to holes  one can attempt to permute the rows and columns to minimize the bandwidth of  the maximum absolute difference between and for which is 1 . having done so  the completion problem can be solved in time exponential in the bandwidth.
모we were surprised to discover that even though satz contains no code that explicitly computes or makes use of bandwidth  indeed  exactly computing the bandwidth of a problem is np-complete   it is extremely efficient for boundedbandwidth problems. we generated bounded-bandwidth instances by first punching holes in a band along the diagonal of a latin square  and then shuffling the rows and columns 1 times to hide the band. satz solved all instances of this type for all problem sizes  up to the largest tested  order 1  and hole ratios in either 1 or 1 backtrack! satz's strategy is davis-putnamaugmentedwith one-step lookahead: this combination is sufficient to uncover limited-bandwidth instances. when the holes are placed randomly then with high probability the resulting matrix will have high bandwidth. alternatively  we can view as a random bipartite graph in which vertex is connected to vertex iff there is a hole in position . the high bandwidth of then follows from relatively standardresults from randomgraph theory fernandez de la ve뫣ga 1 . moreover  having an equal number of holes in each row/column makes the random graph regular  which guarantees that has large bandwidth. intuitively  small balanced instances should exhibit properties that hold of large random instances in the asymptotic limit.
모viewing the hole pattern as a regular random bipartite graph readily suggests a way for generating a uniformly random hole pattern with precisely holes in each row and column for any  i.e.    by the following algorithm:
set	.
repeat	times:
	set	to	.
	repeat	times:
	pick a uniformly random	;
	add	to	;
	remove all elements in row	and column	from	.
모balancing can be applied to either the qwh or qcp models: in the former  the pattern is used to uncolor cells; in the latter  the pattern is used to determine the cells that are not colored incrementally.
1	empirical results
we measured the difficulty of solving problem distributions generated under each of the models using three different al-

figure 1: comparison of generation models for order 1 using walksat. y-axis is the number of flips  log scale  and x-axis is the percentage of holes  re-parameterized . each data point represents the median number of flips for 1 instances. less-

constrained instances appear to the right.
gorithms. as a preliminary step problems were simplified by arc consistency and then translated into sat problems in conjunctive normal form  as described in sec. 1 below. for each data point for a range of percentage of holes 1 problems were generated and solved by three different algorithms: walksat  which performs local search over the space of truth assignments  using 1% noise and no cutoff; satz  which implements the backtracking davis-putnam algorithm augmented with 1-step lookahead; and sato  zhang 1   another davis-putnam type solver that uses dependencydirected backtracking and clause learning. in this paper we present the data for order 1  a size which clearly distinguishes problem difficulty but still allows sufficient data to be gathered. for reasons of space  we will omit detailed results for sato  which are qualitatively similar to those for satz.
모figure 1 displays the results for walksat: note the log scale  so that each major division indicates an order of magnitude increase in difficulty. less constrained problems appear to the right in the graphs: note that is the opposite of the convention for presenting results on random -cnf  but is consistent with achlioptas et al.  1  . on the log scale we see that the most constrained instances are truly easy  because they are solved by forward-checking alone. the underconstrained problems are of moderate difficulty. this is due to the fact that the underconstrainedproblemare simply much larger after forward-checking. again note that this situation is different from that of random -cnf  where it is the overconstrained problems that are of moderate difficulty  cook and mitchell 1 .
모at the peak the balanced qwh problems are much harder than the filtered qcp problems  showing that we have achieved the goal of creating a better benchmark for testing incomplete solvers. balancing can be added to the filtered qcp model; the resulting balanced qcp model are yet more difficult. this indicates that balancing does not make qwh and qcp equivalent: the biases of the two approaches remain distinct. both of the qwh models are harder than the qcp models in the under-constrainedregion to the right; we do not yet have an explanation for this phenomena.
모both the aligned and rectangle models are easy for walksat  and show no hardness peak. in the over-constrained area  to the left  they require more flips to solve than the others. this is because clustering all the holes prevents arc consistency from completely solving the problems in the overconstrained region  there are more than one solutions   as it usually does for the other models.
모figure 1 shows the same ordering of hardness peaks for satz. the behavior of satz on the rectangle case is an interesting anomaly: it quickly becomes lost on the underconstrained problems and resorts to exhaustive search. this is because satz gains its power from lookahead  and on under-constrained rectangular problems lookahead provides no pruning. sato  which employs look-back rather than lookahead  does not exhibit this anomaly: it solves the rectangular problems as easily as the aligned ones.
모we also measured the variance in the number of holes per row or column and the bandwidth of the balanced and random models. as expected  the variance was very low for the balanced case  averaging between 1 and 1 over the rage of ratios  compared with a range of 1 to 1 for the random case. thus non-balanced problems will often have rows or columns that contain only a few  highly-constrained holes that can be easily filled in.
1-d and 1-d encodings

figure 1: comparison of generation models for order 1 using the davis-putnam type solver satz. the x-axis is the percentage of holes  re-parameterized  and the y-axis is the number of backtracks  log scale . each data point represents the medianup to this point of the paper we have been concerned with understanding what makes a problem intrinsically hard. in practice  the difficulty of solving a particular instance using a particular algorithm is also dependent upon the details of the number of backtracks for 1 instances.
holesholes1-d1-d111111111111111111111table 1: average number unit propagations performed immediately after each branch by satz for the 1-d and 1-d encodings of order 1 qwh instances. hardness peak is at
 1 holes .
representation of the problem.
모although quasigroup completion problems are most naturally represented as a csp using multi-valued variables  encoding the problems using only boolean variables in clausal form turns out to be surprisingly effective. each boolean variable represents a color assigned to a cell  so where is the order there are variables. the most basic encoding  which we call the  1-dimensional  encoding  includes clauses that represent the following constraints:
1. some color must be assigned to each cell;
1. no color is repeated in the same row;1. no color is repeated in the same column.
constraint  1  becomes a clause of length for each cell  and  1  and  1  become sets of negative binary clauses. the total number of clauses is .
모the binary representation of a latin square can be viewed as a cube  where the dimensions are the row  column  and color. this view reveals an alternativeway of stating the latin square property: any set of variables determined by holding two of the dimensions fixed must contain exactly one true variable. the  1-dimensional  encoding captures this condition by also including the following constraints:
1. each color much appear at least once in each row; 1. each color much appear at least once in each column;
1. no two colors are assigned to the same cell.
as before  the total size of the 1-d encoding is	.
모as reported in achlioptas et al.  1   state of the art backtracking and local search sat solvers using the 1-d encoding are competitive with specialized csp algorithms. this is particularly surprising in light of the fact that the best csp algorithms take explicit advantage of the structure of the problem  while the sat algorithms are generic. previous researchers have noted that the performance of backtracking csp solvers on quasigroup problems is enhanced by using a dual representation  slaney et al. 1  shaw et al. 1  zhang and stickel 1 . this suggests a reason for the success of davis-putnam type sat solvers: in the csp dual encoding  there are variables for color/row pairs  where the domain is the set of columns  and similarly for color/column pairs  where the domain is the set of rows. the 1-d sat encoding essentially gives us these dual variables and constraints for free.
모this explanationis supportedby the extremelypoorperformance of sat solvers on the 1-d encodings of the problems. neither satz nor sato can solve any instances at the hardness peak for orders larger than 1; using the 1-d encoding  by contrast  either could solve all instances with one backtrack on average. as shown in figure 1  the work required by satz explodes as the problem becomes underconstrained   requiring over 1 backtracks for order 1.

figure 1: comparison of 1-d versus 1-d encodings using satz  backtracks  log scale  for 1-d encodings for orders 1 to 1. all problems of this size using the 1-d encodings could be solved by satz in 1 or 1 backtracks.모an explanation for the difference in performance of satz on the different encodings can be found by examining the number of unit propagations triggered by each split in the search trees. table 1 compares the number of unit propagations around the point at which the 1-d encodes become hard for satz  in bold . note that at this point each split sets about 1 times more variables for the 1-d encoding than for the 1-d encodings.
모are the 1-d encodings inherently hard  consider the performance of walksat  on even larger orders  1 and 1   shown on in figure 1. walksat shows an unusual pattern: the 1-d encodings are somewhat easier than the 1-d encodings at the peak  and somewhat harder than then 1-d encodings in the under-constrained region to the right. thus the 1-d and 1-d are in fact incomparable in terms of any general notion of hardness.
모a significant difference between the 1-d and 1-d encodings is that for both walksat and satz it is difficult to see any hardness peak at the threshhold: the problems become hard and then stay at least as hard as they become more and more under-constrained. note that the most underconstrained instances are inherently easy  since they correspond to a empty completion problem. this reinforces our argument that the 1-d encoding more accurately reflects the underlying computational properties of the quasigroup problem.
모in summary  it is important to distinguish properties of a problem instance that make it inherently hard for all methods of attack and properties that make it accidently hard for particular methods. while the encoding style is such an accidental property  the main conjecture we present in this paper is that balance is an inherent property. the evidence for the conjecture is that increasing balance increases solution time for a variety of solvers.
1	conclusions
models of random problem generation serve two roles in ai: first  to provide tools for testing search and reasoning algorithms  and second  to further our understanding of what makes particular problems hard or easy to solve  as distinct from the fact that they fall in a class that is worst-case intractable. in this paper we introduced a range of new models of the quasigroup completion problem that serve these roles. we showed how a new notion of balance is an important factor in problem hardness. while previous work on balancing formulas considered the roles of positive and negative literals  our notion of balance is purely structural. balancing improves the usefulness of the qwh model - one of the best models known for testing incomplete solvers - while retaining its formal simplicity and elegance.
references
d. achlioptas  c. gomes  h. kautz  b. selman  1 .
generating satisfiable instances. proc. aaai-1.
bayardo  r.j. and schrag  r.c.  1  using csp look-back techniques to solve exceptionally hard sat instances. proc. cp-1   1.
cheeseman  p. and kanefsky  r. and taylor  w.  1 . where the really hard problems are. proc. ijcai-1  1.
cook  s.a. and mitchell  d.  1 . finding hard instances of the satisfiability problem: a survey  in d. du  j. gu  and p. pardalos  eds. the satisfiability problem. vol. 1 of dimacs series in discr. math. and theor. comp.
sci.  1.
fernandez de la ve뫣ga  w.  1  on the bandwidth of random graphs. combinatorial mathematics  northholland  1.
gent  i. and walsh  t.  1 an empirical analysis of search in gsat. j. of artificial intelligence research  vol. 1  1.
gibbs  n.e.   poole  jr.  w.e. and stockmeyer  p.k.  1 . an algorithm for reducing the bandwidth and profile of a sparse matrix  siam j. numer. anal.  1  1   1- 1.

figure 1: comparison of 1-d versus 1-d encodings using walksat  flips  log scale  for orders 1 and 1.gomes  c.p. and selman  b.  1a . problem structure in the presence of perturbations. proc. aaai-1.
hall  m.  1 . an existence theorem for latin squares. bull. amer. math soc.  1   1   1.
hogg  t.  huberman  b.a.  and williams  c.p.  eds.   1 . phase transitions and complexity. art. intell.  1  1.
hoos  h. 1. satlib. a collection of sat tools and data. see www.informatik.tu-darmstadt.de/ai/satlib.
impagliazzo  r.  levin  l.  and luby  m.  1 .	pseudorandom number generation of one-way functions. proc. 1st stoc.
kirkpatrick  s. and selman  b.  1 . critical behavior in the satisfiability of boolean expressions. science  1  1  1.
li  chu min and anbulagan 1 . heuristics based on unit propagation for satisfiability problems. proc. ijcai-1  1.
mitchell  d.  selman  b.  and levesque  h.j.  1 . hard and easy distributions of sat problems. proc. aaai-1  1.
regin  j.c.  1 . a filtering algorithm for constraints of difference in csp. proc. aaai-1  1.
ryser  h.  1 . a combinatorial theorem with an application to latin rectangles. proc. amer. math. soc.  1   1   1.
selman  b. and levesque  h.j.  and mitchell  d.g.  1 . a new method for solving hard satisfiability problems. proc. aaai-1.
shaw  p.  stergiou  k.  and walsh  t.  1  arc consistency and quasigroup completion. proc. ecai-1  workshop.
slaney  j.  fujita  m.  and stickel  m.  1  automated reasoning and exhaustive search: quasigroup existence problems. computers and mathematics with applications  1  1   1.
stergiou  k. and walsh  t.  1  the difference alldifference makes. proc. of ijcai-1.
van gelder  a.  1 . problem generator  mkcnf.c  contributed to the dimacs 1 challenge archive.
zhang  h.  1 . sato: an efficient propositional prover. proc. cade-1.
zhang  h. and stickel  m.e.  1 implementing the davis-
putnam method. journal of automated reasoning 1  1  1.

efficient consequence finding

laurent simon
laboratoire de recherche en informatique
u.m.r. cnrs 1  universite paris-sud뫣
1 orsay cedex  france simon lri.fr
alvaro del val
e.t.s. informatica  b-1뫣
universidad autonoma de madrid뫣
1 madrid  spain delval ii.uam.es

abstract
we present an extensive experimental study of consequence-finding algorithms based on kernel resolution  using both a trie-based and a novel zbdd-based implementation  which uses zerosuppressed binary decision diagrams to concisely store and process very large clause sets. our study considers both the full prime implicate task and applications of consequence-finding for restricted target languages in abduction  model-based and faulttree diagnosis  and polynomially-bounded knowledge compilation. we show that the zbdd implementation can push consequence-finding to a new limit  solving problems which generate over 1 clauses.
1	introduction
many tasks in artificial intelligence can be posed as consequence-finding tasks  i.e. as the problem of finding certain consequences of a propositional knowledge base. these include prime implicates  abduction  diagnosis  nonmonotonic reasoning  and knowledge compilation.  marquis  1  provides an excellent survey of the field and its applications. unfortunately  there have been very few serious computational attempts to address these problems. after some initial excitement with atmss  de kleer  1  and clause management systems  reiter and de kleer  1; kean and tsiknis  1   it was soon realized that these systems would often run out of resources even on moderately sized instances  and interest on the topic waned. thus  for example   marquis  1  says in concluding his survey:
 the proposed approaches  to consequencefinding  are however of limited computational scope  algorithms do not scale up well   and there is only little hope that significantly better algorithms could be designed 
모in this paper  we conduct the first extensive experimental study of consequence-finding  cf   more specifically of one of the approaches referred to in the above quote  namely kernel resolution  del val  1 . kernel resolution allows very efficient focusing of consequence-finding on a subset of  interesting  clauses  similar to sol resolution  inoue  1 . we also introduce novel consequence-finding technology  namely  the use of zbdds  zero-suppressed binary decision diagrams  minato  1   to compactly encode and process extremely large clause sets. as a result of this new technology  the well-known tison method  tison  1  for finding the prime implicates of a clausal theory  a special case of kernel resolution  can now efficiently handle more than 1 clauses. the combination of focused consequencefinding with zbdds makes cf able to deal computationally with significant instances of the applications discussed above for the first time. in particular  our experiments include both full cf  the prime implicate task   and applications of consequence-finding for restricted target languages in abduction  model-based and fault-tree diagnosis  and polynomiallybounded knowledge compilation.
모we assume familiarity with the standard literature on propositional reasoning and resolution. some definitions are as follows. a clause c subsumes a clause d iff c   d. the empty clause is denoted 1. for a theory  set of clauses    we use 뷃   to denote the result of removing all subsumed clauses from . an implicate of  is a clause c such that  |= c; a prime implicate is an implicate not subsumed by any other implicate. we denote by pi   the set of prime implicates of . we are often interested only in some subset of pi  . for this purpose  we define the notion of a target language lt  which is simply a set of clauses. we assume lt is closed under subsumption  c.u.s.   i.e. for any c 뫍 lt and d   c  we have d 뫍 lt. a target language can always be closed under subsumption by adding all subsumers of clauses in the language.
모given these definitions  the task we are interested in is finding the prime lt-implicates of   defined as pilt   = pi   뫌 lt. we will mainly consider the following target languages: l is the full language  i.e. the set of all clauses over the set v ar   of variables of . l1 = {1} contains only the empty clause. given a set of variables v   the  vocabulary-based  language lv is the set of clauses over
v . finally  for a constant k  lk is the set of clauses over v ar   whose length does not exceed k. thus we have l1 
l1  etc.
모each of these languages corresponds to some important ai task. at one extreme  finding the prime implicates of  is simply finding pil   = pi  ; at the other extreme  deciding whether  is satisfiable is identical to deciding whether pi 1   is empty. vocabulary-based languages also have l
many applications  in particular in abduction  diagnosis  and non-monotonic reasoning  see e.g.  inoue  1; selman and levesque  1; marquis  1  among many others . finally  lk or subsets thereof guarantee that pilk   has polynomial size  which is relevant to knowledge compilation  surveyed in  cadoli and donini  1  .
모sometimes we will be interested in theories which are logically equivalent to pilt    but which need not include all lt-implicates  and can thus be much more concise. we refer to any such theory as a lt-lub of   following  selman and kautz  1   see also  del val  1 . we'll see one particular application of lt-lubs in our discussion of diagnosis later.
1	kernel resolution: review
kernel resolution  described in  del val  1; 1a   is a consequence-finding generalization of ordered resolution. we assume a total order of the propositional variables x1 ... xn. a kernel clause c is a clause partitioned into two parts  the skip s c   and the kernel k c . given any target language lt closed under subsumption  a lt-kernel resolution deduction is any resolution deduction constructed as follows:  a  for any input clause c  we set k c  = c and s c  =  ;  b  resolutions are only permitted upon kernel literals;  c  the literal l resolved upon partitions the literals of the resolvent into those smaller  the skip   and those larger  the kernel  than l  according to the given ordering; and  d  to achieve focusing  we require any resolvent r to be ltacceptable  which means that s r  뫍 lt.
모in order to search the space of kernel resolution proofs  we associate to each variable xi a bucket b xi  of clauses containing xi. the clauses in each bucket are determined by an indexing function ilt   so that c 뫍 b xi  iff xi 뫍 ilt c . we can always define ilt c  = {kernel variables of the largest sumed sorted in ascending orderprefix l1 ...lk of c s.t. l1 ...lk  del val  1 뫍 lt}  where ; resolvingc is ason any other kernel literal would yield a non-lt-acceptable resolvent.
모bucket elimination  abbreviated lt-be  is an exhaustive search strategy for kernel resolution. lt-be processes buckets b x1  ... b xn  in order  computing in step i all resolvents that can be obtained by resolving clauses of b xi  upon xi  and adding them to their corresponding buckets  using ilt . we denote the set of clauses computed by the algorithm as lt-be  . the algorithm  which uses standard subsumption policies  so that lt-be   = 뷃 lt-be      is complete for finding consequences of the input theory which belong to the target language lt  that is  lt-be  뫌lt = pilt  . as shown in  del val  1   l-be is identical to tison's prime implicate algorithm  tison  1   whereas l1-be is identical to directional resolution  dr   the name given by  dechter and rish  1  to the original  resolution-based davis-putnam satisfiability algorithm  davis and putnam  1 . l1-be   is called the directional extension of   and denoted dr  .
모for lv we will in fact consider two be procedures  both of which assume that the variables of v are last in the order-

figure 1: zbdd and trie representation for the set of clauses
 = {a 뫈 b 뫈  c  a 뫈 b  a 뫈  c  b 뫈  c}.
ing. l1v -be is simply lv -be under this ordering assumption. l1v -be is identical  except that processing is interrupted right before the first variable of v is processed. thus l1v -be   뫌 lv = pilv     whereas l1v -be   뫌 lv is logically equivalent but not necessarily identical to pilv   ; i.e.  it is a lv -lub of . note that  in either case  the desired set of clauses is stored in the last buckets. the advantage of this ordering is that either form of lv -be behave exactly as directional resolution  a relatively efficient satisfiability method  up to the first v -variable of the ordering. l1v -be stops right there  and is thus strictly cheaper than deciding satisfiability with dr under such orderings   while l1v -be continues  computing the prime implicates of l1v -be   뫌 lv with full kernel resolution over the v buckets.
1	zero-suppressed bbds: review
 bryant  1  provides an excellent survey and introduction of binary decision diagrams  bdd . a bdd is a directed acyclic graph with a unique source node  only two sinks nodes  1 and 1  interpreted respectively as true and false  and with labeled nodes   x n1 n1 . such node x has only two children  n1 and n1  connected respectively to its 1arc and 1-arc  and is classically interpreted as the function f = if xthenf1 elsef1  if f1 and f1 interpret the bdds n1 and n1 .
모the power of bdds derives from their reduction rules. a robdd  reduced ordered bdd  simply noted bdd in the following  requires that its variables are sorted according to a given order and that the graph does not contain any isomorphic subgraph  node-sharing rule . in addition  the nodeelimination rule deletes all nodes   x n n  that do not care about their values. with the classical semantics  each node is labeled by a variable and each path from the source node to the 1-sink represents a model of the encoded formula. the bdd can thus be viewed as an efficient representation of a shannon normal tree.
모in order to use the compression power of bdds for encoding sparse sets instead of just boolean functions   minato  1  introduced zero-suppressed bdds  zbdds . their principle is to encode the boolean characteristic function of a set. for this purpose  minato changed the node-elimination rule into the z-elimination rule for which useless nodes are those of the form   x 1 n . so  if a variable does not appear on a path  then its default interpretation is now false  which means absent  instead of don't care . if one wants to encode only sets of clauses  each zbdd variable needs to be labeled by a literal of the initial formula  and each path to the 1-sink now represents the clause which contains only the litterals labeling the parents of all 1-arcs of this path. figure 1 represents the zbdd encoding the set of clauses  = {a 뫈 b 뫈  c  a 뫈 b  a 뫈  c  b 뫈  c}.
모the  compressing  power of zbdds is illustrated by the fact that there exist theories with an exponential number of clauses that can be captured by zbdds of polynomial size. because their complexity only depends on the size of the zbdd  not on the size of the encoded set  zbdd operators can be designed to efficiently handle sets of clauses of an exponential size.
1	kernel resolution on zbdds
in order to deal with this compact encoding in the context of kernel resolution  we need to define a way to efficiently obtain resolvents  and to identify buckets. for the former task   chatalic and simon  1b; 1c  introduced a very efficient multiresolution rule which works directly on sets of clauses  and thus which can compute the result of eliminating a variable without pairwise resolving clauses.
모definition  multiresolution : let  + and    be two sets of clauses without the variable xi. let ri be the set of clauses obtained by distributing the set  + over   . multiresolution is the following rule of inference:
 xi 뫈   +  뫇   xi 뫈         ri.
모a bucket b xi  can always be expressed in the form required by this rule  so that ri corresponds to the set of resolvents obtained by processing b xi . the main advantage of this definition is that it can be shown that if ri is computed directly at the set level  without explicitly enumerating clauses  its complexity can be independent of the number of classical resolution steps.  chatalic and simon  1b  propose a system called zres which implements the multiresolution principle by means of zbdds. zres manipulates clause sets directly  through their zbdd representation  with no explicit representation of individual clauses. a bucket is easily retrieved from the zbdd storing the current clause set in the form of zbdds for appropriate  +i and   i ; and it is then processed using a specialized zbdd operator  clausedistribution  which implements multiresolution by distributing  over   i to obtain ri. the system comes with other zbdds operators designed for clause-set manipulation  such as subsumption-free union and set-difference. all these three operators delete subsumed and tautologous clauses as soon as possible  during the bottom-up construction of the resulting zbdd.
모to generalize these ideas for focused kernel resolution  we need to deal with complex indexing functions. for this purpose  we keep two zbdds  for dead and live clauses respectively. a  dead  clause is one which can no longer be resolved according to the lt-restriction in place  given the variables that have been processed so far. for example  a clause becomes dead for l1  i.e. for dr  when just one of its variables is processed; and for lk when its first k + 1 variables are processed. right before processing a variable  we obtain  its bucket  from the live zbdd. processing the variable is then done by multiresolution. finally  when done with a variable  new clauses become dead  and they are moved from the live zbdd to the dead zbdd.
1	experimental results
our experimental tests  which include structured benchmark gathering and generating as well as random instances testing  represents more than 1 cpu-days on the reference machine1  more than 1 runs on structured examples and almost the same on random instances. only selected results are presented here  for an obvious lack of space.
모times are reported in seconds. all experiments use a 1 seconds timeout  and a dynamic min-diversity heuristic for variable ordering. this heuristic chooses the variable for which the product of its numbers of positive and negative occurrences is minimal  thus trying to minimize the number of resolvents.
모in addition to the zbdd-based implementations  labeled zres  we also consider algorithms based on the trie-data structure  de kleer  1   namely  picomp and drcomp  for respectively prime implicates and directional resolution. in contrast to the zbdd programs  trie-based programs explicitly represent clauses and buckets  and use pairwise resolution. the trie  illustrated in figure 1  includes a number of significant optimizations to minimize trie traversal  for efficient subsumption.
모it can be shown that zbdds are never worse than tries in space requirements  and can be much better. intuitively  while tries can only share common structure on the prefix of clauses  zbdds can do it on their prefix and postfix  i.e. they allow factoring clauses from both the beginning and the end. in terms of time  as we will see  the situation is more complex. nevertheless  the zbdd implementation is the one that really pushes consequence-finding well beyond its current limits.
1	prime implicates
prime implicate computation remains one of the fundamental consequence finding tasks. we focus in this section on our two very different tison's algorithm implementations  picomp and zres-tison. we use the following benchmarks  among others: chandra-n  chandra and markowsky  1  and mxky  kean and tsiknis  1  are problems with known exponential behavior; adder-n  n-bit adders   serial-adder  n-bit adders obtained by connecting n full adders   bnp-n-p  tree-structured circuits with n layers  where p is a probability   el fattah and dechter  1   represent digital circuits; serial-diag-adder and bnp-ab represent the same circuits in a form suitable for diagnostic reasoning; pipes and pipes-syn are problems drawn from qualitative physics; type-k-n problems 
benchmark#pispicompzres-tisonadder-11.1adder-1.1e+1-1adder-1.1e+1-1chandra-111chandra-1.1e+1-1chandra-1.1e+1-1bnp-1-ts1-rr-111bnp-1-ts1-rr-1.1e+1-1bnp-1-ts1-rr-1.1e+1-1bnp-ab-1-ts1-rr-111bnp-ab-1-ts1-rr-1.1e+1-1m11.1.1m11e+1-1m11e+1-1n-queens-111pigeons-11.1.1pigeons-11.1pipes-simple-11.1pipes-simple-1-1pipes-syn-111pipes-syn-1.1e+1-1serial-adder-11e+1-1serial-adder-11a+1-1serial-diag-adder-11e+1-1serial-diag1-1b1e+1-1type11.1type11e+1-1table 1: prime implicates with picomp and zres-tison
due to  mathieu  1   attempt to mimic  typical  structures of expert system kbs  with each k representing a particular kind of structure.
모the first observation that can be drawn from table 1 is the scalability of zres-tison  which can handle a huge number of clauses on some particular instances  e.g. 1 for type1 . thus it forces us to completely reconsider tison's algorithm efficiency and limitations. in all these examples  picomp simply cannot physically represent such sets of clauses and  like all previous tison's implementations  is no competition for zres-tison. yet  on some instances  picomp remains faster. this is due to small distributions computation  involving less than a thousand clauses  e.g. type-1  at each multiresolution step. on such examples  traditional approaches with explicit representation of clauses remain faster. notice finally the relative independence of the running time of zres-tison w.r.t. the number of pis  in contrast to picomp. for zres-tison there are easy examples with a huge number of pis  and hard examples with few pis. the latter include n-queens and satisfiable pigeon problems  which contrasts with the excellent results for satisfiability of similar problems in  chatalic and simon  1c .
1	polynomial size compilation
one of the major theoretical limits of knowledge compilation  kc  is that it is very unlikely that a kc method can
k 1 k 1 k 1 k 1 z-tisonk 1 -1  1 1  1 1  1 1  1 k 1 1  1 -1  1 1  1 1  1 k 1 1  1 1  1 -1  1 k 1 1  1 1  1 1-1  1 z-tison1  1 1  1 1  1 1  1 -table 1: compilation with zres-kbound and zres-tison
be defined that always produces tractable compiled theories of polynomial size  unless we restrict the query language to have polynomial size. that's what lk-be does. to implement this with zres  clauses having more than k processed variables are moved to the dead zbdd. we test here only the zres-kbound implementation with different k on the same benchmarks used on prime implicates. summarizing so many test runs on so many different benchmarks families is not easy. we attempt to do so in table 1  where a cell on row i and column j contains the percentage of benchmarks quickly solved by the program i in comparison with program j. for instance  zres-kbound 1  terminates more quickly than zres-tison in 1% of the cases  and the median value of the cpu gain is 1. of course  such a table can't take into account the particularities of each benchmark family  but it still can give some taste of how the different approaches behave with respect to each other.
모what is striking is that tison remains the fastest procedure in most cases  with a median cpu gain of around 1%. but  we can see that  in 1% of the cases  kbound 1  perform much better than zres-tison. this phenomenon  also observed with kbound 1   means that when zres-tison fails  kbound remains an available solution. moreover  our results suggest that kbound 1  and kbound 1  perform similarly  yet the latter computes a much more interesting base.
1	directional resolution
directional resolution compiles a theory into an equivalent one in which one can generate any model in linear time. we test here our two dr implementations  drcomp and zres  on some specific benchmarks  with two heuristics for each one. in table 1 the par-1-*  pret1  ssa1 files are taken from the dimacs database  and #kb means the size of the theory obtained by the programs. as this size depends on the order of variable deletions  we only report it for the first case  where the two programs give the same result.
모as shown in table 1  good heuristics have a major effect. using just input ordering often forces the size of the kb to grow in such a way that drcomp can't compete with zres. zbdds pay off in all these cases  showing their ability to represent very large kbs. the min-diversity heuristics seems to invert the picture  mainly because the ordering helps to keep the clause set to manageable size  tens of thousands of clauses . the same applies to theories which are tractable for directional resolution  such as circuit encodings  del val  1b   for which drcomp takes no time. zres  in contrast  seems to include a significant overhead on theories which do not generate many clauses  despite its relatively good performance reported in  chatalic and simon  1b . on the other
input ordermin diversitybench.#kbzresdrcompzresdrcompadder-11.11chandra-1.1e+1-11m11.1.1.1.1m11e+1-11par1-c1-11par11-111pret11e+1.1-11ssa11e+1-11type11.1.1.1.1table 1: directional resolution algorithm
hand  zbdds scale much better when dealing with very large kbs.
1	abduction
given a theory  and a set a of variables  called assumptions  hypothesis or abducibles   an abductive explanation of a clause c wrt.  is a conjunction l of literals over a such that  뫋 l is consistent and entails c  or a subsumed clause of c . l is a minimal explanation iff no subset of it is also an explanation.
모on the adder-n  mult-n  adder and multiplier whose result is smaller than n   and on the bnp circuits introduced in section 1  we are given a circuit  with a set of input variables i and output variables o. we want to explain some observation  an instantiation of some o variables  with only variables from i  thus forgetting all internal variables . this problem can be addressed through lv -be with v = i 뫋 o.
in this case  the trie-based implementation failed in all benchmarks  except adder-1   so we only report results for the zbdd-based implementation  in table 1. we also tried our algorithms on the iscas circuits benchmark family  without success. to interpret these results appropriately  one should note that obtaining pilv means precomputing answers for all abduction problems for   as explanations can be directly read off from pilv   . kernel resolution also provides methods for answering only specific abduction problems  as opposed to all   but we have not tested them; for some tests with a similar flavor  check the diagnosis experiments on iscas benchmarks  section 1.
모note that  surprisingly  all adder examples are treated much more efficiently by zres-tison  table 1  than by l1v -be. a similar phenomenon was already pointed out in  chatalic and simon  1a   where  on some examples  computing dr on a subset of variables was proved harder than on the whole set of variables  l1v -be harder than l1a-be  where v   a . but  here  in addition  the induced hardness overcomes the simplification due to the use of dr instead of tison  which is clearly counter-intuitive. this is not the case for the mult-n nor for the bnp-1 instances  on which picomp and zres-tison failed.
모table 1 presents some easier examples  the path-kr problems from  prendinger et al.  1   abbreviated p-kr   which can be solved by both drcomp and picomp. for comparison with their results  note that we  unlike them  are
l1vl1vbench.time#lv -lubtime#pilvadder-1.111adder-1.111adder-1.1.1e+1.1.1e+1adder-1.1.1e+1--adder-1.1.1e+1--mult-1.111mult-1.1--mult-1.111bnp-1-1.111bnp-1-1.111bnp-1-1.1.1e+1--bnp-1-1.1.1e+1--table 1: abduction on circuits: zbdd-based lv -be.
bench.#kbzresdrcomp#piszres-tis.picompp-1r1.1.111p-1r1.1.111p-1r1111table 1: abduction on path problems
solving all abduction problems for the given instance in one go.
1	fault-tree diagnosis
the set of aralia  rauzy  1  benchmarks arise from fault tree diagnosis  in which one wants to characterize the set of elementary failures  coded by the fi predicates  that entail the failure of the whole system  the root predicate . it thus amounts to computing the set of lv -implicants of the root  where v is the set of fi. we use the well-known duality cnf/dnf and implicants/implicates to express those problems as lv -implicates problems.
모usually  these benchmarks are efficiently solved using traditional bdds approaches  i.e. rewritting the initial formula under shannon normal form . in  rauzy  1   internal variables  defined as an equivalency with a subformula   are not even encoded in the bdd: they are directly identified with the bdd encoding the subformula. in our approach  such internal variable are explicitly present in the cnf of the formula. the work of zres amounts to precisely delete all of them  thus obtaining a formula build on fi  and the root  predicates. in table 1  we show that kernel resolution  with multiresolution  can efficiently handle those problems without needing to rewrite the formula under shannon normal form. to our knowledge  this is the first time that such direct cf approach successes.
1	model-based diagnosis
in diagnosis  de kleer et al.  1   we are given a theory  describing the normal behavior of a set of components; for each component  there is an  abnormality  predicate abi. let v = ab be the set of abi's. given a set of observations o  the diagnosis are given by the prime lv -implicants  noted palv   of pilv   뫋 o . as we can obtain these implicants
benchmarkzres#pilvbaobab1.1baobab1.1das1.1das1.1.1e+1edf1.1edf1.1.1e+1isp1.1.1e+1isp1.1table 1: aralia abduction benchmarks
bench.t. l1v -be#lv -lubt. palv#palvc1-d-1.111c1-d-1.1--c1-d-1.111c1-d-1.111c1-d-1.1--c1-d-1.111e+1c1-d-1.111c1-d-1.111c1-d-1.111c1-d-1.1--c1-d-1.1--c1-d-1.111table 1: model-based diagnosis for iscas benchmarks
from any equivalent lv -lub of   either l1v -be or l1v -be can yield the diagnosis  so we use the cheaper l1v -be.
모we first consider iscas benchmarks. we generated for each of the four easiest circuits their normal behavior formula  by introducing abi variables on each gate   and 1 faulty input-output vectors. the results are given in table 1  with t. denoting the total time   only for the zbdd-based implementation. in a number of cases  where lv -lub is of reasonable size   drcomp can also go through the first phase  but our current implementation fails to complete the second phase  thus we don't report its results here. to generate the implicants with our zbdd-based system  we use one of its procedures that can negate a given formula. 1 of the 1 instances were too hard for l1v -be  and 1 failed to compute the lv -implicants. to our knowledge  this is the first time such instances are tested with a direct consequence-finding approach.
모table 1 presents results for bnp-ab circuits for the much harder  compiled  approach to diagnosis  where we basically precompute pilv for all possible input-output vectors  by computing pilab뫋o뫋i. clearly  in this case it pays off to use the vocabulary-based procedures  as opposed to full pis  for both tries and zbdds.
1	random instances
finally  we consider random instances generated with the fixed clause-length model. while conclusions drawn from them do not say anything about the behavior of algorithms on structured and specially real-world instances  they provide in our case a good tool for evaluating subsumption and optimization techniques for resolution. it was observed in
bench.#kbzresdrcomp#piszres-tis. picompbnp-11.1.1	1	1bnp-11.1.1	1	1bnp-11.1.1.1e+1	1	-bnp-11.1.1.1e+1	1	-bnp-11.1-1e+1	1	-table 1:  compiled  diagnosis on tree-structured circuits
programmeanmed.std1% int.#uns. mt zres-tison1111-11  1 picomp1111-11zres-kbound 1.1.1.1.1.1  1 zres-kbound 1.1.1.1.1.1  1 zres-kbound 1.1.1.1.1.1  1 table 1: consequence finding in random formulae
 dechter and rish  1; chatalic and simon  1a  that such instances are very hard for dr  and  schrag and crawford  1  observed even worse behavior for consequencefinding for instances generated near the sat phase transition threshold. using a more powerful computer than in the previous benchmarks1  we compare picomp  zres-tison  and zres-kbound on 1 formulae at the ratio 1  1 variables  1 clauses  1% satisfiable . if we focus on satisfiable theories  their mean number of prime implicates is 1  std: 1; median: 1; 1% confidence interval: 1; max: 1 . table 1 summarizes cpu-time results  the last column is the number of unsolved instances and the maximal cpu time needed if all instances have been solved .
모we observe from this table that zbdds are always better than tries on random instances. for instance  with zrestison  the maximal number of clauses reached by each run has a median of 1  mean; 1  std: 1  1% int: 1  max: 1   but the maximal number of zbdds nodes has only a median of 1  mean: 1  std: 1  1% int: 1  max: 1 . this gives less than one node per clause on average  despite the fact that most of the clauses are longer than 1. even if such instances are  unstructured   at the beginning of the calculus  resolution  which we have seen really amounts to clause distribution  introduces many redundancies in formulae  which seem to be well-captured by zbdds.
1	conclusion
we presented an extensive experimental study of consequence-finding  including results on many of its applications  using two quite different implementations. by combining the focusing power of kernel resolution with the zbdd representation and multiresolution  we have shown the scalability of our approach and the relative independance of its performances with respect to the size of the handled kb. the until now mainly theoretical applications of cf can now be approached for problems of realistic size  and in some cases of spectacular size.
