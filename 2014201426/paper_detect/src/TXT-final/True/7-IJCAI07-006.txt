
the modelling and reformulation of constraint networks are recognised as important problems. the task of automatically acquiring a constraint network formulation of a problem from a subset of its solutions and non-solutions has been presented in the literature. however  the choice of such a subset was assumed to be made independently of the acquisition process. we present an approach in which an interactive acquisition system actively selects a good set of examples. we show that the number of examples required to acquire a constraint network is significantly reduced using our approach.
1	introduction
constraint programming  cp  provides a powerful paradigm for solving combinatorial problems. however  the specification of constraint networks still remains limited to specialists in the field. an approach to automatically acquiring constraint networks from examples of their solutions and nonsolutions has been proposed by  bessiere et al.  1 . constraint acquisition was formulated as a concept learning task. the classical version space learning paradigm  mitchell  1  was extended so that constraint networks could be learned efficiently. constraint networks are much more complex to acquire than simple conjunctive concepts represented in propositional logic. while in conjunctive concepts the atomic variables are pairwise independent  in constraint satisfaction there are dependencies amongst them.
﹛in  bessiere et al.  1  the choice of the subset of solutions and non-solutions to use for learning was assumed to be made before and independently of the acquisition process. in this paper we present an approach in which the acquisition system actively assists in the selection of the set of examples used to acquire the constraint network through the use of learner-generated queries. a query is essentially a complete instantiation of values to the variables in the constraint network that the user must classify as either a solution or nonsolution of her 'target' network. we show that the number of examples required to acquire a constraint network is significantly reduced if queries are selected carefully.
﹛when acquiring constraint networks computing good queries is a hard problem. the classic query generation strategy is one in which  regardless of the classification of the query  the size of the version space is reduced by half. therefore  convergence of the version space can be achieved using a logarithmic number of queries. furthermore  in the classic setting  a query can be generated in time polynomial in the size of the version space. when acquiring constraint networks  query generation becomes np-hard. this is further aggravated by the fact that in constraint acquisition  while the ordering over the hypothesis space is most naturally defined in terms of the solution space of constraint networks  we usually learn at the constraint level  i.e. a compact representation of the set of solutions of a hypothesis. our main contribution is a number of algorithms for identifying good queries for acquiring constraint networks. our empirical studies show that using our techniques the number of examples required to acquire a constraint network is significantly reduced. this work is relevant to interactive scenarios where users are actively involved in the acquisition process.
1	constraint acquisition using conacq
a constraint network is defined on a  finite  set of variables x and a  finite  set of domain values d. this common knowledge shared between the learner and the user is called the vocabulary. furthermore  the learner has at its disposal a constraint library from which it can build and compose constraints. the problem is to find an appropriate combination of constraints that is consistent with the examples provided by the user. for the sake of notation  we shall assume that every constraint defined from the library is binary. however  the results presented here can be easily extended to constraints of higher arity  and this is demonstrated in our experiments.
﹛a binary constraint cij is a binary relation defined on d that specifies which pairs of values are allowed for variables xi xj. the pair of variables xi xj  is called the scope of cij. for instance  ≒1 denotes the constraint specified on  x1 x1  with relation  less than or equal to . a binary constraint network is a set c of binary constraints. a constraint bias is a collection b of binary constraints built from the constraint library on the given vocabulary. a constraint network c is said to be admissible for a bias b if for each constraint cij in c there exists a set of constraints {b1ij ﹞﹞﹞ bkij} in b such that cij = b1ij ﹎ ﹞﹞﹞ ﹎ bkij.
an instance e is a map that assigns to each variable xi in
x a domain value e xi  in d. equivalently  an instance e can be regarded as a tuple in dn. an instance e satisfies a binary constraint cij if the pair  e xi  e xj   is an element of cij; otherwise we say that cij rejects e. if an instance satisfies every constraint in c  then e is called a solution of c; otherwise  e is called a non-solution of c.
﹛finally  a training set ef consists of a set e of instances and a classification function f : e ↙ {1}. an element e in e such that f e  = 1 is called positive example  often denoted by e+  and an element e such that f e  = 1 is called negative example  often denoted by e  . a constraint network c is said to be consistent with a training set ef if every positive example e+ in ef is a solution of c and every negative example e  in ef is a non-solution of c. we also say that c correctly classifies ef. given a constraint bias b and a training set ef  the constraint acquisition problem is to find a constraint network c admissible for the bias b and consistent with the training set ef.
﹛a sat-based algorithm  called conacq  was presented in  bessiere et al.  1  for acquiring constraint networks based on version spaces. informally  the version space of a constraint acquisition problem is the set of all constraint networks that are admissible for the given vocabulary and bias  and that are consistent with the given training set. we denote as vb ef  the version space corresponding to the bias b and the training set ef. in the sat-based framework this version space is encoded in a clausal theory k. each model of the theory k is a constraint network of vb ef .
﹛more formally  if b is the constraint bias  a literal is either an atom bij in b  or its negation  bij. notice that  bij is not a constraint: it merely captures the absence of bij in the acquired network. a clause is a disjunction of literals  also represented as a set of literals   and the clausal theory k is a conjunction of clauses  also represented as a set of clauses . an interpretation over b is a map i that assigns to each constraint atom bij in b a value i bij  in {1}. a transformation is a map 耳 that assigns to each interpretation i over b the corresponding constraint network 耳 i  defined according to the following condition: cij ﹋ 耳 i  iff cij = {bpij ﹋ b :
i bpij  = 1}. an interpretation i is a model of k if k is true in i according to the standard propositional semantics. the set of all models of k is denoted models k . for each instance e  百 e  denotes the set of all constraints bij in b rejecting e. for each example e in the training set ef  the conacq algorithm iteratively adds to k a set of clauses so that for any i ﹋ models k   the network 耳 i  correctly classifies all already processed examples plus e. when an example e is positive  unit clauses { bij} are added to k for all bij ﹋ 百 e . when an example e is negative  the clause is added to k. the resulting theory k encodes all candidate networks for the constraint acquisition problem. that is  vb ef  = {耳 m  | m ﹋ models k }.
example 1  conacq's clausal representation  we wish to acquire a constraint network involving 1 variables  x1 ... x1  with domains d x1  = ... = d x1  =
{1 1}. we use a complete and uniform bias  with l = as a library. that is  for all 1 ≒ i   j ≒ 1  b con and ≡ij. assume that the network we wish to table 1: an example of the clausal representation built by conacq  where each example e i =  x1 x1 x1 x1 .
f
acquire contains only one constraint  namely; there is no constraint between any other pair of variables. for each example e  first column   table 1 shows the clausal encoding constructed by conacq after e is processed  using the set 百 e  of constraints in the bias b that can reject e. 
﹛the learning capability of conacq can be improved by exploiting domain-specific knowledge  bessiere et al.  1 . in constraint programming  constraints are often interdependent  e.g. two constraints such as ≡1 and ≡1 impose a restriction on the relation of any constraint defined on the scope  x1 x1 . this is a crucial difference with conjunctive concepts where atomic variables are pairwise independent. because of such interdependency some constraints in a network can be redundant. cij is redundant in a network c if the constraint network obtained by deleting cij from c has the same solutions as c. the constraint ≡1 is redundant each time ≡1 and ≡1 are present.
﹛redundancy must be carefully handled if we want to have a more accurate idea of which parts of the target network are not precisely learned. one of the methods to handle redundancy proposed in  bessiere et al.  1   was to add redundancy rules to k based on the library of constraints used to build the bias b. for instance  if the library contains the constraint type ≒  for which we know that  x y z  x ≒ y  ＿  y ≒ z  ↙  x ≒ z   then for any pair of constraints ≒ij ≒jk in b  we add the horn clause ≒ij ＿ ≒jk↙≒ik in k. this form of background knowledge can help the learner in the acquisition process.
1	the interactive acquisition problem
in reality  there is a cost associated with classifying instances to form a training set  usually because it requires an answer from a human user  and  therefore  we should seek to minimise the size of training set required to acquire our target constraint network. the target network is the constraint network ct expressing the problemthe user has in mind. that is  given a vocabulary x d  ct is the constraint network such that an instance on x is a positive example if and only if it is a solution of ct.
﹛during the learning process the acquisition system has knowledge that can help characterise what next training example would be ideal from the acquisition system's point of view. thus  the acquisition system can carefully select 'good' training examples  which we will discuss in section 1 in more depth   that is  instances which  depending on how the user classifies them  can help reduce the expected size of the version space as much as possible. we define a query and the classification assigned to it by the user as follows.
definition 1  queries and query classification  a query q is an instance on x that is built by the learner. the user classifies a query q using a function f such that f q  = 1 if q is a solution of ct and f q  = 1 otherwise.
﹛angluin  angluin  1  defines several classes of queries  among which the membership query is exactly the kind used here. the user is presented with an unlabelled instance  and is asked to classify it. we can now formally define the interactive constraint acquisition problem.
definition 1  interactive constraint acquisition problem  given a constraint bias b and an unknown user classification function f  the interactive constraint acquisition problem is to find a converging sequence q = q1 ... qm of queries  that is  a sequence such that: qi+1 is a query relative to b and vb eif  where ei = {q1 ... qi}  and |vb emf  | = 1.
﹛note that the sequence of queries is built incrementally  that is  each query qi+1 is built according to the classification of q1 ... qi. in practice  minimising the length of q is impossible because we do not know in advance the answers from the user. however  in the remainder of the paper we propose techniques that are suitable for interactive learning.
1	query generation strategies
1	polynomial-time query generation
in practice  it can be the case that an example e from the training set does not bring any more information than that which has already been provided by the other examples that have been considered so far. if we allow for queries to be generated whose classification is already known based on the current representation of the version space  k  then we will ask the user to classify an excessive number of examples for no improvement in the quality of our representation of the version space of the target network. we exemplify this problem with a short example.
example 1  a redundant query  consider an acquisition problem over the three variables x1 x1 x1  with the domains d x1  = d x1  = d x1  = {1 1} using the same constraint library as in example 1. given the positive example
＿  ≡1 ＿  ≡1. asking the user to classify e1 =  is redundant  since all constraints
rejecting it are already forbidden by k. then any constraint network in the version space accepts e1. 
﹛we propose a simple  poly-time  technique that avoids proposing such redundant queries to the user. this irredundant queries technique seeks a classification only for an example e that cannot be classified  given the current representation k of the version space. an example e can be classified by vb ef  if it is either a solution in all networks in vb ef  or a non-solution in all networks in vb ef . e is a solution in all networks in vb ef  iff the subset 百 e  k  of 百 e   obtained by removing from 百 e  all constraints that appear as negated literals in k  is empty. alternatively  e is a nonsolution in all networks in vb ef   if 百 e  k  is a superset of an existing clause of k.
example 1  an irredundant query  consider again example 1 in which the positive example has been considered. the query is irredundant.
this can be seen by considering the literals that would be added to k by this query. if the query is classified as positive  the clauses  will be added to
k  otherwise the clause  will be added.
since we know from example that both ≡1 and ≡1 must be set to false  the only extra literal this new example adds is either   indeed . regardless of the classification of e  something new is learned  so this is an irredundant query.	
1	towards optimal query generation
the technique presented in section 1 guarantees that each newly classified query e adds something new to k. however  different irredundant examples give us different gains in knowledge. in fact  the gain for a query q is directly related to the size k of 百 q  k  and its classification f q . if f q  = 1  k unary negative clauses will be added to k  then k literals will be fixed to 1. in terms of conacq  we do not have direct access to the size of the version space  unless we wish to perform very expensive computation through the clausal representation k. but assuming that the models of k are uniformly distributed  fixing k literals divides the number of models by 1k. if f q  = 1  a positive clause of size k is added to k  thus removing 1k models. we can distinguish between queries that can be regarded as optimistic  or as optimal-in-expectation.
﹛an optimistic query is one that gives us a large gain in knowledge when it is classified  in our favour   but which tells us very little when it is classified otherwise. more specifically  in conacq the larger the 百 q  k  of a query q  the more optimistic it is. when classified as positive  such a query allows us to set |百 q  k | literals to 1. if the query is classified as negative we just add a clause of size |百 q  k |. therefore  an optimistic query is maximally informative - sets all literals it introduces to 1 - if it is classified as positive  but is minimally informative if it is classified as negative.
﹛the optimal query strategy is one that involves proposing a query that will reduce the size of the version space in half regardless of how the user classifies it. we define a query as being optimal-in-expectation if we are guaranteed that one literal will be fixed to either a 1 or a 1 regardless of the classification provided by the user. formally  such a query will have a 百 q  k  of size 1  therefore  if it is classified as positive  we can set the literal in 百 q  k  to 1  otherwise it is set to a 1.
﹛we illustrate a sequence of queries that are sufficient for the version space of the problem presented as example 1 to converge using queries that are optimal-in-expectation.
example 1  optimal-in-expectation queries  we want to converge on the target network from example 1  i.e.  the only constraint in a network with four variables and the complete bias of constraints  . recall that having processed the set of examples  the
unique positive clause in k is
. all other atoms in k are fixed to 1 because of. in the following k refers to
table 1: optimal-in-expectation query generation strategy on example 1.
e百 e  k f e k   ≡1  ＿ ...   ≡1  ＿    ≒1  ＿ ... ＿    ≒1 . according to this notation  the clausal theory k built by conacq having processed e is k = k. table 1 shows a sequence of queries that are optimal-in-expectation on the version space obtained after the three first examples are processed. the goal is to reduce vb e  to contain a single hypothesis. the first column is a query e generated according to the optimal-in-expectation strategy. the second column gives the set 百 e  k  of constraints still possible in a network of the version space that could reject e. the third column is the classification of e by the user  and the fourth column is the update of k. the query e1 is such that is the only constraint still possible in the version space that can reject it. because it is classified as positive  we are sure cannot belong to a network in the version space. conacq adds  to k and the literal is removed from cl by unit propagation. the process repeats with e1  e1 and e1  decreasing the size of cl by one literal at a time  and thus reducing the version space by half. finally  e1 is the last example required to ensure that the version space converges on the target network 
which contains the single constraint.
﹛note that at the beginning of this example  the version space vb e  contained 1 possible constraint networks  and we could converge using o log1|vb e |  queries  which is an optimal worst-case  mitchell  1 . 
in example 1  we always found an example e with
|百 e  k | = 1  as the optimal-in-expectation strategy requires. however  redundancy can prevent us from being able to generate an example e with a given size for its 百 e  k . for instance  consider the acquisition problem  using a complete and uniform bias  withas a library  and with x1 = x1 = x1 as a target network. after processing an initial positive example  for instance   the possible constraints in the version space are ≒1 ≒1 ≒1 ≡1 ≡1  ≡1. hence  every further negative example e has either a 百 e  k  of size 1  if no variables equal  or a 百 e  k  of size 1  if two variables equal . therefore  no example with a 百 e  k  of size 1 can be generated. redundancy preventsus from generating such examples.
1	implementing our strategies
in section 1  we presented two strategies for generating queries: optimal-in-expectation and optimistic. these two strategies are characterised by the target number t of constraints still possible in the version space that reject the instances q they try to produce. however  it may be the case that  due to redundancy between constraints  there does not exist any network in the version space that has a solution s with |百 s  k | = t.  and it is useless to ask classification of an instance if it is not a solution of some network in the version space - see section 1 . we then must allow for some uncertainty in the number of constraints rejecting an instance. we implement the query generation problem as a two step process. first  algorithm 1 tries to find an interpretation i on b such that any solution s of 耳 i  is such that
  where  is the variation accepted
on the size of the 百 q  k  of the query q we want to generate. this algorithm takes another input parameter which is the set l of constraints in which 百 q  k  must be included. we will explain later that this is a way to monitor the 'direction' in which we want to improve our knowledge of the target network of the user. second  once i has been found  we take a solution of 耳 i  as a query. we first present the algorithm  then we will discuss its complexity and describe how we can use it to implement our strategies  by choosing the values t and  .

algorithm 1: query generation problem

input : b the bias  k the clausal theory  l a set of literals  t a target size and  the variation
output: an interpretation i
1 f ↘ k
1 foreach bij ﹋ b   {bij |   bij  ﹋ k} do
1 if b
else f
1 lower ↘ max |l|   t    1 
1 upper ↘ min |l|   t +  |l| 
1 f ↘ f ＿ atleast lower l  ＿ atmost upper l  then return a model of f
1 else return  inconsistency 

﹛algorithm 1 works as follows. it takes as input the target size t  the allowed variation  and the set l of literals on which to concentrate. the idea is to build a formula f for which every model i will satisfy the requirements listed above. f is initialised to k to guarantee that any model will correspond to a network in the version space  line 1 . for each literal bij not already negated in k  line 1   if bij does not belong to l  we add the clause  bij  to f to enforce the constraint bij to belong to the network 耳 i  for all models i of f  'then' instruction of line 1 . hence  any solution s of 耳 i  will be rejected either by a constraint in l or a constraint bij already negated in k  so no longer in the version space . thus  百 s  k    l. we now have to force the size of 百 s  k  to be in the right interval. if bij belongs to l  'else' instruction of line 1   we add the clause  to f to ensure
that either bij or its complementary constraint bij is in the resulting network. is required because  bij only expresses the absence of the constraint bij.  bij is not sufficient to enforce bij to be violated. we now just add two pseudo-boolean constraints that enforce the number of constraints from l violated by solutions of 耳 i  to be in the interval . this is done by forcing at most constraints and at least  constraints to be satisfied  lines 1 . the 'min' and 'max' ensure we avoid trivial cases  no constraint from l is violated  and to remain under the size of l. line 1 searches for a model of f and returns it. but remember that redundancy may prevent us from computing a query q with a given 百 q  k  size  section 1 . so  if  is too small  f can be unsatisfiable and an inconsistency is returned  line 1 .
﹛the following property tells us when the output of algorithm 1 is guaranteed to lead to a query.
property 1  satisfiability  given a bias b  a clausal theory k  and a model i of k. if k contains all existing redundancy rules over b  then 耳 i  has solutions.
﹛if not all redundancy rules belong to k  algorithm 1 can return i such that 耳 i  is inconsistent. in such a case  we extract a conflict set of constraints s from 耳 i  and add the clause to k to avoid repeatedly generating models i with this hidden inconsistency in.
﹛the next property tells us that generating a given type of query can be hard.
property 1 given a bias b  a theory k  a set l of constraints  a target size t and a variation   generating a query q such that: 百 q  k    l and-hard.
﹛the experimental section will show that despite its complexity  this problem is handled very efficiently by the technique presented in algorithm 1. the algorithm can be used to check if there exists a query rejected by a set of constraints from the version space of sizeincluded in a given set l. the optimal-in-expectation strategy requires t = 1 and optimistic requires a larger t. in the following  we chose to be  half-way  optimistic and to fix t to |l|/1. there still remains the issue of which set l to use and which values of  to try.  is always initialised to 1. concerning l  we take the smallest non-unary positive clause of k. a positive clause represents the set of constraints that reject a negative example already processed by conacq. so  we are sure that at least one of the constraints in such a set l rejects an instance. choosing the smallest one increases the chances to quickly converge on a unary clause. if k does not contain any such non-unary clauses we take the set containing all non-fixed literals in k.
﹛since algorithm 1 can return an inconsistency when called for a query  we have to find another set of input parameters on which to call the algorithm. t is fixed by the strategy  so we can change l or . if there are several non-unary clauses in k  we set l to the next positive clause in k  ordered by size .

1
﹛﹛not all libraries of constraints contain the complement of each constraint. however  the complements may be expressed by a conjunction of other constraints. for instance  in library does not exist but it can be expressed by . if no conjunction can express the complement of a constraint  we can post an approximation of the negation  or nothing . we just lose the guarantee on the number of constraints in l that will reject the generated query.
if we have tried all the clauses without success  we have to increase . we have two options. the first one  called closest  will look for a query generated with a set l instantiated to the clause that permits the smallest . the second one  called approximate  increases  by fixed steps. it first tries to find a set l where a query exists with. if not found  it looks  repeatedly  with 1 ﹞ |l|  1 ﹞ |l| and then |l|.
﹛we thus have four policies to generate queries: optimistic and optimal-in-expectation combined with closest and approximate: optimistic means t = l/1 whereas optimal-inexpectation means t = 1; closest finds the smallest  whereas approximate increases  by steps of 1%.
1	experimental results
we implemented conacq using sat1 and choco1. in our implementation we exploit redundancy to the largest extent possible  using both redundancy rules and backbone detection  bessiere et al.  1 .
 problem classes. we used a mix of binary and non-binary problem classes in our experiments. we studied random binary problems  with and without structure  as well as acquiring a csp defining the rules of the logic puzzle sudoku. conacq used a learning bias defined as the set of all edges in each problem using the library. the random binary problems comprised 1 variables  with a uniform domain of size 1. we generated target constraint networks by randomly selecting a specified number of constraints from   retaining only those that were soluble. we also considered instances in which we forced some constraint patterns in the constraint graph to assess the effect of structure  bessiere et al.  1 . we did this by selecting the same constraint relation to form a path in the target network. finally  we used a 1 ℅ 1 sudoku as the target network. the acquisition problem in this case was to learn the rules of sudoku from  counter examples of grid configurations.
﹛as an example of a non-binary problem  we considered the schur's lemma  which is problem 1 from the csplib1.
in this case  conacq used the library of ternary constraints {alldiff  allequal  notalldiff  notallequal}.
results. in table 1 we report averaged results for 1 experiments of each query generation approach on each of the problem classes we studied. in each case the initial training set contained a single positive example. in the table the first column contains a description of the target networks in terms of number of variables and constraints. we report results for each of the query generation approaches we studied. random is a baseline approach  generating queries entirely at random  which may produce queries that are redundant with respect to each other. the irredundant approach generates queries at random  but only uses those that can provide new information to refine the version space. finally  optimistic and optimalin-expectation refer to approaches described in section 1 and

1
 available from: http://www.sat1j.org. 1
 available from: http://choco.sourceforge.net. 1
available from: http://www.csplib.org.
table 1: comparison of the various queries generation approaches on different classes of problems. time is measured in milliseconds on a pentium iv 1 ghz processor. we highlight the smallest number of queries for each problem class in bold.
randomirredundantoptimisticoptimal-in-expectationtarget networkapproximateclosestapproximateclosest|x|	|c|#qtime	#q	time#q	time#q	time#q	time#q	timerandom binary problem111111111111111  1111111  1  111111  1  11111pattern binary problem1  1  11111sudoku 1 ℅11  1  11111schur's lemma111	1111111	1111for both we consider the approximate and the closest variants. each column is divided in two parts. the left part is the number of queries needed to converge on the target network; a limit was set 1 queries. the right part measures the average time needed to compute a query.
﹛with the exception of very sparse random problems and schur's lemma  generating queries with random is never able to converge on the target hypothesis  even with a large number of queries. the irredundant approach is strictly better than random and successfully converged in a number of cases. however  when the density of the target network increases  irredundant begins to struggle to converge.
﹛optimistic and optimal-in-expectation are more accurate  since they always enable us to converge  regardless of the target network used. their closest variants require an average computation time between 1 and 1 times longer than the approximate ones  as to be expected. however  the closest strategies have the advantage of being able to convergeon the target network by asking up to 1% fewer queries than the approximate strategies. optimistic is the best approach on very sparse networks  but as the number of constraints in the target network grows  optimal-in-expectation becomes the best strategy  since it requires both fewer queries to converge and less computationtime. the numberof queries for optimal-inexpectation decreases when density increases because redundancy rules apply more frequently  deriving more constraints. despite this  optimistic performance decays when density increases because the probability that a query is classified negative  unlucky case  grows with density.
1	related work
recently  researchers have become interested in techniques that can be used to acquire constraint networks in situations where a precise statement of the constraints of the problem is not available  freuder and wallace  1; rossi and sperduti  1 . the use of version space learning  mitchell  1  as a basis for constraint acquisition has received most attention from the constraints community  o'connell et al.  1   but the problem of query generation for acquiring constraint networks has not been studied.
1	conclusion
in this paper we have tackled the question of how a constraint acquisition system  based on conacq  can help improve the interactive acquisition process by seeking fewer  but better selected  examples to be proposed as queries for classification by a user. we have provided a theoretical and empirical evaluation of query generation strategies for interactive constraint acquisition  with very positive results.
acknowledgments
the authors would like to thank frederic koriche for very useful discussions and comments. this work has received support from science foundation ireland under grant 1/pi.1/c1.
references
 angluin  1  d. angluin. queries revisited. theoretical computer science  1-1  1.
 bessiere et al.  1  c. bessiere  r. coletta  f. koriche  and b. o'sullivan. acquiring constraint networks using a sat-based version space algorithm. in ecml  pages 1  1.
 freuder and wallace  1  e.c. freuder and r.j. wallace. suggestion strategies for constraint-based matchmaker agents. in proceedings of cp-1  pages 1  1.
 mitchell  1  t. mitchell. generalization as search. ai journal  1 :1  1.
 o'connell et al.  1  s. o'connell  b. o'sullivan  and e.c. freuder. a study of query generation strategies for interactive constraint acquisition. in applications and science in soft computing  pages 1. 1.
 rossi and sperduti  1  f. rossi and a. sperduti. acquiring both constraint and solution p