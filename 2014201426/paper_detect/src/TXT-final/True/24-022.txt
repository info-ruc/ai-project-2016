 
we study what kind of data may ease the computational complexity of learning of horn clause theories  in gold's paradigm  and boolean functions  in pac-learning paradigm . we give several definitions of good data  basic and generative representative sets   and develop data-driven algorithms that learn faster from good examples  and degenerate to learn in the limit from the  worst  possible examples. we show that horn clause theories  kterm d n f and general d n f boolean functions are polynomially learnable from generative representative presentations. 
1 introduction 
in any inductive learning model  how data of the target theory are supplied to the learning programs is a crucial assumption. identification in the limit  gold  1  assumes that the series of examples is an admissible enumeration of all  positive and/or negative  examples of the target concept  and requires the learning algorithm to produce a correct hypothesis in some finite time. however  the computational time and the number of examples needed for convergence depend on the example series  and can not be specified a priori. although there is an enumeration algorithm that identifies any heasy  blum and b l u m   1  model of first order theories in the limit  such an algorithm is extremely inefficient in practice. shapiro in his seminal work  shapiro  1  presented an incremental method mis which searches the hypothesis space from general to specific. however  mis has two major shortcomings. first  the refinement of the refuted clauses may introduce a large number of faulty clauses  which need to be removed using a large number of negative examples. second  the algorithm is still very inefficient  exponential . 
   approximate identification  or pac-learning  probably approximately correct learning   valiant  1  blumer et a/.  1  assumes that the examples of the target concept are drawn randomly according to some fixed distribution for learning and for testing the conjecture. it requires the learner within feasible amount of time  quickly   to produce  with a high probability  confidently   a hypothesis with a small error  accurately . 
however  few things are shown pac-learnable from random examples: many classes of boolean concepts which seem simple to human learners  for example  k-termdnf  are proved not pac-learnable  by k-term-dnf  unless rp = n p. it is still open if general d n f is pac-learnable. 
   human learning  on the other hand  is efficient  interactive  and data-dependent. the study of the interaction between learners and teachers may reveal the essence of efficient human learning. learning equipped with teachers answering various types of queries  angluin  1  is one of such studies. we believe that the current learning model does not reflects the quality of the examples in the role of efficient learning. here we present some results on what kind of data may ease the computational complexity of learning1. 
   we start with an attempt to improve the efficiency of mis. we first discover a heuristic very useful to improve the efficiency of shapiro's mis  while preserving the property of identification in the limit. further study of the heuristic leads us to discover a constructive  or data-driven  algorithm s i m 1 for model inference. sim 
opens a way for lis to study what kinds of data are necessary for successful learning. we give precise definition of  good  examples  and design constructive algorithms that learn faster from the good data  learn in longer time when examples become worse  and degenerate to learn in the limit from the  worst  possible data presentations. however  we believe sim is not a polynomial algorithm. this leads us to study the problem in a more rigorous model of pac-learning. we provide a stronger definition of  good  examples  generative representative sets   and show that k-term d n f and general d n f are pac-learning from generative representative sets. then we return to model inference problem  and give a similar stronger definition of  good  examples for polynomial inference of logic programs  provided enough negative examples are supplied . 
   *lt may be argued that  good** examples may make learning arbitrarily fast. but as we will see  the nature of good data is implicit. that is  what is important is that a initial series of examples contains a set of good examples  to be defined . the order of examples in the series is irrelevant  and other examples in the series can be arbitrary. this eliminates the possibility of coding the target concept by examples. 
1
sim is the inverse process of shapiro's mis. 
	ling 	1 

many data driven algorithms  banerji  1  ling  
1  muggleton and buntine  1  ishizaka  1  have been studied. however  most of them are heuristic. they learn faster from some good data  but fail to learn  in the limit  from  bad  data; little is done to characterize the  good  data for efficient learning. some theoretical studies on learning from  good  examples have been explored. for example  angluin  angluin  1  defines  representatives  for live states as good strings for identifying a dfa polynomially. rivest and sloan  rivest and sloan  1  show how to learn an arbitrary concept by first learning its relevant subconcepts. freivalds et al  freivalds et a/.  1  study good examples in recursive theoretic inductive inference. 
1 shapiro's model inference system 
shapiro's model inference system  mis   shapiro  1l  starts with the most general conjecture  a theory with an empty clause . when the conjecture t proves a negative example  t is too strong  the backtracing algorithm is invoked to remove faulty clauses from t. removing a clause may overly specilize t  in which case the refinements of faulty clauses are added into t. however  the possible refinements of any clause are infinite. so which refinements of which clauses should be added into t  as shapiro pointed out   shapiro  1   mis identifies the theory in the limit  ...as long as it  the refinement  is 'exhaustive' in some natural way . therefore  unless using  good heuristics  unspecified in the paper  for ordering the addition of refinements  an indefinite number of faulty clauses can be inserted into the conjecture  which need to be removed using a large number of negative examples. 
for any faulty clause a removed from the conjecture 
t it suffices to add a set of most general specialization of a; in this case  the set of immediate refinements  a finite number  of  we define rnys  - where p is a refinement operator complete for the theories  shapiro  1 . however  a good rule of thumb  heuristics  is to add those clauses in mgs  that can be used to prove more  or most: a greedy algorithm  positive examples! such a heuristic not only provides a reasonable guideline on theory revision  but also guarantees to find a solution if it exists. the following algorithm is the improved version of mis with the heuristic. 

repeat examine the next example while t proves a negative example do backtracing  remove a faulty  
while t fails to prove a positive example b do find a set such that  
until t is neither too strong nor too weak 
output t loop  
now the question is: what examples are necessary for 
1 	learning and knowledge acquisition convergence  and what examples are crucial for the con-
jecture to be updated appropriately  it turned out that the top-down algorithm mis  driven by negative examples  is not a good place to study this problem: the overgeneralized clauses are not sensitive to the positive examples! since our data-driven algorithm sim constructs hypotheses from specific to general  we may characterize more acutely the properties of  good examples . 
1 intuitions of good data 
the philosophy of what good examples are necessary for successful learning is quite simple. even though the set of data is infinite  but recursively enumerable   there must exist some finite characterizations  such as logic programs  of the data1. if is these finite characterizations that are sought in inductive learning. to be successful in learning  the learner must receive data that go through  use or exercise  every  live  component of one of the finite characterizations1. for example  data that go through all  live  transactions in a dfa  see  angluin  1    all  live  production rules in a grammar for a language  or all  live  horn clauses in a logic program1. clearly such a set of data provides a minimum amount of information about the theory to be identified  and should be supplied in the early stage of learning. we call this set a basic representative set. later we will provide a stronger definition of  good  examples  called generative representative set  which go through every component of a finite characterization several times  and which can be used to construct the component. 
   we give appropriate definitions of  good  examples and study the learnability results using good examples. the good examples supplied in the early stage of learning appear in the example series only implicitly. we assume that the first p{n  1 positive data should contain good examples  to be defined . in the gold's model  the learning algorithm is allowed to ask membership queries; while in pac-leaming  more random examples may be drawn according to the fixed distribution. in both models  the total sample size and computational complexity for successful learning is measured by n and the size of the target theory  if the total sample size and computational complexity are polynomial  we say that the class of theories is polynomially learnable from good examples. if a class of boolean functions not pac-learnable becomes pac-learnable with good examples  it implies that drawing a polynomial sample randomly will not have a high probability of containing good examples. 
　obviously  p measures the quality of the data  or the teacher : as p increases  the learning algorithm gradually degrades to be identification  in the limit . thus  
   1 in model inference of horn clause theories  if such a finite characterization does not exist  theoretical terms may be needed. this is outside the scope of the paper. 
   1  we assume that the components are of disjunctive nature in this paper. 1
　　something is  live  means it has to be used in deriving some data. 
1
　　p is a polynomial function and n is the maximum of the problem sizes. 

our learnability model unifies efficient learning and identification in the limit based on the quality of the data presentation. 
1 basic representative sets 
given a language l  a logic program lp for l is any finite set of horn clauses of l. the model of a logic program is its least herbrand model  i.e.  the set of all ground atoms of l derivable from lp. a complete data presentation of a model is an admissible enumeration of all positive and negative examples from the herbrand base. the class of models of logic programs of l is the set of least herbrand models of all logic programs of l. the model inference problem is: given language l and a data presentation of a unknown model m from the class of models of logic programs of l  find a logic program whose least herbrand model is m. 
d e f i n i t i o n s . a basic representative set of a model m is a basic representative set of any logic program lpm with least herbrand model m. a basic representative* set of a logic program lp is a set s of ground atoms obtained by taking  for each clause p = p :- q   . . .  qn of lp  all ground atoms in a true instantiation of p  all atoms are true in the model . 
   basic representative sets for a model can be very small; the size of a basic representative set is no more than the number of occurrences of atoms used in the corresponding lp. for example  a model about list reverse has a logic program: 

a basic representative set for the model may contain: 

   notice that for any model to be inferred  there may exist more than one logic program whose least model is the target model. for example  there are several sorting programs to sort a list of integers. depending on which logic program whose basic representative set is given  that logic program will be inferred. 
d e f i n i t i o n . a basic representative presentation of a 
model m is an admissible data presentation whose initial portion  size p n  for a polynomial function p  contains a basic representative set of m. 
1 abstraction operators and sim 
our data-driven algorithm sim is the inverse process of mis. sim starts with the most specific conjecture and makes it more general by adding generalizations of clauses in the current conjecture obtained by applying abstraction operators  guided by good examples. on the other hand  we prove completeness for the abstraction operators  showing that sim identifies h-easy models in the limit. 
   let size be a total recursive function from clauses to positive integers that gives a measurement of the complexity of clauses. the only requirement on size is that for any integer n  the set { p   p is a horn clause with size   is finite. 
definition. an abstraction operator a is a mapping from clauses to sets of clauses  such that 
where 
　any such operator a induces an operator  also denoted a  which takes sets of clauses to sets of clauses; for a set s of clauses  
plies 
one important difference between refinement and abstraction operators is that for any abstraction operator a and any finite set s of clauses  there is a finite  least fixed point   the closure of s under a. 
lemma 1 for anv finite set s of horn clauses  there is an integer n with  
　for convenience we will often use a  finite  set a of abstraction operators. the closure of s under a will be denoted .1 . 
1 	completeness and convergence 
definition. a set a of abstraction operators is complete if for every logic program lp and every basic representative set s of lp  a s  contains every clause of lp  up to possible renaming of variables in clauses . 
　since abstraction operators are monotonic  we observe that if a is complete  and s contains a basic representative set of lp  then a s  contains every clause of lp. 
　a simple version of the constructive algorithm sim works as follows: from any finite set. of positive data 1  a s  can be calculated. if a s  cannot prove some positive data  they are added to s and a s  is recalculated. eventually the basic representative set of the model will be contained in s  and with a complete set of abstraction operators  a s  will contain all clauses in the logic program of the model. a s  may contain faulty clauses  which are removed by shapiro's contradictory backtracing algorithm when a negative example is proved to be true. unlike shapiro's mis  the removal of a faulty clause does not introduce any more clauses into the conjecture. clearly  this simple algorithm identifies any h-easy models in the limit. 
theorem 1 if a set of abstraction operators a for the class of theories is complete  then the constructive algorithm above identifies any h-easy model1 of that class in the limit given an enumeration of positive and negative examples of the model. 
　　1 most proofs of the paper are omitted and can be supplied from  ling and dawes  1  ling  1l . 
1
　　the attempted derivation of an atom from t may not halt. a total recursive function h is used to bound the resources in proving. for more details see  shapiro  1  blum an d blum  1 . 
	ling 	1 


figure 1: 	t h e sim 	a l g o r i t h m 
　however  even if s is small  a s  can be huge  exponential . we use the same heuristic for mis to improve the efficiency of the algorithm while retaining the prop-
erty of identification in the limit. 
1 	using good data 
in general  from the current conjecture set t  only a small number of clauses in a t  will be useful in further generalizations  via abstraction operators . these clauses may be identified by some  simple  positive data that can be proved using these clauses. if f+ in the following algorithm contains positive examples ranging from  simple  to  complex  with respect to applications of abstraction operators  sim will be much more efficient and need much fewer negative data than shapiro's mis. 
　in practice what good examples should be supplied to sim  first of all  a small set of basic representative set should be supplied. then  simple examples are those positive examples similar to the examples in the representative set  and more complex examples are those less similar ones. for example  are close to  
rev 
　also we notice that for a given set t containing a representative set  a t  will be calculated. the size of a t  may be exponentially larger than size a  for a t. thus  examples in s of sim must be small  i.e.  size a  must be small . this verifies our intuition that in learning list reverse for instance  the examples that reverse very long lists are not good examples  while in pac-learning  examples drawn randomly may have an indefinite size . 
   figure 1 gives an improved version of sim. the conjecture t starts with the set initial s   which is the most specific conjecture from s in the representational language of the theory  see section 1 for details . the algorithm identifies any a-easy models in the limit  and learns faster given a good set of data. 
1 	learning and knowledge acquisition 
1 various abstraction operators 
we design abstraction operators for various sub-classes of clauses  similar to shapiro's refinement operators for sub-classes of clauses  shapiro  1 . thus  the algorithm is more efficient if the class of models to be inferred is known. 
1 	abstraction operators for a t o m s 
we first discuss the simplest class of models  which have logic programs containing only unit clauses. here initia/ s  in the algorithm sim is just the set of all positive examples in 1. the set of abstraction operators aat for the atom p contains two operators a1 and a1 defined as follows: 
  a1 p  is the set of atoms obtained from p by replacing some or all occurrences of a constant in p by a variable not in p. 
  a1 p  is the set obtained by replacing some or all occurrences of a compound ground term in p by a variable not in p. 
theorem 1  is a set of abstraction operators completc for atomic horn clauses. 
1 	abstraction operators for h o r n clauses 
skipping other subclasses  we now study abstraction operators for general horn clause theories. the most specific 	conjec-
ture  initial s   contains n most specific clauses  each in the form of 	where   are ground atoms in s. the set acl of 
abstraction operators for general horn clauses contains three operators a1 a1  and a1. 
  a1 and a1 are the same as in aat except that they now apply to occurrences of a constant or term in a whole clause rather than in one atom. 
  if p is any horn clause with at least one atom in its body  a1 p  is the set of clauses formed by removing any one atom from the body of p. 
theorem 1  is a set of abstraction operators complete for general horn clauses. 
if the class of models is a sub-class of models of general 
horn clauses  then more efficient abstraction operators are possible. for example  we might consider only horn clauses with no variable occurring only once in a clause  or with the length of body bounded by a small constant. it is easy to modify the abstraction operators for these and other syntactically restricted classes. 
1 implementation of sim 
we have developed sim in quintus prolog. sim learns faster with a small set of good examples containing a representative set. however  in learning various logic programs  sim does not have a consistent good performance. the major reason is that there may not exist any new positive examples  besides those in s of sim  that can be proved in a 1  but not in t  see algorithm sim in figure 1 . in this case abstraction operators have to be applied to the current conjecture  t = a t  in 

the algorithm . this makes the conjecture to grow too fast to be manageable. in general  we believe sim is not a polynomial algorithm. in fact  there is no polynomial algorithm that identifies any horn clause theory from basic representative presentations if the conjecture must contain the same number of clauses as the target theory  see next section . 
1 pac-learning boolean functions from good examples 
definition. a class of concepts is pac-learnable from good examples if for any / in the class  there exists a learning algorithm a such that given any set s of positive examples of / that contains the good examples  and where p is a polynomial function and 
n is the number of variables  a pac-identifies /. notice that a may draw more random examples from the fixed distribution. 
　similar to the good examples in the model inference discussed in the previous sections  we may define good examples in pac-learning as a set of examples that go through every term of the dnf function once  basic representative set . however  a little more thoughts reveal that such definition is insufficient for polynomial learning of k-term dnf. assume that such a polynomial algorithm  denoted as a  existed  then k-term dnf would be pac-learnable  by k-term dnf  without basic representative sets as follows: first a sample of a polynomial size is drawn such that with a high probability each term in the target function is sampled. choose at most a: data from the sample as basic representative sets in a. apply a on all sets  since there is a polynomial number of the sets  the sample size a would draw would be polynomially larger . clearly this polynomial algorithm pacidentifies any k-term dnf. this is contradictory to kteym dnf is not pac-learnable by k-term dnf  unless rp = np . essentially it is np hard to find a k-term 
dnf that is consistent with a set of examples even when a set of basic representative set is provided. since any k-term dnf can be transferred to a propositional horn clause theory with k clauses  therefore  even with a basic representative set  there is no polynomial algorithm that identifies the theory if the conjecture must contain the same number of clauses as in the target theory. it is unknown to us if basic representative set is sufficient for polynomial inference of general dnf formulas  by producing a dnf with a size at most polynomially larger . 
1 polynomial learning of d n f from generative examples 
we provide a stronger definition of good examples where k-term dnf and genera  dnf are polynomially learnable. 
　a boolean function in dnf consists of terms. we first define generative sets for a term. let a be an instance 
let   be a term  and t a  be the truth value of t with a as the assignment of the variables in t. 
definitions. for a set s of instances  lgg s   least general generalization  is a term t such that for all t a  is true; and for any other term t'  if for all  
t' a  is true  then s is a generative set of t if for all and igg s  = t. 
　igg s  is easy to calculate: check iih bits of all instances in s. if they are all 1  then xi is in t; if all 1  then xi- is in t; otherwise neither x - nor xi is in t. in another word  if t does not contain a variable x nor x  i.e. x is an irrelevant variable in t   there are at least two instances in a generative set of t whose bits on x are different. intuitively the generative set of t contains positive examples showing contrast information. notice however  these examples may not constitute a  near-miss   winston  1 . 
　for any term t  there exists a smallest generative set s of t with is a constant throughout the paper. this is the set of positive examples with a constant size showing all contrast information of a term. 
definition. s is a generative representative set for a 
term t if s is a generative set oft with  a generative representative set for a boolean formula f in dnf is the union of generative representative sets for all terms in an equivalent dnf formula with a smaller or equal size. a generative representative presentation r for / is a set of positive examples that contains a generative representative set of / with  where p is a polynomial function and n is the number of variables. 
   using generative representative presentations  we show that many classes of dnf formulas are paclearnable. basically  from a generative representative presentation  we can form all possible terms by applying igg on at most r examples  and form all possible boolean formulas in the hypothesis space  i.e.  form all k-term dnf formulas in learning k-term dnf and form one disjunction of all terms in learning general dnf . draw enough random examples according to a simple and important theorem due to  valiant  1  blumer et a/.  1 : if c contains a finite number of boolean functions  then any polynomial algorithm that requests sample size at ieast  and outputs any consistent function in c pac-identifies c. for proofs of the following theorems  see  ling  1l . 
theorem 1 k-term-dnf is pac-learnable  by kterm dnf  from generative representative presentations 
with a sample size and time complexity  
theorem 1 any 	dnf 	f 	is 	pac-learnable 	 by 
dnf with at most p n  size  terms  from generative representative presentations with sample complexity 
　is the definition of generative representative set too strong  in some sense it is not. in the worst cases it is necessary to enumerate all terms formed by applying igg on subsets of data from initial portion of p n  data. for  to be polynomial   can only be a constant. of course  generative representative set is not a necessary condition for polynomial learning of dnf formulas: there are other spacial cases where dnf is pac-learnable. 
	ling 	1 
1 model inference from generative examples 
the definitions of generative examples for the model inference can be given in a similar way as for pac-learning d n f . the least general generalization  igg  of a set of clauses is taken from protkin's work  plotkin  1 . we define a set of ground atoms as a generative representative set for a logic program lp if it is the union of 
generative representative sets for all clauses in lp. a set s of ground atoms is a generative representative set for a clause a :- b 1  ... b n if there are at most r true instances of the clause in the form of . 
	such that all 	are in s  
and 	contains the clause a :-
b1      bn- the resulting clauses can be very long. some techniques of logical reduction of clauses  buntine  1  muggleton and feng  1  and syntactic restriction 
 such as ij-determination  muggleton and feng  1   may be applied. clearly the total number of possible clauses from the igg of at most r most specific clauses is polynomial  so is the maximum number of faulty clauses in the conjecture. thus the model inference problem can be solved in polynomial time from a generative representative presentation if a proper set  with a polynomial size  of negative examples is provided. this is  as we believe  the condition for other constructive algorithms based on least general generalization to learn in polynomial time. 
1 conclusions and further research 
a new learning model that learns faster if the initial portion of the examples contains a set of good data is developed. the model captures the quality of the data  or teacher  in the role of the speed of the learning. thus  the model provides a new approach to study human learning  which is efficient  interactive and data-driven. 
   currently we are studying the use of  negative representative sets   and the appropriate definitions of good examples for theories with non-disjunctive components  such as m-formulas . we hope to investigate language learning from good sentences in the near future. 
a c k n o w l e d g e m e n t 
the author gratefully thanks referees for their thoughtful and valuable comments  and mike dawes  bob webber for helpful discussions on various aspects of the topics. the author also acknowledges the support from nserc operating grant ogp1  the internal nserc grants  and the new faculty start-up grant. 
