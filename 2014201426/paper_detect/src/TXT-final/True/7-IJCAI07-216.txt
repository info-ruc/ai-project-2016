 
to test large scale socially embedded systems  this paper proposes a multiagent-based participatory design that consists of two steps; 1  participatory simulation  where scenario-guided agents and human-controlled avatars coexist in a shared virtual space and jointly perform simulations  and the extension of the participatory simulation into the 1  augmented experiment  where an experiment is performed in real space by human subjects enhanced by a large scale multiagent simulation. the augmented experiment  proposed in this paper  consist of 1  various sensors to collect the real world activities of human subjects and project them into the virtual space  1  multiagent simulations to simulate human activities in the virtual space  and 1  communication channels to inform simulation status to human subjects in the real space. to create agent and interaction models incrementally from the participatory design process  we propose the participatory design loop that uses deductive machine learning technologies. indoor and outdoor augmented experiments have been actually conducted in the city of kyoto. both experiments were intended to test new disaster evacuation systems based on mobile phones.  
1 introduction 
ubiquitous/pervasive computing systems in the public space often interact with anonymous users. we propose to apply multiagent technologies to the design of those systems that are to be embedded in society.  
 participatory simulations have been already studied intensively for modeling human societies  drogoul et al.  1  guyot et al.  1 . for designing socially embedded systems  however  simulations in virtual space cannot reproduce the reality of the actual application environment. the goal of this paper is to introduce real world experiments to the process of participatory design  so as to bridge the gap between participatory simulations and services in operation. we propose multiagent-based participatory design methodologies  participatory design hereafter  to test socially embedded systems in the following steps. 
1. describe interactions between human users and socially embedded systems so as to define the expected user behaviors with socially embedded systems  interaction model hereafter .  
1. perform a multiagent-based simulation by modeling users under the given interaction scenarios  agent model hereafter . the simulation takes place in virtual space and involves a large number of simulated users. results of the simulation can predict how the entire system would work in society.  
1. replace some of the simulated users by human-controlled avatars to perform a multiagent-based participatory simulation  participatory simulation hereafter . the simulation is performed in virtual space  and the avatars are controlled by human subjects sitting in front of their personal computers.  
1. perform experiments in real space to try out the entire system with human subjects. since the number of subjects is often limited  the experiment should be augmented by a large number of simulated users in virtual space. we called this the multiagent-based augmented experiment  augmented experiment hereafter . 
 to learn from the participatory design process  this paper separates agent models from interaction models: the former covers the beliefs  desires  intentions  and emotions of human users  and the latter covers protocols  methods  rules  or laws that guide users when interacting with the socially embedded systems. we use extended finite state automata for describing interaction models  while production rules are used to describe agent models for approximating users. we view interaction models as behavioral guidelines of human users playing with socially embedded systems; users keep autonomy within the given guidelines. the question is whether or not users will follow the guidelines in an actual service environment. in other words  the effectiveness of interaction models depends on the agent models  which include user personalities such as deliberative and reactive.  in this paper  we call the descriptions of interaction models scenarios. we use scenario description language q  which can simultaneously interpret scenarios for a large number of agents  ishida  1b . q has been already connected to freewalk  nakanishi et al.  1b  for virtual city experi-
ments  ishida  1a   cormas for a natural resource simulation  bousquet et al.  1   and caribbean  yamamoto et al.  1  for massively multiagent simulations  ishida et al.  1 . q  freewalk  and caribbean have been used to conduct indoor and outdoor disaster evacuation experiments. those real-scale experiments confirm the feasibility and usefulness of augmented experiments. 
1 participatory simulation 
in our participatory design methodology  we first conduct multiagent-based simulations. figure 1 a  illustrates how a multiagent-based simulation is realized. the scenario processor interprets interaction models and requests agents in virtual space to perform sensing and acting functions. note that  since agents are autonomous and have their own agent models  though agents receive instruction according to the scenarios  there is no guarantee that they will behave as requested.  
 we can easily extend multiagent-based simulations to yield participatory simulations by replacing some of the scenario-guided agents with human-controlled avatars. figure 1 b  illustrates how human subjects and agents can cooperatively perform a simulation. just as with video games  human subjects can join the simulation by controlling avatars via joy sticks  mice  or other input devices. to analyze simulation results  we monitor the entire process of the simulation by visualizing the virtual space. in addition to video taping the virtual space  we record how the human subjects control their avatars. recording human behavior is useful for analyzing the simulation results and for improving the agent and interaction models. 
 

	a  multiagent-based	b  multiagent-based
	simulation	participatory simulation	 
figure 1. participatory simulation 
 
 in summary  a participatory simulation consists of 1  agents for modeling users  1  avatars to represent human subjects  1  scenarios for modeling interactions  1  human subjects to control avatars  1  virtual space to represent real space  and 1  a monitor to visualize simulations ongoing in the virtual space. 
 we conducted a multiagent-based simulation in the disaster evacuation domain. to establish the process of refining agent models and interaction models  we simulated the precisely controlled experiments conducted by sugiman  sugiman et al.  1 . he developed a simple environment with human subjects to determine the effectiveness of two evacuation methods  i.e.  interaction models : the follow-direction method and the follow-me method. in the former  the leader shouts out evacuation instructions and eventually moves toward the exit. in the latter  the leader tells a few of the nearest evacuees to follow him and actually proceeds to the exit without verbalizing the direction of the exit.  
 a participatory simulation was performed in which we replaced twelve out of twenty scenario-guided agents by human-controlled avatars. the virtual space was designed so that the human subjects could not distinguish agents from avatars. after the simulation  we applied deductive machine learning technology with domain knowledge to refine the agent models from the logs of the participatory simulation  and successfully modified the agent models and original evacuation scenarios  i.e.  interaction models . 
 once an accurate model is acquired from participatory simulations  it becomes possible to simulate experiments that cannot be conducted in real space. for example  sugiman's experiment was performed using two or four leaders  but we can vary this number to clarify the relation between the number of leaders and the time required for evacuation. moreover  we can explore an effective combination of the follow-me and follow-direction methods. given accurate and explainable agent models  even though the models can only capture a part of the variations possible in human behavior  participatory simulations must be seen as useful in educating or training people. people can be provided with a learning environment wherein evacuations or other drills can be experienced.   
1 augmented experiment 
participatory simulations are particularly useful when conducting controlled experiments: they make it easy to prepare the application environments for testing  and user behavior can be recorded for later analysis. however  it is sometimes fails to provide enough reality to allow the testing of ubiquitous/pervasive computing environments. 
 to understand how users behave with socially embedded systems  real-world experiments are often required. in the case of video phones  for example  since user behavior in everyday life is not easy to simulate  it is essential to observe how individual users employ their video phones in real space. in ubiquitous/pervasive computing  however  because a large number of electronic devices will be embedded in public spaces like railway stations and are in continuous service  it is desired but often impossible to conduct experiments with a large number of human subjects. augmented experiments offer the ability to use just a small number of human subjects to perform an experiment that can be enhanced by large scale multiagent-based simulations.  
　figure 1 illustrates how augmented experiments are realized. figure 1 a  represents a real world experiment  where human subjects communicate and perform an experiment in real space. figure 1 b  shows how we introduce a virtual space into the real world experiment. the sensors in the real space captured the behavior of human subjects for reproduction in the virtual space. the sensors can be cameras  rfids  or gps depending on the environment. by using the virtual space  we can monitor the entire experiment from various 
viewpoints. furthermore  we can communicate with the human subjects in the real space through their avatars in the virtual space. transcendent communication is a new monitoring interface  where a visually simulated public space provides a more flexible and consistent view than regular surveillance systems  nakanishi et al.  1a .  
 figure 1 c  illustrates an augmented experiment. in parallel with a real world experiment  a large scale multiagent-based simulation is conducted in virtual space: the experiment is augmented by the simulation. to provide social reality to the human subjects  scenario-guided extras are placed around the subjects. in contrast to the participatory simulations  the human extras do not control avatars: the human extras in the real space are controlled by the scenario-guided agents. for example  we can use human extras acting as evacuation leaders to conduct disaster evacuation drills. in contrast to participatory simulations where human subjects sense and act in virtual space  augmented experiments allow subjects to sense and act in real space. conducting an augmented experiment is possible if the real space is equipped with enough sensors.  

real space
	c  multiagent-based augmented experiment	 
figure 1. augmented experiment 
 
 in summary  an augmented experiment consists of 1  agents for modeling users  1  avatars to represent human subjects  1  scenarios for modeling interactions  1  human subjects to act experiments  1  virtual space to represent real space  1  a monitor to visualize the experiment in real space enhanced by simulations in virtual space  1  sensors to reproduce human subjects in virtual space as avatars  1  communication channels between real and virtual spaces  and 1  human extras to represent agents in virtual space for interacting with human subjects. 
the issues on augmented experiments are as follows.  
1. seamless connections between virtual and real spaces: the difficulty with augmented experiments is to provide the human subjects with a sufficient level of reality. to guide human subjects in a timely fashion  the subjects must receive sensing information through the communication channels or from the corresponding human extras. to ensure the subjects naturally behave in real space  the selection of sensor devices and communication media becomes crucial. visual  auditory and textual interactions are to be organized appropriately. 
1. learning from data obtained in virtual and real spaces: learning issues are similar to those in participatory simulations  but their evaluation is more difficult  since the behaviors of human subjects in real space are harder to analyze than those in participatory simulations. though we can monitor the experiment via virtual space  video capture of the human subjects in real space is also indispensable for analyzing the entire experiment. to confirm the feasibility and usefulness of augmented experiments and to determine their future issues  we conducted indoor and outdoor evacuation experiments in real space augmented by a large scale multiagent-based simulation. 
1 indoor experiment 
everyday  more than 1 passengers pass through kyoto station  the main railway station in kyoto. in this station  using q  ishida  1b  and freewalk  nakanishi et al.  1b   we installed a disaster evacuation system that tracks passengers to help their navigation based on their current positions. beyond conventional navigation systems  which announce route information using public loudspeakers  our system sends instructions to individuals using mobile phones. augmented experiments are required for testing the evacuation system  because there is no other way to conduct experiments with enough reality. 
 

 
figure 1. indoor experiment 
 
 the augmented experiment was designed as follows. as the sensors  we placed twenty eight cameras in kyoto station  and captured the movements of passengers in real time. the cameras are specially designed to detect passengers' behavior but not personal features. figure 1 shows the omnidirectional cameras placed in the station. as the virtual space  we used freewalk  a three dimensional virtual city system and used it to reproduce the passengers' behavior. we implemented a monitor based on transcendent communication  nakanishi et al.  1a . figure 1 includes a snapshot of the monitoring system; human subjects on the station platform are projected 
onto avatars in virtual space. a bird's-eye view of the real space is reproduced on the screen of the control center so that evacuation leaders in the center can easily monitor the experiment. as the communication channels  the leader can point at particular passengers on the screen  and talk to them through their mobile phones. when the monitor detects pointing operations  a wireless connection is immediately activated between the control center and the indicated passengers.  
 a multiagent-based simulation with a large number of agents controlled by evacuation scenarios was performed in parallel with the experiment in real space. see-through head-mounted displays are not suitable for presenting the simulation of augmented experiments  since it is unsafe to mask the views of passengers. as described above  since we used mobile phones  small and low-resolution images of three dimensional virtual spaces are difficult to understand. instead of displaying visual simulations  the mobile phones displayed symbols that directly represent what human subjects need to understand. we did not use human extras in this experiment. 
 we conducted the indoor augmented experiment in february 1. from this experience  we learned that insufficient visual reality prevented the human subjects from recognizing the crowd of agents around the staircase. it appears that the usability of an augmented experiment depends significantly on the user interface for interacting with the agents. 
1 outdoor experiment 
one year after the indoor experiment  we implemented a large-scale outdoor evacuation system for emergency situations using q  ishida  1b  and caribbean  yamamoto et al.  1   and conducted an augmented experiment in january 1. this system architecture is close to that of our indoor experiment  but the sensors were gps devices instead of omnidirectional cameras  and as the virtual space  we used a two dimensional map instead of a three dimensional virtual city system. the virtual space is displayed on the monitor screen of the control center in a birds-eye view  so that the leader could grasp how evacuees were behaving in the experiment. evacuees on a screen consisted of a small number of avatars representing the human subjects in real space  and a large number of agents controlled by evacuation scenarios in virtual space. in the actual experiment  ten to thirteen humans and three thousand agents per each time undertook the augmented experiment.  
 the location of human subjects in real space was projected into virtual space based on their positions acquired by gps. this map showed fires  blocks routes  and safe areas in real time. the human subjects could always get the latest map by sending their location. the evacuation leaders assigned evacuation destinations and directions to each evacuee through the monitor screen shown in figure 1. the leader issued high level instructions to the evacuees using the map  and precise navigation instructions were automatically generated for each evacuee. dragging operations indicating a rectangular area enabled the leader to broadcast an announcement to a group of evacuees. 
 as in the indoor experiments  the evacuation leader activated communication channels to particular evacuees by pointing at the evacuees on the screen. the leader could talk to the particular evacuees and provide customized navigation. we used human extras as evacuation leaders in this experiment to check the evacuation routes taken by human subjects in real space. 

 
figure 1. outdoor experiment 
 
 the human subjects could grasp the experiment from the screen of their mobile phones. the map around the place the subject was standing was displayed together with information of fires  blocked routes  and safe areas  just as on the monitor in the control center. the moves of other evacuees were also displayed on their mobile phones. from this experience  we learned that a map can be an excellent interface between human subjects and agents as well as evacuation leaders. unlike the indoor experiment  since route selection is the main issue in the outdoor experiment  the human subjects did not have much difficulty in imagining the disaster situation. however  if the number of agents was high in the outdoor experiment  then virtual crowding would become a problem. human extras playing evacuation leaders can increase the reality of the experiment. 
1 learning from experiments 
multiagent-based simulations use production rules to describe agent models for approximating human users  and extended finite state automata to describe interaction models among users and socially embedded systems. by analyzing the logs of participatory simulations or augmented experiments  we can refine both agent and interaction models. the learning process of multiagent-based participatory design consists of two phases: the normal design loop and the participatory design loop.  
 at first  scenario writers create agent and interaction models and verify their design by multiagent-based simulations. this process is called the normal design loop and its detailed steps can be found in a previous work  murakami et al.  1 . after verifying the agent and interaction models  participatory simulations and augmented experiments are performed. the results of which may be different from those 
of multiagent-based simulations  because human behaviors are often more variable than those of agents modeled by scenario writers. if this is the case  we first refine the agent models to better express the variation seen in human behavior. we then perform the multiagent-based simulations again with the new agent models. if the new simulations succeed to express various human behaviors but fail to produce the expected results  we then refine the interaction models; i.e. modify protocols  methods  rules  and/or laws. after the improved interaction model is confirmed by multiagent-based simulations  participatory simulation and augmented experiment take place again. this loop  called the participatory design loop  is illustrated in figure 1. 
 

figure 1. participatory design loop 
 
it would seem  at first glance  that inductive machine learning technologies could be applied to refine agent and interaction models. however  it is extremely rare to have a sufficient number of human subjects to gather enough data for inductive learning: deductive methods with domain knowledge are more feasible  torii et al.  1 . to refine agent models  given that the training examples will be limited  explanation-based learning is a reasonable approach. though variations exist in human behavioral rules such as  escape from crowds  and  follow crowds   observing the behavior of human-controlled avatars can create a plausible explanation that selects one of the two rules. 
　by combining machine learning technologies with interviews of human subjects  agent models have been successfully refined in several domains  murakami et al.  1 . to refine interaction models  if we have enough candidate protocols  we can try them  and select the one that best fits. if we need to invent a new model  however  there is no available technology that can create such an interaction model. the visualization of simulations and experiments can  however  support scenario writers in inventing new models. 
1 related work 
there are two types of multiagent-based simulations and they have different goals; a  analytic multiagent-based simulations often with simple agent and interaction models and b  synthetic multiagent-based simulations sometimes with complex agent and interaction models. analytic simulations have been used to model social systems  epstein and axtel  1 . here  the kiss principle  keep it simple  stupid  is often applied  axelrod  1 . the kiss principle states that modeling should be simple even though the observed phenomenon is complex  and that complexity should be a result of repeated interaction among agents and their environment. hence  agents are often modeled with limited parameters. this approach is effective in the analysis of macro-micro links  the relationship between the macro properties of the entire system and the micro properties of the agents constituting the system.  
 on the other hand  synthetic simulations are used to produce realistic situations. agent and interaction models can be complex reflecting the real world to make the simulation as realistic as possible. this approach is used in an early stage of system development  noda and stone  1   and in education or training  rickel and johnson  1 . since our purpose is to design socially embedded systems  our simulation is synthetic: our agent and interaction models can be complex to reproduce realistic situations; after a participatory simulation  an augmented experiment is performed to understand user behavior in a more realistic environment.  
　our work appears to follow various multiagent-based simulation papers including the robocup soccer simulator  noda and stone  1   but is clearly unique due to its focus on participatory design methodology. drogoul proposed a methodological process for developing multiagent-based simulations  and introduced the idea of the participatory design of simulations  drogoul et al.  1   while we are proposing the participatory design for socially embedded systems. bousquet applied role games to the modeling of social systems  bousquet et al.  1 . this paper  however  applies multiagent-based simulation to software system design  not social system analysis.  
　we propose the use of participatory methodology including simulations and experiments for designing large-scale socially embedded systems. this methodology differs from the gaia methodology  zambonelli et al.  1  or opera  v│zquez-salceda et al.  1   which focus on an early stage of designing multiagent systems mainly for enterprise applications  jennings  1 . our methodology can be applied to designing ubiquitous/pervasive computing systems  but the resulting systems are not necessarily multiagent-based  though multiagent systems are often natural solutions of socially embedded systems. 
1 conclusion 
the waterfall model has been used in software development for a long time. given the increase in human-computer interaction  however  it has become essential to use the user-centered design approach when creating usable and 
accessible interactive systems. in future  however  millions of electronic devices with computing facilities will be connected with each other in ad hoc ways  and should behave coherently. it is natural to say we need a new methodology for designing ubiquitous/pervasive computing systems. 
　in this paper  we proposed the multiagent-based participatory design methodology for ubiquitous/pervasive computing systems  which are to be embedded in human society. though the concept of multiagent-based participatory design is open-ended  we include the following two components.  
 we call multiagent-based simulations participatory simulations when they include human-controlled avatars and scenario-guided agents. we can monitor the entire process of a participatory simulation by visualizing the virtual space. we call real-world experiments as augmented experiments when they are augmented by large scale multiagent-based simulations. in ubiquitous/pervasive computing  because a large number of electronic devices will be embedded in public spaces and in continuous use  it is often impossible to conduct experiments with a large number of human subjects. augmented experiments enable us to perform such trials with a small number of human subjects.  
 by introducing augmented experiments and actually trying out indoor and outdoor experiments in the city of kyoto  we learned the following lessons. interaction among the human subjects in the real space and agents in the virtual space should be designed carefully. if there exists a medium like geographical map  which human subjects use daily to grasp situations  it is a good idea to adopt them as the interface between the real and virtual spaces. human extras controlled by scenario-guided agents are also effective to increasing the reality  if they have a clear role such as evacuation leaders. 
