 
in this paper we outline a general approach to the study of problem-solving  in which search steps are considered decisions in the same sense as actions in the world. unlike other metrics in the literature  the value of a search step is defined as a real utility rather than as a quasiutility  and can therefore be computed directly from a model of the base-level problem-solver. we develop a formula for the expected value of a search step in a game-playing context using the single-step assumption  namely that a computation step can be evaluated as it was the last to be taken. we prove some metalevel theorems that enable the development of a low-overhead algorithm  mgss*  that chooses search steps in order of highest estimated utility. although we show that the single-step assumption is untenable in general  a program implemented for the game of othello soundly beats an alpha-beta search while expanding significantly fewer nodes  even though both programs use the same evaluation function. 
1 	introduction 
the ralph  rational agent with limited-performance hardware  project is a long-term research effort aimed at understanding the effects of finite computational resources on the performance of  and the design of algorithms and architectures for  artificially intelligent systems. rather than viewing ai as the production of human-like or commercially valuable programs  or as the design of systems containing only true beliefs  we see the ai problem as that of designing systems capable of achieving optimal behaviour  in a decision-theoretic sense  under computational constraints in complex task environments. such a constrained optimization problem may have very different solutions from the computationally unconstrained case  for which a pure logical  or  more generally  decision-theoretic  implementation may be appropriate. we therefore view the problem of finite resources for real-time behaviour as central to ai  rather than something that necessitates undesirable approximations to a 'perfectly rational' agent. 
   *this research was carried out with support from the at&t foundation and the computer science division of the university of california  berkeley. the second author is supported by a shell foundation doctoral fellowship. 
1 	search 
　in this paper  we outline the application of decision theory to the problem of reasoning about which computations to perform. the theory can be used to analyze the rationality of bounded agents  or to design optimal algorithms for real-time situations - situations in which the utility of an agent's actions depends significantly on the time at which they are carried out  rendering perfect deliberative rationality inoperable. we apply the theory to design a game-playing algorithm that selects nodes to expand according to the expected utility of the expansion  and can beat alpha-beta with the same evaluation function while searching fewer nodes. 
1 	optimal allocation of computational resources 
we are studying the extent to which situation-dependent control of reasoning can benefit an agent operating in real time. in such an agent  choices concerning which computation to carry out  if any  must be made by metalevel computations. this  of course  leads to the potential for an infinite regress of meta-levels. this regress can be avoided by  among other methods  the use of approximate decision-making  the outcome of which is not guaranteed to be optimal  at some point in the hierarchy  or by carrying out an unbounded computation at design time. for the tasks faced in ai the variety of situations and time pressures suggests that a combined approach is appropriate. 
　the basic proposal here is quite simple: choose computations in the same way that any other actions are chosen. decision theory is the standard normative theory for actions  so let's apply it to computations too. this idea  although similar to the concept of information value in decision science  raifta and schlaifer  1  howard  1  good  1   is relatively new in at. in addition  getting the model right and putting it into practice are not such easy tasks. 
　in this section  we give a general formula for the expected value of a computation. the following section discusses ways in which the formula must be modified to make it usable by a limited rational agent that has only estimates of utilities and their expectations. subsequent sections develop the analysis needed  and the simplifying assumptions employed  to create a game-playing program along these lines. 
　notation: the agent's goal is to maximize its utility function u w  on states of the world w. we denote the outcome of an action a performed in a situation wj by  a  wj}  or simply  a  if performed in the  current  situation. at any given time the agent has a default base-level action  α  which is the action that currently appears best to the agent. at the same time it has available to it a set of possible computational actions sj  with which it can continue its deliberations. computation sj might lead the agent to revise its choice of default action; the new default action will be called αsj the meta-level decision problem for the agent is thus to decide among the immediate options a  s 1  ...   sk. 
　computational actions are distinguished from baselevel actions in that they directly affect only the agent's internal state. we define the net value of a computational action to be the resulting increase in utility  compared to the utility of taking the default action instead: 
		 1  
　if sj is a  complete computation   i.e.  is not followed by any further deliberation  then the utility of state  sj  is just the utility of the action αsj chosen as a result of the computation 1  given that it is to be performed after the computation is complete  i.e.  in the state  sj : 
		 1  
　in the general case  the computation will bring about changes in the internal state of the agent that will affect the value of possible further computational actions. we call such a computation a  partial computation . in this case  we want to assess the utility of the internal state in terms of its effect on the agent's ultimate choice of action. hence the utility of the internal state is the expected utility of the base-level action which the agent will ultimately take  given that it is in that internal state. this expectation is defined by summing over all possible ways of completing the deliberation from the given internal state. we denote a computation sequence consisting of computation s1 followed by computation s1 by the expression s1.s1. thus  letting t range over all possible complete computation sequences following sj  and αt represent the action chosen by computation sequence t  we have 
		 1  
where p t  is the probability that the agent will perform the computation sequence t. 
1 	estimating the value of computation 
how should a limited rational agent use these equations to evaluate its own computational actions  the agent cannot simply apply equation 1 directly  because it does not know the exact utilities of its base-level actions.  if it did  it would not need to deliberate at all.  we will assume the agent makes a numerical estimate u  a   of the utility of its available actions  and that deliberation proceeds by revising these estimated utilities. at the end of its deliberations it picks the action with highest estimated utility. this model applies to  or can be adapted to  most search algorithms in ai. since it is important to distinguish the different utility estimates as 
l
　　we ignore learning as a means of generating utility for future situations. 
computation proceeds  we will use s to denote the baselevel computation sequence carried out up to the current state. then qs represents an estimate of a quantity q calculated by computation s. thus the estimated utility of a computation sj is given by: 

t 
here ai ranges over all of the possible base level actions in the current state; hence αt is just the action which maximizes us.sj.t in the state  sj.t . 
　finally  the estimated net value of computation sj is given by 
		 1  
　of course  before the computation sj is performed  v sj  is a random variable. although the agent can't know ahead of time what the exact value of v sj  will be  given sufficient statistical knowledge of the distribution of v for similar actions in past situations  the agent can take its expectation  
	i 	 1  
such an agent can thus employ meta-level rationality in its control decisions. the principle of rationality- choose the action with highest estimated expected utility-applied to the meta-level decision problem is equivalent to: 
1. estimate the expected value e v sj   of each computational action sj. 
1. if any computational action has positive expected value  take the one with highest expected value. otherwise  cease computing and take the current base-level action  α  with highest expected utility. 
　thus far we have captured the real-time nature of the environment by explicitly including the situation in which an action is taken in the argument to the utility function. such a comprehensive function of the total state of affairs captures all constraints and trade-offs; in particular  any form of time constraint can be expressed in this way. however  the inclusion of this dependence on the overall state significantly complicates the analysis. under certain assumptions  it is possible to capture the dependence of utility on time in a separate notion of the cost of time  so that the consideration of the quality of an action can be separated from considerations of time pressure. 
　in applications  typically we are given some value estimator that is independent of time and takes just the action outcome as argument. we will call such a function the estimated intrinsic utility  denoted by uj. we can1then define a function tc presses the difference between total and intrinsic utility  assuming this difference depends only on the temporal duration |sj| of the computation: 
		 1  
　consider again the case of a complete computational action sj  where sj results in taking action αsj  where 
	russell and wefald 	1 

1 	specific base-level problem-solvers 
up to this point  we have been working at a very general level  making no assumptions about the nature of the base-level decision-making mechanism. the above equations are applicable to a brain or a pocket calculator  in principle. naturally  there are some attributes of certain mechanisms that make them amenable to meta-level control. the overall computation should be modular  in the sense that it can be divided into 'steps' that can be chosen between; the steps must be able to be carried out in varying orders. the steps can of course have any grain size. 
　we now look at some particular systems in more detail.1 we will focus on the use of the completecomputation formula  equation 1. this equation gives the net benefit of the computation as a random variable. in order to compute its expectation  we need information about the distribution of possible effects of a computation on the system's value estimates for its available actions. 
　in a search program  computation typically proceeds by expanding 'frontier nodes' of the partially-grown search tree. the value estimates for actions are calculated by backing up values from the new leaf nodes through their parents. the effect on the value estimate for an action therefore is composed of two aspects: 1  the effect on the value of the leaf nodes that are expanded  and 1  the transmission of this effect back through the tree. 
1
　　space limitations prevent more than a cursory justification of the results. for more detailed analysis  see  russell and wefald  1b . 
1 	search 
　the first component depends on the nature of the node being expanded  and the nature of the expansion computation. statistical information on the probability distribution can therefore be acquired by induction on a large sample of similar states using the same type of expansion computation. the second component is an analytic function of the state of the tree  in particular  the current value estimates for the nodes on the path to the root  and their children  that depends on the backingup method used by the search program. 
　in this paper  we consider systems that make decisions using simple search algorithms. in order to make calculation of the expected value of computation tractable  we will introduce explicit simplifying assumptions. these are related to what pearl  has called a  myopic policy . the notation used will be as above  with the addition of b1 b1  etc.  to denote current second-best action  current third-best  and so on. 
meta-greedy algorithms 
explicit consideration of all possible complete sequences of search steps is clearly intractable. the obvious simplification is to consider single computation steps  remember that a single step can represent an arbitrary amount of computation  but without interleaved control   and to estimate their ultimate effect; we then choose the step appearing to have the highest benefit. such meta-greedy algorithms effectively have a fixed meta-meta-level policy of a depth-limit of 1 on the meta-level decision problem. 
single-step assumption 
the expression for the estimated utility of a computation  equation 1  can be used directly with a meta-greedy algorithm  but it is often somewhat hard to evaluate  particularly when the future availability of computational resources is hard to estimate. the computation is greatly simplified if we assume that it is reasonable to act as if we will take at most one more search step. in this case we can use the complete-computation formula 1. we call this the single-step assumption. the assumption sometimes fails. recall that a complete computation has no value unless it changes the choice of move; thus the single-step assumption will predict that certain search steps can have no value  whereas often those steps enable other steps to become valuable. we will see that  although it makes the expected-value computation tractable  this assumption also places certain limitations on the depth of search in some domains  including gameplaying. 
　it is worth emphasizing here that if either the metagreedy or single-step assumptions were completely relaxed  the other would become completely true. that is  if it were possible to consider all possible sequences of computations  then it would be no restriction to consider those sequences as complete computations. on the other hand  if we could accurately compute the full expected value of a single computation step considered as a possibly partial computation  we would thereby be implicitly evaluating all possible continuations of that computation step  and hence we would not need to consider explicitly all possible computation sequences. thus  the simplification lies in employing the two assumptions jointly; neither alone would be a restriction. 
but the distributions are narrow.  this latter case has received scant attention in the literature.  we illustrate the three major situations graphically in figure 1. 
1 	game-playing: the mgss* algorithm 
in this section  we describe how the utility of a computation can be estimated for two-player games  in which the basic decision-making algorithm is some kind of lookahead search using minimax backup. the choice of search steps is then made so as to maximize the system's overall utility  as determined by the move chosen and the time at which it is made. a system with low-overhead  normative control of search would be guaranteed to win  on average  against any other program with equal computational resources and domain-specific knowledge. we believe such a system can be achieved. in addition  we obtain qualitative insight into the nature of efficient search. we assume a standard evaluation function that is independent of time factors  and we therefore use the time cost function described above. the evaluation function is viewed as an intrinsic expected utility function  not as an estimate of the true value of the position. the true value 
for a game such as chess is 1 or ＼1  on a scale in which 1 is a win ; the estimated utility is a computationallybounded  probability-weighted value pwin - ploss  good  1 . 
　in our analysis  russell and wefald  1b   we use both the meta-greedy and single-step assumptions  with individual node expansions as the chosen computational steps. we have developed an efficiently computable formula for evaluating the integrals in equations 1 and 1. the principal tool is the re-expression of the probability distributions pij in terms of the error distribution pjj at the leaf node affected by the search action sj . since pjj is a function of only the board situation  and not of tne game tree  it can be estimated from empirical data on the results of previous tree searches. a computable expression for the integral is then obtained by analyzing how the change in value of the leaf node is propagated to the top level. 
　the proof of correctness of the following equations  corresponding to equations 1 and 1  depends on a theorem proved in  russell and wefald  1b  that gives a simple recursive definition for the nodes that are relevant to a top-level move  that is  nodes whose expansion can affect the top-level move's value  cf. singleton conspiracies in  mcallester  1  . the value of expanding any other node is strictly 1  and our algorithm derives considerable efficiency from restricting its attention to only relevant nodes. 
the expected value of expanding a leaf node j in the 
	russell and wefald 	1 


1 	implementation of mgss* 
we have used the game of othello for our experiments in this domain. in a direct implementation of the analysis of the previous section  the entire search tree is kept in memory  and the tree is grown one step at a time by choosing  at each step  a tip node to expand  and adding its successors to the tree. 
the following information is maintained for each node: 
1. a link to the top-level move  if any  to which the node is relevant; 
1. the searchvalue of the node  i.e.  the expected gain in utility from expanding the node  for leaf nodes; 
1. the game value of the node  i.e.  its staticvalue if it is a leaf node  or its current backed-up value otherwise; 
1. the 1 value for the node  as defined above; 
1. a link pointing to the parent of the node; and 
1. if the node has already been expanded  a pointer to a list containing the node's children. the list is maintained in order of gamevalue. 
relevant leaf nodes with positive search value are maintained in a queue  in decreasing order of searchvalue. 
a l g o r i t h m m g s s * 
1. generate the successors of the root. place the relevant ones in the queue ordered by searchvalue. for each successor  set its 
gamevalue equal to its staticvalue. place the successors in the children list of the root ordered by gamevalue. 
1 	search 
1. remove the first element j of queue. compute e a sj   using equation 1 or 1. estimate the time-cost tc of expanding node j. if  ~ tc    1 then return the first element in the children of the root as the best move. 
1. otherwise 
 a  carry out the computation sj;. for each resulting leaf node  set gamev alue = staticvalue. 
 b  place 	the 	new 	leaf 	nodes  	ordered 	by 
gamev alue  in the children list of j. 
 c  back up the gamevalues of the successors to j's gamev alue. if this changes  re-insert j in its parent's children list and continue backing up recursively  stopping at the root. whenever a children list is re-ordered  or the best move in such a list increases its value  or the second-best move in such a list decreases its value  recompute the appropriate s values and relevant node pointers. the latter step may involve updating queue membership. 
 d  add j's relevant successors  ordered by searchv alue  to the queue. 
1. go to 1. 
　all this means that the overhead for controlling the search is small  provided that the integral expression in equations 1 and 1 can be evaluated quickly and exactly. we now show how this is done. 
　the leaf-node error distributions  the functions pjj in the integral  are generated by prior statistical sampling. for search steps consisting of a single node expansion  such as are used in mgss*  the error is simply defined as the difference between the leaf's static value and its backed-up value from a depth one search. the data points are gathered into buckets according to selected features of the board situation. 1 data points were gathered into roughly 1 buckets. a different error density function is created for each bucket. the functions appear to be normal curves to a reasonable degree of approximation  and can thus be represented by two parameters  the mean u and standard deviation a. 
　given normal curves nua for the density functions pjj and pkk  we can simplify equations 1 and 1 considerably  after some mathematical effort. the new equations 



corresponding to equation  1 . here Φ is a tabulated integral of the normal curve no 1 armed with these formulae  computing search values is a good deal quicker than computing the static evaluation function  enabling mgss* to perform with very low overhead. 
　the time cost estimation function can also in principle be determined empirically. assume that the average time needed to perform a search step is  say  a millisecond. then we want to determine empirically the effect of wasting a millisecond  in a given game situation  on the probability of ultimately winning the game. we then have a common utility scale for moves and time costs. in practice  it is sufficient to have c be an appropriate function of the number of seconds per move remaining  such that a loss on time is impossible. the appropriate sort of time-cost function will depend heavily on the particular rules under which the game is played  such as whether there is a per-move or only a per-game time limit  and so on. for the purpose of testing the implementation  we set a per-game time limit in terms of numbers of nodes expanded  and set the time cost to be an appropriately parameterized inverse function of the average time remaining per move. 
1 	performance of mgss  
the qualitative behaviour of the mgss* algorithm is much like that described for the general decisiontheoretic case  with two distinct classes of search termination points  see figure 1 . the search is highly selective  with some branches reaching depths of 1. 
　our experiments indicate that  with moderately small time allocations  mgss* is significantly better than an alpha-beta search using the same evaluation function  even though mgss* generates significantly fewer nodes of search. our results vs. alpha-beta search to depths 1  1  and 1 are summarized in table 1. for each search depth  the time cost function of mgss* was adjusted to allow the algorithm to generate about as many nodes on average as alpha-beta; beyond depth 1  however  mgss* chose to generate far fewer nodes. each tournament consisted of 1 games played from 1 different starting positions  with the two algorithms alternately playing black.  one game against depth-1 a-/  was tied.  
　note that mgss* is roughly even with depth-1 a-/1  and searches slightly more nodes; evidently  at this depth a-/  is wasting very little of its search effort. however  at greater effort limits  mgss* plays significantly better  while doing much less search  in terms of nodes generated. in terms of time used  the performance of mgss* is less impressive  using more time at depths 1 and 1  and slightly less at depth 1. it is important to emphasize that very little effort was made to optimize the implementation of mgss* with respect to execution time  and we are confident that opportunities exist for considerable savings by improving the data structures used  replacing sorted lists with hash tables and binary search trees  and so on. 
　in terms of nodes generated  we expect further improvement with more work on the features used to classify the error density functions. the number of nodes searched can probably be further reduced by reducing the unit of computation to be the generation and evaluation of a single successor node  as in alpha-beta  rather than the expansion of a node. our algorithm will also make better use of a more reliable evaluation function - as the errors tend to zero  the value of search will decrease  and in the limit the algorithm will only search to depth 1. an interesting possibility is that the evaluation function of bkg  berliner  1  has effectively reached this level already: because of the dice rolls  the branching factor in backgammon at the root is 1 but at all other levels is close to 1  so that no current program uses a significant amount of search. jeff conroy  conroy  forthcoming  has extended mgss* to handle probabilistic outcomes  and initial results for backgammon indicate that additional search still pays significant dividends. 
　although mgss* seems extremely effective for small time allocations  the single-step assumption eventually begins to bar almost all nodes from being expanded as the tree grows larger. hence comparisons against a much deeper-searching alpha-beta are unfavourable. preliminary results indicate that mgss* plays a slightly better than even game against depth-1 alpha-beta  while generating about 1 as many nodes of search. against depth-1 alpha-beta  mgss* appears to be incapable of generating large enough search trees to play effectively. extension of the algorithm to consider sets of search steps may overcome this problem. we are also investigating the effect of applying mgss* to selectively extend search beyond the depth limit of an alpha-beta search. a simpler form of this method appears in the work on singular extensions by campbell  campbell  1   and has proved surprisingly effective in the hitech and deep thought chess machines. 
1 	related work 
a detailed comparison with related work on selective search appears in  russell and wefald  1b . here we can only point the reader to the pertinent literature. 
　many sophisticated programs have been constructed for game-playing  where the infeasibility of search to termination has long been accepted as a fact of life. in some excellent early work  statistician i. j. good  1  1  proposed a decision-theoretic analysis both of move choice and search step choice  but the proposal was never implemented. 
　berliner  pointed out that alpha-beta undertakes needless search when only one legal move is available  or when one move can be seen to be better than all others even without search. his b* algorithm attempts to find the shortest 'proof  based on heuristic value bounds  that one move is best. palay  1  1  has given heuristics for choosing how to expand the tree to produce a 'proof quickly. in our terms  b* is using the wrong utility function at the meta-level  as illustrated 
	russell and wefald 	1 

by the case of symmetrical moves  where one should toss a coin rather than attempt to prove one better. 
   rivest's min/max approximation algorithm  rivest  1  expands nodes that have the highest effect on the root value. this only weakly relates to the expansion's actual utility. mcallester's  conspiracy numbers  approach  mcallester  1  approach is more sophisticated  considering the set of nodes that must change their values to increase or decrease the root value beyond given bounds. 
1 	discussion 
the principal contributions of the paper are the following: 
  the rational choice of search steps was defined and operationalized. both meta- and base-levels should use the same utility function. this leads to improved choice of search steps compared to several other algorithms  and unifies pruning and termination in a natural framework. 
  a class of meta-greedy algorithms was defined  us-ing the single-step assumption  and a general formula for the value of search was developed in this context. the formula depends on a notion of error in evaluation that is both coherent and directly available from empirical data. 
  theorems were proved that allow for efficient im-plementation and for computation of search values using only simple  parameterized distributions. 
  good performance of the mgss* algorithm  despite its restrictive assumptions  suggests that extensions of the approach to cover sequences of search steps may yield very high-quality algorithms that are also theoretically well-founded. 
we also hope this work will help to reduce the gap between research on game-playing and that on more general types of decision-making. 
　as usual in computer science  justification is the prelude to synthesis  and we expect more new algorithms to appear as researchers prove various meta-level theorems concerning search action sequences which provide the basis for efficient implementations. application to singleagent search  wefald and russell  1a  has yielded an algorithm with excellent real-time search behaviour  and a proof that best-first search is in fact optimal in the sense of performing the most valuable node expansions given the information available. 
   several problems and research tasks remain. the implications of our approach for learning techniques are being investigated  wefald and russell  1b   as are its implications for the choice of backing-up procedure. we would like to apply the theory to provide optimal control of general decision-theoretic calculations  for example in an influence diagram system  pearl  1 . however  the current analysis relies on the unit of computation only affecting the value of one top-level action  whereas refining a probability value  for example  might affect the value of several actions. other extensions are needed to handle the case where time cost cannot be treated as independent of move chosen. the most serious theoretical problem is the need for the single-step assumption. the theory of'conspiracy numbers' developed in  mcallester  1  may be of some help here. 
1 	search 
