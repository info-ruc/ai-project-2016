 
   multiple convergence is proposed as a method for acquiring disjunctive concept descriptions. disjunctive descriptions are necessary when the concept representation language is insufficiently expressive to satisfy the completeness and consistency requirements of inductive learning with a single conjunction of generalized features. multiple convergence overcomes this insufficiency by allowing the disjuncts of a complex concept to be acquired independently. by summarizing correlations among features in the training data  disjunctive concepts can provide rich extensions to the representation language which may enhance subsequent learning. this paper presents the benefits of disjunctive concept descriptions and advocates multiple convergence as an approach to their acquisition. multiple convergence has been implemented in the learning system hydra  and a detailed example of its execution is presented. 
i introduction 
   the learning problem addressed is concept acquisition from examples  as formulated in  mitc1 . this entails developing a concept description to summarize training objects  each classified as a positive or negative instance of the target concept. the summary description is constrained to admit every positive instance and to reject all negative training. these are called the completeness and consistency requirements  mich1 . the concern of most 
machine learning research in inductive concept acquisition has been the determination of characteristic descriptions  which represent concepts by summarizing the properties that hold true for all instances of the concept  diet1 . characteristic descriptions are typically encoded as a single conjunction of maximally specific features. 
a. classical concepts 
   classical concept descriptions are composed of features that are individually necessary  and jointly sufficient  for classification of objects as concept instances  smit1 . because of their single-conjunction form  the characteristic 
descriptions produced by most existing concept acquisition systems resemble classical concept descriptions. 
* support for this research was provided in part by the army research office under grant number aro daag1-k-1.    unfortunately  a single conjunction of necessary and sufficient features is not suitable to represent many concepts in natural domains. figure 1 illustrates this with a simple example of learning about trees. after two positive training instances  a classical concept description is adequate. however  when a negative instance is encountered  the existing classical description is found overlygeneral and must be refined into a disjunction. no single conjunction can satisfy both the completeness and consistency requirements for the given training and language. 
   learning systems that produce only classical concept descriptions reflect the assumption that concepts are independently separable. a concept is independently sepa-
rable when the independent determination of admissible values for each feature is sufficient to distinguish all positive instances of the concept from all negative instances. concept acquisition under this assumption can be characterized by the following two steps: 
1  for each attribute  find the set of values valid for the target concept 
1  take the cartesian product of all sets defined by step 1 
this approach fails when the attributes of the target concept interact  and the values allowed for an attribute in one context may not be valid in another. unless all necessary featural correlations are present in the representation language  a system limited to classical concept descriptions cannot learn these concepts. 
	murray 	1 
   
b. disjunctive concepts 
   disjunctive concept descriptions are necessary when the representation language is insufficiently expressive to produce a complete and consistent classical description. this situation arises when either of two anomalies occurs: 
1  incompleteness condition: a positive instance is encountered  but the concept description cannot be generalized to admit it without also admitting negative training 
1  inconsistency condition: a negative instance is encountered  but the concept description cannot be specialized to reject it without also rejecting positive training 
a disjunctive concept description resolves these dilemmas by relaxing the completeness requirement for the individual disjuncts. each disjunct summarizes some subset of positive training while remaining consistent with all negative training. collectively  the disjuncts satisfy both completeness and consistency. identification using disjunctive concept descriptions requires classifying an object as an example of the concept when any disjunct admits it. 
ii multiple convergence 
   multiple convergence enables acquisition of characteristic descriptions for disjunctive concepts. it differs from existing strategies in the way concepts are represented and how disjuncts are created and updated  allowing each disjunct to emerge as an independent concept representing a specialization of the target concept. multiple convergence has been implemented in the learning system hydra. 
a. representing disjunctive concepts 
   although the objective is to produce a complete and consistent characteristic description of the target concept  multiple convergence maintains a discriminant description as well as a characteristic description of each concept it learns. the discriminant description summarizes negative training and defines criteria that all instances of the concept must necessarily satisfy. the characteristic description summarizes positive training and represents hypothesized sufficient criteria. as with focussing  bund1  and 
candidate elimination  mitc1   concept formation proceeds as a bi-directional convergence: the discriminant description progresses from general to specific with negative training while the characteristic description generalizes with positive training. 
   disjuncts formed in the discriminant description are used to define the disjuncts of the characteristic description. intuitively  the discriminant description identifies regions of the feature space that can accommodate a classical description. each disjunct of the discriminant defines such a region. a characteristie disjunct  i.e. a single con-
junction of generalized features  is developed to describe 
the positive instances within that region. 
1 	knowledge acquisition 
   this process applied to the example from figure 1 produces discriminant description  foliage  seed  for the first two training instances. after encountering the negative instance  the discriminant description is specialized into three disjuncts:  foliage  apple    foliage  cone   and  leaf seed . each of these defines a region of the feature space that can currently accommodate a classical description. for each region  a characteristic disjunct is developed to represent the positive training within that region  producing the characteristic disjuncts  leaf  apple  and  needle  cone   as shown in figure 1. 
   each associated pair of disjuncts from the discriminant and characteristic descriptions defines a version space  mitc1  for a hypothesized specialization of the target concept. as the version space of each disjunct converges  it defines a classical description of a generalized exemplar. the target concept is represented by the dis-
junction of the surviving set of generalized exemplars. in the example of figure 1  two generalized exemplars representing deciduous and coniferous trees  e.g.  leaf  fruit  and  needle  cone   will survive the convergence induced by exhaustive training. 
b. updating disjunctive concepts 
   positive training instances are allocated to every disjunct that can be consistently generalized to admit them  as in  iba1  . this is motivated from a desire to remain insensitive to training order. rather than make arbitrary decisions that may require subsequent backtracking  all alternative hypotheses are maintained. 
   when the existing discriminant concept description is found to admit a negative training instance  it is replaced with a minimal-specialization. this involves specializing each disjunct only to the extent necessary to reject the training instance  and then retaining only those new disjuncts not more specific than some other consistent discriminant disjunct. typically  this will introduce one or more new disjuncts into the discriminant description  and unlike other similar techniques  mitc1  iba1  bund1   disjuncts that fail to cover any existing positive instances are retained to avoid subsequent backtracking. disjuncts of the characteristic description that admit the negative training instance must also be specialized. furthermore  the characteristic description may need to be extended with new disjuncts to summarize the positive training admitted by new discriminant disjuncts. 
   specializing and extending the characteristic description are tasks that have required most inductive learning systems to reprocess all prior positive training  e.g. star  mich1   and extensions to candidate elimination 
 mitc1j and focussing  bund1  . under multiple convergence  the reprocessing of prior training can be constrained to consider only a portion of the past positive training instances. this is achieved by indexing positive instances under the disjuncts that admit them  iba1 . 
   
   each discriminant disjunct  d  found to admit a negative training instance is specialized to a set 1 of one or more new disjuncts. the reprocessing of past positive training to develop new characteristic summaries can be limited to those instances admitted by and indexed under d. this follows since each new disjunct s in s is a specialization of d  so the instances s admits must be a subset of the instances admitted by d. therefore only the instances admitted by d need be considered when developing the new characteristic summary for s. 
   reprocessing all the instances of an invalidated disjunct is avoided by retaining a trace of the prior characteristic summaries along with the positive training instances. a new positive training instance and the prior characteristic generalization are indexed under each new characteristic summary. as the disjunct  fills up  with positive training  an exemplar generalization hierarchy emerges. the exemplar generalization hierarchy is always rooted at the current characteristic summary of the disjunct  and its leaves are the positive instances admitted by the disjunct. 
   this data structure can be used to enhance reprocessing required by an inconsistency condition. for each s in 1  a new characteristic summary is developed during an all-paths traversal of the exemplar generalization hierarchy associated with d. each path can terminate when a node is either admitted by or disjoint from the new disjunct 1. when a node of the exemplar generalization graph is admitted by s  then the node itself can be reprocessed as a positive instance  and all the actual instances indexed under it can be ignored. similarly  if the node is completely disjoint from s  then all instances indexed under the generalization must be irrelevant. 
   the multiple convergence approach to learning disjunctive concepts has been implemented in the learning system hydra. an overview of the concept acquisition algorithm of hydra is presented in figure 1  and the approach is illustrated by an example of learning a necessarily disjunctive concept in figure 1. 
repeat 
await the next training instance  1; 
if ti is positive 
then 
for each discriminant disjunct d that admits ti do 
minimally generalize the characteristic summary of d 
to admit 1; 
if ti is negative 
then 
begin 
minimally specialize the discriminant description 
to reject 1; 
for each new discriminant disjunct d' do 
reprocess prior positive training admitted by d' 
¡¡¡¡to produce a characteristic summary for d' end; 
¡¡display the discriminant and characteristic descriptions; until the teacher is satisified. 
figure 1: the learning algorithm of hydra 
i i i extending the representation language 
   disjunctive concepts are inherently taxonomic and introduce new generalization hierarchies rooted at the target concepts. each node of the new generalization hierarchy is a list of attribute-value pairs representing an instance or generalization in the feature space of the concept. since every attribute is defined by a pre-existing generalization hierarchy  the new hierarchy of a disjunctive concept can be viewed as an orthogonal generalization hierarchy. the nodes of the new hierarchy represent useful correlations of features. the links define context-sensitive generalizations of the features. upon the conclusion of a training session  dialogue with the teacher enables naming the useful nodes in the hierarchy. the named nodes are then elevated to the status of concepts. this process is illustrated by the example of figure 1. the resulting orthogonal generalization hierarchy for this example is presented in figure 
1. 
   shifting the generalization biases of representation languages used for inductive concept formation has been recognized as an important research topic  but previously only techniques to weaken the existing biases have been developed   bund1  utgo1 . this involves focusing the internal disjunction provided by a generalization hierar-
chy by inserting a new node to segregate valid from invalid values of an attribute. the new generalization hierarchies introduced by disjunctive concepts represent an entirely new source of bias for generalization. 
   the new generalization hierarchies introduced by disjunctive concepts can enhance subsequent learning tasks. 
for example  consider learning about the concept italianexports after having already defined vehicle. if provided with positive training examples  pedals  handlebars  and  engine  handlebars   the new generalization hierarchy enables immediate generalization to vehicle  i.e. italy exports vehicles . without prior knowledge of vehicles  this training would lead to the more conservative  and incorrect  generalization  power handlebars . 
iv conclusion 
   introducing disjunction into our concept descriptions broadens the class of concepts our systems can learn. multiple convergence has the advantage of producing classical concepts when appropriate  but is also able to describe a concept as a disjunction of generalized exemplars. this becomes necessary when the representation language is insufficiently expressive to produce a single-conjunct summary that is both complete and consistent. 
   many existing learning systems cannot learn disjunctive concepts because they assume the target concept is  independently separable. multiple convergence puts only tentative faith in this assumption  allowing generalization operators to proceed as if it were so  but enabling relatively graceful recovery when the assumption fails. 
	murray 	1 
   
   often the generalized exemplars of a disjunctive target concept themselves represent worthwhile concepts. for example  while learning about the disjunctive concept vehicle  multiple convergence also defines the concepts bicycle  sailboat  motorized vehicle  car  motorcycle  and ship. therefore  multiple convergence emerges as an approach to learning multiple concepts simultaneously. by defining a generalization partial-ordering over the new concepts  disjunctive concepts enable rich extensions to the representation language. 
acknowledgements 
   i would like to thank joe ross  ray bareiss and especially bruce porter for their many insightful and encouraging comments. 
