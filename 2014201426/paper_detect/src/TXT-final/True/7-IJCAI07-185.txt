
we present mboost  a novel extension to adaboost that extends boosting to use multiple weak learners explicitly  and providesrobustness to learning models that overfit or are poorly matched to data. we demonstrate mboost on a variety of problems and compare it to cross validation for model selection.
1 introduction
recent efforts in ensemble learning methods have shown empirically that the use of many methods and models is more powerful than using any single method alone. for example  caruana  shows how constructing an ensemble classifier from a library of around 1 models ranging from decision trees to svms produces classifiers that outperform the best of any single model. oddly boosting  schapire  1 - a particularly popular ensemble technique with strong theoretical and empirical support-is rarely used in such a fashion. in principle  boosting should be able to handle multiple learning methods automatically simply by using a learner that itself chooses among multiple weak learners. on the other hand  such a learner introduces additional complexity and may compound overfitting effects.
모in this paper  we present a new boosting algorithm  mboost  that incorporates multiple models into boosting explicitly. in particular  we extend adaboost  schapire and singer  1; freund and schapire  1  to act as an arbitrator between multiple models. our contributions are:
  a principled technique for extending boosting to enable explicit use of multiple weak learners.
  a technique for controlling weak learners that overfit or are poorly matched to the data  e.g.  embody poor domain knowledge .
in the sections that follow we first present mboost itself  comparing it to adaboost. we analyze the theoretical and practical consequences of mboost  and validate our analysis by presenting several empirical results  including a comparison of mboost to model selection via cross validation. finally  we situate our work in the literature and conclude with some discussion.
1 mboost
adaboost  schapire and singer  1; freund and schapire  1  is an ensemble learning technique that iteratively constructs an ensemble of hypotheses by applying a weak learner repeatedly on different distributions of data. distributions are chosen to focus on the  hard  parts of the data space  that is  where the hypotheses generated thus far perform poorly. if h1 h1 ... ht is a set of hypotheses generated by adaboost  the final boosted hypothesis is then: h x  = sign f x   =
  where 붸t denotes the weighting coefficient for ht.
모although described as a single learner  the weak learner could be a  bag  of several learning models. for example  rather than choosing between a decision tree and a perceptron as the weak learner  one could use a learner that contained both  presumably choosing the better for any given iteration.
모using such a composite weak learner requires some attention to detail. whether the bag chooses the best model or combines the models in some way  it introduces additional inductive bias: one is no longer boosting over a decision tree and a perceptron  but over a potentially complex ensemble learner that happens to include a decision tree and perceptron. to ensure that no additional inductive bias is introduced  we propose that the boosting algorithm itself should be the arbitrator  acting as the mechanism for choosing which model to use.
모the mboost algorithm explicitly supports multiple weak learners and formalizes the notion of boosting as the arbitrator. in each round  each weak learner proposes a hypothesis and mboost selects the  best  one. here   best  is determined by how boosting would reweight the distribution from round to round  allowing mboost to arbitrate among the weak learners without introducing any inductive bias not already intrinsic to the boosting framework.
모boosting is known to be susceptible to overfitting when its weak learner overfits. imagine for example  that the weak learner is a rote learner such as a hash table. the training error will be zero  but there will also be no generalization. no matter how many rounds are run  the rote learner will generate the same hypothesis  assuming there is no noise in the labels . as a result  the final boosting classifier would show the same  lack of  generalization capabilities. using multiple weak learners can only compound this problem.
boosting's susceptibility to this kind of overfitting lies in algorithm 1 mboost

require: weak learners b1 ... bm
data  x1 y1  ...  xn yn  where xi 뫍 뷌 yi 뫍 { 1 +1}
1. initialize d1 i = 1/n
1. for t = 1 ... t  1 do
1. split the data randomly into two parts dtraint and
dvalt.
1. train learners b1...bn with dtraint  generating hypotheses h1...hn.
1. choose hypothesis ht = argminzt  hi ... hn 
1. choose 붸t for ht per usual
1. update: dvalt+1 i = dvalt i e 붸tyiht xi  1. normalize dt+1 = dt+1/zt such that 뫉dt+1 i = 1
i
1. end for
t
1. return final hypothesis: h x = sign 뫉붸tht x  
모모모모모모모모모모모모모모모모모모모모모모모t=1 where 뷌 is the input space and zt is the normalization constant.

part in its use of training error for hypothesis evaluation. training error can be an overly-optimistic and poor measure of true performance. rather than avoid  stronger  learners  mboost divides the data every round into training and validation sets  using the latter to evaluate generated hypotheses. this approach yields a more accurate measure of the hypotheses' performance which in turn is used to choose the best hypothesis and its weight. additionally  mboost only reweights the distribution on data from the validation set. points from the training set are suspect as their prediction may be due to overfitting causes. reweighting these points may incorrectly imply that they have been learned. reweighting only the validation set avoids making such a mistake. note that the convergence of the algorithm does not change. all points will eventually be reweighted since points in the training set of one round may very well be in the validation set of the next.
모the combination of these two techniques means mboost has a much more accurate measure of the hypotheses' generalization errors. further  by being  inside  the bag of competing models  mboost can choose the hypothesis that directly minimizes the loss function. the net effect is that users can insert a variety of models into boosting to take advantage of the empirical power of using multiple models while mitigating the effects of poor weak learners that either overfit or embody incorrect domain knowledge.
모algorithm 1 shows pseudo-code for mboost. we note that while mboost as shown is derived fromadaboost  any boosting scheme can be similarly adapted.
1 analysis and implications
1 using multiple weak learners
boosting can be viewed as a coordinate gradient descent method that searches for the set of hypotheses which  when combined with their weights  minimizes some loss. in each round of adaboost  a hypothesis ht  the coordinate  is given  and adaboost optimizes the weight 붸t for that coordinate to minimize the loss function. an interpretation of this process is that given the coordinate  adaboost is taking the largest possible loss-minimizing step along that coordinate.
모when boosting is extended to multiple weak learners  not only must the optimal weight be chosen  but the optimal hypothesis  among a set of proposed hypotheses  must be chosen as well. in other words  we must find the best coordinate  the one with the largest component in the direction of the gradient  in addition to optimizing the loss-minimizing step.
모because we do not know our learning models a priori  we are precluded from analytical optimizations that find the best hypothesis. instead  we perform an exhaustive search. for each hypothesis proposed by the different models  we find its optimal weight and calculate the resulting loss. the hypothesis with the lowest loss is chosen. because the number of weak learners is typically small and computation of the loss requires only hypothesis predictions and not training  the cost of this step is low.
모applying this technique to adaboost  we see that the best hypothesis is the one minimizing ge yi f xi   where yi is the true label and f is the hypotheses ensemble classifier. the cost of this minimization grows linearly with the number of rounds run. over many rounds  it can become expensive. fortunately  minimizing g f  is equivalent to minimizing zt  the normalization constant in adaboost  in that round.
proof. in any particular round t  the update rule gives us:
dt+1 i = dt i e yi붸tht xi /zt 1 e뫉t  yi붸tht xi 
= 
m뫊t zt 1 e yi f xi 
= 
m뫊t zt
because zt must normalize dt+1 i = 1  we have: 1 zt =뫉dt i e yi붸tht xi 
i 1 =뫉dt+1 i zt
i
e yi f xi  1 =뫉 1  i m zj
thusn
argmin g f  = argmin 뫉e yi f xi  
i=1 1 = argmin e yi f xi   m	zj	i 1 = argmin zt  1 
모we note that in the mboost formulation  zt is defined somewhat differently than in adaboost. this is because mboost's use of a validation set changes the loss function; however  the difference is slight and the proof still applies.
모finally  it is worth noting that because mboost is choosing among multiple hypotheses  the weak learner requirement  in the pac sense  can be relaxed. the ensemble of learners must act as a weak learner but no individual learner must.
1 using an internal validation set
mboost needs an accurate error estimate for candidate hypotheses each round. following  langford  1   we model hypothesis error as a biased coin flip where the bias is the true error  the number of tails is the number of correct predictions and the number of heads is the number of incorrect predictions.
모the probability that one makes k or fewer mistakes in n predictions given that the true error rate is 붼 is given by the binomial cumulative distribution function: cdf k n 붼  =
.
모the largest true error rate r such that the probability of having k or fewer mistakes in those n predictions is at least 붻  is then: maxr r : cdf k n r  뫟 붻 . we will refer to this as the maximum reasonable true error: mrte k n 붻 . a bound on the true errorcan then be formedas: p 붼뫞mrte k n 붻  뫟 1 붻.
모in mboost  a hypothesis is only used if its mrte k n 붻  is less than 1. that is  a hypothesis is only used if the upper bound of its true error is less than 1  suggesting with high confidence that the hypothesis is better than random.
모recall that we can view boosting as a coordinate gradient descent method optimizing a particular loss function. use of the validation set for hypothesis selection  weighting and distribution reweighting essentially amounts to a different loss function:e yi 뫉t ht x i xi뫍/dtraint . mboost's loss function reflects our understanding that the real goal of interest is the ensemble's generalization error. we want to avoid being misled by atypical results on training data. use of the test set for evaluation allows mboost to effectively apply weak learners prone to overfitting  as a  strong  learner may do .
모one could easily imagine extending the use of a single internal validation set to perform full 1-fold cross-validation per round of boosting. we note however  that such a treatment aims towards finding the best weak learner  not the best hypothesis. in building an ensemble of hypotheses  our goal is to find the best hypothesis per round.
모finally  we note that internal validation is also mildly helpful when overfitting is due to noise. such error is a side effect of boosting's emphasis on  hard  examples  or in noisy situations  noise. if noise is non-systematic  mboost should determine that the generated hypotheses cannot predict the noise better than random. as we note in the next section  this will cause mboost to halt.
1 automatic stopping condition
it is natural for mboost to stop when none of its weak learners perform better than random. standard boosting uses training data for evaluation so it may be easier for weak learners to perform better than random than in mboost. in running our experiments  mboost usually stops due to this condition.
모for mboost to determine that none of its weak learners can perform better than random  a single round in which no generated hypotheses perform is insufficient as the training and validation sets are chosen randomly. to determine that the learners are indeed  exhausted   many consecutiverounds in which no hypothesis beats random is required.
모we note that in these rounds  the distribution is static. mboost is essentially performing an internal monte-carlo cross-validation on each of its weak learners. as we run more rounds  we acquire tighter bounds on each learner's true error. eventually  this will either show that at least one of the weak learners does better than random  in which case mboost will continue  or that none of the weak learners can do better than random  with some confidence   in which case mboost halts.
1 error bounds
mboost can be approximately reduced to adaboost. to see this  consider the weak learner l which when trained on data set dt in round t does the following:
  splits the data set into a training set and a validation set  dtraint and dvalt.
  trains each of its member learners b1...bm on dtraint  generating hypotheses h1...hm.
  simulates for each hypothesis the boosting reweighting on dvalt and chooses the hypothesis hbest t with the lowest hypothetical normalization constant z.
  returns  hbest t x i x 뫍/ dtraint   where i p  뫍 1 is the indicatorfunction that returns1 when the predicate p is true  and 1 otherwise. in other words  return h as a copy of hbest t x  except that when asked to predict on data it has been trained on it abstains.
모applying adaboost to the weak learner l  we construct a learner lada. this is essentially mboost. the sole difference is that mboost's final hypothesis is h x  =  instead of h; however  ht x  and hbest t x  differ only in predictions on a randomly-drawn finite set  namely  dtraint. thus  h and can differ only on points in the union of
dtrain1...dtraint  a subset of the overall training data d. in the limit  as the size of the instance space approaches infinity  the probability of predicting on a data point seen in training is vanishingly small. more formally  lim pr x 뫍 d = 1
|뷌|뫸
and therefore lim 1 where 뷌 is the instance space and x is a point randomly drawn from 뷌. further:
  the sum of all errors is bounded by the size of the training set: . x뫍뷌
 and h x  differ only on a randomly drawn  finite set  and the errors  are not systematic. in other words  and h x  represent the same decision boundary.
h and thus  lada is approximately mboost.
모this reduction now allows us to inherit the generalization error bounds that have been derived for adaboost. in particular  both the vc-theory based generalization error bound found in  freund and schapire  1   as well as the marginbased generalization bounds in  schapire et al.  1  and  schapire and singer  1  can be applied straightforwardly.
1 empirical evaluation
we performed four sets of experiments to examine mboost's performance.
1 effects of domain knowledge
this set of experiments focus on exploring mboost's basic ability to exploit multiple domain knowledge sets  as encoded by the weak learners; the validation set is not used. we used a 1d synthetic dataset composed of three gaussians whose points are labeled according to three different lines  one for each gaussian. two weak learners were used. the first is a simple perceptron. the second is a gaussian estimator combined with a perceptron. this represents the accurate domain knowledge case.
모as we can see in figures 1a and 1b  this is an easy dataset and both adaboost and mboost are able to approach perfect classification. mboost  however  by leveraging the additional domain knowledge in the second weak learner  is able to do so five times faster.
모to explore the effect of inaccurate domain knowledge  we replace the second weak learner with one that learns vertical bands of width 1. this hypothesis class can yield no helpful hypotheses  simulating poor domain knowledge. in this case  mboost discards the hypotheses generated by the second weak learner. mboost and adaboost perform identically from round to round.
1 mboost versus adaboost on individual learners
in this set of experiments  we explore the effect of boosting multiple models together  versus boosting each model individually. we want to determine if heterogeneous ensembles are superior to homogeneous ones. we performed the experiments on five of the largest binary classification uci datasets1. a set of twenty-five weak learners were used:
  five naive bayes learners. one uses empirical probabilities  the other four use m-estimates with m at 1  1  1  and 1. we used the naive bayes learner provided in the orange data mining package.
  five knn learners with k at 1  1  1  1  and 1. we used the knn learner provided in the orange data mining package.
  five c1 decision tree learners with minobjs  the minimum number of examples in a leaf  set at 1  1  1  1 and 1. all other options were set to defaults. we used r1 of quinlan's implementation.
  ten svm learners with c  the parameter for the cost of misclassification  set at 1  1  1  1  1  1  1  1  1  and 1. the svms used a rfb kernel. all other options were set to defaults. we used the libsvm library by chih-chung chang and chih-jen lin.
we compared two learners. the first  mboost1  is given all twenty-five weak learners and set to run for ten rounds. the second is a learner that runs adaboost for 1 rounds with

     1the full adult data set is quite large  we used a subsample of 1 examples.
table 1: cv accuracy for mboost and bestada
dataset/learnermboost1bestadap-valueadult111breast-cancer111e-1crx111horse-colic111ionosphere111e-1table 1: cv accuracy for mboost1 and bestcv-ada
dataset/learnermboost1bestcv-adap-valueadult111b-cancer 1.1.1.1crx 1.1.1.1horse-colic111ionosphere111each of the weak learners in turn  for a total of twenty-five runs . the best adaboosted model  according to their reported training error  is chosen and used. we call this second learner bestada. note that the computational complexity of the two learners are equivalent  they are o nm  where n is the number of rounds and m is the number of weak learners.
모table 1 shows the accuracy results on the five datasets. all reported accuracy scores use subsample cross-validation 1 times; that is  in each cross validation round we randomly split the data into training and validation sets with 1% in the training set. we use this variation of cross validation throughout our evaluation because it can be performedany number of times. we find that 1 rounds typically produce enough samples for our significance tests.
모we used one-way anova tests to determine if the reported accuracies are statistically significant  pvalue = f 1 . all datasets show mboost performing statistically significantly better.
모these results are quite surprising. detailed analysis shows that bestada suffered from significant overfitting. for example  adaboosted svm would report a higher accuracy rate than adaboosted c1 when in fact adaboosted c1 generalized better.
모one might imagine using cross-validation to avoid this problem. to this end  we create a third learner  bestcv-ada  that is the same as bestada but uses ten-fold cross validation to select the best adaboosted weak learner. note that this increases the computational cost of bestcv-ada by a factor 1. we present results comparing bestcv-ada and mboost1 in table 1.
모these results are in line with expectations. as commonly reported in the literature  the best single-model booster is quite effective. nevertheless  we see that mboost  using one tenth of the computational cost  can perform better on three of the five data sets and as good on the rest. heterogeneous ensembles appear to perform better than homogeneous ones.
mboost is able to take advantage of this effect and combine

     1additional cv rounds were performed to ascertain statistical significance.

	 a 	 b 
figure 1: comparing adaboost and mboost on gaussian dataset. left shows training accuracy and theoretical bound  rightshows training and testing accuracy. averaged over 1 runs
table 1: cv accuracy for mboost and mboostacc
dataset/learnermboostmboostaccp-valueadult111b-cancer111crx111horse-colic111ionosphere111the ensembles appropriately.
1 mboost as weak learner arbitrator
here  we perform a set of experiments to explore the effects of using mboost's loss function as a weak learner arbitrator. we compare two versions of mboost. the first is standard mboost using its exponential loss function as the arbitrator  the second  mboostacc using validation accuracy of the weak learners. both boosters are run for 1 rounds.
모table 1 shows that the two versions' performance are quite similar. only one dataset shows any difference that might be statistically significant with mboost outperforming mboostacc. in further exploration  we ran the same experiment but allowed both versions to run until exhaustion. in this experiment  all performance differences disappeared. this is unsurprising as the performance difference should be in terms of speed of convergence  not final asymptomatic performance. the exponential loss function is more aggressive  so it is reasonable that it might show some early gains.
1 mboost versus the best individual learners
here we perform a series of experiments comparing mboost with the best individual model as selected via 1 fold cross validation  bestcv-ind . we ran two versions of mboost  one running for 1 rounds and one with the automatic stopping condition  though never more than 1 rounds. mboost1 has the same computational complexity as selecting the best table 1: cv accuracy for mboost-1 and bestcv-ind
dataset/learnermboost1bestcv-indp-valueadult111b-cancer111crx 1.1.1.1horse-colic111ionosphere111table 1: cv accuracy for mboost1 and mboostauto
dataset/learnermboost1mboostautop-valueadult111b-cancer111crx111horse-colic111ionosphere111individual model via 1-fold cross validation. mboost-auto typically requires two to five times the computational cost.
모table 1 shows mboost1 performing better on one of the five datasets and equivalently on the rest. further  as we can see in table 1  the accuracy scores do not degrade even as we run mboost to exhaustion. mboost is quite resistant to overfitting. these factors combined with mboost's low computational complexity lead us to suggest it as an alternative to cross-validation for model selection. instead of trying many models and finding the best  we suggest boosting the models together and synthesizing an ensemble whole.
1 related work
mboost's use of multiple weak learners is similar to a boosting extension known as localized or dynamic boosting   moerland and mayoraz  1; meir et al.  1; avnimelech and intrator  1; maclin  1   where 붸t-the weight on the hypothesis in round t-is generalized to be a function dependent on the data  x. localized boosting can be viewed as a way to boost across two weak learners: one in the hypothesis and one in the 붸t x . our work aims to explicitly enable multiple weak learners in general  where any number of weak learners are allowable. mboost can emulate localized boosting by treating 붸t x ht x  as one weak learner  but we point out that-ignoring mboost's use of internal validation-both extensions are mathematically reducible back to boosting. the difference lies in that mboost makes boosting across multiple learners explicit. we hope that this formulation is easier to reason about  and that at the very least  provides a clearer mechanism for the user to apply multiple weak learners and the domain knowledge they embody.
모we know of no other work in boosting for controlling weak learner overfitting that is similar to our use of internal validation; however  there is general work in ensemble learning  again  see  caruana et al.  1  . our work differs in that it is integrated into the boosting framework  and so we leverage boosting's distribution perturbation and the use of hypotheses trained on different parts of the data space.
모mboost's ability to mitigate the effects of poor weak learners also makes it suitable for arbitrating between and merging different sets domain knowledge embodied in the weak learners. therehas been some priorwork focused on incorporating knowledge into boosting  eg.  schapire et al.  1 . their approach is to add a prior model  informed by domain knowledge  mapping each instance of the data to a probability over the possible label values. the boosting process is modified to also fit the prior. requiring a prior can be burdensome. further  if the prior is incorrect  performance will suffer. priors also offer no way to reconcile different sets of domain knowledge. mboost relaxes these constraints. weak learners provide a very flexible domain knowledge insertion point. mboost can further mitigate any negative effects should incorporated knowledge be inaccurate. finally  mboost can merge the knowledge sets into an ensemble whole.
1 conclusion
we have introduced a novel boosting algorithm  mboost  that explicitly chooses among multiple models  controlling for those weak learners that overfit or are otherwise poor model fits for the data. mboost takes advantage of results in ensemble learning while retaining the theoretical and empirical strengths of boosting. it provides us an effective way to manage and use multiple models and the domain knowledge they embody.
모empirical evaluations show mboost can outperform any single model and any boosted single model on some data sets and performs at least as well on the rest. furthermore  mboost provides a natural stopping criterion that appears robust to overfitting. favorable performance and mboost's low computational complexity also lead us to suggest it as an alternative to model selection via cross validation.
acknowledgments
we thank alex gray and john cassel who both provided useful pointers to hunch.net among other helpful discussions. special thanks to andrew watts for his help editing and proofing this paper. the breast cancer databases used in our benchmarks was obtained from the university of wisconsin hospitals  madison from dr. william h. wolberg.
