 
classifier systems are production rule systems that automatically generate populations of rules cooperating to accomplish desired tasks. the genetic algorithm is the systems' discovery mechanism  and its effectiveness is dependent in part on the accurate estimation of the relative merit of each of the rules  classifiers  in the current population. merit is estimated conventionally by use of the bucket brigade for credit assignment. this paper addresses the adequacy of the bucket brigade and provides a preliminary exploration of two variants in conjunction with enumerated rules and with discovery. in limited experiments  a variant that combines the bucket brigade  ''classifier chunking   and  backwards averaging  has yielded improved performance on simple maze problems. tentative similarities between this hybrid and sutton's adaptive heuristic critic  ahc  are suggested. 
     area b: fundamental problems  methods  approaches subarea b1: knowledge acquisition  learning  analogy 
1. introduction 
credit assignment is the problem of determining how to reinforce individual rules in a multistep chain when the external reward is given only at the chain's conclusion. some of the earliest work in credit assignment was samuel's celebrated checker-playing program  1 and 1  which used a heuristic version of temporal difference  td  methods  sutton  1 . these methods are similar in philosophy to the adaptive heuristic critic  ahc  developed by sutton  1  and barto  sutton and anderson  1   the bucket brigade developed by holland  1 and 1   and the learning systems studied by witten  1   booker  1   and hampson  1 . each of these provides a mechanism whereby adjustments to rule strength are made in an incremental fashion  in contrast to supervised learning with various backwards averaging schemes  grefenstette  1; widrow and stearns  1; holland and reitman  1; and rumelhart  hinton and williams  1 . 
1 	machine learning 
system credit assignment 
m a r k palmer 
gita rangarajan 
energy environment and resources center 1 research drive  suite 1 knoxville  tennessee 1 
     this paper studies credit assignment in an environment with minimal prior knowledge. initial investigation of three credit assignment methods has been undertaken in terms of their speed and accuracy in learning abstract state-space maze problems. the three methods studied are the bucket brigade  backwards averaging with classifier chunking  and a combination of the methods. the correspondence to td and ahc methods is suggested. 
	1 	classifier systems 
classifier systems to automatically discover rules to perform desired tasks were developed by holland and reitman  1  and later refined by holland  1 . in contrast to traditional expert systems where rules are handcrafted by knowledge engineers  classifier systems use the genetic algorithm as a discovery operator to generate rules. each classifier is an  if-then  rule  with a condition part and an action part. a message list is used to store the current environmental state and any internal messages. associated to each classifier is a numerical value called its strength. holland and reitman  1  adjusted classifier strength through backwards averaging and other central methods. the current credit assignment standard is the bucket brigade  holland  1 . 
     should the conditional part of a classifier match a message s  on the message list  the classifier pays a portion of its strength  its bid  for the privilege of acting  whereupon the consequent action  which may be either an explicit action or the posting of an internal message  is taken. in the simplest classifier system implementation  the bid payment made by the acting classifier s  is paid in equal proportions to classifiers acting in the previous cycle. should competing classifiers specify incompatible actions  conflict resolution is based on the magnitude of the effective bids  determined as the sum of the bid plus a random variable chosen from a distribution determined by a noise schedule . if the action results in an evaluation state  an appropriate reward or punishment is assigned to the acting classifiers of the current cycle. the  bid-payment  cycle is repeated for a predetermined number of cycles  whereupon the individual classifiers contribute to a gene pool in direct proportion to their strength and the genetic recombination operators are invoked. 

     since holland and reitman's work  several variants of classifier systems have been successfully demonstrated including solution of a difficult boolean discovery problem  wilson  1a   discovery of an optimal pumping schedule and automatic leak detection for gas pipelines  goldberg  1   and discovery of probabilistic scheduling rules for job shop scheduling problems  hilliard et. al.  1 . for additional descriptions of classifier system applications  the reader is referred to davis  1; and goldberg  1. 
1 	problem selection 
the test bed of problems for this study were one  two  and three dimensional mazes with 1 possible states and with a specified start and goal. the chosen representation encoded the states as lattice points in euclidean space. allowable actions at any state were to make a single step move parallel to the coordinate axes  subject to the constraint that the move did not go outside the lattice . upon attainment of the goal  the system was given a reward r and was reset to the start. 
     certain of the problems incorporated  barriers  at selected non-goal states. in one formulation  barrier states were all equally resistant to entry; in another  the barriers had relative  holes   states less resistant to entry. barriers were not explicitly represented; instead  classifiers responsible for system entry into barrier states had their strength immediately decremented. this challenged the system to learn to associate certain states with punishment either through internally developed messages  mapping expected rewards onto states  or simply through the credit assignment mechanism. the purpose of the barriers was to provide an experimental test to determine whether classifier systems could learn to pass through undesirable states  local rninirnums  to attain a global goal state  global maximums   and whether the systems could learn to distinguish between undesirable states of different intensities. 
1. credit assignment 
although even genetic algorithm and stimulus-response classifier system performance is affected by the reward structure  wilson  1a   reward and credit allocation issues become most apparent when rewards are delayed. such a delay in reward causes difficulties for the genetic algorithm  discovery  stage of the classifier system  since its success depends on accurate estimates of the relative merit of the classifiers. proper assessment of merit requires that the system frequently attain evaluation states and that the rewards and punishments be properly distributed to earlier stage setting classifiers. this must be accomplished rapidly without improperly restricting system exploration. 
1 	earlier studies 
credit assignment for  stage setting  classifiers has already been investigated by wilson  1b   grefenstette  1   and riolo  1 . wilson and riolo's studies have been limited  effectively  to noncompetitive chains of classifiers  riolo had one pair of competing classifiers  and did not study credit assignment in conjunction with genetic discovery. results suggest that the number of steps necessary to reinforce a classifier in a chain  i.e.  the number of cycles until classifier-strength had attained a fixed proportion of steady state strength  is a linear function of the number of steps that the classifier is removed from the reward state. 
     sometimes  even a linear function of the number of steps removed from the goal may be too slow for useful classifier chain development-riolo's results suggested that some 1 cycles were required for attainment of 1% of steady state for a classifier 1 steps removed from the goal. this potential difficulty has been recognized by holland  1  who conjectured that  bridging classifiers  would accelerate allocation of credit. riolo  1  has undertaken experiments that substantiate the potential usefulness of  bridging classifiers  once they have been discovered or manually injected. unfortunately  the authors know of no case where  bridging classifiers  have been discovered by the system  our experiments have discovered rule sets in which certain classifiers act at more than one point in a rule chain and speed the distribution of credit  but they are not of the form riolo suggests and their development will depend on the chosen representation . to address the apparent failure of the bridging classifiers to spontaneously develop  wilson  1b  has formulated the concept of hierarchical credit allocation  but that concept remains untested. 
in contrast to wilson and riolo  grefenstette 
 1  was interested in predictive accuracy of learning and not learning speed. he compared rudi  a variant of ls-1  smith  1   against the classifier system with the bucket brigade. he also provided experimental evidence that the bucket brigade leads to learning the next classifier's strength  whereas psp  the profit sharing plan - backwards averaging with no attenuation  learns the expected system reward along the current subchain. 
     this study differs from previous work insofar as it addresses speed of learning of optimal solutions  shortest paths start to goal; and in the case of barrier with hole  passing through the hole   both with fully enumerated conflicting rules and with discovery. 
1 	desiderata and techniques 
three heuristics motivate the credit assignment methods studied: 1. reward the rules that have acted to form the chain from the start to the goal. 1. draw the reward back from the goal to the start. 1. break cycles. 
     classifier reward is accomplished with three different mechanisms in this study: first  the conventional bucket brigade. second  a geometrically attenuated backwards averaging of the reward with classifier chunking. third  a combination of bucket brigade  backwards averaging  and classifier chunking. backwards averaging is implemented through the maintenance of a list of the classifiers that have fired since the last initialization of the task to the start. once 
	liepins  hilliard  palmer and rangarajan 	1 

the reward is attained each classifier in the chain is rewarded by the factor ¦Ëk  where the term a 1   a   1  is the attenuation factor and k represents the number of steps between the last firing of the classifier and the attainment of the reward. 
     classifier chunking passes the full reward  without attenuation  to all classifiers that have achieved a given percentage of steady state and link to the goal through 
1. experiments 
the environments chosen for the experiments reported here are 1 x 1 mazes. the mazes were used to verify the  sub  linearity of the bucket brigade credit allocation scheme as reported previously by wilson  1b  and riolo  1  and to compare the three credit allocation schemes - unmodified bucket brigade  backwards averaging with classifier chunking  and combination with classifier chunking. 
1 	implementation 
for every cycle of each maze  the classifier system is provided with the current state coordinates. the system matches these coordinates against the current population of rules and  through a stochastic conflict resolution scheme based on rule strengths and noise schedule  determines the next action. the action is taken  and the system is moved to the new state. since the state space is small  1 cells  in each of the mazes  all possible specific rules can be enumerated. if general rules  rules matching multiple states  are considered  then the size of the rule set becomes large under full enumeration and discovery becomes an important mechanism. in the discovery mode  the initial populations of classifiers were randomly chosen  and the genetic algorithm modified these populations as the search progressed. if at 
1 	machine learning 
 minimum subpath  steady state classifiers. any additional attenuation is begun from this point. cycle breaking  preventing  infinite loops  which do not produce useful actions  is accomplished by the assessment of an action tax.  this action tax has one additional benefit; it encourages the development of short chains.  these concepts are illustrated in figure 1  where 
some cycle  the current state matched the conditional part of no classifier in the population  the  cover detector  mechanism  robertson and riolo  1  was invoked. the cover detector generates a classifier that matches the current state by copying the state into the classifier's conditional side  randomly flipping some of the bits to  don't cares  and randomly choosing the consequent action. 
1 	parameter settings 
each system was rewarded 1 units when it reached the goal  and was penalized -1 units if it suggested an action which would take it out of the maze  i.e. it bumped into the wall-a system that bumps into a wall does not change its environmental state  it is simply penalized . a penalty of -1 was associated to each cell of the barrier. the bucket brigade used a bid constant of 1  that is  each rule that acts pays one tenth of its strength to the previously acting rule. an action tax of .1 was also assessed to discourage closed cycles and encourage shorter chains of rules. in contrast to the local payments of the bucket brigade  backwards averaging only occurs when the goal was reached.  the attenuation factor was 1. moreover  each rule is rewarded only once  according to its most recent firing in the chain-the recency heuristic  but not the frequency heuristic.  other parameter settings were tried; these seemed to provide the best compromise between speed and finding an optimal solution. 
     classifier chunking  chunked  the reward back through the maze as classifiers achieved 1% of steady state strength and linked to  minimal path  steady state subchains to the goal. finally  the combination method used both the bucket brigade and backwards averaging. 
1. results 
for the experiments with the bucket brigade and enumerated rules  steady state strengths were analytically calculated. in all other cases  a rule was assumed to be at steady state when the net gain or loss in strength throughout the traversal of the path was less than 1% of the current strength. in all cases  credit allocation was observed to be faster than a linear function of the distance to the goal as previously noted by wilson  1b  and riolo  1  in simpler experiments.  although it might seem that according to the empirical definition of steady state a rule could achieve steady state at one cycle and lose it on another  that was never observed to happen.  it was also observed that 1% of steady state was a conservative measure of chain development; repeatable  sub  chains were observed to develop far earlier. 
     the remaining results are highly tentative. full experimental designs weren't run; parameter settings weren't systematically studied  nor were all combinations of methods investigated. a more systematic study will be undertaken. 
1 m a t r i x maze problems-enumerated rules 
backwards averaging alone or in combination with the bucket brigade provides much faster feedback to the system than the bucket brigade alone. the barrier invariably slowed the learning for all the credit allocation mechanisms studied.  see figure 1.  a potential drawback to more rapid credit assignment is the loss of exploration. with backwards averaging or the combination method  the system sometimes settled on a chain of rules longer than the minimal 1 steps. also  in some limited experiments with the barrier with hole  the bucket brigade found the hole while the backwards averaging and combination runs tended to cross at higher penalties. 
1 	m a t r i x maze problems-discovery 
in comparison to the system with enumerated  specific  rules  convergence was much more rapid for the system with discovery of general rules. presumably  the speed up is attributable to generalization and not to discovery  although follow-up experiments to confirm this hypothesis have not been carried out. the ability to use a rule multiple times within a chain both shortens the effective length of the chain and provides earlier feedback to stage setting rules near the start. the bucket brigade can pass strength from a successor of a rule's instance late in the chain to the predecessor of the rule's instance early in the chain; therefore  the convergence curves are no longer monotonic. the pure backwards averaging 

method was not successful in finding minimal paths in the discovery environment  so that method is omitted from the results. moreover  none of the methods was highly successful with the barrier with hole problem. 
     the bucket brigade credit assignment method encouraged sets of rules which navigated the maze efficiently and avoided bumping into the walls  a serious problem for the combination method. using the bucket brigade  a hierarchy developed in which rules specific to rows and columns adjacent to the walls dominated more general rules specifying movement down or to the right. 
1. 	relationship to other problems and approaches 
the credit assignment problem is of wide interest in all machine learning research that deals with delayed rewards  as for example  game playing. the approach developed here is closely related to sutton's work leading to the adaptive heuristic critic and the method of temporal differences. in fact  the ahc approach currently is being incorporated into the classifier system framework as an extension of this work  brumgard  1 . 
	liepins  hilliard  palmer and rangarajan 	1 

1 	adaptive heuristic critic   a h c   
the mechanism of the adaptive heuristic critic  ahc  can be directly incorporated into the classifier system and generates counterpart behavior to the heuristic credit allocation mechanisms explored in sections 1 and 1 of this paper. let x t  be a  dimension 1  vector representation of the system state at time t; xi t  = 1 if the system is in state t at time t  xj t  = 1 otherwise. let c t  be the indicator vector for classifier firing; c  1  = 1 if classifier i fires at time t  and cj t  = 1 otherwise. let sc t  be the classifier strength vector at time t  and t a tax. r t  represents the environmental reward  positive  negative  or zero  received at state t. the predicted reward ps t  represents the prediction  made while in state s  of the reward at state t  the heuristic reward r t + 1  estimates the net gain  after the tax t  of moving from state t to state t + 1. the vector v t  is the reward association vector at the time t and provides the current estimate of the expected reward at each of the states. equation  1  defines the generic trace operator; the discounted average of past state visitations or classifier firings. the appropriate equations for the ahc formulation of the maze problems studied earlier in this paper are those for tirne-until-success tasks. 

     commonalities with the previously described approach include the following: 1. the term  - t  in the heuristic reward plays the counterpart role to a tax in the classifier system. 1. the heuristic reward r t + 1  estimates the net gain  after tax t  of moving from state t to t + 1.  the bucket brigade is the classifier counterpart to this  but for rules rather than states . 1. the strength update equation  1  affects a geometrically attenuated backwards averaging of reward  both from the goal state  and from the heuristic rewards.  the classifier formulation in sections 1 and 1 did not use a full trace and did not incorporate an heuristic reward into strength updates . 1. the reward association vector v t  in effect  draws the reward back   somewhat analagously to classifier chunking. 
     there are important differences between the combination method and the ahc. the ahc estimates expected reward at a state independent of classifier strengths  allowing the tradeoff between exploration and exploitation to be better controlled. small values for the learning parameter encourage exploration; large values encourage exploitation  sutton  1 . 
1 	machine learning 
     another difference is that classifiers are loosely linked by equation  1  of the ahc formulation and strongly linked by the bucket brigade. preliminary considerations suggest that an ahc - bucket brigade combination could provide a  second differences  effect that could help traverse local minima. 
1 	a h c success 
brumgard  1  reports success with limited experiments using ahc with the square maze. he reports performance comparable to the combination method in all fully enumerated cases and the discovery barrier/no barrier cases. for the discovery barrier with hole case his ahc implementation was unable to find the hole. sutton  1  used ahc to solve a different  but similar  planar maze problem with barrier and hole  with enumerated rules  wherein any suggested move into the barrier was reset to the previous state and the only effect on the ahc system  l - 1  was the incurrence of the single-step tax  -t  and increment in time. 
1. 	conclusions 
the incorporation of backwards averaging and classifier chunking into the classifier system seems to speed convergence; however  in the case of general rules  with discovery   this speed up may sacrifice exploration and generate less than optimal rule sets. whether this is a property of general rule sets alone  or if discovery somehow plays a role  is not known. nor is it known if the various parameters could be tuned to overcome these difficulties. in contrast  the bucket brigade does demonstrate the ability to assign credit in  sub  linear time  and seems to be especially competitive in the discovery setting with generalized rules and a barrier. theoretical considerations and limited experiments suggest that adaptive heuristic critic holds promise for hybrid classifier systems. finally  the acknowledgement is made that these represent a highly preliminary set of experiments and much remains to be learned about credit assignment. 
a c k n o w l e d g m e n t s 
the authors wish to thank richard sutton for many interesting and fruitful discussions. 
