 
¡¡¡¡we suggest for speech understanding systems a esseframe paraing strategy which devistes from 'pure' casefrsme parsing in st lesst two respects: paraing is not exclusively bssed on top-down instantiation of casefrases  and casefrases are merged 
before use with syntactic knowledge. our strategy has been developed to be executed m& an i n f e r e n t i a l process in which a set of knowledge sources cooperate through the blackboard. the knowledge sources are automatically defined by merging syntact i c and semantic knowledge expressed d e c l s r s t i v s l y i t h i s i n t e gration of ayntax rules and caseframes makes it possible to exp l o i t simultsneously both syntactic and semantic constrsints. 
	i. 	introduction 
¡¡¡¡psrsers for spoken nstursl language and parsers for typed natural language must meet different requirements. psrsers for speech work on a set of lexical hypotheses rather than on sequsness of words. an  unpleasant  consequence is that it is usually possible to find sequences of hypotheses not corresponding to the sequence of words actually uttered but which neverthelees constitute complete and correct interpretations. a psrser for speech must be able to work when some words l i k e s r t i c l e s and prepositions have not been recognized  and to decide whether two s l i g h t l y overlapping lexical hypothesee srs competing or not. for a l l these reasons  paraing a spoken utterance requires inferences as complex as those of empiric a l   inductive enterprises as expert systems and originates similar problems. 
casefrsme parsing has recently been proposed   hayes et s i . 
1    bristzmsnn  ehrlich 1   as an alternative to the methods developed during the arpa-sur project l i k e semantic grammars   hayes-roth 1   or networks   lowsrre 1  . having choaan the blackboard architecture  auitable to cope with the probleme of speech paraing aketched above  we have introduced two modifications to 'pure' caseframe paraing to uae it effectively into such a framework: i. alternating between top-down i n s t a n t i a t i o n of caseframes and bottom-up prediction of casefrsmea requiring a caaefiller already recognized; and l i . using caaeframes only after they have been merged with synt a c t i c knowledge. 
i i . a review of current research in caseframe parsing 
¡¡¡¡the basic notion of casse frame is thst of s hesd concept mod i f i e d by s set of related concepts   fillmore 1    tesnisrs 
* this reeearch has bean p a r t i a l l y supported by the eec esprit project n.1  advanced algorithms and architecture for signal processing   
** current adress of massimo poasio is project wisber  univeraity of hamburg  postsl p.o.box 1 d-1 hamburg 1 wast germany. 
1 	natural language 
o**  claudio rullent 
laboraton 	telecomunicazioni 
- 1 torino - i t s l y 
1  . each modifier plays s rols  case  with respect to the hesd concept. casefrsme paraing has been proposed long ago for typed nstursl language psrsers   hsyes csrbonell 1   and i t s a p p l i c a b i l i t y to parsers for speech has recently become an object of study   hayes et a l . 1    brietzmsnn ehrlich 1  . this strategy is promising because the interpretation is anchored to most significant parts of the input  and semant i c expectations generated from these more meaningful psrts can be used both to diacriminate between different candidate f i l l e r s and to hypothesize the meaning of troublesome frag-
ments. but current caaeframe parsers for speech are s t i l l close to parsers developed for typed t e x t   which causes two main problems. 
¡¡¡¡the f i r s t problem is thst the parsing strategy they use is top-down: in s f i r s t stage a l l potential caaeframe headers sre activated  then caseframes are ' i n s t a n t i a t e d ' by finding suitable f i l l e r s . this strategy would be j u s t i f i e d for parsing speech only if the recognizer would assign the beat scores to lhs corresponding to caseframe headers  verbs  common and proper nouns ; in t h i s csse the search would always follow the best directions. in a previous version of susy we discovered that this assumption is only p a r t i a l l y true. in i t a l i a n there are in fact words like  quale    which    what   or  dove    where   that very often have a better score then caseframe headers end can generate useful bottom-up predictions   dove   for instance  strongly prefers caseframes in which s loc esse is required . 
¡¡¡¡another problem of these systems is the integration of syntax and semantics. in the approach followed by   hayes et s i . 1    caseframes are augmented with syntactic constraints on case f i l l e r s and separate phrase structure rules sre developed to combine the resulting caseframes. another approach   brietzmsnn  ehrlich 1   is thst of developing different syntactic interpretations of frsgmsnts  thst sre then compered to find the one thst f i t s best. neither of these approaches is completely s a t i s f y i n g . our idea is t h s t   both for completeness and c l a r i t y   the grammar should be developed independently  using for instance phrase structure rules  and integrated only in a second moment with semantic knowledge  as noted by  hayes 
et s i . 1  t h s i r spprosch gets into trouble when the syntact i c complexity of the sentence increases . but we slso think it is necessary to reduce the size of the i n f e r e n t i a l a c t i v i t y   by applying both syntactic and semantic constraints simultaneously even when aggregating the smaller fragments. 
i i i . the susy system 
¡¡¡¡the speech understanding system  susy  understands and answers continuous speech queries about i t a l i a n geography. susy receives a l a t t i c e of lexical hypotheses produced by an independent word recognition module   laface et a l . 1  . 
¡¡¡¡we have adopted for susy the blackboard approach  very effective in such complex problem solving tasks   erman et a l . 1    and developed susy's parsing strategy by modifying 'pure' caseframe parsing to overcome the problems previously described and make it work in the blackboard framework. our 'impure' methodology can be summarized as follows: 
- a set of knowledge sources  kss  build and validate hypotheses about utterance fragments dedurtion instances  that are put on the blackboard. each 1 represents a sentence fragment at some stage of aggregation  and it is supported by a set of lexical hypotheses  the die are an elaboration of the hwim idea of islands  woods 1  . 1s have a score derived from those of the associated lhs by using the score combining methods described in  woods 1 . 
- only sentence fragments that can be characterized both synt a c t i c a l l y and semantically are aggregated. this way we can exploit syntactic and semantic constraints simultaneously even at the lowest levels of aggregation. syntactic and semantic knowledge are partitioned into kss following the principle that each ks owns the syntactic and semantic competence necessary to deal with a particular cla1 of sentence fragments. 
- top-down  expect at ion-based parsing a c t i v i t i e s are merged with bottom-up  predictive a c t i v i t i e s . an incomplete di  goal pi  can originate a search for missing components  causing the activation of the kss  and of those only  that deal with the classes of fragments that are required; vice versa  complete dis   i . e . covering completely a time interval  when put on the blackboard activate kss requiring a di of their class as a component  for instance  a complete casefiller w i l l activate kss building caseframes requiring that part i c u l a r case f i l l e r   . 
- a complete di d carries on an interpretation of the fragment it represents; this interpretation is used by other kss to generate interpretations of larger fragments having d as a component. an interpretation is not necessarily a case frame. also fragments whose interpretation is a c a s e f i l l e r or a 
d e f i n i t e description are aggregated: these dis can be shared by several die to avoid repeating the asme parsing steps more than once. 
- neither syntactic rules nor caseframes are used as such  but compiled to get the kss. syntactic and semantic knowledge are  however  expressed declaratively and independently developed. using this approach we get more f l e x i b i l i t y than 
either completely compiled systems like harpy or systems that make use of semantic grammars  while maintaining an e f f i ciency and a constraining a b i l i t y superior to those that can be obtained by using the two forms of knowledge independently and declaratively. 
iv. representations for syntax and semantics 
¡¡¡¡for syntactic rules we have used the formalism of dependency grammars   hays 1  . there are two advantages in using dgs: structures based on 'governors' and 'dependents' can be easily napped onto caseframes  which is not casual  as both dependency grammar and valency theory trace back to the work by  tesniere 
1    and parsing is strongly anchored to input since  for a dc rule to be applied  it is necessary to find a suitable candidate for the governor. 
   we have adopted conceptual graphs  cg   sowa 1  to represent caseframes. the primitives repreaentable in cgs are i n d i viduals and relations. each individual is characterized by a type and an individual marker  imarker for short . types are organized in a type hierarchy where any two types tl and t1 have a maximal common subtype 1 l * t 1 . an individual 1 conforms to type i if it is an instance of a type tl  = t. a conceptual graph is recursively defined a1 either a concept  or a concept connected to other concepts by conceptual relations  cr . the canonical graph  cagr   is a special cg associated to types to represent selectional r e s t r i c t i o n s . 
¡¡¡¡sowa's theory has the advantage of formal cagrs unification operations that conserve the selectional r e s t r i c t i o n s   for the purposes of this paper we are interested in three of these operations: by j o i n   two cagrs are unified over a 'common' concept  a concept appearing in both cagrs ; by type r e s t r i c t i o n   the type of a concept can be substituted by any of i t s subtypes; by referent r e s t r i c t i o n   a generic referent in a concept can be substituted by an imarker. 
¡¡¡¡caseframes are represented in susy as cagrs in which one concept  called thp head  represents the head of the caseframe: for instance  the caseframe we use for the predicate locateuin-region  one of the interpretations of the verb   t r o v a r a i     
 to be located ***  is represented by the canonical graph 
 located-in-region  
-xagnt obligatory -  mqunt*province+lake ; -  l1cobligatory -  regiqn . 
fig. 1 - the caseframe for locateu-in-rlgion 
in which cases are represented as crs. in this framework both caseframes connection and case f i l l i n g are reduced to join operations  since simple casefillers are represented as concepts   i . e . conceptual graphs without crb  and complex casef i l l e r s are caseframes themselves. 
v. combining caseframes and syntax to define knowledge sources 
¡¡¡¡all kss share a common body of procedural knowledge  routines to check temporal constraints  functions that compute the score of a d i   ; they a l l include knowledge about syntactic and semantic constraints on case f i l l e r s   and must return an interpretation  that is always b u i l t using joins and restrictions only. the behaviour of a ks is then completely defined by spec i f y i n g l. the class of dis it can aggregate  n. i t s a c t i vation conditions  i n . the set of constraints and i v . the way caseframes must be instantiated to get the interpretation. 
¡¡¡¡since only fragments that can be classified both from a semantic and from a syntactic point of view are aggregated  a 
¡¡¡¡di d could be classified twice: with a syntactic category c 
 syntactic categories are types in the type hierarchy  and with a type t of the domain. the type that is effectively used to classify d is therefore y = c*t  for instance  the ks above w i l l produce dis of class verb*l1cated-in-regi1n . the d e f i n i tion of ks1 in fig. 1 includes a description of the compositional structure of the fragment in terms of semantic types  that is used together with the  isomorph  compositional structures of associated syntactic rules for expectations and predictions purposes: the ks above  for instance  w i l l aggregate sequences whose f i r s t element is of class adv*regi1n or noun region  the second element of class part-rifl*jolly or 
be*xlly  etc. the type jolly is used to classify words whose -
     since in our domain there are restrictions on what entitib1 can be contained in others  an island can only be located in a sea  for instance  and representing these restrictions in cagrs would lead to uneceasary complexities  we have introduced d i f ferent subtypes of located for each 'containing' e n t i t y . 
	poesio and rullent 	1 
ayntsctic c l a s s i f i c a t i o n only is relevant  and that can eventueily be skipped . 
 defks ks1 
{; compositional constrsints 
located-in-region   region jolly  header  mounuprovince+lake 
h associsted syntactic rules 
 dr1 dr1s 1b dr1c or1d or1e  
;jverb a noun part-rifl  g1vern1r  noun 
;;ver1   noun be  q1vernor  noun ;;ver1 = noun part-rifl  g1vernor  proper-noun t;verb s adv part-rifl  g1vernor  noun uverb s aov be  g1vernor  noun 
;;verb s aov part-rifl  g1vern1r  proper-noun 
;; act ivet ion condition 
g * x  s*  action   x located  
;; casefraaes instantiation rule 
 ris  located !   agnt  z 
loc * y    z  
ris recion  action !    x  1   
ris mounuprovince*lake  action !   1w 1y    
f i g . 1 - ks d e f i n i t i o n 
¡¡¡¡ks d e f i n i t i o n s ere the result of e compilstion. they are produced froa dependency rules  ceseframes and mopping rules. for example  the d e f i n i t i o n of ks1 hee been obteined froa the set of aeeociated dependency rules in f i g . 1  one of which le shown in f i g . 1 with i t s mapping rule   f i g . 1.b   and froa the caaefraae in f i g . l . 
s.  dr1  verb 	noun part-rifl  g1vernor  now  
verb  modo  indic    tempo  pres    pers  1   
 num !  n   trans  vit   
 complemento !  stat1-lu1   
 rifl  rifl   
now  t-compl !  stat1-luog1   part-rifl nil 
noun  num i  n   
b.  mr1   located-region located-province wash  * 
loc  jolly   head€r  agnt   
fig.1 - a dependency rule with i t e mapping to caeefranes. 
¡¡¡¡the compilation is f a c i l i t a t e d by the atructural s i m i l a r i t y between dependency rules and caeefranes  both are eeaentielly baaed on the idee of e 'head' modified in soae wey . the compositionel part of the ks le derived from that of the dependency rule by substituting syntactic types with the velue r e s t r i c t i o n s on ceee f i l l e r s specified in the caaefraaei the position of esses in the fregnent la speciflsd in the napping r u l e . only those caaaa that can be r i l l e d uaing infor met ion present in the fragment mre included; thie pre-eelection evoide searching for information that could not be found. the activat i o n conditions are aeeociated to typaa in the knowledge baee. 
¡¡¡¡having partitioned the knowledge in independent chunks  when a new ayntactic rule is added it is never neceessry to recomp i l e the entire eet of kset either a previously defined ks la modified to take into account the new r u l e   or new ksa are added to the existing eet. in this l a t t e r caee  since not a l l eyntactic/eeaentic combinations are admissible  the number of ksa that are generated is l i m i t e d . the cese in which s new type 
1 	natural language 
is added to the domain has more aarioua consequences: with our currant aat of about 1 dependency rules in the average about 1 new ksa ara defined whan adding a new entity type. these problems ara typi-cal of semantic grammars; in our caaa there is the advantage that it la the system i t s e l f that generates the new ksa. 
v i . casefrance instantiation 
¡¡¡¡the aaaning of a word in the dictionary is a aat of predicates able to activate the appropriate ksai for instance  the 
meaning of the word 	m trova  includee the predicate 
acti1n tr1va located  
that activates ks1 of fig.1. that ks includes a cssframe i n s t a n t i s t i o n rule that produces the inatantiated casefremee constituting the interpretation of the sentence fragments it analyzes. by t h i s approach  information duplication is avoided  ainee a single csseframe is shared by many words  and caaefreams instantiation is more e f f i c i e n t   since the inatantiation rule specifies 'a p r i o r i ' how the caseframe associated with the header must be joined with those aasociated with the component ois. 
¡¡¡¡only two operations ara needed to build the interpretation: referent reatrictiona are used to f i l l concepts  and joins are uaed to merge concepts representing cases with canonical graphs representing c a e e f i l l e r e . repreaenting a canonical graph aa a term and generic referents aa logic vanablee  joins can be effectively implemented aa u n i f i c a t i o n oparationa using a prolog interpreter after a l l components indicated in the composition rule have been found. the instantiation rule of ks1  reproduced in fig.1.a  together with the cgs carried by dia of class hoimt+province+lake  1.b  and region  1.c  producaa the inatantiated caaeframe in   1 . d   . this unification haa a vary low coat  since the interpreter runs in a spacial context in which only the results carried by accepted components  the 
n	n
r i a predicatea in fig.1  have been aaaartad. 
fig.1 - caaeframe inatantiation 
v i i . integrating top-dowi and bottom-up caseframe parsing 
¡¡¡¡suay always moves from the best lexical hypothesis or deduct i o n inetanee. a top-down  expectation baaed a c t i v i t y is started if a caaefraae header is selected} a bottom-up predict i v e a c t i v i t y is i n i t i a t e d whan a di representing a c a a a f i l l e r or a lh repreaenting a case marker are sslscted. 
let us consider the sentence  translated word by word t 
in quale regione si trova il monte rosa  
  i n which region le located mount rosa   


