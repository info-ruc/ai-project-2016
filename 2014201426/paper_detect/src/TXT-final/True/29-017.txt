 
this paper presents an interactive method for building a controller for dynamic systems by using a combination of knowledge acquisition and machine learning techniques. the aim is to build the controller by acquiring the knowledge of an operator skilled at that task. this method has been demonstrated for the skill of learning to fill an aircraft in a flight simulator. the simulator has been augmented to interact with a knowledge acquisition program for creating rules and logging the pilot's actions along with flight information. we have developed a method called dynamic ripple down rules for knowledge acquisition and learning dynamic ripple down rules for automatically generating rules from the logged data. the rules were tested by running the flight simulator in autopilot mode where the autopilot code is implemented by the rules. 
1 	introduction 
in this paper  we report on an experiment that demonstrates how a combination of knowledge acquisition and machine learning techniques can be used to build a controller for a dynamic system. the aim is to build the controller by acquiring the knowledge of an operator skilled at that task. to demonstrate the effectiveness of this approach  a flight simulation program has been modified to interact with a knowledge acquisition program. a method  called dynamic ripple down rules  drdr   has been developed to acquire knowl edge by interaction with the pilot and another system  learning dynamic ripple down rules  ldrdr   has been used to generate rules automatically from data logged during a flight. the results have been tested by running the flight simulator in autopilot mode where the autopilot code has been replaced by the generated rules. 
   due to complexity or lack of information about a plant  it is often difficult or impossible to construct a controller using classical methods. however  a competent human operator is often able to control a dynamic system. as a result  there is growing interest in mimicking the skills of the human operator  michie et al  1; sammut  et al 1; shiraz & sammut 1b; urbancic & bratko; 1 . 
creating a set of rules that models the strategy used by the operator to control the task is one way to do this. the problem is how to determine a set of rules that captures the expert's strategy. experiments have shown that it is often difficult for an expert to describe his or her strategy and the reason for choosing a particular strategy  compton & jansen  1 . sometimes it is even impossible for them to describe their strategies  especially when they are control ling a fast dynamic system such as an aircraft. the reason for this is that many skills are performed at a sub-cognitive level. these skills can be demonstrated  but are very hard to describe explicitly. moreover  descriptions may be incomplete and approximate. therefore  such descriptions can not be translated directly into an automatic controller. however  operator descriptions may contain general information which might be used as guidelines for constructing a controller automatically or semi-automatically. 
   in this paper  we propose a new interactive method  in which the expert  operator  and a learning program co-operate with each other to create the controller. the expert's description about his or her task is considered to be a set of general rules for controlling the system. these rules are refined by adding rules that are automatically created from the logged behaviour of the expert. 
1 	previous experiments 
michie et al  1  in their pole-balancing experiments introduced the use of machine learning for learning to control dynamic systems. this method was later called behavioural cloning. in behavioural cloning  an operator skilled in a task is asked to control the system. during his or her performances  the state of the system  along with his or her actions  are logged into a file. this process is repeated several times until enough information is collected. in the next stage  a learning program uses the logged information to construct control rules for the system. behavioural cloning was further refined by sammut et al  1  in their 'learning to fly' experiments and the technique has also been extended to other domains  michie & camacho  1; shiraz & sammut  1a; urbancic & bratko  1; 
esmaili et al  1 . 
   in addition to performing behavioural cloning experiments  in their 'container crane controller' work  urban lit & bratko  1   also recorded the advice that an operator 

would give to a novice. they asked six volunteers to learn to control a crane simulator. after the operators had mastered the task  they asked the operator to write down their advice. they also encouraged them to discuss their experience and their written instructions with other operators to see if such discussions lead to improved performance. 
   one of the goals of this experiment was to see if there was any correspondence between the induced clone and the operator's instructions. it was found that the induced clone can uncover some of operator's subconscious control skill. they also reported how the verbal description of the operator can be used to improve the induced rules  when the induction program has failed to capture the behaviour for a particular condition. however  there was no systematic way of using the human operator's advice as background knowledge for machine learning technique. moreover  often there was considerable divergence between the operator's description of the control strategy and the rules created by induction on that same operators performance. 
   another approach to using an expert's advice during behavioural cloning is to combine a knowledge acquisition technique with induction  shiraz et al  1c . while most of the skills involved in controlling a dynamic system are subcognitive and are therefore inaccessible to introspection  there are some aspects of those skills that may be explainable by an operator  particularly those involving high-level knowledge. by providing a suitable knowledge acquisition tool  this kind of knowledge can be documented in term of rules. as a result  the induction can be assisted to produce more transparent and robust clones. 
   they chose to modify compton's  1  ripple-down rule  rdr  method for knowledge acquisition. rdr's provide a simple and powerful method for building a knowledge base interactively. the initial knowledge base usually consists of a simple rule for specifying what to do in the default case. when a rule is found to fail for a particular case  an exception rule is appended to the failed rule. the conditions required in the body of the exception rule can be easily determined by comparing the new case with cases that were previously handled successfully by the original rule. 
   in previous work  shiraz & sammut  1a  a controller that for flying a simulated aircraft through a predefined flight plan was constructed using a combination of rdr's and gaines'  1  induct program. the use of an expert's advice resulted in the creation of rules that were more transparent and robust than those created by induction alone  sammut et al  1 . a major limitation of this work was that knowledge acquisition and machine learning were two separate tasks. in that experiment  following sammut et al  1   the flight was divided into seven different stages based on the predefined flight plan. for each stage  four separate rdr's for each of the four control actions  elevators  flaps  ailerons and throttle  were created. each of these twenty eight rdr's was constructed either by using the rdr interactive knowledge acquisition method described above or by applying induct to the logged data. in the work presented in this paper  we propose a method that merges knowledge acquisition and machine learning. 
1 	the experiment 
the flight simulator used in this experiment is one available on silicon graphics workstations. the pilot's task is restricted to flying a cessna 1 through a predefined flight. the original program also gives the user the choice to fly different aircraft. the simulator was modified so that the pi lot can run a trace of a flight and using the knowledge acquisition facility  diagnose and correct any incorrect decisions. these modifications will be described in more detail later. 
   the strategy a pilot uses for landng an aircraft is quite different from the strategy needed 'for straight and level flight. sammut et al  1  divided a flight into different stages according to the flight plan. the present experiments follow the same approach and use the same flight plan: 
1. take off: 	take off and fly to 1 feet. 
1. level out: fly straight and level to a distance of 1 feet from the airport. 
1. turn right: at a distance of 1 feet 1  turn right to a compass heading of 1 degree. 
1. turn left: at a north/south distance of 1 feet  turn left toward the runway. 
1. lining up: change the aircraft's heading to line up on the runway. 
1. descending: descend to the runway. 
1. land: 	land on the runway. 
a modification of the original rdr knowledge acquisition method  dynamic ripple down rules  drdr   is used for those parts of the flight where it was possible for the pilot to verbalise his or her strategy and  thus  write rules. however  some parts of the flight involve such subtle control strategies that it is very difficult  or even impossible  to verbalise those strategies. in those cases  ldrdr's  learning dynamic rdr's   an automatic rule generator  produces rules from the data logged data during a flight figure 1 illustrates the architecture of the system. 
1 	dynamic ripple down rules  drdr  
the basic form of a ripple-down rule is as follows: 
if condition then conclusion because case except if condition then conclusion because case except if... 
else if ... 
initially an rdr may consist of the single rule: if true then default conclusion because default case 
1  distances were chosen to correspond to features in the simulators landscape. 


figure 1: the basic algorithm for learning to control a dynamic system using a combination of knowledge acquisition  drdr  and machine learning  ldrdr . 
that is  in the absence of any other information  the rdr recommends taking some default action. for example  in a control application it may be to assume everything is normal and to make no changes. if a condition succeeds when it should not  then an exception is added  ie. a nested if-statement . thus the initial condition is always satisfied so when the 'do nothing' action is inappropriate  an exception is added. if a condition fails when it should succeed  an alternative clause is added  i.e an else-statement . the new condition in the exception or alternative clauses is easy to determine. 
   with each condition/conclusion  rdr's store the cornerstone case  i.e. the case that caused the new condition to be created. when a new cases is incorrectly classified  it is compared with the cornerstone case of the incorrect condition and the differences are used to construct the new condition. usually  the difference list is presented to the expert so that he or she may select the most relevant differences or generalise the conditions. 
　　originally  rdr's were developed for classification tasks such as medical diagnosis. to facilitate knowledge acquisition for controlling dynamic systems  additional facilities have been added: 
multiple knowledge bases: a pilot must often perform severl actions simultaneously  e.g.  to turn the aircraft  the elevators and ailerons must be adjusted at the same time1. 
drdr can manage several knowledge bases simultaneously. in this case  there is one for each of the four control actions. 
new conclusions can be entered graphically. the user interface for a knowledge acquisition tool must be tailored for the application. for example  rules presented as text have little meaning for pilots. therefore a graphical interface that provides analogues to an aircraft's instruments are important. 
transferring information from flight simulatot to drdr: the pilot can pause a flight to investigate the rdr when it is not flying according to the pilot's wishes. when this happens information must be transferred from the flight simulator to the knowledge acquisition tool. much of this information is then presented graphically. 
1 learning dynamic ripple down rules  ldrdr  
the manual knowledge acquisition of drdrs is effective when it is possible for the expert to articulate rules about his or her performance. however  in many circumstances  it is very difficult for an expert to describe the control strategy. often  attempts at such descriptions are incomplete and approximate and cannot be translated directly into an automatic controller. however  these descriptions may contain general information that can be used to guide the search that a machine learning system may conduct when trying to automatically build control rules from performance data. 
1.1. data preparation 
the logged data usually contain information from different stages. these data are usually noisy and contain many redundant records. pre-processing prepares the logs for the learning program. the pre-processing includes: 
segmenting the data: the data may contain information about a complete flight or only those parts of a flight where the pilot decided to record additional information. the first stage of pre-processing is to segment the data into the stages of the flight plan. 
discretising control values: the cessna aircraft's control system consists of three control surfaces  elevators  flaps and ailerons  plus the throttle. the values of the control variables should be continuous. however  the learning algorithm can only deal with discrete class values  therefore the control variable values are discretised. in practice this is easy because the variables are only recorded to a limited precision and are  therefore  already discrete. 
to simplify the task  the rudder is ignored. of course  this would not be appropriate for a real aricraft. 
eliminating spurious values: it is useful to eliminate intermediate values of the state variables so that the learning program is not swamped with spurious data. for example  when there is a change in elevator setting  eg. from 1＜ to 1＜  all values in between are considered redundant and removed. note that this preprocessing is vital  otherwise the number of rules generated by the learner will be extremely large. 
creating separate input for each control action: the data file from each stage contains information about all the manoeuvres performed by the pilot during that stage. four separate files for each control action  elevator  flaps  rollers  throttle  are created for input to the learn ing program. to create a file for each control action  the attribute describing the control action is treated as the class variable and the rest of the attributes including attributes describing other control actions are treated as ordinary attributes. 
1..1. the learning algorithm 
the ldrdr algorithm constructs a controller as follows: 
inputs: current knowledge base; the behavioural traces; the priority list. 
foreach record in the behavioural trace: 
1. test the next record against the knowledge base 
1. if the conclusion of the rdr differs from the recorded trace  create a new rule to correctly handle the new record. 
the condition part of a new rule is constructed by examining those variables which change most in correspondence with changes in control actions. the behavioural trace includes all the information that a pilot would normally see in the aircraft instruments. ldrdr tries to relate the pilot's actions to changes in the instrument readings and thereby predict what actions are required depending on the state of the instruments. 
　it is clear that a human expert only considers a relatively small number of variables at any time. to emulate this behaviour  ldrdr limits the number of conditions per rule. in order to avoid missing the important conditions in rule generation  ldrdr maintains a priority list of attributes for each control action. this list can be provided by the expert or created by the system. during learning this list is updated automatically by considering attributes that contribute more in rule generation. 
　after performing the pre-processing described previously  each of the data files and the existing rdr for a particular control action are used to extend the rdr using the ldrdr algorithm. ldrdr also uses the current priority lists. 
the algorithm extends the rdr as follows: 
foreach attribute in the priority list: 
1. compare the attribute's previous direction with its next direction. if there is a change in direction  e.g. it was increasing and becomes steady  then: 
1. create a test for the attribute. the test is based on the attribute's current value and its previous direction. the test always has the form: 
　　　　　　　　attribute op value where op is   =  if the previous direction was increasing and   =  if it was decreasing. value is the value in the current record. the new test is applied to the cornerstone case associated with the last rule that was satisfied to make sure the test is correct for the current case but excludes the cornerstone case. 
1. add the test into a condition list. 
1. attributes in the priority list are ordered by a numerical score. increment the attribute's priority. 
1. if the number of tests in the condition reaches a user defined maximum  scan the rest of the attributes and just update their priorities if their direction has changed. the maximum is domain dependent and was set to 1 for these experiments. end loop 
if the condition list is not empty  create a rule and add it to the rdr. the conclusion of the rule comes from the action recorded in the trace. the current record becomes the rule's cornerstone case. the rule will be added as an exception to the last rule in the rdr if that rule is evaluated true. it is added as an alternative if false. 
the output of ldrdr is an extension of the original 
rdr to cover cases in the input data that were not covered by the original rdr. the new rdr is converted into c ifstatements by recursively traversing the rdr and creating one if-statement for each rule. an if-statemenf s conditions are the conjunctions of all true conditions in the rdr from the root to the rule. 
1 	rule construction 
the learning task begins with some simple rules created by 
drdr or by using ldrdr  applied to logged information. in both cases  the rules can be tested by running the simulation in autopilot mode where the autopilot code is derived from the rules. during the flight  if the aircraft does not behave as the pilot would expect  the pilot is able to pause the flight and trace the executed rules or the rules currently under execution. the pilot is also able to modify existing rules that seem incorrect by adding new rules. in this case  the previous and current situations of the aircraft  plus all the relevant flight variables  are presented to the pilot. as well  all the rules under execution for each control action are reported to the pilot. whenever the flight is paused  there are several options available. each of these options is described in the following sections. 
1 	analysing rules 
information about the knowledge structure in the system is presented graphically to assist the pilot in understanding the current state of the knowledge base. the pilot may view the rdr either in text form as rules or graphically as a tree 

structure drawn on the monitor. the tree may be traversed by moving the mouse over the tree. 
1 	creating new rules 
after evaluating the existing rules  if the pilot finds a conclusion is wrong or there is no interpretation for the current situation then he or she is able to add new rules. to help the pilot in creating new rules  the system provides a list of differences between the current state of the flight and the situation associated with the last condition that was satisfied. choosing one or more variables from this list guarantees to produce a rule that will correctly interpret the new case but not the old one. 
1 	flying the aircraft 
this option is useful for the pilot to explore new flight plans or become familiar with existing stages. it can also be used by new pilots to get acquainted with the program. moreover  during the learning task  pilots can use this option to fly some part of the flight manually and then put the aircraft in a specific state to investigate the behaviour of the autopilot at that situation or start recording data from that position. 
1 	logging flight information 
for those parts of the flight that are difficult for the pilot to diagnose a problem or suggest correct rules  he or she can simply show the correct action by switching to manual mode and flying the aircraft. during the flight  the pilot's actions  along with the flight information are logged to a data file. the flight's information is logged every second or when a change in a control action has been detected. these data can then be used to construct rules using ldrdr. 
1 	flying on autopilot 
in this mode  the flight simulator is controlled by an autopi lot built by the system. the autopilot code is derived from the rules that have been created by the pilot during his or her previous flights and ldrdr from the logged data. this is used to test the rules. 
1 	constructing an autopilot 
in this section we describe the results of a series of experi ments with this system. 
　after every modification of the rdr  the flight simulation was run in autopilot mode to test the new rdr. to do this  the code of the original autopilot was replaced by the rdr  translated into c . a c function is also incorporated into the flight simulator to determine the current stage of the flight and when to change stages. the appropriate set of rules for each stage is then selected from four independent if-statements created in each stage for every control action  sammut et al  1 . 
   in the following section we describe a controller built for the first stage to demonstrate how these rules operate. 
please note that these are the rules created by one pilot 
 either manually or by cloning . another pilot would almost certainly create slightly different rules. 
stage 1 
the rules for this stage are shown below  after translation to c: 
elevators: 
if  airspeed  = 1  return level pitch; if  elevation   1  return pitch up 1; if  elevation   1 && y feet   1  return pitch down 1; 
if  elevation   1  return pitch up 1; if  elevation   1  return pitch down l; if  elevation   1  return pitch down 1; return levelpitch; 
ailerons: 
if  azimuth  = 1  return left roll l; if  azimuth  = 1  return left roll1; if  azimuth  = 1  return right roll l; if  azimuth  = 1  return right roll l; return no roll; 
flaps: 
if  yjeet  = 1  return full flaps; if  y feet  = 1  return half flaps; return no flaps; 
throttle: 
return throttlc 1oo; 
the rules for the elevator indicate that if the speed of the aircraft reached 1 knots  pull back on the stick to take off until the elevation increases to 1＜  values in the rules are shown in tenths of degrees . then push the stick forward to reach an elevation of 1 degrees. after that try to maintain the degree of elevation at 1* which is close to the value usually obtained by the pilot. when the aircraft reaches an altitude of 1 feet decrease the elevator to get ready for the levelling out. pitch-up-1 indicates a large elevator action and pitch-up-1 means a gentle elevator action. these elvator action names result from the discretisation of the actions  menioned previously. 
the last statement is the default rule. it is the default action in that stage for the particular controller. for example the default rule for the elevator in all stages is level pitch. use of a default rule  results in a reduces number of rules. 
   the flaps rules states that as the aircraft reaches an altitude of 1 feet  decrease the flaps by half and if it reaches an altitude of 1 feet raise the flaps completely. the rules for the aileron keep the aircraft in the straight line. during take-off  the pilot who created these rules always applied full throttle. 
1 	testing the system 
the system tested using three volunteers: one of them was familiar with the system and another one was familiar with flight simulators. their task was to create a set of knowledge bases using drdr and ldrdr that can 
successfully complete the flight plan  described previously 

   prior to the experiments  the subjects received a one hour tutorial in the use of the system. they also attended a demonstration. during the demonstration  the demonstrator explained how he flies the aircraft and the subjects were allowed to ask questions about the flight and the plan. the were allowed to practice until they become proficient in flying the aircraft. 
   during the experiments  the number of times that subjects interrupted the flight simulator  number of rules they created  the frequency use of drdr and ldrdr and the amount of time they spent creating rules were recorded. 
　　all the subjects were successful in creating a set of rules that could fly the aircraft through the given flight plan. the size of knowledge bases varied considerably from one subject to another one. one subject was cautious in creating rules for any possible state of the aircraft and thus produced a large rdr. another subject was only interested in writing a set of rules that could fly the specific plan and so produced a much smaller rdr. 
　　an interesting observation during these experiments was the reuse of knowledge. all the subjects tried to use rules created in an earlier stage if the task was similar. in particular  when creating rules for the elevator in stages three  four and five  they used the rules created for stage two and added some exception rules  if necessary. 
   another observation was that the subjects began trying to use drdr to manually construct rules  but when this became difficult  the subjects switched to ldrdr to build rules. 
1 	conclusion 
we have demonstrated that knowledge acquisition and machine learning techniques can be combined to successfully build control strategies for a dynamic system. we believe that this approach is applicable to other  similar domains  such as the container crane  urbancic & bratko  1  and others. in using  heuristics  such as limiting the number of conditions in each rule  we have followed the observation that experts rarely consider more than a few attributes when making a decision. since we are trying to emulate the human trainer  it is reasonable to impose similar restrictions on the computer program. 
　　all the testing up to this point has been in a noise-free simulation environment. the same technique should be able to handle noise  but further work is required to establish this claim. further work is also required in improving the user interface. ideally  pilots should never have to see any rules  but should be able to interact with the system entirely through the familiar surroundings of the cockpit. we also need more rigorous testing to measure the relative merits of ldrdr versus the more conventional induction programs previously used in behavioural cloning. 
acknowledgment 
we thank paul compton and philip preston for their help in understanding and using rdr's in this project. 
