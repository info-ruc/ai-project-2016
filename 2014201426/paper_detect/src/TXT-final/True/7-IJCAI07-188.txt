
machine learning for predicting user clicks in webbased search offers automated explanation of user activity. we address click prediction in the web search scenario by introducing a method for click prediction based on observations of past queries and the clicked documents. due to the sparsity of the problem space  commonly encountered when learning for web search  new approaches to learn the probabilistic relationship between documents and queries are proposed. two probabilistic models are developed  which differ in the interpretation of the query-document co-occurrences. a novel technique  namely  conditional probability hierarchy  flexibly adjusts the level of granularity in parsing queries  and  as a result  leverages the advantages of both models.
1 introduction
predicting the next click of a user has gained increasing importance. a successful prediction strategy makes it possible to perform both prefetching and recommendations. in addition  the measurement of the likelihood of clicks can infer a user's judgement of search results and improve ranking.
¡¡as an important goal of web usage mining  srivastava et al.  1   predicting user clicks in web site browsing has been extensively studied. browse click prediction typically breaks down the process into three steps:  1  clean and prepare the web server log data;  1  extract usage patterns; and  1  create a predictive model. there have been a variety of techniques for usage pattern extraction  user session clustering  banerjee and ghosh  1; gunduz and ozsu  1   page association rule discovery  gunduz and ozsu  1   markov modeling  halvey et al.  1   and implicit relevance feedback  joachims  1 .
¡¡contrary to the rich research in click prediction in web site browsing  the prediction of user clicks in web search has not been well addressed. the fundamental difference between predicting click in web search and web site browsing is the scale of the problem space. the modeling of site browsing typically assumes a tractable number of pages with a reasonable number of observations of the associations among these pages. this assumption  however  no longer holds in the web search scenario. the vast amount of documentsquickly result in very high dimensional space and hence sparsifies the problem space  or equivalently leads to a lack of observations    which effects model training. in order to reduce the problem space  some previous work first classifies the large document collection into a small number of categories so as to predict topic transition in web search  shen et al.  1 . however  since the prediction of user clicks requires the granularity of a single document  we do not consider issues in document clustering but rather work on the full space of the document collection.
¡¡consider the problem of learning from web search logs for click prediction. we define the task as learning the statistical relationship between queries and documents. the primary assumption is that the clicks by users indicate their feedback on the quality of query-documentmatching. hypothesize that the vocabulary of query in terms of words remains stable over a period of time. denote this by ¦². ideally  if we were able to collect sufficient instances for every combination of words   1¦²   and the clicked documents with these queries  we would be able to estimate the probability of future clicks based on past observations. the feasibility of the approach  however  relies on the assumptions that the training data exhausts all the possible queries.
¡¡however  since the number of different queries in web search explodes with emerging concepts in user knowledge and randomness in the formation of queries  the tracking of all possible queries becomes infeasible both practically and computationally. for example  fig. 1 illustrates the increase in the number of distinct queries submitted to citeseer over a period of three months. the linear growth of distinct queries over time indicates that we can hardly match even a small fraction of the new queries exactly with the old queries. as a result  prediction cannot be performed for new queries  yielding low predictability. furthermore manynew documentsare being clicked even after having accumulated documents that were clicked on over considerably long time. learning of the relationship between the complete queries and the documents is consequently a highly challenging problem.
¡¡an alternative naive solution to the lack of training instances is to break down queries into words. an observation of a query and the corresponding clicked document   querydocument pair  is transformed into several independent observations of word and document  i.e. word-document pair.

figure 1: number of distinct queries and words in citeseer over 1 weeks.
this solution can predict unknown queries as long as the new query contains some known words. however  this solution suffers in prediction accuracy due to the discarding of word proximity information in existing queries.
¡¡we first propose two probabilistic models  namely full model and independent model  in order to capture the ideas behind the above intuition  which interpret query-document pairs differently. the full model tends to achieve high prediction accuracy if sufficient training instances are provided  but it cannot be applied when new queries are encountered. on the other hand  the independent model yields high predictability while suffering in accuracy. in order to tradeoff the prediction accuracy with predictability  we suggest a new conditionalprobability hierarchytechniquethat combines the two models. we are not aware of any previous work studying the click prediction problem for web search on such large scale search logs. in addition  as a by-product of the new combination approach  n-grams of words are discovered incrementally.
1 related work
user click prediction is based on understanding the navigational pattern of users and  in turn  modeling observed past behaviorto generatepredictivefuture behavior. usage pattern modeling has been achieved traditionally by session clustering  markov models  association rule generation  collaborative filtering and sequential pattern generation.
¡¡markov model based prediction methods generally suffer from high order  usually needing clustering of user clicks to reduce the limitations stemming from high state-space complexity and decreased coverage. on the other hand  lower order markov models are not very accurate in predicting user's browsing behavior  since these models keep a small window to look back in the history  which is not sufficient to correctly discriminate observed patterns  deshpande and karypis  1 . markov models seem to be more suitable for mobile applications  halvey et al.  1  where the number of states is low due to the few links a user can navigate.
¡¡user session clustering  banerjee and ghosh  1; gunduz and ozsu  1  identifies users' navigational paths from web server logs and defines session similarity metrics based on similarity of the paths and time spent on each page on a path. clustering is employed on the similarity graph which are utilized for predicting users' requests.
¡¡beeferman and berger  beeferman and berger  1  proposed query clustering based on click-through data. each  query  clicked url  pair is used to construct a bipartite graph and queries that result in clicking to the same url are clustered. the content features in the queries and the documents are discarded. joachims  joachims  1  introduces a supervised learning method that trains a retrieval function based on click-through data that takes the relative positions of clicks in a rank as training data set. we pursue a different approach that seeks to learn the statistical relationship between queries and user actions in this paper.
1 problem statement
descriptions of the problem is formalized. let   be the document full set. {di} = d ¡Ê   denotes the set of documents that have ever been shown to users in search results and c   d is the document set that has been clicked on. ¦² = {wi} denotes the word vocabulary. the query set is q = {qj}  where qj = {wj1 ... wjk} ¡Ê 1¦². our observation of web search log is abstracted as a sequence of querydocument pairs: ¦£   q ¡Á c. the posterior probability for observing each click on a certain document d is p d|q dq   where dq represents the document list returned with query q. the problem is to predict p d|q dq ¦£   which measures the probability of user clicking on document d for query q when presented with the result list dq. the prediction of user click for query q becomes: d = argmaxd p d|q dq ¦£   where d ¡Ê dq and ¦£ is the observation of query-document click so far.
1 probabilistic models
the two probabilistic models we propose are for acquiring estimations of p d|q  for each d in dq given query q. depending on interpretations of a click on d for q  two types of models are introduced  namely  the independent model and the full model.
1	independent model
when we observe a query-document pair  how do we interpret it  the independent model we propose firstly assume each word in q is independent of each other. formally  the independent model we define interprets an instance as observing d and given d  observing the words w1 ... wk independently.
¡¡let us considerhow p d|q  is estimated underthe independent model. in the case where the query consists of k words q =  w1 ... wk  and the clicked document is d  we measure the probability p d|w1 ... wk  as:
		 1 
		 1 
¡¡eq. 1 is obtained using the bayes formula. the transition from eq. 1 to eq. 1 assumes the conditional independence between wi and wj according to the model definition. p d  is the marginal probability of document d  which is proportional to the number of occurrences of d. the above derivations show that for the purpose of calculation  each broken down to a collection of independent instances  where i = 1 ... k.
¡¡as indicated in eq. 1  the estimation requires calculation of p d  and p wi|d . since we are able to keep track of p d  easily  the computation for eq. 1 then transforms to p wi|d . in ¡ì 1  we discuss the bayesian estimation of p wi|d  to address the sparsity in training data.
1	full model
while the independent model treats each query as a set of independent single words  the full model reflects the other extreme where all words within a query are treated as a group.
in our definition  the full model treats the q in an instance  as a singleton. the combination of words in q is re-
garded as an entity.
¡¡the full model emphasizes a query as a group and hence yields high prediction accuracy  provided that a large amount of queries have been observed with large supports. however  as noted before  the number of different queries grows so quickly that the full model always suffers the lack of training data in practice. in the next section  we will introduce the method to combine the full model and the independent model in order to address the sparsity issue.
1 probability hierarchy
ideally  if we are able to observe all future queries with the documents being clicked in the log file  especially with decent number of instances  the full model alone can be sufficient. however  new queries keep emerging which dramatically enlarges the problem space if we simply learn from the co-occurrence. in order to address such sparsity issues  we introduce this new conditional probability hierarchy  cph  method  which recursively generates multiple intermediate combinations between the full and independent models. the shrinkage rate is tunable according to the supports of the full model.
1	hierarchical shrinkage
the conditional probability hierarchy  cph  we propose combines the full model and the independent model. it starts with treating a query as a set of independent words  i.e. obtaining p d|wj . hierarchically  words are merged into ngrams 1  or units  of increasing length  getting p d|uk . the final estimation of p d|q  is the hierarchical combination across several levels.
	fig.	1	illustrates	an	example	estimation	of
. suppose we have a document d and a word sequence u   u can be either a single word of a multiple word query  . let p d|u ¦Á denote the estimation of p d|u  under the full model and p d|u ¦Â under independent model. we use p d|u h to denote the estimation of p d|u 

figure 1: cph: hierarchical combination of conditional probabilities at different levels. to estimate
for document d and query  and  are combined.  is the combination of
  where denotes the
number clicks on d with queries containing.
with the cph. a query  consists of five individual words a  b  c  d and e. suppose we already have the full model estimation ofand e. we set the 's at l1 as equivalent to full model estimation:
		 1 
where w = a b c d e. note that at the single word level  full model and independent model are equivalent. thus we have
.
¡¡then we arrive at the combination of probabilities in level l1 to l1:


where ¦Ë is the shrinkage rate that we set according to our confidence with the full model in this case. note that ¦Ë can vary for every case and is tunable according to observation supports of long units.
¡¡similarly  the estimation  is expressed as:
where the independent model estimation of is the combination of by setting:

finally  the estimationbecomes:

where  again  is estimated using independent model on.
¡¡in general  for a query q  and document d  the cph  is:
	.	 1 
then is estimated using:
	.	 1 
where ql ¨’ qr = q  i.e. ql and qr are a split of is obtained using is the full model estimation. the structure of the tree reflects a particular recursive parsing of the queries into smaller word combinations and ultimately single words. with the structure of the tree fixed  the probability p d|q h can be computed recursively by a bottom-up procedure. eq. 1 and eq. 1 illustrate the recursion. we refer to the tree structure  exemplified in fig. 1  as the conditional probability hierarchy  cph .
¡¡the construction of the tree has much to do with the overall performance. note that we only use 1-way tree here. a greedy algorithm is used to produce the hierarchy. in particular  we iteratively search for the binary adjacent units  can be a word  with largest support in each level and feed the merged units to the higher level.
¡¡now we need to determine the parameter ¦Ë in eq. 1. intuitively  ¦Ë should depend on the number of instances that appear together. it is natural to give higher weight to the full model when there are many such observations since the full model tends to be more accurate. accordingly  we weight  where ¦Á   1. a large ¦Á indicates a higher trust in previous estimations and a slower update rate.
1 bayesian estimation
so far  we have seen that both models depend on obtaining estimation for the probability p d|q  from the observations of
. in the estimation for the independent model  the estimation of p wi|d  is required but boils down to the estimation of p d|wi  using bayesian formula. for brevity  we only focus on deriving the estimation for p d|w . we apply bayesian estimation due to the lack of sufficient observations. let p d|w  = ¦È ¡« b ¦È . we need to estimate ¦È. let n be the number of times that w has been clicked on with any document  x be the number of clicks on d.
¡¡we assume users carry out queries and clicks independently. then p x|¦È  is a binomial distribution. if we use the beta distribution beta ¦Á ¦Â  as the conjugate prior for ¦È  we will easily see that p ¦È|x  also follows the beta distribution and the beta distribution is parametrized as:
	p ¦È|x  ¡« beta ¦Á + x n + ¦Â   x 	 1 
¡¡now p ¦È|x  ¡« beta ¦Á+x n+¦Â  x   we obtain the estimation of ¦È  conditioned on x  as the expectation of p ¦È|x :
	.	 1 
¡¡the estimation of ¦È  in eq. 1 will serve as our estimation for p d|w  in the problem. eq. 1 gets around the sparsity issue in training data and is capable to provide non-zero value for p d|w  even with no previous observation of.
¡¡the only question left for the bayesian estimation of p ¦È|x  is the parametrization of the conjugate prior p ¦È   i.e. the determination of ¦Á and ¦Â for p ¦È . assume each document is equally likely to be clicked on1. we want to set expectation e p ¦È   as 1/m  where m is the number of distinct documents in the whole collection. since we have
  we set   obtaining
. the bayesian estimation for p d|w  becomes:
		 1 
where  again  x is the number of clicks on d and m represents the number of candidate documents. n is the number of times that w has been clicked on with any document. we need ¦Á ¦Â   1. in experiments  we tune ¦Â.
1 experiments
for evaluation  we study the property of our approach from three perspectives:  a  prediction accuracy;  b  query segmentation quality and  c  prediction power  i.e. predictability. the accuracy in prediction is evaluated using both mle and bayesian estimation. the query segmentation quality examines the semantic structure in queries.
1	data preparation
we apply our click model to the search environment in citeseer  citeseer.ist.psu.edu   a popular online search engine and digital library. we collect the apache access logs at one citeseer server over 1 days period. there are in total 1 1 requests in this period.
¡¡we remove the robots by their agent field in apache logs and time constratints. we further identify the queries performed at citeseer obtaining a total of 1 distinct queries and 1 1 query-click pairs. there are in all 1 distinct documents ever shown in search results  1 of which have been clicked on. for each query and the document being clicked  we collect the first 1 documents from which this document was picked.
1	evaluation metrics
two important quantitative metrics for evaluation are  a  prediction accuracy and  b  predictability.
¡¡we define a prediction accuracy in evaluation as proportional to number of  correct  predictions of clicked documents from the candidate list. formally  we have prediction accuracy defined as:   where nc is the number of correct prediction and ns is the size of tested sample queries. for each query  the original returned list of documents are provided as candidates.
¡¡we define the predictability metric as the measurement of the models' robustness to new queries. consider when the model estimates the p d|q  as 1 for all candidate d's  the failure of prediction happens. we denote this percentage as pf. quantitatively  the predictability of a model equals to 1 pf.
1k	1k	1k	1k	1k	1k
	size of training set	size of training set
 a  maximum likelihood estimation.	 b  bayesian estimation.
figure 1: prediction accuracy w.r.t. training size.1	prediction accuracy
we train the independent model  full model  and the cph model over different sized subsets of the collection of queryclick pairs. for each round of testing  we randomly choose 1 query-clicks complementary set of training. in each test  we evaluate the accuracy using the two metrics defined above. since we will study the predictability later  the accuracy we show here is for predictable query-click pairs.
¡¡fig. 1 a  and fig. 1 b  give the experimental evaluation on the accuracy for our three models w.r.t. training size. subsets of the whole collection with sizes from 1k to 1k query-document pair instances are experimented. fig. 1 a  presents the accuracy of prediction using mle for p d|w . comparatively  in fig. 1 b   we present the accuracy comparison using bayesian estimation measurement. in both figures  the shrinkage rate ¦Ë for cph model is set to 1 so that we give full model higher weight in combination. for bayesian estimation prediction  the ¦Â is set to 1.
¡¡we are able to see that the full model usually outperforms independent model in terms of prediction accuracy  usually by 1%. mle works better in prediction than bayesian estimation but the mle leads to lower predictability   we will discuss the predictability in sec. 1.  . the performance of the new cph technique is slightly lower than full model in mle but better than full model in bayesian estimation. the cph technique gains significantly higher accuracy in prediction than the independent model.
¡¡in fig. 1  we show the impacts of the setting of shrinkage rate ¦Ë on the accuracy and predictability. we use the training set sized 1k. as expected from eq. 1  the accuracy tilts up as ¦Ë increases and the predictability goes down.

figure 1: prediction accuracy and power w.r.t. shrinkage rate.
¡¡the sensitivity of ¦Â for bayesian estimation of three models are also tested. in table. 1  we present the accuracy of table 1: prediction accuracy w.r.t. ¦Â setting.
¦Âfull modelindependent modelhierarchy1.1.1.11111.1.1.1111table 1: hierarchies discovered in sample queries.
1  tutorial   target  tracking   1   machine  learning    search  engine   1  partial     least  square   regression    1   k  means    cluster  analysis   1   markov  chain    monte  carlo   1   spectral  clustering   jordan  1   energy  efficient    matrix  multiplication   fpgas  1  distributed   cache  architecture   1     code  red   worm    paper  1  dynamic   channel  allocation     cellular  telephone  cph model w.r.t. setting of ¦Â for priors in hierarchy. as can be seen  the accuracy remains relatively stable but the smaller ¦Â gives higher accuracy.
1	query segmentation
in this section  we present the quality of query segmentation formed in discovered query hierarchies. a nice segmentation of queries detects the n-grams in queries that provides the basis for the full model. due to the limit of space  we present 1 randomly picked queries issued to citeseer and their segmentations performed in the cph.
¡¡the discovered hierarchies in queries are presented by nesting square brackets in table 1. we are able to see  from the limited sample  that the hidden structure of words in plain text queries are well discovered using the n-gram frequency. with propersegmentations  discoveredn-gramsin queries are feed to full models  and the hierarchical structures are followed while evaluating eq. 1 for probabilistic hierarchy. as we have seen in sec. 1  the use of n-grams for full model boosts prediction accuracy of independent model. we will see in sec. 1 that the probabilistic hierarchy improves in predictability from full model as well.
table 1: predictability of cph model w.r.t. shrinkage rate.
s   ¦Ë111111k111111k111111k111111	predictability
the definition of predictability is given in ¡ì. 1. the predictability of three models are compared. fig. 1 compares the predictability of three models w.r.t. the size of training set. generally the predictability increases as the training size grows. in particular  the full model has the worst predictability and the independent model has the best. the cph model is between the two.

figure 1: predictability w.r.t. training size.
¡¡we also present predictability of cph model w.r.t. the shrinkage rate in table 1. s is the training size. note the predictability drops slightly as ¦Ë grows.
¡¡one might expect an overall evaluation of the three models combining prediction accuracy and predictability. we sum up the comparisons among three models in fig. 1. in fig. 1  we plot the product of accuracy and predictability w.r.t. the training size. we are able to see that the cph outperforms both models overall.

figure 1: combined evaluation of model performances w.r.t.
training size.
1	computation complexity
finally  training time for the independent model and full model are compared. analytically  complexity of training for full model should be the training time of independent model times the complexity of breaking queries. let the training size be n and the average length of queries be l. the complexity for independent model training is o ln . consider the maximum length of units for full model is set as k. the complexity for full model training is therefore bounded by o lkn . note that since normally  k and l are small integers  training for both models only requires linear time. our experiment shows that the cpu time of training for full model is 1 times of that for independent model.
1 conclusions and future work
in this paper  we address the click prediction in the web search scenario and explore the sparsity of problem space that often exists for machine learning methods for information retrieval research. our experiments on the citeseer data show that large scale evaluation gives promising results for our three models in terms of prediction accuracy and predictability.
¡¡for future work  we will propose more sophisticated formulation methods for the conditional probability hierarchy  cph . it would also be useful to improve our methods by considering and modeling the intent of users.
