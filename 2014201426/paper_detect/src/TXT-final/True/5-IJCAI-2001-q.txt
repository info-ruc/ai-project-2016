  
ai research has often been driven by popular visions - hal 1  asimov's robot  star trek - and by critical application areas - medical expert systems  spoken dialogue systems  etc. these visions and applications serve to inspire and guide researchers  posing challenges  illustrating technical weaknesses  and generally channeling creative energy.  without doubt  the widely held vision of the autonomous robot  has exerted  a substantial integrative force  such that numerous disciplines  ranging from mechanical engineering to cognitive science  can see how their intellectual endeavors can contribute to the overall endeavor. in this brief position paper  and in the accompanying talk  i would like to propose that the next generation of intelligent multimodal user interfaces can offer a similar intellectual focus for ai researchers.  after providing a brief overview of our work in this area and two examples  i would like to suggest the potential impact that such interfaces could have in the relatively near-term. 
 
introduction 
in this position paper  i would like to illustrate recent work in intelligent user interfaces  specifically the subfield called multimodal interfaces  and argue that multimodal interfaces should occupy a similar place in the ai research endeavor as autonomous robotics.  along the way  i will describe the kinds of skills that were needed to build two multimodal systems in our laboratory  the quickset and rasa systems.  as you will see  ai and computer science methodologies played a central role  but are informed by cognitive science and ethnographic 
                                                 
 
  the work described here was supported in part by the national science foundation  the information systems and information technology offices of darpa under contract numbers 
dabt1-c-1 and n1-d-1  and also in part by onr grants: n1-1  n1-1  and n1-1. the views presented here do not represent those of the us government. 
 
research and methodologies.   this paper is not intended as a survey of the field  cf.    but rather as an example.  numerous other groups  at cmu  dfki  illinois  rutgers  sri  and elsewhere are also engaged in such research  and the reader is urged to consult their work.   
 
the paper first considers problems of building principled multimodal systems that fuse information at the semantic level.  issues to be addressed include deciding on relevant principles  observing users  and building an appropriate software architecture.  then  i provide two examples  quickset  a multimodal pen/voice system for interacting with map-based applications  and rasa  a tangible multimodal system that enables users to employ paperbased interfaces. in each case  evaluations of the systems by the intended user population have taken place and are summarized. finally  i describe the kinds of multidisciplinary methodologies that were used in building these systems.   
 
multimodal interaction 
multimodal interfaces are those that allow the user to employ a substantial range of human sensory capabilities to obtain and interact with desired information  and to perform tasks.  for instance  such interfaces typically enable a user to employ some subset of speech  gaze  body movements  pen strokes  haptics   etc. multiple modes are advantageous for a variety of reasons   including: robustness  flexibility  ability to correct persistent errors in one mode by using another  ability to avoid expected errors by switching modes  expressivenes  and speed  1  1  1  1 .  such systems are also appropriate to ubiquitous computing environments  small devices  and very large devices.  
 
principles 
because of the complexity of building multimodal systems  we choose to investigate a principled approach to their design.  the principle we have adopted is to use each modality for its strengths and to overcome weaknesses of the other s  .  by studying the strengths and weaknesses of various modalities  interfaces can be designed that can dramatically simplify human input  leading to  more robust performance  1  1 .  in addition to inspiring new interface designs  such a principle can be applied at run time  whereby joint use of communication modes can compensate for errors in the individual modalities. for instance  joint use of  spoken language and  pen-based gesture  or speech and lip recognition  can produce better overall recognition rates  than relying on the individual modes alone   1  1  1 .  
 
proactive empirical research 
before building a complete multimodal system  it is important to understand how people would in fact interact multimodally.  a series of proactive empirical studies was undertaken that investigated multimodal interaction in a variety  of domains.  using high-fidelity wizard of oz simulations as well as actual system prototypes  it was discovered that by structuring an interface graphically  users' inputs could be channeled towards a linguistically simpler style  one that led to reduced parse ambiguity  bigram perplexity  and utterance disfluencies  1  1   up to 1 fold. a number of these techniques have recently been implemented in microsoft's mipad prototype  and the predictions have been borne out . 
 
interface simulations investigating multimodal pen/voice interactions with map-based systems have also found that multimodal input is simpler than unimodal speech.  in particular  multimodal speech and pen input to map-based applications is briefer  less syntactically complex  has fewer disfluencies  leads to fewer user errors  and is preferred over unimodal speech . furthermore  people adopt one of  two styles of multimodal integration  -- they gesture first  then speak  typically within 1 seconds   or they speak and gesture together  but do not speak first  gesture later . such results led directly to interface designs and to thresholds used in our quickset system .  furthermore  the empirical simulation-based results were again borne out with quickset user testing. 
 
fusion 
the core of any multimodal system is its method for fusing information derived from each mode. depending on the characteristics of the data  information can be fused  early   at the level of signal features  and/or  late  at the level of meaning.  an example of the former is audio-visual fusion  in which information about lip movement  in terms of so-called  visemes   and about  the spoken words  in terms of phonemes  can be combined  resulting in better overall speech recognition  especially in noisy environments  1  1 .   
 
in order to fuse information at the level of meaning   many groups have  adopted unification of typed feature structures as the main symbolic information fusion process  1  1  1 .  typed feature structures are directed-acyclic attribute-value graphs  whose attributes are arranged in a type hierarchy.   such data structures are commonly used in computational linguistics to represent lexical  grammatical  and semantic information  1  1  1 . in the case of multimodal interaction  the meanings of the signals in each mode would be represented in such structures.  modality fusion occurs when  the structures are unified  1  1   subject to various constraints .  feature structure unification  a generalization of term unification in logic programming coupled with type reasoning  is appropriate as a fusion operation because it combines complementary and redundant information  but rules out inconsistent information. in tests of quickset's mutual disambiguation of modalities based on feature structure unification  it was found that multimodal interaction led to a 1% error rate reduction over unimodal spoken language processing . 
 
fusion architecture 
because machine perception is errorful  the software architecture needs to be designed from the ground up to handle errors. given that recognizers  produce a large number of hypotheses  with even a modest number of interpretations per mode  examining their cross-product quickly can become expensive.  thus  there needs to be rapid filtering of those combinations that could not possibly unify  1  1   a statistical assessment of the joint probabilities of the remaining cross-modal recognition hypotheses  and finally a unification process to combine the fine structure of the interpretations. a hybrid symbolic/statistical process that can handle an arbitrary number of modes  with a variety of spatial and temporal relationships  e.g  some precede others  some co-occur  etc.  needs to be developed to arrive at the best overall interpretation of the inputs  1  1 .  here  statistical speech and natural language processing  machine learning  pattern recognition  and sensor fusion techniques play a crucial role.  
 
distributed software architecture 

figure 1:  left: quickset operating on a handheld pc; right: collaborating quickset operating on a 1  plasma display with touch overlay.  user is speaking through a wireless microphone while drawing. the system's ability to handle parallel asynchronous input is critical to its usability.  this characteristic is a  significant departure from the design of current graphical user interface software  which assumes that its input is certain  and  sequential.1 to address these needs  we have employed a multiagent architecture  1  1  1  that offers faulttolerant  distributed  asynchronous operations  with a facilitated or direct communication model.   
 
example 1: quickset 
quickset is a collaborative handheld multimodal system based on a multiagent architecture   which controls numerous applications  including community fire and flood control  military simulators  modsaf   exercise initialization  exinit   and virtual terrain environments  the naval research laboratory's dragon  ii and spawar's commandvu .   the system enables users to create point  line  and area entities on a pc screen  using a variety of form factors ranging from handheld or wearable devices to wall-sized displays  simply by speaking and sketching.  for example  the user can create and position an m1 company at a given location and with a given orientation and posture by saying:   m1 company facing one two zero degrees in defensive posture   while touching the desired location.  in contrast  a user of a graphical user interface  gui  would have to locate the desired unit in a browser or palette  drag the icon onto the screen  and fill in various parameters in a dialogue box.  likewise  by speaking and sketching  the user can create linear and area features  such as unit boundaries  objectives  routes  fortifications  air corridors  no go/slow go areas  drop zones  supply routes  cultural features  etc.    
 
quickset architecture 
quickset consists of a set of software agents  including speech recognition  natural language processing  text-to-speech  gesture recognition  multimodal integration  a map-based user interface  a database system  and an application bridge. continuous speech and continuous gesture are processed in parallel  with n-best recognition results from each mode represented as typed feature structures.  after parsing  the resulting interpretations  are collected by the multimodal integrator.  the integrator operates as a multimodal chart parser   storing partial feature structure interpretations in a multimodal chart  which is operated upon by rules  and subjected to constraints. the basic fusion operation is unification of feature structures .  
 
these modality agents communicate through a central facilitator via a common language  currently the interagent communication language for the open agent architecture  1  1 .  the presence of the facilitator enables agents to connect disconnect  and reconnect without restarting the system.  however  as a central component  the facilitator can be a bottleneck to high bandwidth information transfer  and potentially a single point of failure.  to overcome these problems  we have developed a successor multiagent system  the adaptive agent architecture  which offers direct as well as facilitated communication  and supports fault-tolerant operation based on the theory of teamwork  1  1  1 .  importantly  the agents can reside on a variety of different types of machines  located anywhere on the internet. if appropriately time-synchronized  distributed agents can participate in analyzing users' multimodal inputs. in particular  handheld systems  such as pdas  can run the interface  but offload computationally intensive multimodal processing to servers operating elsewhere.   
 
evaluation 
multimodal interaction with quickset was recently compared with interaction via a standard graphical user interface  gui  for the task of placing entities on a map .  it was found that multimodal interaction led to a 1 fold increase in the speed with which military users could create entities  lines and areas of various types.  although there were no more errors with multimodal interaction than with the gui  the time to correct multimodal errors was again 1-fold faster than the time needed to correct gui errors. furthermore  all the users preferred interacting multimodally to using the gui.   this is just one study  comparing one gui with multimodal interaction  but it is indicative of the potential this style of interface can offer. 
 
paper too  
one virtue of employing a distributed multiagent architecture  as the core software architecture is that it supports a variety of platforms and hardware configurations.  in particular  it is appropriate for paper-based environments. 
 
to understand why paper is important  consider the scene below  figure 1  from a us army division command post taken at a frenetic time during an exercise.  on the computer screens shown here  and on the 1 other screens arrayed around the room   are military command and control systems.   here is a quiz:   what's missing from this picture  
 
 
figure 1:  scene from a us army division command post during an exercise  photo courtesy of william scherlis . 
indeed  no one is using these or any of the other 1 systems.  what the officers are in fact doing can be found in the next photo  figure 1 .  they are standing on chairs  plotting the positions of military units using an 1-foot by 1-foot paper map and post-it notes.   you will notice that they have turned their backs on computer science and technology.  
 
in general  it is fair to say that despite the best efforts of researchers and numerous well-funded development efforts  many military users persist in employing paper rather than computers.1 however  the officers are not simply trying to be difficult or overly conservative in rejecting digital systems.  rather  the systems they have received are missing qualities that they value highly.   the users tell us that they continue to use paper maps because they have extremely high resolution  are malleable  cheap  lightweight  and can be rolled up and taken anywhere. importantly  paper does not fail  and it supports face-to-face collaboration among the staff members.   for example  figure 1 illustrates the kinds of collaboration officers engage in with paper maps  which are simply unsupported by present day computer systems. 
 
 
figure 1:  officers tracking units with paper maps and post-it notes in preference to computer systems  photo courtesy of william scherlis  
 
figure 1:  multiuser collaboration around a command post map.  
thus  for a variety of factors  officers  as well as medical and air traffic control personnel  prefer paper to digital systems. we believe there is no reason they cannot have the benefits of both.   
to provide such advantages  we have adapted the  quickset system to use paper-based interfaces  thereby enabling officers to employ their highly practiced mode of operation when using digital systems  1 .  the new rasa system  developed for david mcgee's ph.d. thesis  provides both sets of benefits  without substantial task overhead in virtue of its understanding the symbology drawn on the paper maps and post-it notes  as well as  its understanding of the spoken language used in creating and naming units. essentially  both paper maps and post-it notes rest on touch or pen-sensitive digitizers. digital ink used in drawing symbols on a post-it note is routed to the quickset symbology recognizer  which creates an n-best list of hypothesized units that the user drew.  accompanying speech is recognized and parsed  and the fused interpretation then waits at the integrator for a location feature to arrive.  the act of placing the note on the map  which is overlaid on a touch-sensitive digitizer  then supplies the desired location.   it is most important to note that in this tangible multimodal system   the physical artifacts  i.e.   the paper map and post-its   become the computational interface. for example  moving a post-it note on the map moves it in the digital system.   in response  rasa projects system updates onto the paper map.   rasa can also provide data to other displays and visualizations.  
the system has been evaluated with military users  and has been found to be as fast as paper  and is preferred to paper .  moreover  users' work continues when the system or computer communications fail  and the effort to synchronize them when the system is brought on line is well within users' tolerance.  clearly  given the situation shown in figure 1  we would hypothesize that it would offer superior usability to the existing commandand-control systems.  this hypothesis will be tested in field experiments.  
concluding remarks 
with this very brief paper  i hope to have called attention through the quickset and rasa examples to the potential for a variety of subfields within ai and computer science  e.g.  speech  natural language  vision  distributed systems  machine learning  knowledge-representation and reasoning  human-computer interaction   in collaboration with numerous other disciplines  cognitive science  ethnography  linguistics  pattern recognition  sensor fusion   etc.   to contribute to radically changing the human-computer interface. in the development and evaluation of quickset  we see a direct progression from proactive empirical research  to system development  and  finally to formal laboratory and field user testing. for rasa  our taking an ethnographic perspective and observing the actual  work practice   enabled us to identify both problems and opportunities for technology.   in a very real sense  neither of these systems could have been developed without multidisciplinary collaboration.  no one methodology was employed  nor could it have been.  the research within each  core  discipline was focused by the multidisciplinary goals to produce a synergistic whole.   
 
other domains ripe for multimodal interaction include in-home access to digital information  mobile computing  geographic information systems  gis   computer-aided design  cad   games. overall  multimodal interaction can benefit society by enlarging the base of users to children  users with disabilities  or users whose physical situations of computer usage are changing.  a concentrated effort to research and develop such systems can have enormous scientific and technological payoffs  and would be a worthy complement to the other focal areas of ai research. 
 
acknowledgements 
many thanks to my colleagues at chcc/ogi  who made this research possible  including  but not limited to : josh clow  marcus huber  michael johnston  sanjeev kumar   david mcgee  james pittman  sharon oviatt  ira smith  matt wesson  and lizhong wu.  
 
references 
 
1. carpenter  r.  the logic of typed feature structures. 1  cambridge university press: cambridge england. 
1. cohen  p.r.  the role of natural language in a multimodal interface  in proceedings of the conference on  user interface software technology  uist'1 . 1  acm press: new york. 
1. cohen  p.r.  mcgee  d.  and clow  j.  the efficiency of multimodal interaction for a map-based task  in the proceedings of the proceedings of the 1th applied natural language processing conference  seattle  washington  may  1  association for computational linguistics  1. 
1. cohen  p.r.  cheyer  a.  wang  m.q.  et al.  an open agent architecture  in working notes of the aaai spring symposium series on software agents. 1  aaai press  reprinted in readings in agents  huhns and singh  eds.   morgan kaufmann publishers  inc.  san mateo  california  1: stanford  calif. 1. 
1. cohen  p.r.  dalrymple  m.  moran  d.b.  et al.  synergistic use of natural language and direct manipulation  in proceedings of the humanfactors in computing systems conference  chi'1 . 1  acm press: new york. 
1. cohen  p.r.  johnston  m.  mcgee  d.  et al.  quickset:  multimodal interaction for distributed applications  in proceeedings of the fifth acm international multmedia conference  e. glinert  editor. 1  acm press: new york. 1. 
1. cohen  p.r. and levesque  h.j.  teamwork. nous  1. 1 : 1. 
1. denecke  m.  and yang  j.  partial information in multimodal dialogue  in the proceedings of the proceedings of the international conference on 
multimodal interaction  beijing  china  1  1. 
1. gorman  p.  ash  j.  lavelle  m.  lyman  j.  delcambre  l.  and maier  d.  bundles in the wild:  managing information to solve problems and maintain situation awareness. library trends  1. 1 . 
1. huang  x.  acero  a.  chelba  c.  deng  l.  duchene  d.  goodman  j.  hon  h.  jacoby  d.  jiang  l.  loynd  r.  mahajan  m.  mau  p.  meredith  s.  mughal  s.  neto  s.  plumpe  m.  wang  k.  wang  y.  mipad:  a next generation pda prototype  in the proceedings of the proceedings of the international conference on spoken language processing  beijing  china  1  chinese friendship publishers. 
1. johnston  m.  p. r. cohen  d. mcgee  s. l. oviatt  j. a. pittman  i. smith.  unificationbased multimodal integration.  in the proceedings of the proceedings of the 1th annual meeting of the association for computational linguistics and 1th conference of the european chapter of the association for computational linguistics  1  1. 
1. johnston  m.  unification-based multimodal parsing  in the proceedings of the proceedings of coling-acl 1: the 1th international conference on computational linguistics and the 1th annual meeting of the association for computational linguistics  1  association for computational linguistics. 
1. kay  m.  functional grammar  in proceedings of the fifth annual meeting of the berkeley linguistics society. 1: berkeley  california. 1. 
1. kiefer  b.k.  h.-u.  nederhof  m.-j.  efficient and robust parsing of word hypotheses graphs  in verbmobil:  foundations of speech-to-speech translation  w. wahlster  editor. 1  springer: berlin  germany. 1. 
1. kumar  s.  cohen  p. r.  towards a faulttolerant multi-agent system architecture.  in the proceedings of the proceedings of the fourth international conference on autonomous agents  agents 1   barcelona  spain  1  aaai press  1. 
1. levesque  h.j.  cohen  p.r.  and nunes  j.  on acting together  in proceedings of american association for artificial intelligence  aaai-1 . 1: san mateo  california. 
1. mackay  w.e.  is paper safer   the role of flight strips in air traffic control. acm transactions on human-computer interaction  1. 1 : 1. 
1. martin  d.l.  cheyer  a.  j.   moran  d. b.   the open agent architecture: a framework for building distributed software systems. applied artificial intelligence  1. 1 january-march : 1. 
1. mcgee  d.  cohen  p. r.  and wu  l.  something from nothing:   augmenting a paper-based work practice via multimodal interaction  in the proceedings of the proceedings of the conference on design of augmented reality environments  denmark  1  association for computing machinery. 
1. mcgee  d.  cohen  p. r.  creating tangible interfaces by augmenting physical objects with multimodal language  in the proceedings of the proceedings of international conference on intelligent user interfaces  santa fe  new mexico  1. 
1. mcgee  d.r.  augmenting environments with multimodal interaction  in dept. of computer science and engineering. forthcoming  oregon graduate institute of science and technology: beaverton  or. 
1. mckeown  k.r.  generating natural language text in response to questions about database   structure. 1. 
1. neti  c.  iyengar  g.  potamianos  g.  senior  a.  perceptual interface for information interaction:  joint processing of audio and visual information for human-computer interaction  in the proceedings of the proceedings of the international conference on spoken language processing  icslp'1   beijing  china  1  chinese friendship publishers  1. 
1. nigay  l.  and coutaz  j.  a generic platform for addressing the multimodal challenge  in the proceedings of the proceedings of the conference on human factors in computing systems  chi'1   1  acm press  1. 
1. oviatt  s.l.  multimodal interactive maps: designing for human performance. human 
computer interaction  1. 1: 1. 
1. oviatt  s.l.  mutual disambiguation of recognition errors in a multimodal architecture  in the proceedings of the human factors in computing systems  chi'1   new york  1  acm press  1. 
1. oviatt  s.l.  cohen  p. r.  multimodal interfaces that process what comes naturally. communications of the acm  1. volume 1 : 1. 
1. oviatt  s.l.  taming recognition errors with a  multimodal interface. communications of the acm  1. 1 : 1. 
1. oviatt  s.l.  cohen  p. r.  wu  l.  vergo  j.  
duncan  l.  suhm  b.  bers  j.  holzman  t.  winograd  t.  landay  j.  larson  j.  ferro  d.  designing the user interface for multimodal speech and gesture applications: state-of-the-art systems and research directions for 1 and beyond. human computer interaction  1. 1 : 1. 
1. oviatt  s.l.  cohen  p.r.  and wang  m.q.  toward interface design for human language technology: modality and   structure as determinants of linguistic complexity. speech communication  1. 1-1 . 
1. stork  d.g.  wolff  g.  and levine  e.  neural network lipreading system for improved speech recognition  in proceedings of the international joint conference on neural networks vol. ii. 1. 1. 
1. suhm  b.  myers  b.  and waibel  a.  modelbased and empirical evaluation of multimodal interactive error correction  in the proceedings of the  human factors in computing systems  chi'1   new york  1  acm press  1. 
1. wu  l.    oviatt  s.  l. and cohen  p. r.   statistical multimodal integration for intelligent hci  in neural networks for signal processing  y.h. hu  larsen  j.  wilson  e.  and douglas  s.  editor. 1  ieee press: new york. 1. 

plausibility measures: a general approach for representing uncertainty
joseph y. halpern
cornell university
dept. of computer science
ithaca  ny 1 halpern cs.cornell.edu
http://www.cs.cornell.edu/home/halpern1	introduction
the standard approach to modeling uncertainty is probability theory. in recent years  researchers  motivated by varying concerns including a dissatisfaction with some of the axioms of probability and a desire to represent information more qualitatively  have introduced a number of generalizations and alternativesto probability including dempster-shafer belief functions  shafer  1   possibility measures  dubois and prade  1   lexicographic probability  blume et al.  1   and many others. rather than investigating each of these approaches piecemeal  i consider here an approach to representing uncertainty that generalizes them all  and lets us understand their commonalities and differences.
　a plausibility measure  friedman and halpern  1  associates with a set a plausibility  which is just an element in a partially ordered space. the only real requirement is that if is a subset of   then the plausibility of is less than equal to the plausibility of . probability measures are clearly plausibility measures; every other representation of uncertainty that i am aware of can also be viewed as a plausibility measure. given how little structure plausibility measures have  it is perhaps not surprising that plausibility measures generalize so many other notions. this very lack of structure turns out to be a significant advantage. by adding structure on an  as needed  basis  it is possible to characterize what is required to ensure that a plausibility measure has certain properties of interest. this both gives insight into the essential features of the properties in question and makes it possible to prove general results that apply to many representations of uncertainty.
in this paper  i discuss three examples of this phenomenon.
belief  belief revision  and default reasoning  expectation and decision making  compact representations of uncertainty  bayesian networks .
most of the discussion is based on earlier work  some of it joint with nir friedman . in the next two sections i define plausibility measures and conditional plausibility measures. the next three sections considers each of the topics above in more detail.

　　supported in part by nsf under grants iri-1 and iis1.
1	plausibility measures
a probability space is a tuple   where is a set of worlds  is an algebra of measurable subsets of  that is  a set of subsets closed under union and complementation to which we assign probability   and is a probability measure  that is  a function mapping each set in to a number in satisfying the well-known kolmogorov axioms    
	  and	if	and	are
disjoint .1
　a plausibility space is a direct generalization of a probability space. simply replace the probability measure by a plausibility measure that  rather than mapping sets in to numbers in   maps them to elements in some arbitrary partially ordered set. is read  the plausibility of set
　 . if   then is at least as plausible as . formally  a plausibility space is a tuple   where is a set of worlds  is an algebra over   and maps sets in to some set of plausibility values partially ordered by a relation  so that is reflexive  transitive  and anti-symmetric . is assumed to include two special elements  and   such that for all
　. in the case of probability measures    and and are 1 and 1  respectively. as usual  the ordering is defined by taking if and .
i omit the subscript from       and whenever it is clear from context.
　there are three requirements on plausibility measures. the first two are analogues of the conventions that hold for all representations of uncertainty: the whole space gets the maximum plausibility and the empty set gets the minimum plausibility. the third requirement says that a set must be at least as plausible as any of its subsets.
pl1.	.
pl1.	.
pl1. if	  then	.
since	is a partial order  pl1 says that  if	  then the plausibility of	is comparable to the plausibility of	and  moreover 	.

1
	frequently it is also assumed that	is countably additive  i.e.  if
	 	  are pairwise disjoint  then	. since
i focus on finite state spaces here  countable additivity does not play a significant role  so i do not assume it.
　clearly probability spaces are instances of plausibility spaces. almost all other representations of uncertainty in the literature can also be viewed as instances of plausibility measures. here are some examples:
a belief function on is a function satisfying certain axioms  shafer  1 . these axioms certainly imply property pl1  so a belief function is a plausibility measure. there is a corresponding plausibil-

	ity function	defined as	.1
a possibility measure  dubois and prade  1  on is a function poss such that poss  
	poss	  and poss	poss	.
an ordinal ranking  or -ranking  on  as defined by  goldszmidt and pearl  1   based on ideas that go back to  spohn  1   is a function mapping subsets of to such that     and	. intuitively  an ordinal ranking assigns a degree of surprise to each subset of worlds in   where means unsurprising and higher numbers denote greater surprise. it is easy to see that a ranking is a plausibility measure with range   where if and only if under the usual ordering.
a lexicographic probability system  lps   blume et al. 
1  of length is a sequence of probability measures. intuitively  the first measure in the sequence    is the most important one  followed by  
　  and so on. very roughly speaking  the probability assigned to an event by a sequence such as
can be taken to be   where is an infinitesimal. thus  even if the probability of according to is 1  still has a positive  although infinitesimal  probability if .
　in all these cases  the plausibility ordering is total. but there are also cases of interest where the plausibility ordering is not total. for example  suppose that is a set of probability measures on . let be the lower probability of   so that	. similarly  the upper probability is defined as	.
　both and give a way of comparing the likelihood of two subsets and of . these two ways are incomparable: it is easy to find a set of probability measures on
and subsets	and	of	such that	and
　　　　　　　. rather than choosing between	and	  we can associate a different plausibility measure with	that captures both. let	and define	iff	. this puts a partial order on	  with	and	.
define	. thus  associates with a set two numbers that can be thought of as defining an interval in terms of the lower and upper probability of . it is easy to check that if the upper probability of is less than or equal to the lower probability of . clearly  satisfies pl1  so it is indeed a plausibility measure  but one that puts only a partial  pre order on events. a similar plausibility measure can be associated with a belief/plausibility function.
　the trouble with	 	  and even	is that they lose information. for example  it is not hard to find a set of probability measures and subsets	of	such that for all	and	for some
　　　  but	and	. indeed  there exists an infinite set	of probability measures
such that	for all	but
and	. if all the probability measures in
agree that is less likely than   it seems reasonable to conclude that is less likely than . however  none of     or necessarily draw this conclusion.
　it is not hard to associate yet another plausibility measure with that does not lose this important information  and does indeed conclude that is less likely than  . suppose  without loss of generality  that there is some index set such that
	. thus  for example  if	 
then	.  in general 	may be infinite.  let
consist of all functions from to . the standard pointwise ordering on functions-that is  if
for all -gives a partial order on . note that is the function such that for all and is the function such that for all .
for   let be the function such that for all . define the plausibility measure by taking
	. thus 	iff
for all iff for all . it is easy to see that and . clearly satisfies
pl1. pl1 and pl1 follow since	and
                     while pl1 holds because if	  then	for all	.
　to see how this representation works  consider a simple example where a coin which is known to be either fair or double-headed is tossed. the uncertainty can be represented by two probability measures on   which gives heads probability 1  and which gives heads probability . taking the index set to be   this gives us a plausibility measure such that is a function such that
and	; similarly 	is a function	such that and	.
1	conditional plausibility
suppose an agent's beliefs are represented by a plausibility measure . how should these beliefs be updated in light of new information  the standard approach to updating in probability theory is by conditioning. most other representations of uncertainty have an analogue to conditioning. indeed  compelling arguments have been made in the context of probability to take conditional probability as a primitive notion  rather than unconditional probability. the idea is to start with a primitive notion satisfying some constraints  such as
	if	and	are dis-
joint  rather than starting with an unconditional probability measure and defining conditioning in terms of it. the advantage of taking conditional probability as primitive is that it allows conditioning on events of unconditional probability 1.
 if	is the whole space  the unconditional probability of
can be identified with ; note that may be well defined even if .  although conditioning on events of measure 1 may seem to be of little practical interest  it turns out to play a critical role in game theory  see  for example   blume et al.  1; myerson  1    the analysis of conditional statements  see  adams  1; mcgee  1    and in dealing with nonmonotonicity  see  for example   lehmann and magidor  1  .
　most other representations of uncertainty also have an associated notion of conditioning. i now discuss a notion of conditional plausibility that generalizes them all. a conditional plausibility measure  cpm  maps pairs of subsets of to some partially ordered set . i write rather than
         in keeping with standard notation. an important issue in defining conditional plausibility is to make precise what the allowable arguments to are. i take the domain of a cpm to have the form where  roughly speaking  consists of those sets in on which conditioning is allowed. for example  for a conditional probability measure defined in the usual way from an unconditional probability measure   consists of all sets such that .  note that is not an algebra-it is not closed under complementation.  a popper algebra over is a set of subsets of satisfying the following properties:
acc1.	is an algebra over	.
acc1.	is a nonempty subset of	.
acc1.	is closed under supersets in	; that is  if	 
	  and	  then	.
 popper algebras are named after karl popper  who was the first to consider formally conditional probability as the basic notion  popper  1 . this definition of cpm is from  halpern  1a  which in turn is based on the definition in  friedman and halpern  1 . 
	a	conditional	plausibility	space	 cps 	is	a	tuple
	  where	is a popper algebra over	 
                 is a partially ordered set of plausibility values  and is a conditional plausibility measure  cpm  that satisfies the following conditions:
cpl1.	.
cpl1.	.
cpl1. if	  then	.
cpl1	.
cpl1 are the obvious analogues to pl1. cpl1 is a minimal property that guarantees that when conditioning on   everything is relativized to . it follows easily from cpl1 that is a plausibility measure on for each fixed .
a cps is acceptable if it satisfies
acc1. if	 	  and	  then
.
acceptability is a generalization of the observation that if
	  then conditioning on	should be defined. it
says that if	  then conditioning on
should be defined. a cps	is standard if
.
　cpl1 are rather minimal requirements. for example  they do not place any constraints on the relationship between and if . one natural additional condition is the following.
cpl1. if	and	  then iff	.
it is not hard to show that cpl1 implies cpl1. while it seems reasonable  note that cpl1 does not hold in some cases of interest. for example  there are two well-known ways of defining conditioning for belief functions  see  halpern and fagin  1    one using dempster's rule of combination and the other treating belief functions as lower probabilities. they both satisfy cpl1  and neither satisfies cpl1.
　many plausibility spaces of interest have more structure. in particular  there are analogues to addition and multiplication. more precisely  there is a way of computing the plausibility of the union of two disjoint sets in terms of the plausibility of the individual sets and a way of computing	given and	. a cps	where has range	is algebraic if it is acceptable and there are functions	and	such that the following properties hold:
alg1. if	are disjoint and	then
.
alg1. if	 	  then
.
alg1.	distributes over	; more precisely 
domplifand	dompl where dompl
are pairwise disjoint and
and dompl
. alg1. if dompl     and then	. i sometimes refer to the cpm	as being algebraic as well.
　there are well-known techniques for extending some standard unconditional representations of uncertainty to conditional representations. all satisfy cpl1  when viewed as plausibility measures.  indeed  as shown in  halpern  1a   there is a construction for converting an arbitrary unconditional plausibility space to an acceptable standard cps.  in many cases  the resulting cps is algebraic. but one important case that is not algebraic is conditional belief functions  using either definition of conditioning .
　to give one example of a construction that does lead to an algebraic cps  consider lps's. blume  brandenburger  and dekel 1  bbd  define conditioning in lps's as follows.
given	and	such that	for some index
  let	  where
is the subsequence of all indices for which the probability of is positive. thus  the length of the lps depends on . let consist of all sequences
                   such that	for	  and let	. roughly speaking  1 is meant to represent all sequences of the form	  whatever their length; similarly  1 represents all sequences of the form	.	define a partial order	on	so that if	 	  or	and	are vectors of the same length and	is lexicographically less than or equal to	. note that vectors of different length are incomparable.
　an unconditional lps defined on an algebra over can then be extended to a standard cps using the definition of conditioning above. note that although may be incomparable to for   will definitely be comparable to . moreover 
the definition of 1 and 1 guarantees that
	if	  as required by
cpl1 and cpl1.
　the cps is in fact algebraic; and are functions that satisfy the following constraints:
	if	and	are vectors of the same length 
	 where	represents pointwise addition  
 
 
 
　　　　　　　　　　　 	where	represents	a possibly empty sequence of 1s  and	.
i leave it to the reader to check that these definitions indeed make the cps algebraic.
　a construction similar in spirit can be used to define a notion of conditioning appropriate for the representation of a set of plausibility measures; this also leads to an algebraic cps  halpern  1a .
1	belief revision and default reasoning
1	belief
there have been many models used to capture belief. perhaps the best known approach uses kripke structures  hintikka 
1   where an agent believes if is true at all worlds the agent considers possible. in terms of events  sets of worlds   an agent believes if contains all the worlds that the agent considers possible. another popular approach is to use probability: an agent believes if the probability of is at least for some appropriate .
　one of the standard assumptions about belief is that it is closed under conjunction: if an agent believes and   then the agent should also believe . this holds for the definition in terms of kripke structures. it holds for the
probabilistic definition only if . indeed  identifying knowledge/belief with  holds with probability 1  is common  especially in the economics/game theory literature  brandenburger and dekel  1 .
　a number of other approaches to modeling belief have been proposed recently  in the game theory and philosophy literature. one  due to brandenburger 1  uses filters. given a set of possible worlds  a filter is a nonempty set of subsets of that  1  is closed under supersets  so that if and   then     1  is closed under finite intersection  so that if   then    and
 1  does not contain the empty set. given a filter   an agent is said to believe iff . note that the set of sets which are given probability 1 by a probability measure form a filter. conversely  every filter defines a finitely additive probability measure : the sets in get probability 1; all others get probability 1. we can also obtain a filter from the kripke structure definition of knowledge. if the agent considers possible the set   then let consist of all superset of
　. this is clearly a filter  consisting of precisely the events the agent believes . conversely  in a finite space  a filter determines a kripke structure. the agent considers possible precisely the intersection of all the sets in  which is easily seen to be nonempty . in an infinite space  a filter may not determine a kripke structure precisely because the intersection of all sets in the filter may be empty. the events believed in a kripke structure form a filter whose sets are closed under arbitrary intersection.
　another approach to modeling belief  due to brandenburger and keisler 1  uses lps's. say that an agent believes in lps if there is some such that
for all and for . it is easy to see that beliefs defined this way are closed under intersection. brandenburger and keisler give an elegant decision-theoretic justification for this notion of belief. interestingly  van fraassen 1 defines a notion of belief using conditional probability spaces that can be shown to be closely related to the definition given by brandenburger and keisler.
　plausibility measures provide a framework for understanding what all these approaches have in common. say that an agent believes with respect to plausibility measure if

             ; that is  the agent believes if is more plausible than not. it is easy to see that  in general  this definition is not closed under conjunction. in the case of probability  for example  this definition just says that is believed if the probability of is greater than . what condition on a plausibility measure is needed to guarantee that this definition of belief is closed under conjunction  trivially  the following restriction does the trick:

pl1 . if	and	  then

.
　i actually want a stronger version of this property  to deal with conditional beliefs. an agent believes conditional

on	  if given	 	is more plausible than	  that is  if

. in the presence of cpl1  which i im-
plicitly assume for this section   conditional beliefs are closed under conjunction if the following holds:

pl1 . if	and

	  then	.
a more elegant requirement is the following:
pl1. if	 	  and	are pairwise disjoint sets 
	  and	  then
.
in words  pl1 says that if is more plausible than and if is more plausible than   then by itself is already more plausible than .
　remarkably  in the presence of pl1  pl1 and pl1 are equivalent:
proposition 1:   friedman and halpern  1a   satisfies pl1 iff satisfies pl1 and pl1 .
　thus  for plausibility measures  pl1 is necessary and sufficient to guarantee that conditional beliefs are closed under conjunction. proposition 1 helps explain why all the notions of belief discussed above are closed under conjunction. more precisely  for each notion of belief discussed earlier  it is trivial to construct a plausibility measure satisfying pl1 that captures it: give plausibility 1 to the events that are believed and plausibility 1 to the rest.
　pl1 is required for beliefs to be closed under finite intersection  i.e.  finite conjunction . it does not guarantee closure under infinite intersection. this is a feature: beliefs are not always closed under infinite intersection. the classic example is the lottery paradox  kyburg  1 : consider a situation with infinitely many individuals  each of whom holds a ticket to a lottery. it seems reasonable to believe that individual will not win  for any   yet that someone will win. if is the event that individual does not win  this amounts to believing

         and also believing  and not believing  . it is easy to capture this with a plausibility measure.
let   where is the world where individual wins  so that  . let lot be a plausibility measure that assigns plausibility to the empty set  plausibility to all finite sets  and plausibility 1 to all infinite sets.
it is easy to see that lot ratifies pl1. nevertheless  each of is believed according to lot  as is .
　as shown in  friedman et al.  1   the key property that guarantees that  conditional  beliefs are closed under infinite intersection is the following generalization of pl1:
pl1 . for any index set such that   if are pairwise disjoint sets    and for all
	 	  then	.
　because pl1 does not hold for lot  it can be used to represent the lottery paradox. because pl1 does hold for the plausibility measure corresponding to beliefs in kripke structure  belief in kripke structures is closed under infinite conjunction. a countable version of pl1 holds for -additive probability measures  which is why probability-1 beliefs are closed under countable conjunctions  but not necessarily under arbitrary infinite conjunctions .
1	belief revision
an agent's beliefs change over time. conditioning has been the standard approach to modeling this change in the context of probability. however  conditioning has been argued to be inapplicable when it comes to belief revision  because an agent may learn something inconsistent with her beliefs. this would amount to conditioning on a set of measure 1. as a consequence  finding appropriate models of belief change has been an active area in philosophy and in both artificial intelligence  ga：rdenfors  1; katsuno and mendelzon  1 . in the literature  two models have been studied in detail: belief revision  alchourro＞n et al.  1; ga：rdenfors  1  attempts to describe how an agent should accommodate a new belief  possibly inconsistent with his other beliefs  about a static world. belief update  katsuno and mendelzon  1   on the other hand  attempts to describe how an agent should change his beliefs as a result of learning about a change in the world.
　belief revision and belief update describe only two of the many ways in which beliefs can change. using plausibility  it is possible to construct a general framework for reasoning about belief change  see  friedman and halpern  1  . the key point is that it is possible to describe belief changing using conditioning with plausibility  even though it cannot be done with probability. starting with a conditional plausibility measure satisfying pl1  this is necessary for belief to have the right properties  and conditioning on new information gives us a general model of belief change. belief revision and belief update can be captured by putting appropriate constraints on the initial plausibility  friedman and halpern  1 . the same framework can be used to capture other notions of belief change  such as a general markovian models of belief change  friedman and halpern  1b  and belief change with unreliable observations  boutilier et al.  1 . the key point is that belief change simply becomes conditioning  and iterated belief change becomes iterated conditioning .
	1	default reasoning
it has been argued that default reasoning plays a major role in commonsense reasoning. perhaps not surprisingly  there have been many approaches to default reasoning proposed in the literature  see  gabbay et al.  1; ginsberg  1  . many of the recent approaches to giving semantics to defaults can be viewed as considering structures of the form   where is a set of possible worlds  is a truth assignment to primitive propositions for each world   and can be viewed as a  measure  on . some examples of include possibility measures  dubois and prade  1   -rankings  goldszmidt and pearl  1   parameterized probability distributions  pearl  1   these are sequences of probability distributions; the resulting approach is more commonly known as -semantics   and preference orders  kraus et al.  1; lewis  1 .
　somewhat surprisingly  all of these approaches are characterized by the six axioms and inference rules  which have been called the klm properties  since they were discussed by kraus  lehmann  and magidor 1 . assume  as is typical in the literature  that defaults are expressed in terms of an operator   where is read  if then typically/likely/by default . for example  the default  birds typically fly  is represented bird fly. we further assume for now that the formulas and that appear in defaults come from some propositional language with a consequence relation .
lle. if	  then from
 left logical equivalence .inferrw. if	  then from
 right weakening .
ref.	 reflexivity .inferand. from	and	infer.or. from	and	infer
cm. from	and	infer
 cautious monotonicity ..　lle states that the syntactic form of the antecedent is irrelevant. thus  if and are equivalent  we can deduce from . rw describes a similar property of the consequent: if  logically  entails   then we can deduce from . this allows us to combine default and logical reasoning. ref states that is always a default conclusion of . and states that we can combine two default conclusions. if we can conclude by default both and
from   then we can also conclude from . or states that we are allowed to reason by cases. if the same default conclusion follows from each of two antecedents  then it also follows from their disjunction. cm states that if and are two default conclusions of   then discovering that
holds when holds  as would be expected  given the default  should not cause us to retract the default conclusion .
　the fact that the klm properties characterize so many different semantic approaches has been viewed as rather surprising  since these approaches seem to capture quite different intuitions. as pearl 1 said of the equivalence between semantics and preferential structures   it is remarkable that two totally different interpretations of defaults yield identical sets of conclusions and identical sets of reasoning machinery.  plausibility measures help us understand why this should be so. in fact  plausibility measures provide a much deeper understanding of exactly what properties a semantic approach must have in order to be characterized by the klm properties.
　the first step to obtaining this understanding is to give semantics to defaults using plausibility. a plausibility structure is a tuple	  where	is a plausibility measure on	. a conditional	holds in this structure if either or	 where
is the set of worlds satisfying the formula  . this approach is just a generalization of the approach first used to define defaults with possibility measures  dubois and prade  1 .
note that if	satisfies cpl1  this is equivalent to saying that
	if	 the implicit as-
sumption here is that	iff	 .
　while this definition of defaults in terms of plausibility is easily seen to satisfy ref  rw  and lle  it does not satisfy and  or  or cm in general. it is easy to construct counterexamples taking to be a probability measure  in which case the definition boils down to if or
　　　　　　　　 . as observed earlier  if satisfies pl1  which it does not in general if is a probability measure   then the and rule is satisfied. as shown in  friedman and halpern  1a   pl1 also suffices to guarantee cm  cautious monotonicity . the only additional property that is needed to guarantee that or holds is the following:
pl1. if	  then	.
　a plausibility structure is qualitative if satisfies pl1. in  friedman and halpern  1a   it is shown that a necessary and sufficient condition for a collection of plausibility structures to satisfy the klm properties is that they be qualitative. more precisely  given a class of plausibility structures  a default is entailed by a set of defaults in   written   if all structures in that satisfy all the defaults in also satisfy . let consist of all qualitative plausibility structures. write p if is provable from using the klm properties.
theorem 1:  friedman and halpern  1a 	if
and only if for all	 	  and	  if	p	then
.
　in  friedman and halpern  1a   it is shown that possibility structures  -structures  preferential structures  and ppds can all be viewed as qualitative plausibility structures. theorem 1 thus shows why the klm properties hold in all these cases. why are there no further properties  that is  why are the klm properties not only sound  but complete   to show that the klm properties are complete with respect to a class of structures  we have to ensure that contains  enough  structures. in particular  if p   we want to ensure that there is a plausibility structure such that and . the following weak condition on guarantees this.
definition 1: we say that	is rich if for every collection
             of mutually exclusive formulas  there is a plausibility structure such that:

　the richness condition is quite mild. roughly speaking  it says that we do not have a priori constraints on the relative plausibilities of a collection of disjoint sets. it is easily seen to hold for the plausibility structures that arise from preferential structures  resp.  possibility structures  -structures  ppds . more importantly  richness is a necessary and sufficient condition to ensure that the klm properties are complete.
theorem 1:  friedman and halpern  1a  a set of qualitative plausibility structures is rich if and only if for all finite and defaults   we have that implies p .
this result shows that if the klm properties are sound with respect to a class of structures  then they are almost inevitably complete as well. more generally  theorems 1 and 1 explain why the klm properties are sound and complete for so many approaches.
　the discussion up to now has focused on propositional defaults  but using plausibility  it is fairly straightforward to extend to the first-order case; see  friedman et al.  1 .
1	expectation and decision theory
agents must make decisions. perhaps the best-known rule for decision making is that of maximizing expected utility. this requires that agents have probabilities for many events of interest  and numerical utilities. but many other decision rules have been proposed  including minimax  regret minimization  and rules that involve representations of uncertainty other than probability. again  using plausibility allows us to understand what is required to get various desirable properties of decision rules.
　since expectation plays such a key role in maximizing expected utility  i start by considering expectation. given a probability measure on some sample space   the corresponding expectation function maps gambles over  that is  random variables with domain and range the reals  to reals. there are a number of equivalent definitions of . the standard one is
		 1 
 here i am implicitly assuming that
　　　　　　  is measurable.  as is well known  is linear   that is  the set	   monotonic  if	  then   andit maps a constant function to its value  that is  is the gamble that maps all elements of to   the  . moreover  these three properties characterize probabilistic expectation functions. if an expectation function has these properties  then for some probability measure .
　a - expectation function is simply a mapping from random variables with domain and range some ordered set to . here i focus on expectation functions that are generated by some plausibility measure  just as is generated from   using a definition in the spirit of  1 . to do this  we need analogues of and   much in the spirit of  but not identical to  the and used in the definition of algebraic cps.
definition 1 :	an	expectation	domain	is	a	tuple
	  where	and	are sets ordered by	and
　　  respectively  is a set of plausibility values  so that it has elements and such that for all
	  	and	. 
　given an expectation domain and a plausibility measure on some set   it is possible to define a - expectation function by using the obvious analogue of  1   replacing by and by .
　what does this buy us  for one thing  we can try to characterize the properties of and that give properties of interest  such as linearity and monotonicity  see  halpern  1b  for details . for another  it turns out that all standard decision rules can be expressed as expected utility maximization with respect to an appropriate choice of plausibility measure    and . to make this precise  assume that there is a set of possible actions that an agent can perform. an action maps a world to an outcome. for simplicity  i identify outcomes with world-action pairs . assume that the agent has a utility function on outcomes. in the examples below  the range of the utility function is the reals but  in general  it can be an arbitrary partially ordered set . let be the random variable such that . the agent is uncertain about the actual world; this uncertainty is represented by some plausibility measure. the question is which action the agent should choose.
　as i said earlier  if the agent's uncertainty is represented by a probability measure   the standard decision rule is to choose the action that maximizes expected utility. that is  we choose an action such that for all .
however  there are other well-known decision rules.
	for minimax  let worst	;
worst is the utility of the worst-case outcome if is performed. this too leads to a total preference order on actions  where is preferred to if worst
worst	. the minimax rule says to choose the action
　 or one of them  in case of ties  such that worst is highest. the action chosen according to this rule is the one with the best worst-case outcome. notice that minimax makes sense no matter how uncertainty is represented. now take and   both with the standard order  and consider the plausibility measure   where if and
　　　　　　. let	be	and let	be multiplication. with this choice of	and	  it is easy to see that worst	  so expected utility maximiza-
tion with respect to	is minimax.
as a first step to defining regret minimization  for each world   let be an action that gives the best outcome in world ; that is  for all .
the regret of in world is ; that is  the regret of in is the difference between the utility of performing the best action in  the action that the agent would perform  presumably  if she knew the actual world was   and that of performing in . finally  define regret regret . intuitively  if regret   then is guaranteed to be within of the best action the agent could perform  even if the she knew exactly what the actual world was. the decision rule of minimizing regret chooses the action such that regret is a minimum.
to express regret in terms of maximizing expected utility  it is easiest to assume that for each action
 	.	this assumption is with-
out loss of generality:	if
                     then	  and minimizing regret with respect gives the same result as minimizing regret with respect to . with this assumption  take with the standard ordering and with the reverse ordering  that is
if	. let	  let
　　　　　　　  and let	. intuitively  is the regret an agent would feel if she is given utility but could have performed the action that would give her the best outcome on her choice of world in . with this choice of and   it is easy to see that regret   so expected utility maximization with respect to is just regret minimization  given the ordering on  .
suppose that uncertainty is represented by a set of probability measures indexed by some set . there are two natural ways to get a partial order on actions from
and a real-valued utility	. define	so that
iff	. that is  is preferred to if the expected utility of performing is at least that of performing   regardless which probability measure in describes the actual probability. naturally  is only a partial order on actions. a more refined partial order can be obtained as follows: define if for all .
it is easy to show that if then   although the converse may not hold. for example  suppose that and actions and are such that	 	 	  and
           . then	and	are incomparable according to	  but	.
	let the set	of plausibility values be that used for
     that is  the functions from to   with the pointwise ordering. let be the functions from to   let be pointwise addition  and let be pointwise multiplication. the difference between and is captured by considering two different orders on
	.	for	  order	by	  where	if
                         while for	  order by	  where	if	for all	.
if	is the expectation function corresponding to this definition of	and	  then it is easy to see that iff	  for	.
　it can be shown that every partial order on actions can be represented as the ordering induced by expected utility according to some plausibility measure on . that is  given some partial order on actions that can be taken in some set of possible worlds  there is a plausibility measure on and expectation domain such that the range
of	is	and a utility function on	with range
such that iff . thus  viewing decision rules as instances of expected utility maximization with respect to the appropriate expectation function provides a general framework in which to study decision rules. for example  it becomes possible to ask what properties of an expectation domain are needed to get various of savage's 1 postulates. i hope to report on this in future work.
1	compact representation of uncertainty
suppose that is a set of possible worlds characterized by binary random variables  or  equivalently  primitive propositions . that is  a world is a tuple   where is the value of .
that means that there are	worlds in	  say	.
a naive description of a probability measure on requires numbers    where is the probability of world .  of course  the probability of is determined by the other probabilities  since they must sum to 1. 
　if is relatively small  describing a probability measure in this naive way is not so unreasonable  but if is  say  1  certainly not unlikely in many practical applications   then it is completely infeasible. one of the most significant recent advances in ai has been in work on bayesian networks  pearl  1   a tool that allows probability measures to be represented in a compact way and manipulated in a computationally feasible way. i briefly review bayesian networks here and then discuss the extent to which the ideas can be applied to other representations of uncertainty. more details can be found in  halpern  1a .
　recall that a  qualitative  bayesian network  sometimes called a belief network  is a dag  that is  a directed acyclic graph  whose nodes are labeled by random variables. informally  the edges in a bayesian network can be thought of as representing causal influence.
　given a bayesian network and a node in   think of the ancestors of in the graph as those random variables that have a potential influence on . this influence is mediated through the parents of   those ancestors of directly connected to . that means that should be conditionally independent of its ancestors  given its parents. the formal definition requires  in fact  that be independent not only of its ancestors  but of its nondescendants  given its parents  where the nondescendants of are those nodes such that is not the ancestor of .
definition 1: given a qualitative bayesian network   let be the parents of the random variable in ; let be all the descendants of   that is  and all those nodes such that is an ancestor of ; let   the nondescendants of in   consist of . the
bayesian network  qualitatively  represents the probability measure if is conditionally independent of its nondescendants given its parents  for all . 
　a qualitative bayesian network gives qualitative information about dependence and independence  but does not actually give the values of the conditional probabilities. a quantitative bayesian network provides more quantitative information  by associating with each node in a conditional probability table  cpt  that quantifies the effects of the parents of on . for example  if 's parents in are and   then the cpt for would have an entry denoted
for all . as the notation is meant to suggest  for the plausibility
measure represented by .  of course  there is no need to have an entry for   since this is just	.  formally  a quantitative
bayesian network is a pair consisting of a qualitative bayesian network and a function that associates with each node in a cpt  where there is an entry in the interval in the cpt for each possible setting of the parents of . if is a root of   then the cpt for can be thought of as giving the unconditional probability that .
definition 1 : a quantitative bayesian network  quantitatively  represents  or is compatible with  the probability measure if qualitatively represents and the cpts agree with in that  for each random variable   the entry in the cpt for given some setting
of its parents is	if
	.	 it does not mat-
ter what the cpt entry for	is if
.  
　it can easily be shown using the chain rule for probability  see  for example   pearl  1   that if quantitatively represents   then can be completely reconstructed from
. more precisely  the values can be computed from ; from these values 
	can be computed for all	.
　bayesian networks for probability have a number of important properties:
1. every probability measure is represented by a qualitative bayesian network  in fact  in general there are many qualitative bayesian networks that represent a given probability measure .
1. a qualitative bayesian network that represents a probability measure can be extended to a quantitative bayesian network that represents   by adding cpts.
1. a quantitative bayesian network represents a unique probability measure. this is important because if a world is characterized by the values of random variables  so that there are worlds  a quantitative bayesian network can often represent a probability measure using far fewer than numbers. if a node in the network has parents  then its conditional probability table has entries. therefore  if each node has at most parents in the graph  then there are at most entries in all the cpts. if is small  then can be much smaller than .
1. a bayesian network supports efficient algorithms for computing conditional probabilities of the form
; that is  they allow for efficient
evaluation of probabilities given some information.
　to what extent is probability necessary to achieve these properties  more precisely  what properties of probability are needed to achieve them  here again  plausibility measures allow us to answer this question.
　given a cps   are plausibilistically independent given  with respect to    written
pl	  if	implies and	implies
　　　　. this definition is meant to capture the intuition that  conditional on   and are independent if learning about gives no information about and learning about gives no information about . note the explicitly symmetric nature of the definition. in the case of probability  if learning about gives no information about   then it is immediate that learning about gives no information about . this does not hold for an arbitrary plausibility measure.1
	if	 	  and
	are sets of random variables  then	and
are conditionally independent given	 with respect to	  if is conditionally independent of given	for
all choices of	.
　with these definitions  the notion of a qualitative bayesian network as defined in definition 1 makes perfect sense if the probability measure is replaced by a plausibility measure everywhere. the following result shows that representation by a qualitative bayesian network is possible not just in the case of probability  but for any algebraic cps.
theorem 1:   halpern  1a   if is an algebraic cps  then there is a qualitative bayesian network that represents .
　clearly a qualitative bayesian network that represents can be extended to a quantitative bayesian network that represents by filling in the conditional plausibility tables. but does a quantitative bayesian network represents a unique  algebraic  plausibility measure  recall that  for the purposes of this section  i have taken to consist of the worlds characterized by the binary random variables in . let pl consist of all algebraic standard cps's of the form   where   so that all subsets of are measurable  and the range of is . with this notation  the question becomes whether a quantitative bayesian network such that the entries in the cpts are in determines a unique element in pl . it turns out that the answer is yes  provided that satisfies some conditions. the conditions are similar in spirit to alg1  except that now they are conditions on   rather than conditions on a plausibility measure; i omit the details here  again  see  halpern  1a  . the key point is that these conditions are sufficient to allow an arbitrary plausibility measure to have a compact representation. moreover  since the typical algorithms in probabilistic bayesian networks use only algebraic properties of and   they apply with essentially no change to algebraic plausibility measures.
1	conclusions
there is no reason to believe that one representation of uncertainty is best for all applications. this makes it useful to have a framework in which to compare representations. as i hope i have convinced the reader  plausibility measures give us such a framework  and provide a vantage point from which to look at representations of uncertainty and understand what makes them tick-what properties of each one are being used to get results of interest. more discussion of these and related topics can be found in  halpern  1c .
references
 adams  1  e. adams. probability and the logic of conditionals. in j. hintikka and p. suppes  editors  aspects of inductive logic  pages 1. north holland  1.
 alchourro＞n et al.  1  c. e. alchourro＞n  p. ga：rdenfors  and d. makinson. on the logic of theory change: partial meet functions for contraction and revision. journal of symbolic logic  1-1  1.
 blume et al.  1  l. blume  a. brandenburger  and e. dekel. lexicographic probabilities and choice under uncertainty. econometrica  1 :1  1.
 boutilier et al.  1  c. boutilier  j. y. halpern  and n. friedman. belief revision with unreliable observations. in proc.aaai '1  pages 1  1.
 brandenburger and dekel  1  a. brandenburger and e. dekel. common knowledge with probability 1. journal of mathematical economics  1-1  1.  brandenburger and keisler  1  a. brandenburger and j. keisler. epistemic conditions for iterated admissibility. unpublished manuscript  1.
 brandenburger  1  a. brandenburger. on the existence of a  complete  belief model. working paper 1  harvard business school  1.
 dubois and prade  1  d. dubois and h. prade. an introduction to possibilistic and fuzzy logics. in g. shafer and j. pearl  editors  readings in uncertain reasoning  pages 1. morgan kaufmann  1.
 dubois and prade  1  d. dubois and h. prade. possibilistic logic  preferential models  non-monotonicity and related issues. in proc. ijcai '1  pages 1. 1.
 friedman and halpern  1  n. friedman and j. y. halpern. plausibility measures: a user's guide. in proc. uai '1  pages 1. 1.
 friedman and halpern  1a  n. friedman and j. y. halpern. plausibility measures and default reasoning. in proc. aaai '1  pages 1. 1. to appear  journal of the acm. full version available at http://www.cs.cornell.edu/home/halpern.
 friedman and halpern  1b  n. friedman and j. y. halpern. a qualitativemarkov assumption and its implications for belief change. in proc. uai '1  pages 1  1.
 friedman and halpern  1  n. friedman and j. y. halpern. modeling belief in dynamic systems. part i: foundations. artificial intelligence  1-1  1.
 friedman and halpern  1  n. friedman and j. y. halpern. modeling belief in dynamic systems. part ii: revision and update. journal of a.i. research  1-1  1.
 friedman et al.  1  n. friedman  j. y. halpern  and d. koller. first-order conditional logic for default reasoning revisited. acm trans. on computational logic  1 :1  1.
 gabbay et al.  1  d. m. gabbay  c. j. hogger  and j. a. robinson  editors. nonmonotonic reasoning and uncertain reasoning  volume 1 of handbook of logic in artificial intelligence and logic programming. oxford university press  1.
 ga：rdenfors  1  p. ga：rdenfors. knowledge in flux. mit press  1.
 ginsberg  1  m. l. ginsberg  editor. readings in nonmonotonic reasoning. morgan kaufmann  1.
 goldszmidt and pearl  1  m. goldszmidt and j. pearl. rank-based systems: a simple approach to belief revision  belief update and reasoning about evidence and actions. in kr '1  pages 1  1.
 goldszmidt and pearl  1  m. goldszmidt and j. pearl. qualitative probabilities for default reasoning  belief revision  and causal modeling. artificial intelligence  1- 1  1.
 halpern  1a  j. y. halpern. conditional plausibility measures and bayesian networks. in proc. uai 1  pages 1. 1.
 halpern  1b  j. y. halpern. on expectation. unpublished manuscript  1.
 halpern  1c  j. y. halpern. reasoning about uncertainty. book manuscript  1.
 halpern and fagin  1  j. y. halpern and r. fagin. two views of belief: belief as generalized probability and belief as evidence. artificial intelligence  1-1  1.
 hintikka  1  j. hintikka. knowledge and belief. cornell university press  1.
 katsuno and mendelzon  1  h. katsuno and a. mendelzon. on the difference between updating a knowledgebase and revising it. in principles of knowledge representation and reasoning: proc. kr '1  pages 1  1.
 kraus et al.  1  s. kraus  d. lehmann  and m. magidor. nonmonotonic reasoning  preferential models and cumulative logics. artificial intelligence  1-1  1.
 kyburg  1  h. e kyburg  jr. probability and the logic of rational belief. wesleyan university press  1.
 lehmann and magidor  1  d. lehmann and m. magidor. what does a conditional knowledge base entail  artificial intelligence  1-1  1.
 lewis  1  d. k. lewis. counterfactuals. harvard university press  1.
 mcgee  1  v. mcgee. learning the impossible. in e. eells and b. skyrms  editors  probability and conditionals. cambridge university press  1.
 myerson  1  r. myerson. multistage games with communication. econometrica  1-1  1.
 pearl  1  j. pearl. probabilistic reasoning in intelligent systems. morgan kaufmann  1.
 pearl  1  j. pearl. probabilistic semantics for nonmonotonic reasoning: a survey. in proc. kr '1  pages 1  1.
 popper  1  k. r. popper. the logic of scientific discovery  revised edition . hutchison  london  1. first appeared as logik der forschung  1.
 savage  1  l. j. savage. foundations of statistics. john wiley & sons  1.
 shafer  1  g. shafer. a mathematical theory of evidence. princeton university press  1.
 spohn  1  w. spohn. ordinal conditional functions: a dynamic theory of epistemic states. in w. harper and b. skyrms  editors  causation in decision  belief change  and statistics  volume 1  pages 1. reidel  1.
 van fraassen  1  b. c. van fraassen. fine-grained opinion  probability  and the logic of full belief. journal of philosophical logic  1-1  1.
 robust translation of spontaneous speech: a multi-engine approach 
wolfgang wahlster 
dfki  
stuhlsatzenhausweg 1 
d-1 saarbr┨cken  germany wahlster dfki.de abstract 
verbmobil is a speaker-independent and bidirectional speech-to-speech translation system for spontaneous dialogs that can be accessed via gsm mobile phones. it handles dialogs in three business-oriented domains  with context-sensitive translation between four languages  english  german  japanese  and chinese . we show that in verbmobil's multi-blackboard and multi-engine architecture the results of concurrent processing threads can be combined in an incremental fashion. we argue that all results of concurrent processing modules must come with a confidence value  so that statistically trained selection modules can choose the most promising result. packed representations together with formalisms for underspecification capture the uncertainties in each processing phase  so that the uncertainties can be reduced by linguistic  discourse and domain constraints as soon as they become applicable.  distinguishing features like the multilingual prosody  module and the generation of dialog summaries are highlighted.  we conclude that verbmobil  has successfully met the project goals with more than 1% of approximately correct translations  and a  1% success rate for dialog tasks. one of the main lessons learned from the verbmobil project is that the problem of speech-tospeech translation can only be cracked by the combined muscle of deep and shallow processing approaches. 
1 introduction  
verbmobil is a software system that provides mobile phone users with simultaneous dialog interpretation services for restricted topics  wahlster  1; 1b . as the name verbmobil suggests  the system supports verbal communication with foreign interlocutors in mobile situations. it recognizes spoken input  analyses and translates it  and finally utters the translation. the multilingual system handles dialogs in three business-oriented domains  with bidirectional translation between three languages  german  english  and japanese . in contrast to previous dialog translation systems that translate sentence-by-sentence  verbmobil provides context-sensitive translations. verbmobil uses an explicit dialog memory and exploits domain knowledge. the dialog context is used to resolve ambiguities and to produce an adequate translation in a particular conversational situation. 

figure 1: mobile  speech-to-speech translation with verbmobil  
figure 1 illustrates the use of verbmobil in a travel scenario. let's suppose that an american business traveller has arrived at frankfurt airport and wants to call mrs. meyer  the secretary of his german business partner. since he does not speak german and knows that the secretary does not speak english  he activates verbmobil using the voice dialing mode of his cell phone. after telling verbmobil the phone number of mrs. meyer  the speech translation system initiates a conference call between the american traveller  the german secretary and verbmobil. verbmobil translates all input of the american speaker into german and all input of the german speaker into english. 
    verbmobil is the first speech-only dialog translation system. verbmobil users can simply pick up a standard mobile phone and use speech commands in order to  initiate  a dialog translation session  see figure 1 .   the operation of  the final verbmobil system is completely hands-free without any push-to-talk button. since the verbmobil speech translation server can be accessed by gsm mobile telephones  the system can be used anywhere and anytime. no pc  notebook or pda must be available to access the verbmobil translation service  just a phone for each dialog participant. in addition  no waiting time for booting computers and keyboard or mouse input to start the verbmobil system is needed-dialog translation can begin instantaneously.   

figure 1:  three-party conference calls with verbmobil 
   verbmobil is the only dialog translation system to date based on an open microphone condition.  it is not a  push-totalk  system which has to be told which chunks of the sound signal represent coherent contributions by individual speakers: verbmobil works that out for itself from the raw input signal. the signal may be of different qualities-not necessarily from a lab-quality close-speaking microphone  for instance it can be gsm  cell phone  quality. thus  verbmobil includes different speech recognizers for 1 khz and 1 khz sampling rates. verbmobil is a speaker-adaptive system  i.e. for a new speaker it starts in a speaker-independent mode and after a few words have been uttered it improves the recognition results by adaptation. a cascade of unsupervised methods  ranging from very fast adaptation during the processing of a single utterance to complex adaptation methods that analyze a longer sequence of dialog turns  is used to adjust to the acoustic characteristics of the speaker's voice  the speaking rate  and pronounciation variants due to the dialectal diversity of the user community. 
1  understanding spontaneous speech 
verbmobil deals with spontaneous speech. this does not just mean continuous speech like in current dictation systems  but speech which includes realistic disfluencies and repair phenomena  such as changes of tack in mid-sentence  or mid-word   ums and ers  and cases where short words are accidentally left out in rapid speech. for example  in the verbmobil corpus about 1% of all dialog turns contain at least one selfcorrection and 1% include false starts. verbmobil uses a combination of shallow and deep analysis methods to recognize a speaker's slips and translate what he tried  to say rather than what he actually said.  
   at an early processing stage prosodic cues are used to detect self-corrections. a stochastic model is used to segment the repair into the  wrong   part  the so-called reparandum  and the correction. then the corrected input is inserted as a new hypothesis into the word hypotheses graph. thus  verbmobil's repair processing is a filter between speech recognition and syntactic analysis  spilker et al  1 . the 
word lattice is  augmented by an additional path that does   no 
 

figure 1: repairing self-corrections 
 longer contain those parts of the utterances that the speaker tried to correct. this transformation of the word lattice is used in addition to simple disfluency filtering  that eliminates sounds like ahh that users often make while speaking  see figure 1 . 
   in addition to this shallow statistical approach  other forms of self-corrections are also processed at a later stage on the semantic level. a rule-based repair approach is applied during robust semantic processing to a chart containing possible semantic interpretations of the input  the so-called vit hypotheses graph  vhg  . verbmobil applies various hand-crafted rules to detect repairs in semantic representations and to delete parts of the representation that corresponds to slips of the speaker  pinkal et al.  1  
 
 
figure 1: finding a spanning analysis by type-raising 
 
   due to a speech recognition error or a corrupted input signal  the word hypotheses graph in figure 1  does not contain any temporal preposition in front of the temporal nominal phrase the  late afternoon. a type coercion rule maps this phrase to a temporal modifier that expresses an underspecified temporal relation  that is later lexicalized as the  default in during language generation. 
 
verbmobil deals with mixed-initiative dialogs between human participants. each partner has a clear interaction goal in a negotiation task like appointment scheduling or travel planning. although these tasks encourage cooperative interaction  the participants have often conflicting goals and preferences that lead to argumentative dialogs. therefore verbmobil has to deal with a much richer set of dialog acts than previous systems that focused on information-seeking dialogs. 
   in order to ensure domain independence and scalability  verbmobil was developed for three domains of discourse  appointment scheduling  travel planning  remote pc maintenance  with increasing size of vocabularies and ontologies. the travel planning scenario with a vocabulary of 1 words was used for the end-to-end evaluation of the final verbmobil system. the pc maintenance task had a much larger vocabulary of almost 1 words from it sublanguage lexica. verbmobil is a hybrid system incorporating both deep and shallow processing schemes   bub et al.  1 . it integrates a broad spectrum of corpus-based and rule-based methods. verbmobil combines the results of machine learning from large corpora with linguists' handcrafted knowledge sources to achieve an adequate level of robustness and accuracy.  
1 verbmobil's training corpora 
a significant programme of data collection was performed during the verbmobil project to extract statistical properties from large corpora of spontaneous speech.  a distinguishing feature of the verbmobil speech corpus is the multi-channel recording. the voice of each speaker was recorded in parallel using a close-speaking microphone  a room microphone  and various telephones  gsm phone  wireless dect phone and regular phone   so that the speech recognizers could be trained on data sets with various audio signal qualities. the so-called partitur  german word for musical score  format used for the verbmobil speech corpora orchestrates fifteen strata of annotations  see figure 1   burger et al.  1  . multi-channel recordings of 1 spontaneous dialogs with 1 turns from 1 different speakers were transcribed and distributed on 1 cds with a total of 1 gb of annotated speech corpora  available from bas  see www.phonetik.uni-muenchen.de/bas/baskorporaeng.html .     in addition to the monolingual data  the multilingual verbmobil corpus includes bilingual dialogs  from wizardof-oz experiments  face-to-face dialogs with human interpreters  or dialogs interpreted by various versions of verbmobil  and aligned bilingual transliterations. three treebanks for  german  english and japanese have been developed with 1 trees annotated on three strata: morpho-syntax  phrase structure  and predicate-argument structure. the treebanks were used to train the statistical  par- 
 
figure 1:  verbmobil's multi-stratal annotation of speech  
ser and the chunk parser. in addition  machine learning methods were applied to the treebanks to extract semantic construction rules and transfer rules for translation. the endto-end evaluations of the various verbmobil prototypes have shown clearly  that the robustness  coverage  and accuracy of  a speech-to-speech translation system for spontaneous dialogs depends critically on the quantity and quality of  the training corpora. 
1 the anatomy of verbmobil 
a distinguishing feature of verbmobil is its multi-engine parsing and translation architecture. the screenshot of verbmobil's control panel provides an overview of the main components of the system  see figure 1 . the overall control and data flow is indicated by arrows pointing upwards on the left side of the screenshot  from left to right in the middle and downwards on the right side. on the bottom various input devices can be selected.  since verbmobil is a multilingual system it incorporates four speech recognizers and four speech synthesizers for german  english  japanese  and 
chinese.  
      three parsers based on different syntactic knowledge sources are used to process the word hypotheses graphs  whg  that are augmented by prosodic information extracted by the prosody module  see section 1 below . all parsers use the multi-stratal vit representation as an output format. vits  verbmobil interface terms  are used as a multi-stratal semantic representation by the central blackboards for the deep processing threads in verbmobil. the semantic   representation  in  a    vit is    augmented by    
 
figure 1: a snapshot of  verbmobil's control panel 
 
various features concerning morpho-syntax  tense  aspect  prosody  sortal restrictions and discourse information. vits form the input and output of the modules for  robust semantic processing  and semantic-based transfer. the initial design of the vit representation language was inspired by underspecified discourse representation structures  udrs   reyle  1 . vits provide a compact representation of lexical and structural ambiguities and scope underspecification of quantifiers  negations and adverbs. the linguistic information is encoded into variablefree sets of non-recursive terms  see figure 1 . these streams of literals serve as flat multi-stratal representations that are very efficient for incremental processing. the various linguistic strata are cross related by a labelling system. since vit terms are the central information structure in verbmobil  they are treated as an abstract data type. vits are used as a common representation scheme for linguistic information exchange between all components and processing threads of verbmobil.  
   since in most cases the parsers produce only fragmentary analyses  their results are combined in a chart of vit structures. a chart parser and a statistical lr parser are combined in a package that is visualized in the screenshot as  integrated processing .  these shallow parsers produce trees that are transformed into vit structures by a module called semantic construction  see figure 1 . this syntax-semantics interface is primarily lexically driven   schiehlen 1 . the module with the label  deep analysis   is based on a hpsg parser for deep linguistic processing in the verbmobil system. verbmobil is the only completely operational speech-tospeech translation system that is based on a wide-coverage unification grammar and tries to preserve the theoretical clarity and elegance of linguistic analyses in a very efficient implementation. the parser for the hpsg grammars processes the n best paths produced by the integrated processing module. it is implemented as a bidirectional 
bottom-up active chart parser  kiefer et al.  1  
 
 
vit  vitid  sid  1 a en 1 1 en y semantics   % segmentid 
 	 	 word  he  1       	 % whg string 
 	 	 word is  1      	 	 word coming  1      	 	 word at  1      	 	 word the  1      	 	 word beginning  1      	 	 word of  1     
 	 	 word ``august''  1       
  index  1  1  i1     % index   beginning  1  i1      % conditions 
  	 arg1  1  i1  i1     	 come  1  i1     	 arg1  1  i1  i1     	 decl  1  h1     	 pron  1  i1     	 at  1  i1  i1     	 mofy  1  i1  aug     	 def  1  i1  h1  h1   
  	 udef  1  i1  h1  h1    
 	 in g  1  1   in g  1  1    	 % constraints 
  	 in g  1  1   in g  1  1     	 in g  1  1   in g  1  1     	 in g  1  1   in g  1  1     	 leq  1  h1   leq  1  h1     	 leq  1  h1   leq  1  h1     	 leq  1  h1   leq  1  h1   
  	 leq  1  h1    
 	 s sort  i1  situation    	 % sorts 
  	 s sort  i1  time   
 	 s sort  i1  time    
 	 dialog act  1  inform    	 % discourse 
  	 dir  1  no   
  	 prontype  i1  third std    
 	 cas  i1  nom    	 % syntax 
  	 gend  i1  masc     	 num  i1  sg   num  i1  sg   num  i1  sg     	 pcase  l1  i1  of    
 	 ta aspect  i1  progr    	 % tense and aspect 
   ta mood  i1  ind      ta perf  i1  nonperf      ta tense  i1  pres    
 	 pros accent  l1    	 % prosody 
 
figure 1: vit for  he is coming at the beginning of august    
 
   the statistical translation module starts with the single best sentence hypothesis of the speech recognizer  vogel et al.  1 . prosodic information about phrase boundaries and sentence mode are utilized by the statistical translation module. the output of this module is a sequence of words in the target language together with a confidence measure that is used by the selection module  not shown in the control panel  for the final choice of a translation result. verbmobil includes two components for case-based translation. substring-based translation is a method for incremental synchronous interpretation  that is based on machine learning methods applied to a sentence-aligned bilingual corpus. substrings of the input for which a contiguous piece of translation can be found in the corpus are the basic processing units. substring pairs are combined with patterns for word order switching and word cluster information in an incremental translation algorithm for a sequence of input segments  block  1 . the other component for case-based translation is based on 1 translation templates learned from a sentence-aligned corpus. date  time and naming expressions are recognized by definite clause grammars  dcgs  and marked in the whg. an a* search explores the cross-product graph of the whg with the subphrase tags and the template graph. a dcgbased generator is used to produce target language output from the interlingual representation of the recognized date  time and naming expressions. these subphrases are used to instantiate the target language parts of  translation templates. 
   dialog-act based translation includes the statistical classification of 1 dialog acts and a cascade of more than 1 finite-state transducers that extract the main propositional content of an utterance. the statistical dialog classifier is based on n-grams and takes the previous dialog history into account. the recognized dialog act  the topic and propositional content are represented by a simplistic frame notation including 1 nested objects with 1 possible attributes covering the appointment scheduling and travel planning tasks. a template-based approach to generation is used to transform these interlingual terms into the corresponding target language. the shallow interlingual representation of an utterance is stored together with topic and focus information as well as a deep semantic representation encoded as a vit in the dialog memory for further processing by the dialog and context evaluation component. 
 
 
figure 1: the use of stochastic dialog act and plan recognition 
   the dialog component includes a plan processor  that structures an ongoing dialog hierarchically in different dialog phases  games and moves. dialog acts are the terminal nodes of the tree structure that represents the dialog structure. information about the dialog phase is used e.g. during the semantic-based transfer for disambiguation tasks  see figure 1 . in addition  inference services are provided  by the dialog and context component eg. for the completion of underspecified temporal expressions and the resolution of anaphora or ellipsis. temporal reasoning is used for example to transform expressions like two hours later or next week into fully specified times and dates stored in the dialog memory for summarizing the results of a negotiation. the transfer module triggers contextual reasoning process only in cases where a disambiguation or resolution is necessary for a given translation task. for example  the german noun essen can be translated into lunch or dinner depending on the time of day  which can be derived by contextual reasoning. disambiguation and resolution on demand is typical for verbmobil's approach to translation  since various forms of underspecification and ambiguity can be carried over into target language  so that the hearer can resolve them. consider the german sentence wir treffen die kollegen in berlin and its english equivalent we will meet the colleagues in berlin. english and german have the same pp-attachment ambiguity in which in berlin is either attached to the noun phrase the colleagues or to the verb meet. 
   the transfer component is basically a rewriting system for underspecified semantic representations using verbmobil's vit formalism  see emele et al.  1 . semantic-based transfer receives a vit of a source language utterance and transforms it into a vit for the target language synthesis. this means that the transfer module abstracts away from morphological and syntactic analysis results. the final verbmobil system includes more than 1 transfer rules. these rules include conditions that can trigger inferences in the dialog and context evaluation module to resolve ambiguities and deal with translation mismatches  whenever necessary. the transfer component uses cascaded rule systems  first for the phrasal transfer of idioms and other noncompositional expressions and then for the lexical transfer. the translation of spatial and temporal prepositions is based on an interlingual representation in order to cut down the number of specific transfer rules. semantic-based transfer is extremely fast and consumes on the average less than 1% of the overall processing time for an utterance. 
   verbmobil's multilingual generator includes a constraintbased microplanning component and a syntactic realization module that is based on the formalism of lexicalized treeadjoining grammars  see becker et al   1 . the input to the microplanning component are vits produced by the transfer module. a sentence plan is generated that consists basically of lexical items and semantic roles linking them together. the microplanner decides about subordination  aggregation  focus and theme control as well as anaphora generation. the syntactic realization component can either use ltag grammars that are compiled from the hpsg grammars used for deep analysis or a hand-written ltag generation grammar. for english and japanese the grammars that were designed for analysis are usable for generation after an offline-compilation step. 
   the speech synthesizer for german and american english follows a concatenative approach based on a large corpus of annotated speech data. the word is the basic unit of concatenation  so that subword units are only used if a word is not available in the database. 
   the synthesizer applies a graph-based unit selection procedure to choose the best available synthesis segments matching the segmental and prosodic constraints of the input. whenever possible the synthesizer exploits the syntactic  prosodic and discourse information provided by previous processing stages. thus for the deep processing stream it provides concept-to-speech synthesis  whereas for the shallow translation threads it operates more like a traditional text-tospeech system resulting in a lower quality of its output. 
   another novel functional feature of verbmobil is the ability to generate dialog summaries. suppose that two speakers negotiate a travel plan:  one can ask the system either to specify the final agreement  omitting the negotiating steps  or to summarize the steps of argument while leaving out irrelevant details of wording. a dialog summary can be produced on demand after the end of a conversation.  
   the summaries are based on the semantic representation of all dialog turns stored in the dialog memory of verbmobil. it is interesting to note that dialog summaries are mainly a bye-product of the deep processing thread and the dialog processor of verbmobil. the most specific accepted negotiation results are selected from the dialog memory  alexandersson et al.  1 . the semantic-based transfer component and the natural language generators for german and english are used for the production of multilingual summaries. this means that after a conversation over a cell phone the participants can ask for a written summary of the dialog in their own language. the dialog summary can be sent as an html document using email. in the context of business negotiations verbmobil's ability to produce written dialog summaries of a phone conversation is an important value-added service. 
 
1 exploiting prosodic information   
verbmobil is the first spoken-dialog interpretation system that uses prosodic information systematically at all processing stages. the results of verbmobil's multilingual prosody module are used for parsing  dialog understanding  translation  generation and speech synthesis  see figure 1 . this means that prosodic information in the source utterance is passed even through the translation process to improve the generation and synthesis of the target utterance. prosodic differences in one language can correspond to lexical or syntactic differences in another; for instance  a german utterance beginning wir haben noch ... may be translated by verbmobil into english either as we still have ... or as we have another ... depending whether noch is stressed. although prosody is used in some other recent speech recognition systems  the exploitation of prosodic information is extremely limited in these approaches. for example  the atr matrix system  see takezawa et al.  1  uses prosody only to identify sentence mood  declarative vs. question . we believe that verbmobil is the first fully operational system to make significant use of  prosodic aspects of speech.   
   the prosody module of verbmobil uses the speech signal and the word hypotheses graph  whg  produced by the speech recognizer as an input and outputs an annotated whg with prosodic information for each recognized word. the system extracts duration  pitch  energy  and pause features and uses them to classify phrase and clause boundaries  accented words and sentence mood. a combination of a multilayer perceptron and a polygram-based statistical language model annotate the whg with probabilities for the classified prosodic events. 
   verbmobil uses the probabilistic prosodic information about clause boundaries to reduce the search space for syntactic analysis dramatically. during parsing  the clause boundary marks that are inserted into the whg by the prosody module play the role of punctuation marks in written language. dialog act segmentation and recognition is also based on the boundary information provided by the prosody module. prosodic cues about sentence mood is often used in verbmobil's translation modules to constrain transfer results  if there is not enough syntactic or semantic evidence for a certain mood  e.g. question . the information about word accent is used to guide lexical choice in the generation process. finally  during speech synthesis the extracted prosodic features are used for speaker-adaptation. 
 

figure 1: the role of  prosodic information in verbmobil 
1 verbmobil's multi-blackboard architecture 
the final verbmobil system consists of 1 highly interactive modules. the transformation of speech input in a source language into speech output in a target language requires a tremendous amount of communication between all these modules. since verbmobil has to translate under real-time conditions it exploits parallel processing schemes whenever possible. the non-sequential nature of the verbmobil architecture implies that not only inputs and results are exchanged between modules but also top-down expectations  constraints  backtracking signals  alternate hypotheses  additional parameters  probabilities  and confidence values.  
   1 blackboards are used for the necessary information exchange between modules. a module typically subscribes to various blackboards. modules can have several instances  e.g. in a multiparty conversation there may be two german speakers  so that two instances of the german speech recognition module are needed.  
 

figure 1: some key blackboards with their subscribing modules 
 
the final verbmobil system is based on a multi-blackboard architecture that pools processing modules around blackboards representing intermediate results at each processing stage  see figure 1 . it turned out that such a multi-blackboard approach is much more efficient than the more general multi-agent architecture used in the first verbmobil prototype. due to the huge amount of interaction between modules a multi-agent architecture with direct communication among module agents would imply 1 different interfaces for message exchanges between the 1 agents. 
   in a multi-blackboard architecture based on packed representations at all processing stages  speech recognition  parsing  semantic processing  translation  generation  speech synthesis  using charts with underspecified representations the results of concurrent processing threads can be combined in an incremental fashion. all results of concurrent processing modules come with a confidence value  so that selection modules can choose the most promising results at each processing stage or delay the decision until more information becomes available. packed representations such as the whg  word hypotheses graph  and vhg  vit hypotheses graph  together with formalisms for underspecification capture the non-determinism in each processing phase  so that the remaining uncertainties can be reduced by linguistic  discourse and domain constraints as soon as they become applicable.  
1 verbmobil's multi-engine approach 
verbmobil performs language identification  parsing and translation with several engines simultaneously. whereas the multi-engine parsing results are combined and merged into a single chart  a statistical selection module chooses between the alternate results of the concurrent translation threads  so that only a single translation is used for generating the system's output.  
 
   verbmobil uses three parallel parsing threads: an incremental chunk parser  a probabilistic lr parser and a hpsg parser.  these parsers cover a broad spectrum with regard to their robustness and accuracy. the chunk parser  hinrichs et al.  1  produces the most robust but least accurate results  whereas the hpsg parser delivers the most accurate but least robust analysis. all parsers process the same word hypotheses graph with its prosodic annotations. the search for the best scored path  according to the acoustic score and the language model  is controlled  by a central a* algorithm that guides the three parsers through the word hypotheses graph. the hpsg parser may return more than one analysis for ambiguous inputs  whereas the chunk parser and statistical parser return always only one result. each parser uses a semantic construction  component to transform its analysis results into a semantic representation term.  
 
augmentedaugmented
word hypothesesword hypotheses graphgraph
	statistical parserstatistical parser	chunk parserchunk parser	hpsg parserhpsg parser
partial vits
	partial vits	chartwith achartwith a	partial vits
combination ofcombination of
partial vitspartial vits
robust dialog semanticsrobust dialogsemantics
combination and knowledg-ecombination and knowledg-e based reconstruction ofbased reconstruction of complete vitscomplete vits
complete and spanningcomplete and spanning
vitvits
figure 1: verbmobil's multi-engine parsing approach 
   even partial results of the different parsing engines are integrated  into a chart of vits  that is further analyzed by the robust semantic processing component  see figure 1 . 
   the final verbmobil system includes five translation engines  see figure 1 : statistical translation  case-based translation  substring-based translation  dialog-act based translation  and semantic transfer. these engines cover a wide spectrum of translation methods. while statistical translation is very robust against speech recognition problems and produces quick-and-dirty results  semantic transfer is computationally more expensive and less robust but produces higher quality translations. however  it is one of  the fundamental insights gained from the verbmobil project  that the problem of robust  efficient and reliable speech-to-speech translation can only be cracked by the combined muscle of deep and shallow processing approaches.  
the translation quality of the final verbmobil system was rigorously evaluated. 1 evaluators checked 1 verbmobil translations and judged their correctness. we call a translation  approximately correct   if it preserves the intention of the speaker and the main information of his utterance. table 1  shows clearly that no single translation engine achieves more than 1% approximately correct translations  but that the selection of the appropriate translation result increases the overall performance significantly. in verbmobil  we used the judgements of the human evaluators  see table 1  manual selection  to construct a training corpus for an instance-based learning algorithm that picks the best translation for a given whg of a particular turn segment  see table 1  automatic selection . 
 

 
table 1: quality of  translations from german to english 
 
 
   the language identification component of verbmobil uses also a multi-engine approach to identify each user's input language. the three instances of the multilingual speech recognizer for german  english  and japanese run concurrently for the three first seconds of speech input.  a confidence measure is used to decide which language is spoken by a particular dialog participant. the language identification component switches to the selected recognizer that produces a word hypotheses graph for the full utterance. verbmobil's error rate for this type of language identification task is only 1%  see waibel et al.  1 . verbmobil's architecture supports multiple process instances of all components  so that verbmobil can be used as a translation server for multiparty dialogs  e.g. two germans  a japanese and an american planning a joint trip . 
	segment 1segment 1	segment 1segment 1
	wenn wir dwenn wir dif you prefer another hotel if you prefer another hotel n termin vorziehen n termin vorziehen 	das w┨rde mir gut passen.das w┨rde mir gut passen.please let me know.please let me know.
	case-basedcase-based	substring-basedsubstring-based	statisticalstatistical	dialog-act baseddialog-act based	semanticsemantic
	translationtranslation	 translation translation	translationtranslation	translationtranslation	transfertransfer
alternative translations with confidence values
selection moduleselection module
	segment 1segment 1	segment 1segment 1
	translated by semantic transfertranslated by semantic transfer	translated by case-based translationtranslated by case-based translation
figure 1. the multi-engine translation approach of verbmobil 
1   lessons learned from verbmobil 
   the broad range of scientific discoveries in the areas of speech  language and discourse processing  dialog translation  language generation and speech synthesis that resulted from the verbmobil project are documented in more than 1 publications  www.verbmobil.de  and a comprehensive book  wahlster  1a . 
   one of the main lessons learned from the verbmobil project is that the problem of  speech-to-speech translation of spontaneous dialogs can only be cracked by the combined muscle of deep and shallow processing approaches: 
- deep processing can be used for merging  completing and repairing the results of shallow processing strategies 
- shallow methods can be used to guide the search in deep processing 
- statistical methods must be augmented by symbolic models to achieve higher accuracy and broader coverage 
- statistical methods can be used to learn operators or selection strategies for symbolic processes 
   the final verbmobil architecture supports large and robust dialog systems and maximizes the necessary interaction between processing modules:  
- in verbmobil's multi-blackboard and multi-engine architecture   that is based on packed representations on all processing levels and uses charts with underspecified multi-stratal representations   the results of concurrent processing threads can be combined in an incremental fashion 
- all results of concurrent and competing processing modules come with a confidence value  so that statistically trained selection modules can choose the most promising result at each stage  if demanded by a following processing step. 
- packed representations together with formalisms for underspecification capture the uncertainties in each processing phase  so that the uncertainties can be reduced by linguistic  discourse and domain constraints as soon as they become applicable.  in particular  underspecification allows disambiguation requirements to be delayed until later processing stages where better-informed decisions can be made. 
- the massive use of underspecification makes the syntax-semantic interface and transfer rules almost deterministic  thereby boosting processing speed. 
   verbmobil has shown the need to take software engineering considerations in language technology projects seriously.  verbmobil's system integration group included professional software engineers with no particular language or speech technology background; they were responsible for ensuring that the software is robust and maintainable  and that modules developed in different programming languages by a distributed team fit together properly.  these issues are too important to leave to subject specialists who see them as a side issue. an important achievement of the verbmobil project is the consistent integration of a very large number of modules created by diverse groups of researchers from disparate disciplines and  to produce a set of capabilities which have not been demonstrated in an integrated speech-tospeech translation system before.   
   organizationally  verbmobil underlines the importance of competition among research teams  with frequent objective evaluations.  competition was fostered naturally within the verbmobil framework  because the processing model itself is a competitive one. crucial to the success of verbmobil was the fact that various teams within the project developed rival solutions to particular tasks  with formal evaluations being used to winnow out the most successful or to combine it with the next best solutions to improve the overall performance of the system. 
   the objective of the public funding provided by the german federal ministry of education and research  bmbf  for the verbmobil project has been to bring european language technology to the stage of achieving real industrial impact by the turn of the century. participating companies developing spin-off applications at their own expense have already brought twenty products to market that are all based on results from verbmobil.  there have been various patents and inventions resulting from verbmobil  in areas such as speech processing  parsing  dialog  machine translation and generation. seven spin-off companies in language technology have been created by former verbmobil researchers. for example   aixplain  www.aixplain.de  markets speech translation systems  sympalog  www.sympalog.de  develops spoken dialog systems  and xtramind  www.xtramind.com  delivers email response systems based on verbmobil technology.  at present  verbmobil's large industrial partners  daimlerchrysler  philips  siemens  temic  are among the top european companies using language technology in  the marketplace. 
   the sharable language resources collected and distributed during the verbmobil project will be useful beyond the project lifespan  since these transliterated and richly annotated corpora of spontaneously spoken dialogs can be used for building  improving or evaluating natural language and speech algorithms or systems in coming years. 
   along the way  the verbmobil project has done a great deal to bring researchers in germany together across the language/speech and the academic/industrial divides. this is an important contribution from the point of view of a longrange research policy for the field of human language technology. more than 1 young researchers  among them 1 master students  1 phd students  and 1 habilitation  postdocs  gained experience in advanced speech and language technology through their work on verbmobil during  the project lifespan.  
1 conclusion and future work 
although verbmobil was a high-risk and long-term project  1 - 1   it has successfully met its technical project goals. the verbmobil consortium brought together 1 partners across three continents. the total amount of public and private funding was about $1 million  resulting in europe's largest ai project. the technical challenge was to design and implement  
 
  a speaker-independent and bidirectional speech-tospeech translation system for spontaneous dialogs in mobile situations 
  that works in an open microphone mode and can cope with speech over gsm mobile phones 
  for four language pairs  three domains and a vocabulary size of more than 1 word forms 
  with an average processing time of four times of the input signal duration  
  with a word recognition rate of more than 1% for spontaneous speech 
  with more than 1% of approximately correct translations that preserve the speaker's intended effect on the recipient in a large-scale translation experiment  
  a  1% success rate for dialog tasks in end-to-end evaluations with real users 
   various benchmark tests and large-scale end-to-end evaluation experiments with unseen test data have convincingly shown that all these objectives have been met by the final verbmobil system and some goals have been surpassed  tessiore and v. hahn  1 . 
       smartkom  1  is the follow-up project to verbmobil and reuses some of verbmobil's components for the understanding of spontaneous dialogs. smartkom is a multimodal dialog system that combines speech  gesture  and mimics input and output  wahlster et al  1 . spontaneous speech understanding is combined with the video-based  recognition of natural gestures. one of the major scientific goals of smartkom is to design new computational methods for the seamless integration and mutual disambiguation of multimodal input and output on a semantic and pragmatic level. smartkom is based on the situated delegation-oriented dialog paradigm  sddp   in which the user delegates a task to a virtual communication assistant  visualized as a life-like character on a graphical display.  the main contractor of the smartkom consortium is the german research center for artificial intelligence  dfki . the major industrial partners involved in smartkom are daimlerchrysler  philips  siemens and sony. 
