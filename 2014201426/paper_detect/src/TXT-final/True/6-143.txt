 
intelligent agents must function in an uncertain world  containing multiple objects and relations that change over time. unfortunately  no representation is currently available that can handle all these issues  while allowing for principled and efficient inference. this paper addresses this need by introducing dynamic probabilistic relational models  dprms . dprms are an extension of dynamic bayesian networks  dbns  where each time slice  and its dependences on previous slices  is represented by a probabilistic relational model  prm . particle filtering  the standard method for inference in dbns  has severe limitations when applied to dprms  but we are able to greatly improve its performance through a form of relational rao-blackwellisation. further gains in efficiency arc obtained through the use of abstraction trees  a novel data structure. we successfully apply dprms to execution monitoring and fault diagnosis of an assembly plan  in which a complex product is gradually constructed from subparts. 
1 introduction 
sequential phenomena abound in the world  and uncertainty is a common feature of them. currently the most powerful representation available for such phenomena is dynamic bayesian networks  or dbns  dean and kanazawa  1 . dbns represent the state of the world as a set of variables  and model the probabilistic dependencies of the variables within and between time steps. while a major advance over previous approaches  dbns are still unable to compactly represent many real-world domains. in particular  domains can contain multiple objects and classes of objects  as well as multiple kinds of relations among them; and objects and relations can appear and disappear over time. for example  manufacturing plants assemble complex artifacts  e.g.  cars  computers  aircraft  from large numbers of component parts  using multiple kinds of machines and operations. capturing such a domain in a dbn would require exhaustively representing all possible objects and relations among them. this raises two problems. the first one is that the computational cost of using such a dbn would likely be prohibitive. the second is that reducing the rich structure of the domain to a very large   flat  dbn would render it essentially incomprehensible to 
1 
human beings. this paper addresses these two problems by introducing an extension of dbns that exposes the domain's relational structure  and by developing methods for efficient inference in this representation. 
¡¡formalisms that can represent objects and relations  as opposed to just variables  have a long history in ai. recently  significant progress has been made in combining them with a principled treatment of uncertainty. in particular  probabilistic relational models or prms  friedman et al  1  are an extension of bayesian networks that allows reasoning with classes  objects and relations. the representation we introduce in this paper extends prms to sequential problems in the same way that dbns extend bayesian networks. we thus call it dynamic probabilistic relational models  or dprms. we develop an efficient inference procedure for dprms by adapting rao-blackwcllised particle filtering  a state-of-theart inference method for dbns  murphy and russell  1 . we introduce abstraction trees as a data structure to reduce the computational cost of inference in dprms. 
¡¡early fault detection in complex manufacturing processes can greatly reduce their cost. in this paper we apply dprms to monitoring the execution of assembly plans  and show that our inference methods scale to problems with over a thousand objects and thousands of steps. other domains where we envisage dprms being useful include robot control  vision in motion  language processing  computational modeling of markets  battlefield management  cell biology  ecosystem modeling  and the web. 
¡¡the rest of the paper is structured as follows. the next two sections briefly review dbns and prms. we then introduce dprms and methods for inference in them. the following section reports on our experimental study in assembly plan monitoring. the paper concludes with a discussion of related and future work. 
1 dynamic bayesian networks 
a bayesian network encodes the joint probability distribution of a set of variables  as a directed acyclic graph and a set of conditional probability models. each node corresponds to a variable  and the model associated with it allows us to compute the probability of a state of the variable given the state of its parents. the set of parents of  denoted  is the set of nodes with an arc to in the graph. the structure of the network encodes the assertion that each node is conditionally independent of its non-descendants given its parents. the probability of an ar can then be computed as 
¡¡dynamic bayesian networks  dbns  are an extension of bayesian networks for modeling dynamic systems. in a dbn  the state at time t is represented by a set of random variables 
the state at time t is dependent on 
the states at previous time steps. typically  we assume that each state only depends on the immediately preceding state  i.e.  the system is first-order markovian   and thus we need to represent the transition distribution . . this can be done using a two-time-slice bayesian network fragment  1tbn  . which contains variables from whose parents are variables from and/or and variables from  without any parents. typically  we also assume that the process is stationary  i.e.  the transition models for all time slices are identical: 	thus a 
dbn is defined to be a pair of bayesian networks i where represents the initial distribution p{zo   and is a two-time-slice bayesian network  which as discussed above defines the transition distribution 
¡¡the set is commonly divided into two sets: the unobserved state variables and the observed variables . the observed variables are assumed to depend only on the current state variables . the joint diwstribution represented by a dbn can then be obtained by unrolling the 1tbn: 

¡¡various types of inference in dbns are possible. one of the most useful is state monitoring  also known as filtering or tracking   where the goal is to estimate the current state of the world given the observations made up to the present  i.e.  to compute the distribution proper state 
monitoring is a necessary precondition tor rational decisionmaking in dynamic domains. inference in dbns is npcomplete  and thus we must resort to approximate methods  of which the most widely used one is particle filtering  doucet et 1/.  1 . particle filtering is a stochastic algorithm which maintains a set of particles  samples  to approximately represent the distribution of possible states at time given the observations. each particle contains a complete instance of the current state  i.e.  a sampled value for each state variable. the current distribution is then approximated by 
where 	is 1 if the state represented b y i s same as and 1 otherwise. the particle filter starts by generating tv 
particles according to the initial distribution p then  at each step  it first generates the next state for each particle by sampling from it then weights these samples according to the likelihood they assign to the observations  and resamples n particles from this weighted distribution. the particles will thus tend to stay clustered in the more probable regions of the state space  according to the observations. 
¡¡although particle filtering has scored impressive successes in many practical applications  it also has some significant limitations. one that is of particular concern to us here is that it tends to perform poorly in high-dimensional state spaces. this is because the number of particles required to maintain a good approximation to the state distribution grows very rapidly with the dimensionality. this problem can be greatly attenuated by analytically marginalizing out some of the variables  a technique known as rao-blackwellisation  murphy 
and russell  1  . suppose the state space can be divided into two subspaces and such that can be computed analytically and efficiently. then we only need to sample from the smaller space   requiring far fewer particles to obtain the same degree of approximation. each particle is now composed of a sample from 
plus a parametric representation of p  for example  if the variables in are discrete and independent of each other given we can store for each variable the vector of parameters of the corresponding multinomial distribution  i.e.  the probability of each value . 
1 probabilistic relational models 
a relational schema is a set of classes where each class c is associated with a set of propositional attributes   a n d a set of relational attributes or reference slots . the propositional attribute a of class c is denoted c.a  and its domain  assumed finite  is denoted v c.a . the relational attribute r of c is denoted c.r  and its domain is the power set  of a target class in other words  c.r is a set of objects belonging to some class  for example  the aircraft schema might be used to represent partially or completely assembled aircraft  with classes corresponding to different types of parts like metal sheets  nuts and bolts. the propositional attributes of a bolt might include its color  weight  and dimensions  and its relational attributes might include the nut it is attached to and the two metal sheets it is bolting. an instantiation of a schema is a set of objects  each object belonging to some class with all propositional and relational attributes of each object specified. for example  an instantiation of the aircraft schema might be a particular airplane  with all parts  their properties and their arrangement specified. 
¡¡a probabilistic relational model  prm  encodes a probability distribution over the set of all possible instantiations / of a schema  friedman et al.  1 . the object skeleton of an instantiation is the set of objects in it  with all attributes unspecified. the relational skeleton of an instantiation is the set of objects in it  with all relational attributes specified  and all propositional attributes unspecified. in the simplest case  the relational skeleton is assumed known  and the prm specifies a probability distribution for each attribute a of each class c. the parents of each attribute  i.e.  the variables it depends on  can be other attributes of c  or attributes of 
     lc.r can also be defined as a function from but we choose the simpler convention here. 
classes that are related to c by some slot chain. a slot chain is a composition of relational attributes. in general  it must be used together with an aggregation function that reduces a variable number of values to a single value. for example  a parent of an attribute of a bolt in the aircraft schema might be avg bolt.plate.nut.weight   the average weight of all the nuts on the metal plates that the bolt is attached to. definition 1 a probabilistic relational model  prm  ii for a relational schema s is defined as follows. for each class c and each propositional attribute a € -1 c   we have: 
¡¡a prm and relational skeleton can thus be unrolled into a large bayesian network with one variable for each attribute of each object in the skeleton.1 only prms that correspond to bayesian networks without cycles are valid. 
¡¡more generally  only the object skeleton might be known  in which case the prm also needs to specify a distribution over the relational attributes  getoor et al.  1 . in the aircraft domain  a prm might specify a distribution over the state of assembly of an airplane  with probabilities for different faults  e.g.  a bolt is loose  the wrong plates have been bolted  etc. . 
1 dynamic probabilistic relational models 
in this section we extend prms to modeling dynamic systems  the same way that dbns extend bayesian networks. we begin with the observation that a dbn can be viewed as a special case of a prm  whose schema contains only one class z with propositional attributes z  ...  za and a single relational attribute previous. there is one object zt for each time slice  and the previous attribute connects it to the object in the previous time slice. given a relational schema s  we first extend each class c with the relational attribute c.previous  with domain c. as before  we initially assume that the relational skeleton at each time slice is known. we can then define two-time-slice prms and dynamic prms as follows. 
   plus auxiliary  deterministic  variables for the required aggregations  which we omit from the formula for simplicity. 
1 
¡¡dprms are extended to the case where only the object skeleton for each time slice is known in the same way that prms are  by adding to definition 1 a set of parents and conditional probability model for each relational attribute  where the parents can be in the same or the previous time slice. when the object skeleton is not known  e.g.  if objects can appear and disappear over time   the 1tprm includes in addition a boolean existence variable for each possible object  again with parents from the same or the previous time slice.1 as with dbns  we may wish to distinguish between observed and unobserved attributes of objects. in addition  wc can consider an action class with a single attribute whose domain is the set of actions that can be performed by some agent  e.g.  painting a metal plate  or bolting two plates together . the distribution over instantiations in a time slice can then depend on the action performed in that time slice. for example  
the action may with high probability produce and with lower probability set 
part j.mate to some other object of part1's class  i.e.  be improperly performed  resulting in a fault . 
¡¡just as a prm can be unrolled into a bayesian network  so can a dprm be unrolled into a dbn.  note  however  that this dbn may in general contain different variables in different time slices.  in principle  we can perform inference on this dbn using particle filtering. however  the filter is likely to perform poorly  because for non-trivial dprms its state space will be huge. not only will it contain one variable for each attribute of each object of each class  but relational attributes will in general have very large domains. we overcome this by adapting rao-blackwellisation to the relational setting. we make the following  strong  assumptions: 
1. relational attributes with unknown values do not appear anywhere in the dprm as parents of unobserved attributes  or in their slot chains. 
1. each reference slot can be occupied by at most one object. 
proposition 1 assumptions 1 and 1 imply that  given the propositional attributes and known relational attributes at times t and t - 1  the joint distribution of the unobserved relational attributes at time t is a product of multinomials  one for each attribute. 
¡¡notice also that  by assumption 1  unobserved propositional attributes can be sampled without regard to unobserved relational ones. rao-blackwellisation can now be applied with  as the propositional attributes of all objects and as their relational attributes. a rao-blackwellised particle is composed of sampled values for all propositional attributes of all objects  plus a probability vector for each relational attribute of each object. the vector element corresponding to  is the probability that relation r holds between obj 
and the zth object of the target class  conditioned on the values of the propositional attributes in the particle  etc. 
¡¡rao-blackwellising the relational attributes can vastly reduce the size of the state space which particle filtering needs to sample. however  if the relational skeleton contains a large number of objects and relations  storing and updating all the requisite probabilities can still become quite expensive. this can be ameliorated if context-specific independencies exist  i.e.  if a relational attribute is independent of some propositional attributes given assignments of values to others  boutilier et aly 1 . we can then replace the vector of probabilities with a tree structure whose leaves represent probabilities for entire sets of objects. more precisely  we define the abstraction tree data structure for a relational attribute obj.r with target class   as follows. a node of the tree is composed of a probability p and a logical expression over the propositional attributes of the schema. let be the set of objects in that satisfy the 's of and all of 's an-
1 experiments 
in this section we study the application of dprms to fault detection in complex assembly plans. we use a modified version of the schedule world domain from the aips-1 planning competition.1 the problem consists of generating a plan for assembly of objects with operations such as painting  polishing  etc. each object has attributes such as surface type  color  hole size  etc. we add two relational operations to the domain: bolting and welding. we assume that actions may be faulty  with fault model described below. in our experiments  we first generate a plan using the ff planner  hoffmann and nebel  1   we then monitor the plan's execution using particle filtering  pf   rao-blackwellised particle filtering  rbpf  and rbpf with abstraction trees. 
¡¡we consider three classes of objects: plate  bracket and bolt. plate and bracket have propositional attributes such as weight  shape  color  surface type  hole size and hole type  and relational attributes for the parts they are welded to and the bolts bolting them to other parts  e.g.  plate1.bolt1 corresponds to the fourth bolt hole on plate 1 . the bolt class has propositional attributes such as size  type and weight. propositional actions include painting  drilling and polishing  and change the propositional attributes of an object. the relational action bolt sets a bolt attribute of a plate or bracket object to a bolt object. the weld action sets a welded-to attribute of a plate or bracket object to another plate or bracket object. 
¡¡the fault model has a global parameter  the fault probability   with probability 1 - an action produces the intended effect. with probability   one of several possible faults occurs. propositional faults include a painting operation not being completed  the wrong color being used  the polish of an object being ruined  etc. the probability of different propositional faults depends on the properties of the object being acted on. relational faults include bolting the wrong objects and welding the wrong objects. the probability of choosing a particular wrong object depends on its similarity to the intended object. similarity depends on different propositional attributes for different actions and different classes of objects. thus the probability of a particular wrong object being chosen is uniform across all objects with the same relevant attribute values. 
the dprm also includes the following observation model. 
there are two instances of each attribute: the true one  which is never observed  and the observed one  which is observed at selected time steps. specifically  when an action is performed  all attributes of the objects involved in it are observed  and no others. observations are noisy: with probability the true value of the attribute is observed  and with probability an incorrect value is observed. incorrect values for propositional observations are chosen uniformly. incorrect values for relational observations are chosen with a probability that depends on the similarity of the incorrect object to the intended one. 
¡¡notice that  if the domain consisted exclusively of the propositional attributes and actions on them  exact inference might be possible; however  the dependence of relational attributes and their observations on the propositional attributes 

creates complex dependencies between these  making approximate inference necessary. 
¡¡a natural measure of the accuracy of an approximate inference procedure is the k-l divergence between the distribution it predicts and the actual one  cover and thomas  1 . however  computing it requires performing exact inference  which for non-trivial dprms is infeasible. thus we estimate the k-l divergence by sampling  as follows. let d be the k-l divergence between the true distribution p and its approximation p  and let  be the domain over which the distribution is defined. then 
the first term is simply the entropy of x  h x   and is a constant independent of the approximation method. since we are mainly interested in measuring differences in performance between approximation methods  this term can be neglected. the k-l divergence can now be approximated in the usual way by taking s samples from the true distribution: 
where is the probability of the zth sample according to the approximation procedure  and the h subscript indicates that the estimate of is offset by h x . we thus evaluate the accuracy of pf and rbpf on a dprm by generating s = 1 sequences of states and observations from the dprm  passing the observations to the particle filter  inferring the marginal probability of the sampled value of each state variable at each step  plugging these values into the above formula  and averaging over all variables. notice 
that whenever a sampled value is not rep-
resented in any particle. the empirical estimates of the k-l divergence we obtain will be optimistic in the sense that the true k-l divergence may be infinity  but the estimated one will still be finite unless one of the values with zero predicted probability is sampled. this does not preclude a meaningful comparison between approximation methods  however  since on average the worse method should produce earlier in the time sequence. we thus report both the average k-l divergence before it becomes infinity and the time step at which it becomes infinity  if any. 
¡¡figures 1 and 1 show the results of the experiments performed. the observation noise parameter was set to the same value as the fault probability throughout. one action is performed in each time step; thus the number of time steps is the length of the plan. the graphs show the k-l divergence of pf and rbpf at every 1th step  it is the same for rbpf with and without abstraction trees . graphs are interrupted at the first point where the k-l divergence became infinite in any of the runs  once infinite  the k-l divergence never went back to being finite in any of the runs   and that point is labeled with the average time step at which the blow-up occurred. as can be seen  pf tends to diverge rapidly  while the figure 1: comparison of rbpf  1 particles  and pf  1 particles  for 1 objects and varying fault probability. 

figure 1: comparison of rbpf  1 particles  and pf  1 particles  for fault probability of 1% and varying number of objects. 
k-l divergence of rbpf increases only very slowly  for all combinations of parameters tried. abstraction trees reduced rbpf's time and memory by a factor of 1 to 1  and took on average six times longer and 1 times the memory of pf  per particle. however  note that we ran pf with 1 times more particles than rbpf. thus  rbpf is using less time and memory than pf  and performing far better in accuracy. 
¡¡we also ran all the experiments while measuring the k-l divergence of the full joint distribution of the state  as opposed to just the marginals . rbpf performed even better compared to pf in this case; the latter tends to blow up much sooner  e.g.  from around step 1 to less than 1 for 
¡¡¡¡¡¡1% and 1 objects   while rbpf continues to degrade only very slowly. 
1 related work 
dynamic object-oriented bayesian networks  doobns   friedman et al  1  combine dbns with oobns  a predecessor of prms. unfortunately  no efficient inference 

methods were proposed for doobns  and they have not been evaluated experimentally. dprms can also be viewed as extending relational markov models  rmms   anderson et al  1  and logical hidden markov models  lohmms   kersting et al  1  in the same way that dbns extend hmms. downstream  dprms should be relevant to research on relational markov decision processes  e.g.   boutilier et al  1  . 
particle filtering is currently a very active area of research 
 doucet et al.  1 . in particular  the fastslam algorithm uses a tree structure to speed up rbpf with gaussian variables  montemerlo et al.  1 . abstraction trees are also related to the abstraction hierarchies in rmms  anderson et al.  1  and to ad-trees  moore and lee  1 . an alternate method for efficient inference in dbns that may also be useful in dprms was proposed by boyen and roller  and combined with particle filtering by ng et al. . efficient inference in relational probabilistic models has been studied by pasula and russell . 
1 conclusions and future work 
this paper introduces dynamic probabilistic relational models  dprms   a representation that handles time-changing phenomena  relational structure and uncertainty in a principled manner. we develop efficient approximate inference methods for dprms  based on rao-blackwellisation of relational attributes and abstraction trees. the power of dprms and the scalability of these inference methods are illustrated by their application to monitoring assembly processes for fault detection. 
¡¡directions for future work include relaxing the assumptions made  further scaling up inference  formally studying the properties of abstraction trees  handling continuous variables  learning dprms  using them as a basis for relational mdps  and applying them to increasingly complex real-world problems. 
1 acknowledgements 
this work was partly supported by an nsf career award to the second author  by onr grant n1-1  and by nasa grant nag 1. we are grateful to mausam for helpful discussions. 
