 
the 1-puzzle is the largest puzzle of its type that can be completely solved. it is simple  and yet obeys a combinatorially large problem space of 1!/1 states. the n x n extension of the 1-puzzle is np-hard. 
in the first part of this paper  we present complete statistical data based on an exhaustive evaluation of all possible tile configurations. our results include data on the expected solution lengths  the 'easiest' and 'worst' configurations  and the density and distribution of solution nodes in the search tree. in our second set of experiments  we used the 1-puzzle as a workbench model to evaluate the benefit of node ordering schemes in iterativedeepening a*  ida* . one highlight of our results is that almost all ida* implementations perform worse than would be possible with a simple random ordering of the operators. 
1 	introduction 
the 1-puzzle is a prominent workbench model for measuring the performance of heuristic search algorithms 
 gaschnig  1; nilsson  1; pearl  1; russell  1   learning methods  laird et a/.  1  and the use of macro operators  korf  1a . it is simple  but has a 
combinatorially large problem space of 1!/1 states. the 1-puzzle is the largest possible n-puzzle that can be completely solved. there exist larger variants  e.g. the 1puzzle  johnson and storey  1   which can also be solved  korf  1b   but not to completion. the general n x n extension of the 1-puzzle is np-hard  ratner and warmuth  1 . 
complete solution. we enumerated all 1!/1 tile configurations and computed all optimal  shortest  solution paths for all problem instances with a fast iterativedeepening search algorithm. aside from schofield's  analysis of a weaker 1-puzzle variant  our work is the first that gives complete statistical data on this application. our results  for a quick overview see the table in the conclusions  includes statistical data on 
  the lengths of optimal solutions  
1 	constraint satisfaction problems 
  the solution density in the search tree  
  the  hardest  and  easiest  configurations  
  the average heuristic branching factor  
  the distribution of goal nodes in the search frontier. 
benefit of node ordering in i d a * . our experiments were also motivated by the need for a better understanding of the effectiveness of node ordering schemes in the ida* search algorithm  korf  1b . like any other iterative-deepening search  ida* benefits by a good node expansion order  which reduces the time spent in the last  goal  iteration. common ordering techniques include basic hill-climbing methods  the longestpath  principal variation  heuristic  the history heuristic and transposition tables. while some of these methods seem to work in practice  e.g. for the traveling salesman problem  it remained unclear how effective they are and which combination of the methods performs best. our results give evidence that 
  chances to find a solution early in the last iteration are extremely poor for common ida* implementations; simple random node selection is better  
  steepest ascent hill-climbing does not help  
  a  history  table that holds a success score for each operator is better than random ordering  
  exploring the longest path first yields good perfor-mance while requiring only o depth  storage space  
  the benefits of transposition tables are mainly due to caching previously expanded nodes rather than to an improved operator ordering. 
1 	the 1-puzzle 
the objective of the 1-puzzle is to rearrange a given initial configuration of eight squared tiles on a 1 x 1 board into a specified goal configuration by successively sliding tiles into the orthogonally adjacent empty square  the blank square . while it would seem easy to find any solution to this problem  we are only interested in obtaining optimal solutions with the fewest moves. we take  by convention  the following configuration to be the goal state: 


figure 1: distribution of optimal solution lengths /* 

　there exist 1! possible tile permutations on a 1 x 1 board  and every second permutation is solvable  johnson and storey  1 . hence  there is a total of 1!/1 = 1 solvable problem instances. 
　a weaker variant of the 1-puzzle has been investigated by schofield  with a mixed analytical/empirical approach. his 1-puzzle variant includes only mappings from one 'standard position' into another  where a 'standard position' is one with the blank located in the middle square. the search space of this weaker puzzle has only 1!/1 = 1 states. our definition of the 1-puzzle  in contrast  conforms to the larger 1- and 1-puzzles  which have the blank in the upper left square and which do not restrict the search space to 'standard positions'. 
1 	complete solution 
we generated all 1!/1 solvable tile configurations and computed all optimal solutions for all problem instances. we used korfs ida* algorithm  for a description see 
section 1  with the manhattan distance as a heuristic estimate function. the whole experiment took less than one hour cpu-time on a sun-sparcstation. 
solution lengths. the average length of all optimal solution paths is f* = 1  that is  almost 1 moves are needed to solve a given random configuration. figure 1 shows the distribution of the optimal solution path lengths for all tile configurations. the two cases with the 

figure 1: number of solutions per problem 
shortest  non-trivial  solution path f* = 1 have either of tile 1 or 1 in the upper left corner: 

the two configurations with the longest optimal solution path f* = 1 are: 

interestingly  these configurations do not build the largest trees. their  complete  search trees have both 1 leaves  whereas the 'hardest' configurations spawn trees with three times as many nodes. similar to other np-complete problems  cheeseman et al.  1   the 1puzzle has a small number of very hard problem instances which require significantly more leaf node expansions than average. 
solutions per problem. for the 1 solvable configurations  we found a total of 1 optimal solutions. this gives an average solution density of 1 per problem  with the minimum and maximum number lying at 1 and 1 solutions  respectively  see figure 1 . about 1 of the problem instances have only one  two or three solutions. the two configurations with the most  1  solutions are: 

the tiles are almost reversely ordered in these configurations  resulting in long solution paths with many move transpositions. the search tree is relatively large  with 


figure 1: sample tree searched with cost-bound 1. filled circles are leaves. 
1 total leaves   but it is not the largest  which has 
1 leaves . there are 1 problems with more than 1 leaves on the optimal search frontier. 
1 	benefit of node ordering in ida* 
i t e r a t i v e - d e e p e n i n g a * . i d a *  korf  1b  is an efficient search method for applications with high heuristic branching factors and low solution densities  rao et a/.  1 . typical applications of i d a * include single 
agent games like the tv-puzzle  some variants of the traveling salesman problem  floorplan optimization and the cutting stock problem. 
   i d a * performs a series of depth-first searches with successively increased cost-bounds. the total cost f   n   of a node n is made up of the cost already spent in reaching that node g n   plus the estimated cost of the path to a goal state h n . at the beginning  the cost bound is set to the heuristic estimate of the initial state  h root . then  for each iteration  the bound is increased to the minimum path value that exceeded the previous bound: 

   with an admissible  non-overestimating  heuristic function h  i d a * is guaranteed to find an optimal  shortest  solution path  korf  1b . moreover  ida* obeys the same asymptotic branching factor as a*  if the number of newly expanded nodes grows exponentially with the search depth  korf  1b; mahanti et a/.  1 . this growth rate  the heuristic branching factor bh  is defined as the node ratio of two consecutive iterations. 
1 	constraint satisfaction problems 
it depends on the number of applicable operators per node  the edge branching factor be  and the discrimination power of the heuristic estimate function h. using the manhattan distance  the sum of the minimum displacement of each tile from its goal position  as a heuristic estimate function  we computed bh = 1 for the 1-puzzle. the corresponding value for the 1-puzzle  bh= 1  is much higher because there are more interior squares with more move options  reinefeld and marsland  1 . 
   the time to solve a problem is dominated by the time spent in the final  goal  iteration  which in turn depends on the expansion order of the leaves. a node is said to be expanded  when all its successors have been generated  or when it is a goal node. a leaf is an expanded node that has no expanded successors. figure 1 illustrates a sample tree with three leaves. they all lie on a search frontier of an ida* search with cost bound 1. we are primarily interested in optimal search frontiers that contain one or more goal nodes  as shown in the example. 
n o r m a l i z a t i o n . we want to find out how node ordering techniques improve the chances to find a solution early in the search frontier. for this purpose  we run ida* on all 1-puzzle configurations and gathered statistical data on the location of goal nodes in the search front. in order to allow a direct comparison of the experimental data  we normalized the search frontiers  which vary in size between 1 and 1 leaves  to a common scale of 1 discrete data points. here  1 and 1 correspond to the location of the leftmost and the rightmost leaf node  respectively. when the leaves are numbered from left  = 1  to right  = n   the i-th leaf lies on position  in the normalized search frontier. 
　for trees with more than 1 leaves  normalization is no problem. care must be taken when there are only few leaves  because this causes the values to cluster at certain data points. as an example  the first leaf in figure 1 lies at location the second at  and the third at of the normalized search frontier. 
f i x e d successor o r d e r i n g . for practical reasons  common ida* implementations generate the node successors in an arbitrary  but fixed sequence  e.g. up  left  right  down . with such a scheme  one would expect the goal nodes to be evenly distributed over the whole search frontier. surprisingly  this is not the case. figure 1 depicts the distribution of all goal nodes in the last  deepest  search frontier  'all-solution-case' . the continuous plot shows the distribution of goal nodes with a fixed successor ordering. in the middle part  each goal location occurs = 1 times. this is as it should be  because the occurrences of all 1 goal locations must add up to the total solution number 1. the zigzag appearance with extreme points at specific locations  is caused by the normalization of small search frontiers with   1 leaves  as discussed above. 
   more interesting are the 'shoulders' at both sides of the graph. they indicate a lower solution density in the left and right parts of the search frontier. this interesting phenomenon reveals a serious deficiency of our and presumably most other - i d a * implementation: to 


figure 1: location of all solutions in the search front 	figure 1: location of first solution in the search front 

be time efficient  common implementations use a table driven operator selection scheme  which applies the operators always in the same order. our program  and also korf's  1b   first tries to move the blank up  then to the left  to the right and finally down. if done in a row  this results in a circular move path which finally runs into a dead end situation. hence  chances to find a solution in the first path of the search tree are low. 
random node selection is better. figure 1 clearly indicates  that a fixed operator sequence is worse than average. the chances to hit a solution right at the beginning of the search tree can be greatly improved by randomly perturbing the operator sequence in each node. the resulting solution density graph  not shown here  is almost flat in the left part  as it should be. 
expanding the longest path first. while simple random node ordering outperforms any kind of fixed operator sequence  we can do much better by applying domain dependent heuristics. one possibility is to investigate the deepest path of the previous iteration first. in multi-agent games  this path is called principal variation. it is the preferred move sequence  if both parties adhere to the minimax principle. in single-agent  ida*  search  the longest path is the one that came closest to the goal. since all nodes in the search frontier have the same /-value  the leaves on the longest paths have the highest g- and consequently lowest h-values. 
the dotted graph in figure 1 shows a much increased goal density in the left hand part of the search frontier. the chances to find a goal in the first leaf node are more than three times higher than with a random node selection scheme. consequently  the right part of the graph lies at a lower level  because all cases again add up to 1. note that the longest-path heuristic affects only the first few leaf expansions  because only one path is stored. however  additional experiments with a refined version that stores all longest paths were not successful  because the branching factor is too low in the 1-puzzle. sophisticated variants only pay off in domains with a large branching factor  like the traveling salesman problem  reinefeld and marsland  1 . 
　in practice  it often suffices to determine one optimal solution. when the search is stopped after a first solution  the benefit of the longest-path heuristic is even more pronounced  as can be seen in figure 1. other favorite aspects of this heuristic are its low space complexity of o f*  and its negligible cpu time overhead. 
steepest ascent hill-climbing does not help. 
hill-climbing methods have been found useful in many ai applications. they sort the node successors in increasing order of their heuristic estimates. successors with low h-values are expanded first  with the expectation that they are closer to a goal. our empirical results  however  did not reveal any significant advantage of hillclimbing. this observation confirms an earlier assumption of powley and korf  l1l  and our own experiments 

with the larger 1-puzzle. in the 1-puzzle  we spotted a slight advantage of hill climbing in the left part of the tree  but this does not pay for the increased overhead of move pre-sorting. a quick investigation  reinefeld and marsland  1  reveals that hill-climbing favors tile configurations with the blank located in either an edge or border square  because these configurations enjoy  statistically  lower h-values. such configurations  however  have a lower 'mobility' and are thus less desirable. 
h i s t o r y h e u r i s t i c . dynamically acquired knowledge usually outperforms static sorting schemes  such as hillclimbing . one dynamic method  that proved useful in adversary games  is schaeffer's  history heuristic. it maintains a score table  the history table  for every move seen in the search graph. in a given position  all applicable moves are examined in order of their previous success. 
   the history heuristic does not depend on domain specific knowledge  like heuristic estimate functions . it simply learns from the success in previously expanded subtrees. for the 1-puzzle  one needs a three dimensional array that holds a measure of the goodness of a move for each possible tile  each source position and each move direction. this gives 1  tiles  x 1  positions  x 1  move directions  =: 1 scores. as a measure of the goodness of a move  we counted the number of occurrences the specific move led to the deepest subtree. 
　compared to the longest-path heuristic  the history heuristic provides ordering information for all nodes in the tree  and not only for the nodes on the first path. even so  our empirical results  see the dashed lines in 
figures 1 and 1  indicate  that the history heuristic is dominated by the longest-path heuristic. this might be attributed to the low branching factor of the 1-puzzle  and the inexact methods to measure the goodness of a move. we expect the history heuristic to be more suitable in domains with a larger branching factor and a fine grained evaluation function. 
t r a n s p o s i t i o n table. transposition tables are used in two-player games  zobrist  1  to avoid unnecessary re-expansions of duplicate nodes  due to move transpositions and due to node re-expansions in successive iterations . when there is enough memory space available  transposition tables are also used to store node information for guiding the search process into the most promising direction. 
   we implemented a transposition table that holds a representation of every tile configuration seen in the search  the cost-bound to which the configuration has been searched and a best move  the move leading to the deepest subtree . as expected  the transposition table greatly reduces the number of node expansions  see figure 1. we achieved an average 1% node count reduction per problem. however  most of the savings are due to eliminated transpositions. node ordering plays only a minor role  and so the solution density graph looks very similar to that of the longest-path heuristic in figures 1 and 1  hence we did not plot it . this is again caused by the difficulties to distinguish between 'good' and 'bad' moves. we took the depth of the emanating subtree as 
1 	constraint satisfaction problems 

figure 1: node expansions to first solution 
a utility measure. but many subtrees end at the same level and hence a finer grained scoring function is needed. another problem is the low branching factor of this specific application  which leaves only few node successors to be sorted. 
1 	conclusions 
table 1 gives a summary of our empirical results. the 1-puzzle data has been derived by exhaustively solving all possible board configurations. for the 1-puzzle  no exact data can be given  because the search space is too large to be completely enumerated. we used korf's  1b  selection of 1 randomly generated problem instances as a test set1. 
   in our second set of experiments  we used the 1-puzzle as a workbench to evaluate the benefit of node ordering techniques in iterative-deepening search. surprisingly  we found that common i d a * implementations with a fixed operator sequence  e.g. up  left  right  down  perform worse than average. a simple random operator selection scheme is better! 
the longest-path heuristic was found most effective. 
consisting of a linear moves array  it is easy to implement and its space overhead of o depth  is negligible. more sophisticated ordering techniques did not yield better performance  because the 1-puzzle has a low branching factor and it lacks a clear criterion for measuring the goodness of a move. 
1
　　we identified three cases with differing node counts  which are probably due to typographical errors in the original paper  korf  1b : 
no 	korf 	our version 	difference 
	1 	1 1 	1 1 	-1 
1 1 1 	1 1 	+1 1 
1 1 	1 1 	-1 


table 1: summary: the 1-puzzle results are exact; the 1-puzzle data is based on korf's random problem set 

acknowledgements 
thanks to sam loyd for providing the ai community with such an entertaining research subject as early as 1  he surely must have recognized the beauty of simple problem structures that spawn combinatorially large search spaces. thanks also to martin lehmann  univ. hamburg  for his continuous interest and many stimulating discussions. 
