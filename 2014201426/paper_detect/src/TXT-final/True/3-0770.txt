
it is well known that in unidentifiable models  the bayes estimation has the advantage of generalization performance to the maximum likelihood estimation. however  accurate approximation of the posterior distribution requires huge computational costs. in this paper  we consider an empirical bayes approach where a part of the parameters are regarded as hyperparameters  which we call a subspace bayes approach  and theoretically analyze the generalization error of three-layer linear neural networks. we show that a subspace bayes approach is asymptotically equivalent to a positivepart james-stein type shrinkage estimation  and behaves similarly to the bayes estimation in typical cases.
1 introduction
unidentifiable parametric models  such as neural networks  mixture models  and so on  have a wide range of applications. these models have singularities in the parameter space  hence the conventional learning theory of the regular statistical models does not hold. recently  generalization performance of some unidentifiable models has been theoretically clarified. in the maximum likelihood  ml  estimation  which is asymptotically equivalent to the maximum a posterior  map  estimation  the generalization error of linear neural networks was proved to be greater than that of the regular models whose dimension of the parameter space is the same when the model is redundant to learn the true distribution  fukumizu  1 . on the other hand  in the bayes estimation  the generalization error of neural networks  linear neural networks  mixture models  and so on was proved to be less than that of the regular models  watanabe  1; aoyagi and watanabe  1; yamazaki and watanabe  1 .
¡¡however  the bayes posterior distribution can seldom be exactly realized. furthermore  markov chain monte carlo
 mcmc  methods  often used for approximation of the posterior distribution  require huge computational costs. as an alternative  the variational bayes approach  where the correlation between parameters and the other parameters  or the correlation between the parameters and the hidden variables is neglected  was proposed  hinton and van camp  1; mackay  1; attias  1; ghahramani and beal  1 .1
¡¡in this paper  we consider another alternative  which we call a subspace bayes  sb  approach. an sb approach is an empirical bayes  eb  approach where a part of the parameters of a model are regarded as hyperparameters. if we regard the parameters of one layer as hyperparameters  we can analytically calculate the marginal likelihood in some threelayer models. consequently  what we have to do is only to find the hyperparameter value maximizing the marginal likelihood. the computational costs of an sb approach is thus much less than that of posterior distribution approximation by mcmc methods. at first in this paper  we prove that in threelayer linear neural networks  an sb approach is equivalent to a positive-part james-stein  js  type shrinkage estimation  james and stein  1 . then  we clarify its generalization error  also considering delicate situations  the most important situations in model selection problems and in statistical tests  when the kullback-leibler divergence of the true distribution from the singularities is comparable to the inverse of the number of training samples.1 we conclude that an sb approach provides as good performance as the bayes estimation in typical cases.
¡¡in section 1  neural networks and linear neural networks are briefly introduced. the framework of the bayes estimation  that of an eb approach  and that of an sb approach are described in section 1. the significance of singularities for generalization performance and the importance of analysis of delicate situations are explained in section 1. the sb solution and its generalization error are derived in section 1. discussions and conclusions follow in section 1 and in section 1  respectively.
1 linear neural networks
let x ¡Ê rm be an input  column  vector  y ¡Ê rn an output vector  and w a parameter vector. a neural network model can be described as a parametric family of maps {f ¡¤;w  :
. a three-layer neural network with h hidden units is defined by
 1 
wheresummarizes all the parameters  ¦× ¡¤  is an activation function  which is usually a bounded  non-decreasing  antisymmetric  nonlinear function like tanh ¡¤   and t denotes the transpose of a matrix or vector. assume that the output is observed with a
noise subject to nn 1 ¦Ò1in   where nd ¦Ì ¦²  denotes the d-dimensional normal distribution with average vector ¦Ì and covariance matrix ¦²  and id denotes the d¡Ád identity matrix. then  the conditional distribution is given by

in this paper  we focus on linear neural networks  whose activation function is linear  as the simplest multilayer models.1
a linear neural network model  lnn  is defined by
	f x;a b  = bax 	 1 
where a =  a1 ... ah t is an h ¡Á m input parameter matrix and b =  b1 ... bh  is an n ¡Á h output parameter matrix. because the transformdoes not change the map for any non-singular h ¡Á h matrix t  the parameterization in eq. 1  has trivial redundancy. accordingly  the essential dimension of the parameter space is given by
	k = h m + n    h1.	 1 
we assume that h ¡Ü n ¡Ü m throughout this paper.
1 framework of learning methods
1 bayes estimation
let xn = {x1 ... xn} and y n = {y1 ... yn} be arbitrary n training samples independently and identically taken from the true distribution q x y  = q x q y|x . the marginal conditional likelihood of a model p y|x w  is given by
		 1 
where ¦Õ w  is the prior distribution. the posterior distribution is given by
	 	 1 
and the predictive distribution is defined as the average of the model over the posterior distribution as follows:

the generalization error  a criterion of generalization performance  is defined by
	 	 1 
where

is the kullback-leibler  kl  divergence of the predictive distribution from the true distribution  and denotes the expectation value over all sets of n training samples.
1 empirical bayes approach and subspace bayes approach
we often have little information about the prior distribution  with which an eb approach was originally proposed to cope. we can introduce hyperparameters in the prior distribution; for example  when we use a prior distribution that depends on a hyperparameter ¦Ó1 such as
	 	 1 
the marginal likelihood  eq. 1   also depends on ¦Ó1. in an eb approach  ¦Ó1 is estimated by maximizing the marginal likelihood or by a slightly different way  efron and morris  1; akaike  1; kass and steffey  1 . extending the idea above  we can introduce hyperparameters also in a model distribution. what we call an sb approach is an eb approach where a part of the parameters of a model are regarded as hyperparameters. in the following sections  we analyze two versions of sb approach: in the first one  we regard the output parameter matrix b of the map  eq. 1   as a hyperparameter and then marginalize the likelihood in the input parameter space  mip ; and in the other one  we regard the input parameter matrix a  instead of b  as a hyperparameter and then marginalize in the output parameter space  mop .
1 unidentifiability and singularities
we say that a parametric model is unidentifiable if the map from the parameter to the probability distribution is not oneto-one. a neural network model  eq. 1   is unidentifiable because the model is independent of ah when bh = 1  or vice versa. the continuous points denoting the same distribution are called the singularities  because the fisher information matrix on them degenerates. when the true model is not on the singularities  asymptotically they do not affect prediction  and therefore  the conventional learning theory of the regular models holds. on the other hand  when the true model is on the singularities  they significantly affect generalization performance as follows: in the ml estimation  the extent of the set of the points denoting the true distribution increases its neighborhoods and hence the flexibility of imitating noises  and therefore  accelerates overfitting; while in the bayes estimation  the large entropy of the singularities increases the weights of the distributions near the true one  and therefore  suppresses overfitting. in lnns  the former property appears as acceleration of overfitting by selection of the largest singular value components of a random matrix  and in the sb approaches of lnns  the latter property appears as james-stein type shrinkage  as shown in the following sections.
¡¡suppression of overfitting accompanies insensitivity to the true components with small amplitude. there is a trade-off  which would  however  be ignored in asymptotic analysis if we would consider only situations when the true model is distinctly on the singularities or not. therefore  in this paper  we also consider delicate situations when the kl divergence of the true distribution from the singularities is comparable to the inverse of the number of training samples  n 1  which are important situations in model selection problems and in statistical tests with finite number of samples for the following reasons: first  that there naturally exist a few true components with amplitude comparable to n 1 when neither the smallest nor the largest model is selected; and secondly  that whether the selected model involves such components essentially affects generalization performance.
1 theoretical analysis
1 subspace bayes solution
by  we  hereafter  distinguish the hyperparameter ¦Ó from the parameter w  for example . assume that the variance of a noise is known and equal to unity. then  the conditional distribution of an lnn in the mip version of sb approach is given by

we use the following prior distribution:
	.	 1 
note that we can similarly prepare for the mop version. we assume that the true conditional distribution is  where b a  is the true map with rank. we denote by   the true value of a parameter as above  and by a hat an estimator of a parameter  for example  a    bh  etc.. for simplicity  we assume that the input vector is orthonormalized so that. consequently  the central limit theorem leads to the following two equations:

where q xn  is an m ¡Á m symmetric matrix and r xn y n  is an n ¡Á m matrix. hereafter  we abbreviate q xn  as q  and r xn y n  as r.
¡¡let ¦Ãh be the h-th largest singular value of the matrix rq 1  ¦Øah the corresponding right singular vector  and ¦Øbh the corresponding left singular vector. we find from eq. 1  that ¦Ãh for h    h ¡Ü h is of order o n 1 .
hence  combining with eq. 1   we get
   ¦Øbhrq¦Ñ = ¦Øbhr + o n 1  for h   h ¡Ü h   1  where  ¡Þ   ¦Ñ ¡Ê r   ¡Þ is an arbitrary constant. the
sb estimator  defined as the expectation value over the sb posterior distribution  is given by the following theorem:
theorem1 let l = m in the mip version or l = n in the mop version  and. the sb estimator of
the map of an lnn is given by

 the proof is given in appendix a. 
¡¡because the independence between a and b makes the posterior distribution localized  the following lemma holds.
lemma1 the predictive distribution in the sb approaches can be written as follows:

where v  = in + o n 1   and | ¡¤ | denotes the determinant of a matrix.
 proof  the predictive distribution is written as follows:

where ¡¤p denotes the expectation value over a distribution p. since  b a    b a   = o n 1  in the sb approaches  we can expand the predictive distribution as follows:

where v = ¡Ìn b a  b a  x is an n-dimensional vector of order o 1 . calculating the expectation value and expanding the logarithm of eq. 1   we arrive at lemma 1.  q.e.d.  comparing eq. 1  with the ml estimator
		 1 
 baldi and hornik  1   we find that the sb estimator of each component is asymptotically equivalent to a positivepart js type shrinkage estimator. moreover  by virtue of lemma 1  we can substitute the model at the sb estimator for the predictive distribution with asymptotically insignificant impact on generalization performance. therefore  we conclude that the sb approach is asymptotically equivalent to the shrinkage estimation. note that the variance of the prior distribution  eq. 1   asymptotically has no effect upon prediction and hence upon generalization performance  as far as it is a positive  finite constant. we call l the degree of shrinkage. remember that we can modify all the theorems in this paper for the ml estimation only by letting l = 1.
1 generalization error
using the singular value decomposition of the true map b a   we can transform arbitrary a  and b  without change of the map into a matrix with its orthogonal row vectors and another matrix with its orthogonal column vectors  respectively. accordingly  we assume the above orthogonalities without loss of generality. then  lemma 1 implies that the kl divergence  eq. 1   with a set of n training samples is given by

where

is the contribution of the h-th component. here tr ¡¤  denotes the trace of a matrix. we denote by wd m ¦² ¦«  the ddimensional wishart distribution with m degrees of freedom  scale matrix ¦²  and noncentrality matrix ¦«  and abbreviate as wd m ¦²  the central wishart distribution.
theorem1 the generalization error of an lnn in the sb approaches can be asymptotically expanded as
g n  = ¦Ën 1 + o n 1  
where the coefficient of the leading term  called the generalization coefficient in this paper  is given by

here ¦È ¡¤  is the indicator function of an event   is the h-th largest eigenvalue of a random matrix subject to  denotes the ex-
pectation value over the distribution of the eigenvalues.
 proof  according to theorem 1  the difference between the sb and the ml estimators of a true component with a positive singular value is of order o n 1 . furthermore  the generalization error of the ml estimator of the component is the same as that of the regular models because of its identifiability. hence  from eq. 1   we obtain the first term of eq. 1  as the contribution of the first h  components. on the other hand  we find from eq. 1  and theorem 1 that for a redundant component  identifying rq 1 with r affects the sb estimator only of order o n 1   which  hence  does not affect the generalization coefficient. we say that u is the general diagonalized matrix of an n ¡Á m matrix t if t is singular value decomposed as t =  bu a  where  a and  b are an m ¡Á m and an n ¡Á n orthogonal matrices  respectively.
let d be the general diagonalized matrix of   and the matrix created by removing the first
d. then  the first h  diagonal
elements of d correspond to the positive true singular value components and d consists only of noises. therefore  d is the general diagonalized matrix of  where r is an  n   h   ¡Á  m   h   random matrix whose elements are independently subject to n1 1   so that is subject to wn h  m   h  in h  . the redundant components imitate. hence  using theorem 1 and eq. 1   we obtain the second term of eq. 1  as the contribution of the last  h   h   components. thus  we complete the proof of
theorem 1.  q.e.d. 
1 large scale approximation
in a similar fashion to the analysis of the ml estimation in  fukumizu  1   the second term of eq. 1  can be analytically calculated in the large scale limit when m  n  h  and h  go to infinity in the same order. we define the following
 	 	.
 let w be a random matrix subject to  and  the eigenvalues of. the measure of
the empirical distribution of the eigenvalues is defined by

where ¦Ä u  denotes the dirac measure at u. in the large scale limit  the measure  eq. 1    converges almost everywhere to

where um =  ¡Ì¦Á   1 and um =  ¡Ì¦Á + 1  watcher 
1 . calculating moments of eq. 1   we obtain the following theorem:
theorem1 the generalization coefficient of an lnn in the large scale limit is given by

where
and	. here
j 1 ¡¤;k  denotes the inverse function of j s;k .
1 delicate situations
in ordinary asymptotic analysis  one considers only situations when the amplitude of each component of the true model is zero or distinctly-positive. also theorem 1 holds only in such situations. however  as mentioned in the last paragraph of section 1  it is important to consider delicate situations when the true map b a  has tiny but non-negligible singular values such that . theorem 1 still holds in such situations by replacing the second term of eq. 1  with o n 1 . we regard h  as the number of distinctly-positive true singular values such that. without loss of generality  we assume that b a  is a non-negative  general diagonal matrix with its diagonal elements arranged in non-increasing order. let r  be the true submatrix created by removing the

1 1 1 1 1 1
                   h* figure 1: generalization error.
first h  columns and rows from b a . then  d  defined in the proof of theorem 1  is the general diagonalized matrix of
  where r is a random matrix such that is subject to. therefore  we obtain the following theorem:
theorem1 the generalization coefficient of an lnn in the general situations when the true map b a  may have delicate singular values such that 1   ¡Ìn¦Ãh    ¡Þ is given by

where  and are the h-th largest singular value of
r  the corresponding right singular vector  and the corresponding left singular vector  respectively  of which denotes the expectation value over the distribution.
1 discussions
1 comparison with the ml estimation and with the bayes estimation
figure 1 shows the generalization coefficients of an lnn with m = 1 input  n = 1 output  and h = 1 hidden units. the horizontal axis indicates the true rank h . the vertical axis indicates the coefficients normalized by the parameter dimension k  given by eq. 1 . the lines correspond to the generalization coefficients of the sb approaches  clarified in this paper  that of the ml estimation  clarified in  fukumizu  1   that of the bayes estimation  clarified in  aoyagi and watanabe  1   and that of the regular models  respectively.1 the results in fig. 1 have been calculated in the large scale approximation  i.e.  by using theorem 1. we have also numerically calculated them by creating samples subject to the wishart distribution and then using theorem 1  and thus found that the both results almost coincide with each other so that we can hardly distinguish. we see in fig. 1 that the sb approaches provide as good performance as the bayes estimation  and that the mip  moreover  has no greater generalization coefficient than the bayes estimation for arbitrary h  

figure 1: with delicate true components.

figure 1: single-output lnn.
which might seem to be inconsistent with the proved superiority of the bayes estimation to any other learning method when we use the true prior distribution. this suspicion is cleared by consideration of delicate situations in the following.
¡¡using theorem 1  we can numerically calculate the sb  as well as the ml  generalization error in delicate situations when the true distribution is near the singularities. figure 1 shows the coefficients of an lnn with m = 1 input  n = 1 output  and h = 1 hidden units on the assumption that the true map consists of h  = 1 distinctly-positive component  three delicate components whose singular values are identical to each other  and the other one null component. the horizontal axis indicates ¡Ìn¦Ã   where ¦Ãh  = ¦Ã  for h = 1 ... 1. the bayes generalization error in delicate situations was previously clarified  watanabe and amari 
1   but unfortunately  only in single-output  so  lnns  i.e.  n = h = 1 figure 1 shows the coefficients of an solnn with m = 1 input units on the assumption that h  = 1 and the true singular value of the one component  indicated by the horizontal axis  is delicate. we see in fig. 1 that the sb approaches have a property similar to the bayes estimation  suppression of overfitting by the entropy of the singularities. we also see that in some delicate situations  the mip is worse than the bayes estimation  which shows consistency with the superiority of the bayes estimation. we conclude that in typical cases  the suppression by the singularities in the mip is comparable to  or sometimes stronger than  that in the bayes estimation.
¡¡it would be more fortunate if any of the sb approaches  which require much less computational costs than mcmc methods  would always provide comparable generalization performance to the bayes estimation. however  the sb approaches have also a property similar to the ml estimation  acceleration of overfitting by selection of the largest singular values of a random matrix. because of selection from a large number of random variables subject to non-compact support distribution  the  h   h   largest eigenvalues of a rangreater thandom matrix subject tol when wn h  m   h  in h   are muchh  .
therefore  the eigenvalues  in theorem 1 go out of the effective range of shrinkage  and consequently  the sb approaches approximate the ml estimation in such atypical cases. actually  the generalization coefficient of an lnn in the case that m =n =1 h =1 h  = 1 is 1¦Ë/k ¡« 1  which is greater than that of the regular models  though in the bayes estimation  the generalization coefficient never exceeds that of the regular models  watanabe  1 .
1 relation to shrinkage estimation
the relation between an eb approach in a linear model and the js estimator  into which the sb estimator  eq. 1   in an solnn is changed by letting l =  m   1   was discussed in  efron and morris  1 . based on the eb approach  the js estimator can be derived as the solution of an equation with respect to an unbiased estimator of the hyperparameter ¦Ó1  introduced in section 1. the similarity between the js and the sb estimators is natural because in an solnn  the transform makes not only the model linear but also the prior distribution as eq. 1 .
¡¡we focus on solnns in this paragraph. in fig. 1  the sb approaches and the bayes estimation seem to be superior to the ml estimation regardless of the true distribution. the following asymptotic expansion of the generalization coefficient with respect to provides a clue when it occurs:
where ¦Î  increases to be distinctly-positive. the sign of ¦Î indicates the direction of approach to the line 1¦Ë = m  which corresponds to the generalization coefficient of the regular models. it was found that ¦Î =  m   1  m   1  in the bayes estimation  which leads to the conjecture that the bayes estimation would be superior to the ml estimation when m ¡Ý 1
 watanabe and amari  1 . similarly expanding eq. 1   we get ¦Î = m m   1  in the mip  as well as ¦Î =  1m   1  in the mop  which leads to the conjecture that the mip when m ¡Ý 1  as well as the mop when m ¡Ý 1  would be superior to the ml estimation. we have found that the numerical calculation by using theorem 1 supports the conjecture above  nakajima and watanabe  1a . we also find that ¦Î =  m   1 in the js estimation  which is consistent with its proved superiority to the ml estimation when m ¡Ý 1.
1 relation to variational bayes approach
the generalization error of the variational bayes  vb  approach in lnns has just been clarified  nakajima and watanabe  1b . in the parameter subspace corresponding to the redundant components  the vb posterior distribution extends with its variance of order 1 in the larger dimension parameter subspace either input one or the output one; while the sb posterior distribution extends with its variance of order 1 in the parameter space  not in the hyperparameter space  as we find from eqs.  1  and  1  in appendix a. consequently  the vb approach is asymptotically equivalent to the mip version of sb approach.
1 future works
a future work can be consideration of the effect of nonlinearity of the activation function  ¦× ¡¤  in eq. 1 . we expect that the non-linearity would extend the range of basis selection and hence increase the generalization error.
1 conclusions
we have introduced a subspace bayes  sb  approach  an empirical bayes approach where a part of the parameters are regarded as hyperparameters  and derived the solution of two versions of sb approach in three-layer linear neural networks. we have also clarified its generalization error and concluded that the sb approaches have a property similar to the bayes estimation and provide as good performance as the bayes estimation in typical cases.
acknowledgments
the authors would like to thank the reviewers who gave us meaningful advice  which motivates us to add section 1 and some comments in other sections. they also would like to thank kazuo ushida  masahiro nei  and nobutaka magome of nikon corporation for encouragement to research.
a	proof of theorem 1
first  we will prove in the mip version. given an arbitrary map ba  we can have a with its orthogonal row vectors and b with its orthogonal column vectors by using the singular value decomposition. just in that case  the prior probability  eq. 1   is maximized. accordingly  we assume without loss of generality that the optimum value of b consists of its orthogonal column vectors. consequently  the marginal  conditional  likelihood  as well as the posterior distribution  factorizes as
.
then  substituting eqs. 1  and  1  into eq. 1   as well as eq. 1   we can easily derive the marginal likelihood  as well as the posterior distribution  of each component as follows:

where. let
const.  whereis the stochastic complexity  i.e.  the negative log marginal likelihood  of the h-th component. then  we get
const.
	= log|sh|   nbhrsh 1r bh	 1 
hereafter  separately considering the components imitating the positive true ones and the redundant components  we will minimize eq. 1 . we abbreviate.
¡¡for a positive true component  h ¡Ü h   the corresponding observed singular value ¦Ãh of rq 1 is of order 1 with probability 1. then  from eq. 1   we get

to minimize eq. 1   the leading  second term dominates the determination of the direction cosine of bh and leads to bh =
. the first and the third terms determine the norm of bh because the second term is independent of it. thus  we get the optimal hyperparameter value as follows:
	.	 1 
because the average of ah over the posterior distribution  eq. 1   is a h = sh 1rtbh  we obtain the sb estimator for the positive true component of the map ba as follows:
	.	 1 
¡¡on the other hand  for a redundant component  h   h   eq. 1  allows us to approximate eq. 1  as follows:

then  we find that the direction cosine of bh  determined by the second term of eq. 1   is approximated by ¦Øbh with accuracy o n 1 . after substituting for  we get the following extreme condition by partial differentiation of eq. 1  with respect to the norm of bh:
	 f b	m	m
we find that eq. 1  has no solution if ¦Ãh is less than. therefore  using the fact that eq. 1  diverges to infinity with arbitrary  we get the optimum hyperparameter value as follows:
	.	 1 
thus  we obtain the sb estimator of the redundant component as follows:
	.	 1 
selecting the largest singular value components minimizes eq. 1 . hence  combining eqs. 1  and  1   and then using eq. 1   we obtain the sb estimator in theorem 1. we can also derive the sb estimator in the mop version in exactly the same way.  q.e.d. 
