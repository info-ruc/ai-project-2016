
traditional probabilistic mixture models such as latent dirichlet allocation imply that data records  such as documents  are fully exchangeable. however  data are naturally collected along time  thus obey some order in time. in this paper  we present dynamic mixture models  dmms  for online pattern discovery in multiple time series. dmms do not have the noticeable drawback of the svd-based methods for data streams: negative values in hidden variables are often produced even with all nonnegative inputs. we apply dmm models to two real-world datasets  and achieve significantly better results with intuitive interpretation.
1 introduction
multiple co-evolving time series or data streams are ubiquitous in many different real-world applications. considering a sensor network  multiple sensors continuously collect different measurements over time  e.g.  chlorine concentration at different locations in a water distribution system; temperature or light measurements of different rooms; host status of different machines in a data center; etc. . a central site usually analyzes those measurements for main trends and anomaly detection. there has been success in mining multiple streams. however  the existing methods on analyzing such streams still have significant weaknesses:
¡¡first  existing monitoring softwares require considerable time and expertise to be properly configured despite the fact they treat the streams as independent. for each data stream that an administrator intends to monitor  he/she must make decisions upon proper thresholds for the data values. that is  he/she must define  for each data stream  what constitutes normal behaviors. the correlations across streams are usually not captured.
¡¡second  the state-of-the-art algorithms on automatically summarizing multiple streams often adopt matrix decompositions  like singular value decomposition  svd  and its variants. for example  papadimitriou et al. uses online svd tracking to summarize the multiple streams incrementally through a small number of hidden variables  papadimitriou et al.  1 . the hidden variables computed there are linear combinations of all streams projecting onto the maximum variance directions. this construction often seems not intuitive to the end users due to the reification of the mathematical properties of the svd techniques. in particular  some streams may have negative coefficients for hidden variables even when all the measurements are nonnegative. this drawback is intrinsic due to the gaussian distribution assumption on the input streams.
¡¡using mixture of hidden variables for data representation has recently been of considerable interest in text modeling. one early example is the latent semantic indexing  lsi  model  which is also based on the svd technique. more recently  hoffman presents the probabilistic latent semantic indexing  plsi  technique  hoffman  1 . this approach uses a latent variable model that represents documents as mixtures of topics in a probabilistic framework. latent dirichlet allocation  lda   blei et al.  1   which resembles the generative process of plsi but overcomes some of its drawbacks  has quickly become one of the most popular probabilistic text modeling techniques in machine learning and has inspired a series of research works in this direction  girolami and kaban  1; teh et al.  1 . in particular  lda has been shown to be effective in some text-related tasks such as document classification  blei et al.  1  and information retrieval  wei and croft  1   but its effectiveness on continuous data and time-series remains mostly unknown. most topic models mentioned above assume that data records  e.g.  documents  are fully exchangeable  which are not true in time-series model from many real-world applications. exactly because of that  topic modeling over time starts to receive more and more attentions. in the recent dynamic topic models  blei and lafferty  1   topic evolutions are modeled through collections sliced into certain periods of time. however  the time dependency of individual data records  documents  inside a collection/period are not considered. in the topic over time  tot  model  wang and mccallum  1   continuous time stamps are put into an lda-style topic model as observations. but drawing time stamps from one distribution as in the tot model is often not enough for dealing with bursty data  common in data streams.
¡¡in this paper  we present a dynamic mixture model  dmm   a latent variable model that takes into consideration the time stamps of data records in dynamic streams. the values in streams are represented as mixtures of hidden variables  and for each time stamp  the mixture of hidden variables for all streams is dependent on the mixture of the previous time stamp. in this way we can capture the short-term dependencies. compared to the svd techniques  it does not have their common drawback: non-intuitive negative values of hidden variables  and thus has a more interpretable explanation within a probabilistic framework. compared to the static lda-style models  it can take advantage of the time dependency of multiple streams. finally  we want to point out that topic models have been mainly designed for discrete data. to apply the lda-style mixture models to continuous data  the standard way is discretization  which we show in section 1 works well in dmm.
1 related work
data streams  containing much more interesting information than static data  have been extensively studied in recent years. evidence has shown that temporal information plays a crucial role in capturing many meaningful and interesting patterns. the main objective of temporal analysis is to efficiently discover and monitor patterns when data are streaming into a system. a recent survey  muthukrishnan 1  has discussed many data streams algorithms. among of these  svd and pca are popular tools for summarizing multiple time series into a number of hidden variables  papadimitriou et al.  1 . however  it assumes independency across time stamps; the results are often lack of probabilistic argument and intuitive interpretation.
¡¡probabilistic mixture models  hoffman  1; blei et al.  1  are widely used to summarize data based on statistical frameworks. to capture time evolution  the usage of time within probabilistic mixture models has been around for a while. for example  time can be taken care of in a posthoc way. one could first fit a time-unaware mixture model  and then order the data in time  slice them into discrete subsets  and examine the mixture distributions in each time-slice  griffiths and steyvers  1 . alternatively  non-joint modeling can also pre-divide the data into discrete time slices  fit a separate mixture model in each slice and possibly align the mixture components across slices  wang et al.  1 .
¡¡more systematic ways to take advantage of the temporal information in mixture models can be put into two categories. first  the dynamic behaviors are driven by state transitions. this kind of models generally make a markov assumption  i.e.  the state at time t + 1 or t + ¦¤t is independent of all other history given the state at time t. for instance  blei and lafferty present dynamic topic models  dtms  in which the alignment among topics across collections of documents is modeled by a kalman filter  blei and lafferty  1 . dtms target on topic evolution over sets of large amounts of data records and local dependencies between two records are not captured. second  the other type of models does not employ a markov assumption over time  but instead treats time as an observed continuous variable such as the topics over time  tot  models  wang and mccallum  1 . this helps capture long-range dependencies in time  and also avoids a markov model's risk of inappropriately dividing a mixture component  topic  in two when there is a brief gap in its appearance. however  with all time stamps drawn from one

figure 1: graphical model representation of dynamic topic model  left  1 slices  and dynamic mixture model  right 
symbol	description

	k	number of hidden components / topics
d	number of snapshots / documents v	number of parameters / words
nt	sum of parameter values in the tth snapshot / number of words in the tth document
t	snapshot index / time stamp of documents ¦È	mixture distribution of hidden components for a snapshot / document
¦Õ	mixture distribution over parameters / words for a hidden component /topic z	hidden component / topic
w	one unit value of a parameter / word token ¦Á	hyperparameter for the multinomial ¦È1 ¦Â	hyperparameter for ¦Õ

table 1: notation correspondence
distribution  tot is not appropriate for data streams  which are usually bursty and multi-modal.
1 dynamic mixture models
mixture models have been widely used in modeling various complex data with very competitive results. but the usage of latent mixture model in streaming data has not been explored  and largely remains unknown. we now present a dynamic mixture model  dmm  incorporating temporal information in data.
¡¡the graphical model representation of the dmm is shown in figure 1  right . as other mixture models for discrete data  the dmm can be regarded as a topic model as well. for presentation convenience  we explain this model in the background of both text and the streaming data  section 1 describes the streaming data in detail . table 1 summarizes the notation correspondences for both text and data streams. in particular  each snapshot corresponds to a document in topic models; each stream corresponds to a word occurrence over time  e.g.  the number of occurrences of word wi over time represents a single stream wi  which could be the chlorine concentrations over time  or the light intensities over time of ith sensor.
¡¡the generative process of the dmm is similar to latent dirichlet allocation  blei et al.  1   but the mixture distribution ¦È for each document does not have a dirichlet prior  except the very first one ; instead  ¦Èt is dependent on the mixture distribution of the previous snapshot ¦Èt 1.
¡¡first  we assume there are strong evolution dependencies. the continuous stream data are evenly sampled in time as successive snapshots that reflect the intrinsic dependencies between the values along time series.
¡¡second  the time dependency is complicated and the changes hardly follow a continuous time distribution as in the tot model  wang and mccallum  1 . thus neighboring dependency  markov assumption  is more appropriate.
¡¡compared to the tot model  the dynamic mixture model is constructed on discrete time stamps and assumes dependencies between two consecutive snapshots. although the tot model captures both short-term and long-term changes by treating time stamps as observed random variables  dmm is capable to capture more detailed changes and to model the dependency between any two consecutive time shots  which is especially appropriate for many streaming data when the data are equally sampled in time.
¡¡compared to dynamic topic models  dtm  see figure 1  left   dmm captures the evolution between snapshots  documents  instead of between snapshot-groups  collections in text model . in both dtm and lda  documents in a collection and words in a document are fully exchangeable; in dmm  snapshots of multiple time series  which correspond to documents in text model  have very strong temporal order and exchanges of snapshots can lead to very different results. from this perspective  dmm is a true online model. note that svd also treats time series as vectors  thus the permutation of snapshots will not make a difference.
¡¡we model the dependency by setting the expectation of the distribution of ¦Èt to be ¦Èt 1  i.e.  ¦Èt is generated from a distribution with ¦Èt 1 as the 1st order moment  we will discuss the concrete distribution in section 1 . the process of generating time series streams in dmm is as follows:
1. pick a multinomial distribution ¦Õz for each topic  hidden dimension  z from a dirichlet distribution with parameter ¦Â ;
1. for time shot t = 1  sample a multinomial distribution ¦Èt from a dirichlet distribution with parameter ¦Á 
1. for each time shot t   1  sample a multinomial distribution ¦Èt from a distribution with expectation ¦Èt 1 
1. sample a hidden variable / topic z ¡Ê {1 ¡¤¡¤¡¤ k} from a multinomial distribution with parameter ¦Èt 
1. add a unit value to parameter w picked from a multinomial distribution with parameter ¦Õzw / pick a word w from a multinomial distribution with parameter ¦Õzw.
thus  the likelihood of generating a data set of multiple data streams is:
k
p snapshot1 ¡¤¡¤¡¤ snapshot

1 dirichlet dynamic mixture models
as we have described  ¦Èt is generated from a distribution with ¦Èt 1 as the expectation. there are no strict requirements for this distribution  and we just want to build up connections between two successive snapshots  i.e.  the mixture distribution of the current snapshot is dependent on the one of its previous snapshot. in this setting  a continuous one-modal distribution is desired. gaussian distribution is a straightforward selection when ¦È is represented by natural parameters  which was adopted by  blei and lafferty  1  to model the dependency between two consecutive ¦Á  figure 1  left . however  gaussian distribution is not conjugate to the multinomial distribution that is used to generate hidden components / topics  which makes inference more difficult. although  to the best of our knowledge  there is no conjugate prior for dirichlet distribution; dirichlet distribution is conjugate to multinomial distribution; and we use a dirichlet distribution to model the dependency between consecutive ¦È. expectation only is not enough to make the proposed distribution identifiable  since there are infinitely many dirichlet distribution having the same expectation. thus we introduce another precision constraint that all the parameters of the dirichlet distributions sum to ¦× that is also equal to the sum of all parameters ¦Á in the first dirichlet distribution. a dirichlet distribution can be completely parameterized by the mean and precision parameters  minka  1 . for notation convenience  we define ¦È1 = ¦Á/¦×. that is  ¦Èt|¦Èt 1 ¡« dir ¦×¦Èt 1 .
inference
inference can not be done exactly in complex graphical models such as dmms. the non-conjugacy between dirichlet distributions makes standard gibbs sampling methods  geman and geman  1  harder in approximate inference of dmms. here  we use a simple  but effective iterated sampling procedure considering that streaming data are very different from traditional data sets such as large text collections: first  massive amounts of data arrive at high rates  which makes efficiency the most concern; second  users  or higherlevel applications  require immediate responses and cannot afford any post-processing  e.g.  in network intrusion detection   papadimitriou et al.  1 . in summary  the algorithm is expected to be efficient  incremental  scalable  and possibly scaling linearly with the number of streams. as shown in  griffiths and steyvers  1   we define mz w to be the number of tokens of word w assigned to topic z  and the posterior distribution of ¦Õz can be approximated by
.
to estimate the posterior distribution of ¦Èt in streaming data  we use a technique that is commonly used in mean-field variational approximation: assume each latent variable to be independent to the others. then we use an iterative procedure to update all the ¦È periodically. we define nt z to be the number of tokens in document t assigned to topic z. thus  we have 
.
¡¡in each sample  we draw in turn zt i according to a probability proportional to ¦È t zt i ¡Á ¦Õ zt i wt i  and update ¦È  after each iteration.
¡¡streams are very special data and require the corresponding algorithms to be highly efficient in order to react in a timely manner. it could be certainly possible to derive a variational approximation from the scratch  however  we have empirically found that our procedure is very effective and efficient for streaming data presented in section 1. with this iterated sampling algirithm  we keep the information from offline training  and run sampling only for the coming snapshot with a couple of iterations. the parameter estimation is updated after each new snapshot arrives.
1 experiments
in this section we present case studies on real and realistic datasets to demonstrate the effectiveness of our approach in discovering the underlying correlations among streams.
1 chlorine concentrations
description: the chlorine dataset was generated by epanet1 that accurately simulates the hydraulic and chemical phenomena within drinking water distribution systems. given a network as the input  epanet tracks the flow of water in each pipe  the pressure at each node  the height of water in each tank  and the concentration of a chemical species throughout the network  during a simulation period comprised of multiple time stamps. we monitor the chlorine concentration level at all the 1 junctions in the network shown in figure 1 for 1 time stamps during 1 days  one time tick every five minutes . the data was generated by using the input network with the demand patterns  pressures  flows specified at each node.
¡¡data characteristics: the two key features are: 1  a clear global periodic pattern  daily cycle  dominating residential demand pattern . chlorine concentrations reflect this  with few exceptions. 1  a slight time shift across different junctions  which is due to the time it takes for fresh water to flow down the pipes from the reservoirs.
¡¡thus  most streams exhibit the same sinusoidal-like pattern  except with gradual phase shifts as we go further away from the reservoir.
¡¡reconstruction: our method can successfully summarize the data using just two numbers  hidden variables  per time tick  see figure 1   as opposed to the original 1 numbers. figure 1 shows the reconstruction for one sensor  out of 1 . the reconstructions of the other sensors achieve similarly results. note that only two hidden variables give very good reconstruction.
¡¡hidden variables: the two hidden variables  see figure 1  reflect the two key dataset characteristics: 1  the first hidden variable captures the global  periodic pattern; 1  the second one also follows a very similar periodic pattern  but with a slight phase shift.
¡¡interpretation of hidden variables: each hidden variable follows a multinomial distribution with 1 parameters with

figure 1: chlorine reconstruction; reconstruction based on 1 hidden variables are very close to the original value.

figure 1: hidden variables for chlorine data; the first captures the daily cycles; the second reflects the time shift of different sensors.
the constraint that the probability mass over all parameters equals one. unlike the svd where a hidden variable is a linear combination of all streams  our method interprets the hidden variable as a generating process behind all 1 streams which is more intuitive  see figure 1 . notice that there is the high probability mass on stream 1 because those sensors  highlighted on figure 1  are close to the reservoir and on the main pipe of the water distribution network.
1 light measurements
description: this dataset consists of light intensity measurements collected using berkeley mote sensors  at several different locations in a lab  see figure 1  over a period of a month  deshpande et al.  1 . we simulate streams with this dataset by cutting the set into two even parts  and process the snapshots in the second part one by one in an online manner with the trained model from the first part. the results we present of this dataset are all from stream simulation running.
¡¡data characteristics: the main characteristics are: 1  a clear global periodic pattern  daily cycle . 1  occasional big spikes from some sensors  outliers . reconstruction: similar to the chlorine data  the reconstruction error on light data is also very small.
¡¡hidden variable: the first hidden variable exhibits the daily periodicity as shown in figure 1. the probability distri-

figure 1: the distribution of 1st hidden variable; note that sensor 1 to 1 have significantly higher mass because they are on the main pipe of water distribution.

figure 1: water distribution network: sensor 1 are highlighted since they are close to reservoir  red arrow  and on the main pipe.
bution  see figure 1  concentrates on the sensor 1  highlighted in figure 1  since they receive more sunshine and close to window.
¡¡comparison with static mixture modeling: to show the effectiveness of modeling mixtures dynamically  and modeling the dependency between mixtures of hidden variables  we compare our streaming modeling based on time series to the non-time dependency latent variable modeling techniques  e.g.  the lda model. the sums of reconstruction errors over time are compared and significant less  p   1e 1 under t-test  error rate were achieved with the dynamic mixture model from all of 1 to 1 iterations  as shown in figure 1.
1 conclusion and future work
in this paper  we have presented dynamic mixture models  dmms  for online pattern discovery in multiple time series  and shown interesting results on two data sets. the generative process of dmms is similar to traditional  static probabilistic

figure 1: sensor map  the highlighted region receives more sunshine.

figure 1: light reconstruction; reconstruction based on 1 hidden variables are very close to the original value.
mixture models such as latent dirichlet allocation  but in those models  the order of the data is ignored  thus all data records are assumed to be fully exchangeable. the dmms  on the contrary  take into consideration the temporal information  e.g.  order  implied in the data . also  compared to the state-of-the-art svd-based methods for data streams  dmms naturally give positive values of hidden variables  so the results from dmms are much more intuitive and interpretable. we believe that probabilistic mixture modeling is a promising direction for streaming data  especially dynamic mixture models have been shown to be a very effective model for multiple time series application.
¡¡for future work  we plan to apply dmms to larger datasets and improve both of its performance and efficiency on online streaming data. modeling the dependencies using other distributions other than dirichlet distribution will also be interesting.
acknowledgments
this work was supported in part by the center for intelligent information retrieval  in part by the national science foundation under grants no. iis-1  iis-1  int1  sensor-1  iis-1  in part by the

figure 1: 1st hidden variable captures daily periodicity

figure 1: the distribution of 1st hidden variable  the mass focus on the sensor 1 due to the vicinity to window.
pennsylvania infrastructure technology alliance  pita   a partnership of carnegie mellon  lehigh university and the commonwealth of pennsylvania's department of community and economic development  dced   and in part by the defense advanced research projects agency  darpa  under contract number hr1-c-1. any opinions  findings and conclusions or recommendations expressed in this material are those of the author s  and do not necessarily reflect those of the sponsors.
