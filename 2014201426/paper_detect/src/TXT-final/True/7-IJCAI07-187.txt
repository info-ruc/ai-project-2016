
feature interaction presents a challenge to feature selection for classification. a feature by itself may have little correlation with the target concept  but when it is combined with some other features  they can be strongly correlated with the target concept. unintentional removal of these features can result in poor classification performance. handling feature interaction can be computationally intractable. recognizing the presence of feature interaction  we propose to efficiently handle feature interaction to achieve efficient feature selection and present extensive experimental results of evaluation.
1 introduction
the high dimensionality of data poses a challenge to learning tasks such as classification. in the presence of many irrelevant features  classification algorithms tend to overfit training data  guyon and elisseeff  1; dash and liu  1 . many features can be removed without performance deterioration  gilad-bachrach et al.  1 . feature selection is one effective means to remove irrelevant features  blum and langley  1 . optimal feature selection requires an exponentially large search space  o 1n   where n is the number of features   almuallim and dietterich  1 . researchers often resort to various approximations to determine relevant features  e.g.  relevance is determined by correlation between individual features and the class   hall  1; yu and liu  1 . however  a single feature can be considered irrelevant based on its correlation with the class; but when combined with other features  it becomes very relevant. unintentional removal of these features can result in the loss of useful information and thus may cause poor classification performance. this is studied in  jakulin and bratko  1  as attribute interaction. for example  monk1 is a data set involving feature interaction. there are six features in monk1 and the target concept of monk1 is:  a1 = a1  or  a1 = 1 . here a1 and a1 are two interacting features. considered individually  the correlation between a1 and the class c  similarly for a1 and c  is zero  measured by mutual information. hence  a1 or a1 is irrelevant when each is individually evaluated. however  if we combine a1 with a1  they are strongly relevant in defining the target concept.
an intrinsic character of feature interaction is its irreducibility  jakulin and bratko  1   i.e.  a feature could lose its relevance due to the absence of its interacting feature s .
¡¡existing efficient feature selection algorithms usually assume feature independence dash and liu  1; hall  1 . because of the irreduciblenatureof feature interactions  these algorithms cannot select interacting features such as a1 and a1 in monk1. others attempt to explicitly address feature interactions by finding some low-order interactions  1- or 1way . in  jakulin and bratko  1   the authors suggest to use interaction gain as a practical heuristic for detecting attribute interaction. using interaction gain  their algorithms can detect if datasets have 1-way  one feature and the class  and 1-way  two features and the class  interactions. they further provide in  jakulin and bratko  1  a justification of the interaction information  and replace the notion of 'high' and 'low' in  jakulin and bratko  1  with statistical significance and illustrate the significant interactions in the form of interaction graph. below we apply four feature selection algorithms to synthetic data with known interaction and observe how they fare: fcbf  yu and liu  1   cfs  hall  1   relieff  kononenko  1   and focus  almuallim and dietterich  1   all available in weka  witten and frank  1 .
¡¡motivating examples: synthetic data with known feature interaction. four synthetic data sets are used to examine how various algorithms deal with known feature interactions in feature selection. the first data set is corral  john et al.  1   having six boolean features a1 a1 b1 b1 i r. the class y is defined by y =  a1 ¡Ä a1  ¡Å  b1 ¡Ä b1  and features a1 a1 b1 b1 are independent of each other. feature i is irrelevant to y and its values have a uniform random distribution; and feature r is correlated with y 1% of the time and is redundant. the other three training data sets are monks data. their target concepts are:  1  monk1:  a1 = a1  or  a1 = 1 ;  1  monk1: exactly two of
a1 = 1 a1 = 1 a1 = 1 a1 = 1 a1 = 1 a1 = 1; and
 1  monk1:  
 1% class noise added to the training data .
table 1: features selected by each algorithm on artificial data  and theindicates a missing relevant feature.

relevant featuresfcbfcfsreliefffocuscorrala1 a1 b1 b1a1 a1 b1 b1 ra1 	 	 	  ra1 a1 b1 b1 ra1 a1 b1 b1monk1a1 a1 a1a1   a1 a1 a1	 	  a1a1 a1 a1a1 a1 a1monk1a1 a1 a1 a1      a1      a1 a1 a1 a1 a1 a1 a1 a1 a1 a1 a1a1 a1a1 a1a1 a1a1 a1monk1a1 a1 a1a1   a1 a1a1 	  a1a1 a1 a1a1 a1 a1 a1¡¡results are presented in table 1. for corral  all four algorithms remove the irrelevant feature i  but only focus removes the redundant feature r. features a1 a1 and b1 b1 interact with each other to determine the class label of an instance. cfs  fcbf and relieff cannot remove r because r is strongly correlated  1%  with y . for all the three monks data sets  relieff can find true relevant features1 as seen in table 1. both fcbf and cfs perform similarly  and fcbf finds more features. focus can handle feature interaction when selecting features. however  as an exhaustive search algorithm  focus finds irrelevant feature a1 for monk1 due to 1% noise in the data. in a sense  it overfits the training data. focus is also impractical because finding moderately high-order interactions can be too expensive  as can be too large  when dimensionality n is large.
¡¡in this work  we design and implement an efficient approach to deal with feature interactions. feature interactions can be implicitly handled by a carefully designed featureevaluation metric and a search strategy with a specially designed data structure  which together take into account interactions among features when performing feature selection.
1 interaction and data consistency
one goal of feature selection is to remove all irrelevant features. first  we define feature relevance as in  john et al.  1 . let f be the full set of features  fi be a feature  si = f   {fi} and p denote the conditional probability of class c given a feature set.
definition 1  feature relevance  a feature fi is relevant iff
.
otherwise  feature fi is said to be irrelevant.
¡¡definition 1 suggests that a feature can be relevant only when its removal from a feature set will reduce the prediction power. from definition 1  it can be shown that a feature is relevant due to two reasons:  1  it is strongly correlated with the target concept; or  1  it forms a feature subset with other features and the subset is strongly correlated with the target concept. if a feature is relevant because of the second reason  there exists feature interaction. feature interaction is characterized by its irreducibility  jakulin and bratko  1 . a kth feature interaction can be formalized as:
definition 1  kth order feature interaction  f is a feature subset with k features f1  f1  ... fk. let c denote a metric that measures the relevance of the class label with a feature or a feature subset. features f1  f1  ...  fk are said to interact with each other iff: for an arbitrary partition f = {f1  f1  f1  ...  fl} of f  where l ¡Ý 1 and f  we have
 i ¡Ê  1 l   c f    c fi 
¡¡identifying either relevant features or a kth order feature interaction requires exponential time. definitions 1 and 1 cannot be directly applied to identify relevant or interacting features when the dimensionality of a data set is high. many efficient feature selection algorithms identify relevant features based on the evaluation of the correlation between the class and a feature  or a selected feature subset . representative measures used for evaluating relevance include: distance measures  kononenko  1; robnik-sikonja and kononenko  1   information measures  fleuret  1   and consistency measures  almuallim and dietterich  1   to name a few. using these measures  feature selection algorithms usually start with an empty set and successively add  good  features to the selected feature subset  the so-called sequential forward selection  sfs  framework. under this framework  features are deemed relevant mainly based on their individually high correlations with the class  and relevant interacting features of high order may be removed  hall  1; bell and wang  1   because the irreducible nature of feature interaction cannot be attained by sfs.
¡¡recall that finding high-order feature interaction using relevance  definitions 1 and 1  entails exhaustive search of all feature subsets. in order to avoid exponential time complexity  we derive a feature scoring metric based on the consistency hypothesis proposed in  almuallim and dietterich  1  to approximate the relevance measure as in definition 1. with this metric  we will design a fast filter algorithm that can deal with feature interaction in subset selection.
¡¡let d be a data set of m instances  d = {d1 d1 ...  dm}  and f be the feature space of d with n features  f = {f1  f1  ... fn}  we have the following:
definition 1  inconsistent instances  if two instances di and dj in d have the same values except for their class labels  di and dj are inconsistent instances or the two matching instances are inconsistent.
definition 1  inconsistent-instances set  d   d  d is an inconsistent-instances set iff   di dj ¡Ê d   either di and dj are inconsistent or they are duplicate. d is a maximal inconsistent-instances set  iff  d ¡Ê d and d /¡Ê d  d ¡È {d} is not an inconsistent-instances set.
definition 1  inconsistency count  let d be an inconsistent-instances set with k elements d1  d1 ... dk  and c1 c1 ... ct are the class labels of d  we partition d into t subsets s1 s1 ... st by the class labels  where si = {dj| dj has label ci}. the inconsistency count of d is:

definition 1  inconsistency rate  let d1  d1  ...  dp denote all maximal inconsistent-instances sets of d  the inconsistency rate  icr  of d is:

definition 1  consistency contribution  or c-contribution let ¦Ð denote the projection operator which retrieves a subset of columns from d according to the feature subset  the c-contribution  cc  of feature fi for f is defined as:
cc fi f  = icr ¦Ðf {fi} d     icr ¦Ðf  d  
¡¡it is easy to verify that the inconsistency rate is monotonic in terms of the number of features  i.e.   si  sj  si   sj   icr ¦Ðsi d   ¡Ý icr ¦Ðsj d  . hence  c-contribution of a feature is always a non-negative number with the zero meaning no contribution. c-contribution of a feature fi is a function of f   {fi}  where f is the set of features for d. c-contribution of a feature is an indicator about how significantly the elimination of that feature will affect consistency. c-contribution of an irrelevant feature is zero. c-contribution can be considered as an approximation of the metric in definition 1 by using inconsistency rate as an approximation of p  the conditional probability of class c given a feature set.
¡¡the monotonic property of inconsistency rate suggests that the backward elimination search strategy fits c-contribution best in feature selection. that is  one can start with the full feature set and successively eliminating features one at a time based on their c-contributions. backward elimination allows every feature to be evaluated with the features it may interact with. hence  backward elimination with c-contribution should find interacting features. however  backward elimination using inconsistency rate or c-contribution has two problems. the first problem is that it is very costly as it needs to calculate inconsistency rate for each potentially removable feature. as in the work of focus  almuallim and dietterich  1   focus relies on exhaustive search. it is impractical to do so when the dimensionality is reasonably large  which separates this work from focus. we will design a specific data structure next to achieve efficient calculation of c-contribution for our algorithm interact. the second problem is that c-contribution measure is sensitive to which feature is selected to compute first  the so-called the feature order problem. this is because features evaluated first for their consistency are more likely to be eliminated first.
¡¡solutions to the two problems will enable c-contribution to be used in building an efficient algorithm of backward elimination. we present our solutions and the algorithm next.
1 eliminating irrelevant features
we first present our solutions that form two pillar components for the algorithm interact and then discuss its details.
1 efficient update of c-contribution
c-contribution relies on the calculation of inconsistency rate. with the monotonicity of inconsistency rate  the following two properties are true after a feature fi is eliminated from a set {f1 .. fi ... fn} where i = 1 ... n:  i  all inconsistent instances sets are still inconsistent; and  ii  each maximal inconsistent instances set will be either of equal size or bigger. based on these two properties  we implement a hashing mechanism to efficiently calculate c-contribution: each instance is inserted into the hash table using its values of those features in slist as the hash key  where slist contains the ranked features that are not yet eliminated  slist initialized with the full set of features . instances with the same hash key will be insert into the same entry in the hash table. and the information about the labels is recorded. thus each entry in the hash table corresponds to a maximal inconsistency set of ¦Ðslist d . hence  the inconsistency rate of ¦Ðslist d  can be obtained by scanning the hash table. property  i  says that in order to generate an entry in the hash table for a new slist  after eliminating a feature   it is not necessary to scan the data again  but only the current hash table. property  ii  suggests that after each iteration of elimination  the numberof entries of the hash table for new slist should decrease. therefore  the hashing data structure allows for efficient update of c-contribution iterative feature elimination.
1 dealing with the feature order problem
we now consider the feature order problem in applying ccontribution. if we can keep removing the current  most irrelevant feature  we will likely retain the most relevant ones in the remaining subset of selected features. assuming that a set of features can be divided into subset s1 including relevant features  and subset s1 containingirrelevantones. by considering to remove features in s1 first  features in s1 are more likely to remain in the final set of selected features. therefore  we apply a heuristic to rank individual features using symmetrical uncertainty  su  in an descending order such that the  heuristically most relevantfeature is positioned at the beginning of the list. su is often used as a fast correlation measure to evaluate the relevance of individual features  hall  1; yu and liu  1 . this heuristic attempts to increase the chance for a strongly relevant feature to remain in the selected subset. let h x  and h x c  denote entropy and joint entropy respectively  and m x c  = h c +h x   h x c  the mutual information measuring the common information shared between the two variables. su between the class label c and a feature fi is:

¡¡this ranking heuristic cannot guarantee that the interacting features are ranked high. for monk1  for example  su a1 c  = su a1 c  = 1. either one can be evaluated first for its c-contribution. since cc a1 f    cc a1 f   a1 is eliminated. we will experimentally examine the ranking effect in section 1.
1 interact - an algorithm
the above solutions pave the way for c-contribution to be used in feature selection. we present an algorithm  interact  that searches for interacting features. it is a filter algorithm that employs backward elimination to remove those features with no or low c-contribution. the details are shown in figure 1: given a full set with n features and a class attribute c  it finds a feature subset sbest for the class concept. the algorithm consists of two major parts. in the first part  lines 1   the features are ranked in descending order based on their su values. in the second part  lines 1   features are evaluated one by one starting from the end of the ranked feature list. function getlastelement   returns the feature
input:
f: the full feature set with features f1 
¡¡f1  ...   fn c: the class label ¦Ä: a predefined threshold
output:
sbest: the best subset
1 slist = null
1 for i=1 to n do
1 calculate sufi c for fi
1 append fi to slist 1 end
1 order slist in descending values of sui c
1 f = getlastelement slist 
1 repeat
1 if f   null then
1 p = cc f slist  // c-contribution
1 if p ¡Ü ¦Ä then 1 remove f from slist
1 end
1 end
1 f = getnextelement slist f 
1 until f == null
1 sbest = slist
1 return sbest

figure 1: algorithm interact
in the end of the list  slist. if c-contribution of a feature is less than ¦Ä  the feature is removed  otherwise it is selected. function getnextelement slist f  returns the next unchecked feature just preceding f in the ranked feature list  line 1 . the algorithm repeats until all features in the list are checked. ¦Ä is a predefined threshold  1   ¦Ä   1 . features with their c-contribution   ¦Ä are considered immaterial and removed. a large ¦Ä is associated with a high probability of removing relevant features. relevance is defined by c-contribution: the higher value of cc f slist  indicates that f is more relevant. ¦Ä = 1 if not otherwise mentioned. the parameter can also be tuned using the standard cross validation. time complexity of interact: the first part of the algorithm has a linear time complexity of o nm   where n is the number of features and m is the number of instances of a given data set. for the second part of the algorithm  the calculation of a feature's c-contribution using a hash table takes also o nm . for n features  the time complexity of interact is o n1m . this is the worst case analysis. its average time complexity is less because  1  we only use the hash table of current slist in the calculation of ccontribution  and  1  the number of the entries in the hash table decreases after each iteration. if we assume it decreases to a percentage of the initial size  where 1   a   1  then in n iterations  the overall time complexity of interact is o nm 1   an+1 / 1   a    the proof is omitted . in other words  interact is expected to be comparable with heuristic algorithms such as fcbf  yu and liu  1 .
1 empirical study
we empirically evaluate the performance of interact in search of interacting features. for the four synthetic data sets with known feature interaction  table 1   interact finds table 1: summary of the benchmark data sets. f: number of features; i: number of instances and c: number of classes.

data setficdata setficlung-cancer11vehicle11zoo11kr-vs-kp11wine11soy-large11internet-ads11cmc11¡Á1c11
only the relevant features  ¦Ä = 1 . now we evaluate interact using benchmark data sets in comparison with some representative feature selection algorithms with the following aspects:  1  numberof selected features   1  predictive accuracy with feature selection  and  1  run time. we also examine how effective the two solutions  given in sections 1 and 1  are through a lesion study by removing one of them at a time from interact.
1 experiment setup
in our experiments  we choose four representative feature selection algorithms for comparison. they are fcbf  yu and liu  1   cfs  hall  1   relieff  kononenko  1   and focus  almuallim and dietterich  1 . all are available in the weka environment  witten and frank  1 . interact is implemented in the weka's framework. it will be made available upon request. all the experiments were conducted in the weka environment.
¡¡from 1 data sets   jakulin  1  identified 1 data sets having feature interactions without selecting interacting features. here we focus on our discussion on the 1 data sets1. the datasets are from the uci ml repository  blake and merz  1 . we also include another two datasets in the experiment: the 'internet-ads' data from the uci ml repository  and the '1¡Á1c' data from  alizadeh and et al.  1 . the information about the 1 data sets  without 1 monks data sets  is summarized in table 1. for each data set  we run all 1 feature selection algorithms and obtain selected feature subsets of each algorithm. for data sets containing features with continuous values  if needed  we apply the mdl discretization method  available in weka . we remove all index features if any. in order to evaluate whether the selected features are indeed good  we apply two effective classification algorithms c1 and a linear supportvector machine  svm 1  both available from weka  before and after feature selection and obtain predictionaccuracy by 1-fold cross-validation  cv . although c1 itself evaluates features  one at a time   its performanceis sensitive to the given sets of features  e.g.  its accuracy rates on monk1 are 1% and 1% for  a1 a1  and a1  and for all 1 features  respectively . because of focus's exponential time complexity  we only provide those results of focus obtained in 1 hours of dedicated use of the pc. interact  fcbf  cfs and re-

1
¡¡¡¡in which 1 data sets are monks data. we consider them synthetic data and discussed them earlier separately. 1
¡¡¡¡since naive bayes classifier  nbc  assumes conditional independence between features  irina rish  1   selecting interacting features or not has limited impact on it. our experimental results of nbc conform to the analysis and will be presented elsewhere due to the space limit.
table 1: number of selected features for each algorithm  in: interact  fc: fcbf  re: relieff  fo: focus  fs: full set . na denotes not available.
data setinfccfsrefofslung-cancer111zoo111wine111soy-large11na1cmc111vehicle111kr-vs-kp11na1internet-ads11na1¡Á1c11na1average111111lieff all complete their runs in seconds. this is consistent with our understanding and expectation of these algorithms.
1 results and discussion
number of selected features. table 1 presents the numbers of features selected by the five algorithms. all algorithmssignificantly reduced the number of features in many cases  e.g.  from 1 to as few as 1 . the average numbers of selected features for the 1 data sets are 1  interact   1  fcbf   1  cfs   1  relieff   and 1  full set . for four data sets  indicated by na in the table   focus did not finish after 1 hours. for the 1 synthetic data sets with known relevant features  table 1   interact selected only the relevant ones. next we examine the effect of this reduction on accuracy.
¡¡predictive accuracy before and after feature selection. for the 1 data sets with known interactions  we obtained predictive accuracy rates by 1-fold cross validation using c1 and svm. the results are shown in the two sub-tables of table 1. for both classifiers  the reduction of features by interact obtains results comparable with using all features: average accuracy 1%  interact  vs. 1%  full set  for c1  and 1%  interact  vs. 1%  full set  for svm. comparing interact with other feature selection algorithms  interact performs consistently better for c1 with better average accuracy. for svm  interact is comparable with other feature selection algorithms. one exception is the 'soy-large' data for the result of svm. we notice that the data set has 1 features  1 instances  and 1 classes  table 1 ; interact identifies 1 features  table 1  - the smallest number of selected features  fcbf also selected 1 features . we surmise that it may be too easy for the inconsistency measure to be satisfied with a small feature subset when each class has a small number of instances. in sum  for both classifiers  interact can help achieve better or similar accuracy  and hence  interact is effective in search of interacting features.
¡¡the effect of feature ranking. interact ranks features before backward elimination of features begins. as a part of the lesion study  we remove the ranking component from interact to form a version interact r. we summarize the results here due to the space limit: interact is always better than or equivalent to interact r; the average 1-fold cv accuracy for c1 and svm for  interact  interact r  are  1 1  and  1 1   respectively. noticing that interact ranks features using su and

figure 1: tinteract d/tinteract
table 1: run time  in second  for each algorithm.
data setinfccfsrefolung-cancer11111zoo11111wine11111soy-large1111nacmc11111vehicle11111kr-vs-kp1111nainternet-ads1111na1¡Á1c1111naaverage11111fcbf employs su  we observe in table 1 that fcbf does not perform as well as interact in selecting interacting features. the results suggest that the combination of su and c-contribution helps interact to achieve its design goal. clearly  using su is a heuristic. there could be other alternatives to rank the features. studying the effects of different ranking algorithms is one line of our on-going research.
¡¡the effect of the data structure. we devised a hashing data structure to speed up the time consuming calculation of c-contribution during feature selection. here we examine how effective the data structure is by comparing the run time of interact with that of interact d which does not employ the hashing data structure. since data size is a determining factor for run time  we first reorganize the 1 data sets according to their sizes  approximated by n   m - number of features multiplied by number of instances without considering feature types such as nominal or continuous . figure 1 shows the ratios using time of interact as the base:
tinteract d divided by tinteract. it shows that the run time difference between interact and interact d is more pronounced for larger data sets.
¡¡run time comparison. table 1 records the run time for each feature selection algorithm. except for focus  all algorithms finished their runs within the given time. the algorithms are ordered as relieff  fcbf  interact  cfs and focus: relieff is the fastest  and focus is the slowest if it finishes the run within 1 hours.
table 1: accuracy of c1 and svm on selected features: 'acc' denotes 1-fold cv accuracy %  and p-val obtained from a two-tailed t-test. the symbols  +  and  -  respectively identify statistically significant  at 1 level  if interact wins over or loses to the compared algorithm. na denotes the result is not available.
data setinteractfcbfcfsreliefffocuswholedsaccaccp-valaccp-valaccp-valacc	p-valacc	p-valc1
lung-cancer11.1+11+1111+11+zoo11111111111wine111+11+11+11+11+soy-large1111111nana11cmc11111111111vehicle111+11+11+1111kr-vs-kp111+11+11+nana11internet-ads11111+11+nana111¡Á1c1111111nana11average111111win/loss11111svm
lung-cancer111111111+11+zoo11111111111wine1111111+11+11soy-large111-11-11-nana11-cmc111+1111+1111vehicle111+11+11+1111kr-vs-kp111+11+11+nana11internet-ads11111+11+nana11-1¡Á1c1111111nana11average111111win/loss111111 conclusions
we recognize the need to study feature interaction in subset selection using some synthetic data sets  and propose to search for interacting features employing c-contribution to measure feature relevance. we investigated the key issues hindering the use of consistency measures and developed interact that handles feature interaction and efficiently selects relevant features. it ranks features to overcome the feature order problem. interact implements a special hashing data structure to avoid repeated scanning of a data set by taking advantage of the intrinsic properties of the ccontribution measure  which results in a significant speed-up of the algorithm. the efficiency and effectiveness of interact are demonstrated through theoretical analysis as well as extensive experiments comparing with representative feature selection algorithms using the synthetic and real-world data sets with known feature interactions. experimental results show that interact can effectively reduce the number of features  and maintain or improvepredictiveaccuracy in dealing with interacting features. this work presents promising initial efforts on searching efficiently for interacting features.
