
occam's razor is the principle that  given two hypotheses consistent with the observed data  the simpler one should be preferred. many machine learning algorithms follow this principle and search for a small hypothesis within the version space. the principle has been the subject of a heated debate with theoretical and empirical arguments both for and against it. earlier empirical studies lacked sufficient coverage to resolve the debate. in this work we provide convincing empirical evidence for occam's razor in the context of decision tree induction. by applying a variety of sophisticated sampling techniques  our methodologysamples the version space for many real-world domains and tests the correlation between the size of a tree and its accuracy. we show that indeed a smaller tree is likely to be more accurate  and that this correlation is statistically significant across most domains.
1 introduction
occam's razor  attributed to the 1th-century english logician william of ockham  is the principle that  given two hypotheses consistent with the observed data  the simpler one should be preferred. this principle has become the basis for many induction algorithms that search for a small hypothesis within the version space  mitchell  1 . several studies attempted to justify occam's razor with theoretical and empirical arguments blumer et al.  1;quinlan and rivest  1; fayyad and irani  1 . but a number of recent works have questioned the utility of occam's razor  and provided theoretical and experimental evidence against it.
﹛schaffer  proved that no learning bias can outperform another bias over the space of all possible learning tasks. this looks like theoretical evidence against occam's razor. rao et al.   however  argued against the applicability of this result to real-world problems by questioning the validity of its basic assumption about the uniform distribution of possible learning tasks.
﹛domingos  argued that the disagreement about the utility of occam's razor stems from the two different interpretations given to it: the first is that simplicity is a goal in and of itself  and the second is that simplicity leads to better accuracy. while accepting the first interpretation  domingos questioned the second one.
﹛webb  presented c1x  an extension to c1 that uses similarity considerations to further specialize consistent leaves. webb reported an empirical evaluation which shows that c1x has a slight advantage in a few domains and argued that these results discredit occam's thesis.
﹛murphy and pazzani  reported a set of experiments in which all possible consistent trees were produced and their accuracy was tested. their findings were inconclusive. they found cases where larger trees had  on average  better accuracy. still  they recommend using occam's principle when no additional information about the concept is available. the major limitation of their work is the exhaustive enumeration of the version space. such an approach is only applicable to domains with very few features.
﹛in this work we present an alternative approach that performs statistical testing of occam's thesis on a sample of the version space. this approach allows us to use highdimensional domains and complex concepts. one problem with random sampling of the version space is the rarity of small trees in the sample. we therefore use  in addition to random sampling  biased sampling methods based on modern anytime induction algorithms  esmeir and markovitch  1 . these methods produce samples with much higher concentrations of small trees.
﹛the major contribution of this work is to provide convincing empirical evidence for occam's razor in the context of classification trees. furthermore  the various sampling techniques we applied help to better understand the space of consistent decision trees and how top-down induction methods explore it. note that this empirical demonstration of the utility of occam's principle does not pretend to provide a philosophical proof for occam's thesis.
1 occam's empirical principle
in the context of machine learning  the widely accepted interpretation of occam's razor is that given two consistent hypotheses  the simpler one is likely to have a lower error rate. fayyad and irani  have formally defined this notion: for two decision trees t1 and t1 and a fixed    is likely to have a lower error rate than t1 if pr  where is the probability that t has an error rate greater than .
procedure tdidt e a 
if e =  
return leaf nil 
if  c such that  e ﹋ e class e  = c
﹛return leaf c  a ↘ choose-attribute a e 
v ↘ domain a 
foreach vi ﹋ v *
ei ↘ {e ﹋ e | a e  = vi}
si ↘ tdidt ei a   {a} 
return node a 
* when a is numeric  a cutting point is chosen and and a is not filtered out when calling sid1.figure 1: top-down induction of decision trees. e stands for the training set and a stands for the set of attributes.
﹛fayyad and irani  also provided theoretical support for favoring smaller trees. they showed that under a set of assumptions  given two trees t1 and t1 consistent with the observed data  t1 is likely to have a lower error rate than t1 if t1 has fewer leaves. berkman and sandholm   however  have questioned this set of assumptions and argued that the opposite conclusion can be drawn from it.
﹛the main challenge we face in this work is to empirically test the validity of occam's razor in decision tree induction. we therefore define the occam's empirical principle:
definition 1 let etrain and etest be a training and a testing set respectively. let h be a set of hypotheses consistent with etrain. we say that h satisfies occam's empirical principle with respect to etrain and etest if  for any h1 h1 drawn from
h ℅ h 
 
where |h| is the size of hypothesis h and 1 ≒ acc h e  ≒ 1 is the accuracy of h on a test set e.
1 sampling the version space
given a learning problem  we would like to produce all the possible trees consistent with the observations and test whether their size and accuracy are correlated. such an exhaustive enumeration  however  is not practical for most realworld domains. therefore  in what follows we propose sampling as an alternative. first we define the population  i.e.  the space of decision trees we sample from  and then we describe 1 different sampling techniques  each of which focuses on a different part of the sampled space.
1 defining the version space
given a set of attributes a  the hypothesis class we deal with is the set of decision trees over a  denoted by dta. let e be a set of examples. because occam's razor is applicable only to hypotheses that can explain the observations  we limit our discussion to the version space-the set of all trees consistent with e  denoted by dta e . furthermore  since most decision tree learners build a tree top-down  we focus on a subset of the consistent trees-the trees obtainable by top-down induction  denoted by tdidta e . under the tdidt scheme  the set of examples is partitioned into subsets by testing the value of an attribute and then each subset is used to recursively build a subtree. the recursion stops when all the examples have the same class label. figure 1 formalizes the basic procedure for top-down induction.
﹛while tdidta e  is a strict subset of dta e   we claim that the trees in dta e  that are not in tdidta e  are not interesting for the purpose of model learning. there are two types of trees in dta e    tdidta e :
1. a tree containing a subtree with all leaves marked withthe same class. obviously  such a subtree could have been replaced by a single node marked with the class.
1. a tree with an internal node that has no associated examples from e. the subtree rooted at this node is not supported by training examples and is therefore not interesting for induction.
﹛note that including the above trees could unjustly distort the results. in the first case  larger trees that are logically equivalent will be included  arbitrarily weakening the negative correlation. in the second case  the extra branches are not supported by any training example. thus  their leaves must be labeled randomly  lowering the accuracy and hence arbitrarily strengthening the negative correlation.
﹛while we restrict the definition of occam's empirical principle to consistent hypotheses  in our experiments we also examine its applicability to pruned tdidta e  trees. this allows us to draw conclusions for noisy datasets as well.
1 sampling techniques
our goal is to sample the tdidta e  space in order to test occam's empirical principle. our first proposed sampling technique uses tdidt with a random selection of the splitting attribute  and cutting point  where the attribute is numeric . we refer to this method as the random tree generator  rtg . observe that although it has no bias with respect to generalization quality rtg does not uniformly sample tdidta e . for example  if the concept was and the attributes were {a1 a1 a1}  the probability of constructing the smallest tree  with a single split  is much higher than that of constructing a specific large tree. we will later show that the non-uniform sampling should not affect the validity of our conclusions.
﹛one problem with random sampling is the rarity of small trees. many induction methods  however  are likely to concentrate on small trees. theoretically  the correlation could have been statistically significant when sampling the tdidt space but not when sampling a subspace consisting of small trees. to test this hypothesis we need a sample of small trees. such a sample could be obtained by repeatedly invoking rtg and keeping the smaller trees. nevertheless  the number of rtg invocations needed to obatin a reasonable number of small trees is prohibitively high. another alternative is to use id1. repeated invocations of id1  however  result in similar trees that can vary only due to different tie-breaking decisions. esmeir and markovitch  introduced sid1  a stochastic version of id1 that is designed
procedure sid1-choose-attribute e a 
foreach a ﹋ a p a  ↘ gain-1 e a 
if  a such that entropy-1 e a  = 1 a  ↘ choose attribute at random from
{a ﹋ a | entropy-1 e a  = 1}
else a  ↘ choose attribute at random from a; for each attribute a  the probability
       of selecting it is proportional to p a  return a figure 1: attribute selection in sid1
procedure lsid1-choose-attribute e a r 
if r = 1
return id1-choose-attribute e  a 
foreach a ﹋ a
foreach vi ﹋ domain a 
ei ↘ {e ﹋ e | a e  = vi}
mini ↘ ﹢
repeat r times
=1
return a for which totala is minimalfigure 1: attribute selection in lsid1
to sample the version space semi-randomly  with a bias to smaller trees. in sid1  instead of choosing an attribute that maximizes the information gain  we choose the splitting attribute semi-randomly. the likelihood that an attribute will be chosen is proportional to its information gain.1 however  if there are attributes that decrease the entropy to zero  then one of them is picked randomly. the attribute selection procedure of sid1 is listed in figure 1.
for many hard learning tasks such as parity concepts 
id1's greedy heuristic fails to correctly estimate the usefulness of the attributes and can mislead the learner to produce relatively large trees. in such cases  sid1 can  in theory  produce significantly smaller trees. the probability for this  however  is low and decreases as the number of attributes increases. to overcome this problem  we use a third sampling technique that is based on the recently introduced lsid1 algorithm for anytime induction of decision trees  esmeir and markovitch  1 . lsid1 adopts the general tdidt scheme  and invests more time resources for making better split decisions. for every candidate split  lsid1 attempts to estimate the size of the resulting subtree were the split to take place  and favors the one with the smallest expected size. the estimation is based on a biased sample of the space of trees rooted at the evaluated attribute. the sample is obtained using sid1. lsid1 is parameterized by r  the sample size. when r is greater  the sample is larger and the resulting estimate is expected to be more accurate. therefore  lsid1 is expected to improve with the increase in r. figure 1 lists the procedure for attribute selection as applied by lsid1. because our goal is to sample small trees and not to always obtain the smallest tree  we use lsid1 r = 1  as a sampler. observe that lsid1 is stochastic by nature  and therefore we do not need to randomize its decisions.
1 experimental results
we tested occam's empirical principle  as stated in definition 1  on 1 datasets  1 of which were chosen arbitrarily from the uci repository  blake and merz  1   and 1 which are artificial datasets that represent hard concepts: xor-1 with 1 additional irrelevant attributes  and 1-bit multiplexer. each dataset was partitioned into 1 subsets that were used to create 1 learning problems. each problemconsisted of one subset serving as a testing set and the union of the remaining 1 as a training set  as in 1-fold cross validation. we sampled the version space  tdidta e   for each training set e using the three methods described in section 1 and tested the correlation between the size of a tree  number of leaves  and its accuracy on the associated testing set. the size of the sample was ten thousand for rtg and sid1  and one thousand for lsid1  due to its higher costs . we first present and discuss the results for consistent trees and then we address the problem of pruned  inconsistent trees.
1 consistent decision trees
figure 1 plots size-frequency curves for the trees obtained by each sampling method for three datasets: nursery  glass  and multiplexer-1  for one fold out of the 1 . each of the three methods focuses on a different subspace of tdidta e   with the biased sampling methods producingsamples consisting of smaller trees. in all cases we see a bell-shaped curve  indicating that the distribution is close to normal. recall that rtg does not uniformly sample tdidta e : a specific small tree has a better chance of being built than a specific large tree. the histograms indicate  however  that the frequency of small trees in the sample is similar to that of large trees  symmetry . this can be explained by the fact that there are more large trees than small trees. to further verify this  we compared the distribution of the tree size in an rtg sample to that of all trees  as reported in  murphy and pazzani  1   mux-1 dataset . the size-frequency curves for the full space and the sampled space were found to be similar.
﹛occam's empirical principle states that there is a negative correlation between the size of a tree and its accuracy. to test the significance of the correlation  we used the nonparametric spearman correlation test on each of the samples.1spearman's coefficient 老 measures the monotonic association of two variables  without making any assumptions about their frequency distribution. for a paired sample of x and y   老 is defined as   where di is the difference in the statistical rank of xi and yi. there is a special correction for this formula in the presence of ties.

figure 1: frequency curves for the nursery  left   glass  middle   and multiplexer-1  left  datasets
rtgsid1lsid1datasetacc.size老﹟acc.size老﹟acc.size老﹟breast-w1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11bupa1㊣11㊣11.1㊣11㊣11.1㊣11㊣11car1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣11cleveland1㊣11㊣11.1㊣11㊣11.1㊣11㊣1-11corral1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1.1naglass1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11hungerian1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11iris1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11monks-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1nanamonks-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1.1monks-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1.1namux-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11nursery1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11scale1㊣11㊣1.11㊣11㊣1.11㊣11㊣1.1splice1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11tic-tac1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11voting1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11wine1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11xor-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11zoo1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-1natable 1: testing occam's empirical principle using different sampling methods that produce consistent trees. for each method we report the accuracy  tree size  and spearman's correlation coefficient  老  averaged over all 1 partitions. we also report the number of times  out of 1  that a negative correlation was found to be statistically significant with p = 1.﹛table 1 lists summarizing statistics for the rtg  sid1  andlsid1 samplers. the validity of occam's empirical principle  tested by spearman's method  is listed in the rightmost column. for each sampling method  for each dataset  we count how many times  out of the 1 folds  the null hypothesis h1  which states that the variables are not correlated  can be rejected at an 汐 = 1% significance level  against the alternative hypothesis that the correlation is negative.
﹛the results indicate that when random sampling  rtg  in used  occam's empirical principle  as measured by spearman's test  is valid for almost all problems  except for scale. the results for the sid1 sampling method indicate that even when focusing on smaller trees  simplicity is beneficial as a bias for accuracy. again  except for the scale dataset  there is a strong inverse correlation between size and accuracy. the numbers indicate that the correlation is weaker than the rtg case  yet still significant across most domains.
﹛the lsid1 samples focus on very small trees. in several cases  lsid1 could reach the smallest tree possible. again the negative correlation was found to be significant for most domains.1 however  the number of cases where the null hypothesis could not be rejected is higher than in the previous samplers. one possible reason for this phenomenon is that lsid1 samples trees from a much tighter size-range. hence  there is an increased probability for finding two trees  t1 and t1  with the size and accuracy of t1 being greater than t1.
﹛as indicated by the frequency curves  the different sampling methods coverdifferent portions of the space  but sometimes overlap. an interesting question is whether the conclusions hold if we analyze all samples together. note that it is not statistically valid to merge the samples ofrtg sid1  and lsid1 due to their different distributions. nevertheless  we measured the correlation statistics for the combined samples and found that the results are very similar  with a strong negative correlation between the size of a tree and its accuracy.
﹛to illustrate the correlation between the size of a tree and its accuracy  we grouped the trees from a single run into bins  according to their size  and calculated the average accuracy for each bin. we did this for each of the sampling methods. bins with less than 1 observations were discarded. figure 1 plots the results for the nursery  glass  and multiplexer-1 datasets. the error bars represent confidence intervals  with 汐 = 1%.

figure 1: correlation between size and accuracy using rtg  left-most   sid1  middle   and lsid1  right-most . the upper graphs represent the results for the nursery dataset  the graphs in the middle row stand for the glass dataset and the lowergraphs stand for the multiplexer-1 dataset.
﹛again  the graphs show a strong correlation between size and accuracy  confirming occam's empirical principle. for the nursery and multiplexer-1 datasets the correlation is strong for all 1 samplers. for glass  the correlation is weaker when the trees are very small. these graphs  which represent each size range in its own bin  indicate that the positive support we showed for occam's principle is not a result of a size bias in our sampling methods.
1 pruned decision trees
formally  occam's empirical principle  as defined in section 1  is applicable only to consistent hypotheses. most decision tree learners  however  do not necessarily produce consistent models because they output a pruned tree in attempt to avoid overfitting the data. this is usually done in two phases: first a tree is grown top-down and then it is pruned. in our second set of experiments  we examine whether taking simplicity as a bias in the first stage is beneficial even when the tree is later pruned. therefore  we measure the correlation between the size of unpruned trees and their accuracy after pruning.
﹛table 1 summarizes the results. the same statistics were measured with a single change: the accuracy was measured after applying error-based pruning  quinlan  1 . as in the case of consistent trees  examining the overall correlation between the size of a tree and its accuracy indicates that for most datasets the inverse correlation is statistically significant.
﹛table 1 also gives the percentages of pruned leaves. while the trees produced by rtg were aggressively pruned  the percentage of pruned leaves in lsid1 was relatively low. this is due to the stronger support for the decisions made at the leaves of smaller consistent trees.
1 conclusions
occam's razor states that given two consistenthypotheses  the simpler one should be preferred. this principle has been the subject for a heated debate  with theoretical and empirical arguments both for and against it. in this work we provided convincing empirical evidence for the validity of occam's principlewith respect to decision trees. we state occam's empirical principle  which is well-defined for any learning problem consisting of a training set and a testing set  and show experimentally that the principle is valid for many known learning problems. note that our study is purely empirical and does not attempt to reach an indisputable conclusion about occam's razor as an epistemological concept.
﹛our testing methodology uses various sampling techniques to sample the version space and applies spearman's correlation test to measure the monotonic association between the size of a tree and its accuracy. our experiments confirm occam's empirical principle and show that the negative correlation between size and accuracyis strong. although there were several exceptions  we conclude that in general  simpler trees are likely to be more accurate. observe that our results do not contradict those reported by murphy and pazzani   but complement them: we do not claim that a smaller tree is always more accurate  but show that for many domains smaller trees are likely to be more accurate.
﹛we view our results as strong empirical evidence for the utility of occam's razor to decision tree induction. it is important to note that the datasets used in our study do not necessarily represent all possible learning problems. however  these datasets are frequently used in machine learning research and considered typical tasks for induction algorithms.
rtgsid1lsid1datasetacc.size%p老﹟acc.size%p老﹟acc.size%p老﹟breast-w1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11bupa1㊣11㊣111㊣11㊣111㊣11㊣11car1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11cleveland1㊣11㊣111㊣11㊣111㊣11㊣1-11corral1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣11glass1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11hungerian1㊣11㊣111㊣11㊣111㊣11㊣1-11iris1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11monks-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-1namonks-1.1㊣11㊣111㊣11㊣111㊣11㊣11monks-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-1namux-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11nursery1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11scale1㊣11㊣111㊣11㊣111㊣11㊣111splice1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11tic-tac1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11voting1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣11wine1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11xor-1.1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣1-11zoo1㊣11㊣1-11.1㊣11㊣1-11.1㊣11㊣11natable 1: testing occam's empirical principle when the trees are pruned. for each method we report the accuracy  tree size  percentage of pruned leaves  and spearman's correlation coefficient  老  averaged over all 1 partitions. we also report the number of times  out of 1  that a negative correlation between the size of the unpruned trees and their accuracy after pruningwas found to be statistically significant with p = 1.
﹛we also examined the applicability of occam's razor to pruned trees. for most domains we found the correlation between the size of a tree before pruning and its accuracy after pruning to be statistically significant. these results indicate that taking occam's razor as a preference bias when growing a tree is useful even if the tree is post-pruned.
﹛an interesting research direction that we plan to pursue is to test the correlation between a tree's accuracy and a variety of other measures  such as expected error and description length.
acknowledgments
this work was partially supported by funding from the ecsponsored muscle network of excellence  fp1 .
