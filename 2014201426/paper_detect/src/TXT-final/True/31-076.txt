 
we present a optimization formulation for discrete binary csp  based on the construction of a continuous function a p  whose global maximum represents the best possible solution for that problem. by the best possible solution we mean either  i  a solution of the problem  if it is solvable  or  ii  a partial solution violating a minimal number of constraints  if the problem is unsolvable. this approach is based on relaxation labeling techniques used to enforce consistency in image interpretation. we have used a projected gradient ascent algorithm to maximize a p  on the n-queens problem obtaining good results but with a high computational cost. to elude this problem  we have developed a heuristic for variable and value selection inspired in the direction in which a p  is maximized. we have tested this heuristic with forward checking on several classes of csp. 
1 introduction 
the purpose of this paper is to show how discrete constraint satisfaction problems  usually abbreviated as csp  can be effectively analyzed and solved as a kind of optimization problems. this is not a strictly new approach; other authors have used it to solve specific problems  sosic and gu  
1; minton et al.  1; selman et a!.  1; morris  1 . the novel aspect we present here consists in the following: we provide a way to construct  for any instance of csp with binary constraints  a continuous function csp. lp and csp have many points in common: both deal with a finite number of variables {xi}y which take values on discrete domains {di} under a set of binary constraints {rij}. they differ in two main aspects:  i  in the problem formulation and  ii  in what they consider as a solution. with respect to the problem formulation  in csp assignments and constraints are purely boolean  the assignment  xit vj  is true or false  the constraint rij{v*  v/  allows variables xi; and xj to take the values vk and v/ or not   while in lp assignments can be weighted  assign several values with different positive weights to the same variable is a legal assignment  providing the sum of weights is equal to 1   and constraints can express a graded level of consistency  for a given pair of variables different pairs of values can be consistent  but some more consistent -and therefore preferred-than others . with respect to a solution  both approaches look for a consistent assignment but they differ in the level of consistency required. in csp  a solution must be globally consistent  that is  it must satisfy all the constraints  or a maximal number of constraints in the case of maximal constraint satisfaction . in lp different criteria for a consistent solution have been proposed; in this paper we will follow the proposal of  hummel and zucker  1  which  for a lp with symmetric constraints  identifies consistent solutions with local maxima of a p   the average local consistency function. local maxima of a p  are not guaranteed either to satisfy every constraint or to restrict the assignment weights to {1}  so they are not feasible solutions for csp. after this description  we can see a csp as a particular instance of lp  but with more demanding requirements for a solution. 
whose global maximum represents the best possible solution the best solution of a csp is to be a global maximum of we will show that the necessary and sufficient condition for 

for that problem. by the best possible solution we mean cither  i  a solution of the problem  if it is solvable  or  ii  a partial solution violating a minimal number of constraints  if the problem is unsolvable. therefore  a csp can be solved constructing such a function and using any kind of optimization techniques to compute its global maximum. this function  a p   is the average local consistency function associated with the problem. 
   the present work is the result of applying relaxation labeling techniques  used to enforce consistency in image analysis  to csp. relaxation labeling considers labeling problems  lp   which can be seen as a generalization of a p   and in this case it is always possible to restrict the assignment weights to {1}. 
   this paper is organized as follows. in section 1  we revise some of the previous work on csp and relaxation labeling. in section 1  we introduce the concepts needed to analyze a csp as a lp. in section 1  we provide the results relating solutions with local and global maxima of a p . in section 1  we discuss some approaches to compute a global maximum. in section 1  we discuss a heuristic approach for variable and value selection. finally  in section 1 we summarize the main contributions of this work. 
	meseguer and larrosa 	1 
1 related work 
a significant amount of work has been made on csp in the last twenty years  for an wide overview see  tsang  1  . the most common approach to solve csp has been a systematic search algorithm with backtracking. a consistent partial solution is formed by a subset of variables and it is extended by adding variables one by one until a complete solution is found. when no consistent value exists for the variable being added  backtracking occurs changing the value of a previously assigned variable. this approach is complete  it always finds a solution if it exists  but is has an important drawback: backtracking is extremely inefficient. to prevent this problem  several refinements and additions to this approach has been developed  such as local consistency pre-processing  backtrack-free problems  lookahead and look-back algorithms  heuristics  and combinations of these strategies. 
   in the last years  a new iterative approach to solve csp has been proposed. starting from an inconsistent global assignment  each iteration modifies this assignment using local information in such a way that the number of violated constraints decreases  or in some cases  remains unchanged   until global consistency is achieved  sosic and gu  1; minton et al.   1; selman et al.  1 . this approach can be seen as a hill-climbing procedure  where the number of violated constraints is minimized. given that it can be stuck in local minima  where some constraints are still violated   a way to escape from them is needed  morris  1 . this approach is not complete. when a given limit of iterations is achieved without reaching a solution  the process is restarted from another initial assignment. in practice and for some kind of problems  a few restarts are enough to reach a solution  with less computational effort than systematic search with backtracking. 
   on the other hand  image interpretation considers the problem of assigning labels to image parts to produce a global consistent interpretation. given that the presence of a particular object may impose constraints on other objects in its neighbourhood  a common approach uses local contextual information to obtain the most adequate label for each image part. this process is iterated to allow local information to propagate  until a stable state is reached  davis and rosenfeld  1 . an early example of this technique is the waltz's work on interpretation of line segments  waltz  1 . this work allowed unambiguous interpretations only. in general this is too restrictive  so ambiguous interpretations where several labels are assigned to the same image part with different weights are also allowed. in this context a number of techniques  called relaxation labeling  have been developed  kittler and illingworth  1; torras  1 . a relaxation procedure is an iterative and parallel process which  starting from an initial weighted assignment  weights in  1    performs a synchronous weight updating until it does not cause further changes in the current weighted assignment  it converges to a fixed point . many updating formulas have been proposed  rosenfeld et al.  1   some of which have been proved as approximations of a gradient ascent algorithm and their fixed points are local maxima of a continuous function. 
1 	constraint satisfaction 


	meseguer and larrosa 	1 


1 	constraint satisfaction 

the dimension of vectors p and q increases as n1. 
   we have also implemented a discrete hill-climbing algorithm  which starts from a random configuration of queens  a queen for row   selects a row at random and performs a change in the position of its queen if the number of conflicts decreases. when the algorithm stops on a local minimum  the process restarts from another random configuration. for n 1  the algorithm reaches a minimum in approximately n iterations  and it needs between 1n to 1n restarts to achieve a solution. the number of conflicts in local minima commonly ranges from 1 to 1. regarding computational cost  this approach is globally less expensive than the projected gradient algorithm described above. the reason is simple: an iteration of the projected continuous gradient is far more costly than an iteration of hill-climbing  and this cost is not compensated by the lineal number of restarts needed by hill-climbing with respect to the constant number of restarts of the projected gradient. 
source  selecting as the next variable that with the highest weight corresponding with a value of its domain. however   1  is a ratio between supports  from  1    where 1n is added to assure a positive fraction. this ratio is sensitive to small variations of support and it exhibits some unstable behaviour. looking for robustness in variable selection  we moved to another criterion: select the variable with the lowest sum of supports for its remaining values  and order its values by decreasing support. we will refer to this heuristic as the lowest-support heuristic. it is obviously related to the optimization approach: it means to select a 
variable with a low denominator in  1  which implies that this variable will have high values of pi a  although not necessarily the highest. 
   the lowest support heuristic is computed each time a new variable has to be selected. assignment is performed as in standard forward checking. when a new variable is assigned  future domains are filtered  a new labeling is constructed and the heuristic is computed again to select the next variable. when backtracking occurs  a future domain becomes empty   the last assigned variable is reassigned using the value ordering set when it was selected. 
   this heuristic is expensive to compute. to simplify computation  we have considered two approximations. first  restrict heuristic computation to the set of variables with minimal domains  because there is a high chance that the selected variable will belong to this set. second  after assigning a variable we still use the computed supports for further assignments if the next lowest sum of supports is close enough to the initial one. we have accepted sum of old supports when they have become smaller  due to domain pruning  than the initial one. 
¡¡we will refer to forward checking with lowest-support without approximations as fc-ls  and with approximations as fc-ls-app. we will refer to the standard forward checking with first-fail  selecting variable with the smallest domain  breaking ties randomly and selecting values randomly  as fcff. we have tested these algorithms on two problems: random solvable graph colouring  and random problems. empirical results are given in the following. graph colouring. following  minton et al. 1   we tested our heuristic on graph 1-colourability problems. specifically  we have considered solvable random graphs with n nodes and m arcs  forming two classes: sparselyconnected graphs  m = 1n  and densely-connected graphs  m 
= n  n - 1  / 1 . we generated graphs on the range from n = 1 to n = 1  incrementing n by 1. for densely-connected graphs  fc-ls does not bring any real improvement to fc-ff. almost all problem instances were solved without backtracking for both algorithms  and regarding cpu time  fc-ls took a bit longer than fc-ff  because the time required by support computing. for sparsely-connected graphs  both fc-ls and fc-ls-app outperformed clearly to fc-ff in backtrackings and cpu time. results are given in table 1. 


random problems. we tested our heuristic on random problems following  prosser  1 . we worked on problem instances on the peak by choosing the appropriate tightness  p1  for a given connectivity  p1  and problem size  n  m . we considered two types: sparsely-connected instances  
 with p1; = 1  and densely-connected instances  with p1 = 1 . to select problems on the peak  we used the prosser's formula to compute the appropriate tightness. around this value  we varied p1 by steps of 1  in the interval where a fifty percent of problems happened to be solvable. ten instances of each p1 setting were used for testing. the results are given in tables 1 and 1. for sparsely-connected problems  fc-ls saves around half number of backtrackings required by fc-ff  although it needs more cpu time. including approximations  fc-ls-app is slightly worse than fc-ls in backtrackings  but it requires less time than fc-ff  except in the 1  1 case . for densely-connected problems  although the heuristic decreases the number of backtrackings performed by fc-ff  it is not cost-effective regarding time. 
   our results on graph colouring and random problems show that the proposed heuristic provides good advice  fc-ls always performs less backtrackings than fc-ff in every problem type   although it may not be cost-effective for some problem classes. it seems to be appropriate for sparsely-connected problems with a high number of variables relative to the number of values. in these problems  first fail variable selection heuristic is not enough to reduce drastically the number of backtrackings. relation between the usefulness of the heuristic and the shape of a p  requires further investigatioa 
1 conclusions 
this paper offers two main contributions. on the theoretical side we have shown that any binary discrete csp can be formulated as an optimization problem of a continuous function a p   which is constructed from the set of initial constraints. a global maximum of a p  corresponds to the best possible solution of the csp  that is  an assignment violating a minimal number of constraints. on the practical side we have applied this result to solve some csp problems. computing a global maximum of a p  is quite costly  so we moved to an heuristic approach: use the direction of change that increases a p  to generate a heuristic for variable and value selection. the results on two kind of csp show that it causes a low number of backtrackings  being cost-effective for sparsely-connected problems. 
1 	constraint satisfaction 

acknowledgements 
first author has been supported by the spanish cicyt under the project #tap1. we thank carme torras and david cohen for their useful comments. 
