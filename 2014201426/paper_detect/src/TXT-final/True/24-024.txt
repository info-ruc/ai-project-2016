 
this paper presents empirical evidence for five hypotheses about learning from large noisy domains: that trees built from very large training sets are larger and more accurate than trees built from even large subsets; that this increased accuracy is only in part due to the extra size of the trees; and that the extra training instances allow both better choices of attribute while building the tree  and better choices of the subtrees to prune after it has been built. for the practitioner with the common goals of maximising the accuracy and minimising the size of induced trees  these conclusions prompt new techniques for induction on large training sets. although building huge trees from huge training sets is computationally expensive  pruning smaller trees on them is not  yet it improves accuracy. where a pruned tree is considered too large for human or machine limitations  it can be overpruncd to an acceptable size. although this requires far more time than building a tree of that size from a correspondingly small training set  it will usually be more accurate. the paper also describes an algorithm for overpruning trees to user-specified size limits; it is evaluated in the course of testing the above hypotheses. 
	1 	introduction 
algorithms that induce decision trees from examples have been the subject of much machine learning  ml  research in recent years  quinlan  1 . as this family of algorithms has developed and propagated  the size and complexity of the training sets attacked has grown. for example  dietterich  hild & bakiri  1  report using decision tree techniques on 1 instances concerning the pronunciation of english. the large corporate databases that could potentially be used for commercial applications such as retail credit assessment  carter & catlett  1  are commonly much larger. catlett 
 1c  describes an application using hundreds of thousands of examples from the space shuttle; nasa's 
1 	learning and knowledge acquisition 
archives of this flight data contain more than 1 million training examples. 
decision trees induced from noisy data have a welldocumented tendency to  overfit  the training data  quinlan  1 . the lower nodes in the tree  built from few examples  are often testing irrelevant attributes  fitting the noise in the training data. such spurious nodes lower accuracy on unseen data. two common ways of avoiding this are to impose some criterion checking the relevance of attributes before building each node  and to go back after the tree has been built and remove subtrees judged not to be contributing a sufficient increase in accuracy for their size. 
even if all redundant nodes have been perfectly pruned  a tree may still be considered too large  for reasons other than accuracy. the most prominent reason is that large decision trees are incomprehensible. michie  1  argues strongly from many years of knowledge engineering projects that the ai goal of producing concepts human experts can understand dictates that the size of decision trees must be kept within a fairly small 
1
 'human window   comprehensibility is of course not determined by size alone; structure is very important  as shapiro  1  demonstrated. even when the size and structure of two concepts are the same  human beings may find one more comprehensible. but size is the most basic parameter  and it is easy to measure objectively. 
beyond the bounds of comprehensibility  limitations of resources may also force limits on the size of trees. a tree may be required to fit within a certain amount of memory  or to classify examples within a maximum amount of time. 
given that it is sometimes desirable to over prune trees  sacrificing some accuracy in order to reduce size  how much is enough  in their evaluation of their cn1 induction algorithm  which aims at building small sets of rules  clark & niblett  1  comment on the tradeoff between accuracy and size  or complexity .  we do not attempt to combine measures of accuracy and complexity... as such a combining function is dependent on the purpose for which the rules are to be used. for example  a 1% fall in predictive accuracy may or may not justify a 1% decrease in rule complexity  depending on the application.  thus an overpruning algorithm should allow tailoring of the size and accuracy of a tree according to the particular economics of the application. 
this paper investigates the effect on size and accuracy of building and pruning trees from large training sets. section 1 states and illustrates the hypotheses to be tested. section 1 gives a brief summary of the existing techniques for building and pruning trees  then describes new pruning and overpruning algorithms that arc based on them. section 1 briefly describes the domains used for testing  specifies the experiments and presents their results. section 1 discusses further and related work. 
1 	t h e hypotheses to be tested 
before stating the hypotheses  some graphical illustrations of the relationships between training set size  error rate and tree size may help clarify their motivation. 

figure 1: hyperthyroid learning curve  linear version  
figure 1 is an example of the most common graph in ml papers  often called a learning curve. the domain of the training instances is the hyperthyroid data described in  quinlan  compton  horn & lazarus  1 . the  piecewise linear  curve joins the average error rate of pruned decision trees built from various fixed sizes of training set. here the individual results of each of the twenty iterations are also plotted as dots. at each iteration the training set starts with 1 examples  and training instances are added without replacement in batches of the various predetermined sizes  with a new tree grown and assessed after each batch. unsurprisingly  the marginal improvement in error from additional training instances diminishes with the size of set. it is not easy to see whether the marginal improvements have vanished by the right hand side of the graph  but transforming both axes to log scales makes the picture clearer. extrapolation of the roughly straight line in figure 1 suggests that improvements in accuracy may be possible  but at the cost of processing much larger training sets. the graph does indicate whether the comparatively small improvements in accuracy being gained towards the right of the graph are statistically significant; this is tested as the first hypothesis  h1: very large training sets yield significantly more accurate trees than subsets half their size. these experiments used randomly chosen subsets  some selective strategies such as removing duplicated examples and balancing class frequencies with stratified samples are investigated in  catlett  1b . 

figure 1: hyperthyroid learning curve  log version  
turning from accuracy to size  figure 1 plots the size of the pruned trees on a linear scale  showing a roughly linear relationship between the size of the training set and the size of the tree. this is bad news for the goal of smaller trees. the hypothesis that the size of pruned trees continues to grow as the training set becomes very large is formulated as h1: very large training sets yield significantly larger pruned trees than subsets half their size. 

figure 1: growth of tree size  hyperthyroid  
to see the tradeoff between tree size and error rate  the y-values from figures 1 and 1 can be presented as a bivariate scattergraph  hiding the training set sizes. figure 1 shows tree size increasing exponentially as a function of accuracy  decreased error rate   suggesting that if larger training sets are used to reach a goal of greater accuracy  this quickly subverts the goal of small size. the ratio of the computational cost of induction to increased accuracy is even worse; this problem is illustrated in  catlett  1c  and attacked in  catlett  1a . 

figure 1: accuracy versus tree size  hyperthyroid  
a practitioner faced with this analysis and a practical induction task with a huge amount of data might 
	catlett 	1 
reasonably ask how the most accurate tree can be built within given size and time constraints. to design algorithms to meet this requirement requires more knowledge about why the error rates of the larger trees obtained from larger training sets are more accurate. there are three possible reasons  which arc exhaustive but not mutually exclusive; they comprise the last three hypotheses. the first is the most obvious explanation. h1: the greater size of the trees built from larger training sets improves their accuracy. h1 the better choice of attributes made while building trees from larger training sets improves their accuracy. h1: larger sets of examples used for pruning trees provide greater accuracy. 
empirical evaluation strongly corroborates all the hypotheses  motivating new algorithms and methods that are used in the experiments evaluating these hypotheses. h1 motivates the use of very large training sets to improve accuracy. h1 shows that some method of overpruning the resulting trees will be needed to impose a limit on tree size. h1 shows that this will incur a cost in accuracy  but h1 and h1 show that the accuracy will still be superior to trees built from just a small set. h1  along with information on the high cost of evaluating attributes on large training sets  motivates the development of more efficient and effective methods of choosing attributes during the construction of trees  such as  catlett  1a . h1 prompts the simple idea  described in the next section  of using a larger set for pruning than was used for building. 
	1 	tree building and pruning 
the program used in these experiments for building and pruning decision trees was c1  implemented in c and distributed by quinlan. the basic algorithm at the core of c1 is id1. for a comprehensive introduction to id1 see  quinlan  1 . various methods of pruning are described in  quinlan  1 ; c1 uses a variation on pessimistic pruning. this method uses a statistical adjustment to estimate the error rate on unseen data likely to be achieved by each subtree of the tree. a subtree 1 at a node is replaced by a leaf if s is estimated to have an error rate higher than the rate estimated for a leaf with the majority class in that position. similarly  c1 s pruning algorithm also examines the largest branch b of each node s.  b is the subtree immediately below the root of subtree s that represents the most examples  not necessarily the one with the most nodes.  if the estimated error rate of the subtree where s is replaced by b is lower than the estimate for 1 as it stands  the replacement goes ahead and the sibling branches are discarded. one might ask in passing why c1 considers only the largest branch. intuition suggests that this branch is the one most likely to give an improvement  and that searching all of the branches doing a recursive estimates would take a lot more time. experiments on the domains described in this paper showed that although branches other than 
1 	learning and knowledge acquisition 
the largest occasionally gave lower estimates  searching all the branches rarely resulted in a significantly lower error rate for the pruned tree. the time taken was greater  but not by a large factor. 
pessimistic pruning has the advantage over other pruning methods such as cost-complexity pruning  breiman  friedman  olshen & stone  1  that it docs not require that the set of examples used for pruning be distinct from the set used to build the tree. where training instances are scarce  it makes sense to use them all during the treebuilding  and then to use them again in pruning. but where resource constraints demand that only a sample of the available training instances can be used to build a tree  it is possible  and according to h1 may be beneficial  to use extra examples to prune it  pruning a 
tree requires far less time than building it.  this simple technique requires no change to the pruning algorithm and only a minor change to the code to implement  but as shown in table 1 in section 1 below  it gives significantly greater accuracy. 
the next technique builds on the pessimistic pruning methods of c1  aiming to over prune a tree to a userspecified size. it not always possible to prune a tree to exactly the specified size: for example  a tree of three nodes cannot be pruned to two. but the algorithm is guaranteed to produce a tree of at most the specified number of nodes. the algorithm assumes that the tree has already been pruned  but the extension to underpruning unpruned trees is straightforward. this could be useful if it were known that the regular pruning algorithm was pruning too aggressively in a domain. 
the basic idea of the overpruning algorithm is to remove those nodes that result in the least increased error estimate  while still taking the total size of the tree within the limit.  an early version on the overpruning algorithm  called op1  removed the parts of the tree that represent the least number of examples regardless of the error estimates.  finding the set of subtrees that gives the lowest total estimated increased error is a difficult combinatorial problem  so two iterative simplifications of the optimal solution were used. the first  op1  searches the tree at each iteration for the node that gives the lowest estimated increase in error; that node is pruned before the next iteration. this method favours the parents of leaves  and sometimes results in a lot of nibbling at the fringes of the tree  when biting off a big subtree would have given a lower increase in error. to counter this the estimate is adjusted by dividing by the size of the subtree being considered.  actually this may unduly favour lobbing off a subtree larger than necessary to get the tree below the required limit  so the denominator used was the maximum of the size of the subtree and the remaining reduction required.  this version is called op1. finally  the same technique applies to replacing nodes with branches as well as leaves; the version that does both is called op1. empirical evaluation showed the accuracy of trees given by each version superior to its predecessor; 

the results reported below are for op1. 
the current implementation is less efficient than necessary; it rescans the entire tree at each iteration. it would be better to store the statistics at the nodes and only recompute those parts of the tree that change. even with this gross disadvantage  the overpruning program is typically cheaper to run than the tree building program. 
a different way to achieve overpruning  but not to a parametrised size  is to bias the estimated error from subtrees and leaves so as to favour leaves. 1 experimented with a parameter that specifies the minimum ratio of these estimates  but was able to attain only very crude control of tree size. 
a possible advantage of iterative methods of overpruning is that at each iteration statistics can be produced on the size and estimated error rate of the intermediate trees. based on this information a developer might decide to use one of these less severely pared trees instead. the error estimates for trees that c1's pruning algorithm produces are often very accurate  but if more precision were required they could be recalibrated by assessing the error rate of some of the trees on a separate set of examples and adjusting the other estimates accordingly. 
1 description of domains and experiments 
ten noisy domains of a suitable size for these experiments were found. the first three are synthetic in the sense that the data are generated by programs; the remaining seven arc natural domains of interest to people other than computer scientists. the last five are various families of thyroid disorders  quinlan et al.   1   but can be considered separate domains. space does not permit a full description of the domains; this appears in  catlett  1b . two of them are time series data from bio-medical engineering applications  concerning ischemia  oates  cellar  bernstein  bailey & freedman  
1  and sleep disorders  ray  lee  morgan & airthkindree  1 ; others arc familiar from the ml literature: the waveform generator from  breiman et al.  1   and othello generator from  utgoff & heitman  1 . the diff domain simply classifies pairs of real numbers according to the sign of their difference. 
all the experiments followed the same procedure and are reported in the same formal. since the number of examples in the datasets varies from just over 1 to over 1  or an unlimited number in the case of the synthetic domains   and a distinct test set of at least 1 examples was reserved  this left two sizes for the training set: 1 in the case of heart  othello and sleep  and 1 for the others. training examples from synthetic domains were synthcsised anew with a different seed for each of the iterations described below  rather man using a fixed set of 1 examples generated once. 
each experiment compares two values of a single parameter  error rate or tree size  obtained from two different treatments  called a and b  of an series of 1 constructed training sets. for example  in the first experiment treatment b uses the full training set to learn from  and a uses only half the set. for each of the 1 iterations  a training set and a lest set arc constructed by splitting the data randomly according to the sizes specified above. the parameter is measured by evaluating each pair of trees on the test set. the difference of this pair of parameters is averaged over all iterations; the tables report the standard deviation of this difference  as well as the average of the trees from treatment a and b. the aim is to test the null hypothesis that there is no difference observed on the parameter between treatments a and b; this is done with a twosided student t-tcst at the 1% level.  whether this  statistically significant  difference is large enough to be significant to a client obviously depends on the application.  in the tables below  an entry of   a   under the heading   n h   indicates the null hypothesis is accepted. an entry of   r +   indicates that the difference is positive  i.e. a is significantly higher than b   and an entry of  r  indicates that the null hypothesis is rejected and that a is significantly lower than b. 
table 1 shows the result of testing h1  mat very large training sets yield significantly more accurate trees than subsets half their size. treatment a uses a randomly sampled 1% of the training set; b uses the full training set. the result is unanimously  reject+   indicating that a has a higher error rate man b  in other words  the full training set  b  gives more accurate trees  corroborating the hypothesis. 
	domain 	a 	b 	sd a-b  	nh 
i wave 1 1 1 r+ diff 1 1 1 r+ othello 1 1 1 r+ heart 1 1 1 r+ sleep 1 1 1 r+ hyper 1 1 1 r+ hypo 1 1 1 r+ binding 1 1 1 r+ replace 1 1 1 r+ euthy 1 1 1 r+ j table 1: error rates: b&p-1%  a  vs b&p-1%  b  
the second experiment uses the same treatments  but the dependent variable is the size of the trees built. this tests h1  that size of pruned trees continues to grow as the training set becomes very large. the statistical test shows only that the trees have almost certainly grown by some positive amount  but the estimated means in table 1 indicate a growth of about 1%  strongly corroborating the hypothesis. tree size appears to be a roughly linear function of training set size in these 
	catlett 	1 
domains  although on the noise-free shuttle data described in catlett  1c  the function more resembled the cube root 

table 1: tree size: b&p-1%  a  vs b&p-1%  b  
the next experiment tests h1  that a larger training set allows better pruning of the tree. table 1 shows error rate as the dependent variable; table 1 shows tree size. 

table 1: error rate: b&p-1  %  a  vs b-1%;p-1%  b  
in both treatments a and b the trees were built on 1% of the training set  but b was given the full training set for the pruning  whereas a was pruned on the same 1%. the unanimous result on error rate corroborates 
hypothesis 1. table 1 shows that in many domains the smaller trees were obtained by pruning on a larger set. in two domains  othello and diff  they were larger. 

table 1: tree size: b&p-1%  a  vs b-1%;p-1  1%  b  
1 	learning and knowledge acquisition 
table 1 concerns h1  that the larger training set allows a better choice of attributes. to test this both a and b are given a full set for pruning  but for the tree building  a is given only 1% of the examples  whereas b is given the full set. to eliminate differences due to the larger size of b s trees  each tree built by b was overpruned to the same size as the corresponding tree from a. the hypothesis held for all domains except euthyroid. 

table 1: error rate: b-1%;p-1%  a  vs overpruned  b  
the results of two other experiments warrant reporting without the extra tables. varying the previous experiment so that a prunes only on the same 1% it uses for building tests a different hypothesis  that trees built from a small sample are less accurate than trees built from a large set and overpruned to the same size as the small trees. the results unanimously corroborated this hypothesis; the difference in error rates were mostly a factor of two or three times those in table 1. the uncontentious hypothesis that additional size of trees can improve their accuracy  h1  was tested by an experiment similar to that of table 1  except that a was changed to growing and pruning on the full training set  the same as b without overpruning . the larger trees from a were significantly more accurate in all domains except wave. this exception suggesting that pruning may not be aggressive enough in this domain. 
1 discussion and conclusion 
these experiments corroborate the five hypotheses stated in section 1  showing where additional accuracy can be squeezed out of learning algorithms. they also demonstrate the utility of the two new techniques used in the experiments: pruning on a larger set  and overpruning. improvements that are statistically significant can be hard to obtain when error rates are already below 1 %  and in some domains improvements of several percentage points were obtained. these techniques let the practitioner move closer to the fundamental goals obtaining small and accurate concepts from learning systems. 

¡¡several avenues for further work are open. a close evaluation of the efficiency and effectiveness of the overpruning algorithm might lead to improvements  although the construction of the large tree in the first place is more important to the goal of conserving resources. a method of speeding up this step on very large training sets sets is described in  catlett  1a . the conversion of overpruncd trees to production rules is obviously possible  but the control over the size of the rule base will not be directly specified. an evaluation of the tradeoff between accuracy and comprehensibility to humans would be of great interest to those concerned with synthesising knowledge from large data bases. although comprehensibility to human experts degrades as the number of rules or nodes increases  the tolerances around a critical point are probably not as tight as a single node  so this batch-style overpruning algorithm may be less appropriate than some kind oi  interactive pruning system  analogous to the tree building oi shapiro's interactive id1  shapiro  1 . presenting an expert with a series of trees in order of increasing complexity may enhance comprehension. 
comparisons with a wide variety of existing ml  algorithms could be made on the basis that oveqiruning is one way of buying additional accuracy by spending cpu time  assuming that training instances are abundant and cheap . other ways of doing this include spending more time on the tree building  for example by attempting to build a minimal tree  as in  van de velde  1   and improving the attributes by constructive induction  rendell  1 . such investigations should further understanding about the relationship between the fundamental parameters of learning time and concept size and accuracy  and aid the development of techniques for best achieving specific goals for these parameters. 
acknowledgments 
many people deserve thanks for providing data: 	ben 
freedman and john oates of royal prince alfred 
hospital for the heart data  ross quinlan for the thyroid data  sylvian ray for the sleep data  and paul ulgolf for the othello data. thanks are also due to ross quinlan for his advice and suggestions  particularly concerning some of the variations of the overpruning algorithm  and lor supplying code for c1. discussions with donald michie motivated me to focus on size and comprehensibility. this work was supported in part by a grant from the australian research council. 
references 
breiman et al. 1. leo breiman  jerome h. friedman  richard a. olshen  and charles j. stone. classification and regression trees. wadsworth  belmont  ca  
1. 
carter & catlett 1. 	c. carter and j. catlett. 
assessing credit card applications using machine learning. ieee expert 1   fall 1  pp. 1  ieee computer society. 
catlett 1a. j. catlett. choosing attributes through peepholes. basser department of computer science  university of sydney  1. 
catlett 1b. j. catlett. an evaluation of techniques that select subsets of training sets. basser department of computer science  university of sydney  1. catlett 1c. j. catlett. megainduction: a test flight. in birnbaum  l.  and collins  g.  ed.  machine learning: 
proceedings of the eigth international workshop. san mateo  ca. morgan kaufmann. 
clark & niblett 1. peter clark and tim niblett. the cn1 induction algorithm. machine learning 1 
 1 . 
diettench et al 1. thomas g. diettcrich  hermann 
hild  and ghulum bakiri. a comparative study of id1 and backpropagation for english text-to-speech mapping. in porter  b.  and mooney  r.  ed.  proceedings of the seventh international conference on machine learning. austin  texas. morgan kaufmann. 
michie 1. 	d. michie. 	current developments in 
expert systems. in quinlan  j.r.  ed.  applications of 
expert systems. glasgow. turing institute press with addison wesley. 
gates et al. 1. j. oates  b. cellar  l. bernstein  b.p. 
¡¡bailey  and s.b freedman. real-time detection of ischemic ecg changes using quasi-orthogonal leads and arificial intelligence. in proceedings  ieee computers in cardiology conference. quinlan 1. j. r. quinlan. induction of decision trees. machine learning 1  1  pp. 1. 
quinlan 1. j. r. quinlan. simplifying decision trees. international journal of man-machine studies 1  1  pp. 1. 
quinlan et al. 1. j.r. quinlan  pj. compton  k.a. 
horn  and l. lazarus. inductive knowledge acquisition: a case study. in quinlan  j.r.  ed.  applications of expert systems. glasgow. turing institute press with addison wesley  pp. 1 
ray et al. 1. s.r. ray  w.d. lee  cd. morgan  and 
w. airth-kindrec. computer sleep stage scoring: an expert systems approach. international journal of bio-medical computing 1  1  pp. 1. 
rendell 1. l. rendell. comparing systems and analysing functions to improve constructive induction. in segre  a.  ed.  proceedings of the sixth 
international workshop on machine learning. ithaca  
new york. morgan kaufmann. pp. 1 
shapiro 1. allen d. shapiro. structured induction in expert systems. turing institute press  glasgow  1. utgoff & heitman 1. p.e. utgoff and p.s. heitman. 
learning and generalizing move selection p