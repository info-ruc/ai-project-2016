 
even if represented in a way which is invariant to illumination conditions  a 1d object gives rise to an infinite number of 1d views  depending on its pose. it has been recently shown    that it is possible to synthesize a module that can recognize a specific 1d object from any viewpoint  by using a new technique of learning from examples  which are  in this case  a small set of 1d views of the object. in this paper we extend the technique  a  to deal with real objects  isolated paper clips  that suffer from noise and occlusions and b  to exploit negative examples during the learning phase. we also compare different versions of the multilayer networks corresponding to our technique among themselves and with a standard nearest neighbor classifier. the simplest version  which is a radial basis functions network  performs less well than a nearest neighbor classifier. the more powerful versions  trained with positive and negative examples  perform significantly better. our results  which may have interesting implications for computer vision despite the relative simplicity of the task studied  are especially interesting for understanding the process of object recognition in biological vision. 
1 	introduction 
shape-based visual recognition of 1d objects may be solved by first hypothesizing the viewpoint  e.g.  using information on feature correspondences between the image and a 1d model   then computing the appearance of the model of the object to be recognized from that viewpoint and comparing it with the actual image   1; 1; 1; 1; 1  . most recognition schemes developed in computer vision over the last few years employ 1d models of objects. automatic learning of 1d models  however  is in itself a difficult problem that has not been much 
1 	vision 
addressed in the past and which presents difficulties  especially for any theory that wants to account for human ability in visual recognition. 
   recently  recognition schemes have been suggested that  relying on a set of 1d views of the object instead of a 1d model   1; 1; 1    offer a natural solution to the problem of model acquisition. in particular  poggio and edelman    have argued that for each object there exists a smooth function mapping any perspective view into a  standard  view of the object and that this multivariate function may be approximatevely synthesized from a small number of views of the object. such a function would be object specific  with different functions corresponding to different 1d objects. since synthesizing an approximation to a function from a small number of sparse data - the views - can be considered as learning an input-output mapping from a set of examples   1; 1    poggio and edelman used a scheme for the approximation of smooth functions which is equivalent to a class of multilayer networks called regularization networks and  in their more general form  hyperbasis functions. for each 1d object there exist a small network  which is 
 learned  directly from a small set of perspective views of the object. they demonstrated the successful performance of such a scheme using computer simulated 1d wireframe objects similar to paperclips. their experiments assumed that the object had been isolated from the background and that features  such as the specific corners or angles between the segments  had been extracted and matched to the corresponding features of the model views. furthermore  their data were noisefree and without any occlusions. 
   in this paper we extend their technique and experiments to more realistic situations. our ultimate goal is to implement a system for the recognition of human faces by applying the hyperbf technique to view vectors computed from the image by extracting features such as the position of the eyes and mouth and the color of the hairs. 
   real 1d objects introduces several difficulties  namely the presence of noise in the feature data  the ignorance of the correspondence between the features of different 

views of the same object and finally the necessity to use incomplete feature vectors  due to the presence of occlusions and/or to the inability to recover correctly some of the objects features . it seems reasonable to limit  at least in a first step  all of these difficulties to the recognition phase: the learning phase is supervised and uses  good  example views  where the problem of correspondence has been removed and the noise reduced. 
   the main result of the paper is that the hyperbf technique  suitably modified  can deal successfully with the problems of noise  occlusions and missing correspondences  at least for the simple 1d objects we consider here. one of the most useful and interesting of our extensions of the techique is the use of negative examples in the training  that is in the model acquisition  phase. 
   the plan of the paper is as follows. the first section gives a brief review of the simple r b f technique. we then describe the experiments and compare the performance of different version of the algorithm  including performance of a standard nearest neighbor classifier . the more general hyperbf network is then introduced and characterised in terms of experimental performance. 
1 	radial basis functions 
radial basis functions can be regarded as a special case of regularization networks introduced in  as a general approximation technique that can be used in problems of learning from examples. 
   a scalar function can be approximated  given its value on a sparse set of points { x i }   by an expansion in radial functions: 
 1  
where 	represents the usual euclidean norm. 	the 
computation of the coefficients ci rests on the invertibility of matrix  which has been proved 
 see micchelli   for functions such as: 
 1   1  
   it is possible to use fewer radial functions than examples  i.e. data points. the resulting overconstrained system can be solved in a least square way under the conditions of michelli's theorem and proves to be useful when many examples are available. 
   poggio and girosi   1; 1   have shown that the r b f technique is a special case of the regularization approach to the approximation of multivariate functions. in the regularization approach one seeks the approximating function which is closest to the data and smoothest  according to an appropriate criterion. the r b f technique described above  which is the simplest version of the hyperbf scheme described later  was used in the experiments described in the next section. 
1 	experimental setup 
the objects used in the experiments were five paper clips  randomly generated of the same length  rendered through ray-tracing techniques  see fig. 1 . the small number of objects used must be taken into account when the experimental results are considered. the small difference in performance among the different techniques is expected to increase if the number of objects to be classified increases. the clip was first isolated from the background    and the resulting binary image was skeletonised     giving essentially a line drawing. a polygonal approximation routine identified line segments which were then used to reconstruct the clip using an a* search   . the same algorithms were applied to a real clip  images were taken with a c c d camera  and proven to perform equally well. the reconstructed clips were used to simulate different degrees of occlusions by assigning to each clip a finite radius  the bigger the radius the higher the occlusion percentage . 
   for each clip a set of 1 views were available among which the learning and testing sets could be chosen. the attitude was restricted to one octant of the viewing sphere. each clip  having six vertices  was described as a vector of twelve coordinates  representing the position on the image plane of the vertices  expressed in the barycentric coordinate system identified by the vertices to remove translational dependency . the use of the  x y  coordinates of the vertices on the image plane 
is one of the many possible choices of features. for instance  the angles between the clip segments could have been used   . broadly speaking  every feature that can be cast into numerical form and that maps smoothly into the output of the network  may be used in the recognition process. 

figure 1: a real clip  left  and a ray traced clip  right  
1 	experiments 
the purpose of the experiments was to test the recognition performance of several strategies. for each rendered clip  from a set of 1 views  a learning set of given cardinality and a testing set of fixed cardinality  1 views  were extracted. the problem of feature correspondence was constrained by the nature of the objects. the clips could be present in the learning set in one of the two natural orientation  the reconstruction algorithm did not fix this ambiguity . in the recognition phase each clip was used in the two ways attempting recognition and 
	brunelli and poggio 	1 

choosing the best result. the order of the vertices was assumed to be correct but for this ordering ambiguity. the output representation used for recognition was the characteristic function of the clips: 
		 1  
where p ci  represents the space spanned by the perspective projections of the i-th clip. the synthesis of the characteristic function from the learning set is realised by what will be referred to as rbf module. the response of a module to an input vector  a clip to be classified  is the value of the r b f expansion at the given point  clipped to the interval  1 . the clip is then assigned to the module with the highest response. 
several groups of experiments have been performed: 
  using only positive examples and complete vertex information; 
  using only positive examples and occluded clips; 
  using positive and negative examples with complete vertex information. 
   positive examples are views of the correct clip and negative examples are views of other clips  therefore incorrect clips for the module under training . 
¡¡in the first group of experiments the number of centers in the r b f expansions matched that of the available positive examples for the given clip. the performance reflects how well the r b f scheme works with noise in the feature vector and in the specific task. as we mentioned before  real 1d objects exhibit occlusions  topological absence of features   unrecovered features  defective feature extraction  and correspondence problems. the first two characteristics require the use of incomplete feature vectors. in the  vanilla'' version of rbf described in section 1  the radial basis functions must be completely specified with their parameters before training. for gaussian radial basis functions  the value for must be chosen. let be the learning set. to each example a nearest neighbor can be associated  i.e. an example 
 1  
since the theory    allows only for a global value of 
  and not for a different one for each unit   the average nearest neighbor distance was used: 
		 1  
with n the number of examples in the learning set. if the learning set spans uniformly the space where the function is defined  no problems arise. if this is not the 
1 	vision 
case over/under generalisation may be expected in this simple scheme. 
   during the classification task  incomplete feature vectors may be presented to the r b f module. t w o strategies were investigated. the first one is probably the simplest: all possible combination of the available data are tested. its major drawback is that the number of such combinations grows quite rapidly with the number of the available features. the second strategy is directly related to the idea of characteristic views of a three dimensional object  see  . it can be shown that the space of the possible perspective views of a three dimensional object can be partitioned into subspaces with the following properties: 
  all the views in a single subspace are topologically equivalent  their projected edge-structure exhibit a 
junction line identity ; 
  every view in a single subspace can be transformed into another view of the same subspace with a linear transformation  in homogeneous coordinates . 
   an equivalence relation can then be defined and the quotient space considered. the elements of the quotient space are called characteristic views. the topological equivalence of the views in the same subspace implies that each view has the same number of features  such as vertices and faces   the reverse being not necessarily true. it is then natural to assign an r b f module to each characteristic view. 

figure 1: a schematic view of the modified characteristic view approach 
   let us focus on gaussian rbfs. if a sufficiently dense learning set is available  the value returned by the rbf approximation quickly decays with the distance from the learning set. it is reasonable to consider the examples in the learning set as wire frame objects  so that there are no occlusions  somewhat like c a d models . we can then estimate the value of  on this virtual set. the learning set can be subdivided into subsets whose members share the same number of visible  real  features. of course  an equal number of features is necessary but not sufficient to share the same characteristic view. the diagram of fig. 1 shows a possible scenario and how it can be mapped into different r b f modules  one for each dimensionality  instead of one for each characteristic view  of the learning examples. 
   the use of a reduced dimensionality implies the use of a modified  is directly related to a distance  the following transformation rule was adopted: 

where  is the value at dimensionality j. whenever a feature vector with a dimensionality that does not match any of the available examples is presented to the recognition module  the first strategy can be employed. we must  however  solve the problem of how to compute the distance between points for which some coordinates may not be available. let us define the metric in the following way: 
		 1  
where m is a symmetric  positive definite matrix in  called the metric matrix. if the standard eu-
clidean metric is used and the learning set is complete the following metric can be used   : 
		 1  
where  is the kronecker symbol  gi - 1 if the ith coordinate is available in the point to be classified and gi - 1 otherwise. a more useful metric can also be defined    whose diagonal elements take into account the number of missing coordinates. if we choose to preserve the trace of the  diagonal  metric matrix  the elements of the metric 
matrix are given by: 
		 1  
this corresponds to working in a reduced dimensionality r b f module with an effective  chosen according to the equation 1. the resulting mixed strategy proved to be quite effective  especially with high occlusions percentages. the use of multiple r b f modules with the mixed strategy and a weighted metric provides uniform results  so that the answer of the different modules can be compared to determine the best answer. 
   to gauge the performance of the r b f scheme  a nearest neighbor classification scheme was used on the same data sets. several additional experiments not reported here have also been performed   . 
   for each experiment two quantities are shown in the figures: 
m i n / m a x : the minimum error made by the module on a wrong clip divided by the maximum error on a right clip. when min/max   1 the module can avoid false alarm through the introduction of a threshold  sec  . the average m i n / m a x value over the different modules is reported in the graphs. 
r e c o g n i t i o n : is computed assigning each clip to the module with the smallest error  ignoring any information from m i n / m a x . 
1 	performance analysis 
the comparison of  vanilla  r b f and nearest neighbor classification seems to favor the latter  see fig. 1 . this can be explained by some recent theoretical results. it can be shown    see also   that under ortographic projections  the view of a given object as denned earlier  spans a 1-dimensional subspace. if the viewer is at a reasonable distance from the object there is only a negligeable difference between perspective and ortographic projection. this imply that the views of each clip almost certainly span non-intersecting 1-manifolds  embedded in the r1 representation space. in these experiments  the standard euclidean metrics was used. the use of gaussians effectively corresponds to setting a volume 
 around  the 1-manifold. this volume is bigger when the number of examples is smaller  larger inter samples distances  and this can explain the inferior performance of the r b f recognition scheme  which is overgeneralizing. the average m i n / m a x error is better for rbf even if the worst case  worst m i n / m a x figure of the recognition modules  is usually slightly better for n n . 
   it is interesting to compare the performance of exhaustive matching and multiple modules matching   . whereas at low occlusion percentages the performance of the two strategies is very similar  the multiple modules approach performs definitely better with a large number of occlusions. the use of a weighted metric increases all the performance measures for both strategies. this improvement is due to the fact that when some features are missing we require a progressively closer match for those we have: this allows the reduction of false alarms and of overgeneralization. 
   another possible output representation  which we did not use because it can be proved to yield an inferior performance in this case     is that of a prototype view   : every view of a given clip is mapped into a particular view of that clip  called its prototype view  this mapping is realized through a vector valued function . the clip is then assigned to the module exhibiting the smallest distance from the output of the r b f expansion to its prototype view. 
   the differences in performance using only positive examples  positive examples with information from negative examples  rbf centers only on positive examples  and positive and negative examples  centers on both positive and negative examples  have been tested using radial gaussian functions  see fig. 1 . due to the decayto-zero of gaussians negative examples are effective for the vanilla r b f scheme only when the number of examples is small. 
	brunelli and poggio 	1 

figure 1: comparison between r b f with complete vertex information  ad  and the multiple modules strategy  mi  at different occlusion percentages. 

figure 1: recognition rates using information from negative examples: p c p n i uses as centers only the positive examples but takes advantage of the negative examples information  pcpni n  uses only positive centers to compute  p n c p n i uses as centers all of the positive and negative examples; pcpi does not use negative examples and is provided for comparison. 
1 	hyper basis functions technique 
the  vanilla  r b f technique provides a basis for comparison with the extensions of the regularisation network technique described by t. poggio and f. girosi in . from a more general formulation of the variational problem of regularization they derive the following approximation scheme  instead of equation  1 : 
		 1  
where the parameters t a   that we call  centers   and the coefficients ca are unknown  and are in general much fewer than the data points  the term p x  is a polynomial that often can be neglected  though it usually consists of the constant and linear terms. the norm is a weighted norm 
		 1  
where w is an unknown square matrix and the superscript t indicates the transpose. in the simple case of diagonal w the diagonal elements wi assign a specific weight to each input coordinate  determining in fact the units of measure and the importance of each feature. in this formulation the learning stage is used to estimate not only the coefficients of the rbf expansion  but also the metric  problem dipendent dimensionality reduction  and the position of the centers  optimal examples selection . 
   this more general optimization problem cannot be solved through matrix inversion and methods like gradient descent    or random minimization techniques   1; 1   must be used  as the possibility of getting trapped in poor local minima is significant we opted for the latter approach . 
   the possibility of moving the expansion centers  initially located on some of the learning examples  is useful in the presence of noise in the available data and for improving the representativity of the center. the ad-
justable metric is even more important  since it allows to set a more specific dissimilarity measure between new views and centers than euciidean metric. 
   one of the most interesting results is that in the task of object recognition  and if an approach similar to ours is used  an adjustable metric requires necessarily negative examples. the argument  substantiated by experiments  rests on the trivial observation that if only positive examples were used  it would be possible to obtain 

1 	vision 

an optimal  in terms of quadratic error on the examples  approximation to the characteristic function of an object by mapping every possible input view to the value 1  that is by choosing w = 1. this is obviously a poor choice for the task of object discrimination. the use of negative examples is therefore essential. the added complexity is small since the computational complexity of the rbf 

expansion depends linearly on the number of examples  for a fixed number of centers . this means that if minimization techniques such as gradient descent or random minimisation are used  the increase in the computation time is expected to be linear. in the case of object recognition  negative examples allow to remedy in an elegant way what is the main problem in most learning tasks: the need of a large number of examples. use of a large number of positive examples would make the whole approach of this paper quite moot: each object would be represented essentially as a look-up table of very many of its views with all the corresponding complexity of storage and acquisition. our proposal  supported by our experiments  is  instead  to use a small number of positive views and a large number of negative examples  which are easily available if the data basis includes many objects . 
   the plots of fig. 1 gives some information on how the generalization proceeds with the number of positive and negative examples available as well as a comparison with the nearest neighbor results on the same data sets. rbf n  refers to a one center hyperbf expansion with movable coefficient  center and diagonal metric  n is the number of positive examples . 
   the experimental results allow us to rank the different r b f approaches together with the nearest neighbor classification scheme providing a useful gauge. the rbf approaches can be sorted  by increasing performance  in the following way: 
1.  vanilla  rbf: use of only positive examples with as many centers as available examples 
1. r b f with negative examples: only the positive examples are used as centers but the information from the negative examples is used 
1. nearest neighbor: the performance is nearly equal to that of r b f with negative examples 
1. hyperbf with diagonal metric  negative information must be used  
1. hyperbf with complete metric: again negative information must be used. 
1 	conclusions 
we have applied recently proposed networks for learning from examples  the vanilla r b f version as well as the more powerful hyperbf scheme  to the problem of 1d object recognition. experimental results on recognition rate have been obtained for wire frame objects  paper clips represented as polylines  hence with no occlusions  and for more realistic objects  paper clips represented as a set of cilinders of varying radii  hence exhibiting different percentages of occlusions . we have extended the r b f technique in order to cope with the problem of feature occlusion through the introduction of a modified euclidean metric. a characteristic view representation has been compared to an exhaustive search for the best match in the case of self-occluded clips. especially interesting is the use of negative examples which yield some improvement in the vanilla rbf approach and are critically important for the more general hyperbf scheme  while reducing considerably the complexity of the technique in terms of representation and model acquisition. the hyperbf generalization of the basic technique  with the introduction of movable expansion centers and the synthesis of a task dependent metric  proved to be successful in obtaining a compact representation of the ob-
jects that appears to work well in the - admittedly still 
very limited - recognition tasks described here. 
