
we propose a novel approach to the problem of simultaneous localization and mapping  slam  based on incremental smoothing  that is suitable for real-time applications in large-scale environments. the main advantages over filter-based algorithms are that we solve the full slam problem without the need for any approximations  and that we do not suffer from linearization errors. we achieve efficiency by updating the square-root information matrix  a factored version of the naturally sparse smoothing information matrix. we can efficiently recover the exact trajectory and map at any given time by back-substitution. furthermore  our approach allows access to the exact covariances  as it does not suffer from under-estimation of uncertainties  which is another problem inherent to filters. we present simulation-based results for the linear case  showing constant time updates for exploration tasks. we further evaluate the behavior in the presence of loops  and discuss how our approach extends to the non-linear case. finally  we evaluate the overall non-linear algorithm on the standard victoria park data set.
1 introduction
the problem of simultaneous localization and mapping  slam  has received considerable attention in mobile robotics  as it is one way to enable a robot to explore and navigate previously unknown environments. for a wide range of applications  such as commercial robotics  search-and-rescue and reconnaissance  the solution must be incremental and real-time in order to be useful. the problem consists of two components: a discrete correspondence and a continuous estimation problem. in this paper we focus on the continuous part  while keeping in mind that the two components are not completely independent  as information from the estimation part can simplify the correspondence problem.

figure 1: updating the upper triangular factor r and the right hand side  rhs  with new measurement rows. the left column shows the first three updates  the right column shows the update after 1 steps. the update operation is symbolically denoted by   and entries that remain unchanged are shown in light blue.
모current real-time approaches to slam are typically based on filtering. while filters work well for linear problems  they are not suitable for most real-world problems  which are inherently non-linear  julier and uhlmann  1 . the reason for this is well known: marginalization over previous poses bakes linearization errors into the system in a way that cannot be undone  leading to unbounded errors for large-scale applications. but repeated marginalization also complicates the problem by making the naturally sparse dependencies between poses and landmarks  which are summarized in the information matrix  dense. sparse extended information filters  thrun et al.  1  and thin junction tree filters  paskin  1  present approximations to deal with this complexity.
모smoothing approaches to slam recover the complete robot trajectory and the map  avoiding the problems inherent to filters. in particular  the information matrix is sparse and remains sparse over time  without the need for any approximations. even though the number of variables increases continuously for smoothing  in many real-world scenarios  especially those involving exploration  the information matrix still contains far less entries than in filter based methods  dellaert  1 . when repeatedly observing a small number of landmarks  filters seem to have an advantage  at least when ignoring the linearization issues. however  the correct way of dealing with this situation is to switch to localization after a map of sufficient quality is obtained.
모in this paper  we present a fast incremental smoothing approach to the slam problem. we revisit the underlying probabilistic formulation of the smoothing problem and its equivalent non-linear optimization formulation in section 1. the basis of our work is a factorization of the smoothing information matrix  that is directly suitable for performing optimization steps. rather than recalculating this factorization in each step from the measurement equations  we incrementally update the factorized system whenever new measurements become available  as described in section 1. this fully exploits the natural sparsity of the slam problem  resulting in constant time updates for exploration tasks. we apply variable reordering similar to  dellaert  1  in order to keep the factored information matrix sparse even when closing multiple loops. all linearization choices can be corrected at any time  as no variables are marginalized out. from our factored representation  we can apply back-substitution at any time to obtain the complete map and trajectory in linear time. we evaluate the incremental algorithm based on the standard victoria park dataset in section 1.
모as our approach is incremental  it is suitable for real-time applications. in contrast  many other smoothing approaches like graphslam  thrun et al.  1  perform batch processing  solving the complete problem in each step. even though explicitly exploiting the structure of the slam problem can yield quite efficient batch algorithms  dellaert  1   for large-scale applications an incremental solution is needed. examples of such systems that incrementally add new measurements are graphical slam  folkesson and christensen  1  and multi-level relaxation  frese et al.  1 . however  these and similar solutions have no efficient way of obtaining the marginal covariances needed for data association. in section 1 we describe how our approach allows access to the exact marginal covariances without a full matrix inversion  and how to obtain a more efficientconservativeestimate.
1 slam and smoothing
in this section we review the formulation of the slam problem in a smoothing framework  following the notation of  dellaert  1 . in contrast to filtering methods  no marginalization is performed  and all pose variables are retained. we describe the underlyingprobabilistic model of this full slam problem  and show how inference on this model leads to a least-squares problem.
1 a probabilistic model for slam
we formulate the slam problem in terms of the belief network model shown in figure 1. we denote the robot state at the ith time step by xi  with i 뫍 1...m  a landmark by lj  with j 뫍 1...n  and a measurementby zk  with k 뫍 1...k. the joint probability is given by


figure 1: bayesian belief network representation of the slam problem  where x is the state of the robot  l the landmark locations  u the control input and z the measurements.
where p x1  is a prior on the initial state  p xi|xi 1 ui  is the motion model  parametrized by the control input ui  and p zk|xik ljk  is the landmarkmeasurementmodel  assuming known correspondences  ik jk  for each measurement zk.
모as is standard in the slam literature  we assume gaussian process and measurement models. the process model xi = fi xi 1 ui  + wi describes the robot behavior in response to control input  where wi is normally distributed zero-mean process noise with covariance matrix 붦i. the gaussian measurement equation zk = hk xik ljk +vk models the robot's sensors  where vk is normally distributed zeromean measurement noise with covariance 쑒.
1 slam as a least squares problem
in this section we discuss how to obtain an optimal estimate for the set of unknowns given all measurements available to us. as we perform smoothing rather than filtering  we are interested in the maximum a posterior  map  estimate for the entire trajectory x = {xi} and the map of landmarks l = {lj}  given the measurements z = {zk} and the control inputs u = {ui}. let us collect all unknowns in x and l in the vector 붣 =  x l . the map estimate 붣  is then obtained by minimizing the negative log of the joint probability p x l z  from equation  1 :
	붣  = argmin logp x l z 	 1 
붣
combined with the process and measurement models  this leads to the following non-linear least-squares problem
	붣 	=	argmin

where we use the notationfor the squared
mahalanobis distance given a covariance matrix .
모in practice one always considers a linearized version of this problem. if the process models fi and measurement equations hk are non-linear and a good linearization point is not available  non-linearoptimizationmethods solve a succession of linear approximations to this equation in order to approach the minimum. we therefore linearize the least-squares problem by assuming that either a goodlinearization point is available or that we are working on one iteration of a non-linear

figure 1: using a givens rotation to transform a matrix into upper triangular form. the entry marked 'x' is eliminated  changing some or all of the entries marked in red  dark   depending on sparseness. optimization method  see  dellaert  1  for the derivation:
붻붣 	=	argmin

where are the jacobians of hk with respect to a change in xik and ljk respectively  fii 1 the jacobian of fi at xi 1  and gii = i for symmetry. ai and ck are the odometry and observation measurement prediction errors  respectively.
모we combine all summands into one least-squares problem after dropping the covariance matrices 붦i and 쑒 by pulling their matrix square roots inside the mahalanobis norms:

for scalar matrices this simply means dividing each term by the measurement standard deviation. we then collect all jacobian matrices into a single matrix a  the vectors ai and ck into a right hand side vector b  and all variables into the vector 붿  to obtain the following standard least-squares problem:
	붿  = argmin||a붿   b||1	 1 
붿
1 updating a factored representation
we present an incrementalsolution to the full slam problem based on updating a matrix factorization of the measurement jacobian a from  1 . for simplicity we initially only consider the case of linear process and measurement models. in the linear case  the measurement jacobian a is independent of the current estimate 붿. we therefore obtain the least squares solution 붿  in  1  directly from the standard qr matrix factorization  golub and loan  1  of the jacobian a 뫍 rm뫄n:
		 1 
where q 뫍 rm뫄m is orthogonal  and r 뫍 rn뫄n is upper
                                                       붟	ta can triangular. note that the information matrix i = a also be expressed in terms of this factor r as i = rtr  which we therefore call the square-root information matrix r. we rewrite the least-squares problem  noting that multiplying with the orthogonal matrix qt does not change the norm:
		 1 

figure 1: complexity of a simulated linear exploration task  showing 1-step averages. in each step  the square-root factor is updated by adding the new measurement rows by givens rotations. the average number of rotations  red  remains constant  and therefore also the average number of entries per matrix row. the execution time per step  dashed green  slightly increases for our implementation due to tree representations of the underlying data structures.
where we defined  d e t =붟 qtb. the first term ||r붿   d||1 vanishes for the least-squares solution 붿   leaving the second term ||e||1 as the residual of the least-squares problem.
모the square-root factor allows us to efficiently recover the complete robot trajectory as well as the map at any given time. this is simply achieved by back-substitution using the current factor r and right hand side  rhs  d to obtain an update for all model variables 붿 based on
	r붿 = d	 1 
note that this is an efficient operation for sparse r  under the assumption of a  nearly  constant average number of entries per row in r. we have observed that this is a reasonable assumption even for loopy environments. each variable is then obtained in constant time  yielding o n  time complexity in the number n of variables that make up the map and trajectory. we can also restrict the calculation to a subset of variables by stopping the back-substitution process when all variables of interest are obtained. this constant time operation is typically sufficient unless loops are closed.
1 givens rotations
one standard way to obtain the qr factorization of the measurement jacobian a is by using givens rotations  golub and loan  1  to clean out all entries below the diagonal  one at a time. as we will see later  this approach readily extends to factorization updates  as will be needed to incorporate new measurements. the process starts from the bottom left-most non-zero entry  and proceeds either column- or row-wise  by applying the givens rotation:
		 1 
to rows i and k  with i   k. the parameter 뷋 is chosen so that the  i k  entry of a becomes 1  as shown in figure 1. after all entries below the diagonal are zeroed out  the upper triangular entries will contain the r factor. note that a sparse measurement jacobian a will result in a sparse r factor. however  the orthogonal rotation matrix q is typically dense  which is why this matrix is never explicitly stored or even formed in practice. instead  it is sufficient to update the rhs b with the same rotations that are applied to a.
모when a new measurement arrives  it is cheaper to update the current factorization than to factorize the complete measurement jacobian a. adding a new measurement row wt and rhs 붺 into the current factor r and rhs d yields a new system that is not in the correct factorized form:
   new rhs: 
note that this is the same system that is obtained by applying givens rotations to eliminate all entries below the diagonal  except for the last  new  row. therefore givens rotations can be determined that zero out this new row  yielding the updated factor r. in the same way as for the full factorization  we simultaneously update the right hand side with the same rotations to obtain d. in general  the maximum numberof givens rotations needed to add a new measurement is n. however  as r and the new measurement rows are sparse  only a constant number of givens rotations are needed. furthermore  new measurements typically refer to recently added variables  so that often only the right most part of the new measurement row is  sparsely  populated. an example of the locality of the update process is shown in figure 1.
모it is easy to add new landmark and pose variables to the qr factorization  as we can just expand the factor r by the appropriate number of zero columns and rows  before updating with the new measurement rows. similarly  the rhs d is augmented by the same number of zero entries.
모for an exploration task in the linear case  the number of rotations needed to incorporate a set of new landmark and odometry measurements is independent of the size of the trajectory and map  as the simulation results in figure 1 show. our algorithm therefore has o 1  time complexity for exploration tasks. recovering all variables after each step requires o n  time  but is still very efficient even after 1 steps  at about 1 seconds per step. however  only the most recent variables change significantly enough to warrant recalculation  providing a good approximation in constant time.
1 loops and variable reordering
loops can be dealt with by a periodic reordering of variables. a loop is a cycle in the trajectory that brings the robot back to a previously visited location. this introduces correlations between current poses and previously observed landmarks  which themselves are connected to earlier parts of the trajectory. results based on a simulated environment with multiple loops are shown in figure 1. even though the information matrix remains sparse in the process  the incremental updating of the factor r leads to fill-in. this fill-in is local and does not affect further exploration  as is evident from the example. however  this fill-in can be avoided by variable reordering  as has been described in  dellaert  1 . the same factor r after reordering shows no signs of fill-in. however  reordering of the variables and subsequent factorization of the new measurement jacobian itself is also expensive when performed in each step. we therefore propose fast incremental updates interleaved with occasional reordering  yielding a fast algorithm as supported by the dotted blue curve in figure 1 d .

 a  simulated double 1-loop at interesting stages of loop closing  forsimplicity only a quarter of the poses and landmarks are shown .

 b  cholesky factor r.	 c  cholesky	factor	after

 d  execution time per step for different updating strategies are shown in both linear and log scale.
figure 1: for a simulated environment consisting of an 1-loop that is traversed twice  a   the upper triangular factor r shows significant fill-in  b   yielding bad performance  d  continuous red . some fill-in occurs when the first loop is closed  a . note that this has no negative consequences on the subsequent exploration along the second loop until the next loop closure occurs  b . however  the fill-in then becomes significant when the complete 1-loop is traversed for the second time  with a peak when visiting the center point of the 1-loop for the third time  c . after variable reordering  the factor matrix again is completely sparse  c . reordering after each step  d  dashed green  can be less expensive in the case of multiple loops. a considerable increase in efficiency is achieved by using fast incremental updates interleaved with periodic reordering  d  dotted blue   here every 1 steps.
모when the robot continuously observes the same landmarks  for example by remaining in one small room  this approach will eventually fail  as the information matrix itself will become completely dense. however  in this case filters will also fail due to underestimation of uncertainties that will finally converge to 1. the correct solution to deal with this scenario is to eventually switch to localization.
1 non-linear systems
our factored representation allows changing the linearization point of any variable at any time. updating the linearization point based on a new variable estimate changes the measurement jacobian. one way then to obtain a new factorization is to refactor this matrix. for the results presented here  we combine this with the periodic reordering of the variables for fill-in reduction as discussed earlier.
모however  in many situations it is not necessary to perform relinearization  steedly et al.  1 . measurements are typically fairly accurate on a local scale  so that relinearization is not needed in every step. measurements are also local and only affect a small number of variables directly  with their effects rapidly declining while propagating through the constraint graph. an exception is a situation such as a loop closing  that can affect many variables at once. in any case it is sufficient to perform selective relinearization only over variables whose estimate has changed by more than some threshold. in this case  the affected measurement rows are first removed from the current factor by qr-downdating golub and loan  1   followed by adding the relinearized measurement rows by qr-updating as described earlier.
1 results
we have applied our approach to the sydney victoria park dataset  available at http://www.acfr.usyd.edu.au/ homepages/academic/enebot/dataset.htm   a popular test dataset in the slam community. the trajectory consists of 1 frames along a trajectory of 1 kilometer length  recorded over a time frame of 1 minutes. 1 frames are left after removing all measurements where the robot is stationary. we have extracted 1 measurements of 1 landmarks from the laser data by a simple tree detector.
모the final optimized trajectory and map are shown in figure 1. for known correspondences  the full incremental reconstruction  including solving for all variables after each new frame is added  took 1s  1 minutes  to calculate on a pentium m 1 ghz laptop  which is significantly less than the 1 minutes it took to record the data. the average calculation time for the final 1 steps is 1s per step  which includes a full factorization including variable reordering  which took 1s. this shows that even after traversing a long trajectory with a significant number of loops  our algorithm still performs faster than the 1s per step needed for real-time. however  we can do even better by selective reordering based on a threshold on the running average over the number of givens rotations  and back-substitution only every 1 steps  resulting in an execution time of only 1s. note that this still provides good map and trajectory estimates after each step  due to the measurements being fairly accurate locally.

figure 1: optimized map of the full victoria park sequence. solving the complete problem after every step takes less than 1 minutes on a laptop for known correspondences. the trajectory and landmarks are shown in yellow  light   manually overlayed on an aerial image for reference. differential gps was not used in obtaining the results  but is shown in blue  dark  where available. note that in many places gps is not available.
1 covariances for data association
our algorithm is helpful for performing data association  which generally is a difficult problem due to noisy data and the resulting uncertainties in the map and trajectory estimates. first it allows to undo data association decisions  but we have not exploited this yet. second  it allows for recovering the underlying uncertainties  thereby reducing the search space.
모in order to reduce the ambiguities in matching newly observed features to already existing landmarks  it is advantageous to know the projection 붩 of the combined pose and landmark uncertainty  into the measurement space:
		 1 
where h is the jacobian of the projection process  and 붞 measurement noise. this requires knowledge of the marginal covariances
		 1 
between the current pose mi and any visible landmark xj  where 쑐j are the corresponding blocks of the full covariance matrix  = i 1. calculating this covariance matrix in order to recover all entries of interest is not an option  because it is always completely populated with n1 entries.
모our representation allows us to retrieve the exact values of interest without having to calculate the complete dense covariance matrix  as well as to efficiently obtain a conservative estimate. the exact pose uncertainty 쑐i and the covariances 쑐j can be recovered in linear time. by design  the most recent pose is always the last variable in our factor r - if variable reordering is performed  it is done before the next pose is added. therefore  the dim mi  last columns x of the full covariance matrix  rtr  1 contain 쑐i as well as all 쑐j  as observed in  eustice et al.  1 . but instead of having to keep an incremental estimate of these entries  we can retrieve the exact values efficiently from the factor r by backsubstitution. we define b as the last dim mi  unit vectors and solve rtrx = b by two back-substitutions rty = b and rx = y . the key to efficiency is that we never have to recover a full dense matrix  but due to r being upper triangular immediately obtain y =  1 ... 1 rii 1 t. hence only one back-substitution is needed to recover dim mi  dense vectors  which only requires o n  time due to the sparsity of r.
모conservative estimates for the structure uncertainties 쑑j can be obtained as proposed by  eustice et al.  1 . as the covariance can only shrink over time during the smoothing process  we can use the initial uncertainties as conservative estimates. a more tight conservativeestimate can be obtained after multiple measurements are available.
모recovering the exact structure uncertainties 쑑j is more tricky  as they are spread out along the diagonal. as the information matrix is not band-diagonal in general  this would seem to require calculating all entries of the fully dense covariance matrix  which is infeasible. here is where our factored representation  and especially the sparsity of the factor r are useful. both   golub and plemmons  1  and  triggs et al.  1  present an efficient method of recovering exactly all entries of the covariance matrix that coincide with nonzero entries in the factor r. as the upper triangular parts of the block diagonals of r are fully populated  and due to symmetry of the covariance matrix  this algorithm provides access to all blocks on the diagonal that correspond to the structure uncertainties 쑑j. the inverse z =  ata  1 is obtained based on the factor r in the standard way by noting that ataz = rtrz = i  and performing two backsubstitutions rty = i and rz = y . as the covariance matrix is typically dense  this would still require calculation of o n1  elements. however  we exploit the fact that many entries obtained during back-substitution of one column are not needed in the calculation of the next column. to be exact  only the ones are needed that correspond to non-zero entries in r  which leads to:

for l = n ... 1 and i = l   1 ... 1. note that the summations only apply to non-zero entries of single columns or rows of the sparse r matrix. the algorithm therefore has o n  time complexity for band-diagonal matrices and matrices with only a constant number of entries far from the diagonal  but can be more expensive for general sparse r.
1 conclusion
we presented a new slam algorithm that overcomes the problems inherent to filters and is suitable for real-time applications. efficiency arises from incrementally updating a factored representation of the smoothing information matrix. we have shown that at any time the exact trajectory and map can be retrieved in linear time. we have demonstrated that our algorithm is capable of solving large-scale slam problems in real-time. we have finally described how to obtain the uncertainties needed for data association  either exact or as a more efficient conservative estimate.
모an open research question at this point is if a good incremental variable ordering can be found  so that full matrix factorizations can be avoided. for very large-scale environments it seems likely that the complexity can be bounded by approaches similar to submaps or multi-level representations  that have proven successful in combination with other slam algorithms. we plan to test our algorithm in a landmark-free setting  that uses constraints between poses instead  as provided by dense laser matching.
