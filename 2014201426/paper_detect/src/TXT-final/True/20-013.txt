 
the possibility is pursued that a single mental program underlies both sentence comprehension and generation. the horn-logic formalism is exploited here to modelize the mental representation of the linguistic knowledge  as a bundle of constraints rather than as a patchwork of procedures. a notion of dependency in a horn program is defined so that eliminating dependency amounts to solving the problem  of sentence comprehension or generation  represented in terms of that program. thus  formulated is a problem-solving paradigm called dependency propagation  dp : local dependency in some parts of the program invokes execution  which may cause dependency again in some neighboring parts  which in turn invokes further execution  and so on. dp subsumes both sentence comprehension and generation  because  under dp  no heuristics are necessary about when and how to use most efficiently which piece of linguistic knowledge; the major difference between the two processes is in such alleged heuristics  whereas the declarative knowledge is largely shared. another advantage of dp is that it captures not only short-term execution but also longterm transformation of programs. some light is thus shed upon the evolution or acquisition of the mental grammar and lexicon. 
1. introduction 
some systematic relationship must hold between sentence comprehension and generation by humans  as suggested by the following phenomena  among others. first  most naively  the language one speaks and that one hears have similar structures. second  there is an affinity between the process of comprehension and that of generation. for instance  we often literally guess how others' speech could continue  or detect grammatical errors and semantic inconsistencies in our own speech. third  the two processes become equally difficult in the case of  say  deep center-
embedding constructions. 
     in order to account for these phenomena  one might hypothesize some relationship between sentence comprehension and generation; i.e.  between the two  maybe the same  grammars and between the two  maybe the same  too  programs for comprehension and generation. here we adopt the strongest hypothesis: 
 1  a single mental program underlies both sentence comprehension and generation. 
1 	natural language let us call this the common program hypothesis  cph for short   and the mental program mentioned therein the common program  cp . the challenge of the current work is to figure out how cp operates  as well as to what extent cph can be supported. 
     cp must be a coherent system of instructions  and/or constraints  as it will in fact turn out  rather than a patchwork of subroutines independent of each other. as an extreme instance  cp must not consist of two modules  one for comprehension and the other for generation. more precisely  cp is defined to be the maximum domain of the mind every part of which is potentially exploited in both sentence comprehension and generation. this definition ensures the existence of cp without saying anything about its coverage  whereas cph claims that the coverage encompasses some crucial part of both sentence comprehension and generation. another point to be drawn from this definition is that cp contains grammar rules and lexical entries as long as they have chance to be exploited in both comprehension and production. 
     in the following discussion  we shall formulate cp as consisting of two components. one is a declarative representation of linguistic knowledge. this representation is modelized in terms of a logic-programming formalism  and biased in favor of neither sentence comprehension nor generation. the other component of our model of cp is an interpreter of this knowledge. it is the operation of this interpreter that is called dependency propagation. this interpreter is exempt from language-specific aspects  not to speak of comprehension-specific or generation-specific aspects. 
     this model of cp is based upon the observation that sentence comprehension and generation make access to the same linguistic knowledge  but in different ways. for example  consider the syntactic rule about english topicalization as in: mary  tom doesn't like to see. put declaratively  this rule might look like  1   details being omitted. 
 1  a sentence s may consist of any constituent x plus a following sentence s/x which lacks x somewhere. here x is semantically focused on. 
this rule is exploited in different ways between sentence comprehension and generation. in comprehension  perhaps this rule is fully activated only when the beginning of s/x is detected. at that time x would get focused on. in typical cases of generation  on the other hand  the rule could be activated by a focused semantic element  whereby this element is first put into a linguistic expression x. 
     the common linguistic knowledge such as  1  would be modeled in terms of declarative rules  constraints  or the like. the apparent difference between comprehension and generation is in the manners of access to such knowledge. in the former models of language faculty  and in application programs such as those of machine translation   this difference has been stipulated in terms of comprehension- or generation-specific heuristics about when and how to activate most efficiently which piece of linguistic knowledge. for instance  a generation-oriented heuristic rule to exploit 

 1  might state: 
 1  if a verbalization into a sentence is currently attempted  and the topical focus is upon a part of the input semantic content  then first translate the focused part into a language expression x  and next attempt to verbalize the remaining content as a sentence in which x is missing somewhere. 
the major task in our pursuit of cph is to substitute the heuristics of this sort with a general nonbiased paradigm of problem-solving to interpret the common declarative knowledge. that is  such a paradigm should control the timing of and data-flow in the exploitation of linguistic knowledge  just the same way as those heuristics do. for instance  prolog interpreter does not provide such a paradigm. in fact  the existing implementations of dcg  pereira and warren  1  cannot deal with sentence comprehension and generation equally efficiently; they must be biased  by virtue of procedure attachments  etc.  in favor of one or the other task  in order to work efficiency. dp will be proposed later as a candidate for the desired paradigm. 
     such a pursuit of cph should be qualified  however  because there are some good reasons to conclude that some of those heuristics should survive for the sake of processing efficiency  and hence that cp does not encompass the entire language processing. for example  typical cases of broca's aphasia exhibit so-called telegraphic speech  i.e.  one which lacks grammatical markers such as inflections  conjugations  prepositions  etc.   the comprehension ability remaining fairly normal. despite the apparent inconsistency  this phenomenon is compatible with cph. a consistent interpretation is that the function of retrieving words from meaning  plus syntactic features  do not belong to cp  and therefore may be lost without reducing the ability to listen. other evidences  including aphasic symptoms contrasted with telegraphic speech  suggest that cp should also exclude the function of retrieving meaning from words  thus totally excluding search in the lexicon. 
     that cp excludes lexical retrieval is predicted a priori  by taking into account the vastness of the lexicon. in the case of comprehension  a lexical entry is considered to be retrieved with its phonological form as the key. there must be some access paths which you traverse by using phonological keys to reach desired words. these paths are not likely to be exploited in generation. similarly  the access paths through which you find words from semantic keys need not be activated in comprehension. that is  the access paths of either direction must be out of cp. to retrieve grammar rules like  1   on the other hand  is quite another story. as is demonstrated in hg  polloard  1   hpsg  pollard  1   etc.  the inventory of grammar rules is regarded as very small  i.e.  complementation  adjunction  coordination  topicalization  and few more   when the lexicon is maximized. such a demarcation between grammar and lexicon renders trivial the search of grammar rules. hence the  perhaps simplifying  assumption that the access paths to grammar rules are shared between comprehension and generation would not separate the resulting model very far from the reality. 
     to summarize  our assumption is that cp subsumes the grammar rules  the access paths to them  and the lexicon  but not the access paths to the lexicon. the rest of the paper is concerned with how the information included in cp is put to use. 
1. constrained patterns 
sentence comprehension is a task to figure out semantic structures of given strings of words  and sentence generation is a task in which  contrariwise  strings of words are worked out of given semantic structures. among the currently available programming paradigms  unification seems to be most promising in order to capture this bi-directionality of dataflow in cp. 
     another reason for the employment of unification in describing the flow of linguistic information is that there have been developed several unification-based grammar formalisms  such as gpsg  gazdar  klein  pullum  and sag  1   lfg  bresnan  1   hg  hpsg  fug  kay  1   and cug  uszkoreit  1 . these theories provide a basis for a representation of linguistic knowledge shared between comprehension and generation. the reader may consider that the description of the grammar and the lexicon in our model exploits the techniques in the unification-based grammars mentioned above  unless stipulated otherwise. 
     here we introduce a scheme for representing linguistic information. this scheme exploits the horn-logic programming formalism  i.e.  that of prolog   so that information could flow back and forth via unification. ordinary patterns as in prolog in which variables are simply indeterminate  however  are problematic in that they are lacking in expressive power. to remedy this  our scheme incorporates constraints on variables appearing in patterns; thus such patterns are called constrained patterns  formerly called 
conditioned patterns in hasida  1  . 
     a constrained pattern is a pair of a pattern  of prolog  and a constraint. a constraint is a sequence of atomic formulas  again  of prolog   where all the predicates heading those atomic formulas  e.g.  p of an atomic formula 
p x  y   are defined by horn clauses; that is  every predi-
cate considered here must not be system-defined. 	for instance  
 1  a f x y  a b  	  	p x  a   	q y  b . 
is a constrained pattern with pattern a f  xy y   a  b  and constraint  p x  a y q yy b  y provided that predicates p and q are defined in terms of horn clauses. note that a constrained pattern looks just like a horn clause  except that the pattern and the constraint arc separated by    rather than :-. 
     the semantic difference between a horn clause and a constrained pattern is that the former expresses a scheme of logical inference  while the latter denotes a set of patterns. for example   1  represents the set exhibited in  1   in the case where predicates p and q are defined by  1 . 
 1  {a f    a   1  b  a f  % c   1 d   a f 1  a   1  b   
a f 1 c  1 d } 
 1  p l 1 . 	/  1 . 	q a b . 	q c d . 
when some predicate in the constraint has a recursive definition  a constrained pattern may represent a set which cannot be denoted by a finite set of patterns possibly containing variables. for instance  the constrained pattern  1  represents the set of all the lists ending with the null list  i.e.     where predicate list is defined as in  1 . 
 1  x 	t hst x . 
 1  list  . 	list  a x  :- list x . 
     a number of problems can be represented as a constrained pattern; i.e.  a constrained pattern is regarded as denoting the set  or a subset  of the solutions of a problem. the problem for cp to solve  for example  is represented by a constrained pattern such as shown in  1   where the predicate constituent is defined as in  1 . 
 1  struct  category  x y     constituent  category  x  y . 
	hasida and isizaki 	1 

in short  this is a sort of dcg. the predicates lexicon and phrase structure rule are also defined by horn clauses  and  as is indicated by their names  represent the access paths to the mental lexicon and the mental grammar. variables x  y  and z in  1  denote some portions of the terminal string  whereby the last two arguments of constituent constitute the differential list representing the part of the terminal 
string which the constituent in question exhaustively dominates  as in: 
 1  constituent  sentence   tom  loves  mary   x   x  
1. dependency propagation 
this section is devoted to a formulation of a problem-soling paradigm under the representation scheme just illustrated above. in this paradigm  the logical structure of the problem to be solved determines the ordering of execution to process the information represented as constraints. in contrast  the existing prolog interpreters carry out execution simply according to the ordering in which atomic formulas happen to appear in source programs. 
1. dependency versus modularity 
let us say that there is a dependency between two atomic formulas sharing some variable  in the sense that the instantiation of the shared variable can be licensed only through some possibly nontrivial interaction between the two formulas. for instance   are dependent on 
each other; there does not necessarily exist a pattern y satisfying  for every a and b satisfying p a  p . an atomic formula some of whose arguments is not a variable involves dependency as well; i.e.  the dependency between that argument and the predicate of the atomic formula. for example  there is a dependency in  in the sense that there does not necessarily exist a pattern p such that a for every a satisfying p  a . 
     let us say that a constraint is modular when it contains no dependency at all in such a sense. in order to put it more formal  let us define the notion of superficial modularity. a superficially modular constraint is one in which all the arguments of all the atomic formula are variables and no variable occurs twice. a constraint is modular iff all the relevant constraints are superficially modular; all the relevant constraints being defined to be the constraint itself  the bodies of the horn clauses defining the predicates in the constraint  note that the body of a horn clause is looked upon as a constraint   the bodies of the horn clauses defining the predicates in those bodies  and so on. a constrained pattern is said to be modular when its constraint is modular. a predicate is modular when it is defined in terms of only horn clauses whose bodies are modular constraints. for example   1  and  1  are modular constrained patterns  when the predicates exploited there are modular  for instance being defined by  1  and  1   respectively. 
1. a view of problem solving 
as demonstrated above  a constrained pattern represents a solution set of a problem. as a matter of course  however  to represent does not necessarily imply to solve. granted that the solution should also be represented in terms of a 
constrained pattern  that constrained pattern would be of 
1 	natural lanquaqe 
what might be called a 'resolved form.' 
     a modular constrained pattern is looked upon as 'resolved.' that is  it is an almost extensional enumeration of patterns  as typically seen in  1   thus conforming to the intuition that  in general  a resolved form should enable you to enumerate the solutions in time proportional to the sum of their complexity. consequently  a problem represented as a constrained pattern is resolved by modularizing that constrained pattern; i.e.  transforming it into a modular equivalent. the modularization is regarded as driven by dependency: dependency in some parts of a constraint invokes execution which in turn gives rise to dependency in the neighboring parts of the constraint  which invokes further execution  and so on until dependency disappears. we refer to this problem-solving paradigm as dependency propagation  dp . 
for example  if a problem is represented by  1   then 
 1  is a resolved representation  where p is an arbitrary unary modular predicate and member and mp are defined as in  1 . 

a constraint is modularized by means of an algorithm similar to fold/unfold program transformation. in the above example  the dependency  i.e.  the double occurrence of ¡ê  in the constraint-part of  1  invokes unfolding of member  e  s   and then folding the resulting constraint into mp e  s . for further details of the algorithm  see hasida  1  or hasida and sirai  1 . 
     note two aspects of dp here. first  dp is optimized in the sense that it responds only to 'pressing needs' represented as dependency  thus minimizing the waste of processing. second  according to dp  the procedure to interpret a given representation of a problem reflects more of the logical structure of that problem rather than artifacts in the representation. compare this with prolog interpreters  which tend to indulge in investigation of top-down hypotheses which happened to be activated on the way of the execution ordering predetermined by the ordering in the source program. some proposals such as freeze  colmerauer  1  are made to overcome this defect of prolog  but they are partial solutions unlike dp. 
     the current problem  represented by  1  together with  1   may generally be solved by modularizing  1  after instantiating the variables category  x and y according to the given information. put more precise  in the case of parsing  category is first left indeterminate  x is set to be the given sentence  such as  torn  loves  mary . in the case of sentence generation  the semantic part of category is first instantiated to be the given semantic content  e.g.  love  torn  mary   of the target sentence  x being left uninstantiated. y is initially set to nil  i.e.       in both cases. 
1. compilation as partial computation 
one might expect that  1  could be modularized in advance  so that the amount of computation is greatly saved in individual cases of comprehension and generation; a sort of precompilation in terms of partial computation. this expectation fails  however  in almost every nontrivial case. in fact   1  has a modular equivalent only when the language in question is regular  detailed mathematical account being omitted. 

     nevertheless  one can consider instead an equivalent constrained pattern which is semi-modular: modular except that a variable may occur more than once in a constraint iff the instantiation of that variable is influenced at most one occurrence. a variable representing a part of the terminal string is a typical example of such a variable; it appears first as the latter component of a differential list  and second as the first component of another differential list  as y does in  1 . such a semi-modular description of a language is available through precompilation in great many significant cases; i.e.  for the class of languages including all the context-free languages  such repetition languages as 
 n is an arbitrary natural number  and i an 
arbitrary finite set of alphabets   etc. 
     for instance  when the language in question is determined by  1   i.e.  the grammar given in  1    a semimodular equivalent co of constituent  is obtained simply as in  1 . 

note here that the bodies of the two horn clauses in  1  are modular except that y appears twice in the second. 
1. a cognitive model 
parsing and generation by modularizing  1  or  1  in a single stroke  however  fails to fit the reality with respect to the following two aspects. first  humans process sentences from left to right  not necessarily having in mind the global view of what the sentence eventually turns out to be. second  presumably on account of the limitation on stm capacity  humans do not pay attention to every possible solution; otherwise such phenomena as garden-path sentences and resumptive pronouns would not take any place. 
1. left-to-right processing 
sentences may be formulated as processed  i.e.  comprehended or generated  little by little from left to right  so that the first aspect is incorporated in the model. to illustrate this  a sentence tom loves mary is parsed as follows. 

in general  parsing proceeds by successively semimodularizing the constraint  c category   at{  x   y   to that is  at first the constraint   v wi m rm modularized to yield a constraint on variables a1  x  and y. the variables contained in a is concealed in the definition of under this formulation of sentence generation  the whole constraint on the entire sentence should at the very beginning be fit with the semantic structure embedded in a. this result should be rejected  since it is not always possible to have the complete meaning of the entire sentence before generation begins. a further investigation presented later will overcome this weakness. 
1. memory limitation 
in order to capture the limitation on the stm capacity  let us assume that there is a finite bound on the amount of working memory in which to store the horn clauses produced dynamically during comprehension and generation. following this assumption  horn clauses whose activation intensity is weaker than others are pruned off so that the remaining horn clauses should fall together within the limited storage. 
     the activation intensity of a horn clause is mentioned here as a neurophysiological metaphor. a horn clause activates or deactivates some other horn clauses. the strength of this  de activation is positively correlated with the activation intensity of that horn clause. horn clauses competing with each other should be inhibitory against each other. for instance  different clauses defining the same predicate should tend to deactivate one another. 
     this paper does not go any further into the question of what activation intensity should be like. a fuller account would require something like the connectionist approach  waltz and pollack  1 . in the current computer implementation  the intensity is simulated by an integer assigned to a horn clause. 
     it must be emphasized that the memory limitation is crucial in our present approach to cph. for instance  the fluency of generation follows from the memory limitation. in  1   the constraint on a;'s instantiation can be retained 
for only a few j such that  due to the memory limitation. hence the determined value of a; should have been emitted as a part of utterance except for j very near to i. incidentally  therefore  in practice we come up with 
 for some k close to i  instead of 
1. the head-driven propagation 
the currently assumed dp  i.e.  the exhaustive semimodularization in precompilation and execution  eliminates every non-vacuous dependency. this is problematic  however  for several reasons that follow. first   semi-  modularization as currently conceived tend to consume too much memory. for example  the amount of memory occupied by the constraint of any modular equivalent of the constrained pattern  1  is o  m xn   where m and n are prime to each other  and listm and listn are defined as in  1 . 
place when two constrained patterns are unified with each 
is 
hasida and lslzaki 1 
ful unification between g z  and  is attempted next   is simply discarded without being exploited at all. 
     third  there are many important predicates which have no semi-modular equivalents. mathematical details being omitted  those predicates include permutation  a binary predicate to the effect that the two arguments are lists which are permutations of each other   subset  a binary predicate to the effect that the two arguments are lists and that the former represents a subset of the latter   etc. 
     to avoid these problems  let revise dp by relaxing the present requirement that dp semi-modularizes given constraints. to overcome the defects discussed above  dp should instead operate selectively on the parts of the given constraint which are likely to be referenced more often than others. 
     a typical bias of reference likelihood is found between the head and nonhead daughters in a local tree. consider a local tree shown below  for example. 
 1  	
	nonheaddaughter 	headdaughter 
this local tree may be regarded as an instantiation of a rule such as s np vp and np  det n  in a familiar notation. when one of mother and headdaughter is referenced  then it is nearly certain that the other is also referenced  because these two nodes share almost the same information. mother and headdaughter  than to dependency that nonheaddaughter has with mother or headdaughter. let us tentatively formulate this as the following extremely simplified form. 
 1  in both sentence comprehension and generation  dp operates so that: 
a. the constraint on every node is modular which dominates an exhaustively processed part of the terminal string  and 
b. there is no remaining dependency between any node and its grammatical head. 
let us call this the head-driven propagation  hdp  for short . 
   due to  1a   hdp roughly amounts to a parallel execution of what hasida  1  calls the canonical procedure. the canonical procedure is a nondeterministic procedure to handle a coherent substructure of a sentence at a time. in general  the partial structure regarded as resolved  i.e.  modularized  in our current terminology  at any stage during the execution of the canonical procedure looks like the one enclosed in the curve of  1  below  where the categories     d a are memorized in the working storage. 
on account of  1b   the modular domain in the case of 
hdp may be a little wider than the area enclosed in the curve; that is  the former also includes the path from down to its lexical head. for example  when terminal string a dog which is just processed  a maximal coherent structure dynamically generated would look like  1   provided that vp is a head of s and s is a head of 1. 
1 	natural language 

since a node and its heads greatly share information  the substantial difference between  1  and  1  is smaller than it appears. 
     an advantage of adopting principle-based grammars such as gb  chomsky  1  1  and hpsg is that one can remove much of artifact in the grammar by controlling the precompilation. such a control is possible because principle-based grammars are of the 'least precompiled' form. in hpsg  for instance  the phrase-structure rules have been abstracted away from information in the lexicon  and also from general properties of local trees  which are factored out as general principles like the binding inheritance principle  the head feature principle  etc. if a grammar of a more precompiled form such as gpsg were adopted  the distribution of the reference likelihood in compiled representation of grammar would be less controllable. 
1. an example 
now let us look at some concrete cases of dp as presently conceived in sentence processing. since it would be too complicated as an example to precisely demonstrate how an actual natural language sentence is processed  first we examine here the language defined by  1 . several more linguistically real instances will be considered later. 
     shown below is how a sentence aaa ¡ö     of the language in question is parsed  following the procedure illustrated in  1 .  1  

     horn clauses have not been pruned off here. to see what a pruning is  let us simulate the memory limitation by a simplistic requirement that the number of horn clauses in the working memory should be no more than  say  seven  according to miller  1 . after a possible pruning  we might be left with  for instance  the horn clauses listed in  1 : 
 1  
the hom clauses defining c are not discarded dynamically  because they have been yielded by precompilation and thus are regarded as stored in the long-term memory. 
   note that every atomic formula in the bodies of the remaining horn clauses are solvable by finite patterns  based only upon the remaining horn clauses; e.g.    ~ ~    is solvable with 	this is important because a mental representation must be finite. so for instance the first horn clause defining cl must be retained; otherwise no finite pattern could solve c1 b x y . in order for pruning to take care of this  smaller intensity values should be assigned to horn clauses  e.g.  the second clause defining  taking part in recursive definitions of predicates. 
     incidentally  if a predicate is defined by only one horn clause  it is usually more efficient  with regard to both memory and time  to unfold that predicate at all its occurrences and discard it. for instance  in  1  there 
 1  
     this example of dp amounts to a parallel execution of the canonical procedure. for instance  the four horn clauses in  1  extracted from  1  represent together the tentative partial structure depicted in  1 . 
 1  

of a1t being possibly identical to it. these two nodes are related through the second clause defining c1 in  1   and the distance between them in a potential completion of  1  is equal to the number of times that this clause is exploited  i stands in the same sort of relationship with bx 
through the second clause defining .  1  amounts to a snapshot just before  1  with d ' that is  a1 pl  a1  and in  1  correspond to and b1  respectively; a in  1  has no counterpart in  1 . as exemplified here  the amount of memory occupied by horn clauses representing together the partial structure in  1  is proportional to d. 

1. accounts of some linguistic phenomena 
hdp provides a measure of transient memory load 
 tml; i.e.  the load on stm  which is finer-grained than left-branching  yngve  1   center-embedding  church  1   self-embedding  miller and chomsky  1   etc. 
     first  the complexity of a single coherent structure of a sentence is measured in terms of the memory requirement by horn clauses needed to represent a coherent substructure of that structure at a given stage of processing. that is  tml of a maximal coherent structure is estimated to be 1 d  with respect to the moment depicted in  1 . this measure amounts to a refinement of center-embedding; we have  where 1 is the depth of center-embedding. 
     moreover  the entire memory requirement at a stage of processing in hdp captures also local structural ambiguity. some authors have attempted to measure local ambiguity by means of the degree of lookahead  marcus  1; mcdonald  1   but the defect of such a measure is that it is not by itself sensitive to static complexity of sentences. the present framework lays a basis for talking about static complexity and local ambiguity at the same time. 
     hdp seems to approximate the reference likelihood of the actual mental representation of grammar. let us consider two examples. 
     first  hdp is compatible with the observation that humans can predict' the relationship between a category and its left-corner. in english  for instance  a sentence often begins with a determiner  in constructions like  1 . 
 1  	
hdp is enough to ' predict' the relationship between s and 
det  because  the moment det is processed  the information that vp takes np and np in turn takes det as complements  or maybe as specifiers   is exploited. it is also enough to deal with agreements of gender  number  etc. for example  the information about the agreement between the head n and det  and ap  in  1  is incorporated in precompilation. 
     second  hdp does not modularize too much. consider resumptive pronouns  for instance. a typical context where a resumptive pronoun is found is something like  1 . 
 1  the man who i wonder whether he is wise hasida and lsizakl 1 

here  he is a resumptive pronoun coreferring with the man. hdp accounts for why such an apparently ungrammatical utterance is generated. let us take a look at  1   the structure of  1 . 

the man 	who 	i 	wonder 	whether 	he 	is wise 
hdp leaves unresolved the relationship between s1 and np1 and the one between s1 and np1. therefore  it is impossible to detect the grammatical inconsistency until generation of sj is attempted. 
     mcdonald  1  proposes a different explanation of the same phenomenon  based on a deterministic model of sentence generation: a generation version of marcus'  1  parser. mcdonald draws upon the limited lookahead presupposed in the determinism doctrine. his discussion is wrong or at best incomplete  because he fails to pay attention to how far the coming structure can be predicted via hdp rather than lookahead. 
1. final remarks 
a model has been proposed which describes both sentence comprehension and production as a single program. this program is a system of constraint rather than a sequence of instructions  and the procedure cp to interpret given information is derived from the computational structure of the task to be performed. this model exploited a problemsolving paradigm called dependency propagation: dependency in the given constraint invokes execution  which proceeds so that the likelihood of reference becomes homogeneous over the whole constraint. 
     if we note that the likelihood of reference is related with the density of some sort of information  some part of the discussion suggests a general principle that linguistic information should be homogeneously distributed over the entire representation of the mental grammar: a principle perhaps stemming out of the more ubiquitous principle that evolution optimizes the resulting system. in the light of this  dependency propagation lays a promising basis upon which to talk about how cerebral coding of knowledge is reformed as new information comes in; more specifically  how an accumulation of concrete instances gives rise to abstract rules of the mental grammar. 
1 	natural language 
