 
     observable evidence from disparate sources are combined coherently and consistently through a hierarchically structured knowledge tree. prior knowledge of spatial interactions is modeled with markov random fields. a posteriori probabilities of segmentations are maintained incrementally. this paper is a shortened version of  chou and brown 1   which contains many more references.* 
1. a probabilistic view of the segmentation problem 
   represent an image as a set of primitive elements s = . .  a segmentation w of the image with respect to a label set is a mapping from s to l. let represent the label attached to a  in segmentation 
a. let 1 be the set of all segmentations. the image segmentation problem with respect to l can be loosely described as to find the w  that  best fits  the information collected subject to the limitation of the computational resources. this section describes the uses of probability as the representation for various kinds of information and the corresponding criteria for finding the  best fit  
     it is frequently desirable to organise segmentation labels as a hierarchical tree  figure 1 . each internal node in a tree of labels represents the disjunction of its sons. each cross-section is a mutually exclusive and exhaustive label set; i.e.  a segmentation problem can be denned with respect to a cross-section in a label tree. using such a tree  we can represent a particular piece of knowledge about the labels at whatever level of abstraction that is appropriate. we use l to denote a set of mutually exclusive and exhaustive set of labels in a label tree h. 
¡¡¡¡ this work was supported by the air force systems command  rome air development center  griffiss air force base  new york 1  and the air force office of scientific 
research  bolting afb  dc 1 under contract no. f1-c-1. this contract supports the northeast artificial intel* ligence consortium  naic . this work was also supported by the u.1. army engineering topographic laboratories under contract no. daca1-c-1. 
     a visual module could provide opinions on any mutually exclusive set of labels in a hierarchical knowledge tree. we develop an evidence aggregation method that combines consistently and coherently the opinions of the visual modules on a label tree. this method  based on the reasoning proposed in  pearl 1   follows the bayesian formalism. it requires only trivial computations. with this method  we are able to design individual experts for a subset of labels of the tree without having to know about the rest of the world. the combined opinion can thus be fused with the image knowledge represented by the a priori probabilistic distributions. 
1. global prior knowledge 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡be a set of random variables indexed by s  with for all s. a segmentation can be considered a realization  or a configuration  of this random field and  can be considered the configuration space of x. ideally the prior knowledge about can be represented by a probability distribution over in practice this distribution is either unobtainable or unmanageable due to the immense size  of the sample space. in many image understanding applications  however  some restricted classes of distributions can model the image adequately due to the local behavior of the image phenomena. in this paper  we will exclusively use markov random fields  mrfs  as the a priori models for  but the work illustrated here can be extended to other image models as well. section 1 will discuss the mrf model in detail. 
1. local visual observations 
     in our treatment  the opinions of independent vision modules are expressed as likelihood ratios. for example  module a is an expert on the label set'  after observing 
1  the module reports one likelihood ratio for each label in la. a likelihood ratio is the probability of the observation given that one label truly applies divided by the probability of the observation should none of the labels in  apply. for example  the likelihood ratio reported for label /  is: 
 1  
the methods for designing such modules are well known. interested readers can consult  bolles 1  and  sher 1 . 
     for a purpose that will soon become clear  we impose the following assumption of conditional independence between spatially distinct observations: 
 1  
where the superscript a indicates the observations of the module a. this assumption has been used implicitly in numerous applications and is valid whenever the noise processes are spatially independent  derin and cole 1  marroquin et al 1 . 
	chou and brown 	1 
1. posterior probability and bayesian estimation 
     following the bayesian formalism  the goodness of a segmentation can be evaluated in terms of its a posteriori expected 
loss  

where the p f 1  denotes the posterior probability of f given the observation 1. bayes' rule can then be used to derive the a posteriori probability 
 1  
from  1   observe that scaling all by a constant factor for fixed s does not change the posterior distribution in  1 . this fact allows us to combine the likelihoods without having to normalize the results 
¡¡¡¡¡¡the choice for the loss function depends on the characteristics of a particular application in  geman and geman 1   the maximum a posteriori  map  estimation is used. a simulated annealing procedure with a stochastic sampler  gibbs sampler  carries out the computation. in  marroquin et al 1   the maximixer of the posterior marginals  mpm  estimation is proposed. this approach  computing the a posteriori probabilities for the segmentations given the set of opinions from the early modules and the a priori probability distribution  can support both the map and mpm estimation methods as well as other bayesian estimations. 
1. markov random fields 
     markov random fields have been used for image modeling in many applications for the past few years  geman and geman 1   marroquin et al 1   derin and cole 1 . one of the most successful applications of mrfs is to model the spatial interactions of image features. in this section  we review the properties of mrfs and describe how to encode prior knowledge in this formalism. 
1. definition 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡be a set of random variables indexed by s and e a set of unordered 1-tuple i 	i representing the connections between the elements in s. the set e defines a neighborhood system n - where n$ is the neighborhood of 
s in the sense that 

let 
set of all possible configurations. we say x is a markov random field with respect to n and p  where p is a probability function  if and only if 
 1  
 1  
¡¡¡¡¡¡the conditional probabilities in the right-hand side of  1  are called the local characteristics that characterize the random field. an intuitive interpretation of  1  is that the contextual information provided by s - s to a is the same as the information provided by the neighbors of s. thus the effects of members of the field upon each other is limited to local interaction as defined by the neighborhood. a very desirable property of mrfs that makes them attractive to scientists in many disciplines is the mrf-gibbs equivalence described in the following theorem. 
1. mrf-gibbs equivalence 
hammersley-clifford theorem: a random field x is an 
mrf with respect to the neighborhood system n if and only if there exists a function v such that 
c is the set of totally connected subgraphs  cliques  with respect to n. z is a normalizing constant  so that the probabilities of all realizations sum to one. 
¡¡¡¡several terminologies from physics can provide intuition about the gibbs measure - the right-hand side of  1 . t is the temperature of the field that controls the flatness of the distribution of the configurations. a potential v is a way to assign a number to every subconfiguration  of a configuration where the sum of the local potentials  is the energy of the configuration . a system is in thermal equilibrium when the probabilities of its configurations follows the gibbs measure. 
1. encoding prior knowledge 
     for the image segmentation problem  we must choose an appropriate neighborhood system and a potential function for the random field x over the image s to represent prior knowledge about the image. the neighborhoods should be large enough to capture the interactions between the primitive elements but still small enough for a machine to carry out the computations required to make an estimation. the higher the energy measure of a configuration  the less likely it is to occur. 
1. combining opinions of early visual modules 
¡¡¡¡¡¡most research on evidence combination has focused on updating the  belief in a given hypothesis about an individual element when a piece of new evidence becomes available  pearl 1 . this approach  however  is not suitable for our purpose. we believe that an information fusion mechanism should constantly maintain a representation of knowledge to reflect the total information available  except possibly for transient periods of time for aggregating evidence locally. maintaining  marginal belief requires the effects of updating local  belief to be spatially propagated  thus violating such a requirement. 
¡¡¡¡¡¡in this section  we limit our attention to an individual element s of s. we show how the opinions about s can be combined and provide the probabilistic justification for the proposed method. in section 1 we show how the updating of this joint probability distribution given a new set of opinions about a set of primitive elements can be carried out with simple operations. 
1. representations and combination rules 
     as in pearl's construction  pearl 1   we assume the segmentation labels can be organized as a hierarchical tree h  e.g. figure 1 . node / denotes the hypothesis that the corresponding primitive element is of label /   the numbers maintained in our method indicate the degrees of hypothesis confirmation or disconfirmation provided by the collected evidence. 
¡¡¡¡¡¡let  denote the current degree of confirmation/ disconfirmation for node /. the probabilistic interpretations for the a's will be given in section 1. initially  a/ is set to unity for every / indicating  neither confirmed nor disconfirmed . besides a  each internal node / keeps one value  w1 for each son 

1 	perception 

i. initially  wl is set to the a priori probability of i given /. obviously  the  of each node sum up to unity initially. 

¡¡¡¡¡¡suppose a module a reports its opinion as a set of likelihood ratios  is a set of mutually exclusive labels contained in h as described in section 1. the corresponding a's are updated according to the rule: 
		 1  
to maintain the coherence of the a's  the effect of this opinion has to be propagated throughout the label tree by the following process: 
 1  every node  sends a message   to its father and each of its sons. 
 1  any node k that receives a message m from its father  passes m to all its sons and replaces   that is  
		 1  
 1  any node j that receives a message m from one of its sons 
 say i   updates w{ by mwi  i.e.  
		 1  
and sends a message m' to its father  where 
 1  
according to 
 1a  
 1b  
where the summation in  1  is taken over all the sons of 
j-
     the combination and propagation procedures are commutative and associative  so their order is irrelevant. 
1. probabilistic justification 
     the above method fits in the bayesian formalism if we maintain two notions of conditional independence. first  evidence 1a that bears directly on a label / says nothing about the descendants of/: 
		 1  
second  the observations of different modules are conditionally independent. 
		 1  
where the product on the right-hand side is over a set of modules and 1 is the union of their observation 1a's 
¡¡¡¡as suggested by pearl   1  states that when the observation is a unique property of /  common to all its descendants  once we know / is true/false  the identity of  does not make   more or less likely.  1   implicitly used in pearl's scheme  states that each piece of evidence observed by the early modules provides independent information about a label. we believe that the disparate types of image clues in vision applications satisfy this assumption. 
¡¡¡¡¡¡we define consistent states of a's as the states in which for each available opinion  all of the a's are either updated according to rules  1  -  1   or none of the a's have been changed with respect to this opinion. we say that a set of opinions derives a consistent state if all opinions in this set  and no other opinions  have been used to update the a's. the following theorem relates the a's to the likelihood probabilities at consistent states. 
¡¡¡¡theorem 1  chou and brown 1 : let a  denote the a value for / at the consistent state t  and be the probability of  given the label where denotes the union of those observations that form the set of opinions that derives the state 
	 1  
where c  is a constant depending only on t  given  1  and  1 . applying theorem 1 and bayes' rule  we have: 
¡¡¡¡¡¡corollary 1: let a/ denote the a value for / at the consistent state  is the prior  of a set of mutually exclusive and exhaustive labels l  then the posterior probability at the consistent state t is 
     to summarize: we have developed an evidence combination method for a hierarchy of hypotheses based on the notions of conditional independence given by  1  and  1 . this scheme  besides having all the characteristics listed in  pearl  1   has the following advantages: 
 1  the computations involved are extremely simple. simpler and fewer messages must be passed. normalizations are never needed since relative degrees of 
confirmation/disconfirmation are maintained instead of probabilities  theorem 1 . 
 1  this scheme decouples the notion of evidence and a priori belief. in the next section we show this characteristic is very helpful when the prior knowledge is represented as an mrf. 
1. combining prior knowledge with observations 
¡¡¡¡¡¡in this section  we move our attention to the relationships of segments of the image s. recall that in the last section  each primitive element is associated with a set of a's to maintain the opinions of the early visual modules. let  denote the set of a's associated with s€s  and  be the a value for label / in define a global consistent state to be a state of the b's at which each is in a consistent state. 
     assume that the prior knowledge about the image is represented as an mrf x over s  x el - a mutually exclusive and exhaustive label set in h  with respect to a neighborhood system n.  1   theorem 1  bayes rule  and the hammersleyclifford theorem lead to the conclusion that the a posteriori 
gibbs measure of a configuration to at a global consistent state t 
	 1  
     only simple local operations are needed to update the energy measure and local characteristics as new opinions from the early visual modules become available. therefore  map and mpm estimations can easily be implemented in the proposed framework  section 1 . we believe that based on this property  novel estimation algorithms can ultimately be designed that incrementally improve their estimations as more and more information arrives. for now  the existing bayesian estimation methods can be invoked at any global consistent state to provide the up-to-date estimations. 
1. experimental results 
     we demonstrate the method using two images of overlapping rectangular patches  fig 1a  1a . each patch in the first image corresponds to a geometrically identical patch in the second. the intensities of the patches in each image are randomly selected from the range  1  1   with no intensity correlation between images. gaussian zero mean noise is added  with standard deviation 1 and 1 respectively. these two images 
	chou and brown 	1 
can be considered as two different sources of information about the same set of rectangular objects. 
     a set of likelihood edge detectors  an early version of the detectors described in  sher 1   provides a set of likelihood 
ratios for each pixel given the  win-
dow of intensities centered at it  where ne denotes the hypothesis that the given pixel is not an edge element  and e  denotes the hypothesis that the given pixel is an edge of one of the four  horisontal  vertical  and two diagonal  orientations. 
the likelihoods are computed using a model for step edges and a 
gaussian model for additive noise. figure and 1b show the 
maximum likelihood estimation  mle  for edges in figure 1.a 
and 1.a respectively. that is  a pixel s is on if and only if max 	b 
t 
     we use a homogeneous and isotropic mrf with a third order neighborhood system over the image lattice to encode a body of basic knowledge about edges. cliques of size 1 are used to discourage parallel and competing edges  whereas cliques of size 1 are used to encourage line continuations  region homogeneity and to discourage breaks in the line forming process. the potential assignments for the cliques are chosen conservatively in the sense that estimation methods based on  1  make as few false detections of edges as possible while maintaining reasonable detectability. 
     a software package has been implemented to study the behavior of various estimation criteria and schemes  chou and raman 1 . here we show the results of using this package to perform mpm estimations based on the monte carlo procedure proposed in  marroquin et al 1 . figure 1c and 1.c show the mpm estimations based on the statistics collected over 1 iterations. considering figures 1.a and 1.a to provide only partial evidence to support the ne and e 's hypotheses  figure 1.a and 1.b show the mle and the mpm estimations resulting from 
applying the upward propagation rule  1h1 . here the initial 
     alternatively we can consider that figure 1.a and 1a independently support the same set of hypotheses. by applying rule  1   we obtain figure 1.a and 1b representing the mle and the mpm estimations based on the combined information. observe the lines detected in the lower left quadrant of figure 
1b that do not show up in either of figure 1c and 1.c  and the false detections in figure 1.c and 1c that are removed in figure 1b. these sorts of results can not be achieved by multi-modal segmenters that rely on boolean operations to combine evidence. 
1. future research 
     we are currently improving the mrf model to handle curved lines. one of our ultimate goals is to encode all kinds of geometrical and photometrical constraints in terms of local clique potentials in an mrf that has general connectivity. the stochastic estimation methods are computationally very expensive. we are now designing a deterministic estimation algorithm that incremently improves its estimation as new evidence arrives. we believe this method of information fusion can be applied to problems other than image segmentation as well. 
acknowledgements 
     we would like to thank dave sher for providing his likelihood edge detectors  rajeev raman for designing and implementing software for the experiments  and henry kyburg for providing many valuable suggestions. 
1 	perception 
