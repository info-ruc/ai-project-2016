 
understanding three simultaneous speeches is proposed as a challenge problem to foster artificial intelligence  speech and sound understanding or recognition  and computational auditory scene analysis research. automatic speech recognition under noisy environments is attacked by speech enhancement techniques such as noise reduction and speaker adaptation. however  the signal-to-noise ratio of speech in two simultaneous speeches is too poor to apply these techniques. therefore  novel techniques need to be developed. one candidate is to use speech stream segregation as a front-end of automatic speech recognition systems. preliminary experiments on understanding two simultaneous speeches show that the proposed challenge problem will be feasible with speech stream segregation. the detailed plan of the research on and benchmark sounds for the proposed challenge problem is also presented. 
1 	introduction 
¡¡recently emerges a new research on understanding arbitrary sound mixtures including non-speech sounds and music. their understanding represents a challenging and little-studied area of artificial intelligence  automatic speech recognition/understanding  and signal processing. this interdisciplinary research area is called computational auditory scene analysis  hereafter  cas a . 
¡¡at a crowded party  one can attend one conversation and then switch to another one. this phenomenon is known as the cocktail party effect  cherry  1 . as seen in the cocktail-party effect  humans have the ability to selectively attend to sound from a particular source  even when it is mixed with other sounds. current automatic speech recognition systems can understand clean speech well in relatively noiseless laboratory environments  but break down in more realistic  noisier environments. 
¡¡computers also need to be able to decide which parts of a mixed acoustic signal are relevant to a particular 
1 	ai challenges 
purpose - which part should be interpreted as speech  for example  and which should be interpreted as a door closing  an air conditioner humming  or another person interrupting. casa focuses on the computer modeling and implementation for the understanding of acoustic events 
¡¡the research topics concerning casa include modeling  signal processing  sound representational  control and system architecture  and applications as well as sensor integration. some of these topics were discussed at the 1jcai-1 workshop on computational auditory scene analysis  rosenthal and okuno  1 . 
¡¡at the aaa1  the panel entitled  challenge problems for artificial intelligence   brooks proposed two problems concerning sounds  selman et a/.  1 : 
  challenge 1: speech understanding systems that are based on different principles other than hidden markov models. 
  challenge 1: noise understanding systems. 
¡¡although casa shares the above interests  its ultimate goals go further; understanding general acoustic signals such as voiced speech  music and/or other sounds from real-world environments. 
¡¡we propose the problem of understanding three simultaneous speeches1  hereafter  the challenge  as a challenge problem for artificial intelligence  in particular  for casa. a computer capable of listening to several things simultaneously is called prince shotoku computer after the japanese legendary that prince shotoku  a.d. 1  could listen to ten people's petitions simultaneously  okuno et a/.  1 . since psychoacoustic studies have recently showed that humans cannot listen to more than two things simultaneously  kashino and hirahara  1   casa research would make computer 
¡¡¡¡1 the selection of the word  simultaneous  or  concurrent  is controversial. the former carries more physical senses  while the latter carries more mental senses; e.g.   separation of simultaneous talkers    simultaneous voices separation   and  separation of concurrent sentences  make sense. we adopt  simultaneous  because the proposed challenge problem won't pursuit understanding what each speaker talks about. understanding what a speaker says without speech recognition  for example  is beyond our problem. 

audition more powerful than human audition  similar to the relationship of an airplane's ability to that of a bird. 
¡¡the rest of this paper is organized as follows: section 1 explains the research issues for the challenge  in particular  its relevance and significance to ai. section 1 presents its feasibility by showing the result of preliminary experiments on understanding two simultaneous speeches with/without interfering sounds. section 1 discusses the detailed plan of our challenge problem. concluding remarks are given in section 1. 
1 research issues for understanding three simultaneous speeches 
¡¡in this section  we explain the reasons why we focus on three simultaneous speeches  not two simultaneous speeches and how significant the challenge is to ai researches. we also discuss several research issues involved in realizing understanding three simultaneous speeches. the main research areas related to the challenge are automatic speech recognition  asr   signal processing  speech understanding  computational auditory scene analysis  and psychoacoustics. 
1 	automatic speech recognition  asr  
¡¡at present  one of the hottest topics of asr research is how to make asr systems more robust so that they can perform well outside laboratory conditions  hansen et a/.  1 . conventional approaches for robust asr are speech enhancement and many techniques for speech enhancement such as noise reduction and speaker adaptation have been developed  hansen et a/.  1; minami and furui  1 . 
¡¡one possible approach is to enhance a speech by employing noise reduction techniques. once a speech is enhanced  it can be subtracted from a mixture of sounds in waveform. by repeating this procedure to the residue  remaining sounds   it seems possible to extract most speeches from a mixture of sounds. 
¡¡this approach  however  works only up to two simultaneous speeches. the reason is as follows; most conventional noise reduction techniques assume that the signal-to-noise ratio  snr  of speech is 1 db or better. the snr of speech in a mixture of two simultaneous speeches is approximately 1 db and thus noise reduction techniques can be applied to two simultaneous speeches. however  new techniques need to be developed for understanding three simultaneous speeches. 
1 	signal processing 
¡¡speech separation is more aggressive approach than noise reduction. adaptive filters are used for speech separation  ramalingam  1 . spatial information on the sound source plays an important role in separating a speech from a mixture of sounds. this mechanism is called localization  which is performed by using a dummy head microphone  called binaural sounds   blauert  1; bodden  1  or by using microphone arrays  hansen et al.  1; stadler and rabinowitz  1 . 
for a pair of microphones  localization can be obtained better from binaural sounds than from stereo sounds. 
¡¡adaptive window technique for localizing two simultaneous voices by using two microphones is also developed for speech enhancement in real-time  banks  1 . procedures for enhancing the intelligibility of a target speaker  talker  in the presence of a simultaneous talker is developed by using harmonic selection and cepstral filtering  stubbs and summerfield  1 . classification tasks within an automated two-speech separation system are performed by neural net  roger et a/.  1 . 
¡¡most of these systems can separate a speech from a mixture of two simultaneous speeches. a speech separation system is developed by using harmonic structure and directional information and can extract one speech from a mixture of more than two overlapping speeches  luo and denbigh  1 . 
¡¡since the spectrum of speech separated by speech separation techniques developed so far is distorted  they cannot be applied continuously to the remaining signals. we need to develop new techniques for the challenge. in addition  a segregated speech cannot be used as a input to automatic speech recognition systems due to spectral distortion. we also need to develop an interfacing technique between speech separation and asr. 
1 computational auditory scene analysis  casa  
¡¡speech enhancement technologies developed so far focus on only one speech and treat other speeches or sounds as noise. casa takes an opposite approach. first  it deals with the problems of handling mixture of sounds to develop methods and technologies. then it applies these to develop asr systems that work in a realworld environment. the main research topic of casa is sound stream segregation  a process that segregates sound streams that have consistent acoustic attributes from a mixture of sounds. 
¡¡in extracting acoustic attributes  some systems assume the humans auditory model of primary processing and simulate the processing of cocklear mechanism  brown  1; slaney et a/.  1 . brown and cooke designed and implemented the system that builds various auditory maps for input sounds and integrates them to segregate speech from input sounds  brown  1; brown and cooke  1 . an auditory map represents acoustic attributes such as onset  offset  am and fm modulations  and formants. since the integration process becomes complicated when treating a mixture of sounds under the real-world environments  the blackboard architecture is used to simplify this integration process  cooke et a/.  1 . 
¡¡to design a more flexible and expandable system  control mechanisms are needed. ipus  integrated processing and understanding signals   lesser et a/.  1  integrates signal processing and signal interpretation into the blackboard system. ipus has various interpretation knowledge sources which understand actual sounds such 
okuno  nakatani  & kawabata 

as hair driers  footsteps  telephone rings  fire alarms  and waterfalls  nawab  and lesser  1 . 
   nakatani et ai took a multi-agent approach to sound stream segregation which extracts individual sound stream from a mixture of sounds by agents each of which traces harmonic structure with directional information  nakatani et a/.  1 . they use the fourier transformation instead of the auditory model because the former is easy to implement and its properties are well analyzed. 
1 	psychoacoustics 
   psychoacoustic people have studied the human auditory mechanism extensively as auditory scene analysis  bregman  1   but computer modeling has not been exploited yet. emerging computational auditory scene analysis research focuses on computer modeling and has prompted interdisciplinary studies with psychoacoustic and ai and signal processing communities. in addition  our challenge problem has fostered psychoacoustic studies on how many simultaneous speeches human can listen to. kashino et al claimed that human could listen to at most two things simultaneously by performing various experiments  kashino and hirahara  1 . if this is true  the challenge will attempt to make computer audition superior to human's capability of listening. 
1 	preliminary experiments 
   in this section  we demonstrate the feasibility of the challenge by describing the preliminary experiments on understanding two simultaneous speeches  up-to-date information of our aaai-1 paper  okuno et a/.  1  . this problem is attacked by speech stream segregation  one of the main research topics of computational auditory scene analysis. the whole system consists of two components  speech stream segregation and speech recognition  as is shown in figure 1. 
   first speech streams are extracted from a mixture of speeches  and then each speech stream is recognized by conventional automatic speech recognition system. 
1 	speech stream segregation 
   human voice consists of harmonic sounds such as vowel and voiced consonants  and non-harmonic sounds such as unvoiced consonants. by assuming the structure of  vowel  v  + consonant  c  + vowel  v   of speech  speech stream segregation is realized by the following two subprocesses: 
 1  extracting and grouping harmonic stream fragments  harmonic structure extraction   and 
 1  restoring non-harmonic parts by residue  residue substitution . 
rough flow of the computation is depicted in figure 1. 
   harmonic structures are extracted from a binaural input by the bi-hbss  binaural harmonics-based stream segregation  system  nakatani et a/.  1; nakatani et a/.  1 . bi-hbss uses a harmonic structure and the 

direction of sound source as cues of segregation. bihbss adopts a pair of hbsses  nakatani et a/.  1  for the right and left channel to extract harmonic stream fragments. it determines the fundamental frequency  fo  of a harmonic stream fragment by coordinating the pair of hbsses. the direction of sound source is identified by calculating the interaural time difference  itd  and interaural intensity difference  hd  of a pair of harmonic stream fragments of the same f1 extracted by the pair of hbss. harmonic stream fragments are grouped by the direction of the sound source. 
   the residue obtained by subtracting harmonic structures from an input sound is substituted for nonharmonic parts of a group. if a group ends with nonharmonic parts  the residue is substituted for 1 msec. the idea of residue substitution is similar to the psychophysical observation known as auditory induction  green et al.  1; warren  1 . it is a phenomena that human listeners can perceptually restore a missing sound component if it is very brief and masked by appropriate sounds. 
1 	a u t o m a t i c speech recognition 
   the automatic speech recognition system  hmm-lr  kita et al  1   is used to recognize speech streams. hmm-lr is based on hidden markov model of each phonetic transition  in spite of rodney brooks' challenge problem. the parameters of hmm-lr are trained by a set of 1 words uttered by five speakers. 
   since the spectrum of speech streams segregated by the speech stream segregation is distorted due to binaural input  binauralized training data is used to recover from the degradation of the performance of recognition  okuno et a/.  1 . 
1 	performance evaluation 
   the performance of automatic speech recognition is usually measured by the cumulative accuracy up to the 1th candidate  or simply cumulative accuracy  of word recognition  since asr returns the first about 1 candidates of each word. such candidates are further selected 

by successive speech understanding systems. therefore  
1 	ai challenges 

we adopted the same measurement with open tests. by open tests  we mean that the training and benchmark  testing  data are disjoint. 
   we used three sets of 1 benchmark sounds; one set of 1 two-sound mixtures and two sets of 1 threesound mixtures  table 1 . the first sound is uttered by the first speaker at 1¡ã to the left from the center  and the second sound is uttered after 1 msec by the second speaker at 1¡ã to the right from the center  figure 1 . to recognize the first speech in the mixed sound directly by hmm-lr  the utterance of the second speaker is delayed by 1 msec. 
   the third sound with f1 of 1 hz is an intermittent harmonic sound from the center. it starts before the first speaker and repeats to last for 1 sec with 1 msec of pause. the average power ratios of the first and second sounds to the third sound in benchmarks triple and triple' are 1 db and -1 db  respectively. 
   the error rate caused by interfering sounds is defined as follows. let the cumulative accuracy of recognition of original data up to the 1th candidate be caorg  and let the cumulative accuracy of recognition of  non-binaural  mixed sounds up to the 1th candidate be camix the error rate caused by interfering sounds  ¡ê  is calculated as € = ca org can 
to evaluate the performance of speech stream segregation  error reduction rate is defined. let the cumulative accuracy of recognition up to the 1th candidate be caseg- the error reduction rate  hseg  is calculated as follows: 

   the original cumulative accuracies of word recognition uttered by single speakers  woman 1  and woman 1  are 1%  and 1%  respectively. the error rate by interfering sounds is shown in table 1. 
   error reduction rates by speech stream segregation for the three benchmark sets are shown in figure 1. the ideal shows the upper limits of error reduction  which are calculated for the case in which the utterances of a single speaker are recognized after speech stream segregation. for double  1% of errors caused by an interfering speaker were reduced by speech stream segregation. by additional noise  the snr of each speech is decreased further  by about 1 db and 1 db for triple and triple'  respectively   but  1% and 1% of errors are reduced respectively. 
   since this performance was attained without using any features specific to human voices  we believe that understanding three simultaneous speeches is a shortterm research problem. 
okuno  nakatani  & kawabata 


1 	detailed plan for the challenge 
   the research issues depend on an approach taken by a challenger. some possible approaches are listed below: 
  either speech separation system or speech stream segregation may be exploited. 
  speech separation or speech stream segregation may run either incrementally or in batch. 
  multiple speech enhancement or separation sys-tems may run concurrently to extract all speeches or one system may extract all speeches. 
  speech segregation/separation system may be ei-ther used as a front-end to asr or integrated with asr. 
  top-down or hybrid approaches needed for contin-uous speech recognition or understanding may be employed  although word recognition is requested by the challenge. 
   we only give a general guideline on the benchmarks and evaluation criteria in this paper. further information will be made available at the url of http://www.nue.org/casa1/. 
1 	b e n c h m a r k sounds 
   the common platform for the challenge is quite important in order to share and transfer the methodology and technology developed by each challenger. monaural data of speech used for the challenge should should be widely available. the current candidates are as follows: 
  the darpa timit acoustic-phonetic continuous speech corpus for english speeches. it contains a total of 1 sentences by 1 speakers uttering the same 1 sentences.  http://www.ldc.upenn.edu/  
  the continuous speech corpus developed by the acoustic society of japanese for japanese speeches. it contains a total of 1 sentences by 1 speakers uttering some of 1 sentences  which were recorded by atr. 
   since these corpus are copyrighted  only the combination of word utterances will be made available. one 
1 	ai challenges 
benchmark set contains a total of 1 combination of words; three speakers  arbitrary combination of men and women  utter a different word simultaneously. 
   the acoustic field is made simple enough to produce benchmark sounds easily. a sound source  speaker  should be placed from 1 meters to 1 meters from the microphone on the floor in a free-field without reverberation. several combinations of speaker positions selected from 1¡ã  1¡ã  1¡ã  1¡ã  1¡ã  1¡ã  1¡ã  1¡ã  and 1¡ã may be strongly recommended. the challenge does not assume that any speaker move during speaking. however  challengers may attack the problem of moving speakers. 
   a mixture of sounds may be either recorded or generated artificially. the number of microphones should be less than or equal to 1. if a challenger wants to use a binaural input  it may be generated artificially by using head-related transfer function  hrtf   which specifies the spectral transformation of a binaural sound. the data of hrtf for the kemar dummy head microphone is available from mit  gardner and martin  1 . for this hrtf  a sound source should be placed 1 meters from the dummy head microphone. in our preliminary experiments  all sound sources were placed at the distance of 1 meters from the dummy head microphone. 
1 	measurement of evaluation 
   the measurement of performance evaluation is error reduction rate as well as cumulative accuracy of up to 1th candidate  both of which are defined in the previous section. the first measurement may be important because it is rather independent for automatic speech recognition systems used. 
   the first stage of the challenge investigates the performance of word recognition. 
1 	tentative schedule 
  the challenge problem will be presented at ijcai-1 as well as ijcai-1 workshop on computational auditory scene analysis. 
  the guideline on the benchmarks will be made available by the end of 1. 
  intermediate progress reports will be presented at aaai-1 or an appropriate conference in 1. 
  final progress reports will be submitted in jan.  1 and will be presented at ijca1. 
1 	conclusions 
   in this paper  we proposed understanding three simultaneous speeches as a new challenge and standard ai problem. it provides rich research issues for a wide range of ai including automatic speech recognition  speech understanding  and casa  as well as psychoacoustics. we expect that research on the challenge would play an important role in realizing the  prince shotoku computer  or powerful computer audition systems. 
