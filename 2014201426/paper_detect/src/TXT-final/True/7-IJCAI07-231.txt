
few existing argumentation frameworks are designed to deal with probabilistic knowledge  and none are designed to represent possibilistic knowledge  making them unsuitable for many real world domains. in this paper we present a subjective logic based framework for argumentation which overcomes this limitation. reasoning about the state of a literal in this framework can be done in polynomial time. a dialogue game making use of the framework and a utility based heuristic for playing the dialogue game are also presented. we then show how these components can be applied to contract monitoring. the dialogues that emerge bear some similarity to the dialogues that occur when humans argue about contracts  and our approach is highly suited to complex  partially observable domains with fallible sensors where determining environment state cannot be done for free.
1 introduction
most research in argumentation theory has focused on interactions between arguments  prakken and vreeswijk  1 . researchers have proposed various underlying logics capable of representing different domain attributes. for example  a large body of work exists on representing and using defaults for legal argumentation  prakken and sartor  1 . a number of researchers have also looked at how argument takes place in domains with uncertainty. most of the work in this area is probability theory based  and includes bayesian based argumentation frameworks  vreeswijk  1  and pollock's oscar system  pollock  1 .
¡¡due to its basis in plausible reasoning  we believe that uncertainty theory is more applicable than probabilistic reasoning to many domains of argumentation. in this paper  we present a subjective logic  j sang  1  based approach to argumentation. using subjective logic  we are able to naturally present arguments in which uncertainty exists  and can encapsulate concepts such as accrual of arguments  prakken  1   which many existing frameworks handle poorly.
¡¡after describing the argumentation framework  we present a dialogue game which allows agents to make use of the framework. we then present a utility based heuristic that allows agents to participate in the dialogue game.
¡¡one possible application of such a framework is contract monitoring and enforcement. that is  given a contract and an environment  determining current contract state as well as what sanctions should come into effect. not only is our approach able to operate in partially observable domains with fallible sensors  but we also take into account the fact that probing a sensor might be associated with a cost. other systems for contract monitoring often require fully observable domains with  honest  sensors  e.g.  xu and jeusfeld  1  . even when able to handle faulty sensors  some approaches assume that all domain states can be probed  and that all such probing is free  daskalopulu et al.  1 .
¡¡in the next section  we provide an overview of subjective logic. after that  we present our framework  and provide an illustrative example. we conclude the paper by examining related research  and examine possible avenues of future work.
1 subjective logic
subjective logic1 is an approach for combining and assessing subjective evidence from multiple sources based on dempster-schafer theory. an opinion ¦Ø about an atomic proposition ¦Õ is represented as a triple ¦Ø ¦Õ  =  where b ¦Õ  represents the level of belief that ¦Õ holds; d ¦Õ  stands for the level of disbelief  i.e. the probability that ¦Õ does not hold   and u ¦Õ  represents the level of uncertainty of the opinion. b ¦Õ  + d ¦Õ  + u ¦Õ  = 1  and b ¦Õ  d ¦Õ  u ¦Õ  ¡Ê  1..1 .
¡¡a number of operations on opinions have been defined  allowing one to create compound opinions from atomic ones. these are listed in figure 1.
where ¦Õ is an opinion about ¦×
figure 1: the subjective logic negation  recommendation and consensus operators¡¡subjective logic allows us to use common logical operators such as conjunction  disjunction and negation. the last of these is shown in the figure. the remaining operators in the figure are unique to subjective logic. the   operator is called the recommendation operator  where ¦Ø ¦Õ    ¦Ø ¦×  yields an opinion about ¦Ø ¦×  given an opinion ¦Ø ¦Õ  about the source informing the computing agent of ¦Ø ¦× . the consensus operator ¨’  yields a combination of two opinions about the same thing. note that the consensus operator is undefined when no uncertainty in opinions exists.
1 formalism
in this section  we present three strands of work that tie up to form our framework. at the lowest level lies the argumentation framework. it is here that we define arguments  and how they interact with each other. afterwards  we present a dialogue game  as well as the agents which play the game and the heuristics they use for deciding which arguments to advance and which sensors to probe. finally  we describe how to transform contracts into a form usable by our framework.
1 the argumentation framework
many frameworks for argumentation ignore probabilistic and possibilistic concepts  focusing instead purely on the interactions  such as rebutting and undercutting attacks  between arguments. while important in their own right  applying frameworks like these to domains in which uncertain evidence exists is difficult. we thus propose a new argument framework which  while borrowing ideas from existing work  suggests a different way of thinking about arguments.
¡¡we define our language of discourse ¦² as the set of literals and their negation  while ¦²+ is the set containing only unnegated literals. a literal l has an opinion associated with it.
¡¡an argument a is a tuple  p c  where p ¡Ê 1¦² and c ¡Ê ¦². we assume that if a literal p appears in p  then  p does not appear  and vice-versa. p represents the premises of an argument  while c is the argument's conclusion. a fact a can be represented by an argument  {} a   which we will also write as   a  . let args ¦²  represent the set of all possible arguments in the language ¦².
¡¡given a set of arguments and facts  as well as opinions stating how likely those facts are to be true  we would like to determine what conclusions we may reasonably be allowed to draw. this is done by repeatedly computing the conclusions of arguments from known  opinions about  facts  until nothing more can be deduced. at this stage  we can draw conclusions based on what literals are justified within the system. a literal may be assigned a value in one of three ways. first  it could be the conclusion of an argument. second  it might be based on a fact. finally  it could be based on a combination of values from both arguments and facts.
¡¡given an argument a =  p c   we assign an opinion to it using a slightly modified form of statistical syllogism. let ¦Øpi be the opinion for premise pi ¡Ê p 1. then
 is not negated  and otherwise. this definition captures the
intuition that an argument is as strong as its weakest element.
¡¡facts are a special type of argument  as their conclusions may not be negated literals. an opinion ¦Ø f  may be associated with a fact f =   l  where l ¡Ê ¦²+.
¡¡given a set of arguments about a literal l and its negation  as well as a set of supporting facts  we may compute a final opinion regarding the value of the literal by utilising the consensus operator ¨’. slightly more formally  assuming we are given a set of arguments and facts s as well as their opinions ¦Ø s   s ¡Ê s  we may compute an opinion ¦Ø l  for a literal l as for all s =  ps cs  t =  pt ct  ¡Ê s such that cs = l and ct =  l. a special case arises when no uncertainty exists. we handle this case in an intuitively justifiable manner as follows: given opinionsand
  we compute bc = b1 +b1 d1  d1 and create the
combined opinion  and otherwise.
¡¡we have shown how to compute the value of a literal given all the applicable arguments and facts  as well as opinions on their values. to complete the description of the argumentation framework  we must show how we can determine whetheran argumentis applicable  as well as givea procedure to decide the values of all literals in an instance of a dialogue.
¡¡models of varying complexity have been proposed regarding whether an argument can be advanced  toulmin  1 . in this paper  we use a simple model stating that if the premises of an argument hold  it can be used. for a premise to hold  it should exceed some threshold strength. both computing a premise's strength  as well as setting the threshold are domain specific tasks. for example  the strength of a literal could be based on its level of uncertainty  strength of belief  or some other combination of its attributes  see  j sang  1  for more details . terms from human legal reasoning such as  beyond reasonable doubt  and  on the balance of probabilities  show us the subjective nature of the threshold. nevertheless  we assume that some function admissible ¦Ø  ¡ú  true false  exists and allows us to compute whether an opinion exceeds the required threshold.
¡¡before describing the algorithm to determine the state of literals formally  we provide an informal overview. we can represent the set of arguments under consideration as a directed graph containing two types of nodes; those representing arguments  and those representing literals. an edge connects literals to arguments in which they appear as premises  while edges also exist from arguments to those literals which appear in their conclusion. as shown in figure 1  such a graph has two types of edges  namely edges linking negative literals to positive ones  or vice-versa   and all other edges.
one weakening assumption we make  discussed further in section 1  is that the resulting graph is acyclic.
apart from the arguments  we assume that a set of facts has been put forward. our algorithm operates by repeatedly forward chaining applicable arguments from the set of facts. in other words  we compute what literals we can derive from the facts  and propagate those forward into the arguments  computing literals from the derived arguments  and repeating this procedure until nothing new can be computed. two things complicate this algorithm: accrual of arguments/evidence  and the threshold of acceptability of literals. we cannot compute an opinion regarding the value of a literal until all the arguments and facts regarding the literal have been taken into account. similarly  we cannot determine the status of an argument until the value of all its premises have been computed. as mentioned previously  if a literal's strength falls below a certain level of acceptability  the literal cannot be applied.
¡¡to deal with these issues  a node's opinion is only computed once all influencing opinions have been calculated. if a literal falls below the threshold of acceptability  we erase all the arguments in which it appears as a premise. a literal exceeding this threshold is referred to as admissible.
¡¡more formally givena set of argumentsa  and defining the non-negated set of all literals appearing in a as l  we create a directed graph of arguments ga =  a l;pe ne  with nodes a and l. for convenience  we write e = pe ¡È ne.
¡¡an edge e appears in pe  going from a ¡Ê a to l ¡Ê l if a is of the form  p l . an edge would also appear in pe  going from l ¡Ê l to a ¡Ê a if a =  p c  and l ¡Ê p. an edge will appear in ne  going from a ¡Ê a to l ¡Ê l if a is of the form  p  l . alternatively  an edge will be a member of
ne  going from l ¡Ê l to a ¡Ê a if a =  p l  and  l ¡Ê p.
in other words  an edge is in ne if it links the negated and non-negated form of a literal  and is in pe otherwise. differentiating between these types of edges allows us to compute an opinion correctly while only storing non-negated literals in our graph.
¡¡given a set of opinions on facts ¦Ø f  where f   a with the restriction that  f =  p c  ¡Ê f p = {}  a graph representing a set of arguments generated as described above  and an admissibility function mapping opinions to truth and falsehood  we can perform reasoning according to the algorithm shown in figure 1.
1 agents and the dialogue game
the argumentation framework described above allows us to statically analyse groups of arguments; letting us compute an opinion about specific literals. the process of arguing is dynamic  with agents introducing arguments at various points in the conversation. in this section  we specify what our agents should look like  what environment they argue within  and the protocol they use to argue with each other. this protocol is referred to as a dialogue game. the aim of our dialogue game is  fairness   that is  allowing an agent to utilise any arguments at its disposal to win. while many dialogue games provide restrictions on agent utterances so as to maintain the focus of a dispute  we leave this task to the heuristic layer of our framework.
¡¡an agent ¦Á  is a tuple  name kb ¦¸¦Á g c   consisting of its name  name  a private knowledge base kb   1¦² containing arguments  a private opinion base ¦¸¦Á = {¦Ø¦Á l1  ... ¦Ø¦Á ln } storing opinions about literals given a graph g =  a l;pe ne   a set of opinions ¦¸ over a =   c    a where c ¡Ê ¦² an admissibility function admissible
sourcenodes t  = {s| s t  ¡Ê e} targetnodes s  = {t| s t  ¡Ê e}
repeat
for each ¦Ø n  ¡Ê ¦¸ such that n /¡Ê v
v = v ¡È n
for each t ¡Ê targetnodes n 
if sourcenodes t    v
compute ¦Ø t  if t ¡Ê l and  admissible ¦Ø t   for each r ¡Ê targetnodes t 
for each s ¡Ê targetnodes r 
e = e   { r s }
for each s ¡Ê sourcenodes r 
e = e   { s r }
targetnodes t 
   
untilsuch that n /¡Ê v
l /¡Ê v
¦¸ = ¦¸	    = 1 1
figure 1: the algorithm for propagating opinions through the argumentation network. note that e = pe ¡È ne
l1 ... ln ¡Ê ¦²  a function mapping the state of proven and unproven literals to a utility value  and a record of how much probing actions performed to date have cost. thus  for example  g { a b} {c d}  would return theutility for the state where  a and b are proven  and nothing is known about the status of c and d.
¡¡agents operate within an environment. this environment is a tuple of the form  agents cs s admissible pc  where agents is a set of agents  and contains a record of all the utterances made by an agent  cs   1args ¦²    as well as a set of sensors s. the function admissible operates over opinions  while pc maps probing actions to costs.
¡¡a sensor is a structure  ¦¸s ¦¸p . the set ¦¸s contains an opinion for a set of literals l   ¦²+  and represents the environment's opinion about the reliability of the sensor. the opinion for a literal l ¡Ê l  is ¦Øs l  ¡Ê ¦¸s. the set of probed literals ¦¸p  stores the sensor's opinions regarding the value of the literal l  ¦Øp l  ¡Ê ¦¸p. we compute a sensor's opinion for l as ¦Øs l    ¦Øp l .
¡¡agents take turns to put forward a line of argument and probe sensors to obtain more information about the environment. such an action is referred to as an agent's utterance. in each turn  the contents of an agent's utterance is added to the global knowledge base; any sensors probed are marked as such  a sensor may not be probed more than once for the value of a specific literal   and costs are updated. we are now in a position to formally define the dialogue game.
¡¡the utterance function utterance : environment ¡Á name ¡ú 1args ¦²  ¡Á probes takes in an environment and an agent  via its name   and returns the utterance made by the agent. the first part of the utterance lists the arguments advanced by the agent  while the second lists the probes the agent would like to undertake. probes ¡Ê 1s¡Á¦²+ is the power set of all possible probes. the domain specific function  allows us to compute the cost of performing a probe.probing costs are defined in this way to allow for discounts on simultaneous probes  which in turn allows for richer reasoning by agents.
¡¡we define a function turn : environment ¡Á name ¡ú environment that takes in an environment and an agent label  and returns a new environment containing the effects of an agent's utterances. given an environment env and an agent ¦Á  we define the turn function as follows: turn env name  =  newagents cs ¡È ar newsensors pc  where ar newagents and newsensors are computed from the results of the utterance function. if utterance env name  =  ar pr  then newagents = agents   ¦Á ¡È  name kb ¦¸¦Á g c + pc pr   and    s l  ¡Ê pr  where l is a literal sensor s is able to probe  newsensors = sensors   s ¡È  ¦¸s ¦¸p ¡È ¦Øp l  .
¡¡it should be noted that the utterance depends on agent strategy; we will define one possible utterance function in the next section. before doing so  we must define the dialogue game itself.
	we	may	assume	that	our	agents	are	named
agent1 agent1 ... agentsn 1 where n is the number of agents participating in the dialogue. we can define the dialogue game in terms of the turn function by setting turn1 = turn  agents cs1 s admissible pc  agent1   and then having turni+1 = turn turni agenti mod n . the game ends when turni ...turni n+1 = turni n.
¡¡cs1 and ¦¸1 contains any initial arguments and opinions  and are usually empty. note that an agent may make a null utterance { } during its move to  eventually  bring the game to an end. in fact  given a finite number of arguments and sensors  our dialogue is guaranteed to terminate as eventually  no utterance will be possible that will change the public knowledge base cs.
¡¡we can compute an agent's utility by combining its utility gain  for achieving its goals  with the current costs. at any stage of the dialogue  given the environment's cs  and the set of all opinions probed by the sensors {¦¸p|s =  ¦¸s ¦¸p  ¡Ê s   and the environment's admissibility function  we can run the reasoning algorithm to compute a set of proven literals as proven = {l|l is a literal in cs and admissible l }. literals which fall below the proven threshold  or for which nothing is known are unproven: unproven = { literals in cs   proven}. then the net utility gain for an agent ¦Á is u ¦Á proven  = g proven unproven    c.
¡¡at the end of the dialogue  we assume that agents agree that literals in the proven set hold in the world.
1 the heuristic
agents are able to use the underlying argumentation framework and dialogue game to argue. in this section  we will describe a heuristic which agents may use to argue. informally  this heuristic attempts to make utterances which maximise agent utility using one step lookahead. if multiple arguments still remain  one is selected at random.
¡¡when making an utterance  an agent has to decide what arguments to advance  as well as what probing actions it should undertake. given an environment env and an agent ¦Á  let the set of possible arguments be pa = 1kb. we define the set of all possible probes as pp = { s l |s =  ¦¸s ¦¸p  ¡Ê s such that ¦Øs l  ¡Ê ¦¸s and ¦Øp l  ¡Ê/ ¦¸p}. the cost for a probe   pp is pc probe .
¡¡our agents use multiple sources of information to estimate the results of a probe. first  they have opinions regarding literals  stored in ¦¸¦Á. second  they can compute the current most likely value of the probe by examining sensors that already contain ¦Øp l . lastly  they can infer the most likely value of l by computing the effects of a probing action.
¡¡a naive agent will believe that an unprobed sensor s will return an opinion ¦Ø¦Á l . a non-naive agent will alter its beliefs based on existing probed values. one possible combination function will have the agent believe that the sensor will return an opinion for a literal
l given an agent's personal belief ¦Ø¦Á l   a set of sensors p containing for any t ¡Ê p  and the unprobed sensor's ¦Øs l . informally  this means that a na¡§ ve agent will believe that a probed sensor will share the same opinion of a literal as the agent has  while a non-na¡§ ve agent will weigh its expected opinion due to probes that have already been executed. different combination functions are possible due to different weights being assigned to different opinions.
¡¡for each possible utterance in pa ¡Á 1pp  the agent calculates what the environment will look like  as well as the net utility gain  using the functions pc and u . it then selects the utterance that will maximise its utility in the predicted future environment.
1 contracts
having fully defined the environment in which argument can take place  we now examine one possible application domain  namely contract monitoring. since contract representations are highly domain dependent  most of this section provides only guidelines as to how to transform contracts into a form suitable for use within the framework. to simplify the discussion  only two party contracts are examined.
¡¡to make use of the framework  a number of things must be defined. the  domain dependent  sensor set s and probing cost function pc must be initialised  and the agent's goal function g must also be specified. the agent's private knowledge base kb and the environment's starting knowledge base cs1 must also be populated  as well as the agent's prior opinions.
¡¡while nothing can be said about the generation of the sensor set except that it should reflect the sensors within the environment  probing costs can often be obtained from the contract. contract based sanctions and rewards can also be used to generate g. for example  if in a certain contract state an agent ¦Á must pay ¦Â a certain amount of money  the literal representing that state might have an associated negative utility for ¦Á  and positive utility for ¦Â. one point that should be noted when assigning utility to various states is the unevenness of burden of proof  prakken  1 . assume that ¦Â gains

figure 1: an argumentgraph. dashed lines represent negated edges  while solid lines are normal edges.
utility for proving a literal l  while ¦Á gains if the first agent does not  e.g. an agent has to pay a penalty to another in a certain state . in such a situation  the second agent loses utility if l is true. utility is not lost if l is either false or unproven.
¡¡we assume that a trivial mapping exists between contract clauses and arguments  for example  a clause stating  if a and b then c  can be represented as  {a b} c} . then one must decide whether to place the translated clauses into the agent's private knowledge base kb  or cs1. putting them into kb will mean that agents will advance those clauses relevant to the dispute during the course of the argument. having the arguments in cs1 represents a more realistic scenario wherein the contract is public knowledge. kb can also contain domain specific arguments the agents might want to use in the course of the dialogue. generating the agent's opinions about the state of literals is also outside the scope of our work.
1 example
the example we present here is based on a simplified contract from the mobile multi-media domain. a supplier has agreed to provide a consumer with a set of multi-media services  and the consumer believes that the package it has received has not lived up to the the provider's contractual obligations. assume that we have a contract of the form framerate   1 ¡ú givesanction horrormovie ¡ú  givesanction cinderella ¡ú  horrormovie
 textsent ¡ú givesanction
 phonefull  textreceived ¡ú  textsent
from this  we can generate the argument graph shown in figure 1  with the letter in the node being the first letter of the literal in the contract.
¡¡assume further that we have sensors for whether the movie's name is  cinderella   c   whether it is a horror movie  h   two sensors to detect whether the framerate is less than 1  f1 f1   whether a text was received  tr   and whether the phone's memory was full  p . let the opinion sensors for these ratings be ¦Øs c  =  

 
and let the cost of probing each of these sensors be 1  except for the sensor probing f1  which has a utility cost of 1.
¡¡finally  let the utility for agent ¦Á showing g holds be 1  with agent ¦Â having utility -1 for this state. assume ¦Á believes that literals c h f  tr  p hold with opinion strength . also assume that the contract is initially stored in kb  and the admissibility function is
  for a literal  or its
negation  to be admissible.
¡¡agent ¦Á might begin with the following utterance  {  tr    p   { p  tr}  ts   { ts} g } {p tr} . this will cost it 1 utility. given its reasoning algorithm  it will assume that once probed 
¦Ø g  = . however  the sensor probing p returns  making  p fall below the threshold of acceptability  thus invalidating this chain of argument. agent ¦Â passes.
¡¡agent ¦Á tries again  probing f1 and advancing the argument  {f} g   which returns  resulting in an opinion  after the   operator is applied. this means that g is now valid  with opinion
.
¡¡agent ¦Â counters  probing h and claiming  {h}  g . the probe returns   which means the argument is of sufficient strength to make g inadmissible  as ¦Ø g  ¡Ö

¡¡agent ¦Á undercuts ¦Â's argument by probing c and advancing  {c}  h . given the sensor's opinion 1.  this reinstates g by disallowing argument  {h}  g .
¡¡assume that ¦Â believes that f's true value is. it would then probe f1 with the utterance   {f1} . assume this returns an opinionwould then remain admissible  
¡¡both agents now pass. since g is admissible ¦Á has a net utility gain of 1  while ¦Â's utility loss is 1.
1 discussion
as seen in the preceding example  our framework is able to represent standard argumentation concepts such as rebutting attacks  defeat and reinstatement  as well as higher level concepts such as accrual of arguments  which many frameworks have difficulty dealing with. when compared to standard argumentation systems  the main shortcoming of our approach lies in its inability to handle looping arguments. however  many other frameworkssuffer from a similar  though weaker  problem  prakken and vreeswijk  1 . the use of possibilistic rather than probabilistic reasoning makes our framework suitable for situations where uncertainty exists regarding the probabilities to be assigned to an event. our sample domain is one area where such reasoning capabilities are useful.
¡¡most argument frameworks can be described in terms of dung's  semantics. these semantics are based on the notion of defeat. since defeat in our framework is dynamically calculated  applying these semantics to our framework is difficult. investigating the semantics of our framework more closely  and looking at how they compare to those of standard argumentation frameworks is one interesting direction in which this research can be taken.
¡¡some minor modifications of our framework might be necessary so that it can be used in other domains. for example  when arguing in the legal domain  the loser is often obliged to pay the winner's costs. by changing the way costs are computed in the turn function  our dialogue game is able to function in such an environment.
¡¡our heuristic has high computational complexity. the unoptimised version presented here runs in exponential time  o 1n   for n arguments in the agent's knowledge base  and while optimisations are possible  exponential running time is still possible in the worst case. however  it is easy to generate solutions of increasing length  and thus probably increasing utility cost . by relaxing the requirement that the heuristic find the best possible move  given one move lookahead   computation can be stopped at any time. it is trivial to show that the algorithm presented in figure 1 runs in o n  time where n is the number of graph edges. extending our heuristic to look ahead further is possible  but requires some model of the opponent's knowledge base and utility costs.
¡¡as a contract monitoring mechanism  our framework is intended to operate  between  actions. whenever a state change occurs  the agent should reason about whether a contract enforcement action should begin. if one does occur  it should finish before another state change happens. by integrating our framework with an expressive contract monitoring language  this limitation can be overcome.
¡¡another useful avenue for future work involves examining how default arguments can best be incorporated in the framework. the presence of uncertainty allows us to represent many default rules with the simple addition of a conflicting literal edge. however  we have not examined how generic default rules can best be represented.
¡¡our framework cannot deal with loops in arguments  and our main short term focus involves addressing this weakness. in the longer term  we hope to add opinions and weightings to the arguments themselves  for example stating that one argument is more persuasive than another . by also having a more complicated model for when an argument may be used  such as toulmin's model of argument  this path of research should allow us to represent argument schemes  walton and krabbe  1  in our framework.
¡¡finally  there are clear parallels between our work and the work done on trust and reputation frameworks  teacy et al.  1 . we intend to investigate whether any of the work done on opinions in that field will be of benefit to our model.
1 conclusions
in this paper we have put forward a new subjective logic based framework for analysing combinations of arguments and opinions. we then showed how this framework could be used to have agents discuss the state of literals in an environment where probing for environment state has an associated utility cost. after providing strategies for agents to argue in such an environment  we demonstrated its applicability as a technique for performing contract monitoring. while our framework has some limitations  it promises to be a springboard for further research in this area of argumentation  and has applications as a powerful mechanism for contract monitoring in complex domains.
1 acknowledgements
this work is supported by the dti/epsrc e-science core program and bt  via the conoise-g project
 http://www.conoise.org .
