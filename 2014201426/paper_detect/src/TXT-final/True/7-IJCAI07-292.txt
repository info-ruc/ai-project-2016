
in this paper we propose a suite of techniques for planning with temporally extended preferences  teps . to this end  we propose a method for compiling tep planning problems into simpler domains containing only final-state  simple  preferences and metric functions. with this simplified problem in hand  we propose a variety of heuristic functions for planning with final-state preferences  together with an incremental best-first planning algorithm. a key feature of the planning algorithm is its ability to prune the search space. we identify conditions under which our planning algorithm will generate optimal plans. we implemented our algorithm as an extension to the tlplan planning system and report on extensive testing performed to evaluate the effectiveness of our heuristics and algorithm. our planner  hplan-p  competed in the 1th international planning competition  achieving distinguished performance in the qualitative preferences track.
1 introduction
standard goals enable a planner to distinguish between plans that satisfy goals and those that do not. they provide no further means of discrimination between successful plans. preferences  on the other hand  convey information about how  good  a plan is  thus enabling a planner to distinguish between successful plans of differing quality. simple preferences express preferences over properties of the final state of a plan  while temporally extended preferences  teps  refer to properties of the whole plan. planning with teps has been the subject of recent research  e.g.   delgrande et al.  1; son and pontelli  1; bienvenu et al.  1 . it was also a theme of the 1th international planning competition  ipc-1 . in this paper we propose techniques for planning with teps such as those specified in pddl1  gerevini and long  1 . pddl1  is a planning domain definition language designed specifically for ipc-1. it extends pddl1 to include 
among other things  facilities for expressing teps  described by a subset of linear temporal logic  ltl . a metric function over simple and teps is then used to quantify the plan's quality. the aim in solving a pddl1 planning instance is to generate a plan that satisfies the hard goals and constraints while optimizing the metric function.
모a key insight  developed early in our investigation  was that  to be effective  a preference-based planner must actively guide search towards the achievement of preferences. to this end  we propose a compilation method that reduces a planning problem with teps into a new problem containing only simple preferencesand some metric functions. the new problem contains additional domain predicates that emulate the satisfaction of the teps in the original problem. roughly  this means that for any tep    we have a new predicate p  that is true in the final state of a plan iff   was satisfied during the execution of the plan. the advantage of having such a compilation is that the possibly complex process of satisfying a tep is reduced to the problem of satisfying a simple preference  i.e.  a final-state preference. hence  we observed that if we could find ways of adapting existing heuristic search techniques to achieve simple preferences  we would obtain a method for solving planning problems containing teps.
모unfortunately  heuristics for classical planning goals are not directly applicable to planning with simple preferences. a contribution of this paper is the development of a number of new search heuristics for planning with simple preferences  that we exploit for planning problems with teps encoded as simple preferences.
모using these heuristics  we propose a planning algorithm that incrementally finds better plans. once a plan is found  its metric value can be used as a bound for future plans: any plan that exceeds this metric value can be pruned from the search space. we also prove that under certain fairly natural conditions our algorithm can generate optimal plans.
모we have implemented a planner  hplan-p  which uses these techniques to find good quality plans. the planner is built as an extension of the tlplan system  bacchus and kabanza  1  1. we used our implementation to evaluate the performance of our algorithm and the relative performance of different heuristics on problems from both the ipc-1 simple and qualitative preferences tracks.
모in what follows  we briefly describe pddl1. then we outline our compilation method  the proposed heuristics  the algorithm we used to realize them and the experiments we performed to evaluate them. we conclude with a discussion of system performance and related work.
1 brief description of pddl1
pddl1 extends pddl1 by enabling the specification of preferences and hard constraints. it also provides a way of defining a metric function that defines the quality of a plan. the rest of this section briefly describes these new elements.
temporally extended preferences/constraints pddl1 specifies teps and temporally extended hard constraints in a subset of ltl. both are declared using the :constraints construct. preferences are given names in their declaration  to allow for later reference. by way of illustration  the following pddl1 code defines two preferences and one hard constraint.
 :constraints
 and
 preference cautious
 forall   o - heavy-object 
 sometime-after  holding  o 
 at recharging-station-1    
 forall   l - light 
 preference p-light  sometime  turn-off  l    
 always  forall  x - explosive   not  holding  x     
the cautious preference suggests that the agent be at a recharging station sometime after it has held a heavy object  whereas p-light suggests that the agent eventually turn all the lights off. finally  the  unnamed  hard constraint establishes that an explosive object cannot be held by the agent at any point in a valid plan.
모when a preference is externally universally quantified  it defines a family of preferences  containing an individual preference for each binding of the variables in the quantifier. therefore  preference p-light defines an individual preference for each object of type lightin the domain. preferences that are not quantified externally  like cautious  can be seen as defining a family containing a single preference.
모temporal operators  such as sometime-after in the example above  cannot be nested in pddl1. however  our approach can handle the more general case of nested teps.
precondition preferences
precondition preferences are atemporal formulae expressing conditions that should ideally hold in the state in which the action is preformed. they are defined as part of the action's precondition. for example  the preference labeled econ below specifies a preference for picking up objects that are not heavy.
 :action pickup :parameters   b - block 
 :precondition  and  clear  b 
                      preference econ  not  heavy  b       :effect  holding  b   
precondition preferences behave something like conditional action costs. they are violated each time the action is executed in a state where the condition does not hold. in the above example  econ will be violated every time a heavy block is picked up in the plan. therefore these preferences can be violated a number of times.
simple preferences
simple preferences are atemporal formulae that express a preference for certain conditions to hold in the final state of the plan. they are declared as part of the goal. for example  the following pddl1 code:
 :goal  and  delivered pck1 depot1 
              preference truck  at truck depot1     specifies both a hard goal  pck1must be deliveredat depot1  and a simple preference  that truck is at depot1 . simple preferences can also be externally quantified  in which case they again represent a family of individual preferences.
metric function
the metric function defines the quality of a plan  generally depending on the preferences that have been achieved by the plan. to this end  the pddl1 expression  is-violated name   returns the number of individual preferences in the name family of preferences that are violated by the plan. when name refers to a precondition preference  the expression returns the number of times this precondition preference was violated during the execution of the plan.
모the quality metric can also depend on the function total-time  which returns the plan length. finally  it is also possible to define whether we want to maximize or minimize the metric  and how we want to weigh its different components. for example  the pddl1 metric function:
 :metric minimize  +  total-time 
 * 1  is-violated econ  
 * 1  is-violated truck    
specifies that it is twice as important to satisfy preference econ as to satisfy preference truck  and that it is less important  but still useful  to find a short plan.
1 preprocessing pddl1
the preprocessing phase compiles away many of the more complex elements of pddl1  yielding a simpler planning problem containing only simple preferences  a new metric function that must be minimized that only refers to those simple preferences  and possibly some hard atemporal constraints. in the new problem  the teps have been encoded in new domain predicates.
모this phase is key in adapting existing heuristic techniques to planning with teps. one reason for this is that now the achievement of a tep is reduced the simple satisfaction of a domain predicate  i.e.  a new optional goal condition. generating a compact compiled problem is also key for good performance; our compilation achieves this in part by avoiding grounding the planning problem. the rest of this section describes how we do this for each of the pddl1 elements described in the previous section.
temporally extended preferences/constraints we use techniquespresented by baier and mcilraith to represent the achievement of first-order temporally extended formulae within the planning domain  ending up with a new augmented problem. the new problem contains  for each temporally extended preference or hard constraint    a new domain predicate that is true in the final state of a plan if and only if the plan satisfied   during its execution.
모the advantage of using such a compilation  is that firstorder ltl formulae are directly compiled without having to

	 a 	 b 
figure 1: pnfa for  a   sometime  exists   c   and  cafe  c   at  c      and  b   forall   x 
 sometime-after  loaded  x   delivered  x   
convertthe formulainto a possibly very large set of groundinstances. as a result  the compiled domain is much more compact  avoiding the exponential blowup that can arise when grounding. as we see later in section 1  this is key to our planner's performance.
모the compilation process first constructs a parameterized nondeterministic finite state automata  pnfa  a  for each temporally extended preference or hard constraint expressed as an ltl formula .1 the pnfa represents a family of nondeterministic finite state automata. its transitions are labeled by sets of first-order formulae. its states intuitively  monitor  the progress towards satisfying the original temporal formula. a pnfa a  accepts a sequence of domain states iff such a sequence satisfies  . figure 1 shows some examples of pnfa for first-order ltl formulae.
모parameters in the automata appear when the ltl formula is externally quantified  e.g. figure 1 b  . the intuition is that different objects  or tuples of objects  can be in different states of the automata. as an example  consider a transportation domain with two packages  a and b  which are initially not loaded in any vehicle. focusing on the formula of figure 1 b   both objects start off in states q1 and q1 of the automata because they are not loaded in the initial state. this means that initially both objects satisfy the temporal formula  since both are in the automaton's accepting state q1. that is  the null plan satisfies the formula  b  of figure 1. now  assume we perform the action load a truck . in the resulting state  b stays in q1 and q1 while a now moves to q1. hence  a no longer satisfies the formula; it will satisfy it only if the plan reaches a state where delivered a  is true.
모to represent the automata within the domain  for each automaton  we define a predicate specifying the automaton's current set of states. when the automaton is parameterized  the predicate has arguments  representing the current set of automaton state for a particular tuple of objects. in our example  the fact  aut-state q1 a  represents that object a is in q1. moreover  for each automaton we define an accepting predicate. the accepting predicate is true of a tuple of objects if the plan has satisfied the temporal formula for such a tuple. as actions are executed  the automata states  as well as properties of the world  need to be updated. to accomplish this we define an automata update for each automata. our planner performs this update automatically after performing any domain action. for the automata of figure 1 b   the update would include rules such as:
 forall   x   implies  and  aut-state q1  x   loaded  x  
 add  aut-state q1  x    
that is  an object  x moves from state q1 to q1 whenever  loaded  x  is true.
모analogously  we define an update for the accepting predicate  which is performed immediately after the automata update-if the automata reaches an accepting state then we add the accepting predicate to the world state.
모in addition to specifying how the automata states are updated  we need also to specify what objects are in what automata states initially. this means we must augmentthe problem's initial state by adding a collection of initial automata facts. given the original initial state and an automaton  the planner computes all the states in which a tuple of objects can be  and then adds the corresponding facts to the new problem. in our example  the initial state of the new compiled problem contains facts stating that both a and b are in states q1 and q1.
모if the temporal formula originally described a hard constraint  the accepting condition of the automaton can be treated as additional mandatory goal. during search we also use tlplan's ability to incrementally check temporal constraints to prune from the search space those plans that have already violated the constraint.
precondition preferences
precondition preferences are very different from teps: they are atemporal  and are associated with the execution of actions. if a precondition preference p is violated n times during the plan  then the pddl1 function  is-violated p  returns n.
모therefore  the compiled problem contains a new domain function is-violated-counter-p  for each precondition preference p. this function keeps track of how many times the preference has been violated. it is  conditionally  incremented whenever its associated action is performed in a state that violates the atemporal preference formula. in the case where the preference is quantified  the function is parameterized  which allows to compute the number of times different objects have violated the preference.
모for example  consider the pddl1 pickup action given above. in the compiled domain  the original declaration is replaced by:
 :action pickup :parameters   b - block 
 :precondition  clear  b  
 :effect  and  when  heavy  b 
 increase  is-violated-counter-econ  1   
 holding  b    ;; add  holding  b 
simple preferences
as with teps  we add new accepting predicates to the compiled domain  one for each simple preference. these predicates become true iff the preference is satisfied. moreover  if the preference is quantified  they are parameterized.
metric function
for each preference we define a new function is-violated.
its value is defined in terms of the accepting predicates  for temporally extended and simple preferences  and in terms of the violation counters  for precondition preferences . if preference p is quantified  then the is-violated function counts the number of object tuples that fail to satisfy the preference.
모the metric function is then defined just as in the pddl1 definition but making reference to these new functions. if the objective was to maximize the function we invert the sign of the function body. therefore  we henceforth assume that the metric is always to be minimized.
1 planning with heuristic search
after applying the preprocessing phase described above we are left with a planning problem containing only simple preferences. we propose to solve this problem with a novel combination of heuristic search techniques.
모heuristic search has been very successful in solving classical planing problems where the conjunction of all goals must be achieved. in our case  however  it is generally impossible to satisfy all preferences. instead the planner must try to achieve a  good  subset of the preferences. in particular  this subset of preferences must be jointly achievable and must yield a preferred value. some planners have used techniques for selecting  during search  a subset of preferences and then solving that subset as a classical goal using standard planning heuristics  e.g.  yochanps  benton et al.  1  . however  this introduces the non-trivial problem of first selecting such a subset.
모our approach is to utilize a unified heuristic search technique that attempts to tradeoff preference desirability and ease of achieving during the search for a plan. another important factor is that in addition to the preferences the planning problem generally contains a classical goal  that must be achieved. hence  the search must give priority to achieving the hard goal.
모to solve this problem of tradeoffs we have developed an iterative planning technique that uses a sequence of heuristically guided planning episodes. instead of selecting the preferences we want to satisfy  we simply ask the planner to find a better plan in each planning episode.
모turning to the details we first present the suite of heuristics we have developed to use within each planning episode  and then we explain how we control the sequence of planning episodes.
1 heuristics for planning with preferences
many of our heuristics are based on the well-known technique of computing a relaxed planning graph  hoffmann and nebel  1 . we can view this graph as composed of relaxed states. a relaxed state at depth n+1 is generated by applying all the positive effects of actions that can be performed in the relaxed state of depth n  i.e.  by ignoring delete lists and by applying all of the actions in one step .
모most of the heuristics given below are computed for a state s by constructing the relaxed graph starting at s and growing this graph until all goal facts and all preference facts appear in the relaxed state  or we reach an empirically determined bound on the depth of the relaxed states. the goal facts correspond to the hard goals  and the preference facts correspond to instantiations of the accepting predicates used to convert teps to simple preferences  as described above .
goal distance function  g  this function is a measure of how hard it is to reach the goal. it is based on a heuristic proposed by zhu and givan . formally  let g be the set of goal facts that appear in the relaxed graph. furthermore  if f is a fact  let d f  be the depth at which f first appears during the construction of the graph. if all the problem's goal facts are in g  then g =뫉f뫍g d f k  where k is a positive  real parameter. otherwise g =.
preference distance function  p  this function is a measure of how hard it is to reach the various preference facts. it is analogous to g but for preferences. let p be the set of preference facts that appear in the relaxed graph. then p =뫉f뫍p d f k  for a parameter k. notice that p is not penalized when there are unreachable preference facts since in general the plan will not achieve all preferences.
optimistic metric function  o  this is an estimate of the value achievable by any plan extending the partial plan reaching s. o does not require constructing the relaxed planning graph. rather it is computed by evaluating the metric function in s assuming no precondition preference will be violated in the future  and that all unviolated preferences will be achieved in the future. under the condition that the metric function is non-increasing in the number of achieved preferences  o will be a lower bound on the best plan extending s. o is a variant of  optimistic weight  bienvenu et al. .
best relaxed metric function  b  b is another estimate of the value achievable by extending s. b utilizes the relaxed planning graph to obtain its estimate. in particular  we evaluate the metric function in each of the relaxed worlds of the planning graph and take b to be the minimum among these values. b will generally yield a better  higher  estimate of the optimal value achievable by extending s since it will regard preferences that do not appear in the relaxed states as being unsatisfiable. under a slightly more complex method for building the relaxed planning graph b can be guaranteed to be a lower bound under the same condition as o  i.e.  that the preferencemetric is non-increasingin the numberof achieved preferences . however this techniquewas not used in the empirical results reported. hence in our experiments b was not a guaranteed lower bound.
discounted metric function  d r   a weighting of the metric function evaluated in the relaxed states. assume s1 s1 ... sn are the relaxed states of the graph  where si is at depth i. the discounted metric  d r   is:
n 1
d r = m s1 + 뫉 m si+1  m si  ri 
i=1
where m si  is the metric function evaluated in the relaxed state si  r is a discount factor  1 뫞 r 뫞 1 .
모the d function is optimistic with respect to preferences that seem easy and pessimistic with respect to preferences that look hard. intuitively  the d function estimates the metric of a plan that is a successor of the planning state by  believing  more in the satisfaction of preferences that appear to be easier. observe that m si+1  m si  is the amount of metric value gained when passing from state si to state si+1.
this amount is then multiplied by ri  which decreases as i increases. observe also that  although the metric gains are discounted  preferences that are weighted higher in the pddl1 metric will also have a higher impact on the value of d. that is  d achieves the desired tradeoff between the ease of achieving a preference and the value of achieving it.
모a computational advantage of the d function is that it is very easy to compute. as opposed to other approaches  this heuristic never needs to make an explicit selection of the preferences to be pursued by the planner.
sequence of planning episodes
when search is started  i.e.  no plan has been found   the algorithm uses the goal distance function  g  as its heuristic in a standard best first search. the other heuristics are ignored in this first planning episode. this is motivated by the fact that the goal is a hard condition that must be satisfied. in some problems the other heuristics  that guide the planner towards achieving a preferred plan  can conflict with achieving the goal  or might cause the search to become too difficult.
모after finding the first plan  the algorithm restarts the search from scratch  but this time it uses some combination of the aboveheuristics to guidethe plannertowards a preferredplan.
let userheuristic   denote this combination.	user-
heuristic   could be any combination of the above heuristic functions. nevertheless  in this paper we consider only a small subset of all possible combinations. in particular  we consider only prioritized sequences of heuristics  where the lower priority heuristics are used only to break ties in the higher priority heuristics.
since achieving the goal remains mandatory  user-
heuristic   always uses g as the first priority  and with some of the other heuristics at a lower priority. for example  consider the prioritization sequence gd 1 o. when comparing two states of the frontier  the planner first looks at the g function. the best state is the one with lower g value  i.e. lower distance to the goal . however  if there is a tie  then it uses d 1   the best state being the one with a smaller value . finally  if there is still a tie  we use the o function. in section 1  we investigate the effectiveness of several such prioritized heuristics sequences.
모one other component of the planning algorithm is that we utilize a scheme for caching relaxed states  and the heuristic computed at these states  so that we can short-circuit the relaxed planning graph construction if the same relaxed state is encountered again. since constructing the relaxed states can be expensive this caching scheme yields a useful speedup.
increasing plan quality once we have completed the first planning episode  using g  we want to ensure that each subsequent planning episode yields a better plan.
모this is achieved by using increasingly restrictive pruning in each planning episode. in particular  in each planning episode the algorithm prunes from the search space any state s that we estimate cannot reach a better plan than the best plan found so far. this estimate is provided by the function metricboundfn   which is given as an argument to the search algorithm. metricboundfn s  must compute or estimate a lowerbound on the metric of any plan extending s. we have used two of the above heuristics  b and o  for this bounding
input : init: initial state  goal  hardconstraints: a set of hard
constraints  userheuristic  : a heuristic function  metricboundfn  : function estimating metric for a partial plan
begin
bestmetric 뫹 worst case upper bound; heuristicfn뫹 g frontier 뫹 initfrontier init  while frontierdo
current 뫹 removebest frontier 
f 뫹evaluate hardconstraints in current
if f is not false then
if current is a plan and its metric is   bestmetric then output the current plan if this is first plan found then
heuristicfn뫹 userheuristic  
frontier 뫹 initfrontier init   search restarted  hardconstraints 뫹 hardconstraints뫋
모모모{always metricboundfn    bestmetric } bestmetric 뫹 metricfn current 
succ 뫹 expand current 
모모frontier 뫹 merge succ frontier heuristicfn  endalgorithm 1: hplan-p's search algorithm.
function.
모pddl1 domains may also contain hard constraints. hence  in addition to pruning by bounding  the algorithm prunes from its search space any state that violates a hard constraint.
putting the above together we obtain the algorithm 1.
1 properties of the algorithm
we can show that under certain conditions our search algorithm is guaranteed to return an optimal or a k-optimal solution. it is important to note that our conditions impose no restriction on the userheuristic   function. in particular  we can still ensure optimality even if this function is inadmissible. in planning this is important  as inadmissible heuristics are typically required for adequate search performance.
모we require in our proofs that metricboundfn s  be a lower bound on the metric value of the optimal plan extending s. in this case we say that the pruning is sound. when sound pruning is used  optimal plans are never pruned from the search space. therefore  we can be sure no state that leads to an optimal plan will be discarded by the algorithm. moreover  optimality can be guaranteed when the algorithm stops. lemma 1 if algorithm 1 terminates and a sound  or no  pruning has been used  then the last plan returned  if any  is optimal.
proof: each planning episode has returned a better plan  and the algorithm stops only when the final planning episode has rejected all possible plans. since the bounding function never prunes an optimal plan this means that no plan better than the last one returned exists.	
모as described above  if the metric function is nonincreasing in the number of achieved preferences  o will be a lower bound. as a matter of fact  all metric functions used in ipc-1 are non-increasing in the number of achieved preferences.
모lemma 1 still does not guarantee that an optimal solution will be found because the algorithm might never terminate. to guarantee this  we impose further conditions to sound pruning. first  we require that the initial value of bestmetric  worst case upper bound  be finite. second  we require of the plan metric function that for every value r less than the initial value of bestmetric the number of plans with metric value less then r is finite.
theorem 1 if the metric function satisfies the conditions above  the initial value of bestmetric is finite  and a sound  or no  pruning is used  then algorithm 1 is guaranteed to find an optimal plan  if any exists.
proof: each planning episode only examines plans with metric value less than bestmetric. by assumption this is a finite set of plans  so each episode must complete and the algorithm must eventually terminate. now the result follows from
lemma 1.	 k-optimality another condition that our algorithm can achieve is k-optimality. we say that a planning algorithm is k-optimal if it is always able to find an plan that is optimal with respect to the set of plans of length i 뫞 k.
theorem 1 if a sound  or no  pruning is used  then algorithm 1 is k-optimal when search is restricted to plans of length bounded by k.
proof: follows from a similar argument as lemma 1. 
1 implementation and evaluation
we have implemented our ideas in the planner hplan-p.
hplan-p consists of two modules. the first is a preprocessor that reads pddl1 problems and generates a planning problem with only simple preferences expressed as a tlplan domain. the second module is a modified version of tlplan that is able to compute the heuristic functions and implements the algorithm of section 1.
모recall that two of the key elements in our algorithm are the iterative pruning strategy and the heuristics used for planning. in the following subsections we evaluate the effectiveness in obtaining good quality plans using several combinations of the heuristics. as a testbed  we use the problems of the qualitative preferences track of ipc-1  all of which contain teps. the ipc-1 domains are composed by two transportation domains: tpp and trucks  a production domain: openstacks  a domain which involves moving objects by using machines under several restrictions: storage  and finally  rovers  which models a rover that must move and collect experiments. each domain consists of 1 problems. the problems in the trucks  openstacks  and rovers domains have hard goals and preferences. the remaining problems have only preferences. preferences in these domains impose interesting restrictions on plans  and usually there is no plan that can achieve them all.
모at the end of the section  we compare our planner against the other planners that participated in ipc-1. the results are based on the data available from ipc-1  gerevini et al.  1  and our own experiments.
1 the effect of iterative pruning
to evaluate the effectiveness of iterative pruning we compared the performance of three pruning functions: the optimistic metric  the best relaxed metric  and no pruning at all. from our experiments  we conclude that most of the time pruning produces better results than no pruning  and that  overall  pruning with b usually produces better results than pruning with o.
모the impact of pruning varies across different domains. in the tpp domain pruning has a minor effect. pruning with o produces very slight improvements in the plans' quality  around 1%   whereas pruning with b produces no clear improvement. in the openstacks domain  pruning with o has no effect  whereas pruning with b improves the quality of plans by over 1%. a similar phenomenon occurs in the rovers domain  where o has no effect  but b improves the quality of plans by over 1%. in storage  pruning with o improves the plan's metric by about 1%  whereas b worsens the quality by about 1%. finally  in trucks  pruningwith o improvesquality by over 1%  and pruning with b by over 1%.
모an interesting observation is that  although pruning with b can sometimes remove optimal solutions  in some domains its use is critical to obtaining good performance. notably  in most of the openstacks problems it was impossible to improve upon the first plan found without the use of b for pruning. for example  when using the best-performing heuristic along with b for pruning  the planner was able to improve the first plan found in 1 of 1 problems. however  the same heuristic used with no pruning or with o pruning was able to improve plans in only 1 of 1 problems.
모a side effect of pruning is that it can sometimes prove  when the conditions of lemma 1 are met  that an optimal solution has been found. indeed  the algorithm stops on most of the simplest problems across all domains. if no pruning was used the search would generally never terminate.
1 performance of heuristics
to determine the effectiveness of various prioritized heuristic sequences  section 1  we compared 1 heuristic sequences using b as a pruning function  allowing the planner to run for 1 minutes over each of the 1 ipc-1 problem instances. all the heuristics had g as the highest priority  therefore we omit g from their names . specifically  we experimented with o 
b  op  po  bp  pb  om  mo  and bd r   d r b  od r   d r o for r 뫍{1.1.1.1.1.1.1.1}. the m in om and mo denotes the pddl1 metric function. note that the o  om  and mo heuristics are the only ones that do not use the relaxed planning graph to guidethe search towards the satisfaction of the preferences.
모in general  we say that a heuristic is better than another if it produces plans with better quality  where quality is measured by the metric of the plans. to evaluatehow gooda heuristic is  we measure the percent improvement of the metric of the last plan found with respect to the metric of the first plan found. thus  if the first plan found has metric 1  and the last has metric 1  the percent improvement is 1%. since first plan is always found using g its metric value is always the same  regardless of the heuristic we choose. hence this measure can be used to objectively compare performance.
모table 1 shows the best and worst performing heuristics in each of the domains tested. in many domains  several heuristics yield very similar performance. moreover  we conclude that the heuristic functions that use the relaxed graph are key
domain1 plan 1 planbest heuristicsworst heuristicsopenstacks1for all r's: do r   1   od r  1   bp 1   mo 1   om 1 po 1   op 1   pb 1   o 1   b 1 trucks1pb 1   bp 1   for r뫟 1: do r  1   od r  1   bd r  1   db r  1 all remaining 1 storage1for r   1: bd r  1  
od 1  1   bd 1  1   om 1 b 1   o 1   po 1   pb 1   mo 1 rovers1do 1  1   db 1  1   for r뫍{.1 .1 .1} db r   do r  1 bp 1  	op 1  
pb 1  po 1 tpp1o 1   b 1   do 1  1 po 1   pb  1  bd 1  1   bd 1  1   bd 1  1 table 1: performance of different heuristics in the problems of the qualitative preferences track of ipc-1. the second column shows the number of problems where at least one plan was found. the third  shows how many of these plans were subsequently improved upon by the planner. the average percent metric improvement wrt. the first plan found is shown in square brackets.to good performance. in all problems  save tpp  the heuristics that used the relaxed graph had the best performance.
1 comparison to other approaches
we entered hplan-p in the ipc-1 qualitative preferences track  gerevini et al.  1   achieving 1nd place behind sgplan1  hsu et al.  1 . despite hplan-p's distinguished standing  sgplan1's performance was superior to hplan-p's  sometimes finding better quality plans  but generally solving more problems and solving them faster. sgplan1's superior performance was not unique to the preferences tracks. sgplan1 dominated all 1 tracks of the ipc-1 satisficing planner competition. as such  we conjecture that their superior performance can be attributed to the partitioning techniques they use  which are not specific to planning with preferences  and that these techniques could be combined with those of hplan-p.
hplan-p consistently performed better than mips-bdd
 edelkamp et al.  1  and mips-xxl  edelkamp  1 ; hplan-p can usually find plans of better quality and solve many more problems. mips-bdd and mips-xxl use related techniques  based on propositional bu몮chi automata  to handle ltl preferences. we think that part of our superior performance can be explained because our compilation does not ground ltl formulae  avoiding blowups  and also because the heuristics are easy to compute. for example  mips-xxl and mips-bdd were only able to solve the first two problems  the smallest  of the openstacks domain  whereas hplan-p could quickly find plans for almost all of them. in this domain the number of preferences was typically high  the third instance already contains around 1 preferences . on the other hand  something similar occurs in the storage domains. in this domain  though  there are many fewer preferences  but these are quantified. more details can be found on the results of ipc-1  gerevini et al.  1 .
모while we did not enter the simple preferences track  experiments performed after the competition indicate that hplanp would have done well in this track. to perform a comparison  we ran our planner for 1 minutes1 on the first 1 instances1 of each domain. in table 1  we show the performance of hplan-p's best heuristics compared to all other
domainhplan-psgplan1yochanpsmips-bddmips-xxl#sratio#sratio#sratio#sratio#sratiotpp11.1-.11-11-11.1.1trucks11111storage11.1-.11-11.11.1.1pathways11.1-.111.11-1table 1: relative performance of hplan-p's best heuristics for simple preferences  compared to other ipc-1 participants. ratio compares the performance of the particular planner and
hplan-p's. ratio   1 means hplan-p is superior  and ratio   1 means otherwise. #s is the number of problems solved.
participants  in those domains on which all four planners solved at least one problem. in the table  #s is the number of problems solved by each approach  and ratio is the average ratio between the metric value obtained by the particular planner and the metric obtained by our planner. thus  values over 1 indicate that our planner is finding better plans  whether values under 1 indicate the opposite.
모we conclude that hplan-p is typically outperformed by sgplan1. although not in the table  in the most simple instances usually hplan-p does equally well or better than sgplan1. hplan-p can solve more instances than those solved by yochanps  mips-xxl and mips-bdd. furthermore  it outperforms yochanps and mips-xxl in terms of achieved plan quality. hplan-p's performance is comparable to that of mips-bdd in those problems that can be solved by both planners. finally  we again observed that the bestperforming heuristics in domains other than tpp are those that use the relaxed graph  and  in particular  the d heuristic.
1 summary and related work
in this paper we presented a suite of techniques for planning with teps and hard constraints. the techniques are amenable to integration with a variety of classical and simplepreference planners.
모our first contribution was to propose a compilation method that reduces planningproblemswith pddl1 ltl preferences into problems containing only simple  i.e.  final-state  preferences. a unique feature of our compiled representation is that it is parameterized  preserving the quantification inherent in pddl1. with the planning problem conveniently transformed  we proposed a number of heuristics for planning with simple preferences. we also proposed an incremental best-first search planning algorithm  guided by a prioritized sequence of these heuristics. a key feature of our algorithm is that it can use heuristic functions to prune the search space during incremental planning. we proved that under some fairly natural conditions our algorithm can generate optimal plans.
모we implemented our algorithm and heuristics as an extension to the tlplan planning system and performed extensive experiments on ipc-1 problems to evaluate the effectiveness of our heuristic functions and algorithm. while no heuristic dominated all test cases  several clearly provided superior guidance towards solutions. in particular  those that use the relaxed graph in some way proved to be the most effective in almost all domains. experimentsalso confirmed the essential role of pruning in solving large problems. hplanp scales better than many other approaches to planning with preferences. we attribute much of this superior performance to the fact that we do not ground our planning problems.
모there is a variety of related work on planning with preferences. systems by bienvenu et al.  and son and pontelli  can plan with teps; however  they are far less efficient predominantly because they do not use heuristics to guide search towards achievement of preferences. these planners also use a different  qualitative language to describe the quality of a plan that can refer explicitly to numbers  as opposed to the pddl1 metric function.
모yochanps  benton et al.  1  is a heuristic planner for simple preferences. our approach is similar to theirs in the sense that both use a relaxed graph to obtain a heuristic estimate. however yochanps is not an incremental planner and does not use pruning. to compute its heuristic  it explicitly selects a subset of preferences to achieve. this can be very costly in the presence of many preferences.
   mips-xxl  edelkamp et al.  1  and mips-bdd  edelkamp  1  both use bu몮chi automata to plan with temporally extended preferences. however  since the ltl formulae need to be grounded it is prone to exponential blow-up. further  the search techniques used in both of these planners are quite different from those we exploit. mips-xxl iteratively invokes a modified metric-ff  hoffmann  1  forcing plans to have decreasing metric values. mips-bdd  on the other hand  performs a cost-optimal breath-first search that does not use heuristics.
모sgplan1  hsu et al.  1  uses a completely different approach. it partitions the planning problem into several subproblems. it then solves this problems using heuristics  and then integrates their solutions.
모finally  less related is the approach by brafman and chernyavsky  proposed a csp approach to planning with final-state qualitative preferences specified using tcpnets. the preferences cannot be temporal.
references
 bacchus and kabanza  1  f. bacchus and f. kabanza. planning for temporally extended goals. annals of mathematics and artificial intelligence  1-1 :1  1.
 baier and mcilraith  1  j. a. baier and s. a. mcilraith. planning with first-order temporally extended goals using heuristic search. in proc. of the 1st national conference on artificial intelligence  aaai-1   pp. 1  boston  ma  1.
 benton et al.  1  j. benton  s. kambhampati  and m. b. do. yochanps: pddl1 simple preferences and partial satisfaction planning. in 1th international planning competition booklet  ipc-1   pp. 1  lake district  england  july 1.
 bienvenu et al.  1  m. bienvenu  c. fritz  and s. mcilraith. planning with qualitative temporal preferences. in proc. of the 1th int'l conference on knowledge representation and reasoning  kr-1   pp. 1  lake district  england  1.
 brafman and chernyavsky  1  r.	brafman	and
y. chernyavsky. planning with goal preferences and constraints. in proc. of the 1th int'l conference on automated planning and scheduling  icaps-1   pp. 1  june 1.
 delgrande et al.  1  j. p. delgrande  t. schaub  and h. tompits. domain-specific preferences for causal reasoning and planning. in proc. of the 1th int'l conference on automated planning and scheduling  icaps-1   pp. 1  whistler  canada  june 1.
 edelkamp et al.  1  s. edelkamp  s. jabbar  and m. naizih. large-scale optimal pddl1 planning with mips-xxl. in 1th international planning competition booklet  ipc-1   pp. 1- 1  lake district  england  july 1.
 edelkamp  1  s. edelkamp. optimal symbolic pddl1 planning with mips-bdd. in 1th international planning competition booklet  ipc-1   pp. 1  lake district  england  july 1.
 gerevini and long  1  a. gerevini and d. long. plan constraints and preferences for pddl1. technical report 1  department of electronics for automation  university of brescia  brescia  italy  1.
 gerevini et al.  1  a. gerevini  y. dimopoulos  p. haslum  and a. saetti. 1th international planning competition  july 1. http://zeus.ing.unibs.it/ipc-1/.
 hoffmann and nebel  1  j. hoffmann and b. nebel. the ff planning system: fast plan generation through heuristic search. journal of artificial intelligence research  1-1  1.
 hoffmann  1  j. hoffmann. the metric-ff planning system: translating  ignoring delete lists  to numeric state variables. journal of artificial intelligence research  1-1  1.
 hsu et al.  1  c.-w. hsu  b. wah  r. huang  and y. chen. constraint partitioning for solving planning problems with trajectory constraints and goal preferences. in proc. of the 1th int'l joint conference on artificial intelligence  ijcai-1   hyderabad  india  january 1. to appear.
 son and pontelli  1  t. c. son and e. pontelli. planning with p