 
the problem of efficiently finding similar items in a large corpus of high-dimensional data points arises in many real-world tasks  such as music  image  and video retrieval.  beyond the scaling difficulties that arise with lookups in large data sets  the complexity in these domains is exacerbated by an imprecise definition of similarity. in this paper  we describe a method to learn a similarity function from only weakly labeled positive examples.  once learned  this similarity function is used as the basis of a hash function to severely constrain the number of points considered for each lookup.  tested on a large real-world audio dataset  only a tiny fraction of the points  ~1%  are ever considered for each lookup.  to increase efficiency  no comparisons in the original high-dimensional space of points are required.  the performance far surpasses  in terms of both efficiency and accuracy  a state-of-the-art locality-sensitive-hashing based technique for the same problem and data set. 
1 introduction  
this work is motivated by the need to retrieve similar audio  image  and video data from extensive corpora of data.   the large number of elements in the corpora  the high dimensionality of the points  and the imprecise nature of  similar  make this task challenging in real world systems.   
　throughout this paper  we ground our discussion in a real-world task: given an extremely short  ~1 second  audio  snippet  sampled from anywhere within a large database of songs  determine from which song that audio snippet came.   we will use our system for retrieval from a database of 1 songs with 1 snippets extracted from each song  resulting in 1 1 snippets  figure 1 . the short duration of the snippets makes this particularly challenging.  
　the task of distortion-robust fingerprinting of music has been widely examined.  many published systems created to this point attempt to match much longer song snippets  some report results on smaller datasets  and others use prior probabilities to scale their systems   ke et al.  1; haitsma & kalker  1; burges et al.  1; shazam  1 .   we assume a uniform prior and match extremely small snippets.   
while our system can easily be incorporated into those designed for longer snippet recognition  by testing on short snippets we highlight the fundamental retrieval issues that are often otherwise masked through the use of extra temporal coherency constraints made possible with longer snippets.  further  we will demonstrate improved performance on the retrieval of both short and long snippets. 
1 background 
generally stated  we need to learn how to retrieve examples from a database that are similar to a probe example in a manner that is both efficient and compact.  one way to do this is to learn a distance metric that  ideally  forces the smallest distance between points that are known to be dissimilar to be larger than the largest distance between points that are known to be similar  hastie & tibrishani  1; shental et al.  1; bar-hillel et al.  1;  tsang et al.  1 .  these methods can be used with k-nearest neighbors  knn  approaches.  knn approaches are well suited to our task: they work with many classes and are able to dynamically add new classes without retraining.   approximate knn is efficiently implemented through locality-sensitive hashing  lsh   gionis et al.  1 .  lsh and other hash functions are sublinear in the number of elements examined compared to the size of the database.   
　lsh works for points with feature vectors that can be compared in a euclidean space.  the general idea is to partition the feature vectors into l subvectors and to hash each point into l separate hash tables  each hash table using one of the subvectors as input to the hash function. candidate 

figure 1:   a typical spectrogram and extracted snippets;  note the overlap. the task: given any snippet  find others from the same song.   
neighbors can then be efficiently retrieved by partitioning the probe feature vector and collecting the entries in the corresponding hash bins.  the final list of potential neighbors can be created by vote counting  with each hash casting votes for the entries of its indexed bin  and retaining the candidates that receive some minimum number of votes  vt.  if vt = 1  this takes the union of the candidate lists.  if vt = l  this takes the intersection. 
　this idea has been extended by  shakhnarovich  1  to parameter-sensitive hashing  psh .  to remove the euclidean requirement   psh uses paired examples  both positive  similar  and negative  dissimilar  to learn the l hash functions that best cluster the two halves of each positive example while best separating the two halves of each negative example.   this approach is closest to the one presented here. one difference is the choice of learning constraints.   shakhnarovich  1  used positive examples  similar pairs  and explicitly provided negative examples  dissimilar pairs  to induce the learning algorithm to find a hash function.  the unstated assumption is that any hash function that adequately learns the negative examples will provide  nearly  equal occupancy on the various hash bins so that the validation step does not then process unpredictably large numbers of potential neighbors. in our work  the learning function is created using only weakly labeled positive examples  similar pairs  coupled with a forced constraint towards maximum entropy  nearly uniform occupancy .  we do not explicitly create negative examples  dissimilar pairs   but instead rely on our maximum-entropy constraint to provide that separation.  as will be shown  our learning constraints have the advantage of allowing for similarities between only nominally distinct samples  without requiring the learning structure to attempt to discover minor  or nonexistent  differences in these close examples. 
　the remainder of this paper is organized as follows.  the next section will describe the properties we need for learning a hash function  the training procedure and the analysis.  section 1 shows how the system can be scaled to handle large amounts of data.  section 1 demonstrates this concretely on a real-world audio-retrieval task. section 1 closes the paper with conclusions and future work.  
1 learning hash functions 
　all deterministic hash functions map the same point to the same bin.  our goal is to create a hash function that also groups  similar  points in the same bin  where similar is defined by the task.  we call this a forgiving hash function in that it forgives differences that are small with respect to the implicit distance function being calculated. 
　to learn a forgiving hash function for this task  we need to satisfy several objectives:   1  the hash function must be able to work without explicitly defining a distance metric.   1  the hash function must learn with only weakly labeled examples; in our task  we have indications of what points are similar  the song label   but we do not have information of which parts of songs sound similar to other parts.   1  the hash function must be able to generalize beyond the examples given; we will not be able to train it on samples from all the songs with which we expect to use it.  effectively  this means that the learned function must maintain entropy in the hashes for even new samples.  whether or not the songs have been seen before  they must be well distributed across the hash bins.  if entropy is not maintained  points will be hashed to a small number of bins  thereby rendering the hash ineffective.   
　the requirement to explicitly control entropy throughout training is a primary concern.   in the next section  we demonstrate the use of a neural network with multiple outputs to learn a hash function. entropy is explicitly controlled by carefully constructing the outputs of the training examples.   other learning methods could also have been used; however  neural networks  hertz et al.  1; pollack  1  are suited to this task since  as we will show  controlling the entropy of each output and between outputs  to reduce correlation  is possible.  the incremental training of the network provides an opportunity to dynamically set the target outputs to give similar songs similar target outputs. 
1 explicitly maintaining entropy in training 
we train a neural network to take as input an audio spectrogram and to output a bin location where similar audio spectrograms will be hashed.  we represent the outputs of the neural network in binary notation; for these experiments  we create a hash table with 1 bins and therefore train the network with 1 binary target outputs.1 
　for training  we select 1 consecutive snippets from 1 songs  sampled 1 ms apart  total 1 snippets .   each snippet from the same song is labeled with the same target song code.  this is a weak label.  although the snippets that are temporally close may be 'similar'  there is no guarantee that snippets that are further apart will be similar - even if they are from the same song.  moreover  snippets from different songs may be more similar than snippets from the same song. however  we do not require this detailed labeling; such labels would be infeasible to obtain for large sets. 
　the primary difficulty in training arises in finding suitable target outputs for each network.  every snippet of a song is labeled with the same target output.  the target output for each song is assigned randomly  chosen without replacement from Ρ   s   where s is 1 bits - i.e.  each song is assigned a different set of 1 bits and Ρ   s is the power set  containing 1 different sets of 1 bits.  sampling without replacement is a crucial component of this procedure.  measured over the training set  the target outputs will have maximal entropy.  because we will be using the outputs as a hash  maintaining high entropy is crucial to preserving a good distribution in the hash bins. the drawback of this randomized target assignment is that different songs that sound similar may have entirely different output representations  large hamming distance between their target outputs .  if we force the network to learn these artificial distinctions  we may hinder or entirely prevent the network from being able to correctly perform the mapping.      
                                                 
       1  1 is a small number of bins for a hash table that will hold millions of items; in section 1  we will show how to efficiently scale the size. 
　instead of statically assigning the outputs  the target outputs shift throughout training.  after every few epochs of weight updates  we measure the network's response to each of the training samples.  we then dynamically reassign the target outputs for each song to the member from Ρ   s that is closest to the network's response  aggregated over that song's snippets. during this process  we maintain two constraints: all snippets from each song must have the same output and no two songs can have the same target  so that each member of Ρ   s is assigned once .  this is a suitable procedure since the specific outputs of the network are not of interest  only the high-entropy distribution of them and the fact that same-song snippets map to the same outputs.     
　by letting the network adapt its outputs in this manner  the outputs across training examples can be effectively reordered to avoid forcing artificial distinctions.  through this process  the outputs are effectively being reordered to perform a weak form of clustering of songs: similar songs are likely to have small hamming distances in target outputs.   more details of a similar dynamic reordering can be found in  caruana et al.  1 .  the training procedure is summarized in figure 1.   figure 1 shows the progress of the training procedure through epochs and compares it to training without output reordering1.  note that without reordering  the smaller network's performance was barely above random: 1/1.  the larger network without reordering performs marginally better  1/1 .  however  both networks trained with output reordering learn a mapping more consistently than networks without reordering  1/1 and 1/1 . 
1 using the outputs as hash-keys 
in this section  we take a first look at how the network's outputs perform when used as hash keys.  there are two metrics that are of primary interest in measuring performance:  1  the number of candidates found in each hashed bin  that is  how many candidates must be considered at each 
select m=1n songs and define b t  = tth binary code 
foreach  song sm  
  for  xm iterations   { add snippet sm x  to training set } 
  select target tm （ {1} n s.t. tm1 = tm1   m1 = m 1 for  idyn iterations  
  train network for edyn epochs  edyn passes through data    foreach  song sm  
    set  am = Σx（xmo sm x  / xm  
      where o sm x  = actual network output for snippet sm x  
  set ut = { t | 1   t ＋ m}  the unassigned binary codes   
  set us = {s | 1   s ＋ m }  the unassigned songs    for  m  iterations  
    find  s  t  = arg min s（us  t（ut  || b t  - as ||1  
    set ts = b t  
    remove s from us  and remove t from ut 
train network for efixed epochs 
 
settings used: 
n=1 m= 1  xm =1   m   edyn =1   idyn =1  efixed=1                                                 figure 1:  training algorithm and paramete r settings used. 
training progress   correct out of 1 bits 

	1	1 1 1 1 1 1
epoch
figure 1:   training with and without output reordering  for small and large networks.   note that without output reordering  the number of correct outputs ~1/1.  with reordering  it approaches 1.    
lookup  indicating how well the candidates are distributed  with respect to queries  and  1  the number of hashed bins that include a snippet from the same song  indicating accuracy . in this section  we use a simple recall criterion:  of the snippets in the hashed bin  are any from the correct song.  note that this is a very loose indicator of performance; in section 1  when we utilize multiple hashes  we will make this constraint much tighter. 
　for these exploratory experiments  we trained 1 networks  1 with 1 hidden units  1 with 1 hidden units  etc  up to 1 with 1 hidden units.  each was then used to hash approximately 1 snippets  drawn from 1 songs with 1 snippets each .  these 1 songs formed our test set for this stage  and were independent from the 1-song training set.  for this test  we removed the query q from the database and determined in which bin it would hash by propagating it through the networks.  after propagation  for each of the 1 network outputs  if the output was greater than the median response of that output  as ascertained from the training set   it was assigned +1; otherwise  it was assigned 1.  the median was chosen as the threshold to ensure that the network has an even distribution of 1/+1 responses. the 1 outputs were treated as a binary number representing the hash bin.   
　figure 1 shows the total number of candidates  both correct and incorrect  found in the hashed bin  on average  as a function of the number of hidden units in the network.   figure 1 also shows the percentage of lookups that resulted in a bin which contained a snippet from the correct song.   to understand figure 1  the raw numbers are important to examine.  looking at the middle of the graph  every query on average was hashed into a bin with approximately 1 other snippets  ~1% of the database .  in ~1% of the retrievals  the hash bin contained a snippet from the correct song.  note that if the hashes were completely random  we would expect to find a snippet from the same song less than 1% of the time.   figure 1 also shows the negative effects of poorer training on the small networks.   returning to figure 1  we see that even with the output reassignment  the small networks   1 hidden   lags behind the large networks   1 
performance vs. hidden units

figure 1:   performance of networks in terms of candidates found in hashed bin and % of queries finding matches from the same song in the bin to which it is hashed. 
hidden   by about 1 bit of accuracy.  the effect of the poorer training is seen in the larger number of candidates in the hashed bin coupled with the small absolute change in recall  from ~1% to ~1%  given by the networks with 1 hidden units  figure 1 .     however  by using only networks with 1 or more hidden units  we can reduce the number of candidates by ~1%  from ~1 to ~1  with minor impact on the recall rate    1% .  
　 in figure 1  we compare the performance of using the outputs of the networks trained with and without output reassignment1  and using networks with random weights.  note that using random weights is not the same as random hashing; by passing snippets through the network  it will be more likely to map similar snippets to the same bin; however  the weightings and combinations of the inputs will not be tuned to the task.  as can be seen in figure 1  for every lookup  there is a higher probability of finding a snippet from the same song using the fully trained networks  over networks with static-output training or with no training.   
1 using the networks in practice 
in the previous section  we analyzed the performance of each of the networks in terms of the average number of candidates in the hashed bins and the accuracy of lookups.  in this section  we combine the outputs of the networks to create a large-scale hashing system.   so far  each of the networks was trained with only 1 outputs  allowing 1 bins in the hash.   a na ve way to scale the hash to a larger number of bins is to train the network with more outputs.  however  because larger networks necessitate more training data  this becomes computationally prohibitive.1 
　instead  we use two facts: that training networks is a randomized process dependent on the initial weights  and that networks with different architectures may learn different mappings.  hence  given that we have already trained multiple networks  we can select bits for our hash from any of the networks' outputs. numerous methods can be used to select which outputs are used for a hash.  we explored three methods of selecting the outputs:  1  random selection   1  mini-
training with and without reordering & random weights

figure 1:   performance of training procedures.  measured by % of queries finding the same song in the bin to which it is hashed.  avg # of snippets examined per bin for the training w/reordering is: 1; for static-output training:  1; and for random networks: 1.    
mum-correlation selection  and  1  minimum-mutualinformation selection.  both of the last two methods are chosen in a greedy manner  chow & liu  1 .  by accounting for mutual information  we explicitly attempt to spread the samples across bins by ensuring that the combined entropy of the hash-index bits remains high.  this method had a slight advantage over the others  so this will be used going forward. 
　we are no longer constrained to using only 1 bits since we can pick an arbitrary number of outputs from the network ensemble for our hash function.  figure 1 shows the performance in terms of the number of candidates in each of the hashed bins and correct matches  as a function of bits that are used for the hash  or equivalently  the number of total hash bins .   figure 1 also shows two other crucial results.  the first is that by selecting the hash bits from multiple networks  we decrease the number of candidates even when using the same number of bins  1 .  the number of candidates in each hashed bin decreased by 1% - from 1  figure 1  to 1  figure 1  column 1 .   second  when increasing the number of bins from 1  1  to 1 1  1   we see a decrease in the number of candidates from 1 to 1 per hashed bin.   although there is also a decrease in the number of correct matches  it is not proportional; it decreases by ~1%  instead of decreasing by 1% as does the number of candidates .   in the next section  we describe how to take advantage of the small number of candidates and regain the loss in matches.  
performance vs. hash bins

figure 1:   as we increase the number of bins by 1〜  the matches decrease by 1%.  meanwhile  the candidates decrease from ~1 to ~1.  
1  a complete hashing system  	original high-dimensional spectrogram representation.  
after passing through the networks  the snippet is repre-
from the previous sections  when a new query  q  arrives  it sented by the quantized binary outputs: the original repreis passed into an ensemble of networks from which select sentation is no longer needed.  outputs are used to determine a single hash location in a 
　　　　　　　　　　　　　　　　　　　　　　　　　　 as can be seen in figure 1a  the top line is the performsingle hash table.  analogous to lsh  we can also generalance of the system with l=1 hashes; with only 1 bins  ize to l hash tables  with l distinct hash functions.   to do the number of candidates is unacceptably large: over 1.   this  we simply select from the unused outputs of the net-
　　　　　　　　　　　　　　　　　　　　　　　　　　with smaller numbers of hashes  shown in the lower lines   work ensemble to build another hash function that indexes the number of candidates decreases.  as expected  looking into another hash table.   now  when q arrives  it is passed towards the right of the graph  as the number of bins inthrough all of the networks. the outputs of the networks  if creases  the number of candidates decreases rapidly for all they have been selected into one of l hashes  are used to of the settings of l considered. determine the bin of that hash.  in figure 1  we experiment 
　　　　　　　　　　　　　　　　　　　　　　　　　　 figure 1b provides the most important results.   for the with the settings for l  1  and # of network outputs per top line  1 hashes   as the number of bins increases  the hash  1:  1 - 1 1 bins per hash . ability to find the best match barely decreases - despite the 
 in the previous section  we measured the recall as being large drop in the number of candidates  figure 1a .  by whether the hashed bin contained a snippet from the correct examining only 1 candidates per query  ~1% of the song.  here  we tighten the definition of success to be the database   we achieve between 1% accuracy  l   1 .   one we use in the final system.  we rank order the snippets 
　　　　　　　　　　　　　　　　　　　　　　　　　　　in figure 1b  note that for the lowest two lines  l=1   as in the database according to how many of the l hash bins the number of bins increases  the accuracy rises - in almost provided by the l hashes contain each snippet.   the top every other case  it decreases.  the increase with l=1 ocranked snippet is the one that occurs most frequently. we curs because there are a large number of ties  many songs declare success if the top snippet comes from the same song have the same support  and we break ties with random seas query q.  note we never perform comparisons in the figure 1:   performance of systems with  l=1..1  hashes  as a function of the number of bins. 
 
a: # of candidates considered  smaller better . 
 
b:  % of times correct song identified  higher better . 
a. unique candidates across all hashes 
shown for system with l=1 sets of hashes
match in top position
b. shown for system with l=1 sets of hashes
lection.  as the number of snippets hashed to the same bin decreases  the correct song competes with fewer incorrect ties and has a higher chance of top rank.  
　finally  in figure 1a  note that as the number of hashes increases  the number of candidates increases almost linearly.  this likely indicates that the hashes are largely independent.  if they were not independent  the number of unique candidates examined would overlap to a much greater degree.   if there was significant repetition of candidates across the hashes  we would see the same  frequenttie  phenomena that we commented on for l=1: the same songs would be repeatedly grouped and we would be unable to distinguish the correct co-occurrences from the accidental ones.  
　in summary  it is interesting to compare this procedure to lsh in terms of how each approach handles hashing approximate matches.   lsh hashes only portions of the input vector to each of its multiple hashes.  intuitively  the goal is to ensure that if the points differ on some of the dimensions  by using only a few dimensions per hash  similar points will still be found in many of the hash bins.  our approach attempts to explicitly learn the similarities and use them to guide the hash function. further  our approach allows similarity to be calculated on  potentially non-linear  transformations of the input rather than directly on the inputs.  the use of multiple hashes in the system used here is to account for the imperfect learning of a difficult similarity functions.   
1    large scale experiments 
in this section  we conduct a large-scale test of the system.   
the trained networks are used to hash 1 1 snippets.  in this experiment  we use 1 songs and 1 snippets from each song.  as before  the snippets are of length 1 sec  1.1-ms slices .   this spacing follows previous studies  ke et al.  1; haitsma & kalker  1 .  our snippets are drawn approximately 1 ms apart.   as with our previous experiments  for every query snippet  we examine how many other snippets are considered as potential matches  the union of the l hashes   and also examine the number of times the top-ranked snippet came from the correct song.   for these experiments  we attempted 1 different parameter settings  varying l and the number of bins per hash.  table i shows the performance for 1 groups of desired numbers of candidates; the first row    1%  indicates that if we want the average query to examine less than 1% of the database  what accuracy we can achieve  and with which parameter settings.   in summary  we can find 1 sec snippets in a database of 1 songs with 1% accuracy by using 1 bins and l=1 sets of hashes - while only examining 1% of the database per query.   
	 	table i: best results for 1 - 1% candidates 
% candidates 
    examined best 1 results     accuracy  1bins  l  sets   top second third   1% 1%  1  1%  1  1%  1  1 - 1% 1%  1  1%  1  1%  1  1 - 1% 1%  1 * 1%  1  1%  1  1 - 1% 1%  1  1%  1  1%  1  1 - 1% 1%  1  1%  1  1% 1   
　beyond looking at the top match entry  figure 1 demonstrates that by examining the top-1 snippets that appeared in the most bins  there is a smooth drop-off in finding the correct song beyond simply examining the single most frequently occurring snippet. because the database contains 1 snippets from each song  multiple snippets can be found for each query.  
　finally  we bring all of our results together and examine the performance of this system in the context of a longer song recognition system. we compare this system with a state-of-the art lsh-based system  baluja & covell  1   which extends  ke et al.  1  by allowing fingerprints to be coarsely sampled to lower the memory required to recognize a large database of songs  while maintaining high recognition rates.  ke's system was an improvement over the de facto standard of  haitsma & kalker  1 .   
　for these experiments  we used s sec of a song  1 ＋ s ＋ 1  and integrated the evidence from individual snippet lookups to determine the correct song.  note that these experiments are harder than the hardest of the cases expected in practice.  figure 1 illustrates why: even though the  baluja & covell  1  system does not sample the database snippets as closely  their sampling of the probe snippets insures a database-to-nearest-probe misalignment of 
1 ms  at most  at each database snippet: at least one of the 
　
figure 1: song prediction for each of the top-1 matches.  here   position 1 = 1%  position 1%  position 1 = 1%. 
a. sampling used in this study 	b. sampling used in  baluja & covell 1  	figure 1:  sampling used to generate 
　
probes will be at least that close  since the probe density is 1 ms.  by removing the matching snippet from the database  our experiments force all database-to-probe misalignments to be a full 1 ms  1 times more than the closest sampling of  baluja & covell  1 . we do not rely on dense probe sampling to guarantee nearby  in-time  matches.  results are shown in table ii. 
table ii: performance vs. query length  in seconds   with sampling shown in figure 1a  
 1s 1s 1s 1s 1s forgiving-hash 1% 1% 1% 1% 1% lsh-system 1% 1% 1% 1% 1%  
　for the forgiving hasher  we use 1 bins & l=1 hashes.  for the lsh system  we set the parameters individually for each trial to maximize performance while allowing approximately equal hash-table lookups.  unlike  baluja & covell 1   we count the candidates returned from each hash-table  not just the candidates that passed the hashvoting thresholds and thereby triggered retrieval of the original fingerprint. we use this metric on the lsh system since under forgiving hashing  there is no retrieval of an original fingerprint. each candidate simply has a counter associated with it; neither the spectrogram  nor a representation of the spectrogram  is ever compared with each candidate.  in summary  for our system  near perfect accuracy at even 1 sec. is achieved while considering only a tiny fraction of the candidates.   
1 conclusions & future work 
we have presented a system that surpasses the state of the art  both in terms of efficiency and accuracy  for retrieval in high-dimensional spaces where similarity is not welldefined. the forgiving hasher ignores the small differences in snippets and maps them to the same bin by learning a similarity function from only weakly labeled positive examples.  the system is designed to work with video and images as well; those experiments are currently underway. 
　beyond further experiments with scale  there are many directions for future work. the number of bits trained per learner should be explored; by training only 1 bit  numerous binary classifiers become usable. to ensure high entropy for 
1	& covell 1   since that system al-
ways encounters many snippets with small probe-database separations.  
unseen songs  training with a song set that  spans  the space is desirable; this may be approximated by training learners with different songs.  currently  the system handles approximate-match retrieval which is similar to handling noise. tests are currently being conducted to quantify the resilience to structured and unstructured noise.  finally  it will be interesting to test forgiving hashing on music-genre identification  where 'similarity' is even less well-defined. 
 
