 
 
many organizations employ lessons learned  ll  processes to collect  analyze  store  and distribute  validated experiential knowledge  lessons  of their members that  when reused  can substantially improve organizational decision processes.  unfortunately  deployed ll systems do not facilitate lesson reuse and  fail to bring lessons to the attention of the users when and where they are needed and applicable  i.e.  they fail to bridge the lesson distribution gap . our approach for solving this problem  named monitored distribution  tightly integrates lesson distribution with these decision processes. we describe a case-based implementation of monitored distribution  alds  in a plan authoring tool suite  hicap . we  evaluate its utility in a simulated military planning domain. our results show that monitored distribution can significantly improve plan evaluation measures for this domain.  
1 introduction 
verified experiential lessons teach improvements about a work practice  fisher et al.  1 .  many large government  e.g.  dod  doe  nasa  and private organizations develop lessons learned  ll  systems to assist with the knowledge management process of collecting  analyzing  storing  distributing  and reusing lessons  davenport and prusak  1; weber et al.  1a .  lessons record tacit experiential knowledge from an organization's employees whose knowledge might be lost when they leave the company  shift projects  retire  or otherwise become unavailable.  it is often crucial to record lessons; lives are sometimes saved by preventing recorded catastrophes from recurring  doe  1 .  thus  sharing lessons  even if they are used infrequently  can be very important.  ll processes and systems are needed to assist with lesson sharing  which can be complicated  especially for large organizations or large lesson databases. 
 
lessons are usually in unstructured text format  and distribution is commonly supported using standalone text or keyword retrieval tools that require users to  pull  lessons from a repository. unfortunately  problems with text representations and with this approach to distribution negatively affect lesson reuse  which results in widespread underutilization  weber et al.  1a .  in particular  they are responsible for what we term the lesson distribution gap. this gap exists when an organization fails to properly promote lesson reuse and available lessons are not deployed when and where they are needed and applicable.  at least three approaches exist to eliminate this gap. first  identified lessons can be incorporated directly into doctrine  which defines the processes to be employed by an organization's members. the doctrine is updated to include the knowledge contained in the lesson.  for example  the army's call center  call  1  deploys teams of lesson analysts and doctrine experts to perform such updates.  however  not all lessons can be incorporated into rule-like doctrine  e.g.  because they may be true exceptions   and not all organizations have close working relations between doctrine and lessons learned personnel.   a second way to bridge this gap involves  pushing  lessons to potential users  such as via list servers  e.g.   sells  1   or intelligent spiders. for example  two of the doe's sites already employ portals containing spiders  sells  1 .  however  spiders are not integrated with the decision support processes that the lessons target.  thus  after retrieving lessons with a spider  users must characterize the situations for which they are useful  recall them when they encounter an applicable decision support context  and interpret them correctly so that they are properly reused.  these are challenging tasks  requiring a high level of expertise and time that most users do not have.  we investigate a third approach to bridging the lesson distribution gap that involves tightly integrating the lesson repository with a decision support tool.  our approach  detailed in section 1  requires inserting a monitor into the decision support process so that it can determine when a
decision 
figure  1. most lessons learned processes are separated from the decision processes they support. 

figure  1. monitored lesson distribution integrates the lessons learned process with lesson-targeted decision processes. 

lesson's conditions are well matched by a decision context. in section 1  we describe a case-based implementation of this monitored distribution approach in alds  active lesson delivery system   a module of the hicap plan authoring tool suite  mu oz-avila et al.  1 . we describe alds's evaluation in section 1  where we provide evidence that it can significantly improve performance measures for hicap-generated plans for a simulated military planning domain  i.e.  noncombatant evacuation operations  neos  .    based on a recent survey  weber et al.  1a  and analysis of the aaai'1 workshop on intelligent lessons learned systems  aha and weber  1   we believe that monitored distribution is novel with respect to deployed ll systems  and has great potential for deployment.  we discuss the implications of our findings and future research issues in section 1. 
1 monitored lessons learned processes 
a lesson is a knowledge artifact that represents a validated  i.e.  factually and technically correct  distillation of a person's experience  either positive or negative  that  if reused by others in their organization  could significantly improve a process in that organization.  in particular  it identifies a specific design  process  or decision that reduces or eliminates the potential for failures and mishaps  or reinforces a positive result  secchi et al.  1 . the knowledge management process involving lessons  i.e.  the lessons learned process  llp   implements strategies for collecting  analyzing  storing  distributing  and reusing a repository of lessons to continually support an organization's goals. 
 llps typically target decision-making or execution processes for various types of user groups  i.e.  managerial  technical  and organizations  e.g.  commercial  military . in this paper  we focus on managing lessons to support planning processes. 
 flowcharts describing llps abound; organizations produce them to communicate how lessons are to be collected  analyzed  and distributed  sells  1; fisher et al.  1; secchi  1 . figure 1 displays a typical llp  composed of the five sub-processes mentioned above  where reuse does not take place in the same environment as the other sub-processes.  
existing  deployed ll systems do not support all processes in a llp. in particular  organizations typically do not develop software to support verification or reuse. instead  they use electronic submission forms to facilitate lesson collection  and use a standalone retrieval tool for lesson distribution  weber et al.  1a .  users interacting with this standalone tool are expected to browse the stored lessons  studying some that can assist them with their decision-making processes. however  based on our interviews and discussions with members of several ll organizations  e.g.  in the navy  joint warfighting center  department of energy  and nasa   and many intended users  we found that they do not use available standalone ll systems  which are usually ineffective because  1  they force users to master a separate process from the one they are addressing  and  1  they impose the following unrealistic assumptions: 
 
  users are convinced that using an ll system is beneficial  e.g.  contain relevant lessons . 
  users have the time and skills to successfully retrieve relevant lessons. 
  users can correctly interpret retrieved lessons and apply them successfully. 
  users are reminded of the potential utility of lessons when needed. 
 
we believe that lessons should be shared when and where they are applicable  thus promoting their reuse. this motivated us to develop an architecture for proactive  integrated lesson distribution  figure 1 . in this monitored distribution approach  reuse occurs in the same environment as other sub-processes; the decision process and llp are in the same context.  this embedded architecture has the following characteristics/implications: 
  the llp interacts directly with the targeted decisionmaking processes  and users do not need to know that the ll module exists nor learn how to use it. 
  users perform or plan their decision-making process using a software tool. 
  lessons are brought to the user's attention by an embedded ll module in the decision-making environment of the user's decision support tool. 
  a lesson is suggested to the user only if it is applicable to the user's current decision-making task and if its conditions are similar to the current conditions. 
  the lesson may be applied automatically to the targeted process. 
this process shifts the burden of lesson distribution from a user to the software  but requires an intelligent  monitoring  module to determine whether/when a lesson should be brought to a decision maker's attention. 
1 implementation 
we implemented the monitored distribution process in alds  a module of hicap  mu oz-avila et al.  1 .  this section details hicap and then alds. 
1 	plan authoring using hicap 
hicap  hierarchical interactive case-based architecture for planning  is a multi-modal reasoning system that helps users to refine a planning hierarchy  mu oz-avila et al.  1 . a hierarchy is represented as a triple h = {t   :}  where t is a set of tasks    defines a  partial  ordering relation on t  and t1:t1 means that t1 is a parent of t1 in t. task hierarchies are created in the context of a state s={ q a +}  represented as a set of  question answer  pairs. 
 hicap provides three ways to refine tasks into subtasks. first  it supports manual task decomposition. second  users can decompose a selected task using hicap's interactive case retriever  nacodae/htn   which involves iteratively answering prompted questions that refer to state variables. third  users can select a generative planner  shop  to automatically decompose t.   
1 	monitored lesson distribution using alds 
planning tasks  e.g.  for military operations  involve several decisions whose affect on plan performance variables  e.g.  execution time  depends on a variety of state variables  e.g.  available friendly forces .  without a complete domain theory  hicap cannot be guaranteed to produce a correct plan for all possible states.  however  obtaining a complete domain theory is often difficult  if not impossible. in addition to representing typical experiential knowledge  lessons can help fill gaps in a domain theory so that  when reused appropriately during planning  they can improve plan performance.  this is the motivation for applying lessons while using hicap. 
 figure 1 summarizes the behavior of alds  the monitored distribution module.  alds monitors task selections  decompositions  and state conditions to assess similarities between them and the stored lessons. when a stored lesson's applicable decision matches the current decision and its conditions are a good match with the current state  then the lesson is brought to the user's attention to influence decision-making. when a user implements a prompted lesson's task decomposition  i.e.  reusing the lesson   the current task hierarchy is modified appropriately.  
abstractly  reusable lessons contain indexing and reuse components. indexing components include the target task and the lesson's applicability conditions.  the reuse components include a suggestion that defines how to reuse an experience and an explanation that records how the lesson was learned.  this explanation can be used to justify the lesson's use in a new situation.   in alds  a lesson is indexed by the  target  task that it can modify and a set of  question answer  pairs defining its applicability conditions  and contains a suggestion  e.g.  a task substitution  and the lesson's originating event  i.e.  the explanation .   
 we use a case-based approach for lesson distribution primarily because the indexing components  i.e.  task and conditions  must support a partial matching capability. furthermore  the applicability of a lesson depends on the context of the task that it targets  which suggests using domain-specific similarity functions. 
 thus  if both the task and the conditions are a  good  match to the current planning state  then the user should consider decomposing the current task into the lesson's suggested subtasks. we borrowed nacodae/htn's similarity function for cases  and used a thresholded version to define  good   i.e.  determine when a lesson should be prompted to a user .  

figure 1. hicap's lessons distribution sub-process  implemented in alds  during plan elaboration. 
1 evaluation 
we wanted to evaluate the hypothesis that the monitored distribution approach  e.g.  as implemented in alds  is superior to the traditional standalone approach for lesson distribution and promoting lesson reuse. for hicap/alds  this hypothesis requires evaluating the plans created by operational users who use the two lesson distribution approaches in repeated planning tasks.  dependent variables would include agreed-upon measures of plan quality  which depend on the planning domain. 
 	unfortunately  	hicap/alds 	has 	not 	yet 	been 
scheduled for testing in a military training exercise  which prevents us from working with operational planners.  therefore  we instead performed an evaluation using simulated users on a simulated neo  noncombatant evacuation operations  domain.  sophisticated full-scale neo simulators do not yet exist.  therefore  we constructed our own plan evaluator for a simulated neo domain  section 1 .  this allowed us to evaluate hicap/alds's plan authoring and lesson distribution capability for an entire plan  rather than be limited to an evaluation on a single task decomposition task  mu oz-avila et al.  1 .  simulating how a user might benefit from a standalone lesson distribution tool is difficult. therefore  we instead compared plan generation when using alds vs. not using it  section 1   where our revised hypothesis is that using lessons will improve plan quality.  this central hypothesis to llps  although simple  has not been previously investigated for lessons learned systems  and thus is appropriate for an initial evaluation focus. 
1 	methodology 
the plans authored by hicap concerned performing a rescue mission where troops are grouped and move between an initial location  the assembly point  and the neo site  where the evacuees are located   followed by evacuee relocation to a safe haven. 1 possible routes and 1 means of transportation were encoded.  in addition  other conditions were determined during planning such as whether a communications expert was available and the method for processing evacuees.  hicap's plans had 1 steps  and its knowledge base included 1 operators  1 methods  and 1 cases. we randomly selected 1 initial plan states  1 independent variables  and produced plans for each state with the simulated user interacting with hicap.  this user assigned  through task decomposition  an additional 1 variables  with from one to four values each  for each plan  which required hicap an average of about 1 seconds to generate.  the same set of initial states to produce plans in hicap was used  to guide task decomposition  both with and without lessons. each of the two sets of 1 plans  i.e.  one set obtained using lessons  and the second set obtained without using lessons  authored by hicap was input to the evaluator  section 1 . due to the non-deterministic behavior of the evaluator  we executed each plan ten times. 
 the version of hicap used in this paper is deterministic; given a state and a top-level goal  i.e.  perform a neo   it will always generate the same plan. a simulated user interacts with hicap by choosing task decompositions to generate a plan  using the process shown in figure 1. in nacodae/htn conversations  it always answers the top-ranking displayed question for which it has an answer  and it answered questions until either none remained unanswered or until one of the solutions exceeded a retrieval threshold  which we set to 1%. 
 we selected 1 lessons for our experiment  representing a subset of approximately 1 neo-related lessons from the active navy lesson repository  containing 1 lessons  from the november 1 copy of the unclassified navy lessons learned system.  these were selected according to their relevance to neos and their clarity  so that we could recognize their relation to the plans authored using hicap. for example  one lesson was defined as: 
 
task: standard medical inventory  
  applicability conditions:   q a  pairs  
  is the medical inventory of standard size or is it standard minus 1  yes 
  is the climate tropical   yes 
suggestion: add 1 to the medical inventory 
1 	plan evaluation 
we built a stochastic evaluator for neo plans that take into account general knowledge of the neo domain and computes the performance measures  described below . this evaluator is not a simulator because it does not use specific distributions for each type of event  but simply computes  according to a uniform distribution  what are the expected consequences of some choices in building a plan  i.e.  the causal chain of events that are generated by these choices will influence each of the dependent variables differently . we built the evaluator and the hicap knowledge base for mock neos based on available applicable lessons.  we defined plan quality based on official measures of neos  which are planning domain dependent. these measures are defined in the universal naval task list  untl  1  under measures of performance suggested for joint and naval tasks. these measures primarily concern execution duration and casualty rates. to avoid a redundant evaluation  we have selected one measure for total duration of the operation  one for duration until evacuees receive medical assistance  and the percentage of casualties among evacuees  friendly forces  and enemies.  these summarize the most important aspects suggested in the untl.  we defined bounds for variables based on actual neos. for example  we limited the percentage of casualties that occurred after a severe enemy attack takes place. enemy attacks will only be possible in two planning segments  out of a total of five segments  and their likelihood increases when users choose land transportation and decreases when weather is troublesome. there is a small chance of a crash when helicopters are used that increases if the weather is not favorable; the resulting number of casualties is proportional to the number of passengers in each aircraft. a long planning segment flown by helicopter will have added the time and risks associated with in-flight refueling  e.g.   siegel  1  . 
1 	results 
as summarized in table 1  alds using lessons substantially improved the first four of five performance variables.  a brief examination of the results  i.e.  the first run for each of the 1 plans   using a standard student's t test  revealed significant differences for both overall duration  p 1  t=1  df=1  and duration until medical assistance arrived  p 1  t=1 .  all lessons were used in generated plans  and an average of approximately three lessons were used per plan. 
table 1. experimental results with the 1 plans. 
 without lessons  with lessons % reduction with lessons  mean duration 1 1 1 s.d. 1 1 - mean duration  until medical asst. 1 1 1 s.d. 1 1 - mean % casualties:  
to evacuees 1 1 1  to friendly forces 1 1 1 to enemies 1 1 -1  
 the significance of an overall reduction of 1% in the percentage of casualties among evacuees was estimated in each plan based on the parameter number of evacuees  which was randomly set to dozens  hundreds  or thousands.  based on the number of evacuees selected for these simulated neos  using the lessons reduced the average number of casualties by 1  from 1 to 1.  
 these results suggest that the monitored distribution approach can potentially generate better plans for realistic problem domains  e.g.  planning for neo operations . however  the experimental conditions were designed so that lessons were available for a reasonable percentage of the generated plans  and thus could be prompted to the simulated hicap user so that  when applied  they could improve plan quality  with high probability . nonetheless  we expect that similar improvements may yield benefits in plans for domains where safety issues and speed are paramount to success.   
 the capabilities of certain learning algorithms can be evaluated by varying dataset characteristics to determine when certain learning algorithms can be expected to perform well  e.g.   aha  1  .  similarly  we plan to characterize the set of experimental conditions for which alds can use lessons to significantly improve plan evaluation performance measures. 
1 discussion 
this paper proposes a technology  i.e.  case-based reasoning  solution to part of a knowledge management  km  problem  i.e.  managing lessons learned .  however  km problems typically require challenging organizational dynamics issues  and these require precedence in the context of bridging the lesson distribution gap.  thus  monitored distribution can at most play only one part of a much larger solution. 
 our evaluation of alds demonstrates how monitored distribution  when embedded in a decision-making  i.e.  planning  process  can improve the results of that process. although we used simulated users in our experiments to reduce human biases during the evaluation  we stress that this is a mixed-initiative approach  where humans interact with hicap to generate plans.  the unique aspect of alds is that it allows users to execute a lesson's suggestion  i.e.  here  a task substitution   rather than limit them to simply browsing the suggestion.   
 hicap's nacodae/htn module manipulates cases that represent task decompositions corresponding to either standard operating procedures or decompositions that were derived from decision making during training exercises and actual operations. in contrast  alds manipulates lessons that capture experiences that  if reused  can significantly improve the performance of subsequent plans.  unlike cases  lessons are not conceptually limited to representing task decompositions  but can be used to apply edits to any of hicap's objects  e.g.  resource assignments  resources  task durations .  
 	several workshops  e.g.  organized by the department of
energy  the european space agency  the joint warfighting center  and each branch of the armed services  have now taken place on the topic of lessons learned. however  few efforts on lessons learned systems have examined the potential utility of ai  e.g.  vandeville and shaikh  briefly mention using fuzzy set theory to analyze elicited lessons   and there is a lack of closely related work to monitored distribution.  however  one recent workshop brought attention to this area from an ai perspective  aha and weber  1   and a few of its contributors touched on issues related to proactive lesson distribution.  for example  leake et al.'s  calvin system implements a taskoriented llp that collects lessons about research topics and research results with an active distribution sub-process. like alds  calvin prompts users with suggestions  i.e.  alternative www pages to browse  that can be immediately executed.  however  while calvin focuses on a diagnosis task  alds operates in the context of a synthesis task  i.e.  planning   and can potentially update any of the planning scenario's objects. like both of these systems  watson  also describes a case retrieval system  in this case for extending cool air to distribute trouble tickets.  however  cool air does not operate in a mixed-initiative setting. some km approaches  reimer et al.  1; abecker et al.  1  also target distribution in the context of organizational knowledge  but use formats that do not support indexing.  
 several limitations exist concerning our approach and its implementation in alds.  for example  lessons can be complex  and suggest changes to a variety of objects in the planning scenario.  although hicap represents several such objects  e.g.  resources  resource assignments   it is currently limited to processing only task substitution lessons.  in future implementations of hicap and alds  lessons will be able to represent suggestions that  when applied  will not be limited to task substitution.  for example  a lesson might suggest a task decomposition  or using an alternative resource assignment for a given task  recommend changing some temporal orderings of tasks  or suggest edits to any of the objects used by hicap to define plans. 
 in addition  to be useful  our approach assumes that the decision processes targeted by the lessons are managed by a software tool  thus allowing integration with alds.  furthermore  our approach requires identifying each lesson's indexing and reuse components  which requires significant knowledge engineering effort.  we are currently developing lesson collection tools that reduce this effort.  weber et al.  1b  describe interactive elicitation approaches that use taxonomies to guide lesson collection to populate alds's lesson repository.  
 in future work  we will conduct subject experiments that compare the monitored distribution approach vs. traditional keyword search tools for lesson distribution.  we will also demonstrate how monitored distribution is not restricted to planning tasks. 
1 summary 
we identified a problem with distributing lessons  called the lesson distribution gap  which is crucial to many lessons learned organizations.  to address this problem  we introduced an approach called monitored distribution  which is characterized by a tight integration with a decision support tool that manages processes that the lessons can potentially improve.  we implemented this approach in alds  a case retrieval system  and evaluated its capability in the context of a module for hicap  a plan authoring tool. our experiments with a simulated military planning domain  i.e.  for noncombatant evacuation operations  showed that  by using lessons  monitored distribution can help to significantly improve plan performance measures.  in summary  we demonstrated a technology that brings lessons to the attention of users when and where they are needed and applicable. 
acknowledgements  
this research was supported by the office of naval research and darpa.   
references 
 abecker et al.  1  abecker  a.; bernardi  a.; hinkelmann  k.; kuehn  o.; and sintek  m. contextaware  proactive delivery of task-specific information: the knowmore project. information systems frontiers 1/1  1  1.  
 aha  1  aha  d.w. generalizing case studies: a case study. proceedings of the ninth international conference on machine learning  pp. 1 . aberdeen: morgan kaufmann  1. 
 aha and weber  1  aha  d.w.  and weber  r.  eds. . intelligent lessons learned systems: papers from the aaai workshop  technical report ws-1 . menlo park  ca: aaai press  1. 
 call  1  center for army lessons learned: virtual research library. http://call.army.mil. 
 davenport and prisak  1  davenport  t.h.  and prusak  l. working knowledge: how organizations manage what they know.  boston  ma: harvard business school press  1. 
 doe  1  doe  the doe corporate lessons learned program  technical report doe-std-1 .  
washington  dc: u.s. department of energy. 
 fisher et al.  1  fisher  d.  deshpande  s.  & 
livingston  j. modeling the lessons learned process 
 research report 1 . albequerque  nm: the university of new mexico  department of civil engineering  1. 
 leake et al.  1  leake  d.b.  bauer  t.  maguitman  a.  and wilson  d.c. capture  storage  and reuse of lessons about information resources: supporting task-based information search.  in  aha & weber  1 . 
 mu oz-avila et al.  1  mu oz-avila  h.  aha  d.w.  breslow  l.a.  & nau  d. hicap: an interactive casebased planning architecture and its application to neos. proceedings of the eleventh conference on innovative applications of artificial intelligence  pp. 1 .  
orlando  fl: aaai press  1. 
 reimer et al  1  reimer  u.  margelisch  a.  staudt  m. a knowledge-based approach to support business 
processes  in: proceedings of the aaai 1 spring symposium series: bringing knowledge to business processes  stanford  ca  march 1.  
 secchi et al.  1  secchi  p.; ciaschi  r.; spence  d. the esa alert system. in p. secchi  ed.  proceedings of alerts and lessons learned: an effective way to prevent failures and problems  technical report wpp-1 . noordwijk  the netherlands: estec  1.   sells  1  sells proceedings of the sells fall meeting.  
 tis.eh.doe.gov/ll/proceedings/proceedings1.htm   www.estec.esa.nl/confannoun/1   fall  1. 
 siegel  1  siegel  a. eastern exit: the noncombatant evacuation operation  neo  from mogadishu  somalia  in january 1. center for naval analyses  1.  untl  1  untl universal naval task list  opnavinst 1 . arlington  va: navy modeling and simulation office  september  1. 
 vandeville and shaikh  1  deville  j.v. and shaikh  m. 
a. a structured approximate reasoning-based approach for gathering  lessons learned  information from system development projects. systems engineering  1   1  1. 
 watson  1  watson  i. lessons learned during hvac installation.  in  aha & weber  1 . 
 weber et al.  1a  weber  r.  aha  d.w.  and becerrafernandez  i. intelligent lessons learned systems.  to appear in expert systems with applications  1   1. 
 weber et al.  1b   weber  r.  aha  d.w.  sandhu  n.  & mu oz-avila  h.  1 . a textual case-based 
reasoning framework for knowledge management 
applications. professionelles wissenmanagement erfahrungen und visionen  1. aachen:shaker verlag. 
minimizing dialog length in interactive case-based reasoning
david mcsherry  
school of information and software engineering
 university of ulster  coleraine bt1sa
northern irelandabstract
decision trees induced from stored cases are increasingly used to guide case retrieval in casebased reasoning  cbr  systems for fault diagnosis and product recommendation. in this paper  we refer to such a decision tree as an identification tree when  as often in practice  each of the faults to be identified  or available products  is represented by a single case in the case library. we evaluate common splitting criteria for decision trees in the special case of identification trees. we present simplified versions of those that are most effective in reducing the average path length of an identification tree  or equivalently  the average number of questions asked when the tree is used for problem solving. we also identify conditions in which no such reduction is possible with any splitting criterion.
1   introduction
interactive trouble-shooting  help-desk support  and internet commerce represent the majority of fielded applications of case-based reasoning  cbr   aha  1; bergmann et al.  1; watson  1 . increasingly  decision trees induced from stored cases are used to guide case retrieval with the aim of minimizing the number of questions required to identify a fault  or a product that meets the requirements of the user. a problem-solving session takes the form of an interactive dialog in which questions are presented in the order determined by the decision tree and the user's answers. algorithms for building decision trees are usually evaluated in terms of predictive accuracy  whereas performance measures like efficiency and precision of retrieval are often more relevant in interactive cbr  aha and breslow  1; breslow and aha  1; doyle and cunningham  1 .
   the ability to solve problems by asking a small number of focused questions has been a major factor in the success of help-desk applications of cbr  watson  1 . in on-line decision guides  minimizing the length of productselection dialogs is important not only to avoid frustration for the user but also to reduce network traffic  doyle and cunningham  1 . improving the clarity of explanations is another good reason for minimizing the length of problem-solving dialogs  without sacrificing solution quality  in interactive cbr  breslow and aha  1 .
   we will refer to the task of a cbr system as an identification task when  as often in practice  each of the faults to be identified  or available products  is represented by a single case in the case library. a decision tree for case retrieval in an identification task will be called an identification tree. some researchers have questioned the usefulness of algorithms for building decision trees when instances in the data set are unlabelled; that is  not grouped according to the outcome class to which they belong  aha and breslow  1; doyle and cunningham  1 . however  an identification task can be regarded as a classification task in which each fault or product to be identified is a distinct outcome class. so  in principle  there is no reason why algorithms for top-down induction of decision trees cannot be used for building identification trees.
   most decision-tree algorithms  though  are designed for data sets with relatively small numbers of outcome classes  each of which is usually represented by many instances in the data set. in contrast  a data set for an identification task is irreducible in the sense that the deletion of a single instance means that the corresponding outcome class is no longer represented in the data set. an important question  therefore  is how the performance of decision-tree algorithms is affected by irreducibility in the data set.
   most algorithms for building decision trees differ in the criterion used to split the data set at a given node. in this paper  we evaluate splitting criteria such as the information gain criterion from id1  quinlan  1  and the gini criterion from cart  breiman et al.  1  when used to build identification trees from irreducible data sets. the performance measure on which we focus is the average path length of the identification tree  or equivalently  the average number of questions asked when the tree is used for problem solving.
   in section 1  we show that a substantial reduction in average path length is possible in an irreducible data set

year = 1
displacement = 1 to 1 : mercury capri 1
  	displacement = 1 to 1 : plymouth satellite custom   	displacement = 1 to 1 : chevrolet chevelle malibu   	displacement = 1 or more : pontiac safari  sw  year = 1 : oldsmobile vista cruiser
 	year = 1
  	acceleration = 1 to 1 : opel manta
  acceleration = 1 to 1 : audi fox  year = 1
  	acceleration = 1 to 1 : volkswagen rabbit   	acceleration = 1 to 1 : peugeot 1   	year = 1 : ford granada ghia   
 	year = 1 : ford mustang ii 1  	year = 1
  	mpg = 1 to 1 : ford futura    	mpg = 1 to 1 : buick century special    	mpg = 1 to 1 : dodge omni  	year = 1
  	displacement = 1 to 1 : dodge st. regis  
  	displacement = 1 to 1 : cadillac eldorado
 	year = 1
  	origin = 1 : chevrolet citation
  	origin = 1 : audi 1s  diesel   	year = 1
  	horsepower = 1 to 1 : plymouth champ   	horsepower = 1 to 1 : oldsmobile cutlass ls

figure 1.  an identification tree for 1 automobiles selected at random from the autompg data set.
with information gain  infogain  as the splitting criterion. we also identify conditions in which no such reduction is possible with any splitting criterion. in section 1  we present simplified versions of the infogain and gini criteria for irreducible data sets  and identify another splitting criterion that can never discriminate between attributes in an irreducible data set. in section 1  we present an empirical comparison of splitting criteria for identification trees. we discuss related work in section 1  and present our conclusions in section 1.
1   irreducible data sets
whether a given data set is irreducible depends on which attribute is defined as the class attribute. for example  the class attribute in the autompg data set from the uci repository  blake and merz  1  is normally miles per gallon  which is not unique for every instance. however  with the identity of each automobile as the class attribute  the data set is irreducible. most of the attributes in the data set  such as year  acceleration and miles per gallon  are features that one would expect to see in a cbr system for recommending previously-owned automobiles.
   there are missing values for only one attribute  namely horsepower. to simplify our comparison of splitting criteria  the six instances that have missing values for this attribute were omitted from the data set in our experiments.
however  we shall return to the issue of missing values in our discussion of related work. continuous attributes in the data set were discretised by dividing their ranges into intervals that seemed most natural for the expression of user preferences. there is a trade-off in this process between preserving the ability of the attributes to discriminate between the available alternatives and limiting the number of options from which the user is required to select.
   following the discretisation of continuous attributes  the numbers of values of attributes in the data set are as shown below.
year	1 mpg  displacement  weight 1 cylinders  horsepower      1 acceleration                  1 origin        	1
   although most instances in the data set can be uniquely identified by these attributes  some have the same values for all the attributes. a possible solution to this problem  which is not uncommon in product data  is to present the user with a list of recommended products when no attribute can distinguish between those that remain  doyle and cunningham  1 .
   figure 1 shows an identification tree for 1 automobiles selected at random from the 1 instances that have no missing values in the autompg data set. a point we would like to emphasize is that the values of an attribute in a given subset of a data set are those that are actually represented in the subset. for example  some values of the attributes in the autompg data set  such as year = 1  are not represented in the subset from which the example identification tree was constructed.  similarly  displacement has only two values in the smaller subset with year = 1.
   the example identification tree was constructed with infogain as the splitting criterion. the average path length is only 1 compared with 1 for an identification tree constructed with attributes selected purely at random. on the other hand  it can be seen from the following proposition that no splitting criterion can reduce the average path length of an identification tree for a data set that is both irreducible and complete. a data set is complete if every combination of attribute values is represented in the data set  cendrowska  1 .
proposition 1  in any identification tree for a data set that is both irreducible and complete  the path length required to identify a given instance is never less than the number of attributes in the data set.  
   however  real-world data sets are seldom complete. for example  the instances in the autompg data set represent less than 1 in 1 of the possible combinations of attribute values.  it is also worth noting that the problem does not usually arise in data sets that are not irreducible. for example  cendrowska's  contact lens data set is complete  but only a single test is required to reach one of the leaf nodes in the decision tree induced from the data set with infogain as the splitting criterion.
with single instances at their leaf nodes  identification
trees tend to be larger than decision trees for classification tasks. for data sets of realistic size  it is seldom feasible to identify an optimal tree by exhaustive search. the worstcase scenario occurs when the data set is complete  although in this case the average path length is always the same by proposition 1. the number of possible trees is more easily determined if all attributes have the same number of values.
proposition 1  for a data set that is both irreducible and complete  and has k attributes each with r values  the
 k! r
number of possible identification trees is 	r 1   k
1   splitting criteria for identification trees
measures used to define splitting criteria for building decision trees include the entropy function:
 ¡Æq j logq j
j
and the gini index:
1 ¡Æq1j
j
where q1  q1 ...  qm are the proportions of the outcome classes in the data set. the infogain criterion  quinlan  1  selects the attribute that maximizes the expected gain in information  or reduction in entropy  when used to split the data set. similarly  the gini criterion  breiman et al.  1  selects the attribute that maximizes the expected reduction in the gini index.
1  the gain ratio criterion
to compensate for its bias towards attributes that have most values  the infogain criterion was replaced in later versions of id1 and in c1 by the gain ratio criterion  quinlan  1   in which the information gain for an attribute  with at least average information gain  is divided by:
 ¡Æpi log pi
i
where p1  p1 ...  pr are the proportions of the attribute's values in the data set. as the following theorem and corollary show  all attributes in an irreducible data set are equally good according to the gain ratio criterion. gain ratio can therefore be eliminated from consideration as a basis for attribute selection in an irreducible data set.
theorem 1  for any attribute in an irreducible data set  the expected information gain is:
 ¡Æ pi log pi
i
where p1  p1 ...  pr are the proportions of the attribute's values in the data set.
proof  since each outcome class is represented by a single instance  the entropy of the data set is:
 ¡Æ 1 log 1 = logn n	n
where n is the number of instances in the data set. similarly  the entropy of any subset of size k is logk. the expected information gain when an attribute is used to split the data set is therefore:
logn  ¡Æ pi logki = logn  ¡Æ pi  log pi +logn  = ¡Æ pi log pi
	i	i	i
where k1  k1 ...  kr are the frequencies of the attribute's values in the data set.
corollary  the gain ratio is the same for any attribute in an irreducible data set.
1  the infogain criterion
theorem 1 provides a simplified version of the infogain criterion for irreducible data sets. as theorem 1 shows  it can alternatively be expressed in terms of the frequencies of an attribute's values in the data set.
theorem 1  in an irreducible data set  the infogain criterion is equivalent to selecting the attribute for which:
¡Æ
ki logki
i
is minimum  where k1  k1 ...  kr are the frequencies of the attribute's values in the data set.
proof it follows from theorem 1 that the expected information gain for any attribute in an irreducible data set is:
 ¡Æpi log pi = ¡Æki  logki  logn  =  1 ¡Æki logki + logn i	i n	n i
where n is the size of the data set and p1  p1 ...  pr are the proportions of the attribute's values in the data set.
1  the gini criterion
as the following theorem shows  the gini criterion is equivalent in an irreducible data set to selecting the attribute with the largest number of values.
theorem 1  for any attribute in an irreducible data set  the r 1
expected reduction in the gini index is	  where n is the n
size of the data set and r is the number of values of the attribute.
proof the gini index for the data set is 1 ¡Æ 1 = 1  .
1
similarly  the gini index for a subset of size k is 1   1 . the k
expected reduction in the gini index when an attribute is used to split the data set is therefore:
 1  1    1 ¡Æki  1  1   =1  1  1+ r = r  1 n n i ki n n n
where k1  k1 ...  kr are the frequencies of the attribute's values in the data set.
 at first sight  it may seem to follow from theorem 1 that the gini criterion can be of no use as a basis for attribute selection in an irreducible data set if all attributes in the data set have the same number of values. however  as noted in section 1  the number of represented values of an attribute decreases as the data set is partitioned. the gini criterion does therefore discriminate between attributes that initially have equal numbers of values. in an irreducible data set in which all attributes are binary  the gini criterion is equivalent to selecting any attribute that is capable of splitting the current subset of the data set; that is  does not have the same value for every instance in the current subset. as we show in section 1  this simple criterion  which we call reduce  is much better than selecting attributes purely at random. in the latter strategy  the effect of selecting an attribute for which all instances have the same value is to increase the path length without reducing the size of the data set.
 1   comparison of splitting criteria
we now present an empirical comparison of infogain and gini as splitting criteria for building identification trees from irreducible data sets. our experiments are based on the simplified versions of infogain and gini presented in section 1. a baseline for their evaluation is provided by a tree-building strategy in which attributes are selected purely at random. also included in our evaluation is the reduce criterion which is equivalent to gini for irreducible data sets in which all attributes are binary. the strategies to be compared are:
infogain:	select the attribute for which ¡Æki logki is minimum 
i
where k1  k1 ...  kr are the frequencies of the attribute's values in the current subset
gini: select the attribute that has most distinct values in the current subset
reduce: select any attribute that does not have the same value for every instance in the current subset
random:	select an attribute purely at random
our algorithm for building identification trees  called identify  works in a similar way to algorithms for top-down induction of decision trees and can be used with any splitting criterion. it recursively partitions the data set until a subset is reached that contains only a single instance  or none of the remaining attributes can discriminate between the instances in the current subset  or all attributes have been used in the random strategy.
1  the autompg data set
our first experiment compares the performance of the splitting criteria on randomly generated subsets of the autompg data set ranging in size from 1 to 1 per cent. as noted in section 1  the data set is itself only a small subset of the complete data set consisting of all combinations of the attribute values.

figure 1. comparison of infogain  random and reduce on subsets of the autompg data set.
   average path lengths over 1 repeated trials for each sample size are shown in figure 1.  the results for gini are not shown because they differ only slightly from those for infogain. while infogain performed slightly better than gini for all sample sizes  its reduction in average path length relative to gini was never more than one per cent. on the other hand  the reduce criterion can be seen to provide a considerable reduction in average path length compared with selecting attributes purely at random. the additional reduction provided by infogain  though relatively small  is consistent over the range of sample sizes. while the average path length of the infogain trees increases as sample size increases  their efficiency is apparent even for the 1 per cent sample.
1  which criterion is best for binary attributes 
our second experiment compares the performance of infogain and gini on irreducible data sets in which all attributes are binary. as noted in section 1  gini is equivalent in such data sets to the reduce criterion. the data sets used in this experiment are randomly generated subsets of a complete  irreducible data set with 1 binary attributes. the class attribute is an integer that uniquely identifies each of the 1 instances in the data set. average path lengths for infogain and gini over 1 repeated trials are shown in figure 1 for subsets of the complete data set ranging in size from 1 to 1 per cent.

sample size  % 
figure 1. comparison of infogain  gini and random on subsets of a complete data set with 1 binary attributes.
   the effectiveness of both splitting criteria can be seen to decrease as sample size increases  with little reduction in average path length for sample sizes greater than 1 per cent. as predicted by our theoretical results  there is no reduction in average path length for the 1 per cent sample. however  for sample sizes less than 1 per cent  infogain now gives a noticeable improvement in comparison with gini. the ability of gini to almost match the performance of infogain on subsets of the autompg data set may therefore be attributable to the absence of binary attributes in the data set.
1  how close to optimal is infogain 
we can attempt to answer this question only for data sets in which the number of attributes is small enough for optimal trees to be identified by exhaustive search.  the data sets in our third experiment are randomly generated subsets of a complete  irreducible data set in which each of 1 attributes has 1 values. by proposition 1  the number of possible trees for any such data set is at most 1.
   figure 1 compares the average path length of infogain trees and optimal trees for subsets ranging in size from 1 to 1 per cent of the complete 1 ¡Á 1 ¡Á 1 data set. based on averages over ten repeated trials  the results show that the infogain trees are close to optimal. of the 1 infogain trees constructed in the experiment  1 were optimal.
1   related work
doyle and cunningham  applied a clustering algorithm to product data before using infogain to construct a decision tree. as shown by our results  however  infogain is often very effective in reducing average path length when applied directly to irreducible data sets. doyle and cunningham used a demand-driven or lazy approach to decision-tree induction  adapted from  smyth and
cunningham  1   in which an explicit decision tree is not constructed. instead  the user's answers are used to construct a single decision path. one advantage is that the user can select the attribute they consider most important instead of the one considered most useful by the system. in fault diagnosis  another advantage of a demand-driven approach to decision-tree induction is the system's ability to select the next most useful attribute to partition the case library when the user is unable to answer the most useful question  mcsherry  1 . a demand-driven approach to the construction of identification trees is expected to provide similar benefits in a future version of identify. the effects of incomplete data on retrieval performance  however  remain to be investigated.

sample size  % 
figure 1. comparison of infogain trees and optimal trees for subsets of a complete 1 ¡Á 1 ¡Á 1 data set.
   aha and breslow  used decision trees as an intermediate representation to improve the quality of conversational case libraries. in conversational cbr  the problem of irreducibility addressed in this paper is often complicated by the use of different features to describe cases. in fault diagnosis  for example  certain attributes may not be relevant or even meaningful for every case. the result is a high frequency of unanswered questions  or missing values  in cases. the splitting criterion used by aha and breslow to address this problem selected the attribute with fewest missing values. in the absence of missing values  though  this criterion is at best equivalent to selecting any attribute that is capable of splitting the data set and therefore inferior to infogain as shown by our results. in this paper  we have assumed the absence of missing values to simplify our initial comparison of splitting criteria for irreducible data sets. however  the impact of missing values on retrieval performance is an important issue to be addressed by further research.
   algorithms for building decision trees are usually evaluated by dividing a data set into a training set that is used to build a decision tree and a test set that is used to assess its predictive accuracy  breslow and aha  1 .
however  an identification tree has no predictive accuracy; that is  it can correctly identify only the instances from which it was constructed. evaluation in terms of predictive accuracy is equally compromised by irreducibility in the case library in cbr systems that rely on similarity-based retrieval. aha et al.  propose a leave-one-in approach to evaluating retrieval precision for conversational case libraries in which most  or all  cases have unique solutions. in their approach  each case is used as a test case but without removing it from the case library during testing. precision is measured by how often the solution for the most similar case matches the solution for the test case. we plan to use a similar approach to the evaluation of precision in an investigation of the effects of incomplete data and missing values on retrieval performance in interactive cbr.
1   conclusions
minimizing the number of questions required to identify a fault  or a product that meets the requirements of the user  is an important objective in interactive cbr applications such as fault diagnosis  help-desk support  and on-line decision guides. often in these applications  each case has a unique solution or represents a unique product in the case library. we have examined the performance of common splitting criteria for building identification trees to guide the retrieval of cases from such irreducible case libraries.
   our results show that the infogain criterion  quinlan  1  is often more effective than other splitting criteria in reducing average path length when used to build identification trees from irreducible data sets. while the gini criterion  breiman et al.  1  appears to compete well with infogain for attributes with several values  our results suggest that infogain may be more effective for binary attributes. the effectiveness of both criteria decreases as a data set  or case library  approaches completeness  and no reduction in average path length is possible in an irreducible data set that is also complete.
   we have shown that for irreducible data sets  infogain and gini can be expressed in terms of the frequencies of an attribute's values  thus providing a reduction in computational effort that may be of particular benefit in online decision guides. issues to be addressed by further research include the effectiveness of techniques for tolerating missing values in decision tree learning  quinlan  1  in the special case of identification trees.
references
 aha  1  david aha. the omnipresence of case-based reasoning in science and application. knowledge-based systems  1-1  1.
 aha and breslow  1  david aha and leonard breslow. refining conversational case libraries. in proceedings of the second international conference on case-based reasoning  pages 1  providence  rhode island  1. springer-verlag.
 aha et al.  1  david aha  tucker maney and leonard breslow. supporting dialogue inferencing in conversational case-based reasoning.  in proceedings of the fourth european workshop on case-based reasoning  pages 1  dublin  ireland  1. springer-verlag.
 bergmann et al.  1  ralph bergmann  sean breen  mehmet g ker  michael manago and stefan wess. developing industrial case-based reasoning applications: the inreca methodology. springerverlag  berlin heidelberg  1.
 blake and merz  1  catherine blake and christopher merz. uci repository of machine learning databases. university of california  irving  california  1.
 breiman et al.  1  leo breiman  jerome friedman  richard olshen and charles stone. classification and regression trees. wadsworth  belmont  california  1.
 breslow and aha  1  leonard breslow and david aha. simplifying decision trees: a survey. knowledge engineering review  1-1  1.
 cendrowska  1  jadzia cendrowska. prism: an algorithm for inducing modular rules. international journal of man-machine studies  1-1  1.
 doyle and cunningham  1  michelle doyle and p¨¢draig cunningham. a dynamic approach to reducing dialog in on-line decision guides. in proceedings of the fifth european workshop on case-based reasoning  pages 1  trento  1.  springer-verlag.
 mcsherry  1  david mcsherry. interactive case-based reasoning in sequential diagnosis. applied intelligence  1-1  1.
 quinlan  1  ross quinlan. induction of decision trees. machine learning  1-1  1.
 quinlan  1   ross quinlan. unknown attribute values in induction.  in proceedings of the sixth international workshop on machine learning  pages 1  ithaca  new york  1. morgan kaufmann.
 quinlan  1  ross quinlan. c1: programs for machine learning. morgan kaufmann  san mateo   california  1.
 smyth and cunningham  1  barry smyth and p¨¢draig cunningham.  a comparison of incremental case-based reasoning and inductive learning.  in proceedings of the second european workshop on case-based reasoning  pages 1  chantilly  november 1.  springerverlag.
 watson  1  ian watson. applying case-based reasoning: techniques for enterprise systems.  morgan kaufmann  san francisco  california  1.
sin: integrating case-based reasoning with task decomposition 
 
h¨¦ctor mu oz-avila 1 david w. aha 1  dana s. nau 1 rosina weber 1   
len breslow1 & fusun yamal1 
1
 department of computer science  university of maryland  college park  md 1 
1
navy center for applied research in artificial intelligence  
naval research laboratory  code 1   washington  dc 1 
	1
lastname cs.umd.edu lastname aic.nrl.navy.mil   
abstract 
this paper describes sin  a novel case-based planning algorithm that combines conversational case retrieval with generative planning.  sin is provably correct  and can generate plans given an incomplete domain theory by using cases to extend that domain theory. sin can also reason with imperfect world-state information by incorporing preferences into the cases. our empirical validation shows how these preferences affect plan quality. 
1 	introduction  
generative planners traditionally require a complete domain theory  which provides a clear semantics for the planner's inferencing mechanism.  this allows a planner to be used in different domains. however  in many planning domains  developing a complete domain theory is infeasible.  
¡¡in this paper we present a case-based planning algorithm called sin  shop integrated with nacodae   which integrates the shop generative planner  nau et al.  1  with nacodae  a conversational case retriever  breslow & aha  1 . sin is a provably correct algorithm that does not require a complete domain theory nor complete information about initial or intermediate world-states. 
¡¡in addition to describing sin  which has been implemented in hicap  mu oz-avila et al.  1   we present sufficient conditions to ensure its correctness  show how sin represents preferences in cases to generate plans in the context of imperfect world state information  and describe an empirical analysis that demonstrates the impact of the preferences on plan quality. 
¡¡in the following sections  we introduce some terminology  detail sin  including theoretical results on its semantics with respect to incomplete domain theory  present sin's empirical evaluation to show the role of preferences to handle incomplete world state information  and discuss the implications of these results. 
1 	motivation 
sin's design was partly motivated by the following characteristics of military planning operations.    military operations are strongly hierarchical   mitchell 1; mu oz-avila et al.  1 .  thus  we chose to represent plans using hierarchical task networks  htns   erol et al.  1 .  
  there is an incomplete domain theory  in the form of general guidelines  doctrine  and standard operating procedures  sops .  however  neither doctrine nor sops can be used to derive detailed tactical plans  which often require knowledge about previous experiences. thus  sin uses shop to perform first-principles reasoning and nacodae to employ previous experiences.   military planners do not have complete information about the current situation; part of the planning includes dynamic information gathering  typically to assess enemy capabilities and/or deployment. in sin  nacodae is used to plan with an incomplete world state using preferences  which we define in section 1. 
1 	notation and definitions  
an htn is a set of tasks and their ordering relations  denoted as n= {t1 ... tm}     m¡Ý1   where   is a binary relation expressing temporal constraints between tasks. decomposable tasks are called compound  while nondecomposable tasks are called primitive.  
¡¡¡¡a domain theory consists of methods and operators for generating plans. a method is an expression of the form m= h p st   where h  the method's head  is a compound task  p is a set of preconditions  and st is the set of m's  children  subtasks. m is applicable to a task t  relative to a state s  a set of ground atoms   iff matches h t s   i.e.  h and t have the same predicate and arity  and a consistent set of bindings ¨¨ exists that maps variables to values such that all terms in h match their corresponding ground terms in t  and the preconditions p are satisfied in s  i.e.  there exists a consistent extension of ¨¨  named ¨¨'  such that   p¡Êp {p¨¨'¡Ês}   in which case m t s =st ¨¨'. 
¡¡¡¡an operator is an expression of the form o= h al dl   where h  the operator's head  is a primitive task  and al and dl are the so-called add- and delete-lists. these lists define how the operator's application transforms the current state s: every element in the add-list is added to s and every element in the delete-list is removed from s. an operator o is applicable to a task t  relative to a state s  iff matches h t s . 
¡¡¡¡a planning problem is a triple  t s d   where t is a set of tasks  s is a state  and d is a domain theory. a plan is the collection of primitive tasks obtained by decomposing all compound tasks in a planning problem  t s d . 
1 	cases in sin 
in many domains it is impossible to assume that a complete domain theory of the world is known. for example  this is true when planning for non-combatant evacuations  neos .  however  a partial domain theory exists for neos  and it can be elicited from doctrine and standard operating procedures  dod  1 . 
¡¡¡¡reasoning about parts of the domain for which no domain theory is available is done through cases. a case c is an instance of a method  denoted by c= h p st q   where h  p  and st are defined as for methods and q is a set of  question answer  pairs. q defines preferences for matching a case to the current state.  preferences are useful for ranking cases in the context of incomplete world states and/or domain theories because  as we will show  they focus users on providing relevant additional state information. 
1 	sin mixed-initiative planner 
sin integrates shop and nacodae's task decomposition algorithms. a single  current  state s is maintained in sin that is accessible to and updateable by both shop and nacodae. answers given by the user during an interaction with nacodae are added to s  i.e.  each question has a translation into a ground atom . changes to the state that occur by applying shop's operators are also reflected in s.  
shop generative planner. at any point during the planning process  shop is refining a task list t' relative to a state s and a domain theory d. initially  t' is the set of tasks t in the planning problem  t s d . shop performs ordered task decomposition  nau et al.  1   meaning that the tasks must be totally ordered  i.e.  the   relation on htns is a total order .  shop also maintains the partial solution plan p being derived  i.e.  the primitive tasks in t' . initially p is empty. shop selects the first task t in t' and continues as follows: 
  if t is primitive and has an applicable operator o  then o is applied to t  s is updated accordingly  t is removed from t' and added to the end of p. 
  else if t is compound and has an applicable method m  that has not yet been applied to t   then m is applied  which replaces t in t' with m's subtasks.  
  else if t' is not empty  then shop backtracks. 
  else shop fails. 
shop terminates when t' is empty  in which case p is the solution  or when shop tries to backtrack on a compound task t whose applicable methods have been exhausted. 
nacodae mixed-initiative case retriever. users interact with nacodae in conversations  which begin when the user selects a task t. nacodae responds by displaying the top-ranked cases whose pre-conditions are satisfied and whose heads match t.  cases are ranked according to their similarity to the current state s  which is the state that exists at that time during the conversation. similarity is computed for each case c by comparing the contents of s with q  c's  q a  preference pairs.  that is  each pair is represented as a monadic atom in s  and similarity for a given  q a  preference pair becomes a membership test in s . nacodae also displays questions  whose answers are not known in s  ranked according to their frequency among the top-ranked cases.  the user can select and answer  with a  any displayed question q  which inserts  q a  into s.  this state change subsequently modifies the case and question rankings.  a conversation ends when the user selects a case c  at which time the task t is decomposed into st  i.e.  c's subtasks . 
sin integrated planning algorithm. sin receives as input a set of tasks t  a state  s  and a knowledge base i¡Èb consisting of an incomplete domain theory i and a collection of cases b. the output is a solution plan p consisting of a sequence of operators in i. both shop and nacodae assist sin with refining t into a plan. as does shop  sin maintains the set of tasks in t' that have not been decomposed and the partial solution plan p. at any point of time  either shop or nacodae is in control and is focusing on a compound task t¡Êt' to decompose. sin proceeds as follows: 
  rule # 1: if shop is in control and can decompose t  it does so and retains control. if shop cannot decompose t  but nacodae has cases for decomposing t  then shop will cede control to nacodae. 
  rule # 1: if nacodae is in control  it has cases for decomposing t whose pre-conditions are satisfied. if the user applies one of them to decompose t  then nacodae retains control. if nacodae has no cases to decompose t or if the user decides not to apply any applicable case  then if t is shop-decomposable  nacodae will cede control to shop. 
if neither of these rules applies  then sin backtracks  if possible. if backtracking is impossible  e.g.  because t is a task in t   this planning process is interrupted and a failure is returned.  
¡¡¡¡by continuing in this way  and assuming that the process is not interrupted with a failure  sin will eventually yield a plan p  i.e.  consisting only of primitive tasks . 
1 	correctness of sin 
in this section we will assume that sin performs ordered task decomposition. that is  we assume that all tasks are totally ordered and at each iteration  when refining a set of tasks t'  sin will start by decomposing the first task in t'.  a relaxation of this condition should be possible once we modify sin to include the extended version of shop that can represent partial-order task relations  nau et al  1 . 
¡¡¡¡if i is an incomplete domain theory and b is a case base  i.e.  a set of cases   then a domain theory d is consistent with i¡Èb iff  1  every method and operator in i is an instance of a method or operator in d and  1  for every case c= h p st q  in b  there is a method m= h' p' st'  in d such that h  p  and st are instances of h'  p' and st' respectively.  although many different domain theories might be consistent with i¡Èb  in general we will not know which of these is the one that produced i and b.  however  we can prove that sin is correct in the sense that  if it succeeds in outputting a plan  then that plan could have been generated by shop using any domain theory consistent with i¡Èb.  
proposition  correctness of sin . let t be a collection of tasks  s be an initial state  i be an incomplete domain theory  and b be a case base  and let sin t s i b  represent the invocation of sin with those items as inputs. suppose that sin performs ordered task decomposition. then: 
 1  if sin t s i b  returns a plan p  then for every domain theory d consistent with i¡Èb  p is a solution plan for the planning problem  t s d .   
 1  if sin t s i b  cannot find a plan  then there is a domain theory d consistent with i¡Èb such that no solution plan exists for  t s d . 
the proof is done by induction on the number of iterations of the sin algorithm. the proof shows that each sin task decomposition in  t s i¡Èb  corresponds to a shop task decomposition in  t s d .  this is sufficient to prove correctness because shop is known to be correct  nau et al.  1 . we omit the details of the proof due to space limitations. 
¡¡¡¡this proposition suggests that cases in sin supply two kinds of knowledge: first  they provide control knowledge  similar to the knowledge encoded in cases using derivational replay when a complete domain theory is available  veloso  1; ihrig & kambhampati  1 . because cases are instances of methods  applying a case is comparable to a replay step in which the method selected to decompose a task is the one in the case's derivational trace. the main difference is that  while cases in replay systems correspond to a complete derivational trace  cases in sin correspond to a single step in the derivational trace. second  cases in sin augment the domain theory and  thus  provide domain knowledge as do cases in many case-based planners  e.g.   hammond  1  .  
1 	imperfect world information  
sin uses nacodae to dynamically elicit the world state  which involves obtaining the user's preferences. depending on the user's answers  cases will get re-ranked. when solving a task  the user can choose any of the cases  independent of their ranking  provided that all their preconditions are met. the preferences play a pivotal role in determining plan quality due to the absence of a complete domain theory.  
consider the following two simplified cases: 
case 1:  
head: selecttransport isb neosite  
preconditions: helosavailable isb  
questions-answer pairs: weather conditions  fine subtasks: transport isb neosite helos  
case 1:  
head: selecttransport isb neosite  preconditions: groundtransportavailable isb  questions-answer pairs:   weather conditions  rainy 
  imminent danger to evacuees  no 
subtasks: transport isb neosite groundtransport  
these cases both concern the selection of transportation means between an intermediate staging base  isb  and the neo site  neosite . the first case suggests using helicopters provided that they are available at the isb. the second one suggests using ground transportation provided that the corresponding transportation means are available at the isb. if the two cases are applicable  because both preconditions are met  the answers given by the user will determine a preference between them. for example if the weather is rainy and there is no immediate danger for the evacuees  nacodae would suggest the second case. the rationale behind this is that flying in rainy conditions is risky. thus  selecting ground transportation would be a better choice. 
1 	evaluation  
our experiments focused on the role of preferences in the plan generation process in the context of incomplete world state information and how they affect the quality of the resulting plans. towards this goal  we developed two planning domains where the contents of the world state can significantly impact choices during planning.   
¡¡¡¡for these experiments  we encoded an automatic user that dynamically provided preferences when asked by nacodae. the user provided preferences with a predefined bias  defined in the following sections  towards certain kinds of solutions. the automatic user will always select the case with the highest similarity. in situations where several candidate cases had the same highest similarity  the automatic user selected one of them randomly. the purpose was to observe whether the resulting plans reflect the user's bias despite the incomplete information about the world state. 
1 	the personal travel domain  
the first domain was the personal travel domain. its plans concern traveling from locations in washington  dc to downtown new york city  nyc .  we encoded 1 transportation methods  1 inter- and 1 intra-city . plans consist of 1 planning segments.  states indicate different locations  whether connections between locations exists and by which means  weather conditions  etc.  the knowledge base consists of 1 methods and 1 operator for shop and 1 cases for nacodae.  
¡¡¡¡our personal travel plan evaluator can generate a different time duration each time it is given a plan and world state because of its non-deterministic execution.  for each run  it outputs whether the plan succeeded and  if so  the trip's duration.  a plan fails when segment delays cause a late arrival for a segment requiring a fixed time departure  e.g.  an airplane flight .  for each segment  we applied a delay function that is influenced by world state conditions. for example  a flight segment will incur a longer delay for higher chances of large snow accumulation  especially on holidays  i.e.  high travel days .  segments are categorized into short  medium  and long lengths  and delays can range from 1 up to 1 times a segment's anticipated duration. smaller multiples are used for maximum delays for medium  1  and long  1  duration segments. 
¡¡¡¡we selected ten goals  corresponding to ten pairs of departing and arrival locations in washington  dc and downtown nyc  respectively.  for each goal  we generated 1 random world states  thus yielding 1 total planning problems. sin was then used to generate a plan for each problem  thus yielding 1 plans. each was executed 1 times by the plan evaluator  for a total of 1 runs.  
 
table 1: results for the personal travel domain. 
preference duration price success bus    1   1  1% train    1   1  1% plane    1   1  1%  
¡¡¡¡table 1 summarizes the results. when the user's bias was given towards taking the bus for the intercity part of the trip.  this preference yields maximal  1 minutes  durations  but has the cheapest price. on the other extreme  when the plane is the preferred means of transportation  the duration is the shortest but the price is the most expensive. the lowest success rate occurs for plane trips; it reflects missing connections due to external factors such as the weather. although a bias expresses a preference for a certain transportation mode  it does not imply that it was always selected; world state conditions may prevent the use of some travel modes for particular situations. 
1 	the neo planning domain  
the second domain was the noncombatant evacuation operations domain. its plans involve performing a rescue mission where troops are grouped and transported between an initial location  the assembly point  and the neo site  where the evacuees are located . after the troops arrived at the neo site  evacuees are re-located to a safe haven. planning involves selecting possible pre-defined routes  consisting of 1 segments each. the planner must also choose a transportation mode for each segment.  in addition  other conditions were determined during planning such as whether communication exists with state department personnel and the type of evacuee registration process. sin's knowledge base included 1 operators  1 methods  and 1 cases. 
¡¡¡¡as with the personal travel domain evaluator  the neo planning evaluator can generate a different output each time it is given a plan and a world state because of its nondeterministic execution.  for each run  it outputs the plan execution duration  the time it took to reach the evacuees  and the evacuee casualties.  similar to the personal travel domain  we applied a delay function that is influenced by world state conditions. the neo planning evaluator is more complex than the personal travel evaluator because there are more conditions that can affect the output variables and these conditions may interact.  for example  a small-sized force will incur fewer delays because embarking troops in the transportation means will take less time than for largesized forces. however  smaller force increase the chances of hostile attacks  which if they occur will delay the operation. 
¡¡¡¡we had a single task  to perform a neo  and generated 1 random world states  thus yielding 1 planning problems. sin was then used to generate a plan for each problem  thus yielding 1 plans  and each was executed 1 times by the plan evaluator for a total of 1 runs.  
table 1: results for the neo planning domain. 
preference duration time to reach evacuees evacuee 
casualties helicopter    1     1   1% ground 
vehicle    1     1   1%  
¡¡¡¡table 1 summarizes the results. the helicopter transport preference yields plans that have shorter execution durations  1 hours  and require less time to reach the evacuees  1 hours . in addition  average casualties among evacuees is less  1%   due mainly to the shorter time to reach them  and land travel is generally riskier than air travel. still the number of casualties among evacuees is high even with helicopters. this is due to the simple bias encoded in the simulated user. human users could yield better  and worse  plans by dynamically providing more sophisticated preferences depending on the world state conditions.  
1 	discussion  
the experiments show the capabilities of sin in allowing the user to guide the planning process towards their preferences while dynamically capturing world-state conditions. despite our use of simplistic simulated users  the quality of the plans reflect the user's bias. 
1 	related work  
table 1 compares seven different features of sin to those of other planning systems.  
 
	table 	1: 	comparisons 	between 	different 	systems. 
conventions: gen=generative; cbp=case-based; mi=mixed-initiative; i=interleaved control structure; dk=cases are used to supply domain knowledge; ck=cases are used to supply control knowledge. 
system gen cbp m-i i dk ck sin ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì ¡Ì chef  ¡Ì   ¡Ì  mi-cbp ¡Ì ¡Ì ¡Ì   ¡Ì nacodae  ¡Ì ¡Ì  ¡Ì  prodigy/ 
analogy 
shop 
sipe ii ¡Ì ¡Ì 
  
 
  
 
 ¡Ì 
 
  
¡¡¡¡we first discuss the features shown in columns 1 of table 1.  shop  nau et al.  1   as is typical of generative planners  requires a complete domain theory. chef  hammond  1  and dial  leake et al.  1  are case-based  but do not have a generative component  and thus need a large case base to perform well across a wide variety of problems.  prodigy/analogy  veloso  1   dersnlp  ihrig & kambhampati  1   and paris  bergmann & wilke  1  integrate generative and casebased planning  but require a complete domain theory and are not mixed-initiative. sipe ii  wilkins  1  is a mixedinitiative generative planner  but does not use cases. nacodae  mu oz-avila et al.  1  is a mixed-initiative case-based planner  but does not employ generative planning.  
¡¡¡¡at least three other integrated  case-based/generative   mixed-initiative planners exist. mi-cbp  veloso et al.  1   which extends prodigy/analogy  limits interaction to providing it with user feedback on completed plans. thus  it must input  or learn thru feedback  a sufficiently complete domain theory to solve problems.  in contrast  sin gathers information it requires from the user through nacodae conversations  but does not learn from user feedback.  caplan/cbc  mu oz-avila et al.  1  and mitchell's  system use interaction for plan adaptation rather than to acquire state information. 
¡¡¡¡among integrated case-based/generative planners  sin's interleaved control structure is unique in that it allows both subsystems to equally control the task decomposition process. in contrast  other approaches either use heuristics  prodigy/analogy; mi-cbp  or order case-based prior to generative planning  dersnlp; mitchell  1   although paris does this iteratively through multiple abstraction levels.  distinguishing the relative advantages of these control strategies is an open research issue. 
¡¡¡¡the final two columns of table 1 refer to the types of contributions made by cases in cbp systems. chef and nacodae both use cases to provide domain knowledge  while prodigy/analogy uses cases for control knowledge  i.e.  determining which planning constructs to apply .  in contrast  sin uses cases to both provide domain knowledge  i.e.  instances of methods  and control knowledge  i.e.  it allows the user selects which of these instance methods to apply .  
¡¡¡¡caseadvisor  carrick et al.  1   like sin  integrates conversational case retrieval with planning. while caseadvisor applies pre-stored hierarchical plans to gather information to solve diagnosis tasks  sin instead uses its case retriever to gather information and applies cases to refine hierarchical plans.  
¡¡¡¡planning with incomplete information has been the subject of frequent research in planning  e.g.   golden et al.  1  . typically  a distinction between sensing and planning actions is made  where the former involve queries to external information sources and the latter involves inferencing steps. this is comparable to querying using nacodae and task refinement using shop.  
1 final remarks 
we presented the sin algorithm for case-based htn planning. sin was motivated by three requirements for planning military operations: plans are hierarchical  there is no complete domain theory explaining all possible courses of action  and planners do not have complete information about the current situation. 
our work includes the following contributions:   sin is a provably correct algorithm for case-based planning with incomplete domain theories.   sin can tolerate incomplete world state information by representing preferences in the cases. our experimental results show that a user can dynamically guide sin by giving preferences to it as part of the user's normal interaction with sin during the planning process.   sin provides a bridge between two classical approaches for case-based planning  in which cases either provide control knowledge or domain knowledge. in sin  cases provide both kinds of knowledge.  
  sin's ability to combine both experiential and generative knowledge sources can be beneficial in realworld domains where some processes are well known and others are obscure but recorded memories exists on how they were performed. planning for neos is a typical example of this type of domain. we have integrated sin into hicap  mu oz-avila et al.  1   a system designed to support these kinds of operations. 
¡¡¡¡our creation of sin was made possible because of the similarity between nacodae's cases and shop's methods.  for our future work  we want to further exploit this similarity to ease knowledge acquisition for plan generation.  to this end  we have started working to create algorithms for learning htn methods automatically from cases. 
acknowledgements 
the authors would like to thank the reviewers for their helpful comments. this research was supported by funding from darpa  onr  afrl  f1 and f1-1   arl  daal1   and the university of maryland general research board. opinions on sin expressed in this paper are those of the authors and do not necessarily reflect the opinions of the funding agencies. 
 
references 
 bergmann & wilke  1  bergmann r.  & wilke w. building and refining abstract planning cases by change of representation language. journal of artificial intelligence research  1  1  1. 
 breslow & aha  1  breslow  l. a. & aha  d. w.  
nacodae: navy conversational decision aids environment  technical report aic-1 .  ncarai  washington  dc  1. 
 carrick et al.  1  carrick  c.  yang  q.  abi-zeid  i.  & 
lamontagne  l. activating cbr systems through autonomous information gathering.  proceedings of the third international conference on case-based reasoning  pp. 1 . seeon  germany: springer  1. 
 dod  1 .  joint tactics  techniques and procedures for noncombatant evacuation operations  joint report  1.1 .  washington  dc: department of defense  joint chiefs of staff  1. 
 erol et al.  1  erol  k.  nau  d.  & hendler  j. htn planning: complexity and expressivity.  proceedings of the twelfth national conference on artificial intelligence  pp. 1 .  seattle  wa: aaai press  1. 
 hammond  1  hammond  k. case-based planning: viewing planning as a memory task. boston  ma: academic press  1. 
 ihrig  & kambhampati  1  ihrig  l.h.  & kambhampati  
s. derivational replay for partial order planning. in proceedings of the twelfth national conference on artificial intelligence  1 .  seattle  wa: aaai press  1. 
 golden et al.  1  golden  k.  etzioni  o.  and d. weld.  planning with execution and incomplete information   technical report  department of computer science  university of washington  tr1-1  february  1. 
 leake et. al.  1  leake  d.  kinley  a.  & wilson  d. a case study of case-based cbr. proceedings of the second international conference on case-based reasoning  pp. 1 . providence  ri: springer  1. 
 mitchell  1  mitchell  s.w.  1 . a hybrid architecture for real-time mixed-initiative planning and control. proceedings of the ninth conference on innovative applications of artificial intelligence  pp. 1 . providence  ri: aaai press  1. 
 mu oz-avila et al.  1  mu oz-avila  h.  mcfarlane  d.  aha  d.w.  ballas  j.  breslow  l.a.  & nau  d. using guidelines to constrain interactive case-based htn planning. proceedings of the third international conference on case-based reasoning  pp. 1 . munich: springer  1. 
 mu oz-avila  & weberskirch   1  mu oz-avila  h.  & weberskirch  f. planning for manufacturing workpieces by storing  indexing and replaying planning decisions. proceedings of the third international conference on ai planning systems. edinburgh: aaai press  1. 
 nau et. al.  1  nau  d.  aha  d.w.  & mu oz-avila  h. 
ordered task decomposition.  in y. gil & k. myers  eds.  representational issues for real-world planning systems: papers from the aaai workshop  technical report ws-1 .  menlo park  ca: aaai press  1. 
 nau et. al.  1  nau  d.  cao  y.  lotem  a.  & mu ozavila  h. shop: simple hierarchical ordered planner. proceedings of the sixteenth international joint conference on artificial intelligence  pp. 1 . stockholm: aaai press  1. 
 nau et. al.  1  nau  d.  mu oz-avila  h.  lotem  a.  mitchell  s. & yue  c. total-order planning with partially ordered subtasks. to appear in proceedings of the seventeenth international joint conference on artificial intelligence.  
 veloso  1  veloso  m. planning and learning by analogical reasoning. berlin: springer-verlag  1. 
 veloso et. al.  1  veloso  m.  mulvehill  a.m.  & cox  m.t. rationale-supported mixed-initiative case-based planning. proceedings of the ninth conference on 
innovative applications of artificial intelligence  pp. 1 .  providence  ri: aaai press  1. 
 wilkins  1  wilkins  d.e. using the sipe-1 planning system: a manual for version 1  working document . menlo park  ca: stanford research international  artificial intelligence center  1. 
a distributed case-based query rewriting
maurizio panti luca spalazzi loris penserini
istituto di informatica  university of ancona  via brecce bianche  1 ancona  italy
panti spalazzi pense  inform.unian.itabstract
in literature  the mediator architecture has been proposed for taking information from distributed  heterogeneous  and often dynamic sources and making them work together as a whole. in this paper we propose a distributed case-based approach for the main problem of a mediator  i.e. rewriting queries according to mediator's schema. according to this approach we use a case memory as mediator's schema. therefore  such a schema is not static  as in other systems  but is dynamically updated through the cooperation with information sources and other mediators  strongly influenced by the queries submitted by a consumer. from the analysis of different cooperation strategies arises that it is more efficient and effective for a mediator to directly cooperate with information sources  when the sources are few. otherwise  it is more efficient to cooperate with other mediators.
1	introduction
nowadays  information systems can be thought as collections of information sources and information consumers that are often distributed in world-wide networks. this means that sources and consumers can be autonomous and thus are often heterogenous and dynamic. information sources are heterogenous due to discrepancies at the physical level  different dbmss  software and hardware platforms   logical level  different data models   and conceptual level  different schemas  concept and relation names .
example 1 figure 1 shows an example of three information sources about articles on computer science and their authors. let us suppose that the three sources receive the query . can answer to  it finds a view that matches with    whereas and can not. indeed  the schema of is different  heterogeneity of schemas . notice that  if we rewrite the query as
       is able to answer since is part of its schemas. for what
concerns   it is not a matter of heterogeneity  simply does not have the required information.
information consumers are heterogeneous as well  since each consumer may have different customization needs due to distinct business objectives. moreover autonomous information sources are dynamic  since they may be added to the system  may become  temporarily or definitively  unavailable  or  sometimes  may also vary their conceptual schemas. information consumers are also dynamic. indeed new consumers can be inserted or removed. furthermore  their information needs can change very often. our work deals with information integration in distributed  heterogeneous  and dynamic information systems  i.e.  constructing answers to query from consumers. our primary goal consists on rewriting a consumer's query into queries to specific information sources. in  m. panti  l. spalazzi  a. giretti  1  we proposed a distributed case-based approach to the problem of rewriting queries. according to this approach we have a dynamic mediated schema  the case memory  instead of a static one  as in other systems . indeed  our mediator'sschema is dynamically updated through the cooperation with information sources and other mediators  strongly influenced by the queries submitted by a consumer. in this paper we focus on the cooperation strategies of the distributed case-based reasoner. indeed  we compare different strategies from a theoretical point of view and discuss the results. the paper is organized as follows. section 1 reports the related work. an overview of our work is provided in section 1. section 1 summarizes results about local query rewriting  i.e. when a query can be reformulated by a single mediator . section 1 describes the distributed query rewriting  i.e.  when a query can be reformulated only through cooperation among mediators  and discuss some theoretical results. finally  some conclusions are given in section 1.
1	related work

figure 1: an example of information sources.intelligent information integration  i  . the most significative approach to information integration relies on the so called mediator architecture  wiederhold  1 : an architecture with three-layers. a layer is devoted to information consumers  another layer is devoted to information sources  the middle layer  i.e. the mediator  deals with the reformulation of a given query in a set of queries  each targeted at a selected source. intuitively  such a reformulation would be equivalent to the original query  i.e.  denote the same set of instances . nevertheless  often this is not possible. therefore  a query reformulation contained in  instead of equivalent to  the original query is considered adequate  see for example  ullman  1; beeri et al.  1  . furthermore  when a description logic  e.g.  borgida and patel-schneider  1   is used as data modeling and query language  e.g.  beeri et al.  1; kirk et al.  1; ullman  1    the problem of query containment corresponds to the subsumption problem.
¡¡there are two basic approaches to intelligent information integration: the procedural approach and the declarative approach. in the procedural approach  e.g.  tsimmis  garciamolina et al.  1    mediators integrate information from sources through ad-hoc procedures defined with respect to a set of predefined information needs. when such needs or sources change  i.e.  we have a dynamic information system   a new mediator must be generated. in the declarative approach  e.g.  sims  arens et al.  1   information manifold  kirk et al.  1   and infomaster  duschka and genesereth  1    mediators use suitable mechanisms to rewrite queries according to source descriptions. usually  the proposed systems do not allow automatic adaptation of such descriptions when something changes.
distributed case-based reasoning  dcbr . a distributed case-based reasoner  prasad et al.  1; plaza et al.  1  learns by experience and cooperation how problems can be solved. therefore  it is appropriate for distributed and dynamic application domains  when it is impossible to have predefined solutions. nevertheless  as far as we know  its application to information integration is a novelty. in dcbr  there are several agents; each agent has its own case memory since it could have acquired its own independent problemsolving experiences. a new problem is solved through agent cooperation. indeed each agent reuses the local past case that best contributes to the overall case. description logic is applied to case-based reasoning  and thus dcbr  as well  e.g.  koehler  1  . indeed  in case-based reasoning  subsumption becomes a powerful tool to automatically derive case hierarchies that can be used in case retrieval and case retention.
1	work overview
our goal is to build an information system capable of interconnecting information consumers and sources. previous approaches solve several problems concerned with distributed  heterogeneous information systems. their main limitation is related to the capability of evolving according to dynamic information systems. we propose  see also  m. panti  l. spalazzi  a. giretti  1   a multi-agent system which is based on a mediator architecture  i.e. mediators  sources  and consumers are agents. all the agents adopt a description logic called c-classic  borgida and patel-schneider  1  as data modelling and query language. indeed  subsumption in c-classic can be computed in polynomial time.
¡¡in our work  the capability of a mediator agent to face heterogeneous and dynamic systems relies on a thesaurus and a distributed case-based reasoner. each mediator has its own thesaurus in order to solve name heterogeneity. a thesaurus is composed by a set of classes of synonyms. each element of a class has the reference to information sources that use it as term. every time a new query arrives  its terms are translated in standard terms by means of the thesaurus. the reverse process occurs when the rewritten query must be sent to information sources. the thesaurus must be dynamically updated when a change in the system occurs. the classes of the thesaurus are modified by means of clustering. the explanation of this technique is out of the scope of the paper  for more details we remind to  diamantini and panti  1 . therefore  in the rest of the paper we assume that no name heterogeneity occurs  i.e.  the thesaurus has a maximum precision . each mediator has also its own case-based reasoner in order to solve heterogeneity of schemas and consumer's needs. each case contains a query and the corresponding reformulation  and it is stored in the mediator's case memory. when a new query from the consumer arrives  the mediator looks for past queries subsumed by the new one and adapts the corresponding solutions in order to obtain a reformulation of the new query. this means that the mediated schema is strongly influenced by the queries submitted by consumers  mediator's experience . when this experience does not help  the mediator cooperates with other mediators and/or information sources. this approach allows the mediator to update its schemas and therefore to take into account changes in information sources and consumer's needs. this can be considered a kind of distributed case-based reasoning.
¡¡in our system  information source agents1 are able to cooperate with mediators through a query reformulation based on local schemas. indeed each source is able to find a view over its schema that is subsumed by an input query.
example 1 let us consider the distributedinformation system of figure 1  example 1 . according to the above assumption  we consider that all the  concept and role  names have been already translated by a thesaurus and thus no name heterogeneity occurs. nevertheless  conceptual schemas are different  as showed in example 1 .

query rewriting of the query	information sources


figure 1: an example of case memory.let us suppose that source	receives a query
¡¡	.	finds that	subsumes acmtrans and ieeetrans and thus returns the corresponding instances  denoted by			 .
1	local query rewriting
generally speaking  a case is an arbitrary set of features  attribute-value pairs . some features are devoted to represent a problem  in our application the query to be reformulated  in order to make easier the retrieval of a past problem similar to the current situation. the rest of features are devoted to represent the problem solution  the reformulated query and information sources where the reformulated query has been sent  to reuse it in the current problem. furthermore  the problem of rewriting queries has a fundamental condition that must be satisfied: the rewritten query must be contained in the original query  beeri et al.  1; ullman  1 . we use c-classic as a language for representing queries and thus cases. the subsumption relation is used as query containment and thus for case retrieval. according to above considerations  a case is a structure as the following: where
are information sources and are queries to respectively. is a given query and
is its reformulation  i.e.  a set of arbitrary combinations of such that each element of is subsumed by . a collection of cases is called case memory. a terminology  i.e.  a set of concept definitions and their relations  is always associated to a case memory. represent the top and bottom concepts  respectively. the initial case memory can be built as static mediated schemas of traditional mediators  e.g. sims  arens et al.  1  . in our system  the added value is that the dcbr works to maintain updated such a schema.
example 1 figure 1 and figure 1 show the case memory and the terminology of a mediator of example 1.
query retrieval. every time a new query arrives  it is inserted in the terminology and classified by means of subsumption. if there exists a past problem equal to the new query  then its solution can be used as solution for the new query. otherwise  the solutions of past problems that are subsumed by the new query can be used as solution for the new problem. the closest to the new query the retrieved cases are  the best the solutions are. this drives us to the notion of problems maximally contained in the new query. this notion allows the definition of the retrieval function as a set of conjunctions of concepts     of the given terminology    . for each conjunction of concepts there does not exist another conjunction of concepts of the same terminology     such that	.
example 1 let us suppose to have as input query in example 1. applying the retrieval function we obtain:
query reuse. the basic reuse algorithm     simply copies past solutions of past problems returned by the retrieval function.
example 1 applying the basic reuse algorithm to example
1  we obtain:	1
now we have all the building blocks to define the local reuse algorithm    . such an algorithm decomposes each consumer's query in basic components  applies the basic reuse algorithm to each component  and finally assembles the corresponding results  a more detailed description can be found in  m. panti  l. spalazzi  a. giretti  1  .
example 1 let us suppose to have as input query in example 1. applying the reuse
algorithm we obtain:
notice that this is just one possible answer  another mediator with a different case memory may rewrite the query in a different way and thus return a different answer.
it is easy to prove that each element of and is subsumed by . therefore  a reformulation is contained in the original query as required in the definition of case. finally  notice that in general .
query evaluation. when the mediator has a reformulation of the received query  it must send such a reformulation to related information sources and wait for their answers.
example 1 the evaluation of	consists on querying sources	 	  and	with	 	  and respectively  and integrating the related answers. the semantics of concepts helps us on integrating the answers.
let	 	  and	be answers from	 	  and	respectively  then the answer to	is:
.
failures. we have a local failure in query reuse when a mediator is not able to rewrite a given query   i.e.  1. this means that the mediator's case memory contains no past cases that can be used to reformulate . usually this is due to the fact that it is the first time that the consumer formulates such a query  i.e.  the consumer has a new information need.


figure 1: an example of terminology.example 1 let us consider the case memory of example 1 and the query
. the mediator is not able to
rewrite the query. indeed  we have
furthermore  we have a local failure in query evaluation when a mediator sends a rewritten query to related sources and receives at least an empty answer. this means that the case memory of the mediator is not updated. typically  an information source has been removed from the system or changed its schema.
example 1 let us suppose that source has been  perhaps temporarily  removed  then the evaluation of the reformulated query in example 1 fails. indeed
1	distributed query rewriting
even if a local failure  in query reuse or evaluation  occurs  the distributed case-based reasoning has still a possibility of solving the problem by means of cooperation with other mediators and sources. notice that this provides the system the most important way of learning. indeed  if the query is reformulated  the new rewritten query and/or the new sources can be stored as a new case. in such a way the mediator can support dynamic information systems. furthermore  it does not need to maintain consistent its case memory every time something changes  but only when a consumer sends a query that fails. therefore  the distributed information system is not overloaded with consistency maintenance operations  that usually are time consuming. on the contrary  traditional mediation systems can only update mediated schema with expensive hand-made procedures.
¡¡cooperation strategies can be classified according to three parameters: partners  queries  and answers. in the following we describe such strategies  we also analyse them  from a theoretical point of view   since the choice of the right cooperation strategy is crucial for effectiveness and efficiency  and discuss the results.
partners. when a mediator  say   fails  it can cooperate with other mediators asking them to rewrite a query according to their own case memories. if they succeed  can store the result as a new case in its case memory. the mediators involved in this strategy can be all the mediators of the system  just mediators that have never been in touch with m  or mediators that successfully cooperated with in the past. the mediator can directly cooperate with information sources as well. this strategy can involve all the sources  it is very expensive   sources responsible of the local failure  this strategy takes into account changes of schemas   recently added sources  this strategy takes into account new sources   or the sources that successfully cooperated with in the past  when a user has a new information need  the application of this strategy supposes that the new need involves the same sources of the past . each source classifies the query and returns the subsumed concepts. let us analyse such strategies focusing on recall and precision ratios in order to evaluate the effectiveness of the strategy. we also evaluate the information redundancy that such strategies produce.
¡¡first  let us consider a mediator that cooperates with other mediators  sends them the original query  and asks for receiving a rewritten query. this strategy has a possible disadvantage that another mediator can send a wrong rewriting. this fact has harmful consequences on the precision  as stated by the following theorem.
theorem 1 letbe	information sources. let	be a view of	.is represented as a case memorythat does not change. let 	be two mediators such that	interacts with	whenfails. let	be the casememory of	after	interactions with	. then
	recall:	
	precision:		is indeterminate
in the above theorem  denotes the case memory of the mediator  e.g. see figure 1 . denotes the information need of a given consumer  i.e.  it is the schema to which must converge. such a theorem states that when the mediator sends a rewriting that is part of the recall and the precision grow  otherwise the recall does not change and the precision decreases. this means that is crucial the choice of what and how many mediators to cooperate with. another possible disadvantage of such a strategy is that asymptotically all the mediators may have the same case memory  i.e. this strategy produces a certain redundancy. this result is a consequence of the following theorem.
theorem 1 let   be two mediators such that interacts with when fails. let be the case memory of after interactions with . let be the case memory of such that it does not change while interacts with . then

the above theorem states that the case memory of converges to the case memory of when cooperates with . this seems to suggest that when the mediator cooperates with other mediators it would directly ask data instead of the rewritten query. this would allow any mediator to maintain its expertise and avoid redundancy.
¡¡as a second strategy  let us consider a mediator that directly cooperates with information sources  sends them the original query  and receives the rewritten query  and eventually the data . this strategy guarantees a given consumer that the mediator converges to the consumer's information need. indeed it is possible to prove the following theorem.
theorem 1 let be information sources. let be a view of . is represented as a case memory
that does not change.	let	be a mediator such that
interacts with when it fails. let be the case memory of after interactions with . then
	recall:	
	precision:	
the above theorem states that the mediator will asymptotically satisfy the information need of a given consumer  denoted by  . indeed  under the hypothesis that no name heterogeneity occurs  see section 1   it is impossible that the source has wrong schemas. therefore  the information source has either the right answer or no answer at all. this means that the choice of sources to cooperate with is crucial only for efficiency of the system  i.e. how fast the mediator's case memory converges to user's needs.
example 1 in the situation of example 1  let us suppose that the mediator chooses to look for new sources and cooperate with them. let us suppose that is a new information source which contains acm publications and has the following concepts in its schema:



¡¡the mediator decomposes the query and sends each component to . for example  when receives	  it looks for maximally contained concepts and obtains
				.	when
the mediator receives the answer  retains as new case:



queries. cooperation strategies can also be classified according to queries. indeed can send the query as received by the consumer  this is a strategy that takes into account the new user's need . it can send the retrieved query  if any   i.e. the problem returned by the retrieval function  this is a strategy that takes into account the need of maintaining consistent the mediator's case memory . finally  the mediator can send the reformulated query  if any . in the latter case the  approximation of the solution grows   as stated by the following theorem.
theorem 1 let be the original query. let be one of the past queries retrieved by . let be one of the reformulations of used by as rewriting of . let be the mediator or source that cooperates with . let be the query rewritten by if
receives	as input query. then
as a consequence  the solution returned by is more distant from than when does not send the original query.
¡¡cooperation strategies can also be classified depending on the fact that the query can be sent as it is or be decomposed in basic components. this second strategy is applicable since it is straightforward to prove that the following theorem holds when we use c-classic:
theorem 1 let	be queries.	let
             be the user's query. let be the mediator or source that cooperates with . let be the query rewritten by if receives as input query. then
the theorem above states that does not need to decompose a query before its sending  since the query is directly decomposed by partners. this suggests that decomposition is convenient  efficient  only if failed to reformulate / evaluate just some components of the query.

	partner	answer	new case	data	no. of messages.	redundancy

	1st. time	next time

	source    	rewriting	1	no
	 	data	1	no
mediator     rewriting 1 yes   data	1 no

table 1: a comparison of different cooperation strategies.answers. finally  cooperation strategies can be classified according to answers. can ask for rewriting the query. its goal is to update its case memory with a rewritten query obtained from its collaborators. can also ask for data that answer the query. in such a situation  it stores in the case memory the addresses of the mediators/sources that answered. we can analyse such strategies depending on partners. table 1 reports the results of cooperation with only one agent. we assume that such an agent  say a source or a mediator   does not change while the strategy is in progress. we also suppose that has in its case memory. for each strategy  table 1 reports the new case that stores in its case memory  the set of instances related to the reformulation  the number of messages to obtain instances of  when receives the first time and the next time   and the results about redundancy  see theorem 1 . when we consider cooperation with mediators  it seems more efficient to ask for a rewriting  since interacts with only the first time  the next time directly interacts with . on the other hand  the strategy of requiring data does not produce redundancy  even if it is less efficient. notice that  for what concerns such evaluation parameters  the strategy of cooperating with sources requiring data seems to be the most appropriate when the number of sources is comparable to the number of mediators.
1	conclusions
we considered the problem of rewriting queries by means of distributed case-based reasoning. as far as we know  this is a novel approach. this allows us to have dynamic mediated schemas instead of static ones. we showed that a real dynamic mediator is obtained by means of distributed query reformulation  i.e.  cooperation with other mediators and sources. thanks to this approach  the mediator is able to be updated when sources and consumers are added/removed or change their schemas/needs. this operation is performed only when a consumer sends a query that fails and not every time something changes. this allows us to avoid of overloading the distributed information system with expensive consistency maintenance operations. in distributed case-based reasoning  the choice of the right cooperation strategy is crucial. we hinted several possible strategies and discussed their advantages. from such an analysis arises that we can obtain good results for effectiveness and efficiency when a mediator directly cooperates with information sources when the number of information sources is small. otherwise  the most efficient strategy is the cooperation with other mediators. finally  notice that the choice of c-classic is appropriate since the algorithm complexity for query retrieval and reuse is polynomial.
acknowledgements
we thank j. mylopoulos for suggestions and useful discussions about the topics presented in this paper. we also thank p. coupey and s. gianfelici that have participated to the development of the c-classic reasoner.
references
 arens et al.  1  v. arens  c. y. chee  c-n. hsu  and c. a. knoblock. retrievingand integrating data from multiple information sources. international journal on intelligent and cooperative information systems  1 :1  1.
 beeri et al.  1  c. beeri  a. y. levy  and m.-c. rousset. rewriting queries using views in description logics. in proc. of the 1th acm symposium on principles of database systems  pods   pages 1  new york  ny  may 1  tucson  arizona 1. acm press.
 borgida and patel-schneider  1  a. borgida and p. f. patel-schneider. a semantics and complete algorithm for subsumption in the classic description logic. journal of artificial intelligence research  1-1  1.
 diamantini and panti  1  c. diamantini and m. panti. a
conceptual indexing method for content-based retrieval. in a. m. tjoa  a. cammelli  and r. r. wagner  editors  tenth international workshop on database and expert systems applications  dexa'1   florence  italy  1 september 1. ieee computer society.
 duschka and genesereth  1  o. m. duschka and m. r. genesereth. infomaster - an information integration tool. in proc. of the international workshop on intelligent information integration  freiburg  germany  september 1.
 garcia-molina et al.  1  h. garcia-molina  y. papakonstantinou  d. quass  a. rajaraman  y. sagiv  j. ullman  v. vassalos  and j. widom. the tsimmis approach to mediation: data models and languages. journal of intelligent information systems  1-1  1.
 kirk et al.  1  t. a. kirk  a. y. levy  y. sagiv  and d. srivastava. the information manifold. in aaai spring symposium on information gathering. aaai  1.
 koehler  1  j. koehler. an application of terminological logics to case-based reasoning. in p. torasso  j. doyle  and e. sandewall  editors  proceedings of the 1th international conference on principles of knowledge representation and reasoning  pages 1  bonn  germany  may 1. morgan kaufmann.
 m. panti  l. spalazzi  a. giretti  1  m. panti  l. spalazzi  a. giretti. a case-based approach to information integration. in proceedings of the 1th international conference on very large databases  cairo  egypt  1 september 1.
 plaza et al.  1  e. plaza  j. l. arcos  and f. mart¡ä n. cooperative case-based reasoning. in g. weiss  editor  distributed artificial intelligence meets machine learning  lecture notes in artificial intelligence  berlin  1. springer verlag.
 prasad et al.  1  m. v. n. prasad  v. r. lesser  and s. e. lander. retrieval and reasoning in distributed case bases. journal of visual communication and image representation  special issue on digital libraries  1 :1  march 1.
 ullman  1  j. d. ullman. information integration using logical views. in proceedings of the international conference on database theory  pages 1  1.
 wiederhold  1  g. wiederhold. mediators in the architecture of future information systems. ieee computer magazine  1-1  march 1.
using case-base data to learn adaptation knowledge for designjacek jarmulak and susan craw
the robert gordon university
aberdeen  ab1hg  scotland  uk
s.craw scms.rgu.ac.uk
ray rowe
astrazeneca  silk road business park
macclesfield sk1na  uk

abstract
one advantage of case-based reasoning  cbr  is the relative ease of constructing and maintaining cbr systems  especially as a number of commercial cbr tools are available. however  there are areas of cbr that current tools have not yet addressed. one of these is easing or automating the acquisition of adaptation knowledge. since tasks like design or planning typically require a significant amount of adaptation  cbr systems for these tasks still do not fully benefit from cbr's promise of reducing the development effort. to address this  we have developed several  knowledge-light  methods for learning adaptation knowledge from the cases in the case-base. these methods perform substitutional adaptation  for both nominal and numerical values  and are suitable for decomposable design problems  in particular formulationand configuration. tests performed on a tablet formulation domain show promising results. the automatic adaptation methods we present can easily be incorporated in general-purpose cbr tools  thus further contributing to reducing the cost of cbr systems.
1	introduction
the popularity of case-based reasoning  cbr  can be attributed to a number of factors. one of these is cbr's ability to cope with ill-defined problem domains by basing solutions to new problems on solutions that worked in the past. given a new problem  a cbr system retrieves a set of similar problems from a case-base of previously solved problems. the retrieved problems provide ballpark solutions to the new problem; if needed  they can be adapted to fit the new problem better. this final solution can be reviewed and stored in the case-base to improve future performance if appropriate. this ability to learn is an important advantage of cbr.
¡¡a further advantage of cbr is the relative ease of constructing and maintaining cbr systems  especially as a number of commercial cbr tools are available. however  this advantage applies mainly to cbr systems relying on retrieval for findingsolutions  e.g.  many systems for classification and

this work is supported by epsrc grant gr/l1.
estimation tasks. this is because current cbr tools concentrate only on certain aspects of building a cbr system: automating construction of the case-base  defining retrieval  and implementing the user interface. although the tools usually also provide some basic facilities for coding adaptation rules  they do not provide means of automating the task of acquiring adaptation knowledge. this means that cbr systems for more complex tasks  like planning or design  can be developed relatively easily only if their need for automatic adaptation can be eliminated  or reduced  by either having a large number of cases or by using the system in a decisionsupporting role where adaptation is performed by an expert.
¡¡recent developments in cbr tools seem to have concentrated on increasing their possible field of application: improving the way they interface with data-base systems  facilitating their deployment in web environments  increasing portability  and improving case representation. however  so far  commercial systems provide few facilities for automating the process of constructing competent case-bases  optimising retrieval with the goal of retrieving most useful cases  and  in particular  automating the acquisition of adaptation knowledge. these issues are now being addressed in recent research: smyth and mckenna  aim to build compact competent case-bases  jarmulak et al.  introduce ways of automatically optimising retrieval  and wilke et al.  consider knowledge-light approaches for learning adaptation suitable for incorporation into cbr tools.
¡¡in this paper we present a selection of generic methods suitable for automating the acquisition of adaptation knowledge for configuration and formulation tasks  a subclass of design tasks. we begin by placing our work in the context of related research in cbr. then  we describe the methods that we have implemented for automated acquisition of adaptation knowledge from cases in the case-base  and show how the acquired knowledge is used in adaptation. after that  we present our test domain and the results of our experiments  before drawing conclusions and discussing further research.
1	context & related work
¡¡¡¡¡¡sub-subproblem problem problem
	   	  
a  single retrieval	b  simultaneous multiple retrieval	c  sequential retrieval
figure 1: solving a decomposable configuration problem.we are interested in automating the acquisition of adaptation knowledge for case-based design  in particular for configuration or formulation tasks  where the final solution consists of a more-or-less constant number of components with relatively limited interactions between them. the solution is usually determined by finding the correct  fillers  for components  as well as deciding the values of  filler  parameters. examples of this type of task are: tablet formulation  craw et al.  1   formulation of rubber compounds for tires  bandini and manzoni  1   and pc configuration  stahl and bergmann  1 .
¡¡in general  finding a solution for a design problem is made more tractable if the task can be decomposed. figure 1 contrasts a  a single step retrieval with two multi-step retrievals where  either b  multiple retrievals are merged  or c  the solution retrieved in one cycle informs the retrieval in the next cycle. decomposition has special advantages for cbr design  because it allows the use of multiple cases. this results in better problem space coverage  and is important for the sparse case-bases typical of cbr design systems. this is demonstrated in the cbr systems de¡äj`a vu  smyth and cunningham  1  and eadocs  netten and vingerhoeds  1 . configuration and formulation tasks are in particular suitable for solving by decomposition; e.g.  our cbrtfs uses decomposition for tablet formulation  craw et al.  1 . cbrtfs makes use of the fact that the interaction between solution components is not strong  and that there is a difference in the degree to which the choice of the different components is subject to constraints. thus it performs a sequential retrieval/adaptation  figure 1c   proceeding from the most constrained to the least constrained components. using the solutions to  earlier  subproblems  when searching for solutions to  later  subproblems  achieves good results  while keeping the complexity manageable.
¡¡the difficulty of acquiring adaptation knowledge was identified early in cbr research  kolodner  1 . however  overall  little research has been done on automating acquisition or learning adaptation knowledge. hanney and keane  have implemented a cbr system that estimates property values  and learns adaptation rules from cases stored in its case-base. dial  leake et al.  1  also learns adaptation knowledge  in this case for a cbr planning system. it stores previous successful adaptation strategies that combine domain independent abstract adaptation rules with search procedures for domain specific information.
¡¡there are simple adaptation methods suitable for adjusting a single numeric solution. these methods make use of lazy learning to acquire their adaptation knowledge  i.e.  at the point of retrieval they look at neighbouring cases to determine the amount of adaptation required. the simplest such method is weighted voting. another example of simple adaptation is the difference heuristic of mcsherry . this heuristic compensates for differences in feature values between the problem and the best matching case by searching for case pairs with the same difference in corresponding features. the major restriction of this method is the assumption that the estimated value is approximated well by an additive function. this severely limits its application area.
¡¡wilke et al.  present a framework for learning adaptation knowledge using knowledge-light approaches. simple algorithms  like optimisation or inductive learning  can make use of the knowledge contained in the case-base  the similarity measure  and possibly already existing adaptation knowledge  to learn or improve adaptation knowledge. wilke et al. suggest that one can consider optimisation of the parameter in weighted voting as an example of learning adaptation knowledge; we view this as learning retrieval knowledge jarmulak et al.  1 . such knowledge-light methods can not only be used for estimation or other simple cbr tasks  but can also be usefully applied for design tasks  especially if the design can be decomposed into subtasks  as is often the case with configuration or formulation.
¡¡because we are interested in methods of learning adaptation knowledge that are suitable for implementation in general-purpose cbr tools  simpler  knowledge-light methods  like those described by hanney and keane and wilke et al. seem more suitable  as opposed to the more powerful  but also more complex  methods used in dial. in the following  we use several knowledge-light approaches to acquire adaptation knowledge for a formulation task.
1	adaptation

figure 1: constructing an adaptation case.adaptation methods in cbr are typically classified in the following 1 categories: substitutional adaptation substitutes or modifies a parameter in the solution; transformational adaptation involves structural changes to the solution; and generative adaptation redoes the reasoning process that led to a solution  smyth and cunningham  1 . in this paper we are interested in substitutional adaptation  which lends itself best for automatic knowledgeacquisition. the other two methods  often used in design tasks  present more difficulties as far as knowledge acquisition is concerned. however  a decomposable design task allows the adaptation to be decomposed into simpler adaptation subtasks  often suitable for substitutional types of adaptation.
1	learning adaptation knowledge
the adaptation learning methods we present extract adaptation knowledge from the case data. we propose methods suitable for predicting values for nominal as well as numeric features; we call these targets. both are needed in configuration or formulation tasks where often the nominal-valued types of components are determined first  and then their numeric parameters are estimated.
constructing adaptation cases
one method we use to extract knowledge that is then used for adaptation  performs leave-one-out retrieval experiments and constructs  adaptation cases . figure 1 illustrates how each of the cases is used as a probe case. after retrieving the most-similar case s  from the reduced case-base  the probe and similar cases are analysed to construct an adaptation case that will be used to adapt the proposed solution from the similar case into the correct known solution for the probe case.
¡¡the way that adaptation cases for numeric targets are constructed is illustrated in figure 1. we store differences between the problem features of the probe and retrieved cases  and the difference between their solutions. this is sufficient if we expect a uniform dependency between the solution difference and the problem differences over the whole problem space. if  however  the dependency differs in various parts of the problem space  then it is advisable to include also the problem features of the probe and the proposed solution from the retrieved case  as shown in figure 1.
¡¡for nominal targets our adaptation cases are used to predict whether the solution to a probe case proposed by a retrieved case is a good solution. the construction of an adaptation case for nominal targets is illustrated in figure 1. firstly  the

figure 1: adaptation cases for numeric targets.
adaptation case must contain data describing the problem features of the probe and the solution proposed by the retrieved case. because we are dealing with a design task  it may also be possible to include two other types of data in the adaptation case. one type of information is an estimate  est  of important properties of the final design that make use of the proposed solution. we estimate these properties using cbr   a cbr system with the same case-data but with a differenttarget and a more instantiated probe. the final type of information that can be included in the adaptation case is simpler to obtain. given the predicted solution we determine feature s  from the problem description  rel  which are related to that particular solution. an example from the tablet formulation domainwould be chemical stability of a drug  part of problem description  with the chosen component  proposed solution .
constructing preference-profiles
in the method of constructing adaptation cases shown in figure 1  we estimated values of some design properties  est  and found some features  rel  related to the proposed solution. when placed in the adaptation case they are used to determine the correctness of the solution. but we can also use these values without constructing adaptation cases. we assume that the values of est & rel actually occurring in cases correspondto an acceptable range of values for designs 

figure 1: adaptation cases for nominal targets.
and construct  preference profiles  for these features which reflect the distribution of est & rel in the case data. figure 1 shows an example of a profile for the estimated feature typ: a  frequency distribution  of the values of typ in the case-data. such profiles can be also interpreted as membership functions for a fuzzy concept  preference . profiles  once constructed  are then used to estimate the  quality  of suggested solutions. the assumption is that values occurring often in the actual case-data are preferable. looking at the profile in figure 1  suppose two nearest neighbours  whose similarity to the problem case are virtually equal  have different solutions  one resulting in an estimated typ of 1  and the other with equal to 1. we would choose the first solution with the higher preference typ.

figure 1: preference profile.
1	applying adaptation knowledge
now we describe how adaptation cases or preference profiles are used in adaptation. adaptation cases are applied in a cbr system to determine the adaptation actions. another possibility would be to induce decision trees to decide the correctness of a nominal solution or to suggest an approximate adjustment for a numeric solution. we could also use other methods  e.g.  induce adaptation rules  hanney and keane  1 .
numeric targets
adapting values of numeric targets  is conceptually simple 
figure 1. given the differences between the problem and the retrieved case s  we use cbr   a cbr system with adaptation cases like figure 1  to predict adjustments to the proposed solutions. we can use just a single suggested correction  or updates from the nearest neighbours or several cbr systems and combine the adjustments.

figure 1: problem differences to adapt numeric solutions.
nominal targets
figure 1 shows how we can use information about the estimated correctness/quality of the solution in adaptation. the working cycle of the presented system is as follows:  1  given a new problem we retrieve cases from the case-base  these cases give us suggested solutions;  1  we estimate the correctness of the suggested solutions applied to our problem;  1  if the estimate of correctness suggest a high possibility of the solution being correct we return this solution;  1  otherwise  we store this solution with its correctness estimate  and  1  repeat retrieval  but this time ignoringall cases with the solution values which have already been tried out;  1  if all possible solutions have been tried without finding a really good one we will return the one that had the highest estimated correctness  giving preference to the solutions that were retrieved first  the nearest neighbours of the problem case . this approach might look like a search  however  the aspect distinguishing it is the order in which the solutions are tried - this order is determined by case similarity as defined by retrieval.
¡¡in this  iterative-adaptation  method the quality of a solution can be estimated using preference profiles; pprof in figure 1. to use the profiles  first  we have to obtain values of est & rel as we did to construct nominal adaptation cases in figure 1. if several profiles are used then their values can be combined using a weighted sum. if we consider  preference profiles  as fuzzy membership functions then their values can be combined using a fuzzy and operator  e.g.  a minimum or a product.
¡¡figure 1 also shows solution correctness being estimated using a cbr system containing adaptation cases like those in figure 1  instead of pprof. in this method  we use a combination of the case similarity and the accuracy of the leaves of the case-base index to give us a numeric estimate of the correctness of the solution.
1	experiments and results

figure 1: adaptation of nominal values by iterative  search  for solution.we have tested these adaptation methods on a simple design task - tablet formulation  craw et al.  1 . the design of a new tablet involves identifying inert substances called excipients to balance the properties of the drug so that the tablet is manufactured in a robust form  and the desired dose of drug is delivered and absorbed by the patient. excipients play the role of fillers  binders  lubricants  disintegrants and surfactants in the tablet. furthermore  a formulation specifies the quantity of each of the added excipients. the difficulty of the formulation task arises from the need to select a set of excipients compatible with the drug  whilst at the same time satisfying a variety of soft and hard constraints. current tablet formulation practice suggests that this design task is decomposable into subtasks consisting of the choice of excipient component plus determining the excipient amount. it is possible to find a sequence in which the components will be determined  from the most constrained  filler  to the least constrained  surfactant  so that most tablets can be formulated without the need to backtrack in the formulation process.
¡¡we conducted adaptation experiments on a case-base containing 1 formulations. table 1 shows the results for all tablet formulation components  except those where retrieval with no adaptation already achieves 1% accuracy. entries of the form  -  indicate there was no gain from adaptation. the table shows the accuracy as measured by the percentage of acceptable solutions for default cbr retrieval and our automatically optimised retrieval  jarmulak et al.  1 . it also shows the gains from using adaptation cases  cbr  . for nominal targets  the gains from preference profile adaptation  pprof  are shown in brackets. the accuracies and gains are averaged over repeated cross-validation experiments that separate the available data into case-data and test problems.
¡¡we can see that for several tablet formulation subtasks learned adaptation did indeed bring improvements. we were especially interested in improving the results for the three tasks for which optimised retrieval with no adaptation had accuracies below 1%. therefore  we are pleased with the improvements in the tabletsrs and binder prediction. for
cbr /retrievaladaptationoptimisedadaptation pprof only  % gain  % only  % gain  % filler1-  - 1-  - binder11  1 11  1 disinteg.11  1 11  - filler1111binder1-1-lubricant1111typfiller1111tsrs1-11table 1: percentage accuracy and adaptation gain
the latter task  we notice that the pprof method gives much better results than cbr . preference profiles capture the notion of optimality directly  representing the distribution of feature values for a given prediction; cbr relies on more local feedback. moreover  preference profiles are less susceptible to data over-fitting since their knowledge is based on larger training sets.
¡¡because no gains were achieved for the filler excipient task  we analysed this task more closely by looking at the index trees of the cbr system  see figure. 1. our expert confirmed that parts of the index reflected correct domain knowledge. all est. tsrs nodes correctly capture the preference for tablets with low srs. however  other nodes were incorrect  mainly due to the lack of sufficient training examples  adaptation cases . for example  the second level nodes under cca and man are obviously incorrect  and the nodes labelled fillerrelated represent the opposite of the domain knowledge. it is encouraging that the knowledge contained in the case-base index was so easy to interpret. it is also easy to make corrections to the index. this suggests that learning adaptation knowledge should be followed by a review and refinement of that knowledge by a domain expert. this would still require considerably less effort than acquiring all the adaptation knowledge from the expert.

figure 1: example case-base index for cbr .
1	conclusions & future work
our research has identified several  knowledge-light  methods of learned adaptation suitable for implementation in general purpose cbr tools. the tool's adaptation phase must allow the preference profile or cbr systems to be invoked and repeated retrieval to be undertaken. since both adaptation methods are based around cases  the cbr tool itself could be the adaptation platform.
¡¡the methods described are suitable for decomposable design tasks as illustrated by our tablet formulation problem. for these adaptation subproblems we have developed effective methods for learning adaptation knowledge. these methods are generic and particularly suited to component-based design because they use constraints and optimality of the components. tasks other than design may also benefit; features describing the suitability of a proposed solution would be used as the estimated feature.
¡¡in our experiments  we found that  depending on the task  different methods  like pprof or cbr   may bring better results. this is in part due to the inherent differences in the design tasks: some may be guided mainly by a search for optimal components while others may use constraints to reject unsuitable components. we therefore think that cbr tools should be equipped with a choice of learned adaptation methods  from which the user selects those giving better results.
¡¡an important advantage of the methods presented here is that the acquired knowledgeis relatively easy to interpret  e.g  the adaptationcases  the cbr systems that use them  and the adaptation profiles. our further research will concentrate on refining acquired knowledge. we intend to study methods of presenting the learned adaptation knowledge to an expert  in order to obtain  and then process  expert feedback with the goal of further improving the cbr adaptation stage.
