
a word sense disambiguation  wsd  system trained on one domain and applied to a different domain will show a decrease in performance. one major reason is the different sense distributions between different domains. this paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. even though our training examples are automatically gathered from parallel corpora  the sense distributions estimated are good enough to achieve a relative improvement of 1% when incorporated into our wsd system.
1 introduction
many words have multiple meanings. the process of identifying the sense of a word in a particular context is known as word sense disambiguation  wsd . wsd is an important task of natural language processing  nlp   and is important for applications such as machine translation and information retrieval. past research has shown that a good approach to wsd is the use of supervised machine learning. with this approach  a large text corpus in which each ambiguous word has been annotated with the correct sense has to be collected to serve as training data.
모this corpus-based approach  however  raises the problem of domain dependence faced by these supervised wsd systems. a system trained on data from one domain  for example  sports   will show a decrease in performance when applied to a different domain  for example  economics . this issue of domain difference affecting wsd performance had been brought up by various researchers. escudero et al.  showed that training a wsd system in one domain and applying it to another could lead to a drop of 1% to 1% in wsd accuracy. it was shown that one of the reasons is due to differentdistributions of senses across domains  as a 1% improvement in accuracy was observed when they sense-balanced the instances from the different domains. recently  agirre and martinez  conducted experiments with training data for wsd which were automatically gathered from the internet. they reported a potential improvement of 1% in accuracy if they have access to the sense distribution of their test data.
모the work of these researchers showed that the different sense distributions across domains have an important effect on wsd accuracy. therefore  estimation of the target domain's sense distribution will be crucial  in order to build wsd systems that are portable across different domains. a partial solution to this would be to determine the predominant  or most frequently occurring sense of each word in a corpus. research has shown that the baseline heuristic of selecting the most frequently occurring sense of a word gives high wsd accuracy. this is due to the often skewed distribution of word senses. mccarthy et al.  1b  addressed this when they presented a method to automatically rank word senses in a corpus  in order to determine the predominant sense of each word. they obtained good results when the method is applied on the senseval-1 english all-words task  palmer et al.  1 . the same method is used in a related work  mccarthy et al.  1a  to identify infrequently occurring word senses.
모in the machine learning literature  algorithms to estimate class a priori probabilities have been developed. in this paper  we present novel application of two distribution estimation algorithms to provide estimates of the sense distribution  or a priori probabilities of senses  and show that they are effective in improving the wsd accuracy of systems trained and applied on different domains. note that determining the predominant sense is a special case of trying to estimate the complete sense distribution. to the best of our knowledge  these algorithms have not been used in nlp or for wsd.
모we recognize that a wsd system should be scalable. this is a problem faced by current supervised learning systems  as they usually relied on manually annotated data for training. to tackle this problem  our prior work ng et al.  1  exploited parallel texts for wsd. encouraging results were obtained in our evaluation on the nouns of senseval-1 english lexical sample task  kilgarriff  1   which used the wordnet 1 sense inventory  miller  1 . note that the usage of parallel texts as training data represents a natural domain differencewith the test data of senseval-1 english lexical sample task  of which 1% is drawn from the british national corpus  bnc . in this paper  we chose to draw training data for our experiments from parallel texts  and similarly base our evaluation on the nouns of senseval-1 english lexical sample task.
parallel corporasize of english textssize of chinese texts in million words  mb   in million characters  mb  hong kong hansards1  1 1  1 hong kong news1  1 1  1 hong kong laws1  1 1  1 sinorama1  1 1  1 xinhua news1  1 1  1 english translation of chinese treebank1  1 1  1 total1  1 1  1 table 1: size of english-chinese parallel corporathis paper is structured as follows. in the next section  we discuss the process of gathering training data from parallel texts. in section 1  we describe the two distribution estimation algorithms used. we also give a brief description of the predominant sense method presented by  mccarthy et al.  1b . in section 1  we evaluate the effectiveness of the two algorithms and the predominant sense method in improving wsd accuracy. evaluation results on the nouns in senseval-1 english lexical sample task are presented. discussions are made in section 1  before we conclude in section 1.
1 training data from parallel texts
in this section  we briefly describe the parallel texts used in our experiments  and the process of gathering training data from them.
1 parallel text alignment
table 1 lists the 1 english-chinese parallel corpora  available from linguistic data consortium  used in our experiments. the sentences of the 1 corpora were already pre-aligned  either manuallyor automatically  when they were prepared. after ensuring the corpora were sentence aligned  we tokenized the english texts  and perform word segmentation on the chinese texts. next  word alignment was done on the parallel corpora using the giza++ software  och and ney  1 .
1 target translations selection
we took the same approach as described in our previous work  ng et al.  1  to select some possible chinese translations for each sense of an english word. for instance  wordnet 1 lists 1 senses for the noun nature. senses will be lumped together if they are translated in the same way in chinese. for example  sense 1 and 1 of nature are lumped together as they are both translated as  ddd  in chinese. then  from the word alignment output of giza++  we select those occurrences of the noun nature which have been aligned to one of the chinese translations chosen. the english side of these occurrences will then serve as training data for the noun nature  as they are considered to have been disambiguated and  sense-tagged  by the appropriate chinese translations.
모the time needed to perform manual target translations of word senses is relatively short  compared to the effort needed to manually sense annotate training examples. also  this step could potentially be automated with the use of an englishchinese dictionary.
1 distribution estimation
to estimate the sense distribution  or a priori probabilities of the different senses in a new data set  we made use of a confusion matrix algorithm  vucetic and obradovic  1  and an em based algorithm  saerens et al.  1   which we describe in this section. a brief description of the predominant sense method  mccarthy et al.  1b  is also given  before we discuss how to make use of the estimated a priori probabilities to improve classification accuracy.
1 confusion matrix
let us assume that from a set of labeled data sl  a classifier is used to compute the conditional probabilities   which is an estimate of the probability of classifying an instance as class 뷎j   when in fact it belongs to class 뷎i . then  one can apply this classifier with known conditional probabilities to a set of unlabeled data su to obtain its predictions. from these predictions  one can estimate the probability of predicting class 뷎j on su  which we will denote as q 뷎j  . the a priori probabilities p 뷎i  on su are then estimatedb by solving the following equation:b
n
	q	p 뷎i  	j = 1 ... n	 1 
whereof classes. equation  1  can be represented in matrix form as q =p몫p  from which the a priori probabilities on su  representedby p   can be estimated by solving:
	p = p 1 몫 q	 1 
to obtain estimates of p and q  bootstrap sampling  efron and tibshirani  1  is employed. we will first describe the basic idea of bootstrap before giving the bootstrap algorithm.
모in bootstrap sampling  given an original sample x with n examples  bootstrap sample x  is obtained by randomly sampling n examples from x with replacement. to estimate the conditional probabilities pl 뷎j|뷎i   we need to split sl into strain and stest. further b let us define ntest    뷎j 뷎i  as the number of examples in a bootstrap sample stest predicted to be of class 뷎j when true class is 뷎i. also  we define as the numberof examplesin a bootstrap sample su  predicted to be of class 뷎j. we now give the bootstrap algorithm below: given stest  su  and a classifier  repeat the following for b iterations  in our experiments  we set b to 1 :
  generate a bootstrap sample from ntest examples of stest and calculate:
	n 	뷎  뷎
for i j = 1 ... n
  generate a bootstrap sample from nu examples of su and calculate:   	nu 뷎j 
		for j = 1 ... n
  use  1  to calculate p  뷎i  b for i = 1 ... n
after	b	iterations 	estimateb	the	a	priori probabilities
 b 
1 em based algorithm
once again  let us assume we train a classifier on a set of labeled data sl with n classes. further  suppose we have a set of n independent instances  or independent realizations of the stochastic variable x   x1 x1 ... xn  from a new data set. the likelihood of these n instances can be defined as:
n
l x1 x1 ... xn  = y p xk 
k=1
	n	n
= y xp xk 뷎i #
k=1 i=1
	n	n
= y xp xk|뷎i p 뷎i #  1 
                                   k=1 i=1 if we assume the generation of the instances within the classes  and thus the within-class densities p xk|뷎i   i.e.  the probabilities of observing xk given the class 뷎i  do not change from the training set sl to the new data set  we can define p xk|뷎i  = pl xk|뷎i  . to determine the a priori probability estimates p 뷎i  of the new data set that will maximize the likelihood ofb 1  with respect to p 뷎i    we can apply the iterative procedure of the em algorithm. in effect  through maximizing the likelihood of  1   we obtain the a priori probability estimates as a by-product.
모before we show the steps of the em algorithm  it will be helpful to define some notations. when we apply the classifier trained on sl on an instance xk drawn from the new data set su  we get pl 뷎i|xk    which we define as the probability of instance xbk being classified as class 뷎i by the classifier trained on sl. further  let us define pl 뷎i  as the a priori probabilities of class 뷎i in sl   which canb be estimated by the class frequency of 뷎i in sl. also  we will define p s  뷎i  and pb s  뷎i|xk  as estimates of the new a priori andba posteriori probabilities at step s of the iterative em procedure. assuming we initialize p 1  뷎i  by the a priori probabilities of the classes in the labeledb data:
	p	pl 뷎i 
then for each new instance	su  and each class 뷎i  the
em algorithm provides the following iterative steps:
		 1 
		 1 
where  1  represents the expectation e-step   1  represents the maximization m-step  and n represents the number of instances in su. note that in  1   the probabilities pl 뷎i|xk  and pbl 뷎i  will stay the same through the iterationb steps s for each particular instance xk and class 뷎i . observe that for  1   the new a posteriori probabilities p s  뷎i|xk  at step s are simply the a posteriori probabilities inb the conditions of the labeled data  pl 뷎i|xk   weighted by the ratio of the new priors pb s  뷎i  tobthe old priors pbl 뷎i . the denominator in  1  is simply a normalizing factor.
모at each iteration step s  both the a posteriori p s  뷎i|xk  and a priori probabilities pb s  뷎i  are re-estimatedb sequentially for each new instance xk and each class 뷎i  until the convergence of the estimated probabilities p s  뷎i . this iterative procedure will increase the likelihoodb of  1  at each step.
1 predominant sense
a method of automatically ranking wordnet senses to determine the predominant  or most frequent sense  of a noun in the bnc corpus is presented in  mccarthy et al.  1b . a thesaurus is first acquired from the parsed 1 million words of written english from the bnc corpus to provide the k nearest neighbors to each target noun  along with the distributional similarity score  dss  between the target noun and its neighbor. the wordnet similarity package  pedersen et al.  1  is then used to give a wordnet similarity measure  wnss   which is used to weight the contribution that each neighbor makes to the various senses of the target noun. through a combination of dss and wnss  a prevalence score for each sense of the target noun is calculated  from which the predominant sense of the noun in bnc is determined.
모though the focus of the method is to determine the predominant sense  we could obtain an estimated sense distribution by normalizing the prevalence score of each sense. as noted in section 1  1% of the test examples of senseval1 english lexical sample task were drawn from bnc. thus  it is reasonable to expect the sense distribution estimated by this predominant sense method to be applicable to the test examples of senseval-1 english lexical sample task. we implemented this method in order to evaluate its effectiveness in improving wsd accuracy. our implementation achieved accuracies close to those reported by  mccarthy et al.  1b . mccarthy et al.  1b  reported a jcn measure predominant sense accuracy of 1% on the semcor corpus  while we measured 1% using our implementation. on the senseval1 english all-words task  a 1% precision and 1% recall was reported. our implementation gave 1% precision and 1% recall. the differences could be due to some minor processing steps which were not described in detail in  mccarthy et al.  1b . finally  note that this predominant sense method is only effective on a very large text corpus  as the thesaurus can only be effectively generated from a very large corpus.
maximum number of parallel examples per nounlem sampletrue   lem   lconfm   lpredominant   ladjustsampleadjustsampleadjustsampleadjustsample1.1.1.1.1.1.1.1.1.1.111111111111.1.1.1.1.1.1.1.1.1.11111111111table 1: 1 trials micro-averaged scores for 1 se1 nouns
maximum number of parallel examples per nounem   lconfm   lpredominant   ladjustsampleadjustsampleadjustsample1.1.1.1.1.1.11111111.1.1.1.1.1.1111111table 1: relative improvement percentage from using the a priori probabilities estimated by the 1 methods1 improving classification based on a priori estimates
if a classifier was trained to estimate posterior class probabilities when presented with a new instance xk from
su  it can be directly adjusted according to estimated class probabilities p 뷎i  on su.
모denoting predictionsb of the classifierb as pbl 뷎i|xk   a priori probability of class 뷎i from sl as pl 뷎i   estimated a priori probability of class 뷎i from su as p 뷎i   adjusted predictions pbadjust 뷎i|xk  can be calculated as:b i
pl 뷎i|xk pbpb  뷎뷎  
padjust 뷎i|xk  = pbj pbl 뷎j|xk l pbl i 뷎뷎jj  	 1  pb
note thesimilarity of  1  with  1 .
1 evaluation
in our experiments  we implemented the supervised wsd approach of  lee and ng  1   which was reported to achieve state-of-the-art accuracy. the knowledge sources used include parts-of-speech  surrounding words  and local collocations. we use the naive bayes algorithm as our classifier  which was reported to achieve good wsd accuracy and it is fast to train and test with the naive bayes classifier. also  the naive bayes classifier outputs posteriori probabilities in its classifications  which can be used directly to adjust the predictions based on  1 .
1 results
we used the approach outlined in section 1 to obtain training examples from parallel texts. two senses s1 and s1 of a noun are lumped together if they are translated into the same chinese word. although there are 1 nouns in the senseval-1 english lexical sample task  the senses of 1 nouns are lumped into one sense  i.e.  all senses of each of these 1 nouns are translated into one chinese word . thus  we limit our evaluation to the remaining 1 nouns.
모saerens et al.  reported that increasing the training set size results in an improvement in the a priori estimation. to investigate this issue  we gradually increased the maximum number of examples per noun selected from the parallel texts. for each condition  we sampled training examples from the parallel text using the natural distribution of the parallel text  up to the maximum allowed number of examples per noun. we noted the wsd accuracy achieved by the classifier without any adjustment  in the column labeled l in table 1. to providea basis for comparison  we adjusted the classifier posteriori probability output by the true sense distribution of the test data  using equation  1 . the increase in wsd accuracy thus obtained is listed in the column adjust under true l. for example  if we knew exactly the true sense distribution of the test data  we would have achieved an accuracy of 1% + 1% = 1%  when trained on a maximum of 1 examples per noun.
모besides adjusting the classifier posteriori probability output  one could also re-sample training examples from the parallel texts according to the true sense distribution. the increase in wsd accuracy obtainedusing this approachis listed in the column sample under true l. note that scores listed under true l indicate the upper-bound accuracy achievable had we known the true sense distribution of the test data.
모increase in wsd accuracy by adjusting the classifier output using the sense distribution estimated by the em algorithm is listed in column adjust under em l. corresponding increase by re-sampling training examples from the parallel texts according to the sense distribution estimated by the em algorithm is listed in column sample under em l. similar increases by using sense distribution estimated by the confusion matrix algorithm  and the predominant sense method  are listed under the columns adjust and sample  under confm l and predominant l. the corresponding relative improvement of the various accuracy improvements  comparingagainst using the true sense distribution achieved by the two algorithms and the predominant sense method is given in table 1. as an example  for 1 parallel examples

figure 1: comparison of parallel-em's performance against senseval-1 participating systems. the single shaded bar represents parallel-em  the empty white bars represent the supervised systems  and the pattern filled bars represent the unsupervised systems.per noun  em adjust gives an improvement of 1%  against an improvement of 1% using true adjust. thus  relative improvement is 1/1 = 1%.
모the results show that the confusion matrix and em algorithms are effective in improving wsd accuracy. also  the two algorithms are most effective when a large training data set  which is 1 examples per noun in our experiments  is used. the results also indicate the potential of parallel text scaling up to achieve better wsd accuracy as more training examples are used  as shown by the increasing accuracy figures under the l column of table 1. this is a phenomenonnot explored in  ng et al.  1   which used the same number of parallel text examples as the official senseval-1 training data. at the same time  however  this caused a progressive reduction in the absolute amount of possible wsd accuracy improvement  as indicated by the reduction under the columns adjust and sample of true l in table 1 when more parallel texts examples were used. our experiments also indicate that application of the two algorithms achieved higher increases in wsd accuracy when compared against the predominant sense method  which has the primary aim of determining the predominant sense of a word in a corpus.
모we note that re-sampling the training examples gives better wsd improvements than merely adjusting the posteriori probabilities output by the classifier. the number of training examples for a sense will affect the accuracy of the classifier trained for that sense. thus  if one has a reliable estimate of the sense distribution  it might be more beneficial to resample the training examples. the results show that sampling a maximum of 1 examples per noun from the parallel texts according to the distribution estimated by the em algorithm gives our best achievable wsd accuracy of 1%  1% + 1% . in the column em sample of table 1  we show the wsd accuracy resulting from sampling parallel text examples according to the em estimated sense distribution  with different maximum number of examples per noun  i.e.  this column is obtained by adding the figures from the columns l and em l sample .
1 comparison with se1 systems
we	compare	our	wsd	accuracy	achieved	with	the
senseval-1 english lexical sample task participating systems. we performed sense lumping on the publicly available predictions of the senseval-1 participating systems. the wsd accuracy of each system is then calculated over the set of 1 nouns. in the senseval-1 english lexical sample task  there were 1 supervised and 1 unsupervised systems. figure 1 shows the relative ranking of our system  which we named parallel-em  against the various se1 systems.
모we note that our system achieved better performance than all the unsupervised systems  and would have ranked at the 1% percentile among the list of 1  including ours  supervised systems.
1 discussion
the score of 1% achieved by parallel-em is competitive  considering that the se1 supervised systems are trained with the manually annotated official data  while our system was trained with examples automatically extracted from parallel texts. moreover  the following factors will also have to be considered.
official training data has similar sense distribution as test data
this is an advantage that the participating senseval-1 supervised systems enjoy. note that the results in table 1 show that if we sample according to the true distribution with a maximum of 1 examples per noun  we could achieve a
score of 1%.
some official training and test examples are from the same document
our prior work  ng et al.  1  highlighted the fact that many of the official senseval-1 training and test examples are extracted from the same document  and that this has inflated the accuracy figures achieved by senseval-1 participating systems.
모to investigate this effect  we trained our wsd classifier using the officialtraining examples. when evaluated on the official test examples  a wsd accuracy of 1% was achieved. next  we followed the steps taken in  ng et al.  1 : we took the official training and test examples of each noun w and combined them together. the combined data is then randomly split into a new training and a new test set such that no training and test examples come from the same document. while performing the split  we ensured the sense distribution of the new training and test data sets followed those of the official data sets. a wsd classifier was then trained on this new training set  and evaluated on this new test set. 1 random trials were conducted  and our wsd classifier achieved an accuracy of 1%  a drop of 1% from 1% .
lack of parallel training examples for some senses
a disadvantage faced by parallel-em is that  though the senses are lumped  we are still lacking training examples for 1% of the test data. we believe that with the steady increase in volume and variability of parallel corpora  this problem will improve in the near future.
domain difference
lastly  we note that 1% of the senseval-1 english lexical sample task test examples were drawn from bnc. this represents a domain difference with our parallel corpora  which is mainly a mix of legislative  law  and news articles. examples from different domains will contain different information  thus presenting different classification cues to the learning algorithm.
1 conclusion
differences in sense distribution between the training and test sets will result in a loss of wsd accuracy. addressing this issue is important  especially if one is to build wsd systems that are portable across different domains  of which the difference in sense distributionis an inherentissue. in this paper  we used two distribution estimation algorithms to provide estimates of the sense distribution in a new data set. when applied on the nouns of the senseval-1 english lexical sample task  we have shown that they are effective in improving wsd accuracy.
acknowledgements
the first author is supported by a singapore millennium foundation scholarship  ref no. smf-1 . this research is also partially supported by a research grant r1-1 from national university of singapore academic research fund.
