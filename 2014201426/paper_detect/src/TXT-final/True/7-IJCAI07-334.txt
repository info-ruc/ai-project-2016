
in human-robot interaction  hri  it is essential that the robot interprets and reacts to a human's utterances in a manner that reflects their intended meaning. in this paper we present a collection of novel techniques that allow a robot to interpret and execute spoken commands describing manipulation goals involving qualitative spatial constraints  e.g.  put the red ball near the blue cube  . the resulting implemented system integrates computer vision  potential field models of spatial relationships  and action planning to mediate between the continuous real world  and discrete  qualitative representations used for symbolic reasoning.
1 introduction
for a robot to be able to display intelligent behaviour when interacting with humans  it is important that it can reason qualitatively about the current state of the world and possible future states. being an embodied cognitive system  a robot must also interact with the continuous real world and therefore must link its qualitative representations to perceptions and actions in continuous space. in this paper  we present an implemented robot system that mediates between continuous and qualitative representations of its perceptions and actions.
　to give an impression of the robot's capabilities  consider a hypothetical household service robot which is able to accept an order to lay the dinner table such as  put the knives to the right of the plate and the forks to the left of the plate.  the robot has to interpret this utterance and understand it as a goal it must achieve. it has to analyse its camera input to find the objects referred to in the owner's command and it must also interpret the spatial expressions in the command in terms of the camera input. finally  it must plan appropriate actions to achieve the goal and execute that plan in the real world. in this paper  we present a system that is able to accomplish such tasks. in our domain we use cubes and balls in place of cutlery as our robot's manipulative abilities are limited  see figure 1 . in this domain our system acts on commands such as   put the blue cube near the red cube  and  put the red cubes and the green balls to the right of the blue ball .
　we are particularly interested in the consistent interpretation and use of spatial relations throughout the modalities available to a robot  e.g. vision  language  planning  manipulation . for their different purposes  these modalities use vastly different representations  and an integrated system must be able to maintain consistent mappings between them. this is a hard problem because it means mediating between the quantitative information about objects available from vision  e.g. where they are in the world   the qualitative information available from language  e.g. descriptions of objects including spatial prepositions   the qualitative information that must be generated to reason about actions  e.g. hypothetical future configurations of objects   and the quantitative information required by an action system in order manipulate objects. additionally  when a robot interacts with humans  mediation capabilities must extend across system borders: the robot must be able to interpret the intended meaning of human input in terms of its own representational capabilities and react in a way that reflects the human's intentions. our system makes the following contributions in order to tackle these problems:
i  planning-operator driven interpretation of commands: we describe a generic method which uses formal planning operators to guide the interpretation of commands in natural language and automatically generates formal planning goals. referential expressions in the goal are kept for  lazy  resolution by the planner in the context of a given world state. this allows replanning to dynamically adapt behaviour without having to re-evaluate commands.
ii  spatial model application: we use potential field models of spatial prepositions to generate qualitative representations of goals that satisfy a human command. this model accords with the ways that humans talk about spatial relations  costello and kelleher  1 . this approach allows us to generate discrete solutions that fit typical human descriptions of continuous space.
although the individual techniques are still somewhat limited

figure 1: the system architecture.
in scope  by combining them we provide each component in the architecture access to more information than it would have in isolation. thus  the overall system is able to demonstrate intelligent behaviour greater than the sum of its parts.
　in the following section we describe our robot platform and the system architecture. we then expand on this in sections ′1  planning domain   ′1  command interpretation   ′1  potential field models for spatial relations  and ′1  qualitative spatial information from vision . finally ′1 presents an example of the functionality of the complete system.
1 relation to previous work
the work presented in this paper is related to various subfields of robotics and artificial intelligence. in particular it is closely related to human-robot interaction and situated language understanding  kruijff et al.  1 . we do not focus solely on the process of understanding an utterance  but instead examine the steps necessary to mediate between the various representations that can exist in systems that must act on the world as a result of a command in natural language.
　in terms of gross functionality there are few directly comparable systems  e.g. those presented in  mavridis and roy  1  and  mcguire et al.  1 . whereas these systems specify complete architectures for following manipulation commands  we focus on a particular aspect of this behaviour. as such our approach could be utilised by existing systems.
for example  it could be used in layer l1 of mavridis and roy's grounded situation model  to produced discrete  categorical encodings of spatial relationships.
there are many plan-based dialogue systems that are used
 or potentially usable  for hri  e.g.   sidner et al.  1; allen et al.  1  . most such systems try to exploit the context of the current  dialogue  plan to interpret utterances. we are not aware  however  of any system that  like ours  actually uses the formal action representation from the planning domain to resolve referential expressions in at least a semiformal way. critically  the  guidance  provided by the planning domain leads to a logical representation of the command that the planner can reason about. for example  the planner is able resolve referential expressions as part of the problem solving process. this can be significant in dynamic environments: if a situation changes then the planner can resolve the same referential expression differently.
1 architecture
to enable a robot to follow action commands such as those described in the introduction  we break the problem into a number of processing steps. these steps are reflected by the design of our overall processing architecture  which can be
 :action put
:parameters
  a - agent  obj - movable  wp - waypoint 
:precondition  and
 pos  obj :  a 
 not  exists   obj1 - movable   pos  obj1 :  wp    
:effect
 pos  obj :  wp  
figure 1: mapl operator put for placing objects.
seen in figure 1. our system is based on a combination of an irobot b1r mobile robot and a table-mounted katana 1m robotic arm. mounted on the b1r is a pan-tilt unit supporting two parallel cameras which we use for visual input. from these cameras we create a 1d representation of the scene using depth information from stereo to instantiate a collection of simple object models. to produce actions  information from vision is fed into a workspace-based visual-servoing system along with instructions about which object to grasp  and where to put it. actions are limited to pick and place. this suffices for the current experimental scenarios.
1 planning domain
for the purpose of this paper  a simple ontology was designed which consists mainly of agents and objects. objects may be movable or not. they can have properties  e.g. colours  that can be used to describe them or constrain subgroups of objects in a scene. positions of objects in a scene are described by waypoints. concrete instances of waypoints are generated on-the-fly during the problem-solving process  cf. ′1 . relations between waypoints include near  right of  and left of. despite being quite simple  this ontology allows us to represent complex situations and goals. moreover  it is very easy to extend to richer domains. for example  adding just one new subtype of movable objects would enable the robot to distinguish between objects that are stackable.
　the ontology has been modelled as a planning domain in mapl  brenner  1   a planning language for multiagent environments based on pddl  fox and long  1 . mapl is suitable for planning in hri because it allows us to model the beliefs and mutual beliefs of agents  sensory actions  communicative actions  and different forms of concurrency. although these features make mapl highly suitable for humanrobot interaction  in this paper we mostly use the adl subset of mapl. figure 1 shows the operator for placing objects. note that mapl uses non-boolean state variables  e.g.  pos obj   which are tested or changed with statements like  pos obj :  wp . thus  in mapl there is no need to state that the robot no longer holds the object after putting it down  a statement which would be necessary in pddl .
　currently  no planner is available that is specifically designed for mapl. instead  we use a compiler for transforming mapl into pddl and back. this enables us to use a state-of-the-art planner in our system without losing the descriptive power of mapl; the planning system used currently is ff  hoffmann and nebel  1 .
1 converting linguistic input to mapl
in ai planning  goals are typically formulated in  a subset of  first-order logic  i.e. as formulae that must hold in the state achieved by the plan  see  for example  the definition of goals in the adl subset of pddl  fox and long  1  . humans  however  usually use imperative commands  like  clear the table   when communicating goals. one reason for verbalising an action command instead of a goal description could be that the former provides a very compact representation of the latter by means of its postconditions  i.e. the immediate changes to the world caused by the action. speaking in ai planning terms  if the action  clear table  has an adl effect saying that after its execution  there exists no object that is on the table   the action name plus its parameters is a much simpler means to convey that goal than the effect formula. what complicates the matter is that  in contrast to ai planners  humans usually do not use unique names for objects  but refer to them in expressions that constrain the possible referents  i.e.  the red ball  instead of object1 . altogether  the  human way  to describe goals can be described as goal = action + parameters + reference constraints.
　deliberative agents that have adl-like action representations can exploit this goal description scheme when trying to understand a natural language command: after matching the verb phrase of a command with an appropriate planning operator  this operator can be used to guide the further understanding of the command  namely determining the action parameters and reference constraints.
　we will illustrate this process with the command  put the blue cubes to the left of the red ball . our system parses the command using a simple english grammar and a chart parser. the parse tree of the example command describes the phrase as a verb  followed by a nominal phrase and a prepositional phrase  v np pp . when the system detects the verb  put   it is matched to the planning operator put  cf. figure 1 . the subsequent interpretation procedure is specific to that operator and aims at determining the constraints describing the three parameters of the operator   a   obj and  wp. this prior knowledge drives the interpretation of the phrase and simplifies this process significantly. in our example  the np is interpreted to describe the object  obj that is to be moved while the pp describes the target position  wp. the following logical constraint on the parameters  a   obj and  wp is found  in which  obj1 is the landmark1 object in relation to which the goal position is described :  blue  obj  … type  obj : cube  …
  wp1.   left-of  wp  wp1  …   obj1.   red  obj1  …
 type  obj1 : ball  …  pos  obj1 : wp1   
additionally  the interpretation states that all objects satisfying the constraints on  obj must be moved. this quantification becomes visible in the final translation of the command into a mapl goal  shown in figure 1  where type constraints are transformed into the types of the quantified variables .
　one important aspect of the natural language command is that it refers both to the goal state  where should the blue
 forall   obj - cube   imply
 and  initially  blue  obj   
 exists   wp - waypoint 
 exists   obj1 - ball  wp1 - waypoint   and
 initially  red  obj1  
 initially  pos  obj1 :  wp1  
 initially  left-of  wp  wp1  
 pos  obj :  wp      
figure 1: automatically generated mapl goal for  put the blue cubes to the left of the red ball 
cubes be put   and to the initial state  the reference constraints determining the objects . it is crucial for the planning representation to be able to model this difference  otherwise contradictory problems may be generated. for example  the command  put down the object that you are holding  provides two constraints on the object's position: that it is held by the robot now  but is on the ground after plan execution. therefore  mapl supports referring back to the initial state in the goal description as shown in figure 1. the facts that must hold after execution of the plan are described by the effect of the put action. in our example  this effect describes the new position of the object in question.
　it is important to realise that the goal descriptions generated by this process still contain the referential expressions from the original command  i.e. they are not compiled away or resolved directly. instead they will be resolved by the planner. we call this  lazy  reference resolution. it enables the robot to dynamically re-evaluate its goals and plans in dynamic situations. if  for example  another blue cube is added to the scene  the planner will adapt to the changed situation and move all of the blue blocks.
1 computational models of spatial cognition
to act on the kinds of action commands we are interested in  the robot must be able to translate from the qualitative spatial linguistic description of the location to place the object  to both a geometric description of the location that can be used by the manipulation system  i.e. a geometric waypoint positioned in the robot's world   and a logical description for the planning domain  i.e. a symbolic representation of this waypoint and its relationships with other waypoints . this translation involves constructing computational geometric models of the semantics of spatial terms.
　spatial reasoning is a complex activity that involves at least two levels of representation and reasoning: a geometric level where metric  topological  and projective properties are handled; and a functional level where the normal function of an entity affects the spatial relationships attributed to it in a context. in this paper we concentrate on the geometric level  although using functional spatial information would not require any significant changes to our overall system.
　psycholinguistic research  logan and sadler  1; regier and carlson  1; costello and kelleher  1  indicates that people decide whether the spatial relation associated with a preposition holds between and landmark object and the regions around it by overlaying a spatial template on the landmark. a spatial template is a representation of the regions of acceptability associated with a given preposition. it is centred on the landmark  and for each point in space it denotes the acceptability of the spatial relationship between it and the landmark. figure 1 illustrates the spatial template for the preposition  near  reported in  logan and sadler  1 .
111111111111111111111111o111111111111111111111111figure 1: mean goodness ratings for the relation near.
　if a computational model is going to accommodate the gradation of a preposition's spatial template it must define the semantics of the preposition as some sort of continuum function. a potential field model is one widely used form of continuum measure  olivier and tsujii  1; kelleher et al.  1 . using this approach  a spatial template is built using a construction set of normalised equations that for a given origin and point computes a value that represents the cost of accepting that point as the interpretation of the preposition. each equation used to construct the potential field representation of a preposition's spatial template models a different geometric constraint specified by the preposition's semantics. for example  for projective prepositions  such as  to the right of   an equation modelling the angular deviation of a point from the idealised direction denoted by preposition would be included in the construction set. the potential field is then built by assigning each point in the field an overall potential by integrating the results computed for that by point by each of the equations in the construction set. the point with the highest overall potential is then taken as the location that the object should be placed at to satisfy the relationship.
1 qualitative representations from vision
the previous sections have discussed the representation we use for planning  how we translate action commands into goal states in this representation  and how we model spatial relationships. this section describes a process that produces an initial state description for the a planning process by applying these techniques to mediate between geometric visual information and the symbolic planning representation.
　we break the task of generating a state description from vision and language into three steps: converting information about visible objects into a symbolic representation  adding information about specific spatial relationships to this representation  and generating new information required by the planning process. these last two steps use potential field models in two different ways. the first applies them to known waypoints in the world to generate logical predicates  e.g.
for rel in goal relationships do
for wpl in waypoints do initialise scene sc
add landmark wpl to sc for wpt to in waypoints   wpl do add waypoints   wpl   wpt to s as distractors compute field pf for rel in s check value val of pf at wptarget if val   1 then add  rel wpt wpl  to state
figure 1: algorithm for generating spatial relationship rel.
 left of   for the planning domain. the second applies a field to a single known waypoint to generate a new set of waypoints that all satisfy a predicate for the planning domain.
　the first step in the process of interpreting and acting upon a command is to translate the information directly obtainable from vision into our planning domain. this is done in a straight-forward way. for each object in the world we generate a description in the language of the planning domain. each object is represented in the planning domain by an id which is stored to allow other process to index back into the geometric vision representation via the planning representation. representing an object involves describing its colour and type  information which is directly available from our vision system . to position the object in the world we must also place the object at a waypoint. to do this we add a waypoint to the planning domain at the centre of the object's bounding box on the table. waypoints are also represented by a stored id that can be used access its position in the real world  so that later processes can use this information.
　the second step in the process of generating an initial state is to add information about the spatial relations of the waypoints to the planning problem. this allows the planner to reason about how moving objects between waypoints changes their spatial relationships. rather than add information about all of the spatial relationships that exist between all of the waypoints  we focus only on the relationships included in the goal state because any additional information would be irrelevant to the current task. thus our approach is explicitly taskorientated. the algorithm we use is presented in figure 1. in this algorithm  distractors represent points in the potential field that may influence the field in some way  e.g reducing its value  or altering its extent .
　the final step of the initial state generation process is to add additional waypoints in order to give the planner enough suitable object locations to produce a plan. this is necessary because the waypoints from the previous step are all initially occupied  and may not satisfy the spatial constraints in the goal description. to add waypoints  we first ground the target description  e.g.  the blue cubes  from  put the blue cubes left of the red ball   in the visual information to provide a count of the number of objects that must be moved. we then find the waypoint for the landmark object  e.g.  the red ball    and generate the required number of waypoints in the potential field around the landmark for the given spatial relationship  e.g.  left of  . the algorithm we use to generate the new waypoints is presented in figure 1. because
initialise scene sc
add landmark wpl to sc add waypoints - targets - wpl to s as distractors compute field pf for rel in s for i =1 to n do get max of pf if max   1 then
　add new waypoint at location of max else
return failure
figure 1: algorithm for generating n new waypoints for targets at the spatial relationship rel around landmark wpl.

	 a  the initial state.	 b  the state after execution
figure 1: images of the world before and after plan execution.
this algorithm is greedy  it may fail to place the waypoints in the potential field even if enough space is available. this is something that must be addressed in future work.
1 worked example
this section presents an example processing run from the implementation of our system. the initial scene for the example can be seen in figure 1 a . a visualisation generated by the system is presented in figure 1 a . the scene contains a red ball  two green cubes and two blue cubes. processing is started by the command  put the blue cubes to the left of the red ball . this is passed into the linguistic processing component. this component processes the text as described in ′1  which produces the mapl goal state shown in figure 1. the linguistic input triggers the current scene to be pulled from vision. this returns a scene with a red ball centred at  1   green cubes at  1  and  1   and blue cubes at  1  and  1   these numbers have been adjusted for a simpler presentation .
　the goal and visual information is then used as input into the discrete-continuous mediation process. as described in ′1  this process assigns ids for each object and a waypoint for each object position. this results in the following mapping for the scene  the brackets contain information that is accessible from vision via the ids :
obj d1  blue cube  at wp d1  1  obj d1  blue cube  at wp d1  1  obj d1  green cube  at wp d1  1  obj d1  green cube  at wp d1  1  obj d1  red ball  at wp d1  1 
　the qualitative part of this is transformed into a mapl expression to form part of the initial state for planning:
 pos obj d1 : wp d1   pos obj d1 : wp d1 
 pos obj d1 : wp d1   pos obj d1 : wp d1 
 pos obj d1 : wp d1 
 blue obj d1   blue obj d1 
 green obj d1   green obj d1   red obj d1 
　next  the mapping process uses potential fields to generate the spatial relationships between the waypoints for all of the visible objects. only the relationships necessary to satisfy the goal state are considered  so in this case only the  left of  relationship is considered. part of this process is presented in figure 1 b   which shows the  left of  field for the top right cube. in this picture the camera is positioned directly in front of the red ball  hence the field being tilted . this results in the following information being added to the initial state:
 left of wp d1 wp d1   left of wp d1 wp d1 
 left of wp d1 wp d1   left of wp d1 wp d1 
 left of wp d1 wp d1   left of wp d1 wp d1 
 left of wp d1 wp d1 
　the next step is to generate new waypoints that can be used to satisfy the goal state. this is done by grounding the landmark and target elements of the goal state in the information from vision. the target group   the blue cubes   is grounded by counting how many of the visible objects match this description. because there are two objects that match the colour and shape of the objects described by the human  two new waypoints are generated at the specified spatial relationship to the landmark group. the waypoint for the landmark object is identified  in this case wp d1   and then the new waypoints must be placed as dictated by the appropriate potential field. in this case a projective field is generated around the red ball's waypoint  with the non-target objects  the green cubes  as distractors. this field can be seen in figure 1 c . the new waypoint positions are selected by picking the points in the field with the highest values  and inhibiting the area around the points selected . this final step is presented in figure 1 d   and results in the following extra information being added to the mapping: wp d1  1   wp d1  1 .
　to complete the planning problem  its initial state is extended with left of propositions describing the spatial relations of the newly generated empty waypoints to the already occupied ones. finally the ff planner is run  returning:
1: pickup robot obj d1 wp d1
1: put robot obj d1 wp d1
1: pickup robot obj d1 wp d1
1: put robot obj d1 wp d1
although the plan looks simple in this example  note that the referential constraints in the goal description  cf. figure 1  are correctly resolved: the two blue blocks are picked up. note further that even this problem contains non-trivial causal constraints between actions to which the planner automatically adheres: neither does it try to pick up several objects at once  nor does it place several objects on the same waypoint.

 a  the initial state.  b  the potential field for  left   c  the potential field for  left   d  the target cubes placed at the for the top right cube. with two target cubes removed. highest points in the field.
figure 1: the progression of generated potential fields in for the processing of  put the blue cubes to the left of the red ball . the two squares on the left represent green cubes  whilst the two on the right represent blue ones.　before plan execution  the plan must be updated to include information about the current scene. this is done by querying the mediation process to determine the objects from vision referred to by the object ids. using this information the manipulation system acts out the human's command by picking up each blue cube in turn and placing them at the points indicated in figure 1 d   resulting in the scene in figure 1 b . 1	conclusions and future work
in this paper we presented a novel approach to mediating between quantitative and qualitative representations for a robot that must follow commands to perform manipulative actions. within this approach we have demonstrated two novel techniques: a generic method for the interpretation of natural language action commands driven by planning operators that enables  lazy  resolution of referential expressions by a planner; and the task-orientated use of potential field models to both automatically generate waypoints in real space that the planner can use to solve an under-constrained problem  and to add spatial relationships between existing waypoints.
　as this approach is still in its early stages there are a number of features we would like to add to it. these include optimisation functions for the planner  possibly based on spatial knowledge; more robust methods for placing waypoints in potential fields  perhaps using local search; and methods of detecting failure when no waypoints can be placed  or no plan can be found. in this latter case there are a number of alterations that can be made to the state generation process that may allow a plan to be found  even if it is not of a high quality. future scenarios for our robot will consist of multistep mixed-initiative interactions with humans. to this end we want to extend our mediation methods to support the generation of descriptions for spatial configurations and plans.
acknowledgements
this work was supported by the eu fp1 ist cognitive systems integrated project cognitive systems for cognitive assistants  cosy  fp1-ip. the authors would like to acknowledge the impact discussions with other project members have had on this work  and also thank mark roberts for contributing to the development of the integrated system.
