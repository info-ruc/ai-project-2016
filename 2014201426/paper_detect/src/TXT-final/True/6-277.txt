 
¡¡a corpus-based knowledge representation system consists of a large collection of disparate knowledge fragments or schemas  and a rich set of statistics computed over the corpus. we argue that by collecting such a corpus and computing the appropriate statistics  corpus-based representation offers an alternative to traditional knowledge representation for a broad class of applications. the key advantage of corpusbased representation is that we avoid the laborious process of building a  often brittle  knowledge base. we describe the basic building blocks of a corpus-based representation system and a set of applications for which such a paradigm is appropriate  including one application where the approach is already showing promising results. 
1 introduction 
declarative representation of knowledge has long been acknowledged as a key building block of al systems. a knowledge base  kb  with associated reasoning mechanisms can serve as a backbone for tasks such as query answering  learning and diagnosis. one of the key challenges in deploying knowledge representation  kr  systems is the cost and the complexity of building the knowledge base  especially in cases where it needs to cover a broad domain. building a kb is highly labor intensive for several reasons. first  the acquisition of domain knowledge and articulating it in a formal language can be incredibly difficult. second  since the kb needs to form a coherent logical whole  it is hard to distribute its creation of a knowledge base over multiple experts or knowledge engineers. finally  no matter how broad the kb  it will be brittle at the edges of its knowledge - it can only be used for tasks whose knowledge needs have been anticipated in advance. while all of the aforementioned difficulties have received significant attention  together they still remain a fundamental challenge for knowledge representation technology. 
¡¡this paper argues that there is an alternative approach to knowledge representation that has the potential of providing competitive functionality for a broad class of applications. in a nutshell  this approach  which we term corpus based representation  is based on collecting a large corpus of disparate fragments of knowledge  and building a set of tools that are based on analyzing properties of the corpus. the 
   jayant madhavan university of washington seattle  washington  u.s.a. jayant cs.washington.edu 
fragments of knowledge in the corpus can include individual kbs  database schemas with or without data instances of the schema  queries written over kbs and databases  and any form of meta-data associated with them. unlike a kb that needs careful ontological design  the corpus is a set independent uncoordinated contributions. the intuition is that if the corpus is large enough  then the patterns we identify in it can be of great use for knowledge intensive tasks. 
¡¡corpus-based knowledge representation is an outgrowth of our work on schema and ontology matching  doan et al.  1; 1 . the matching problem  which we elaborate on in section 1   is to find a semantic mapping between two disparate representations  be they database schemas or ontologies. while a detailed kb about the domain of the representations being matched could be of great value for the matching task  such kbs rarely exist  and their construction is not cost effective. instead  our approach to matching is based on analyzing the variations of representations in a corpus of schemas. this paper extends the idea of using a corpus to a general approach to knowledge representation. specifically  we identify other tasks in which corpus-based representation can be useful  and then define preliminary version for the contents of a corpus and the interfaces it should support. 
¡¡we emphasize that corpus-based representation is not a replacement for traditional knowledge representation. there are many tasks in which very finely tuned reasoning is required  and such reasoning can only be done with a very well designed knowledge base  e.g.  medical diagnosis  monitoring spacecraft  and making sense of tax laws . our goal is to explore the space of problems in which the laborious construction of knowledge bases can be avoided. 
1 a motivating application: matching disparate representations 
the discussion in this paper spans many kinds of representations  ranging from database schema to ontologies in expressive kr formalisms. collectively  we use the term domain model to refer to representations in any of these formalisms. the term schema refers to intensional knowledge  such as the names of database tables and their respective columns  or to the terminological component of a kb. the ground facts  or  rows in a database  are called instance data. 
sharing data among multiple data sources and applica-
tions is a problem that arises time and again in large enterprises  b1b settings  coordination between government agencies  large-scale science projects  and on the worldwide web. the problem has received significant attention in both the al and database communities  and over the 
years  several paradigms for data sharing have been implemented  each appropriate for certain scenarios. examples include data warehousing oriented architectures  data integration systems  lenzerini  1; draper et al  1   message passing systems  e.g.   mqseries  1    web services  and peer-data management systems lhalevy et al  1b; 
kalnis et al  1; bernstein et al  1 .  
¡¡no matter what paradigm we employ to share the data  a key problem is the semantic heterogeneity between the domain models of data sources that were originally designed independently. thus  to obtain meaningful interoperation  one needs a semantic mapping between the schemas. a semantic mapping is a set of expressions that specify how the data in one database corresponds to the data in another database  madhavan et al  1 . while languages for specifying semantic mappings have been developed and are well understood  see  lenzerini  1; ha1evy  1   for surveys   the creation of semantic mappings has become a key bottleneck as it is labor intensive and error-prone. 
¡¡the goal of schema matching is to assist a human to relate two domain models. complete automation of the process is unlikely to be possible  but the goal is to significantly increase the productivity of human experts. the matching problem is difficult because it requires understanding the underlying semantics of the domain models being matched. while a domain model  with its instance data  provides many clues on its intended semantics  it does not suffice in order to relate it to a different domain model. 
¡¡the process of generating a semantic mapping has traditionally been divided into two phases. the first phase finds a match between the two schemas. the match result is a set of correspondences between elements in the two schemas  stating that these elements are somehow related. for example  a correspondence may state that agent in one domain model corresponds to contact in another. the second phase builds on the correspondences by creating the mapping expressions. the mapping expressions  often expressed as queries or rules  enable translating data from one data source to another  or reformulating a query over one data source into a query on the other. a plethora of techniques have been proposed for schema matching: see  rahm and bernstein  1  for a survey  and  noy and musen  1; doan et al  1; do and rahm  1  for some work since then. collectively  these techniques mirror the heuristics that a human designer may follow. for example  techniques have considered exploiting relationships between names of elements in the domain models  structural similarities between them  similarities in data values  and even correlations between values in different attributes. several recent works on schema matching are based on combining multiple techniques in a principled fashion  madhavan et al  1; doan et al  1; 
¡¡¡¡this paper accompanies an invited talk at the conference that surveys the successes and challenges in data integration to date. 
do and rahm  1 . 
corpus-based matching 
conceivably  detailed knowledge about the domain in which the matching is being performed can be an important resource in a schema matching system. however  creating an appropriate kb is often hard  and furthermore  the result may be brittle in the sense it only helps on its domain of coverage  and only provides a single perspective on the domain. 
¡¡we are pursuing an alternate approach in which knowledge is gleaned by analyzing a large corpus of database schemas and previously validated mappings. there are two types of knowledge that we can glean from such a corpus. first  we can learn the different ways in which words  or terms  are used in database structures  i.e.  as relation names  attribute names and data values . second  the validated mappings show how variations in term usages correspond to each other in disparate structures. 
¡¡although such a corpus is not easy to construct  it is a very different kind of activity than building a detailed and comprehensive knowledge base. it does not require the careful ontological design as a knowledge base does  nor the careful control on its contents  thereby removing the key bottlenecks present in the design of knowledge bases. the corpus offers multiple perspectives on modeling a particular domain  including different coverages of the domain. thereby  it is more likely to provide knowledge that is useful for matching two disparate schemas. 
¡¡in our initial work on the lsd system  doan et al  1   we investigated the benefit of learning from previously validated mappings. in  doan et al  1  we considered the case where multiple data sources are mapped to a single mediated schema  on which users pose queries. we provided lsd with the mediated schema and a set of training matches for some data sources. lsd used these matches to learn models of the elements of the mediated schema. since no single learning algorithm captures all the cues from the domain  we used a multi-strategy approach that combined the predictions of several learners. we then asked lsd to predict matches between the mediated schema and a set of test schemas. our experiments showed that  1  it is possible to achieve high accuracy with multi-strategy learning  and  1  additional accuracy is obtained by considering domain constraints  i.e.  a simple form of domain knowledge . overall  lsd achieved matching accuracy of 1% on small to medium sized schemas of data sources on the web. we extended lsd to consider simple taxonomies of concepts in  doan et al  1 . 
¡¡in recent work  madhavan et al  1   we investigate the benefit of a corpus of schemas and matches  and the ability to use such a corpus to predict mappings between a pair of schemas that have not been previously seen. like in lsd  we learn models for elements in the corpus  using both the information available in the schema and validated matches that are provided in the corpus. given two schemas  s1 and s1 we calculate for each element in them a similarity vector w.r.t. the corpus  i.e.  how similar each element in si is to each element in the corpus. very roughly speaking  if the similarity vectors of two elements a  e s1 and a1 € s1 are similar to each other  then we predict that a1 matches a1. the results of our experiments show that  1  even with a modest corpus of 1 schemas we are able to achieve good accuracy  and  1  the correct matches found by using the corpus and those found by other previously known techniques overlap  but have significant differences. hence  the use of the corpus is finding matches that would not have been predicted by other techniques. 
1 	the corpus-based representation principle 
this paper argues that using a corpus of domain models is a general technique that can be applied in many applications as an alternative or complement to traditional knowledge representation. section 1 describes a few such applications  and section 1 presents the components of a corpus-based representation system. 
1 	applications of corpus-based representation 
the following are classes of applications that can benefit from corpus-based representation. 
web search and query answering: the first class of applications concerns various information finding tasks on the web  or intranet . the first application is query answering on the web  e.g.   kwok et al  1; radev et al  1  : a natural language query is posed to a web search interface  and rather than finding relevant pages  the search engine tries to find the answers to the query. a second  very related application  is focused crawling  e.g.   sizov et al  1    where the search engine is given a particular topic  and it tries to find pages relevant to it by starting at a set of initial pages  and crawling from them in a focused fashion. in both of these applications  the presence of additional domain knowledge has been shown or argued to be useful. however  the cost of constructing knowledge bases with such wide domain coverage would be prohibitive. 
creating and querying structured knowledge: one of the greatest impediments to using database and knowledge base technology is the conceptual difficulty of dealing with structured data versus unstructured text  the so called structure chasm  halevy et al  1a  . it is well known that creating domain models  whether for database schema or knowledge bases  is a conceptual effort. querying requires a different kind of effort - understanding someone else's domain model. hence  a significant challenge for the kr and database communities is to create tools that facilitate the creation and querying of structured data.  halevy et al  1a  argues that a corpus of schemas enables building several tools for this purpose. 
¡¡one such tool would be a schema design advisor  which assists in the authoring of structured data  much in the spirit of an auto-complete feature. a user of the such a tool creates a schema fragment and some data in a particular domain  and the tool then proposes extensions to the schema using the corpus. the user may choose a schema from the list and modify it further to fit the local context. note that besides time savings  such a tool has other advantages  such as possibly resulting in better designs and helping users conform to 

figure 1: a corpus-based representation system contains a large collection of disparate domain models  instance data  validated mappings between domain models and meta-data. applications access the corpus and the statistics computed on it through a set of interfaces. the interfaces support different types factual queries and similarity queries. 
certain standards  when these are applicable. one can also imagine using corpus-based representation during the phase of verifying the correctness of a knowledge base. hence  corpus-based representation can be an aid in creating traditional knowledge-based systems. 
¡¡on the querying side  corpus-based representation can facilitate the querying of unfamiliar data. specifically  consider a tool that enables you to pose a query using your own terminology to any database. the tool would then use the corpus to propose reformulations of the your query that are well formed w.r.t. the schemas of the database at hand. the tool may propose a few such queries  possibly with example answers   and let you choose among them or refine them. 
general properties of candidate applications: the exact characterization of which applications lend themselves to corpus-based representation is a subject for future research. one property of such applications that immediately stands out is that they use the knowledge base as an aid in resolving ambiguity. that is  the applications use a set of techniques to propose plausible answers to a task  and then use the knowledge base to help rank the answers  rule out certain ones  or guide the search for answers. hence  beyond the applications mentioned above  corpus-based representation can be useful in learning tasks  natural language processing and interfaces  popescu et al  1   and information retrieval. in fact  as we explain in section 1  corpus-based representation was inspired by the use of corpora  of different kinds!  in natural language processing and information retrieval. in contrast  corpus-based representation would be of limited use in applications that require intricate logical inference on carefully designed domain models  e.g.  medical diagnosis  control of spacecraft  or reasoning about income-tax rules . 
1 	corpus-based representation systems 
we now offer some specific details on how corpus-based representation systems can be built and used. we describe the types of contents we may put in a corpus  and the interfaces that the corpus provides to an application  see figure 1 . 

contents of a corpus 
a corpus can include any kind of information related to structured data. in particular  it includes the following. 
1. various forms of domain-model information: on the less expressive end  the corpus can include relational and object-oriented database schema or entity/relationship diagrams  xml dtds or schemas  possibly with associated integrity constraints  e.g.  functional dependencies . on the more expressive end  the corpus can include terminologies. 
1. instance data: we can include the actual rows of tables  or representative rows   xml documents  or the assertion 
 a-box  component of a terminological knowledge base or horn theory. in fact we can include data sets that do not have a schema  e.g.  certain file formats . note that it is very often the case that elements in the schema of one model may be instance data in another model. hence  the distinction between schema and instance data is not clean cut. 
1. validated mappings: mappings can be given directly between pairs of domain models  or go through an intermediate domain model  madhavan et al.  1 . 
1. queries: queries  posed by users or applications  provide important information about how certain data is used. for example  when a database query performs a join over attributes of two different tables  that indicates that the columns are modeling the same domain  and this often not evident from the schema that only specifies the data type . 
1. other meta-data: there are many forms of meta-data that accompany domain models. they range from text descriptions of the meaning of different fields to statistics about table cardinalities or histograms on the set of possible values within a column. 
¡¡it is important to emphasize that a corpus is not a logically coherent universal database. rather  it is a collection of disparate structures  and there is relatively little control on the logical design of elements of the corpus. we expect that the domain-model information of the corpus will be stored and accessed using tools for model management  bernstein  1   which attempt to provide a set of operators for manipulating models of data  as opposed to the data itself . 
statistics on the corpus 
there is a plethora of possible analyses that can be performed on such a corpus  and finding the most effective ones is a long term research challenge. below we describe certain kinds of statistics that can be computed over the corpus. we classify them according to whether they apply to individual words or terms  partial structures  or to elements of particular schemas. 
word and term statistics: these statistics are associated with individual words  in any language  and with noun or verb phrases. these statistics indicate how a word is used in different roles in structured data. for each of these statistics  we can maintain different versions  depending on whether we take into consideration only word stemming  synonym tables  inter-language dictionaries  or any combination of these three. the statistics include: 
1. term usage: how frequently the term is used as a relation name  attribute name  or in data  as a percent of all of its uses or as a percent of structures in the corpus . 
1. co-occurring schema elements: for each of the different uses of a term  which relation names and attributes tend to appear with it  what tend to be the names of related tables and their attribute names  what tend to be the join predicates on pairs of tables  are there clusters of attribute names that appear in conjunction  are there mutually exclusive uses of attribute names  
1. similar names: for each of the uses of a term  which other words tend to be used with similar statistical characteristics  
composite statistics: the same statistics can be applied to partial structures. examples of partial structures are sets of data instances  relations with associated attribute names  a relation with some data  possibly with missing values . 
¡¡clearly  we need to significantly limit the number of partial structures for which we keep statistics  e.g.  use techniques for discovering partial structures that occur frequently e.g.   polyzotis and garofalkis  1  . given statistics for certain partial structures  we can estimate the statistics for other related structures. 
statistics for schema elements: the same word  used in different structures  can have different meanings. hence  we may want to characterize the specific usages of terms in structures  and relate them to usage of terms in other structures. for example  in  madhavan et al.  1  we learn a classifier for every relation and attribute name in the corpus. following  doan et al  1   we use meta-strategy learning. the training data used for learning is gleaned from the schema to which the element belongs and the training data of elements that have been mapped to it by a validated mapping in the corpus. intuitively  the classifier is meant to recognize the particular usage of the term  even if it appears differently in another structure. 
application interfaces to the corpus 
applications built on a corpus should be able to access directly the statistics computed on the corpus  and perform the obvious lookups and queries on the corpus. in addition  just as knowledge based systems provide a set of higher-level interfaces  e.g.  in the spirit of  chaudhri et al.  1    the same could be done for corpus-based representation systems. the following are examples of functions such an interface could support. note that the implementation of these interfaces poses many research challenges. 
¡¡we classify the interfaces on whether they ask for factual queries  or similarity queries. in all cases  we expect that the answers returned will be a ranked list of answers. in addition  as in traditional knowledge bases  we should be able to obtain an explanation for the answers. in the notation used below  variables are prefaced by $. 
factual information: in the simplest case  a query is a nonground formula  and the corpus should return the possible groundings of the formula. as a simple example  we can ask for the values of $x  where killed $x  elvis . a different example would be a higher-order query  e.g.  find all the $p  such that $p toyota  honda . 
¡¡a more complicated function would take as input a set of constants and return formulas that include all of these constants. here  the goal is to find how two terms are related without specifying exactly what role they play in a formula. for example  the input may be gpa and student id  and the answers could be gpa studentld  $value  and student studentld  gpa  address . note that the two terms play different roles in the two answers  and that the answers are templates for formulas  i.e.  schema description   rather than particular sets of formulas. 
¡¡in both of these cases  one can also complement the query with background knowledge. the knowledge may eliminate some answers or result in a different ranking of answers. 
similarity information: one of the key imports of the corpus is that it contains different ways of saying similar facts. similarity queries try to get at these different perspectives. in the simplest case  a query can be a ground fact  and the corpus will return similar ways of saying the same thing. for example  the input can be class lexus  luxury   and the output could be luxurycar lexus  toyota   or even carreview lexus  luxuryclass  verygood  goodtires . 
¡¡in a more complex case  the query would consist of two terms  and the answer would be sets of facts in which these terms tend to play similar roles. for example  the input could be review and referee  and the answer would be ljcaireview paper1  reviewer1  accept  and a|journalreferees round1  paper1  reviewed  reject . the answer could also specify where the terms are used differently  such as refereeing a soccer game and reviewing a movie. 
¡¡in summary  we note some of the fundamental challenges in building an interface to a corpus. first  unlike logical knowledge/databases  answers are ranked rather than defining a boolean condition on data. second  we are often interested in similarity querying  looking for different ways of saying the same thing 1. third  the answers returned often summarize sets of facts or describe schema fragments. finally  it is often useful to tell the user what is not an answer. 
1 	related work 
the corpus-based representation approach stands in stark contrast to an approach like the cyc project  lenat and guha  1  that attempts to build a comprehensive knowledge base of all common sense. a corpus is a collection of any set of small domain models that are not centrally designed or coordinated. importantly  the corpus is likely to include several models of the same domain. one of the key engineering challenges that arose in building cyc was how to enable several knowledge engineers to add to such a large knowledge base without having to understand the entire knowledge base. contexts  e.g.   guha  1; nayak  1; giunchiglia and ghidini  1   are a mechanism proposed to address this complexity  where each knowledge engineer can focus on a small knowledge base  and then a set of lifting rules makes sure the entire knowledge base is semantically related. unlike a corpus  contexts are still logically coordinated through a set of lifting rules. 
¡¡the use of a corpus of structured data was first proposed in  halevy et al.  1a . there  the goal was to create tools 
¡¡¡¡1 there is a large literature on similarity querying in databases and content based image retrieval  but these focus on similarities between objects represented as points in multi-dimensional space. that facilitate the creation  querying and sharing of structured data. that work was partially inspired by the use of large corpora in information retrieval and in statistical natural language processing  e.g.  charniak  1j . 
¡¡several works have considered creating knowledge bases by a large set of independent contributors. the open minds project  singh et al.  1  collects knowledge from contributors on the web and builds a kb of common sense. although the contributes are independent  the knowledge entry is guided by a set of templates  and hence the creation of a coherent kb is possible.  richardson and domingos  1  describe how to create a feedback loop by which consumers of knowledge provide indirect feedback to contributors of knowledge as to the usefulness of their contributions  thereby ultimately filtering out the bad contributions. 
1 	conclusions 
the analysis of large corpora is the key to the success of information retrieval techniques and to recent progress on natural language processing. corpus-based representation is predicated on the idea that large corpora of domain models can be leveraged to create novel tools in service of knowledge representation. the key advantage of corpus-based representation is that it avoids the need for careful logical design of a single comprehensive ontology. in addition  since a corpus will contain multiple ways of representing the same information  it opens up opportunities for exploring transformations on data and addressing problems related to representational heterogeneity. this paper laid the foundation for corpus-based representation  by describing preliminary versions of the contents of a corpus  the interfaces to it  some applications in which it would be a powerful representational tool  and an application from which the approach was inspired. 
¡¡corpus-based representation offers many exciting research challenges  not the least of which is actually collecting a large enough corpus to be of interest. in addition  there is a question about how focused the corpus needs to be in order to be useful in a particular domain. that is  can it only have domain models in that domain  can domains models in other  possibly related  domains be useful  or do they only introduce noise that degrades the performance  finally  no matter how a corpus is constructed  it can probably be manually tuned to perform even better. what form does such tuning take  while the challenges for corpus-based representation are enormous  we believe the payoffs could be huge  and the results can profoundly impact how we create and use structured knowledge. 
acknowledgments 
many colleagues have contributed to the ideas that led to corpus-based representation. we would like to thank phil bernstein  anhai doan  pedro domingos  oren etzioni  zack ives  pradeep shenoy  and igor tatarinov. funding was provided by an nsf career/pecase grant 1s-1  itr grant iis-1  a sloan fellowship  and gifts from microsoft research intel  nec and ntt. 
