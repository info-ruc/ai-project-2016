 
we describe a logic-based ai architecture based on brooks' subsumption architecture. in this architecture  we axiomatize different layers of control in first-order logic  fol  and use independent theorem provers to derive each layer's outputs given its inputs. we implement the subsumption of lower layers by higher layers using circumscription to make assumptions in lower layers  and nonmonotonically retract them when higher layers draw new conclusions. wte also give formal semantics to our approach. 
finally  we describe four layers designed for the task of robot control and an experiment that empirically shows the feasibility of using fully expressive fol theorem provers for robot control with our architecture. 
1 	introduction 
in  brooks  1   rodney brooks provided a decomposition of the problem of robot control into layers corresponding to levels of behavior  rather than a sequential  functional form. within this setting  he introduced the idea of subsumption  that is  that more complex layers not only depend on lower  more reactive layers  but could also influence their behavior. the resulting architecture was one that could simultaneously service multiple  potentially conflicting goals in a reactive fashion  giving precedence to high-priority goals. 
　because of its realization in hardware  the architecture lacks declarativeness  making it difficult to implement higher-level reasoning and making its semantics unclear. the increasing hardware complexity with new layers introduces scaling problems. and  relying on hardware specifications  the architecture is specifically oriented to wards robot control and is not applicable to softwarebased intelligent agents. the problem of extending similar architectures to more complex tasks and goals and to agents that are not necessarily physical has already been raised and discussed in general terms by  minsky  1  and  stein  1   but to our knowledge  no practical ai architecture has been developed along these lines. 
　in this paper we describe an architecture that is modeled in the spirit of brooks' subsumption architecture but relies on a logical framework and has wider applicability and extendibility in the manner described above. our logic-based subsumption architecture  lsa  includes a set of first-order logic  fol  theories  each corresponding to a layer in the sense of brooks' architecture. each layer is supplied with a separate theorem prover  allowing the system of layers to operate concurrently. we use nonmonotonic reasoning to model the connections between the theories. in addition  by allowing the layers to make nonmonotonic assumptions  we have made each layer's performance independent of the performance of other layers  thus supporting reactivity. 
   we demonstrate our architecture by modeling four layers for the task of robot control  the bottom two of which are brooks1 first  two layers. we show empirically that the layer in greatest need of reactivity is sufficiently fast  1-1 seconds per control-loop cycle . this result shows that general-purpose theorem provers can be used in intelligent agents without sacrificing reactivity 
the remainder of the paper is organized as follows: 
after giving a brief introduction to brooks' system and behavioral decomposition  we describe the lsa and give formal semantics to the approach using circumscription. we then describe the robot control system we have implemented using the architecture. we conclude with a discussion of implementation issues  comparisons to related work and a description of future directions. 
1 	subsumption and decomposition 
1 	brooks' subsumption architecture 
brooks showed that it is often advantageous to decompose a system into parallel tasks or behaviors of increasing levels of competence rather than the standard functional decomposition. whereas a typical functional decomposition might resemble the sequence sensors  perception modeling  planning 
	 task recognition 	motor control. 
brooks would decompose  the same domain as avoid objects  wander 	explore build maps  monitor changes 	identify objects  plan 
actions  reason about object behavior 
	amir and maynard-reid ii 	1 

where  denotes increasing levels of competence. potential benefits from this approach include increased robustness  concurrency support  incremental construction and ease of testing. 

figure 1: layers 1 and 1 of brooks' subsumption architecture robot control system. 
   in general  the different layers are not completely independent. in the decomposition above  wandering and exploring depend on the robot's ability to avoid objects. but the system may be able to service multiple goals in parallel  despite the dependence. the goals of one layer will occasionally conflict with those of another layer  in which case higher-priority goals should override lowerpriority ones. consequently  the subsumption architecture provides mechanisms by which higher  more competent layers may observe the state of lower layers  inhibit their outputs and override their inputs  thus adjusting their behavior. high-priority tasks in lower layers  such as reflexively halting when an object is dead ahead  will still have a default precedence if the designer disallows any tampering with these particular tasks. 
   brooks implemented a control system of layers corresponding to the first three levels of competence described above  avoidance  wandering and exploration . the first two layers are shown in figure 1. briefly  the avoid layer endows the robot with obstacle avoidance capabilities by moving it in directions that avoid obstacles as much as possible and forcing it to stop if a head-on collision is imminent. the wander layer causes the robot to move around aimlessly when u is not otherwise occupied. the explore layer gives the robot some primitive goal-directed behavior by periodically choosing a location in the distance and heading the robot towards it if idle. while in explore mode  this layer inhibits the wander layer so that the robot remains on track towards its destination. when either the wander or the explore layer is active  it overrides the default heading computed by the avoid layer  but the avoid layer still ensures that the robot does not have a collision. we refer the reader to  brooks  1  for further details. 
1 	behavioral decomposition 
the first important idea we borrow from brooks' architecture is that of decomposing the domain along behavioral lines rather than along the standard  sequen-
automated reasoning 
tial functional lines. a logic-based subsumption architecture  lsa  is composed of a sequence of fol the-
ories. each represents a layer with an axiomatization of the layer's behavior  that is  the layer's inputs  outputs  goal   state and any dependencies between them. the inputs are axioms coming from either the sensors or higher layers. the outputs are proved theorems determined by running a separate theorem prover for that layer only. these outputs may be sent to lower layers or to the robot effectors. 
   because the axiomatization of a layer is usually much smaller than that of the whole system  each cycle is less computationally expensive than running one theorem prover over the whole compound axiomatization  leading to an overall higher performance. another advantage of the layer-decoupling is the possibility of achieving more reactive behavior. as in brooks' system  lower layers controlling basic behaviors are trusted to be autonomous and do not need to wait on results from higher layers  they assume some of them by default  before being able to respond to situations. because these layers typically have simpler axiomatizations  and given the default assumptions  the cycle time to compute their outputs can be shorter than that of the more complex layers. 
1 	subsumption principles 
of course  the layers are not fully independent. we adopt the view that  together with the task-based decomposition idea  the coupling approach represented by subsumption in the subsumption architecture is an important and natural paradigm for intelligent agents in general  and robot control in particular  see  stein  1  . we want each layer in an lsa to be able to communicate with those underneath it in the hierarchy. 
   in general  however  when one layer overrides another  the two disagree on what some particular input should be. in a classical logic setting  the two corresponding theories will be inconsistent. we need to formalize the higher-layer theory's precedence over the lower layer's in such a way that  a  if there is no conflict  both layers keep their facts and the higher layer asserts its relevant conclusions in the lower layer  and  b  if there is conflict  the lower layer tries to give up some assumptions to accommodate the higher layer's conclusions. a number of techniques developed in the logic community are applicable  e.g.  nonmonotonic techniques and belief revision. we have chosen to use circumscription  although other approaches may be equally interesting and appropriate. 
1 	logical subsumption 
this section describes in detail how we implement the principles discussed above. 
1 	basic machinery 
we distinguish three parts of the logical theory associated with each layer:  1  the body of the layer   1  the sensory and input latches  and  1  the output the body of the layer is the invariant theory for that layer. 

the latches are used to accept the input and replace it at the beginning of every cycle  rather than accumulate it . the output is the set of goal sentences proved from the layer's theory  including the latches . 
the processing loop of each layer proceeds as follows: 
first  collect any pertinent sensor data and assert it in the form of logical axioms. simultaneously  assert any inputs from higher-level theories. the theorem prover of that layer then attempts to prove the layer's goal. upon reaching the conclusions  transmit the relevant ones either to the layer below or  in the case of layer 1  to the robot manipulators. figure 1 illustrates this process. 

figure 1: a detailed look at two layers. 
1 	circumscription-based subsumption 
in the logical paradigm  mccarthy's circumscription 
 mccarthy  1  is one of the first major nonmonotonic reasoning tools. mccarthy's circumscription formula 

says that in the theory .1  with parameter relations and function sequences p z  p is a minimal element such that a p  z  still holds while z is allowed to vary in order to allow p to become smaller. 
　take  for example  the theory block b1 . the circumscription of block in t  varying nothing  is 
b1  . by minimizing block  we have concluded that there are no other blocks in the world besides those mentioned in the original theory t. 
　in the lsa  we use circumscription for two distinct tasks: assuming defaults in the layers and giving semantics to the system of layers as one big logical system. 
　to implement the idea of subsumption  we let each layer make default  assumptions  about the inputs that later may be adjusted by other  higher-level  layers. these assumptions take the form of the closed-world 
assumption  cwa  by minimizing a predicate in the layer's input language  extended cwa  a generalization of cwa  was shown to be equivalent to circumscription  gelfond et a/.  1  . 
　　more formally  let be the theory of layer  and  a set of predicates in for which we wish to assert cwa. then  subsumption is achieved for layer  by using the parallel circumscription policy 

when implemented  this formula often can be substituted with a simple  external to the logic  mechanical interference determining the value of the minimized predicates; we discuss this issue in section 1. 
1 	semantics for lsa 
if we ignore the time differences between the theorem provers in different layers and consider the entire system of layers as one logical theory  we can give the system a simple semantics. let  be the theory of layer i  including any cwa as a fol schemata    be the goal formula of layer  i.e.  the formula that we try to prove in that layer  and be its translation to 
's input. we call such a system of layers t a layered theory. for we write if the 
mechanical entailment we described above derives 
  where 
every abi is a relation symbol that does not show in t. 
definition 1  semantics for layered theories  is a model of the layered theory t 
iff it is a 	first-order 	model of the circumscriptions 

this semantics assumes  1  we are interested in the for some   as opposed to for example  and  1  all the 
symbols in the various theories are different  e.g.  the symbol 1 actually has the name  1 in layer 1 and in layer 1 . 
　let  be the fol language including only =  and the constant and function symbols of 
for a sentence be the translation of by replacing g with g' and every term t by the following theorem validates the semantics for our prooftheoretic system of transferring goals from one layer to another. 
theorem 1  completeness  assume that is layered theory and is formula in the 
language 	. i f 	t h e n there 	i s a n d 
sequence of sentences 
  and 1 
our lsa obeys this semantics  assuming that transferring a single instantiation of the goal between any pair of layers is sufficient. in case one layer proves only a disjunction of goal instantiations  we need to refine our lsa to support such a transfer  but this refinement can be done for any size of disjunctions. additionally  there is no need to consider quantification in our lsa since we assume skolemization of the clauses  see section 1 . 
　we omit the proof for lack of space  but mention that it relies on craig's interpolation theorem for fol and on the following lemma. 
	amir and maynard-reid ii 	1 


　　for soundness  we need to assume that the set of layers of is consistent with and that all the circumscriptions in tor t have smooth preference relations  i.e.  every model is either minimal or has a minimal model that is preferred to it . 

1 	a model of brooks' system 
we briefly describe the logical theories for a control system we have implemented for a robot operating in a multi-story office building. the first two layers correspond roughly to the first three layers in brooks' system. for simplicity  we list only selected axioms from the theories and refer the reader to the full version of the paper for the complete system. 
　we assume the architecture is used to control a cylindrical robot with sonar sensors on its perimeter and wheels that control its motion. we also assume that it is able to determine its current location and orientation. 
1 	layer 1: obstacle-avoidance 
layer 1 takes its input  asserted in the form of the axiom schema s'onarreading  sonar .number  =  list  from the physical sonars and translates it into a map of objects  recording their distance and direction  relative to the robot 1. it may also discover  virtual  objects by way of layer l's subsumption latch. 

　layer 1 checks to see if it has detected objects lying directly in front of it  and halts the robot if it has. 

 objectaheadthe robot's 1-radians reference point is straight ahead   haltrobot. the front sonar is numbered 1  and the sonars are numbered consecutively  counter-clockwise from 1 to nsonars - 1. 
automated reasoning 
   using the map  layer 1 executes the function getforce  computing the combined  repulsive force  exerted on the robot by the detected objects as force -direction and force.strength. it uses the former to specify a heading angle for the robot away from this force. once headed in the right direction  the robot is commanded to move away at a speed proportional to the strength of the force  slowing down as it moves farther away from the objects. 



　during each cycle of layer 1  it applies the cwa to the symbols haltrobot  object  distance  direction in the input language. it then uses its theorem prover to try to prove fwd speed  and turn angle   where speed and angle are instantiated by the proof. the results are translated into the appropriate actuator commands. 
1 	layer 1: destination-seeking 
layer 1 supports simple movements towards a goal location  more closely resembling the exploration layer of brooks' system than the wandering layer. given a particular pair of coordinates specified by the input movecmd from layer 1 and given the robot's current location 1 it makes a simple calculation to find in which of the eight quadrants surrounding the robot this goal position is  and asserts the existence of a  virtual pushing object  in the opposing quadrant. 
object  	push 	.object . 


　during each cycle  layer l's theorem prover attempts to prove object obj   direction  obj  = dir  and distance obj  = dist  and introduces them into layer 1's input latch if successful. the avoidance capabilities of layer 1 effectively push the robot away from the object in the direction of the goal  although it may deviate from a direct path if there are physical objects in the vicinity. 
1 	layer 1: mid-level planning 
layer 1 performs two tasks:  1  translate logical locations into cartesian coordinates and  1  reason in the situation calculus  mccarthy and hayes  1  about using the elevators. 
1
　　these coordinates are with respect to the fixed coordinate system of the domain. 

　the inputs for this layer are the current location data from the robot  currlandmark  and the output from layer 1  tar get landmark . during each cycle  it tries to plan for the next landmark and prove movecmd  next-landmark . 

　the situation calculus theory includes three fluents: the two elevators' locations and the robot's location  we explicitly state frame axioms . since the domain and depth are small  planning here is simple. 
1 	layer 1: high-level planning 
layer 1 performs high-level robot motion planning using situation calculus. mere there is only one fluent  the robot's location ; thus  deeper reasoning can be performed in a reasonable time. 
　the input for this layer is the current location of the robot. the goal is targetlandmark land.inark . 

1 	implementation issues 
we have implemented the above theory using the pttp theorem prover   stickel  1    stickel  1   on a sun sparc station  running quintus prolog as the underlying interpreter for pttp. pttp  prolog technology theorem prover  is a model-elimination theorem prover. given a theory made of clauses  not necessarily disjunctive  without quantifiers  pttp produces a set of prolog-like horn clauses  makes sure only sound unification is produced  and avoids the negation-as-failure proofs that are produced by the prolog inference algorithm. it is sound and complete. 
　we subjected our system to a battery of experiments in a simulated office building environment. figure 1 summarizes the results for three scenarios of varying difficulty:  1  planning a path towards a location on the same floor as the robot   1  creating a plan that requires a low-level plan for using the elevator  and  1  planning a path towards a location on a different floor. in each scenario  we experimented with various robot orientations and obstacle positions in the robot's vicinity. for each layer  we measured the number of inference steps and time taken to prove its goal.1 
　layer 1  the critical layer  achieved its results in an average of 1 seconds when a turn action was required  and 1 seconds when a forward action was required.  because of space concerns  we have included in figure 1 only the data for easels of the former kind.  layers 1  1  and 1 worked fairly fast  although the long planning involved in scenario 1 took more than 1 seconds  for a depth of 1 in the proof space . however  because we rely on the speed of only layer 1  safety is not compromised; the avoidance capabilities ensure that the robot does not fall off a cliff while planning a way to avoid the cliff edge. 
　we attribute the speed achieved to three optimizations. first  we used a few semantic attachments in layer 1. in particular  the predicate get force was embodied in a c function that returns the force vector  strength  direction . it calls prolog's bagof operator to collect all the objects for which existence proofs can be found  then computes the sum of the forces contributed by each object. this cwa is achieved by limiting proofs to be no longer than a specified constant  after some experimentation  we settled on a constant of 1.  
second  we applied caching to the proof of getforce. 
since every proof  re-proved  getforce many times  this improved the performance of layer 1 significantly  from approximately 1 seconds to 1 seconds per proof . 
third  we divided the planning so that layer 1 executes 
 local planning  for the elevator domain. this allowed layer 1 to avoid an explosion of the proof space  which otherwise would have occurred since there are four principal actions as well as a number of frame axioms associated with the robot and the elevator. the separation also helped prevent complex unifications. 
1 	related work 
compared to other approaches to agent architecture and robot control using logic  lsa is the only one using full fol theorem provers for the low-level control loop and the first one to propose an architecture built on theorem provers that is suitable for realizing complex tasks. 
　 shanahan  1  describes a map-building process using abduction  but then implements his theory in an algorithm that is proved to have his abductive semantics. baral and tran  1  define control modules to be of a form of stimulus-response  s-r  agents  see  nilsson  
1    relating them to the family of action languages a  e.g.   gelfond and lifschitz  1    giunchiglia et al  
1  . they provide a way to check that an s-r module is correct with respect to an action theory in a or a1  and provide an algorithm to create an s-r  agent from an action theory.  levesque et a/.  1    giacomo et a/.  1   and other work in the golog project have a 
planner that computes/plans the golog program offline  only later letting the robot execute the golog 
1 we do not list averages or standard deviations for layers 
1 and 1 because their performances are independent of both the robot's orientation and sonar readings. 
	amir and maynard-reid ii 	1 

layer 1 
	time 	infer. 
mean 	sd 	mean 	sd layer 1 
	time 	infer. 
mean 	sd 	mean 	sd layer 1 
time 	infer. lay er 1 
time 	infer. seen. 1 
seen. 1 
seen. 1 1 1 1 1 1 1 
1 	1 	1 	1 1 1 1 1 1 1 
1 	1 	1 	1 1 	1 
1 	1 
1 	1 1 	1 
1 	1 
1 	1 figure 1: proof time and inference steps measurements for the lsa during experiments in three different scenarios: 
 1  single-floor planning   1  lower-level elevator planning  and  1  multi-floor planning.  sd is standard deviation.  

program on-line. here again  logic is used only to give semantics for golog programs by way of situation calculus   mccarthy and hayes  1  . 
　none of this work uses fol theorem provers for controlling robots at run-time. to our knowledge  there has been no such system since shakey  nilsson  1 . 
1 	conclusion 
we have shown that theorem provers can be used for robot control by employing them in a layered architecture. we demonstrated that the architecture and the versatility of theorem provers allow us to realize complex tasks  while keeping individual theories simple enough for efficient theorem proving. furthermore  we have grounded our proposal by giving it formal semantics based on circumscription. 
　at this time  the system is implemented in four layers on a simulating computer. besides installing the system on a mobile robot  our future work plan includes adding layers that create maps and layers that reason about and update explicit beliefs about the world. we are currently working on incorporating vision sensory capabilities and implementing concurrency. 
　this work is a first step towards our long-term goal of creating a general logic-based ai architecture that is efficient and scalable  and that supports reactivity. 
1 	acknowledgments 
we wish to thank mark stickel for allowing us to use his pttp sources  both for prolog and lisp  and providing helpful answers to our inquiries regarding its use. this research was supported by an a iip a  onr  grant n1-1 and by a national physical science consortium  npsc  fellowship. 
