
this paper proposes a new method to estimate the class membership probability of the cases classified by a decision tree. this method provides smooth class probabilities estimate  without any modification of the tree  when the data are numerical. it applies a posteriori and doesn't use additional training cases. it relies on the distance to the decision boundary induced by the decision tree. the distance is computed on the training sample. it is then used as an input for a very simple onedimension kernel-based density estimator  which provides an estimate of the class membership probability. this geometric method gives good results even with pruned trees  so the intelligibility of the tree is fully preserved.
1 introduction
decision tree  dt  algorithms are very popular and widely used for classification purpose  since they provide relatively easily an intelligible model of the data  contrary to other learning methods. intelligibility is a very desirable property in artificial intelligence  considering the interactions with the end-user  all the more when the end-user is an expert. on the other hand  the end-user of a classification system needs additional information rather than just the output class  in order to asses the result: this information consists generally in confusion matrix  accuracy  specific error rates  like specificity  sensitivity  likelihood ratios  including costs  which are commonly used in diagnosis applications . in the context of decision aid system  the most valuable information is the class membership probability. unfortunately  dt can only provide piecewise constant estimates of the class posterior probabilities  since all the cases classified by a leaf share the same posterior probabilities. moreover  as a consequence of their main objective  which is to separate the different classes  the raw estimate at the leaf is highly biased. on the contrary  methods that are highly suitable for probability estimate produce generally less intelligible models. a lot of work aims at improving the class probability estimate at the leaf: smoothing methods  specialized trees  combined methods  decision tree combined with other algorithms   fuzzy methods  ensemble methods  see section 1 . actually  most of these methods  except smoothing  induce a drastic change in the fundamental properties of the tree: either the structure of the tree as a model is modified  or its main objective  or its intelligibility.
모the method we propose here aims at improving the class probability estimate without modifying the tree itself  in order to preserve its intelligibility and other use. besides the attributes of the cases  we consider a new feature  the distance from the decision boundary induced by the dt  the boundary of the inverse image of the different class labels . we propose to use this new feature  which can be seen as the margin of the dt  to estimate the posterior probabilities  as we expect the class membership probability to be closely related to the distance from the decision boundary. it is the case for other geometric methods  like support vector machines  svm . a svm defines a unique hyperplane in the feature space to classify the data  in the original input space the corresponding decision boundary can be very complex . the distance from this hyperplane can be used to estimate the posterior probabilities  see  platt  1  for the details in the two-class problem. in the case of dt  the decision boundary consists in several pieces of hyperplanes instead of a unique hyperplane. we propose to compute the distance to this decision boundary for the training cases. adapting an idea from  smyth et al.  1   we then train a kernel-based density estimator  kde   not on the attributes of the cases but on this single new feature.
모the paper is organized as follows: section 1 discusses related work on probability estimate for dt. section 1 presents in detail the distance-based estimate of the posterior probabilities. section 1 reports the experiment performed on the numerical databases of the uci repository  the comparison between the distance-based method and smoothing methods. section 1 discusses the use of geometrically defined subsets of the training set in order to enhance the probability estimate. we make further comments about the use of the distance in the concluding section.
1 estimating class probabilities with a dt
decision trees  dt  posterior probabilities are piecewise constant over the leaves. they are also inaccurate. thus they are of limited use  for ranking examples  or to evaluate the risk of the decision . this is the reason why a lot of work has been done to improve the accuracy of the posterior probabilities and to build better trees in this concern.
모traditional methods consist in smoothing the raw conditional probability estimate at the leaf  where k is the number of training cases of the class label classified by the leaf  and n is the total number of training cases classified by the leaf. the main smoothing method is the laplace correction pl  and its variants like m-correction . the correction
  where c is the number of classes  shifts the probability toward the prior probability of the class   cestnik  1; zadrozny and elkan  1  . this improves the accuracy of the posterior probabilities and keep the tree structure unchanged  but the probabilities are still constant over the leaves. in order to bypass this problem  smoothing methods can be used on unpruned trees. the great number of leaves allows the tree to learn the posterior probability with more accuracy  see  provost and domingos  1  . the intelligibility of the model is most reduced in this case  so specialized building and pruning methods are developped for pet's  see for instance  ferri and hernandez  1  .
모in order to produce smooth and case-variable estimate of the posterior probabilities  kohavi  deploys a naive bayes classifier  nbc  at each leaf  using a specialized induction algorithm. thus the partition of the space in nbtree is different from classical partition  since its objective is to better verify the conditional independance assumption at each leaf. in the same idea  zang and su  use nbc to evaluate the choice of the attribute at each step. but the structure of the tree is essentialy different.
모other methods obtain case-variable estimates of the posterior probabilities by propagating a case through several paths at each node  mainly fuzzy methods like in  umano et al.  1  or in  quinlan  1 ; or  more recently   ling and yan  1  . these methods aim at managing the uncertainty both in the input and in the training database.
모smyth  gray and fayyad  propose to keep the structure of the tree and to use a kernel-based estimate at each leaf. they consider all the training examples but use only the attributes involved in the path of the leaf. the dimension of the subspace is then at most the length of the path. but the resulting dimension is nevertheless far too high for this kind of technique and the method cannot be used in practice.
모we propose here to reduce the dimension to 1  since kde is very effective in very low dimensions. we preserve the structure of tree  and our method theoretically applies as soon as it is possible to define a metric on the input space.
1 distance-based probability estimate for dt
1 algebraic distance as a new feature
we consider an axis-parallel dt  adt  operating on numerical data: each test of the tree involves a unique attribute. we note 붞 the decision boundary induced by the tree. 붞 consists of several pieces of hyperplanes which are normal to the axes. we also assume that it is possible to define a metric on the input space  possibly with a cost or utility function.
모let x be a case  c x  the class label assigned to x by the tree  d = d x 붞  the distance of x from the decision boundary 붞. the decision boundary 붞 divides the input space into different areas  possibly not connected areas  which are labeled with the name of the class assigned by the tree.
모by convention  in a two-class problem  we choose one class  the positive class  to orient 붞: if a case stands in the positive class area  its algebraic distance will be negative. definition 1 algebraic distance to the dt  1-class problem 
모the algebraic distance of x is h x  =  d x 붞  if c x  is the positive class c and h x  = +d x 붞  otherwise.
모this definition extends easily to multi-class problems. for each class c  we consider 붞c  the decision boundary induced by the tree for class c: 붞c is the inverse image of the class c area.  we have 붞c   붞 .
definition 1 class-algebraic distance to the dt
모the c-algebraic distance of x is hc x  =  d x 붞c  if c x  = c and hc x  = +d x 붞c  otherwise.
the c-algebraic distance measures the distance of a case to class c area. actually  algebraic distance is a particular case of c-algebraic distance where c is the positive class.
모these definitions apply to any decision tree. but in the case of axis-parallel dt  adt  operating on numerical data  a very simple algorithm computes the algebraic distance  adapted from the distance algorithm in  alvarez  1  . it consists in projecting the case x onto the set f of leaves f whose class label differs from c x   in a two class problem . in a multi-class problem  each class c is considered in turn. when c x  = c  the set f contains the leaves whose labels differs from c; otherwise it contains the leaves whose class label is c. the nearest projection gives the distance.
algorithm 1 algebraicdistance x dt c 
1. d = カ
1. gather the set f of leaves f whose class c f  verifies:
 c f  xor c x   = c;
1. for each f 뫍 f do: {
1. compute pf x  = projectionontoleaf x f ;
1. compute df x  = d x pf x  ;
1. if  df x    d  then d = df x  }
1. return hc x  =  sign c x  = c    d
algorithm 1 projectionontoleaf x f =  ti i뫍i 
1. y = x;
1. for i = 1 to size i  do: {
1. if y doesn't verify the test ti then yu = b } where ti involves attribute u with threshold value b
1. return y
the projection onto a leaf is straightforward in the case of adt since the area classified by a leaf f is a hyper-rectangle defined by its tests. the complexity is in o nn  in the worst case where n is the number of tests of the tree and n the number of different attributes of the tree.
모the distance to the decision boundary presents two main advantages  because it is a continuous function: it is relatively robust to the uncertainty of the value of the attributes for new cases  and to the noise in the training data  assuming that only the thresholds of the tests are modified  not the attributes .
1 kernel density estimate  kde  on the algebraic distance
kernel-based density estimation is a non-parametric technique widely used for density estimation. it is constantly improved by researches on algorithms  on variable bandwidth and bandwidth selection  see  venables and ripley  1  for references .
모univariate kde basically sums the contribution of each training case to the density via the kernel function k. to estimate the density f x  given a sample m = {xi}i뫍 1 n   one computes  where k is the kernel function and b the bandwidth  b can vary over m .
모many methods could be used in this framework to compute the distance-based kernel probability estimate of the class membership.  we could also use kernel regression estimate . we have used very basic kde for simplicity.
모we consider here the algebraic distance h x  as the attribute on which the kde is performed. so we compute the density estimate of the distribution of the algebraic distance  from the set of observations h x  x 뫍 s:
		 1 
모in order to estimate the conditional probabilities  we consider the set s of training cases and its subset sc of cases such that c x  = c. we estimate the density of the two populations: f  the density estimate of the distribution of the algebraic
distance to the decision boundary; f c  the density estimate of the distribution of the algebraic
distance of points of class c.
모f is computed on s and f c on sc. we then derive from the bayes rule  if p  c  estimates the prior probability of class c:
		 1 
definition 1 distance-based kernel probability estimate p  c|h x   is called the distance-based kernel  dk  proba-
bility estimate
모the algorithm is straightforward. we note s the training set used to build the decision tree dt.
algorithm 1 distancebasedprobest x dt c s 
1. compute the algebraic distancey = h x  = algebraicdistance x dt c ;
1. compute the subset s x  of s from which the probability density is estimated: default value is s x  = s; 1. select sc x  = {x 뫍 s x  c x  = c};
1. compute;
1. compute;
1. compute and return p  c|h x   from equation  1 
모several possibilities can be considered for the set s x  used to compute the kernel density estimate  we discuss them in section 1. the simplest method consists in using the whole sample s. the algebraic distance is taken into account globally  without any other consideration concerning the location of the cases. we call the corresponding conditional probability estimate p g the global distance-based kernel  gdk  probability estimate. figure 1 shows how the gdk probability estimate varies over a test sample  compared with the laplace probability estimate which is piecewise constant.

figure 1: variation of laplace and gdk probability estimate over a test sample from the wdbc database. dt errors are highlighted
1 experimental study
1 design and remarks
we have studied the distance-based kernel probability estimate on the databases of the uci repository  blake and merz  1  that have numerical attributes only and no missing values. for simplicity we have treated here multi-class problems as 1-class problems. we generally chose as positive class the class  or a group of class  with the lowest frequency in the database.
모for each database  we divided 1 bootstrap samples into separate training and test sets in the proportion 1 1  respecting the prior of the classes  estimated by their frequency in the total database . it is certainly not the best way to build accurate trees for unbalanced datasets or different error costs  but here we are not interested in building the most accurate trees  we just want to study the distance-based kernel probability estimate. for the same reason we grow trees with the default options of j1  weka's  witten and frank  1  implementation of c1  although in many cases different options would build better trees. for unpruned trees we disabled the collapsing function. we used laplace correction smoothing method to correct the raw probability estimate at the leaf for pruned trees and unpruned trees. we also built nbtree on the same samples.
모we used two different metrics in order to compute the distance from the decision boundary  the min-max  mm  metric and the standard  s  metric. both metrics are defined with the basic information available on the data: an estimate of the range of each attribute i or an estimate of its mean ei and of its standard deviation si. the new coordinate system is defined by  1 .
		or		 1 
the parameters of the standard metric are estimated on each training sample.  on each bootstrap sample for the min-max metric  to avoid multiple computation when an attribute is outside the range . in practice  the choice of the metric should be guided by the data source.  for instance  the accuracy of most sensors is a function of the range  therefore it suggests to use an adapted version of the min-max coordinate system. 
모to compute the kernel density estimate of the algebraic distance we choose simplicity and used the standard algorithm available in r  venables and ripley  1  with default options  although the setting of kde parameters  choice of the kernel  optimal bandwidth  etc.  or specialized algorithms  also available in some dedicated packages  would certainly give better results. we systematically used the default option except for the bandwidth selection  since it is inappropriate for using the bayes rule in equation  1 : we used the same bandwidth for f as for f c. we set it to a fraction 뷉 of the range of the algebraic distance . this is the only parameter we used to control the kde algorithm. more sophisticated methods could obviously be used but the bayes rule in equation  1  should be reformulated if variable bandwidths are used.
1 results for the global distance-based kernel probability estimate
in order to compare our method with other methods of probability estimate  we used the auc  the area under the receiving operator characteristic  roc  curve  see  bradley  1  . the auc is widely use to assess the ranking ability of the tree. although this method cannot distinguish between good scores and scores that are good probability estimates  it doesn't make any hypothesis on the true posterior probability  so it is useful when the true posterior probability are unknown  since it is clear that good probability estimates should produce on average better auc . we also used mean squared error and log-loss  although these methods make strong hypothesis on the true posterior probability.
모table 1 shows the difference of the auc from global distance-based kernel  gdk  probability estimate and the laplace correction. apart from a few cases  it gives better values than laplace correction  within a 1% confidence interval . from the intelligibility viewpoint  it is interresting to note that gdk probability estimate on pruned tree is generally better than smoothing method on unpruned tree  which is better than smoothing method on pruned tree . we also performed a wilcoxon unlateral paired-wise test on each batch of 1 samples. the tests confirm exactly the significant results from table 1  with p-values always   1 .
모the choice of the metric has a limited effect on the result in term of auc. the difference is always of a different order except for vehicle  pima and especially glass for which it can reach 1.
모we used several bandwidths  with 뷉 from 1 to 1% of the range of the algebraic distance: results vary very smoothly with the bandwidth  table 1 would present completely similar results.
모table 1 shows the comparison using mean squared-error  mse  as a metric to measure the quality of the probability estimate. the error at test point p  x|c  1  where p c|x  is the true conditional probability  here p c|x  is set to 1 when c x  = c . se x  is then summed over each test sample. the performance of the gdk probability estimate diminishes quickly when 뷉 increases  although
both	both	normal vsdk vs.datasetred.-errornormal	unprunednbtreebupa glass iono. iris
thyroid pendig. pima
sat segment. sonar vehicle vowel wdbc wine1.1.1.1.1.1.1.1챠챠챠챠챠챠챠챠챠챠챠챠1.1.1.1.1.1.1.1-1

1.1.1.1.1.1-1챠챠챠챠챠챠챠챠챠1 1.1.1 -1.1.1 1.1.1 1.1.1 1.1.1 1.1.1 1-1.1챠챠챠챠챠챠챠챠챠챠챠챠1.1.1.1.1.1.1.1
1-1-1-1-1.1.1.1.1.1챠챠챠챠챠챠챠챠챠챠챠챠1.1.1.1.1.1.1.1				
table 1: mean difference of the auc obtained with global dk probability estimate  standard metric  뷉 = 1%  and with laplace correction. gdk on pruned tree versus laplace on pruned or unpruned tree.  mean values and standard deviations are 뫄1. insignificant values are italic. bad results are bold 
the auc remains better. this is easily comprehensible: when 뷉 increases  the kernel estimate tends to erase sharp variations. as a consequence  probability estimate cannot reach the extremes  1 or 1  easily. the log loss metric is not shown since it gives useless results  almost always infinite .
1 local estimate: partition of the space
in order to get more local estimate  the first idea would be to use the leaves to refine the definition of the sets s x  used to run the kernel density estimate  step 1 of algorithm 1 . s x  would be simply the leaf that classifies x. however  we'll argue on a simple example that this option shows severe drawbacks  and that a definition based on the geometry of the dt boundary should generally be preferred.
gd vs. laplacegd vs. nbtreedataset	mse	p-value	mse	p-valuebupathyroid -1-1.1.1챠챠챠챠1 1e-1.1.1e-1.1 1e-1e-1.1 glass iono. iris
pendig.pima -1-1챠1 1e-1 1 satsegment. -1.1챠1.1.1e-1 sonarvehicle -1.1챠1 1e-1 1
vowelwdbcwine	-1-1-1챠1 1e-1 1e-1-1-1-1.1.1챠챠챠챠챠1.1.1 1e-1 1e-1	1.1.1
-1-1-1.1챠챠챠챠1.1.1	1e-1e-1.1
-1.1챠1 1e-1	1e-1
1		
table 1: mean mse difference  and p-value of the corresponding unilateral wilcoxon paired test  between gdk probability estimate  standard metric  뷉 = 1%  and either laplace correction  normal significant values are italic. bad results for gdk are bold pruned tree  or nbtree.  values are 뫄1 except p-values. in-
1 dt boundary-based partition
the rationale behind our partition is to consider the groups of points for which the distance is defined by the same normal to the decision surface  because these points share the same axis defining their distance  and they relate to the same linear piece of the decision surface.
모let {h1 h1 .. hk} be the separators used in the nodes of the tree  and s the learning sample. to partition the total sample s  we associate with each separator hi a subset si of s  which comprises the examples x such that the projection of x onto the decision boundary 붞 belongs to hi. generically  the projection p x  of x onto 붞 is unique  but p x  can belong to several separators. in that case we associate to x the separator hi defining p x  with the largest margin. we define s x  as si. the kde is then run on the h xj  xj 뫍 si  as explained previously.
1 advantage of our partition
we now illustrate on a simple example the advantage of our partition compared with the partition generated by the leaves. suppose that the learning examples are uniformly distributed in a square in a space x  y  and that the probability of class 1 depends only on axis x  with the following pattern  figure 1 :
  for x growing from  1 to 1  the probability of class 1 grows smoothly from 1 to 1  with a sigmoid function centered on  1.
  for x growing from 1 to 1  the probability of class 1 decreases sharply from 1 to 1 around x = 1.
모a typical decision tree obtained from a sample generated with this distribution is represented on figure 1  top . it has three leaves  two yield class -1 and are defined by a single separation  and one yields +1 and is defined by both separations  the gray area on the figure . the partition defined by the leaves thus generates 1 subsets.
모figure 1  bottom  shows the partition obtained with our method. it includes only two subset  one for each separation. figure 1 shows the results obtained with a kernel estimation  a simple parzen window of width 1  on the leaf partition  and on our partition  compared with the actual probability function which was used to generate the sample. one can notice that our partition gives a result which is very close to the one that would be obtained with a kernel estimation on the x axis. this is not the case for the leaf partition  which introduces strong deviations from the original probability.
모the main explanation of these deviations is a side effect which artificially increases the probability of 1 around the border  in the case of the leaf partition. actually  this is the same bias as the one obtained when estimating directly the probabilities on the leaves. moreover  the leaf partition introduces an artificial symmetry on both sides of the leaf  because of the averaging on all the examples at similar distance of the boundary. these problems are not met with our partition. we claim that the problems illustrated in this example have good chances to occur very frequently with the leaf partition. in particular  the side effect due to the border of the leaf will always induce a distortion  which is avoided with our partition. artificial symmetries will always be introduced when a leaf includes several separators.

figure 1: top: partition defined by the three leaves of the tree. bottom: partition into 1 subsets  light gray and gray   defined by the separators.
모table 1 shows the results of the comparison between local distance-based kernel estimate and smoothing method at the leaf for some databases. because of the partition of the space  the results from the auc comparison and mse are not necessarily correlated  and mse is globally better than for the global dk probability estimate.

figure 1: results with a kernel estimator of width 1  parzen window . top: with the leaf partition. bottom: with the separator partition.  in black the true probability  in gray the estimate .
	both	normal vs	mse	p-dataset	normal	unpruned normal  valuebupathyroidpimasatsonarvehiclevowelwdbcwine	-1-1-1-1-1.1.1.1챠챠챠챠챠챠챠챠챠챠1 -1.1.1.1 -1.1.1.1 -1.1 -1.1 -1.1.1 -1.1.1 1-1.1챠챠챠챠챠챠챠챠챠챠챠1.1.1.1.1.1.1 glass iono. iris
pendigits -1
-1-1-1-1-1-1.1챠챠챠챠챠챠1 1.1 1.e-1 1.1.e-1 1.e-1 1.e-1 1.1
-1-1-1-1.1챠챠챠챠1 1.e-1 1.e-1 1.e-1 1.1.e-1			
table 1: comparison between local dkpe  standard metric  뷉 = 1%  and smoothing method: auc mean difference and mse.  all values expect p-values are 뫄1. insignificant values are italic. bad results for local dk are bold 
1 conclusion
we have presented in this article a geometric method to estimate the class membership probability of the cases that are classified by a decision tree. it applies a posteriori to every axis-parallel tree with numerical attributes. the geometric method doesn't depend on the type of splitting or pruning criteria that is used to build the tree. it only depends on the shape of the decision boundary induced by the tree  and it can easily be used for real multi-class problem  with no particular class of interest . it consists in computing the distance to the decision boundary  that can be seen as the euclidean margin . a kernel-based density estimator is trained on the same learning sample than the one used to build the tree  using only the distance to the decision boundary. it is then applied to provide the probability estimate. the experimentation was done with basic trees and very basic kernel estimate functions. but it shows that the geometric probability estimate performs well  the quality was measured with the auc and the mse .
모we also proposed a more local probability estimate  based on a partition of the input space that relies on the decision boundary and not on the leaves boundaries.
모the main limit of the method is that the attributes are numeric. it could be extended to ordered attributes  but it cannot be used with attributes that have unordered modalities. the methods could also be used with oblique trees  but the algorithms to compute the euclidean distance are far less efficient. further work is in progress in order to improve the local estimate. another feature  linked to the nearest part of the decision boundary  could be used to train the kernel density estimator  which are still very efficient in dimension 1 .
