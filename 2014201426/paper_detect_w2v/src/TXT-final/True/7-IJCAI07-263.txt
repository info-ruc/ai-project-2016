 
this paper presents a novel approach to improve the named entity translation by combining a transliteration approach with web mining  using web information as a source to complement transliteration  and using transliteration information to guide and enhance web mining. a maximum entropy model is employed to rank translation candidates by combining pronunciation similarity and bilingual contextual co-occurrence. experimental results show that our approach effectively improves the precision and recall of the named entity translation by a large margin. 
1 introduction 
named entity  ne  translation accepts a named entity from a source language as input and outputs its translations into a 
target language. for instance     english to chinese . large quantities of new named entities appear each day in newspapers  web sites and technical literature  but their translations normally cannot be found in translation dictionaries. improving named entity translation is important to translation systems and cross-language information retrieval applications. moreover  it benefits bilingual resources acquisition from the web and translation knowledge acquisition from corpora. while named entities normally refer to a range of concepts like people names  place names  organization names  product names etc.  the focus of this paper is the translation of people names from english to chinese. 
 many of the previous works extract named entity translation dictionaries from bilingual corpora  kupiec  1; feng et al.  1 . to alleviate the strong dependency on parallel corpora   rapp  1; fung and yee  1  try to obtain named entity translations from nonparallel  comparable texts  or unrelated bilingual corpora.  
 since a large proportion of english named entities can be translated by transliteration  many works try to build transliteration models with a rule-based approach  wan and verspoor  1  and statistics-based approach  knight and graehl  1; lin and chen  1; virga and khudanpur  1; gao  1 . both approaches  however  still have room for improvement. rule-based approaches adopt linguistic rules for a deterministic generation of translation. however  it is hard to systematically select the best chinese character 
from multiple chinese characters that have the same pronunciation  such as . statistics-based transliteration approaches select the most probable translations based on knowledge learned from the training data. this approach  however  still may not work perfectly when there are multiple standards  gao  1 . for example   ford  at the end of an english ne is transliterated into  in most cases  e.g.     but some-
times  it is transliterated into 	 e.g.  
 . obviously  there is significant flexibility in transliteration generation of foreign names in real world  and transliteration selection is somewhat subjective. hence  translation relying on the statistical machine transliteration only may not work well. 
 the web contains an enormous dataset of languages  many of which are instantly available and up to date. in recent years  some researchers  like  wang et al.  1; cheng et al.  1; nagata et al.  1; zhang et al  1  have used it to extract translations with promising results. in particular   wang et al.  1; cheng et al  1  propose a novel approach. they search target language web pages using the source term or ne. then it extracts the translation candidates based on 1  score + local maxima algorithm and ranks the generated candidates with chi-square method and context vector method. this approach gets excellent results for high-frequency terms and nes. however  it cannot handle the low-frequency words or ambiguous words  e.g. 
  well  because their translations scarcely 
appear in the search results.  
                                                 
1
	 	  1...	 	  1...	 
	  1...	 
	1	  1...	 	 	1...	 
1
   where  is the n-gram to be estimated  is the number of unique left adjacent characters.  is the number of unique right adjacent characters. is the frequency of the n-gram .  cheng et al  1 .  other efforts have been made to undertake combinations of transliteration and corpora mining  shao and ng  1; huang et al.  1  or web mining  al-onaizan and knight  1 . in particular   al-onaizan and knight  1  use statistical machine transliteration model to generate translation candidates  and use the web for translation identification. this method demonstrates positive results when using the web for selecting correct translation from candidates  but not for helping generate translation candidates. 
 in this paper  we are interested in the question of : how to and to what extent the ne translation can benefit from the combination of transliteration and web mining in both candidate generation and translation identification. specifically  we are concerned with how to enhance the transliteration results with web information. that means  for example 
      output of transliteration system for   could be corrected as   the correct translation of 
       and how to guide web mining with transliteration information  rather than using statistical association alone. to do this  we propose a novel framework of ne translation that effectively incorporates approaches of transliteration and web mining. specifically  we build a transliteration model first  and the transliteration result is used to guide web mining for generating translation candidates. furthermore  a maximum entropy  me  model is designed to rank the translation candidates with various relevant features.  
 benchmarks are performed on each component and the overall system to reveal our improvement to previous works. experimental results show that ne translation can be consistently improved with our new approach by a large margin.  
1. using transliteration-guided web mining  the translation candidate coverage is improved from 1% to 1%.  
1. by including pronunciation similarity and bilingual contextual co-occurrence features  a me model help to improve the overall translation precision from 1% to 1% in top 1 answer.  
 the remainder of this paper is structured as follows. section 1 presents steps of our approach. in section 1  the transliteration model is described. section 1 introduces our new approach to provide translation candidates by combining transliteration with web mining. the ranking method for translation candidates with a me model is presented in section 1. experiments and performance comparison with previous works are reported in section 1. the last section points out the directions of future work and concludes our study. 
1 our approach  
specifically  the translation process we proposed can be described as follows: 
1. transliteration: to transliterate english ne into chinese word base on their pronunciation similarity. in this step  a three-level transliteration model is built: english surface string to chinese pinyin string  chinese pinyin string to chinese character string and chinese character language model.  
1. web mining for translation candidates enhanced by transliteration: compared with  wang et al.  1; cheng et al  1   transliteration-based similarity is used to improve the translation candidate generation for low-frequency words. the chinese pages are searched using the input english ne as query. the n-gram strings 
are selected from the snippets as translation candidates if they are pronounced similarly to the input ne. 
1. web mining for translation candidates using queries expanded by transliteration: the input english ne is combined with each chinese character  anchor  in the transliteration results to form expanded queries. then the expanded queries are used to search web pages. from the returned snippets  we select the n-grams which contain the anchors and have strong pronunciation similarity to the input ne as additional translation candidates. this approach aims to deal with ambiguous words whose majority usage is not a proper name  e.g.
 .  
1. finally  all the translation candidates generated from the step 1 and step 1 are ranked by a me model that has features covering pronunciation similarity and bilingual contextual co-occurrences. 
1 transliteration 
given an english ne  denoted as   we first syllabicate it into a  syllable  sequence  with the following linguistic rules we defined: 
1. are defined as vowels.  is defined as a vowel when it is not followed by a vowel. all other characters are defined as consonants;  
1. duplicate the nasals  and  whenever they are surrounded by vowels. and when they appear behind a vowel  they will be combined with that vowel to form a new vowel; 
1. consecutive consonants are separated;  
1. consecutive vowels are treated as a single vowel;  
1. a consonant and a following vowel are treated as a syllable;  
1. each isolated vowel or consonant is regarded as an individual syllable. 
 	for example  	 is split into  is split into 	 is split . 	 is split into 	. 
 then a generative model is used to transliterate the syllabicated english name into chinese character string  which follows  knight and graehl  1 . specifically  for the generated  syllable  sequence   we seek a chinese character sequence  that maximizes the product of    and    
 
	*	argmax  	|	   	|	      
 
where  is a chinese pinyin sequence  is the probability of translating  into    is the probability of translating  into  and  is the generative probability of a character-based chinese language model.  different from  knight and graehl  1; virga and khudanpur  1  whose transliteration model is based on phoneme  we use  syllable  as translation units  tu . 
 here  a chinese  syllable  means a chinese pinyin string that is corresponding to a chinese character. and english  syllable  means a combination of several english letters that can generate a corresponding chinese pinyin  syllable . 
 using syllables as tu has the following advantages: 1  compared with phoneme-based approach  a syllable has far less ambiguity in finding the corresponding chinese pinyin string. for instance   has fewer corresponding chinese pinyin strings than the phoneme sequence //; 1  a syllable always corresponds to a legal pinyin sequence. 
 	the translation model 	 can be trained with 
giza++1 on parallel english names and the pinyin representation of chinese names. the translation model  can be approximately obtained from a pronunciation dictionary. in our experiment  we used an in-house dictionary. the language models  is approximated by using a trigram model and trained with a list of chinese transliterated names. 
1 web mining  
after we get the transliteration result of source ne  we want to use it to help finding the translation candidates on the web. the specific methods are as follows.  
1 wm-ne: candidate collection by searching the web with ne  
same with  wang et al.  1   we use the input english ne as a query to search for the chinese pages and extract candidates in the returned top 1 snippets. in our experiments  google search engine1  is used for obtaining the snippets.   it is noticed that usually the results of statistical transliteration model are not exactly correct  but very similar with the correct translation. if we use web data to correct the transliteration result  the accuracy would be improved a lot. based on this insight  the n-grams that have similar pronunciation with the transliteration results are collected from the search engine snippets  and are used for transliteration correction. 
 suppose  is the best transliteration result output by previous transliteration model. we define the score of an n-gram  being a translation candidate as the product of  
and	  	  
 
	score wn 	tl wn w  p wn   
 
where   indicates the pronunciation similarity between the n-gram  and the source english ne  and  indicates the probability of  being a transliterated name which is the generative probability of the chinese character-based language model used in section 1. and  can be computed using the following formula. 
  	 	 	 
	 	 	 	1	 
 	 	 	 	 
 
where  is the pinyin sequence of  and  is the pinyin sequence of . and  are the lengths of  and  respectively  and  is the 
edit distance between 	 and 	.  
 based on the formulas above  we compute the scores for all n-grams in the snippets. and then select those n-grams as candidate whose score of being a translation candidate is over a threshold. 
　 this approach can make adequate use of the transliteration information obtained in the previous model and hand those transliterated translation even though it is a low-frequency word in the snippets. by the way this method is also independent of ne tagger. 
1 wm-ne-anchor: enhancing web mining using transliteration result as anchors 
in addition to wm-ne  the result of transliteration  is used as anchor information to guide the web mining to obtain additional translation candidates by using the following steps.  
1. search the web with an anchor character and the input ne. each character    is combined with source english ne as a query to search for web pages. the first 1 snippets are selected; 
1. surrounding the position of  in a snippet  we extract all the n-gram character strings that contain and its score of being a translation candidate described in subsection 1 is over a threshold as candidates.  
 for example   is transliterated into  by our transliteration model in section 1. then the chinese word is split into three characters:   and . each of these characters is combined with  to form a boolean query to search for web pages. for each query  we select the top 1 returned snippets to form a small corpus  e.g. for the 
query 	  we get a corpus {
}. in the 
corpus we search the position where  appears  and select all n-gram strings that contains  and has score higher 
than the threshold to be a translation candidate  and the results are {  }. 
 this approach can effectively enhance wm-ne. in wm-ne  the returned snippets may not contain the anchor + ne cases within 1 snippets. one of the reasons is that the source ne may be quite ambiguous. it can be used as a person name sometimes  but this is not its majority usage. for example   spain  can be translated into    when used as a person name  but more popularly  it should be translated into    . when we search web using only  spain  as query  all returned snippets by google do not contain  
　   but when we search web using      the top 1 returned snippets contain      {
	 	} . so the wm-ne-anchor method can 
find the specific translation of many ambiguous words. 
1 ranking translation candidates 
a me model is used to rank the translation candidates obtained above  which contains the following four features functions . where   denotes a chinese candidate   denotes the source english ne. 
1. same with  cheng et al.  1   the chi-square of translation candidate  and the input english named entity  is used as one of our features. the chi-square is computed using the formula below: 
	 	1
	1	 	 
 
	 	 = the number of pages containing both  and  
	 	 = the number of pages containing  but not  
 	 = the number of pages containing  but not   	 = the number of pages containing neither  nor  
	 	 = the total number of pages  	  	 
　in our experiments   is set to 1 billion. actually  the value of  doesn't affect the ranking once it's positive. to get the value of parameters in the formula above  we combine  and  as a query to search with google for web pages. and the returned page contains the number of total pages containing both  and  which is corresponding to a in the formula above. then we use  and  as queries respectively to search the web and we can get the two numbers  and   which represent the number of pages contain  and  respectively. 
then we can compute 	  	 and 
.  
1. contextual feature   where  is the number of occurrence  in any of the snippets selected  that  is in a bracket following  or  is in a bracket following ;  
1. contextual feature  where  is the number of occurrence  in any of the snippets selected   and  are immediate neighbors;  
1. similarity of  and  in terms of transliteration similarity   which is equal to  described in 
subsection 1. 
 
with these features  the me model is expressed as:  
	 	1

erative scaling algorithm of  darroch and ratcliff  1 . 
1 experiments  
we use the same method as  gao  1  to collect the training data    we extract all named entity pairs from the ldc chinese-english name entity lists version 1  which also appear in cmu pronunciation dictionary1. we finally obtain 1 person names. out of these name pairs  1 are selected as development set  1 as testing set  and the rest is for training. 
1 transliteration evaluation  
to evaluate our transliteration model using syllables as translation units  we compare its performance with  virga and khudanpur  1  which is also on english to chinese transliteration. the measures are pinyin error rate  pinyin er  and chinese character error rate  char er . here  pinyin er is the edit distance between the  correct  pinyin representation of the correct transliteration and the pinyin sequence output by the system  virga and khudanpur  1 . char er is defined similarly    it is the edit distance between the character sequence of the  correct  transliteration and the character sequence output by the system. the results are listed below: 
 pinyin er char er  virga and khudanpur  1  1% n/a our transliteration model 1% 1% table 1 transliteration model evaluation 
　from table 1  we can see that our model outperforms  virga and khudanpur  1 . the results support using  syllable  as translation unit in both sides in transliteration model. in our experiments  the tu on english side contains about 1 english syllables and the tu on chinese side contains 1 pinyin syllables. 
1 effectiveness of candidate generation by combining transliteration with web mining  
we want to know whether our new web mining approach 
  wm-ne and wm-ne-anchor  improves the coverage of translation candidates. so we made some experiments to compare our method with  al-onaizan and knight  1  and  wang et al.  1 . because they used non-public data set for evaluation and  al-onaizan and knight  1  performed arabic-english translation instead of english-chinese translation  which make the direct comparison difficult. instead  we reimplemented their methods and benchmarked our improvement using ldc data set. the results are listed below: 
 candidates coverage average number of candidates  al-onaizan 	and 
knight  1  1% 1  wang   1  1% 1 wm-ne 1% 1 wm-ne 	+ 
wm-ne-anchor 1% 1 table 1 candidates coverage comparison with previous approaches 
 in  al-onaizan and knight  1   n-best outputs of transliteration model are used as translation candidates for re-ranking. obviously  when  increases  the coverage of translation candidates increases. so which number should we assign to  to make a fair comparison  in our reimplementation experiment of their algorithm  we found that after n increases over 1  the increment of candidate coverage becomes negligible  see figure 1 . so we finally decide to set n equal to 1 for our comparison experiment.  
 

+ local maxima algorithm. this method is good at handling high-frequency words  but worse for low-frequency words. in our testing data  the average frequency of the correct translation appearing in the search result is about eight  and for 1% of nes  the frequency of its correct translation is less than three. so  + local maxima algorithm is hard to extract these translations. from table 1  we also notice that wm-ne-anchor improved the coverage significantly. that means our wm-ne-anchor method does work for many ambiguous nes. 
1 me model evaluation  
to test the effectiveness of each feature used in our me model  we conduct four experiments. like  cheng et al.  1   the metric is  which is defined as the percentage of the ne whose translations could be found in the first  extracted translations.  
 chi-squ are tl chi-square + tl all features 1% 1% 1% 1% 1% 1% 1% 1% 1% 1% 1% 1% 1% 1% 1% 1% table 1 candidate ranking experiment result 
 table 1 shows that incrementally adding features is effective in improving the . ranking with 
chi-square method alone  the precision is 1%. ranking with tl score alone  the precision is 1%. combining these two features together  precision reaches up to 1%  and using all features mentioned in section 1  it reaches a 1% precision rate.  
 we further analyze the reasons. chi-square  as the statistical association of the english ne and the translation candidates  is effective mainly on high-frequency candidates  but less effective on low-frequency candidates. moreover  chi-square cannot differentiate candidates based on contents. however  transliteration scores bring information on linguistic association between ne and candidates. therefore  they can improve ranking effects. in addition  from our results we can see that contextual features are also helpful in improving rankings with about 1% for the . 
1 comparing previous approaches 
to compare with the previous approaches for transliteration and web mining  three experiments are conducted: 
1. exp 1  based on  al-onaizan and knight  1  : uses transliteration model to generate candidates and then combine straight web counts of candidates to select correct translations. 
1. exp 1  based on  wang et al.  1   provides translation candidates via  + local maxima algorithm and ranks them by chi-square method and context vector method. 
1. exp 1  our approach  gets translation candidates via wm-ne and wm-ne-anchor and ranks them with all of the four features mentioned in section 1.  
 table 1 indicates our new approach surpasses the previous approaches of transliteration and web mining by a substantial margin.  
 exp 1 exp 1 exp 1 1% 1% 1% 1% 1% 1% 1% 1% 1% table 1 comparing previous approaches 
 to analyze errors  we checked a sample of 1 nes. 1  1%  nes provide a correct translation. in the remaining data  1  1%  nes acquired translations different from the ones defined in ldc data though they are acceptable. further study shows that most are actually more popular than that ldc data. for instance  the translation of  dockery  in ldc data is    . our system outputs    .  we check with google      and     co-occurs in 1 pages  but     and     co-occurs in only three pages. this may indicate that     is also a popular translation.  
 we further analyze the error sources for the remaining 1  1%  incorrect ne translations. for five nes  the correct translations do not co-occur in any chinese web page. for eight nes  the correct translations are not ranked high enough in the candidate sets by tl score. for the other three nes  the translation errors result from mistakenly identifying lexical boundaries in the candidate generation stage. it should be noted that  since our testing data is randomly selected from a large ldc data set  and many of them are scarcely used in real world  the performance of our system should not be compared directly to those work based on their specific testing data. 
1 conclusions 
in this paper  we present a new approach to combine transliteration and web mining for ne translation. experimental results show that our approach effectively improves the precision and recall of the named entity translation by a large margin.  
our approach has the following contributions:  
1. translation candidate generation is enhanced by combining transliteration-based similarity and web mining. transliteration is especially helpful for low-frequency word translation. 
1. using queries expanded by transliteration  web mining can further improve the coverage of candidate generation. 
1. a me model incorporating different knowledge is effective in ranking translation candidates.  
　in the future  we want to extend this approach to ne translation for other language pairs. we are also interested in adapting the method to translate terminologies. 
