 
many search domains are non-deterministic. although real-time search methods have traditionally been studied in deterministic domains  they are well suited for searching nondeterministic domains since they do not have to plan for every contingency they can react to the actual outcomes of actions. in this paper  we introduce the min-max lrta* algorithm  a simple extension of korf's learning real-time a* algorithm  lrta*  to non-deterministic domains. we describe which non-deterministic domains min-max lrta* can solve  and analyze its performance for these domains. we also give tight bounds on its worst-case performance and show how this performance depends on properties of both the domains and the heuristic functions used to encode prior information about the domains. 
1 	introduction 
real-time  heuristic  search methods  a term coined by korf  korf  1   interleave search with action execution  limiting the amount of deliberation performed between action executions after an action has been executed  the deliberation-act cycle is repeated - until a goal state is reached.  korf  1  demons  rated that real-time search methods are powerful suboptimal search methods that can often outperform more traditional search methods in terms of total running time. for example  they are among the few search methods that can find suboptimal solution paths for the 1-puzzle  a domain with more than 1 x 1 states. 
   real-time search methods have usually been investigated in the context of traditional ai search domains: sliding tile puzzles such as the 1- or 1 puzzle  blocks worlds  grid worlds  and others. these domains are usually assumed to be deterministic: whenever an action is executed in the same state  the same successor state 
   'this research was supported in part  by nasa under contract nagw-1. the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies  either expressed or implied  of nasa or the u.s. government. 
1 	planning 
results. many domains  however  are non-deterministic  such as many robotics  control  or scheduling domains. in this paper  we present a first step towards extending real-time search methods to non-deterministic singleagent search domains by viewing real-time search as a game where the search method selects the actions and nature  a fictitious opponent  chooses their outcomes. 
¡¡we investigate suboptimal search  i.e. how to get the agent to any goal state. the path traversed by the agent does neither have to be optimal nor repeatable  which is a sufficient condition for many real-world problems. real-time search methods appear to be well suited for suboptimal search in non-deterministic state spaces. in contrast to traditional  off-line  search techniques  which must plan for every possible outcome  real-time search methods only need to choose actions for those outcomes that actually occur. thus  real-time search methods can potentially decrease search time  although possibly at the expense of action execution time: since they do not plan exhaustively for every possible outcome of an action  one cannot be sure how good it really is to execute the action. it might well be that the action has an outcome that makes it hard for the agent to reach a goal state. in this paper we begin to quantify the tradeoff between search time and action execution time by analyzing the performance of real-time search in nondeterministic domains. 
¡¡our new technique  which we call min-max lrta*  is based on korf's learning real-time a* algorithm  lrta*   korf  1; 1; 1 . lrta* is a singleagent real-time search algorithm that can be used to find suboptimal and optimal solution paths in deterministic domains. it performs only minimal computations between action executions  we constrain it to a lookahead of one   choosing only which action to execute next  and basing this decision only on information local to its current state. we extend lrta* to non-deterministic domains  describe which non-deterministic domains it can solve  and analyze its performance for these domains. we also give tight bounds on its worst-case performance and show how this performance depends on properties of the domains and the heuristic functions. our theoretical analysis  which suggests what constitutes easy and hard real-time search problems in non-deterministic domains  also applies to both deterministic domains and multi-agent domains  such as moving target search . 

1 	a s s u m p t i o n s a n d 
lrta*-type real-time search algorithms differ from traditional search algorithms  such as the a* algorithm  in that they always maintain a current state. this is a state of the search space; it can only be changed by executing actions. the real-time search algorithm can choose the action freely from the actions that are applicable in its current state. while chronological backtracking is such a search method  it can only be used in undirected  deterministic state spaces. in non-deterministic state spaces  a real-time search algorithm might not be able to backtrack  i.e. undo action executions . 
   we view real-time search in non-deterministic domains as a two-player game. the action that the real-time search algorithm selects determines the possible successor states  from which some mechanism  which we call nature  has to choose one we do not impose any restrictions on how nature makes its decisions  its strategy  and  furthermore  assume that we do not know nature's strategy. although a second agent might indeed exist in some real-time search scenarios  our assumption of its existence is simply an analysis tool. 
   we use the following notation: s denotes the finite set of states of the state space  of size n := |.s'|   .smart s is the start state  and g  with | is the set of goal states. a s  is the finite set of actions that can be executed in state s i s. executing action causes a  potentially non-deterministic  state transition into one of the states succ s a   with succ{s a  
s . 	identity actions are actions a 	with .s-
succ s a   i.e. those actions that might not. result in a state change. we call a state space deterministic iff the cardinality of succ  s  a  is one for all s 
for deterministic state spaces  we use suec s  a  not only to denote the set of successor states  but also the only element of this set. note that every deterministic state space is per definition non-deterministic as well. we call a state space non-deterministic if we want to stress that we do not require it to be deterministic. 

1 d e t e r m i n i s t i c d o m a i n s : 	l r t a * 
   we describe a simple version of korf's l r t a * a l g o r i t h m that has lookahead one. it consists of a termination checking step  line 1   an action selection step  line 1   a value update step  line 1   and an action execution step  line 1   see figure 1. 
   first  l r t a * checks whether it has reached a goal state and thus can terminate successfully. if not  it decides on the action to execute next. it looks only one action execution ahead  picking the action that leads to the successor state with the smallest state value v s   which approximates the goal distance gd s   ties can he broken arbitrarily . note that the algorithm is greedy since it always chooses the action that appears to be best locally. the algorithm then replaces the value v{s  with the onestep lookahead value m a x   v   s     1 + v succ s a     which is a more accurate estimate. finally  l r t a * executes the selected action and iterates. 
   korf showed that l r t a * is correct for deterministic  strongly connected  i.e. d s s'    for all $¡Þ s' e s  state spaces. t h a t is  it reaches a goal state eventually and terminates; the sequence of executed actions is a suboptimal solution path. 
1 n o n - d e t e r m i n i s t i c 
	d o m a i n s : 	m i n - m a x 	l r t a * 
   the extensions necessary to make l r t a * work in non-deterministic state spaces are fairly straightforward. since we do not know which strategy nature uses  we use 
	koenig and simmons 	1 1 

a  worst-case  minimax approach and let the search algorithm act as if nature tries to maximize the goal distance of the search algorithm while the search algorithm tries to minimize it. if the search algorithm can reach a goal state and terminate for such a vicious strategy of nature  it will also reach a goal state if nature uses a different  and therefore less vicious  strategy. as a consequence  the search algorithm does not depend on assumptions about the strategy that nature actually uses. 
¡¡the m i n - m a x l r t a * a l g o r i t h m is shown in figure 1. it uses triax  ' e   u       fll v' s'  at the places in the action selection step  line 1  and value update step  line 1  where lrta* uses v succ s  a  . in deterministic state spaces  min-max lrta* reduces to the original lrta* algorithm. 
1 performance analysis 
in this section  we analyze the performance of min-max 
lrta*  which we measure as the total number of action executions until a goal state is reached. this is justified  because the time needed to execute an action in the world often dominates the minimal amount  of computation that min-max lrta* performs between action executions. even if this is not the case  the total number of actions that min-max lrta* executes can still be roughly proportional to its total running time  because it performs only a bounded and in many domains essentially constant amount of computation between action executions. we define its complexity to be an upper bound on the number of action executions that holds for all possible topologies of state spaces of a given size  start and goal states  tie breaking rules among actions that evaluate to the same value  and strategies of nature. 
¡¡there exist state spaces in which every real-time search algorithm has infinite complexity this is the case if the search algorithm can get trapped in a part of the state space that does not contain a goal state. traditionally  researchers have therefore restricted their attention to strongly connected state spaces or  more generally  state spaces with d   ¡Þ. we use the same assumption for non-deterministic state spaces and call state spaces with this property safely explorable.  to be more precise: the goal distances of all states that the agent can reach from its start state without passing through a 
goal state have to be finite.  moore and afkeson's partigame algorithm  moore and atkeson  1   for example  learns non-deterministic abstractions of spatial state spaces that are safely explorable. 
   intuitively  we expect min-max lrta* to do well in safely explorable state spaces when the state spaces are relatively small or contain many goal states. in the latter case  the we expect it to perform the better  the more the goal states are spread out over the state space. in the following  we analyze this intuition formally. 
1 	c o m p l e x i t y : u p p e r b o u n d s 
in this section  we provide upper bounds on the complexity of min-max lrta*. but first  we introduce some definitions of properties of the state values v s  that we need in order to be able to state our results. 
1 	planning 


	koenig and simmons 	1 


   in this case  the min-max lrta* algorithm must execute a total of n1 - n actions. thus  the complexity of n1 - n is tight. 
theorem 1 also states that a zero-initialized min-max 
lrta* algorithm reaches a goal state after at most 
  action executions if the state space has no identity actions.  for safely explorable state spaces. now consider again the state space in figure 1  this time with the identity actions removed. a zero-initialized min-max lrta* algorithm can traverse the following state sequence  which is equal to the above state sequence  but with repeated occurrences of the same state deleted : 

¡¡since the min-max lrta* algorithm executes l / 1 r r l/1n actions in this case  the complexity of l/1n  - l/1n is tight for state spaces that have no identity actions. note that identity actions can always be safely deleted from a state space  since their removal does not affect whether min-max lrta* can solve a given search problem in the worst-case. our results show  however  that their removal can at most halve the complexity of uninformed  i.e. zero-initialized  min-max lrta*. 
¡¡the state space used in the above examples was artificially constructed. however  the complexity of 1 n1  is tight even for more realistic state spaces  such as grid worlds. they have often been used as testbeds for real-time search methods  pemberton and korf  1; ishida and korf  1 . consider the grid world shown in figure 1 and assume n   1 with n mod 1 = 1. a zero-initialized min-max lrta* algorithm can traverse the following state sequence: 


¡¡in this case  the min-max lrta* algorithm executes ' 1/1 - 1 actions before it reaches the goal state. this also shows that the complexity of g n1  is tight for undirected state spaces for which the number of actions that can be executed in any state is bounded from above by a small constant  here: three . 
1 	d e c r e a s i n g t h e c o m p l e x i t y 
this section demonstrates how properties of the search domains and the heuristic functions can decrease the complexity of min-max lrta*. 
d o m a i n properties 
since the complexity of uninformed min-max lrta* is tight at  our intuition was correct: the smaller the state space and/or the average goal distance  the smaller the complexity. consider  for instance  the sliding tile puzzles. these deterministic domains are sometimes considered to be hard search problems  because they have a low goal density. the 1-puzzle  for example  has 1 states  that are reachable from the start state   but only one goal state. our complexity results  however  imply that average goal distance  not goal density  is among the factors that determine the hardness of a search problem for  min-max or original  lrta*. although increasing the goal density tends to decrease the average goal distance  there are search problems with low goal density and low average goal distance. the 1-puzzle is an example: figure 1 shows for every goal distance how many of the 1 states have this particular goal distance. it turns out that the average goal distance of the 1-puzzle with the american goal state is only 1 and the largest goal distance is 1. similarly  the average goal distance of the 1-puzzle with the european goal state  that cannot be reached from the american goal state  is 1 and the largest goal distance is 1.  see  reinefeld  1  for extensive statistics on the 1-puzzle.  in both cases  the average goal dis-

1 	planning 

tances are much smaller than the size of the state space. thus  sliding-tile puzzles are rather well-suited search problems for  min-max or original  lrta* - compared to many grid worlds of the same size  for example. this does not imply  however  that lrta* can solve sliding tile puzzles with a huge number of tiles  since the complexity of lrta* does not only depend on the average 


 that is  theorem 1 predicts correctly that the search algorithm needs only at most gd s  action executions to reach a goal state from any given s € s and  thus  that it follows a shortest path to a goal 
   it is easy to determine admissible heuristic functions for non-deterministic state spaces if one can determine them for deterministic state spaces. one can simply assume that nature decides in advance which successor state g s a  € succ s a  to choose every time the agent executes action a € a s  in state s € s - all possible assumptions about which particular actions nature chooses are fine. if nature really used this strategy and the agent found out about it  then the state space would effectively become deterministic for the agent. one can easily see that any admissible heuristic function for the goal distances in this deterministic state space is admissible for the original  tion-deterministir search problem as 
well  regardless of the strategy that nature actually uses note  however  that the informedness of this heuristic function depends on how close the assumed behavior of nature is to its most vicious strategy. 
1 example domains 
in this section  we give two examples that demonstrate how min-max lrta* can be applied to nondeterministic search problems. in particular  we discuss search problems with coarse models  for example  abstract state spaces  and moving target search. 
1 	s e a r c h w i t h c o a r s e m o d e l s 
our complexity results for min-max lrta* do not depend on how nature selects successor states. thus  they apply to scenarios where the search algorithm is not able to make assumptions about nature's strategy. assume  for example  that one can model a deterministic world only with low granularity. then  one might not be able to identify one's current state uniquely  and actions can appear to have non-deterministic effects. assume  for instance  that a search algorithm occupies either state 1 or state 1 in some state space  but cannot distinguish between these two states. action a is a deterministic action that results in state 1 when it is executed in state 1 
and in state 1 when it is executed in state 1. thus  the execution of action a can result in either state 1 or 1  but the search algorithm has no way of predicting which of these states will result and could attribute this to nature having a strategy that is unknown to the search algorithm. 
¡¡an application with these characteristics is the trayt i l t i n g p r o b l e m  christiansen  1; kadie  1; erdman and mason  1 : one puts an object into a tray in a given starting position and then slides it repeatedly by tilting the tray until it is in a given goal position. in our version of the tray-tilting problem  one can observe the position of the object with an overhead camera before deciding on a tilting action. the corresponding state space is non-deterministic  because one can neither observe the position of the object precisely nor control the motion of the tray precisely. min-max lrta* can be used to control the tilting artions directly and eventually orients the object in the desired position if the state space is safely explorable.  we have performed experiments to verify that a large number of tray tilting problems are indeed safely explorable   our complexity results provide an upper bound on the number of tray tilting actions needed. 
1 	m o v i n g t a r g e t s e a r c h 
when deriving the complexity results  we assumed the existence of a fictitious opponent  nature.  but the results also apply to scenarios where there is a real opponent. in a way  applying single-agent real-time search methods to two-player games closes a loop  since realtime search was originally inspired by time-constraints present in antagonistic two-agent domains such as game playing. consider  for example  m o v i n g target search - the task for a hunter is to catch an independently acting prey. both agents move on a known directed graph. the hunter moves first  then they alternate moves to adjacent vertices.  if the agents can pass their moves  one can model this by adding identity actions to the graph.  both agents can always sense the current vertices of themselves and the other agent  but the hunter does not know where the prey will move. the hunter catches the prey if both agents occupy the same vertex. 
¡¡in our framework  the agent is the hunter and nature is the prey. it is straightforward to map the moving 
target search problem to a non-deterministic single agent search problem against nature  figure 1 . the hunter can catch the prey for sure if the derived state space is safely explorable. if the hunter uses min-max lrta* with lookahead one  then we can utilize our complexity result to derive an upper bound on the number of actions thai  the hunter executes before it catches the prey 
¡¡ ishida and korf  1  have also applied real-time search methods to moving target search  but utilize lrta* for the hunter in a different way. their mts algorithm learns the following strategy for the hunter  until it catches the prey : always move to an adjacent vertex that is on a shortest path to the current vertex of the prey. they prove that the hunter eventually catches the prey on a strongly connected graph if it is faster than the prey. note the differences between the two ap-
	koenig and simmons 	1 


proaches: obviously  one has to make some assumptions to ensure that the prey can not force the hunter into a cycle in which the hunter cannot decrease its distance to the prey. ishida and korf do not restrict the topology of the graph  but. have to assume that the hunter has a speed advantage over the prey. consider for example the graph in figure 1  note that one of its edges is directed  and assume that both agents are equally fast. korf and lshida's algorithm always follows the prey at the same distance if the algorithm is totally informed and the prey runs around in an anti-clockwise cycle. minmax lrta*  however  will eventually go left until the prey takes the one-way street and later go right until the prey is caught  no matter which strategy the prey uses. 
1 extensions 
in this paper  we have measured the complexity of minmax lrta* in action executions. therefore  every action has a cost of one associated with it. however  our analysis can easily be generalized to arbitrary strictly positive cost structures  including ones with non-uniform costs  in a way analogous to  koenig and simmons  1  where we assume deterministic  state spaces . furthermore  various methods have been proposed that improve the performance of lrta*  see for example  matsubara and ishida  1; ishida  1; knight  1; hamidzadeh  1; ishida  1; russell and wefald  
1; shekhar and dutta  1  - we have applied and analyzed these methods in the context of min-max lrta*. 
if one could make assumptions about nature's strategy 
 for example  if one knew that nature is a neutral coin flipper  or the state space were not safely expiorable  one would use a more sophisticated search strategy than a minimax approach. consequently  our future publications will report real-time search results for these cases. 
1 	planning 
1 related work 
 korf  1  considered deterministic  strongly connected domains and showed that lrta* reaches a goal state eventually. he also showed that lrta* eventually finds a shortest path from the start state to a goal state if it is repeatedly reset into the start state when it reaches a goal state   ishida  1  performed additional systematic experiments to understand how the performance of lrta* can be improved by utilizing initial knowledge in form of heuristic functions.  barto ef a/.  1  showed how lrta* can be generalized to finding paths of minimal average lengths in probabilistic domains.  heger  1  used an on-line minimax algorithm based on qlearnmg  watkins  1  to learn paths of minimal worstcase length. since his algorithm  q-learning  is similar to min-max lrta*  it  benefits from our complexity analysis.  for the relationship between q-learning and lrta* see  koenig and simmons  1 .  neither of the above researchers have analyzed the complexity of their algorithms  but most of them report empirical results. 
¡¡ ishida and korf  1  proposed the moving target search algorithm mts  showed how to utilize initial knowledge in form of heuristic functions for mts  and analyzed its complexity in deterministic  strongly connected state spaces that do not contain identity actions. mts reduces to lrta* with lookahead one if the target  does not move.  littnian  1  pointed out that in antagonistic two-agent situations such as moving target search it can be advantageous to use probabilistic  strategies over minimax strategies if both agents can move simultaneously. 
1 	c o n c l u s i o n 
in this paper  we have relaxed the standard assumption that search domains are deterministic and studied suboptimal real-time search methods in non-deterministic domains. we viewed real-time search as a game where the search algorithm selects the actions and nature  a fictitious opponent  chooses their outcomes. in particular  we introduced the min-max lrta* algorithm  a 
simple extension of korf's lrta* algorithm to nondeterministic domains. 
¡¡we analyzed the worst-case performance of min-max lrta* theoretically. in particular  we introduced the notion of a safely expiorable state space and showed that the complexity of uninformed min-max lrta* in safely expiorable state spaces is proportional to the product of the size of the state space  s  and the average goal distance over all states. we proved that the complexify of uninformed min-max lrta* can get as large as |s'|1-  s  action executions  but not larger  1| s|1 - 1|s| action executions if the state space does not have identity actions that can leave the state unchanged . we also showed how min-max lrta* can take advantage of initial knowledge in form of heuristic functions for the goal distances. 
¡¡our complexity results hold for deterministic domains as well. in particular  deterministic state spaces are not easier to solve with min-max lrta* than nondeterministic ones. since min-max lrta* reduces to 

lrta* in deterministic domains  the complexity results also apply to the original lrta* algorithm. it follows  for example  that uninformed lrta* can search large state spaces with small average goal distances  such as sliding tile puzzles  much faster than equally large state spaces with large average goal distances. 
acknowledgements 
thanks to alan christiansen  matt mason  andrew moore  sebastian thrun  and especially lonnie chrisman and matthias heger for helpful discussions. thanks also to torn ishida for providing an english translation of one of his papers that was written in japanese. 
