 
       we define the concept of meta-level knowledge  and illustrate it by briefly reviewing four examples that have been described in detail elsewhere  1 . the examples include applications of the idea to tasks such as transfer of expertise from a domain expert to a program  and the maintenance and use of large knowledge bases. we explore common themes that arise from these examples  and examine broader implications of the idea  in particular its impact on the design and construction of large programs. 
this work was supported in part by the bureau of health sciences 
research and evaluation of hew under grant hs-1 and by the advanced research projects agency under arpa order 1. it was carried out on the sumex-aim computer system  supported by the nih under grant rr-1. the views expressed are solely those of the author. 
 1  introduction 
       the representation and use of knowledge has been a central problem in al research. a range of different encoding techniques have been developed  along with a number of approaches to applying knowledge. most of the effort to date  however  has concentrated on representing and manipulating knowledge about a specific domain of application  like game-playing     natural language understanding       speech understanding       chemistry     etc. 
       this paper explores a number of issues involving representation and use of what we term meta-level knowledge  or knowledge about knowledge. it begins by defining the term  then exploring a few of its varieties and considering the range of capabilities it makes possible. four specific examples of meta-level knowledge are described  and a demonstration given of their application to a number of problems  including interactive transfer of expertise and guiding the use of knowledge. finally  we consider the long term implications of the concept and its likely impact on the design of large programs. 
{1} meta-level knowledge 
       in the most general terms  meta-level knowledge is knowledge about knowledge. its primary use here is to enable a program to  know what it knows**  and to make multiple uses of its knowledge. that is  the program is not only able to use its knowledge directly  but may also be able to examine it  abstract it  reason about it  or direct its application. to see in general terms how this can be accomplished  imagine taking some of the available representation techniques and turning them in on themselves  using them to describe their own encoding and use of knowledge. the result is a system with a store of both knowledge about the domain  the object level knowledge   and knowledge about its representations  the meta-level knowledge . 
{1} background 
       some early efforts in al involved the search for a single problem solving paradigm that would be both powerful and widely  or even universally  applicable. by the late 1's it became clear that a single such paradigm was at best elusive  and that high  i.e.  near human level  performance on non-trivial tasks required large stores of domain specific knowledge. a number of such knowledge-based systems have been developed and the methodology applied to a wide range of tasks  including speech understanding  1}  algebraic symbol manipulation  1 and chemistry {1} because of the magnitude of the task of assembling the knowledge base for these systems  the accumulation  management and use of large stores of task specific knowledge has itself become a significant research problem. 
       it was this problem that provided the context for the development and exploration of meta-level knowledge reported here. the examples described below are all aimed toward the three aspects of the problem noted just above  knowledge accumulation  management  and use : 
schemata  section 1  and rule models  section 1  support accumulation of knowledge via interactive transfer of expertise from a human expert to the knowledge base of the system. 
the schemata  along with the function templates  section 1   provide a mechanism for handling some aspects of knowledge base maintenance. 
finally  meta-rules  section 1  are applied to the problem of guiding the use of knowledge by offering a means of expressing strategies. 
all of these are part of the teiresias system  1  an 
interlisp program designed to function as an assistant in the construction of high performance programs. a key element in this construction process is the transfer of expertise from a human expert to the program. since the domain expert often knows nothing about programming  his interaction with the performance program usually requires a human programmer as intermediary. we have sought to create in teiresias a program to supply the same sort of assistance as that provided by the programmer  in order to remove the programmer from the loop. 
       we view the interaction between the domain expert and the performance program in terms of a teacher who continually challenges a student with new problems to solve  and carefully observes the student's performance. the teacher may interrupt to request a justification of some particular step the student has taken in solving the problem  or may challenge the final result. this may uncover a fault in the student's knowledge of the subject  and result in the transfer of information to correct it. 
       figure 1 below shows the overall architecture of the sort of program teiresias is designed to help construct. the knowledge base is the program's store of task specific knowledge that makes possible high performance. the inference engine is an interpreter that uses the knowledge base to solve the problem at hand. 

figure 1 - architecture of the performance program 
       the main point of interest in this very simple design is the explicit division between these two parts of the program. this division allows us to assign the human expert the task of augmenting the knowledge base of a program whose control structure  inference engine  is assumed both appropriate and debugged. the question of how knowledge is to be encoded and used is settled by the selection of one or more of the available representations and control structures. the expert's task is to enlarge what it is the program knows. if all of the control structure information has been kept in the inference engine  then we can engage the domain expert in a discussion of the knowledge base and be assured that the discussion will have to deal only with issues of domain specific expertise  rather than with questions of programming and control structures . 
       in this discussion we will assume the knowledge base contains information about selecting an investment in the stock market; the performance program thus functions as an investment consultant.1 knowledge is in the form of a collection of associative triples  attribute  object  value  which characterize the domain  and approximately 1 inference rules built from them  figure 1 . each rule is a single  chunk  of domain specific information indicating an action  in this case a conclusion  which is justified if the conditions specified in the premise are fulfilled. 

languages & systems-1: davis 1 

rule1 
if 	 	the time-scale of the investment 1s long-term  
 the desired return on the investment 1s greater than 1%  
         1   the area of the investment 1s not known  then there 1s evidence  .1  that the name of the stock to invest 1n 1s at&t. 
premise 	 sand  same objct timescale long-term  
 greater objct returnrate 1  
 notknown objct investment-area   
action 	 conclude objct stock-name at&t .1  
figure 1 - inference rule  english and lisp forms  
 1  types of meta-level knowledge 
       we examine below four examples of meta-level knowledge  and review for each  /  the general idea;  //'  a specific instance  
detailing the information it contains;  iii  an example of how that information is used to support knowledge base construction  maintenance  or use; and  /v  the other capabilities it makes possible. figure 1 summarizes the type of information contained in each of the four examples. 
knowledge about is encoded in *************** representation of objects ************* schemata representation of functions function templates inference rules rule models reasoning strategies meta-rules figure 1 - four types of meta-level knowledge 
{1} 	example 1: schemata {1.1} 	introduction: the need for knowledge about representations 
       as data structures go beyond the simple types available in most programming languages  to extended data types defined by the user  they typically become rather complex. large programs may have numerous structures which are complex in both their internal organization and their interrelationships with other data types in the system. that is  the design and organization of data structures in any sizable system often involves a non-trivial store of detailed information. yet such information is typically widely scattered  perhaps throughout comments in system code  in documents and manuals maintained separately  and in the mind of the system architect. 
       this presents a problem to someone who wants to make any sort of change to the system. consider  for example  the difficulties typically encountered in such a seemingly simple problem as adding a new instance of an existing data type to a large program. just finding all of the necessary information can be a major task  especially for someone unfamiliar with the system. 
       one particularly relevant set of examples comes from the numerous approaches to knowledge representations which hava been tried over the years. while the emphasis in discussions of predicate calculus  semantic nets  production rules  frames  etc. has naturally concerned their respective conceptual power  at the level of implementation each of these has presented a non-trivial problem in data structure management. 
       the second example of meta-level knowledge involves describing to a system a range of information about the representations it employs. the main idea here is  first  to view every knowledge representation in the system as an extended data type  and write explicit descriptions of each of them. these descriptions should include all the information about structure and interrelations that was noted earlier as often widely scattered. next  we devise a language in which ail of this can be put in machine-comprehensible terms  and write the descriptions in those terms  making this store of information available to the system. finally  we design an interpreter for the language  so that the system can use its new knowledge to keep track of the details of data structure construction and maintenance. 
       this is of course easily said and somewhat harder to do. it involves answering a number of difficult questions concerning the content of the required knowledge  and concerning how that information should be represented and used. this paper gives an overview of the answers  details can be found in  and {1} the discussion here demonstrates briefly that the relevant knowledge includes information about the structure and interrelations of representations  and shows that it can be used as the basis for a form of knowledge acquisition. 
       the approach is based on the concept of a data structure schema  a device which provides a framework in which representations can be specified. this framework  like most  carries its own perspectives on its domain. one point it emphasizes strongly is the detailed specification of many kinds of information 
 bout representations. it attempts to make this specification task easier by providing ways of organizing the information  and a relatively high level vocabulary for expressing it. 
	{1.1} 	schema example 
       there are three levels of organization of the information about representations  figure 1 . at the highest level  a schema hierarchy links the schemata together  indicating what categories of data structures exist in the system and the relationships between them. at the next level of organization are the individual schemata  the basic unit around which the information about representations is organized. each schema indicates the structure and 
interrelationships of a single type of data structure. at the lowest level are the slotnames  and associated structures  from which the schemata are built; these offer knowledge about specific conventions at the programming language level. each of these three levels supplies a different sort of information; together they compose an extensive body of knowledge about the structure  organization  and implementation of the representations. 
schema hierarchy - indicates categories of representations and their organization 
i n d i v i d u a l 	schema - describes structure of a single representation 
slotnames 	- the schema building blocks  describe 
implementation conventions 
figure 1 
       the hierarchy is a generalization hierarchy that indicates the global organization of the representations. it makes extensive use of the concept of inheritance of properties  so that a particular schema need represent only the information not yet specified by schemata above it in the hierarchy. this distribution of information also aids in making the network extensible  see  for examples and further details . 
       each individual schema contains several different types of information: 
1  the structure of its instances 
1  interrelationships with other data structures 
1  a pointer to all current instances 
1  inter-schema organizational information 
1  bookkeeping information 
       figure 1 shows the schema for a stock name; information corresponding to each of the categories listed above is grouped together. 
stockname-schema 
	plist 	   instof 	stockname-schema 	givenit 
	synonym 	 kleene  1    atom    	askit 
tradedon  kleene   1 1   
  market-inst firstyear-inst   
askit 
	riskclass class-inst 	askit 
createit  
relations   and* stocknamelist hilotable  
 xor* common pfd cumpfd particpfd  
  or* pfd cumpfd  pfdratetable  
  and* cumpfd  omitteddivs    
instances  american-motors at&t . . . xerox zoecon  father  value-schema  offspring nil descr  the stockname-schema describes the format for a stock name  author davis date 1 instof  schema-schema  figure 1 - schema for a stock name 
       the first five lines in figure 1 contain structure information  and indicate some of the entries on the property list  plist  of the data structure which represents a stock name. the information is a triple of the form 
	 s1otname  	 b1ank  	 adv1ce  
the slotname labels the  kind  of things which fills the blank  and serves as a point around which much of the  lower lever information in the system is organized. the blank specifies the format of the information required  while the advice suggests how to find it. some of the information needed may be domain specific  and hence must be requested from the expert. but some may concern solely internal conventions of representation  and hence should be supplied by the system itself  to insulate the domain 

l a n g u a r g e s & s y s t e n s - 1 : 	d a v i s 

expert from such details. the advice provides a way of indicating which of these situations holds in each case. 
       the next five lines in the schema indicate its interrelations with other data structures in the system. the main point here is to provide the system architect with a way of making explicit all of the data structure interrelationships upon which his design depends. expressing them in a machine-accessible form makes it possible for teiresias to take over the task of maintaining them  as explained below. 
       the schemata also keep a list of all current instantiations of themselves  primarily for use in maintaining the knowledge base. if the design of a data structure requires modification  it is convenient to have a pointer to all current instances to insure that they are similarly modified. 
       the next two lines contain organizational information indicating how the the stockname schema is connected to the schema hierarchy. 
       finally  there is four lines of bookkeeping information that helps in keeping track of a large number of data structures: each structure is tagged with the date of creation and author  along with a free text description supplied by the author. in addition  each structure has a pointer to the schema of which it is an instance  note in this case that it is the schema itself which is the data structure being described by this information . 
{1.1} 	schemata: use in knowled♀e acquisition 
       use of the schemata for knowledge acquisition relies on several ideas: 
- information in the schema is viewed as a guide to creating a new instance of the representation it describes. 
- that guidance is supplied by the structure description information  which is in the form of a prototype to be instantiated  and the relations information  which is interpreted as pointers to a number of structures which may require updating to insure that necessary data structure interrelations are maintained. 
- it is this instantiation and interpretation process that drives the knowledge transfer dialog. 
- the advice present in the schema adds a level of sophistication in the dialog. 
       for instance  suppose in the process of adding a new rule to the system the expert mentions a stock the system hasn't heard about yet. learning about the new stock  i.e.  creating a new instance of the concept stock name  becomes a subproblem in the task of acquiring the new rule. the stock name schema is retrieved  and used as the basis for a dialog that requests the necessary information from the expert. detailed examples are found in  and ; an abbreviated version of the dialog is shown below.  in all traces  computer output is in mixed upper and lower case in t h i s font  while user responses are in boldface capitals  tnd 
commentary is  inside braces}. . 
the new r u l e w i l l be called rule1 
if 1 - the desired rate of return on the investment is greater than 1. 
1 - the amount of investment experience of the client is moderate 
1 - the area of the investment is natural resource 
development 
then 
1 - there is evidence  .1  that the name of the stock to invest in is georgia pacific 
sorry  	but i don't understand 
there is evidence  .1  that the name of the stock t  
           invest in is georgia pacific because these words are unknown: georgia pacific 
{teiresias finds it cannot generate any interpretations of the text  so it reprints the original line  and specifies the trouble.} 
would you care to t r y rephrasing that  ++** y 
 please use * ' s to mark what you think might be a new a t t r i b u t e or a new value of an existing attribute  
1 - there is evidence  .1  that the name of the stock to invest in is * georgia pacific * 
{while the language in this example is very simple  there are more complex cases where it would be quite hard to decide from the context what sort of new item the expert had in mind. to avoid the necessity for more sophisticated natural language handling  he is asked to mark off the new word or phrase with asterisks.} 
it looks as though 
　　　georgia pacific refers to a new value of an attribute  correct  
+ + * * y 
. . . 1 s 1t a name of a stock  
+ + * * y 
{the system can then do some very simple pattern matching.} now please t e l l me a few things about 1 t . . . 
{teiresias retrieves the schema for a stock  and  reads  it to find out what it needs to know about any stock.} 
on what markets is georgia-pacific traded  the answer should be 1n the form of 1 or more of the f o l l o w i n g -  market name   year f i r s t traded   type 1 set to a l i n e   then an empty line when done.  
**** 	nyse 1 
**** 	pce 1s 
{each of the questions from the system is prompted by the attempt to fill in some component of the data structure being built  according to the representation conventions indicated by the schema. a translated form of the  blank  is printed to guide the expert; it is then used to parse his answer and insure syntactic validity.} 
what 1s the current r i s k c l a s s i f i c a t i o n of georgia-
pacific  the answer should be 1n the form of a 
　　　　　　 r1sk 	c l a s s i f i c a t i o n   **** 	amex 
sorry  but the following are invalid amex is not a recognized  risk c l a s s i f i c a t i o n   please answer again. 
**** 	blue-chip 
{teiresias now beings working on updating other data structures in response to the addition of the new stock name.} 
should this new item be added to1 - the 1st of common stocks  or 
1 - the 1st of preferred stocks  or 
1 - the 1st of cumulative prefered stocks  or 
1 - the 1st of participating prefered stocks 
select one of the above 
**** 	1 
 adding georgia-pacific to cumulativepfd  
{one of these tasks involves adding the new stock to one of four mutually exclusive lists.  several other necessary updates are also performed; these have been edited out here for brevity  see  for details .} 
ck  	done with georgia-pacific now... back to the r u l e . 
{having finished acquiring the new stock  the system returns to parsing the new rule  and with some help from the expert gets the correct result  the parsing process is omitted here  see  for a detailed example  } this may take a few moments. 
this 1s my understanding of your r u l e : 
	if 1 	- the desired rate of return for the investment 
1s greater than 1% 
1 - the amount of investment experience of the c l i e n t 1s moderate 
1 - the area of the 	investment 	1s naturalresource-development 
then 
1 - there 1s evidence  .1  that the name of the stock to choose is georgia-pacif1c 
{1.1} 	schemata: other uses 
the schemata also support a number of other capabilities. 
they are useful in maintaining the knowledge base  for instance  and offer a convenient mechanism for organizing and implementing data structure access and storage functions. 
       the data structure updating demonstrated in the previous section is one instance of their maintenance capabilities. this updating helps to insure that one change to the knowledge base  adding a new instance of representation  will not violate necessary relationships between data structures. 
languages & systems-1: davis        one of the ideas behind the design of the schemata is to use them as points around which to organize knowledge. the information about structure and interrelationships described above  for instance  is stored this way. in addition  access and storage information is also organized in this fashion. by generalizing the advice concept slightly  it is possible to effect all data structure access and storage requests via the appropriate schema. that is  code which wants to access a particular structure  sends  an access request  and the structure  answers  by providing the requested item1. this offers the well known advantages of insulating the implementation of a data structure from its logical design. code which refers only to the latter is far easier to maintain in the face of modifications to data structure implementation. 
       while they have not yet been implemented  two other interesting uses of the schemata appear possible. first  straightforward extensions to the current system should support a more complex form of knowledge base maintenance. suppose  for instance  it became necessary to modify the representation of a 
       stock  i.e.  we want to edit the stock name schema. it should be possible to have teiresias  watch  as the schema is modified and then carry out the same sequence of modifications on each of the current instances of the schema. where new information was required  e.g.  if new structure descriptors were added to the schema  the system could prompt for the appropriate entry for each instance. while major redesigns would be more difficult to carry out in this fashion  a number of common modifications could be accommodated  easing the task of making changes to structures in the knowledge base. 
       second  the schema also appear to make possible a limited form of introspection. if the information in the relations slot were made accessible via simple retrieval routines  this would make it possible to answer questions like what else in the system will be affected if i add a new instance of this data structure  or what are all the other structures that are related to this one  this would be a useful form of on-line documentation. 
{1} example 1: rule models  1.1} 	rule models as empirical abstractions of the knowledge base 
       in reviewing the rules in the knowledge base  a number of regularities become apparent. in particular  rules about a single topic tend to have characteristics in common - there are  ways  of reasoning about a given topic. this idea of patterns of reasoning has been given a formal  statistical  definition  and provides the basis for the automated construction of a set of empirical generalities about the knowledge base: the rule models. 
       a rule model is an abstract description of a subset of rules  built from empirical generalizations about those rules. it is used to characterize a  typical  member of the subset  and in this sense is similar to the structures used in    and is composed of four parts. first  a list of examples indicates the subset of rules from which this model was constructed. 
       next  a description characterizes a typical member of the subset. since we are dealing in this case with rules composed cv premise-action pairs  the description currently implemented contains individual characterizations of a typical premise and a typical action. then  since the current representation scheme used in those rules is based on associative triples  we have chosen to implement those characterizations by indicating  a  which attributes  typically  appear in the premise  and in the action  of a rule in this subset  and  b  correlations of attributes appearing in the premise  action .1 
       note that the central idea is the concept of characterizing a 
       typical member of the subset. naturally  that characterization would look different for subsets of rules  procedures  theorems  etc. but the main idea of characterization is widely applicable and not restricted to any particular representational formalism. 
       the two other parts of the rule model are pointers to models describing more general and more specific subsets of rules. the set of models is organized into a number of tree structures. these structures determine the subsets for which models will be constructed. at the root of each tree is the model made from all the rules which conclude about  attribute   below this are two models dealing with all affirmative and all negative rules  and below this are models dealing with rules which affirm or deny specific values of the attribute. 
       there are several points to note here. first  these models are not hardwired into the system  but are instead formed by teiresias on the basis of the content of the knowledge base. 
second  where the rules in the knowledge base contain object level information about a specific domain  the rule models contain information about those rules  in the form of empirical generalizations. as such they offer a global overview of the regularities in the rules  and may possibly reflect useful trends in the reasoning of the expert from whom those rules were acquired {1.1} 	rule model example 
       figure 1 shows an example of a rule model  one that describes the subset of rules concluding affirmatively about the area for an investment.  since not all of the details of implementation are relevant here  this discussion will omit some. see  for a full explanation.  as indicated above  there is a list of the rules from which this model was constructed  descriptions characterizing the premise and the action  and pointers to more specific and more general models. each characterization in the description is shown split into its two parts  one concerning the presence of individual attributes and the other describing correlations. the first item in the premise description  for instance  indicates that  most  rules about what the area of an investment should be mention the attribute rate of return in their premise; when they do mention it they  typically  use the predicate functions same and notsame; and the  strength   or reliability  of this piece of advice is 1  see  for precise definitions of the quoted terms . 
       the fourth item in the premise description indicates that when the attribute rate of return appears in the premise of a rule in this subset  the attribute timescale of the investment  typically  appears as well. as before the predicate functions are those typically associated with the attributes  and the number is a indication of reliability. 
examples 	  rule1 .1  
 rule1   rule1 .1   rule1 .1  
 rule1.1  
 rule 1.1   
description 
premise   returnrate same notsame 1  
 timescale same notsame 1  
 trend same 1  
  returnrate same   timescale same  1    timescale same   returnrate same  1  
  bracket samexfollows samexexperience same  
1   
action   investment-area conclude 1  
 risk conclude 1  
             investment-area conclude   risk conclude  1   more-genl  investment-area  
more-spec  investment-area-is-utilities  
figure 1 - example of a rule model 
{1.1} 	rule models; use in knowledge acquisition 
       use of the rule models to support knowledge acquisition occurs in several steps. first  as noted above  our model of knowledge acquisition is one of interactive transfer of expertise in the context of a shortcoming in the knowledge base. the process starts with the expert challenging the system with a specific problem and observing its performance. if he believes its results are incorrect  there are available a number of tools that will allow him to track down the source of the error  see  for details . teiresias keeps track of this debugging process  and responds to the discovery of the source of the error by selecting the appropriate rule model. for instance  if the problem is a rule missing from the knowledge base that concludes about the appropriate area for an investment  then teiresias will select the model shown in figure 1 as the appropriate one to describe the rule it is about to acquire. note that the selection of a specific model is in effect an expression by teiresias of its expectations concerning the new rule  and the generalizations in the model become predictions about the likely content of the rule. 
       at this point the expert types in the new rule  figure 1   using the vocabulary specific to the domain  and expressing it as much as possible in the associative triple format. telresias's problem now is to try to understand what the expert has said. as is traditional   understanding  is determined by converting the text into an internal representation  like that shown in figure 1   then converting this back into english and requesting approval from the expert. 
       since understanding natural language is known to be difficult  we have taken a simpler approach. the basic idea is to allow the text to  suggest  interpretations via a simple keyword-based approach  and to intersect those results with the expectations provided by the selection of a particular rule model. we thus have a data directed process  interpreting the text  combined with a goal directed process  the predictions made by the rule model . each contributes to the end result  but it is the combination of them that is effective. details of this process are described in  and . 
languages & systems-1: davis the new rule will be called rule1 
	i f : 	1 - the client's income tax bracket is 1. 
and 1 - the client is following up on market trends 
　　　　　carefully and 1 -
then : 1 - there is evidence  .1  that the investment area 
　　　　　should be high technology and 1 -
figure 1 
       teiresias displays the results of this initial interpretation of the rule  figure 1 . if there are mistakes  as there are in this case   a rule editor is available to allow the expert to indicate required changes. this is easily accomplished  since teiresias can often make an effective second choice by determining the likely source of error in its initial guess. 
this 1s my understanding of your r u l e : 
rule1 
if 	1  the client's income-tax bracket is 1%  
1  the market has followed a upward trend recently 
1  the client manages his assets carefully 
then there is evidence  .1  that the area of the investment should be high-technology 
figure 1 
       once the expert is satisfied that teiresias has correctly understood what he said  it is the system's turn to see if it is satisfied with the content of the rule. the main idea is to use the rule model to see how well this new rule  fits in  to the system's model of its knowledge - i.e.  does it  look like  a typical rule of the sort expected  
       in the current implementation  the presence of a partial match between the new rule and the generalizations in the rule model triggers a response from teiresias. recall the last line of the premise description in the rule model of figure 1: 
     bracket same   follows same   experience same  1   this indicates that when the tax bracket of the client appears in the premise of a rule of this sort  then how closely he follows the market  and how much investment experience he has typically appear as well. note that the new rule has the first two of these  but is missing the last  and teiresias points this out. 
i hate to c r i t i c i z e   randy  but did you know that most r u l e s about what the area of a investment might be  t h a t mentionthe income-tax bracket of the c l i e n t   and how closely the c l i e n t follows the market 
also mention-
a  - the amount of 	investment 	experience of the c l i e n t shall 	i t r y to w r i t e a clause to account for  a    
++** y 
how about-
a  the amount of 	investment 	experience of the c l i e n t 
　　　1s moderate ok  
**** 	y 
figure 1 
       if the expert agrees to the inclusion of a new clause  teiresias attempts to create it. the system relies on the context of the current dialog  which indicates that the clause should deal with the amount of the client's investment experience   and the fact that the rule must work for this case  or it won't fix the bug  earlier in the interaction  not shown  the expert indicated that the client had a moderate amount of experience . telresias's guess is not necessarily correct  of course  since the desired clause may be more general  but it is at least a plausible attempt. 
       it should be noted that there is nothing in this concept of  second guessing  which is specific to the rule models as they are currently designed  or indeed to associative triples or rules as a knowledge representation. the most general and fundamental point was mentioned above - testing to see how something  fits in  to the system's model of its knowledge. at this point the system might perform any kind of check  for violations of any established prejudices about what the new chunk of knowledge should look like. additional kinds of checks for rules might concern the strength of the inference  number of clauses in the premise  etc. different checks might be devised for other knowledge encoding schemes. 
       the automatic generation of the rule models by teiresias has several interesting implications  since it makes possible a 
       synthesis of the ideas of model-based understanding and learning by experience. while both of these have been developed independently in previous al research  their combination produces a novel sort of feedback loop: rule acquisition relies on the set of rule models to effect the model-based understanding process; this results in the addition of a new rule to the knowledge base  and this in turn prompts the recomputation of the relevant rule model s . 
       note first that performance on the acquisition of the next rule may be better  because the system's  picture  of its knowledge base has improved - the rule models are now computed from a larger set of instances  and their generalizations are more likely to be valid. 
       second  since the relevant rule models are recomputed each time a change is made to the knowledge base  the picture they supply is kept constantly up to date  and they will at all times be an accurate reflection of the shifting patterns in the knowledge base. 
       finally  and perhaps most interesting  the models are not hand-tooled by the system architect  or specified by the expert. they are instead formed by the system itself  and formed as a result of its experience in acquiring rules from the expert. thus despite its reliance on a set of models as a basis for understanding  telresias's abilities are not restricted by the existing set of models. as its store of knowledge grows  old models can become more accurate  new models will be formed  and the system's stock of knowledge about its knowledge will continue to expand. this appears to be a novel capability for a model-based system. 
{1.1} 	rule models; other capabilities 
       as a form of meta-level knowledge  the rule models give the system a picture of its own knowledge. the system can  for instance   read  a rule model to the user  supplying an overview of the information in part of the knowledge base. this may suggest global trends in the knowledge of the expert who assembled the knowledge base  and thus helps to make clear the overall approach of the system to a given topic  for examples see  . 
{1} example 1: function templates 
       associated with each predicate function in the system is a template  a list structure which resembles a simplified procedure declaration  figure 1 . it indicates the order and generic type of the arguments in a typical call of that function  and makes possible very simple versions of two interesting  parallel capabilities: code generation and code dissection. 
	function 	template 
	same 	 obj 	attribute value  
figure 1 - template for the predicate function same 
the template is used as the basis for the simple form of code generation alluded to in section  1}. while details are beyond the scope of this paper  see    code generation is essentially a process of  filling in the blanks : processing a line of text in a new rule involves checking for keywords that implicate a particular predicate function  and then filling in its template on the basis of connotations suggested by other words in the text. 
       code dissection is accomplished by using the templates as a guide to extracting any desired part of a function call. for instance  as noted earlier  teiresias forms the rule models on the basis of the current contents of the knowledge base. to do this  it must be able to pick apart each rule to determine the attributes to which it refers. this could have been made possible by requiring that every predicate function use the same function call format  i.e.  the same number  type  and order of arguments   but this would be too inflexible. instead  we allow every function to describe its own calling format via its template. to dissect a function call  then  we need only retrieve the template for the relevant function  i.e.  the template for the car of the form   and then use that as a guide to dissecting the remainder of the form. the template in figure 1  for instance  indicates that the attribute would be the cador of the form. this same technique is also used by telresias's explanation facility  where it permits the system to be quite precise in the explanations it provides  see  for details . 
       this approach also offers a useful degree of flexibility. the introduction of a new predicate function  for instance  can be totally transparent to the rest of the system  as long as its template can be written in terms of the available set of primitives like attribute  value  etc. the power of this approach is limited primarily by this factor  and will succeed to the extent that code can be described by a relatively small set of such primitive descriptors. while more complex syntax is easily atcomodated  e.g.  the template can indicate nested function calls   more complex semantics are more difficult  e.g.  the appearance of multiple attributes in a function template can cause problems . 

l a n g u g e s & systems-1: d a v i s 

{1} 	example 1: meta-rules {1.1} 	strategies to guide the use of knowledge 
       meta-rules embody strategies - knowledge that indicates how to use other knowledge. this discussion considers strategies from the perspective of deciding which knowledge to invoke next in a situation where more than one chunk of knowledge may be applicable. for example  given a problem solvable by either heuristic search or problem decomposition  a strategy might indicate which technique to use  based on characteristics of the problem domain and nature of the desired solution. if the problem decomposition technique were chosen  other strategies might be employed to select the appropriate decomposition from among several plausible alternatives. 
       this view of strategies can be useful because many of the paradigms developed in al admit  or even encourage  the possibility of having several alternative chunks of knowledge plausibly useful in a single situation  e.g.  production rules  planner-like languages  etc. . faced with a set of alternatives large enough  or varied enough  that exhaustive invocation becomes infeasible  some decision must be made about which should be chosen. since the performance of a program will be strongly influenced by the intelligence with which that decision is made  strategies offer an important site for the embedding of knowledge in a system. 
       this type of guidance can be especially useful in the sort of rule-based performance program that teiresias is designed to help build. the rules in this system are invoked in a simple backward-chaining fashion that produces an exhaustive depth-first search of an and/or goal tree. if the program is attempting  for example  to determine which stock would make a good investment  it retrieves all the rules which make a conclusion about that topic  i.e.  they mention stock-name in their action . it then invokes each one in turn  evaluating each premise to see if the conditions specified have been met. the search is exhaustive because the rules are inexact: even if one succeeds  it was deemed to be a wisely conservative strategy to continue to collect all evidence about a subgoal. 
       the ability to use an exhaustive search is of course a luxury  and in time the base of rules may grow large enough to make this infeasible. as this point some choice would have to be made about which of the plausibly useful rules should be invoked. meta-rules were created to address this problem. 
{1.1} 	meta-rules: examples 
       figure 1 below shows two meta-rules. the first of them says  in effect  that in trying to determine the best investment for a non-profit organization  rules that base their recommendations on tax bracket are not likely to be successful. the second indicates that when dealing with clients nearing retirement age  more secure stocks should be considered before more speculative ones. 
metarule1 
if 1  you are attempting to determine the best stock to invest in  
1  the client's tax status 1s non-profit  
1; there are rules which mention 1n their premise the income-tax bracket of the client  
then 1t 1s very likely  .1  that each of these rules 1s not going to be useful. 
premise 
 $and same objct curgoal stock-name  
 same objct status non-profit  
 thereare olrules  sand 
              mentions freevar premise bracket   set1   action  conclude set1 utility no .1  
metarule1 
if 	1  the age of the client is greater than 1  
1  there are rules which mention in their premise blue-chip risk  
1  there are rules which mention in their premise speculative risk  
then it 1s very likely  .1  that the former should be used before the latter. 
premise 
 $and greater objct age 1  
 thereare olrules  sand 
        mentions freevar premise blue-chip   set1   thereare olrules  sand 
 mentions freevar premise speculative   set1   
action 
 conclude set1 dobefore set1 .1  
figure 1 - two meta-rules 
       it is important to note the character of the information conveyed by meta-rules. first  note that in both cases we have a rule which is making a conclusion about other rules. that is  where object level rules conclude about the stock market domain  meta-rules conclude about object level rules. these conclusions can 
' in the current implementation  be of two forms. as in the first meta-rule  they can make deductions about the likely utility of certain object level rules  or  as in the second  they can indicate a 
partial ordering between two subsets of object level rules. 
       note also that  as in the first example  meta-rules make conclusions about the utility of object level rules  not their validity. that is  metarule1 does not indicate circumstances under which some of the object level rules are invalid  or even  very likely  .1   invalid . it merely says that they are likely not to be useful; i.e.  they will probably fail  perhaps only after requiring extensive computation to evaluate their preconditions. this is important because it has an impact on the question of distribution of knowledge. if meta-rules did comment on validity  it might make more sense to distribute the knowledge in them  i.e.  delete the meta-rule  and just add another premise clause to each of the relevant object level rules. but since their conclusions do concern utility  it does not make sense to distribute the knowledge. 
       adding meta-rules to the system requires only a minor addition to the control structure described above. as before  the system retrieves the entire list of rules relevant to the current goal  call it l . but before attempting to invoke them  it first determines if there are any meta-rules relevant to that goal1. if so  these are invoked first. as a result of their action  we may obtain a number of conclusions about the likely utility  and relative ordering of the rules in l. these conclusions are used to reorder or shorten l  and the revised list of rules is then used. viewed in tree-search terms  the current implementation of meta-rules can either prune the search space or reorder the branches of the tree. 
{1.1} 	meta-rules: guiding the use of the knowledge base 
       there are several points to note about this approach to encoding knowledge. first  the framework it presents for knowledge organization and use appears to offer a great deal of leverage  since much can be gained by adding to a system a store of  meta-level  knowledge about which chunk of object level knowledge to invoke next. considered once again in tree search terms  we are talking about the difference between  blind  search of the tree  and one guided by heuristics. the advantage of even a few good heuristics in cutting down the combinatorial explosion of tree search is well known. thus  where earlier sections were concerned about adding more object level knowledge to improve performance  here we are concerned with giving the system more information about how to use what it already knows. 
       consider  too  that part of the definition of intelligence includes appropriate use of information. even if a store of  object level  information is not large  it is important to be able to use it properly. meta-rules provide a mechanism for encoding strategies that can make this possible. 
       second  the description given in section  1.1  has been simplified in several respects for the sake of clarity. it discusses the augmented control structure  for example  in terms of two levels - the object and meta-levels. in fact  there can be an arbitrary number of levels  each serving to direct the use of knowledge at the next lower level. that is  the system retrieves the list  l  of object level rules relevant to the current goal. before invoking this  it checks for a list  l'  of first order meta-rules which can be used to reorder or prune l. but before invoking this  it checks for second order meta rules which can be used to reorder or prune l'  etc. recursion stops when there is no rule set of the next higher order  and the process unwinds  each level of strategies advising on the use of the next lower level. 
       consider once again the issue of leverage  and recall the value of heuristics in guiding tree search. we can apply the same idea at this higher level  gaining considerable leverage by encoding heuristics that guide the use of heuristics. that is  rather than adding more heuristics to improve performance  we might add more information at the next higher level about effective use of existing heuristics. 
       the judgmental character of the rules offers several interesting capabilities. it makes it possible  for instance  to write rules which make different conclusions about the best strategy to use  and then rely on the underlying model of confirmation  to weigh the evidence. that is  the strategies can  argue  about the best rule to use next  and the strategy that presents the best case  as judged by the confirmation model  will win out. 
       next  recall that the basic control structure of the performance program is a depth-first search of the and/or goat tree sprouted by the unwinding of rules. the presence of meta-rules of the sort shown in figure 1 means that this tree has an interesting characteristic: at each node  when the system has to 

languages & svstems-1: d a v i s 
1 

choose a path  there may be information stored advising about the best path to take. there may therefore be available an extensive body of knowledge to guide the search  but that knowledge is not embedded in the code of a clever search algorithm. it is instead organized around the specific objects which form the nodes in the tree; i.e.  instead of a smart algorithm  we have a  smart tree . 
       finally  there are several advantages associated with the use of strategies which are goal-specific  explicit  and embedded in a representation which is the same as that of the object level knowledge. that fact that strategies are goal-specific  for instance  makes it possible to specify quite precise heuristics for a given goal  without imposing any overhead in the search for any other goals. that is  there may be a number of complex heuristics describing the best rules to use for a particular goal  but these will cause no computational overhead except in the search for that goal. 
       the fact that they are explicit means a conceptually cleaner organization of knowledge and ease of modification of established strategies. consider  for instance  alternative means of achieving the sort of partial ordering specified by the second meta-rule in figure 1. there are several alternative schemes by which this could be accomplished  involving appropriate modifications to the relevant object level rules and slight changes to the control structure. such schemes  however  share several faults that can be illustrated by considering one such approach: an agenda with multiple priority levels like the one proposed in . that is  rather than dealing with a linear list l of relevant rules  those rules would be put on an agenda. partial ordering could be accomplished simply by setting the priority for some rules higher than that of others: rules in subset a  for instance  might get priority 1 while those in subset b were given priority 1. 
       but this technique presents two problems: it is both opaque and likely to cause bugs. it will not be apparent from looking at the code  for instance  why the rules in a were given higher priority than the rules in b. were they more likely to be useful  or is it desirable that those in a precede those in b no matter how useful they each may be  consider also what happens if  before we get a chance to invoke any of the rules in a  an event occurs which makes it clear that their priority ought to be reduced  for reasons unrelated to the desired partial ordering . if the priority of only the rules in a are adjusted  a bug arises  since the desired relative ordering may be lost. 
       the problem is that this approach tries to reduce a number of different  incommensurate factors to a single number  with no record of how that number was reached. meta-rules offer one mechanism for making these sorts of considerations explicit  and for leaving a record of why a set of processes has been queued in a particular order. they also make subsequent modifications easier  since all of the information is in one place - changing a strategy can be accomplished by editing the relevant meta-rule  rather than searching through a program for all the places priorities have been set to effect that strategy. 
       lastly  the use of a uniform encoding of knowledge makes the treatment of all levels the same. for example  second order meta-rules require no machinery in excess of that needed for first order meta-rules. it also means that all the explanation and knowledge acquisition capabilities developed for object level rules can be extended to meta-rules as well. the first of these  explanation  has been done  and works for all levels of meta-rules. adding this to telresias's explanation facility makes possible an interesting capability: in addition to being able to explain what it did  the system can also explain how it decided to do what it did. knowledge in the strategies has become accessible to the rest of the system  and can be explained in just the same fashion. we noted above that adding meta-level knowledge to the system was quite distinct from adding more object level knowledge  since strategies contain information of a qualitatively different sort. explanations based on this information are thus of a correspondingly different type as well. 
{1.1} 	meta-rules; broader implications 
       there are a number of interesting generalizations of the basic scheme presented above  two of which we touch on briefly here. first  while we have been examining the idea of strategies in the context of the depth-first search used by the performance program  the concept is in fact more widely applicable and can be used with a range of control structures. second  meta-rules effect their selection of the relevant object level rules by what we have termed content-directed invocation  an approach which offers advantages over previous knowledge source invocation techniques. 
applications to other control structures 
       the concept of strategies as a mechanism for deciding which chunk of knowledge to invoke next can be applied to a number of different control structures. we have seen how it works in goal-directed scheme  and it functions in much the same way with a data-directed process. in that case meta-rules offer a way of controlling the depth and breadth of the implications drawn from any new fact or conclusion. pursing this further  we can imagine making the decision to use a data- or a goal-directed process itself an issue to be decided by a collection of appropriate meta-rules. at each point in its processing  the system might invoke one set of meta-rules to choose a control structure  then use another set to guide that control structure. this can be applied to many control structures  demonstrating the range of applicability of the basic concept of strategies as a device for choosing what to do next. 
content-directed invocation 
       if meta-rules are to be used to select from among plausibly useful object level rules  they must have some way of referring to the object level rules. the mechanism used to effect this reference has implications for the flexibility and extensibility of the resulting system. 
       to see this  note that the meta-rules in figure 1 refer to the object level rules by describing them  and effect this description by direct examination of content. for instance  metaruleool refers to rules which mention in their premise the income tax bracket of the client  a description  rather than an equivalent list of rule names. the set of object level rules which meet this description is determined at execution time by examining the source code of the rules. that is  the meta-rule  goes in and looks  for the relevant characteristic  in this case the presence of the attribute bracket   using the function templates as a guide to dissecting the rules. we have termed this content-directed invocation. 
       part of the utility of this approach is illustrated by its advantages over using explicit lists of object level rules  e.g.  if metaruleool had been written to indicate   i t is very l i k e l y 
  . 1   that rule1  rule1  rule1  and rule1 are not going to be useful  . if such lists were used  then tasks like editing or adding an object-level rule to the system would require extensive amounts of bookkeeping. after an object level rule has been edited  for instance  we would have to check all the strategies that name it  to be sure that each such reference was still applicable to the revised rule. by using content-directed invocation  however  these tasks require no additional effort  since the meta-rules effect their own examination of the object level rules  and will make their own determination of relevance. 
       additional advantages of this technique are discussed in more detail in  and  
{1  implications 
       the examples reviewed above illustrate a number of general ideas about knowledge representation and use that may prove useful in building large programs. 
       we have  first  the notion that knowledge in programs should be made explicit and accessible. use of production rules to encode the object level knowledge is one example of this  since knowledge in them may be more accessible than that embedded in the code of a procedure. the schemata  templates  and meta-rules illustrate the point also  since each of them encodes a form of information that is  typically  either omitted entirely or at best is left implicit. by making knowledge explicit and accessible  we  make possible a number of useful abilities. the schemata and templates  for example  support the forms of system maintenance and knowledge acquisition described above. meta-rules offer a means for explicit representation of the decision criteria used by the system to select its course of action. subsequent  playback  of those criteria can then provide a form of explanation of the motivation for system behavior  see  for examples . that behavior is also more easily modified  since the information on which it is based is both clear  since it is explicit  and retrievable  since it is accessible . finally  more of the system's knowledge and behavior becomes open to examination  especially by the system itself. 
       second  there is the idea that programs should have access to their own representations. to put this another way  consider that over the years numerous representation schemes have been proposed and have generated a number of discussions of their respective strengths and weaknesses. yet in all these discussions  one entity intimately concerned with the outcome has been left uninformed: the program itself. what this suggests is that we ought to describe to the program a range of information about the representations it employs  including such things as their structure  organization  and use. 
languages & systrms-1: davis 1        as noted  this is easily suggested but more difficult to do. it requires a means of describing both representations and control structures  and the utility of those descriptions will be strongly dependent on the power of the language in which they are expressed. the schemata and templates are the two main examples of the partial solutions we have developed for describing representations  and both rely heavily on the idea of a task specific high level language - a language whose conceptual primitives are task specific. the main reason for using this approach is to make possible what we might call  top down code understanding . traditionally  efforts at code understanding  e.g.      have attempted to assign meaning to the code of some standard programming language. rather than take on this sizable task  we have used the task specific languages to make the problem far easier. instead of attempting to assign semantics to ordinary code  a  meaning  is assigned to each of the primitives in the high level language  and represented in one or more informal ways. thus  for example  attribute is one of the primitives in the  language  in which templates are written; its meaning is embodied in procedures associated with it that are used during code generation and dissection  see  for details . 
       this convenient shortcut also implies a number of limitations. most important  the approach depends on the existence of a finite number of  mostly independent  primitives. this means a set of primitives with only a few  well specified interactions between them. the number of interactions should be far less than the total possible  and interactions that do occur should be uncomplicated  as for example  the interaction between the concepts of attribute and value . 
       but suppose we could describe to a system its representations  what benefits would follow  the primary thing this can provide is a way of effecting multiple uses of the same knowledge. consider for instance the multitude of ways in which the object level rules have been used. they are executed as code in order to drive the consultation  see  and  for examples ; they are viewed as data structures  and dissected and abstracted to form the rule models; they are dissected and examined in order to produce explanations  see  ; they are constructed during knowledge acquisition; and finally they are reasoned about by the meta rules. 
       it is important to note here that the feasibility of such multiplicity of uses is based less on the notion of production rules per se  than on the availability of a representation with a small grain size and a simple syntax and semantics.  small   modular chunks of code written in a simple  heavily stylized form  though not necessarily a situation-action form   would have done as well  as would any representation with simple enough internal structure and of mangable size. the introduction of greater complexity in the representation  or the use of a representation that encoded significantly larger  chunks  of knowledge would require more sophisticated techniques for dissecting and manipulating representations than we have developed thus far. but the key limitations are size and complexity of structure  rather than a specific style of knowledge encoding. 
       two other benefits may arise from the ability to describe representations. we noted earlier that much of the information necessary to maintain a system is often recorded in informal ways  if at all. if it were in fact convenient to record this information by describing it to the program itself  then we would have an effective and useful repository of information. we might see information that was previously folklore or informal documentation becoming more formalized  and migrating into the system itself. we have illustrated above a few of the advantages this offers in terms of maintaining a large system. 
       this may in turn produce a new perspective on programs. early scarcity of hardware resources led to an emphasis on minimizing machine resources consumed  for example by reducing all numeric expressions to their simplest form by hand. more recently  this has meant a certain style of programming in which a programmer spends a great deal of time thinking about a problem first  trying to solve as much as possible by hand  and then abstracting out only the very end product of all of that to be embodied in the program. that is  the program becomes simply a way of manipulating symbols to provide  the answer   with little indication left of what the original problem was  or more important  what knowledge was required to solve it. 
       but what if we reversed this trend  and instead view a 
       program as a place to store many forms of knowledge about both the problem and the proposed solution  i.e.  the program itself . this would apply equally well to code and data structures  and could help make possible a wider range of useful capabilities of the sort illustrated above. 
       one final observation. as we noted at the outset  interest in knowledge-based systems was motivated by the belief that no single  domain independent paradigm could produce the desired level of performance. it was suggested instead that a large store of domain specific  object level  knowledge was required. we might similarly suggest that this too will eventually reach its limits  and that simply adding more object level knowledge will no longer  by itself  guarantee increased performance. instead it may be necessary to focus on building stores of meta-level knowledge  especially in the form of strategies for effective use of knowledge. 
such  meta-level knowledge based  systems may represent a profitable future direction. 
{1} conclusions 
       we have reviewed four examples of meta-level knowledge  and demonstrated their application to the task of building and using large stores of domain specific knowledge. this has showed that supplying the system with a store of information about its representations makes possible a number of useful capabilities. for example  by describing the structure of its representations  schemata  templates   we make possible a form of transfer of expertise  as well as a number of facilities for knowledge base maintenance. by supplying strategic information  meta-rules   we make possible a finer degree of control over use of knowledge in the system. and by giving the system the ability to derive empirical generalizations about its knowledge  rule models   we make possible a number of useful abilities that aid in knowledge transfer. 
notes 
 1  teiresias was developed in the context of the mycin system  1   which deals with infectious disease diagnosis and therapy. the domain has been changed to keep the discussion phrased in terms familiar to a wide range of readers  and to emphasize that neither the problems attacked nor the solutions suggested are restricted to a particular domain of application. the dialogs shown are real examples of teiresias in action  with a few word substitutions: e.g  primary bacteremia became georgia pacific  infection became investment  etc. 
 1  both of these are constructed via simple statistical thresholding operations. 
 1  this was suggested by the perspective taken in work on smalltalk  and actors   
 1  that is  are there meta-rules directly associated with that goal. meta-rules can also be associated with other objects in the system  but that is beyond the scope of this paper. the issues of organizing and indexing meta-rules are covered in more detail in  1j and . 
