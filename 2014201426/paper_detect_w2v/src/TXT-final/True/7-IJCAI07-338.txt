
in this paper  we present the agent programming language teamgolog  which is a novel approach to programming a team of cooperative agents under partial observability. every agent is associated with a partial control program in golog  which is completed by the teamgolog interpreter in an optimal way by assuming a decision-theoretic semantics. the approach is based on the key concepts of a synchronization state and a communication state  which allow the agents to passively resp. actively coordinate their behavior  while keeping their belief states  observations  and activities invisible to the other agents. we show the usefulness of the approach in a rescue simulated domain.
1 introduction
during the recent years  the development of controllers for autonomous agents has become increasingly important in ai. one way of designing such controllers is the programming approach  where a control program is specified through a language based on high-level actions as primitives. another way is the planning approach  where goals or reward functions are specified and the agent is given a planning ability to achieve a goal or to maximize a reward function. an integration of both approaches has recently been proposed through the seminal language dtgolog  boutilier et al.  1   which integrates explicit agent programming in golog  reiter  1  with decision-theoretic planning in  fully observable  mdps  puterman  1 . it allows for partially specifying a control program in a high-level language as well as for optimally filling in missing details through decision-theoretic planning  and it can thus be seen as a decision-theoretic extension to golog  where choices left to the agent are made by maximizing expected utility. from a different perspective  it can also be seen as a formalism that gives advice to a decision-theoretic planner  since it naturally constrains the search space.
¡¡dtgolog has several other nice features  since it is closely related to first-order extensions of decision-theoretic planning  see especially  boutilier et al.  1; yoon et al.  1;

  alternate address: dis  universita` di roma  la sapienza .
  alternate address: institut fu¡§r informationssysteme  tu wien.guestrin et al.  1    which allow for  i  compactly representing decision-theoretic planning problems without explicitly referring to atomic states and state transitions   ii  exploiting such compact representationsfor efficiently solving largescale problems  and  iii  nice properties such as modularity  parts of the specification can be easily added  removed  or modified  and elaboration tolerance  solutions can be easily reused for similar problems with few or no extra cost .
¡¡however  dtgolog is designed only for the single-agent framework. that is  the model of the world essentially consists of a single agent that we control by a dtgolog program and the environment summarized in  nature . but there are many applications where we encounter multiple agents that cooperate with each other. for example  in robotic rescue  mobile agents may be used in the emergency area to acquire new detailed information  such as the locations of injured people in the emergency area  or to perform certain rescue operations. in general  acquiring information as well as performing rescue operations involves several and different rescue elements  agents and/or teams of agents   which cannot effectively handle the rescue situation on their own. only the cooperative work among all the rescue elements may solve it. since most of the rescue tasks involve a certain level of risk for humans  depending on the type of rescue situation   mobile agents can play a major role in rescue situations  especially teams of cooperative heterogeneous mobile agents.
¡¡another crucial aspect of real-world environments is that they are typically only partially observable  due to noisy and inaccurate sensors  or because some relevant parts of the environment simply cannot be sensed. for example  especially in the robotic rescue domain described above  every agent has generally only a very partial view on the environment.
¡¡the practical importance of controlling a system of cooperative agents under partial observability by a generalization of dtgolog has already been recognized in recent works by ferrein et al.  and finzi and lukasiewicz . a drawback of these two works  however  is that they are implicitly centralized by the assumption of a global world model resp. the assumption that every agent knows the belief states  observations  and actions of all the other agents  and so  ferrein et al.  1; finzi and lukasiewicz  1  have no explicit communication between the agents   which is very
often not possible or not desirable in realistic applications.
in this paper  we present the agent programming language teamgolog  which is a novel generalization of dtgolog for controlling a system of cooperative agents under partial observability  which does not have such centralization assumptions. it is thus guided by the idea of truly distributed acting in multi-agent systems with a minimal interaction between the agents. the main contributions are as follows:
  we introduce the agent programming language teamgolog for controlling a system of cooperative  middle-size  agents under partial observability. we define a decision-theoretic semantics of teamgolog  which are inspired by decentralized partially observablemdps  dec-pomdps   nair et al.  1; goldman and zilberstein  1 .
  we introduce the concepts of a synchronization state and a communicationstate  which are used to coordinatethe agents  taking inspiration from artificial social systems  shoham and tennenholtz  1 : the behavior of each agent is encoded in advance in its domain theory and program  and depends on the onlinetrace of synchronizationand communicationstates.
  we define an interpreter for teamgolog  and provide a number of theoretical results around it. in particular  we show that the interpreter generates optimal policies.
  we describe a first prototypeimplementationof the teamgolog interpreter. we also provide experimental results from the rescue simulation domain  which give evidence of the usefulness of our approach in realistic applications.
¡¡notice that detailed proofs of all results in this extended abstract are provided in the full version of this paper.
1 the situation calculus and golog
the situation calculus  mccarthy and hayes  1; reiter  1  is a first-order language for representing dynamic domains. its main ingredients are actions  situations  and fluents. an action is a first-order term of the form  where a is an action name  and u are its arguments. for example  moveto r x y  may represent the action of moving an agent r to the position  x y . a situation is a first-order term encoding a sequence of actions. it is either a constant symbol or of the form do a s   where a is an action and s is a situation. the constant symbol s1 is the initial situation and represents the empty sequence  while do a s  encodes the sequence obtained from executing a after the sequence of s. for example  do moveto r 1  do moveto r 1  s1   stands for executing moveto r 1  after executing moveto r 1  in s1. a fluent represents a world or agent property that may change when executing an action. it is a predicate symbol whose most right argument is a situation. for example  at r x y s  may express that the agent r is at the position  x y  in the situation s. in the situation calculus  a dynamic domain is encoded as a basic action theory at = ¦² ds1  dssa duna dap   where:
  ¦² is the set of foundational axioms for situations.
  duna is the set of unique name axioms for actions  saying that different action terms stand for different actions.
  ds1 is a set of first-order formulas describing the initial state of the domain  represented by s1 . e.g.  at r 1 s1  may express that the agent r is initially at the position  1 .
  dssa is the set of successor state axioms  reiter  1 .
for each fluent  it contains an axiom 
 is a formula with free
variables among. these axioms specify the truth of the fluent f in the next situation do a s  in terms of the current situation s  and are a solution to the frame problem
 for deterministic actions . for example 
	at	 a s   ¡Ô a=	 o x y  ¡Å
may express that the object o is at  x y  in do a s  iff it is moved there in s  or already there and not moved away in s.
  dap is the set of action precondition axioms. for each action a  it contains an axiom   which characterizes the preconditions of a. for example   may ex-
press that it is possible to move the object o to  x y  in s iff no other object o is at  x y  in s.
¡¡golog is an agent programming language that is based on the situation calculus. it allows for constructing complex actions from the primitive actions defined in a basic action theory at  where standard  and not so standard  algol-like constructs can be used  in particular   i  action sequences: p1;p1;  ii  tests: ¦Õ ;  iii  nondeterministic action choices: p1|p1;  iv  nondeterministic choices of action argument: ¦Ðx p x  ; and  v  conditionals  while-loops  and procedures.
1 team golog under partial observability
we now introduce the agent programming language teamgolog  which is a generalization of golog for programming teams of cooperative agent under partial observability.
¡¡our approach is based on the key concepts of a synchronization state and a communication state  which allow the agents to passively resp. actively coordinate their behavior  while keeping their belief states  observations  and activities invisible to the other agents. here  the synchronization state is fully observable by all the agents  but outside their control. the communication state is a multi-dimensional state  containing one dimension for each agent  which is also fully observable by all the agents. but every agent may change its part of the communication state whenever necessary  which encodes its explicit communication to all the other agents.
¡¡since both the synchronization state s and the communication state c are fully observable by all the agents  they can be used to condition and coordinate the behavior of the agents. at the same time  each agent can keep its belief state  observations  and actions invisible to the other agents. we thus realize a maximally distributed acting of the agents. the teamgolog program of each agent encodes the agent's behavior conditioned on s and c  and thus on the current situation. hence  teamgolog programs bear close similarity to social laws in artificial social systems  shoham and tennenholtz  1 . the basic idea behind such systems is to formulate a mechanism  called social law  that minimizes the need for both centralized control and online resolution of conflicts.
¡¡there are many real-world situations where we encounter such a form of coordination. e.g.  the traffic law  right has precedence over left  regulates the order in which cars can pass a street cross. in most cases  this law is sufficient to make the cars pass the street cross without any further interaction between the car drivers. only in exceptional cases  such as the one where on each street a car is approaching the cross or when a car has a technical defect  some additional communication between the car drivers is necessary. similarly  a soccer team can fix in advance the behavior of its team members in certain game situations  such as defense or attack   thus minimizing the explicit communication between the members during the game  which may be observed by the adversary . in these two examples  the synchronization state encodes the situation at the street cross resp. the game situation  while the communication state encodes the explicit communication. the correct behavior of the car drivers resp. soccer players is encoded by traffic laws resp. the strategy fixed by the team in their training units and before the game.
¡¡in the rest of this section  we first define a variant of decpomdps  which underlies the decision-theoretic semantics of teamgolog programs. we then define the domain theory and the syntax of teamgolog programs.
1 weakly correlated dec-pomdps
we consider the following variant of dec-pomdps for n¡Ý1 agents  which essentially consist of a transition function between global states  where every global state consists of a communication state for each agent and a synchronization state  and a pomdp for each agent and each global state  where every agent can also send a message to the others by changing its communication state. a weakly correlated dec-pomdp  i s  ci i¡Êi  p   si i¡Êi   ai i¡Êi   oi i¡Êi   pi i¡Êi   ri i¡Êi  consists of a set of n¡Ý1 agents i ={1 ... n}  a nonempty finite set of synchronization states s  a nonempty finite set of communication states ci for every agent i¡Êi  a transition function p:c ¡Ás ¡ú pd c ¡Ás   which associates with every global state  consisting of a joint communication state c¡Êc = ¡Ái¡Êici and a synchronization state s¡Ês  a probability distribution over c ¡Ás  and for every agent i¡Êi:  i  a nonempty finite set of local states si  a nonempty finite set of actions ai   ii  a nonempty finite set of observations oi   iii  a transition function
pi:c ¡Ás ¡Ási ¡Áai ¡ú pd ci ¡Ási ¡Áoi   which associates with every global state  c s  ¡Ê  c s   local state si ¡Êsi  and action ai ¡Êai a probability distribution over ci ¡Ási ¡Áoi  and  iv  a reward function ri:c ¡Ás ¡Ási ¡Áai ¡ú r  which associates with every global state  c s ¡Êc ¡Ás  local state si ¡Êsi  and action ai ¡Êai a reward ri c s si ai  to agent i.
¡¡the q- and v-functions for agent i¡Êi of a finite-horizon value iteration are defined in fig. 1 for n 1 and m¡Ý1  where is the conditioning of  and  denotes c without. that is  an optimal action of agent i in the global state  c s  and the local state si when there are n steps to go is given by argminai¡Êaiqni  c s si ai . notice that these are the standard definitions of q- and v-functions  adapted to our framework of local and global states.
1 domain theory
teamgolog programs are interpreted relative to a domain theory  which extends a basic action theory by stochastic actions  reward functions  and utility functions. formally 

figure 1: q- and v -functions
a domain theory dti = ati sti oti  consists of n¡Ý1 agents i ={1 ... n}  and for each agent i¡Êi: a basic action theory ati  a stochastic theory sti  and an optimization theory oti  where the latter two are defined below.
¡¡the finite nonempty set of primitive actions a is partitioned into nonempty sets of primitive actions a1 ... an of agents 1 ... n  respectively. we assume a finite nonempty set of observationso  which is partitioned into nonempty sets of observations o1 ... on of agents 1 ... n  respectively.
¡¡a stochastic theory sti for agent i¡Êi is a set of axioms that define stochastic actions for agent i. we represent stochastic actions through a finite set of deterministic actions  as usual  finzi and pirri  1; boutilier et al.  1 . when a stochastic action is executed  then with a certain probability   nature  executes exactly one of its deterministic actions and produces exactly one possible observation. as underlying decision-theoretic semantics  we assume the weakly correlated dec-pomdps of section 1  along with the relational fluents that associate with every situation s a communication state cj of agent j ¡Êi  a synchronization state z  and a local state si of agent i  respectively. the communication and synchronization properties are visible by all the agents  the others are private and hidden. we use the predicate stochastic a s n o ¦Ì  to encode that when executing the stochastic action a in the situation s   nature  chooses the deterministic action n producing the observation o with the probability ¦Ì. here  for every stochastic action a and situation s  the set of all  n o ¦Ì  such that stochastic a s n o ¦Ì  is a probability function on the set of all deterministic components n and observations o of a in s. we also use the notation prob a s n o  to denote the probability ¦Ì such that stochastic a s n o ¦Ì . we assume that a and all its nature choices n have the same preconditions. a stochastic action a is indirectly represented by providing a successor state axiom for every associated nature choice n. the stochastic action a is executable in a situation s with observation o  denoted poss ao s   iff prob a s n o  1 for some n. the optimization theory oti for agent i¡Êi specifies a reward and a utility function for agent i. the former associates with every situation s and action a  a reward to agent i¡Êi  denoted reward i a s . the utility function maps every reward and success probability to a real-valued utility utility v pr . we assume utility v 1 =v and utility v 1 =1 for all v. an example is utility v pr =v ¡¤pr. the utility function suitably mediates between the agent reward and the failure of actions due to unsatisfied preconditions.
example 1  rescue domain  we consider a rescue domain where several autonomous mobile agents have to localize some victims in the environment and report their positions to a remote operator. we assume a team of three heterogeneous agents a1  a1  and a1 endowed with shape recognition  sh   infrared  if   and co1 sensors  respectively. a victim position is communicated to the operator once sensed and analyzed by all the three sensing devices. each agent ai can execute one of the actions gotoi pos   analyzei pos  typei   and reporttoopi pos . the action theory ati is described by the fluents ati pos  s   analyzedi pos  typei  s   and reportedi x  s   which are accessible only by agent ai. e.g.  the successor state axiom for ati pos s  is
ati pos do a s   ¡Ô a=gotoi pos  ¡Å ati pos s  ¡Ä
  posgoto
and the precondition axiom for the action analyzei is given by poss analyzei pos typei  s ¡Ôati pos s . as for the global state  the communication state is defined by the fluent csi data s   where i is the agent  and data is the shared info  e.g.  cs1 atvictim  1  if  s  means that a1 detected a victim in position  1  through the if sensor. other global data are repvictim p   victim reported in position p  and novictim p   position p was inspected and there is no victim . some synchronization states are start s  and reset s  standing for starting resp. resetting the rescue session. in sti  we define the stochastic versions of the actions in ati  e.g.  gotosi pos  and analyzesi pos typei . each of these can fail resulting in an empty action  e.g. 
prob gotosi pos  s gotoi pos  obs succ  =1  prob gotosi pos  s nop obs fail  =1.
in oti  we provide a high reward for a fully analyzed victim correctly reported to the operator  a low reward for the analysis of a detected victim  and a  distance-dependent cost is associated with the action goto. since two agents can obstacle each other when operating in the same location  we penalize the agents analyzing the same victim at the same time. more precisely  we employ the following reward:
reward i a s =r =def  p t a=analyzei p t  ¡Ä  detvictim p s  ¡Ä   conflictsi a s  ¡Ä r =1 ¡Å conflictsi a s  ¡Ä r =1  ¡Å  detvictim p s  ¡Ä r=   1  ¡Å a=reporttoopi p  ¡Ä fullyanalyzed p s  ¡Ä r =1 ¡Å a=goto 
where conflictsi is true if another agent communicates the analysis of the same location in the global state; detvictim p s  is true if at least one agent has discovered a victim in p  i.e.  csi atvictim p t  s  for some i and t; and fullyanalyzed p s  means that all the analysis has been performed  i.e.  cs1 atvictim p sh  s ¡Äcs1 atvictim p  if  s ¡Äcs1 atvictim p co1  s . notice that the action gotoi p  has a cost depending on the distance between starting point and destination  hence  in a greedy policy  the agent should go towards the closest non-analyzed victim and analyze it. however  given the penalty on the conflicts  the agents are encouragedto distribute their analysis on different victims taking into account the decisions of the other agents.
1 belief states
we next introduce belief states over situations for single agents  and define the semantics of actions in terms of transitions between belief states. a belief state b of agent i¡Êi is a set of pairs  s ¦Ì  consisting of an ordinary situation s and a real ¦Ì¡Ê 1  such that  i  all ¦Ì sum up to 1  and  ii  all situations s in b are associated with the same joint communication state and the same synchronization state. informally  every b represents the local belief of agent i¡Êi expressed as a probability distribution over its local states  along with unique joint communication and synchronization states. the probability of a fluent formula ¦Õ s   uniform in s  in the belief state b  denoted ¦Õ b   is the sum of all ¦Ì such that ¦Õ s  is true and  s ¦Ì ¡Êb. in particular  poss a b   where a is an action  is defined as the sum of all ¦Ì such that poss a s  is true and  s ¦Ì ¡Êb  and reward i a b  is defined in a similar way.
¡¡given a deterministic action a and a belief state b of agent i¡Êi  the successor belief state after executing a in b  denoted do a b   is the belief stateposs a  b  | s ¦Ì ¡Êb  poss a s }. furthermore  given a stochastic action a  an observation o of a  and a belief state b of agent i¡Êi  the successor belief state after executing a in b and observing o  denoted do ao b   is the belief state b  is obtained from all pairs  such that  s ¦Ì ¡Êb  poss a s   andprob a s n o  1 by normalizing the probabilities to sum up to 1.
¡¡the probability of making the observationo after executing the stochastic action a in the local belief state b of agent i¡Êi  denoted prob a b o   is defined as the sum of all such that prob a s n o  1.
example 1  rescue domain cont'd  suppose that agent a1 is aware of its initial situation  and thus has the initial belief state { s1  1 }. after executing the stochastic action gotos1  1  and observing its success obs succ   the belief state of a1 then changes to { s1  1    do goto1  1   s1   1 }  here  prob gotos1 pos   s  goto1 pos   obs succ  =1  and gotos1 pos  is always executable .
1 syntax
given the actions specified by a domain theory dti  a program p in teamgolog for agent i¡Êi has one of the following forms  where ¦Õ is a condition  p p1 p1 are programs  and a a1 ... an are actions of agent i :
1. deterministic or stochastic action: a. do a.
1. nondeterministic action choice: choice i:a1|¡¤¡¤¡¤|an . do an optimal action among a1 ... an.
1. test action: ¦Õ . test ¦Õ in the current situation.
1. action sequence: p1;p1. do p1 followed by p1.
1. nondeterministic choice of two programs:  p1 |p1 . do p1 or p1.
1. nondeterministic choice of an argument: ¦Ðx p x  . do any p x .
1. nondeterministic iteration:zero or more times.
1. conditional: if ¦Õ then p1 else p1.
1. while-loop: while ¦Õ do p.
1. procedures  including recursion.
example 1  rescue domain cont'd  the following code represents an incomplete procedure explorei of agent i:
proc explorei 
¦Ðx gotosi x ;
if obs succ  then  analyzesi x typei ; if obs succ ¡Äfullyanalyzed x  then reporttoopi repvictim x    ;
explorei .
here  agent i first has to decide where to go. once the position is reached  agent i analyzes the current location deploying one of its sensing devices. if a victim is detected  then the position of the victim is communicated to the operator.
1 teamgolog interpreter
in this section  we first specify the decision-theoretic semantics of teamgolog programs in terms of an interpreter. we then provide theoretical results about the interpreter.
1 formal specification
we now define the formal semantics of a teamgolog program p for agent i¡Êi relative to a domain theory dt. we associate with every teamgolog program p  belief state b  and horizon h ¡Ý1  an optimal h-step policy ¦Ð along with its expected utility u to agent i¡Êi. intuitively  this h-step policy ¦Ð is obtained from the h-horizon part of p by replacing every nondeterministic action choice by an optimal action.
¡¡formally  given a teamgolog program p for agent i¡Êi relative to a domain theory dt  a horizon h ¡Ý1  and a start belief state b of agent i  we say that ¦Ð is an h-step policy of p in b with expected h-step utility u to agent i iff
dt  and u =utility v pr   where the macro is defined by induction on the different constructs of teamgolog. the definition of g for some of the constructs is given as follows  the complete definition is given in the full version of this paper :   null program  p=nil  or zero horizon  h=1 :
stop.
intuitively  p ends when it is null or at the horizon end.
  stochastic first program action with observation and h 1:
  poss ao b =1 ¡Ä ¦Ð =	 poss ao b  1 ¡Ä
q	q ¦Ð =ao ;for q=1 to l do if oq then ¦Ðq ¡Ä
v =reward prob ao b oq  ¡Ä pr =possprob ao b oq   .
here    f  is obtained from f by existentially quantifying all free variables in f. moreover  o1 ... ol are the different pairs of a joint communication state and a synchronization state that are compatible with ao  and prob ao b oq  is the probability of arriving in such oq after executing ao in b. informally  suppose  where ao is a stochastic action with observation. if ao is not executable in b  then p has only the policy ¦Ð =stop along with the expected reward v =1 and the success probability pr =1. otherwise  the optimal execution of  in b depends on that one of p in do ao b .
  stochastic first program action and h 1:
def
q
 ¦Ð =aoq;for q =1 to l do if oq then ¦Ðq ¡Ä  prob a b oq  ¡Ä pr  prob a b oq  .
here  o1 ... ol are the possible observations of the stochastic action a. the generated policy is a conditional plan in which every such observation oq is considered.   nondeterministic first program action and h 1:
g  def
  q=1   aq ;p   q ;¦Ðq  vq prq   ¡Ä k = argmaxq¡Ê{1 ... n} utility vq prq  ¡Ä ¦Ð =¦Ðk ¡Ä v=vk ¡Ä pr =prk .
1 theoretical results
the following result shows that the teamgolog interpreter indeed generates an optimal h-step policy ¦Ð along with its expected utility u to agent i¡Êi for a given teamgolog program p  belief state b  and horizon h ¡Ý1.
theorem 1 let p be a teamgolog program for agent i¡Êi w.r.t. a domain theory dti  let b be a belief state  and let h ¡Ý1 be a horizon. then  the optimal h-step policy ¦Ð of p in b along with its expected utility u to agent i¡Êi is given  and u =utility v pr .
¡¡the next result gives an upper bound for the number of leaves in the evaluation tree  which is polynomial when the horizon is bounded by a constant. here  n is the maximum among the maximum number of actions in nondeterministic action choices  the maximum number of observations after actions  the maximum number of arguments in nondeterministic choices of an argument  and the number of pairs consisting of a synchronization state and a communication state.
theorem 1 let p be a teamgolog program for agent i¡Êi w.r.t. a domain theory dti  let b be a belief state  and let h ¡Ý1 be a horizon. then  computing the h-step policy ¦Ð of p in b along with its expected utility u to agent i¡Êi via g generates o n1h  leaves in the evaluation tree.
1 rescue scenario
consider the rescue scenario in fig. 1. we assume that three victims have already been detected in the environment  but not completely analyzed: in position  1   the presence of alice was detected by a1 through the sh sensor; in position  1   agent a1 discovered bob through if  and a1 analyzed him through the co1 sensor; finally  in position  1   victim carol was detected by a1 with if. we assume that this information is available in the global state  that is  the properties cs1 atvictim  1   sh   s   cs1 atvictim  1   if   s   cs1 atvictim  1   co1   s   and cs1 atvictim  1   if   s  hold in the communication state of the agents. as for the local state  we assume the belief states b1 ={ s1  1    s1  1 }  b1 ={ s1  1 }  and b1 ={ s1  1 }  with at1  1  s1   at1  1  s1   at1  1  s1   and at1  1  s1 .

figure 1: rescue scenario
¡¡given this situation  the task of the team of agents is to fully analyze the discovered victims and report their positions to the operator once the victim analysis is completed. this task can be encoded by the following procedure:
proc explorei 
¦Ðx¡Ê{ 1   1   1 } gotosi x ; if obs succ  then  analyzesi x typei ; if obs succ ¡Äfullyanalyzed x  then
reporttoopi repvictim x    ;
explorei  
where type1 =sh  type1 =if  and type1 =co1. every agent ai with i¡Ê{1 1} has to separately compile the procedure explorei using its global and local information. assuming the horizon h =1 and the initial belief state bi  the optimal 1-step policy ¦Ði for agent ai  produced by the team-
golog interpreter  is such that dti |=g  explorei; nil  bi 
. here  ¦Ð1  ¦Ð1  and ¦Ð1 are complexconditional plans branching over all possible observations and global states. e.g.  the beginning of ¦Ð1 is as follows:
gotos1 1 ; if obs succ  then  analyzes1  1   sh ;
if obs succ ¡Äfullyanalyzed 1  then reporttoop1 repvictim 1   ;
gotos1 1 ;...
this scenario has been realized in an abstract simulator. the simulator captures the key features of the environmentand allows to execute agent actions computing associated rewards. a greedy control strategy has been devised as a comparison with the derived policies ¦Ði. in the greedy strategy  at each time step  each agent searches for the best victim to analyze  based on the current distance to the victim. whenever a victim has been completely analyzed  the agent reports the victim state to the operator. both the greedy strategy and the policies ¦Ði were able to correctly report all victims to the operator  however  the policies ¦Ði revealed to be superior with respect to the greedy strategy gaining more reward. the greedy gain is around 1% of the policies ¦Ði  because these were able to minimize conflicts among team members. notice that  information available to the two strategies are the same  and that the better performance of the policies ¦Ði are achieved using planning over the global and local states.
1 summary and outlook
we have presented the agent programming language team-
golog for programming a team of cooperative agents under partial observability. the approach is based on a decisiontheoretic semantics and the key concepts of a synchronization state and a communication state  which allow the agents to passively resp. actively coordinate their behavior  while keeping their belief states  observations  and activities invisible to the other agents. we have also provided experimental results from the rescue simulation domain.
¡¡an interesting topic for future research is to develop an adaptive version of this approach. another topic for future work is to explore whether the approach can be generalized to multi-agent systems with competitive agents.
acknowledgments. this work has been partially supported by the austrian science fund project p1-n1 and by a heisenberg professorship of the german research foundation  dfg . we thank the reviewers for their constructive comments  which helped to improve this work.
