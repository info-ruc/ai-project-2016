 
this paper investigates a new approach for training discriminant classifiers when only a small set of labeled data is available together with a large set of unlabeled data. this algorithm optimizes the classification maximum likelihood of a set of labeledunlabeled data  using a variant form of the classification expectation maximization  cem  algorithm. its originality is that it makes use of both unlabeled data and of a probabilistic misclassification model for these data. the parameters of the labelerror model are learned together with the classifier parameters. we demonstrate the effectiveness of the approach on four data-sets and show the advantages of this method over a previously developed semi-supervised algorithm which does not consider imperfections in the labeling process. 
1 	introduction 
in many real-life applications  labeling training data for learning is costly  sometimes not realistic and often prone to error. for example  for many rapidly evolving data bases available via the web  there is not enough time to label data for different information needs. in some cases  like medical diagnosis or biological data analysis  labeling data may require very expensive tests so that only small labeled data sets may be available. in other cases  like object identification in images  noise is inherent in the labeling process. 
the statistician and pattern recognition communities were the first to consider the problem of forming discriminant rules using either partially classified or random misclassified training data in order to cope with this type of situation. more recently this idea has motivated the interest of the machine learning community and many papers now deal with this subject. the use of partially classified data for training  known as semi-supervised learning  has been the subject of intense studies since 1 and more recently there has been a resurgence of interest for training with misclassified data also called learning in presence of label noise. 
we consider here semi-supervised learning for classification. 
most approaches to this problem make use of a mixture density model where mixture components are identified as classes. labeled data are known to belong to exactly one mixture component whereas unlabeled data may belong to any components. using the expectation maximization  em  algorithm  dempster et al.  1   proposed approaches usually attempt to optimize the likelihood of the whole labeledunlabeled data. starting from an initial labeling  these approaches proceed by computing at each e-step  tentative labels for unlabeled data using the current parameters of the model and update in the m-step  these parameters using the estimated labels. our departure point here is the work of  amini and gallinari-a  1  who proposed a semisupervised discriminant algorithm using a variant form of the cem algorithm  celeux and govaert  1 . discriminative approaches which attempt to estimate directly posterior class probabilities are often considered superior to generative models which compute these posteriors after learning class conditional densities. tests on different datasets in i amini and gallinari-/   1  led to the same conclusion for semisupervised learning. like for other methods  at each step of the algorithm  the model computes tentative labels for unlabeled data. we extend here the system by incorporating a model which takes into account label errors. this provides an unifying framework for semi-supervised learning and learning with label noise. to our knowledge  this form of model has not been studied yet. we detail the algorithm for the case of a logistic classifier and give a convergence proof for the general case. we then show experimentally that modeling the stochastic labeling noise  increases notably the performance  especially when only small labeled datasets are available. this paper is organized as follows; we first make a brief review of work on semi-supervised learning and learning in the presence of label noise  section 1 . in section 1 we present the formal framework of our model and describe in section 1 the semi-supervised approach we propose. finally we present a series of experiments on four data sets. 
1 related work 
1 learning with both labeled and unlabeled data 
learning 	1 the idea of using partially labeled data for learning started in the statistician community at the end of 1 s. the seminal paper by  day  1  presents an iterative em-like approach for learning the parameters of a mixture density under the assumption of multivariate normal components with a common covariance matrix for two group-conditional distributions. 
other iterative algorithms for building maximum likelihood classifiers from labeled and unlabeled data based on the same type of assumption followed  o'neill  1   mclachlan and ganesalingam  1 . some other authors have suggested updating procedures for no-normal group conditional densities using for example kernel methods for modeling mixture components imurray and titterington  1 . there has been considerably fewer work on discriminative approaches. in his fundamental paper on logistic regression  anderson suggests modifying a logistic regression classifier to incorporate unlabeled data in order to maximize the likelihood function  anderson  1 . 
the semi-supervised paradigm has been recently rediscovered by the machine learning community. most papers propose mixture density models for combining labeled and unlabeled data for classification.  miller and uyar  1  consider a mixture density model where each class is described by several component densities.  roth and steinhage  1  propose kernel discriminant analysis as an extension to classical linear discriminant analysis. this framework can be used also for semi-supervised learning. fnigam et al.  1  propose a semi-supervised em algorithm which is essentially similar to the one in  mclachlan  1  but makes use of naive bayes estimator for modeling the different densities. they present empirical evaluation for text classification tasks. some authors make use of discriminant classifiers instead of modeling conditional densities. for example  joachims  1  propose a transductive support vector machine which finds parameters for a linear separator using both the labeled data in the training set and the current test data whose class is unknown. iblum and mitchell  1  introduce the co-training paradigm where each sample x is supposed to be described by two modalities. two classifiers are then used  one for each modality  operating alternatively as teacher and student. this framework can be used for unsupervised and semi-supervised learning. based on the multi-modal framework introduced by  blum and mitchell  1    muslea et al.  1  propose to combine both active and semi-supervised learning. since that  different authors have proposed semi-supervised learning schemes  they usually follow one of the above ideas. 
1 learning with imperfectly labeled data 
practical applications of pattern recognition  like eg image classification problems  have motivated in the early 1 s some work on the problem of learning in presence of mislabeled data for fully supervised learning.  chittineni  1  obtained error bounds on the performance of the bayes and nearest neighbor classifiers with imperfect labels.  krishnan  1  considered a 1-class classification problem for two group multivariate normal mixture when training samples are subject to random misclassification and derived the likelihood estimation of parameters.  titterington  1  proposed a logistic-normal distribution model and worked out an em algorithm for the estimation of its parameters. more recently   lawrence and scholkopf  1  proposed an algorithm for constructing a kernel fisher discriminant from training examples in the presence of label noise. 
1 a semi-supervised probabilistic model for mislabeling 
we consider here the problem of semi-supervised learning. we start from the logistic-cem algorithm described in  amini and gallinari-a  1 . it is a generic scheme in the sense that it can be used with any discriminant classifier estimating the posterior class probabilities. the classifier is first trained on labeled data  it then alternates two steps until convergence to a local maximum of the classification maximum likelihood  cml  criterion  symons  1 . unlabeled data are first labeled using the output of the current classifier. classifier parameters are then learned by maximizing the cml function computed using the known labels of labeled data and the current estimated labels for unlabeled data. in this algorithm  at each iteration  labels computed for unlabeled data are considered as desired outputs. 
we suppose here that labels from the labeled dataset are correct and that at each step of this algorithm  labels computed for unlabeled data are subject to error. we propose to model the imperfection of these labels using a probabilistic formalism and to learn the semi-supervised classifier by taking into account its labeling errors according to this error model. parameters of the error model and of the classifier will be learned simultaneously in this new algorithm. compared to the baseline algorithm  amini and gallinari-a  1   we explicitly take into account the fact that the current classifier is not optimally trained at each step  since many data labels are missing and are only estimated. 
in the following we present the general framework and the learning criterion of our model. 
1 	general framework 
we suppose that each example belongs to one and only one class and that there are available a set of n labeled examples  d1 and a set of m unlabeled examples  du. a discriminant classifier is to be trained on the basis of these n + ra  d-dimensional feature v e c t o r s 1 . for each labeled example xt in / /  let ct and be respectively the class label and the indicator vector class associated to xt : 

we suppose that for each unlabeled example xi in du  there exists a perfect and an imperfect label respectively denoted ci and ci. we propose to model the imperfections in the labels by the following probabilities: 
	 1  which are subject to the constraint: 
		 1  
in order to simplify the presentation  we consider in the following a two-class classification problem. 

this is not restrictive since we can easily extend the analysis to multi-class cases. 
'components of x could also be discrete 

1 	learning 

1 updating a discriminant function on the basis of labeled and imperfect labeled data 
we now present an iterative discriminant cem algorithm for 

this writing makes apparent the posterior probabilities which are directly estimated in their algorithm instead of the conditional densities in  mclachlan  1 . as no assumptions are made on the distributional nature of data  maximizing lc is equivalent to the maximization of l'c   mclachlan  1   page 1 . 
	n	'	1 
		 1  
 let us now introduce our misclassifiction model. for that  we will express the i in the second summation in  1  as a function of the mislabeling probabilities and of the posterior probability of correct labels . consider 
conditional class probabilities: 
i 
following  chittineni  1  we make the assumption that the density of an example  given its true label  does not depend on its imperfect label: 

learning 
learning a classifier for semi-supervised learning  which incorporates the mislabeling error model. the training criterion is  1 . for simplification  we consider a simple logistic classifier  anderson  1   but the algorithm can be easily adapted 
for training any discriminant classifier. consider i 	a logistic 
classifier with parameters 	the output of 
for 	i	n	p	u	t	.	. 	after 
have been learned  are respectively used t o e s t i m a t e b e the 
current partition for the unlabeled data  the parameters for the misclassification model and the logistic classifier at iteration p of the algorithm. the learning criterion  1  is a function of . an iterative approach is then adopted for its maximization  algorithm 1 . parameters  are first initialized by training the classifier on the labeled dataset d1. two steps are then iterated until the convergence of criterion l'c. in the first step  the classifier is considered as an imperfect supervisor for the unlabeled data. its outputs  g x  and 1 - g x   are used to compute the posterior imperfect class probabilities p c - k   x  for each x in du and   x is then labeled according to the maximum imperfect output. in the second step  the parameters of the error model and of the classifier are updated using the imperfect labels obtained in the previous step as well as the labeled data. we adopted in this step  a gradient algorithm to maximize  1 . an advantage of this method is that it only requires the first order derivatives at each iteration. in the following lemma  we provide a proof of convergence of the algorithm to a local maximum of the likelihood function for semi-supervised training. 
1 

1 	evaluation measures 

tionary value. ¡ö 
in the following section we will present results on four datasets  using a baseline logistic classifier trained with the updating scheme presented above. 
1 experiments 
1 data sets 
in our experiments we used the spambase  credit screening and mushroom collections from the uci repository1 as well as the computation and language  cmp.lg  collection of tipster summac1 for text summarization. table 1 summarizes the characteristics of these datasets. we removed 1 samples with missing attributes from the credit data set. for summarization  we 
1 ftp://ftp.icss.uci.edu/pub/machine-leaming-databases/ for the three uci datasets there is approximately the same proportion of examples for each class  table 1 . we used as performance criterion the percentage of good classification  pgc  defined as: 
for text summarization  we followed the summac evaluation by using a 1% compression ratio. hence  for each document in the test set we have formed its summary by selecting the top 1% sentences having higher score with respect to the output of the classifier. for evaluation we compared these extractive sentences with the desired summary of each document. the desired extractive summaries were generated from the abstract of each article using the text-span alignment method described in  banko et al.  1 . since the 

1
 http://www.itl. nist.gov/iaui/1/related-projccts/tipster summac/ collection is not well balanced between positive and negative 
1 	learning 


figure 1: performance curves for the 1 datasets showing  for a classical logistic classifier trained in a fully supervised scheme  square   the baseline semi-supervised logistic-cem algorithm  circle  and our semi-supervised algorithm with label imperfections  star . each point represents the mean performance for 1 arbitrary runs. the error bars show standard deviations for 

the estimated performance. 
examples  pgc is meaningless  we used the average precision  ap  measure for the evaluation. let be the number of sentences extracted by the system which arc in the target summaries  and  be the total number of sentences extracted by the system. the precision is defined as the ratio 
1 	results 
for each dataset and each cross validation  1% of examples are held aside as a test set. we vary the percentage of labeledunlabeled data in the remaining 1% training set of the collections. figure 1  shows the performance on the test sets for the four datasets as a function of the proportion of labeled data in the training set. on the x-axis  1% means that 1% of data in the training set were labeled for training  the 1% remaining being used as unlabeled training data. each experiment was carried on twenty paired trials of randomly selected trainingtest splits. on the y-axis  each point represents the mean performance for the 1 runs and the error bars correspond to the standard deviation for the estimated performance  tibshirani  1 . 
all figures exhibit the same behavior. in all four datasets  semi-supervised algorithms are over the baseline classifier trained only on the labeled training data. for example  if we consider text summarization  using only 1% of labeled sentences  our algorithm allows to increase performance by 1% compared to a fully supervised algorithm trained on the 1%. 1% labeled data are needed to reach the same performance with the baseline method. our model is uniformly better than the two reference models used in our experiments. it provides an important performance increase especially when there are only few labeled data available which is the most interesting situation in semi-supervised learning. for the credit card problem  the dataset is small and semi-supervised learning allows for a smaller increase of performance than for other datasets where there are a lot of unlabeled data available. semi-supervised learning allows to reach 'optimal' performance with about 1% of labeled data  only 1% for the mushroom set . 

learning 	1 

1 conclusion 
we have described how to incorporate a label error model into an iterative semi-supervised discriminant algorithm. we have detailed a version of this general algorithm in the case of a simple logistic classifier  shown its convergence and proved empirically its efficiency on four datasets with different characteristics. the algorithm allows for an important performance increase compared to a reference efficient semi-supervised algorithm without mislabeling model. the main contribution of the paper is to provide a general framework for handling simultaneously semi-supervised learning and learning in the presence of label noise. the noise model we have used is simple and allows for efficient estimations in the semi-supervised setting. more sophisticated models have still to be investigated. however  our experience is that simple models do often perform better when only few labeled data are available. 
