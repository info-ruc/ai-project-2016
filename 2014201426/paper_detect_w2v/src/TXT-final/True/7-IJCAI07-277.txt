
this paper addresses the task of aligning a database with a corresponding text. the goal is to link individual database entries with sentences that verbalize the same information. by providing explicit semantics-to-text links  these alignments can aid the training of natural language generation and information extraction systems. beyond these pragmatic benefits  the alignment problem is appealing from a modeling perspective: the mappings between database entries and text sentences exhibit rich structural dependencies  unique to this task. thus  the key challenge is to make use of as many global dependencies as possible without sacrificing tractability. to this end  we cast text-database alignment as a structured multilabel classification task where each sentence is labeled with a subset of matching database entries. in contrast to existing multilabel classifiers  our approach operates over arbitrary global features of inputs and proposed labels. we compare our model with a baseline classifier that makes locally optimal decisions. our results show that the proposed model yields a 1% relative reduction in error  and compares favorably with human performance.
1 introduction
uncovering the mapping between text and its underlying semantic representation lies at the core of natural language processing. for instance  the goal of information extraction is to structurally represent the semantic content of a given text. natural language generation  on the other hand  seeks to create such texts from structured  non-linguistic representations. training both information extraction and natural language generation systems generally requires annotated data that specifies the mapping between structured representations and corresponding fragments of text. creating these mappings by hand is prohibitively expensive.
¡¡an alternative source of data for learning text-to-semantics mappings are parallel collections of databases and their corresponding texts. such parallel collections are abundant and are readily available in multiple domains  including terrorism  sports  finance  and weather. however  text-database pairs cannot be used directly for system training without more refined alignment of their components. such an alignment should explicitly link database entries to the sentences that verbalize their content.
¡¡in this paper  we investigatea supervisedlearning approach to aligning database entries and sentences. for example  consider a database containing statistics on an american football game and the corresponding newspaper summary. figure 1 shows an excerpt from such a database and several summary sentences along with the target alignment.
¡¡text-database alignment differs in several important respects from word alignment in machine translation. first  the databases and texts are not exactly parallel: many database entries may not be verbalized in a text  and conversely  some sentences may contain information not found in a database. second  the number of items to be aligned is greater than the number of words in translated sentences. a typical database may contain hundreds of entries compared to an average of 1 words in a newspaper sentence. finally  the database and text units to be aligned exhibit rich internal structure and complex interdependencies. thus  the database schema provides a key for mining semantic relations between entries and corresponding sentences. for instance  in figure 1 we can see that the three entries aligned to the final sentence bear a strong relationship to one another: the interception leads to a drive which culminates in a touchdown run. this relationship and many others like it can be determined by examining the database structure.
¡¡one possible approach is to formulate semantics-to-text alignment as simple binary classification of sentence-entry pairs. by considering the content overlap between a database entry and a sentence  the classifier determines whether they are aligned. this approach  however  fails to capture dependencies between local alignment decisions such as the one we just saw. local alignments could also lead to overmatching when a sentence contains a single anchor that locally matches multiple database entries. for instance  the final sentence in figure 1 contains the name  brown  and the number  1.  besides matching an entry in the play-by-play table  these anchors also match the second entry in the fumbles table.1 by making independent decisions for each of these entries  a lo-

figure 1: sample target alignment between the nfl database and summary text.
cal classifier may erroneously align them both to the same sentence.
¡¡to capture global features of alignments  we cast our problem as a structured multilabel classification task. in the multilabel framework  each instance can be assigned any subset of the labels. in our case  we treat the sentences as instances and label them with the subset of database entries that match in content. by jointly considering a sentence with an entire set of candidate entries  our method ensures the global coherence of the aligned set. moreover  we guide the alignment towards sets of entries with favorable structural properties. this task is accomplished by using a model that operates over arbitrary global features of the sentence and proposed entry set. however  the number of potential entry sets per sentence is exponential  and therefore this feature space cannot be searched exhaustively. to focus on promising label sets  we first rank all the database entries based on their individual match to the input sentence. we then only consider entry sets formed by choosing a cut-off in this ranking. our model then selects among the remaining sets by examining their global alignment features. although the target entry set may not be among these candidates  we train our model to choose the set closest to the target in hamming distance.
¡¡we evaluate our algorithm on a corpus of manually aligned text-database pairs in the sports domain. to assess the contribution of our global model  we compare our method with a local classifier which treats every sentence-entry pair independently. our method achieves a 1% relative reduction in error over this baseline  demonstrating the contribution of global features in our task.
1 related work
multilabel classification a typical multilabel classifier has two stages: first  the labels of each input are ranked by a local scoring function. then  a regression model predicts a threshold as a function of the label ranking scores. finally  all the labels above the predicted threshold are returned.
research has focused on developing local ranking models aiming to minimize the number of incorrect labels predicted above correct ones  often ignoring threshold selection or treating it as a secondary concern  elisseeff and weston  1; crammer and singer  1; mcdonald et al.  1; schapire and singer  1 .
¡¡our approach shares the initial local ranking step with traditional multilabel classifiers. however  in a task with structured labels  where the set of correct labels exhibit compatibility properties  it is insufficient to choose a cutoff by merely looking at the local ranking scores. therefore  we treat ranking as a mere pruning step and learn to make intelligent threshold selections based on ranking scores as well as relations between entries. by considering collective properties of various label sets  we can mitigate ranking errors.
¡¡ghamrawi and mccallum  propose an approach to multilabel classification using conditional random fields. their approach models co-occurrence dependencies between pairs of class labels and individual input features. our approach  on the other hand  learns weights for features defined jointly on the input and sets of  structured  labels. also  our global feature function is not constrained to examining pairs of labels  but instead operates over arbitrary label sets.
¡¡alignment the problem of word alignment of parallel bilingual corpora has been extensively studied in machine translation. while most of the research focuses on unsupervised models  brown et al.  1   recently a number of supervised discriminative approaches have been proposed  taskar et al.  1; lacoste-julien et al.  1 . taskar et al.  cast alignment as a graph matching problem and solve it in an optimization framework. their model finds the global alignment which maximizes word-pair alignment scores  subject to a hard fertility constraint. recently  they have reformulated the optimization problem to instead apply soft constraints on alignment fertility as well as control for other properties specific to word alignment  lacostejulien et al.  1 .
¡¡our model differs in that it can consider arbitrary features of the alignment decisions. for instance  depending on the structural properties of the aligned objects  we often wish to encourage  rather than penalize  additional alignments.
1 problem formulation
in an alignment problem each input consists of a set of x variables l and a set of y variables r. for each input  l r  there is a target alignment consisting of edges between individual x and individual y variables: a   l ¡Á r. note that an individual x or y variable may occur many times in such an alignment.
¡¡in a structured alignment problem  rather than taking on categorical or numerical values  each x variable encodes an element from a set of permissible structures x  and each y value similarly represents a structure from y. for example  in our case the x variables are sentences in a text  and the y variables encode entries in a relational database. thus  an input  l r  represents a text-database pair for a particular nfl football game  and the target alignment a is the set of all sentence-entry pairs referring to the same fact or event.
¡¡given a set of inputs and their true alignments  our goal is to learn a mapping from inputs to alignments which minimizes the errors in the predicted alignments.
1 modeling assumptions
unfortunately  for a particular input  l r  with |l| = m and |r| = n  there are 1mn possible alignments. for instance  in our scenario  where  l r  is a text-database pair with around 1 sentences and 1 database entries  it is infeasible to search through this entire space. however  if some local decisions in the alignments are assumed to be independent of others  we can learn a model which makes optimal partial alignment decisions and then combines them into a full alignment. next we describe several degrees of independence between local alignment decisions that might occur.
¡¡full independence in the case of full independence  the likelihood of any pair  x y  ¡Ê l ¡Á r being aligned is independent of the alignment of any pair when either. assuming this complete independence  we can train a local binary classifier using the m ¡Á n  x y  pairs from each of our inputs.
¡¡partial independence a weaker  asymmetric independence might occur in one of two ways. consider any two pairs  x y  ¡Ê l ¡Á r and . if the alignment decisions for these pairs are independent only if then we say that the alignments are left-side partially independent. if  on the other hand  the alignment decisions are independent only if  then the alignments are right-side partially independent.
¡¡to illustrate these ideas  consider their application to our task of text-database alignment. left-side partial independencewould occurif the set of entries verbalizedin a sentence is independent of those chosen for any other sentence. but at the same time  the entries to be verbalized in a particular sentence are chosen jointly and dependently - if  for example  a sentence mentioning a score is more likely now to mention a play which led to the score. we could thus train a model to choose the optimal aligned set of entries individually for each sentence.
1 algorithm
to strike an appropriate balance between tractability and model richness  we base our text-database alignment method on a partial independence assumption. in particular  we assume that alignment decisions for each sentence are independent of other alignment decisions. at the same time  we allow dependencies within the set of database entries aligned to a sentence. following the terminology introduced in section 1  this model makes a left-side partial independence assumption. we believe that this is a natural choice for this task because a typical sentence conveys several facts that are semantically related in a meaningful way. when these facts are represented in a database  we can capture their inter-relations by examining the structural links of the corresponding database entries.
¡¡as a result of this independence assumption  we can train a model to make optimal alignment decisions separately for each sentence. in other words  each sentence is an independent instance that must be labeled with a subset of database entries. in essence  by treating entries as structured labels of sentences  we reformulate alignment as a structured multilabel classification task. the key advantage of our approach lies in its ability to consider arbitrary global features of each sentence and its proposed label set. however  since we cannot possibly examine every subset of labels  we need to intelligently prune the decision space. this pruning is driven by local alignments: for each sentence  all the database entries are ranked by their individual alignment likelihood. we then only consider sets of entries consistent with the ranking. that is  for every predicted label  the labels rankedaboveit must be predicted as well. now that the number of considered candidates is linear in the number of entries  we can train a model to choose the one with optimal global characteristics. the latter stage of this approach is similar in spirit to global parse reranking  collins and koo  1 . however  our approach constrains the decision space through local ranking of labels rather than an n-best list produced by a generative model.
¡¡below we formally describe the training and decoding procedures for text-database alignment.
1 model structure
the model takes as an inputa set of sentences l with |l| = m and a set of database entries r with |r| = n. each sentenceentry pair  x y  ¡Ê l ¡Á r is represented as a local feature vector ¦µx y x y . each component of this vector is a binary feature. for instance  the ith component of this vector may indicate whether the sentence and entry contain the same number:
	1	if x and y contain the same number
 ¦µx y 	  i =
	1	otherwise
¡¡in addition  our model captures dependencies among multiple entries aligned to the same sentence. we represent a sentence x ¡Ê l paired with a set of candidate entries y   r with the global feature vector ¦µx x y  . the components here are also binary features. for example  the ith may indicate whether all the entries in y match the same name in the sentence:
contain the same name
	 ¦µx 	  i =
1
our model predicts alignments in two basic steps.
step one the goal of this step is to prune the decision space by inducing a ranking of database entries for each sentence x ¡Ê l. to do so  for each each entry y ¡Ê r  the model maps the feature vector ¦µx y x y  to a real number. this mapping can take the form of a linear model 
¦µx y x y  ¡¤ ¦Á1
where ¦Á1 is the local parameter vector.
we then order the entries in r: y1 y1 y1 ... yn such that ¦µx y x yi  ¡¤ ¦Á1 ¡Ý ¦µx y x yi+1  ¡¤ ¦Á1  i.
step two given a particular ranking of entries y1 y1 ... yn  for each i ¡Ê 1 ... n  we define the set of i initial elements yi = {y1 ... yi}. the goal of this step is to choose a cut-off i  and return yi  as the aligned set of entries. to do so  our model computes a score for each i ¡Ê 1 ... n based on the global feature vector ¦µx x yi 
¦µx x yi  ¡¤ ¦Á1
where ¦Á1 is the global parameter vector. the final output of the model is yi   for i  = argmaxi ¦µx x yi  ¡¤ ¦Á1.
1 training the model
we are given a training corpus with text-databasepairs  l r  and alignments a   l ¡Á r.
training for step one for the label ranking model ¦µx y x y  ¡¤ ¦Á1  the scores returned by a classifier or ranking model may be used. standard training techniques can be applied to learn this model. in fact  we employed an svm classifier with a quadratic kernel as it yielded the best results for our task.
training for step two the global model ¦µx x yi  ¡¤ ¦Á1 is trained using the label ranking returned by the local model. for each sentence x ¡Ê l it considers all entry sets y1 ... yn formed by varying the cut-off point of the initial ranking. however  the true alignment may not be among these sets: the ranking may have incorrectly placed an unaligned entry above an aligned entry  in effect pruning away the true target. therefore  to identify a new target alignment set yi   we need to identify the closest remaining entry set.
as our similarity metric between two sets a and b  we use the hamming distance h a b   defined as the cardinality of the symmetric difference of a and b . for each input  l r   and each sentence x ¡Ê l  we identify the cut-off in the ranking which produces the entry set closest in hamming distance to the true label set t:
i  = argminh yi t 
i
this process of identifying a training target for the global model is illustrated in table 1. once all the training targets in the corpus have been identified through this procedure  the global linear model is learned using a variant of the perceptron algorithm  collins and duffy  1 . this algorithm encourages a setting of the parameter vector ¦Á1 that will assign the highest score to the global feature vector associated with the optimal cut-off i .
¦Á1 ¡û 1
for each  l r  and each sentence x ¡Ê l:
imax ¡û argmaxi ¦µx x yi  ¡¤ ¦Á1 if
¡û¦µx x {} h = 1-y1¡û¦µx x {y1} h = 1+y1¡û¦µx x {y1 y1} h = 1+y1¡û  ¦µx x {y1 y1 y1} h = 1-y1¡û¦µx x {y1 y1 y1 y1} h = 1table 1: to select the training target for the global model  we select the cut-off i  which minimizes the hamming distance h to the true label set.  +  and  -  indicate whether a label is in the true label set or not. in this example  i  = 1  since the set {y1 y1 y1} has the minimum hamming distance.
1 features
in this section we describe local and global features used by our model.
local features
we assume that a high overlap in names and numbers increases the likelihood that a given sentence-entry pair is aligned. thus  our local alignment is driven by anchor-based matching. a feature vector generated for each sentence-entry pair captures various statistics about name and number overlap. to further refine the matching process  these features also represent the unigrams and bigrams surrounding each matched name and number. in addition  the feature vector includes the type of database entry as certain entry types  i.e.  scoring summary  are more commonly verbalized in text. close to 1 local features are generated. see table 1 for a list of local feature templates.
global features
our global model jointly considers a sentence with an entire set of candidate entries  seeking alignments with favorable structural properties. we select global features that express these properties  thereby allowing our model to correct local decisions. two simple global features that we used are the numberof aligned entries and the hamming distance between aligned entries and the set predicted by a local model. we can group the remaining global features into three classes based on the type of dependency they capture:
  co-occurrence of entry types a simple analysis of the aligned corpus reveals that certain entry types tend to be aligned together to the same sentence. for instance  entries from the play-by-play table are often verbalized together with entries from the scoring-summary table. to model this property  we introduce features for each commonly occurring set of entry types. these features parallel the label-pair co-occurrencesmodeled in ghamrawi and mccallum   but are not limited to pairs.
  local match overlap a common error of local alignments is overmatching. a number that appears once in a sentence may be erroneously matched to multiple database entries during local alignment. for instance  consider the example in figure 1: the number 1 in the sentence causes an overmatch with two database entries that contain this number. by examining the entire set
namematch= name category in entry lex= bigrams 
nummatch= num category in entry lex= bigrams 
entry type= entry type 
count nummatches= count of num matches 
count namematches= count of name matches 
%nums= percent of nums in entry matched 
%names= percent of names in entry matched table 1: local feature templates. text enclosed by brackets is replaced as appropriate. also  various backoffs are used  such as using unigrams or no lexicalization at all in the first two features.
count= number of entries in set 
diff= hamming distance to local prediction 
entry types= combination of entry types 
plays alone= plays w/o corresponding scores 
scores alone= scores w/o corresponding plays 
pairs= corresponding play/score pairs 
matched drive= drive with its plays or score 
unmatched drive= drive with other plays or scores 
matched1 entries= # entries matched by num 
matched1 entries= # entries matched by name 
share1 entries= # entries sharing nums  share1 entries= # entries sharing name 
unmatched1 entries= # entries unmatched by num 
unmatched1 entries= # entries unmatched by name 
matched nums= # of matched nums in sentence 
shared nums= # of shared nums in sentence 
matched names= # of matched names in sentence 
shared names= # of shared names in sentence table 1: global feature templates.
of proposed entries simultaneously  we can separately count the entries that match distinct sentence anchors as well as those that share anchors. the latter number indicates the degree of overmatching and our model should learn to discourage such erroneous alignments.
  domain-specific semantic constraints besides these generic global features  we further refine the alignments using the database semantics defined by its schema. we can express a semantic relation between entries by analyzing their structural links in the database. for example  when summarizing the result of a drive  any play that occurred during the drive is likely to be mentioned as well. while the type co-occurrence feature described above would capture the correlation between these two entry types  it would fail to distinguish this case from semantically unrelated drives and plays. we engineered several domain-specific features of this variety.
¡¡overall  1 global features were generated. see table 1 for a complete list of the global feature templates.
precisionrecallf-measuresvm baseline1%1%1%multilabel global1%1%1%threshold oracle1%1%1%multilabel regression1%1%1%graph matching1%1%1%table 1: 1-fold cross validation results for the baseline  our global multilabel model  an oracle  and two other approaches.
1 evaluation setup
corpus for our evaluation  we used the nfl football corpus previously used by barzilay and lapata . this corpus contains text-database pairs for 1 football games played over the 1 and 1 seasons. the database contains a wealth of statistics describing the performance of individual players and their teams. it includes a scoring summary and a play-by-play summary giving details of the most important events in the game together with temporal  i.e.  time remaining  and positional  i.e.  location in the field  information. this information is organized into several table types including aggregate statistics  scoring summary  team comparison  drivechart  and playbyplay. each game is typically represented by 1 database entries. the accompanying texts are written by associated press journalists and typically contain 1 sentences.
¡¡annotation in order to train and evaluate our model  we had a human annotator explicitly mark the linked sentenceentry pairs for a set of 1 games. we also conducted a human interannotator agreement study on a smaller set of 1 games. we compute a kappa statistic over the chance of agreement for both positive and negative alignment decisions. we found high agreement between annotators  yielding a kappa score of 1.
¡¡on average  1 sentence-entry pairs  out of 1 ¡Á 1  are aligned per game. these alignments include about 1% of all database entries and about 1% of all sentences. most aligned sentences are aligned to more than one database entry.
¡¡training and testing regime from 1 manually annotated texts  we obtain about 1 1 entry-sentence pairs  from which 1 represent positive alignments. given the relatively low number of positive examples  we opted to use 1-way cross validation for training and testing our algorithm. we compute precision and recall using aligned pairs as positive examples and unaligned pairs as negative examples.
1 results
tion. in contrast to existing multilabel classifiers  our method operates over arbitrary global features of inputs and proposed labels. our empirical results show that this model yields superior performance without sacrificing tractability.
acknowledgments
the authors acknowledge the support of the national science foundation  barzilay; career grant iis-1 and grant iis1 . thanks to eli barzilay  michael collins  pawan deshpande  yoong keok lee  igor malioutov  and the anonymous reviewers for helpful comments and suggestions. any opinions  findings  and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the nsf.
