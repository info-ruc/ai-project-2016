
a key problem in playing strategy games is learning how to allocate resources effectively.  this can be a difficult task for machine learning when the connections between actions and goal outputs are indirect and complex.  we show how a combination of structural analogy  experimentation  and qualitative modeling can be used to improve performance in optimizing food production in a strategy game.  experimentation bootstraps a case library and drives variation  while analogical reasoning supports retrieval and transfer.  a qualitative model serves as a partial domain theory to support adaptation and credit assignment.  together  these techniques can enable a system to learn the effects of its actions  the ranges of quantities  and to apply training in one city to other  structurally different cities.  we describe experiments demonstrating this transfer of learning. 
1 introduction
when a novice learns how to play a strategy game  his initial focus of attention is usually on figuring out how things work.  long before building up a repertoire of competitive strategies  he is trying out basic actions  seeing their effects and discovering traps to avoid.  we believe analogy plays a key role in this discovery process  especially in mapping across structurally different situations.  yet  analogy by itself cannot account for the very active process that learners go through in exploring a world and constructing explanations.  in this paper  we show how experimentation strategies and qualitative reasoning can support planning and analogy in learning resource allocation tasks in a turn-based strategy game.  
　games of this sort have several interesting properties:  1  they involve incomplete knowledge of the world  1  they entail complex interrelationships between actions and observable quantities  1  goals may be more like optimization problems than states to be achieved  and 1  planning and executing are tightly interleaved.  qualitative representations can serve as a partial domain theory to guide planning and learning at different levels of expertise. 
 in this paper  we describe how the integration of analogy  experimentation  and a qualitative model of city management can support planning and learning in the game of freeciv  freeciv  1 .  in the rest of this section  we outline the context of this work  providing a brief synopsis of the freeciv domain  our htn planner  and analogical reasoning and experimentation.  next we describe how we are using a qualitative model of city economics to support credit assignment and avoid local maxima.  we describe some experiments that demonstrate the ability to transfer and adapt prior training on different cities  and we close by discussing related work and future plans. 
1 the freeciv domain
freeciv is an open-source turn-based strategy game modeled after sid meier's series of civilization tm  games  freeciv  1 .  the objective of the game is to start a civilization from initial settlers in the stone age and expand and develop it until you either conquer the world or win the space race and escape to alpha centauri.  in either case  the game can be characterized as a race to build your civilization and technological sophistication faster than your opponents.  along the way  there are many competing demands for limited resources  investment  and development. for example  players must improve terrain with irrigation and roads  while avoiding famine and military defeat. too much emphasis on military preparedness  for example  can make citizens unhappy and therefore less productive.  money must be 

figure 1: freeciv
allocated to research into new technologies  such as ironmaking and democracy  which enable players to create new improvements to cities  new types of units  and adopt new types of governments  each with their own tradeoffs.    in our current set of experiments  we focus on the subtask of managing cities to optimize their use of resources  build infrastructure  improve terrain  and research technologies.  while our planner can direct exploration  city management tasks offer clearer evaluation metrics.  we also currently ignore military operations  focusing instead on how to make a rich  productive civilization. 
 
1 htn planning
to support performing and learning in the strategy game  we have implemented a hierarchical task network  htn  planner using the shop algorithm  nau et al.  1 .  in the htn planner  complex tasks are decomposed into primitive executable tasks.  the primitives in freeciv correspond to packets that are sent to the game server  representing actions such as sending a unit to a particular location or telling a city what to build.  complex tasks are at the level of figuring out what a unit should do on a particular turn  or deciding how to ameliorate a crisis in a city  such as a famine or revolt.   the planner generates plans for each unit and city at every turn and integrates them in a combined planning/execution environment.  planning is invoked partly in an event-driven manner  such that reified events from the game trigger certain decisions.  for example  the planning agent does not re-compute its global strategy on every turn  but checks to see if it has acquired any new technologies in the last turn  and only then does it re-evaluate its strategy.   a critical aspect of this game is that it requires planning with incomplete and uncertain information.  terrain is not known until it is explored.  the outcomes of some actions are stochastic  for example  village huts may contain barbarians that will kill an explorer  or they may contain gold or new technologies.  there is also vastly more information in the game than can be considered within a planning state.  consequently  the planner cannot plan an agent's actions starting with a complete initial state.  it must reify information on demand by querying the game state.  at the same time  the planner may project the effects of actions such that the planned state deviates from the game state.  to reconcile these competing demands  we maintain two contexts  cf.  lenat 1 : a game context that always reflects the incomplete  but correct current state of the game and a planning context in which states are projected forward.  every query for information that cannot be directly answered from the planned state proceeds to query the game state.  before returning such game-state information  it is checked for consistency against the plan state  to ensure that  for example  a unit is not believed to be in two places at the same time.  this is a simple heuristic that checks for explicit negations and for inconsistent values of functional predicates.  in addition to reifying on demand  the other way we accommodate incomplete information is through second-order planning primitives.  the doplan primitive allows a plan to defer a decision until execution time  thus implementing a kind of conditional plan.  other domain-independent primitives include bookkeeping methods for updating facts in cases  and docallback which suspends a plan until a condition becomes true  at which time it instantiates the plan with the new bindings and resumes execution.  this can play an important role in evaluating delayed effects of actions. 
1 analogical learning
a high-level goal of this research is to demonstrate how analogy and qualitative reasoning can support machine learning across increasingly distant transfer precedents.  to do this  we are using the structure mapping engine  sme   falkenhainer et al.  1   the mac/fac retrieval mechanism  forbus et al.  1   and the seql generalization system  kuehne et al.  1  as core components.  these analogical mechanisms are tightly integrated in the underlying reasoning engine and provide the mechanisms for retrieval  comparison  and transfer.  the structure mapping engine in particular not only assesses the similarity of a precedent to the current situation  but also projects the previous solution into the new case by translating entities to their mapped equivalents  in the form of candidate inferences. thus  analogy provides the first level of adaptation automatically. 
 the unit of comparison and retrieval is a case  and in this approach  cases are not entire games  though some lessons can certainly be gleaned from that granularity   nor even entire cities.  instead  a case is an individual decision in the context of a particular city at a particular moment in time in a given game.   for example  cases can capture a decision about what improvements to build  what tiles to work  and at the broader game level  what technologies to research.   for each type of decision  there is a set of queries represented in the knowledge base that are designated as possibly relevant to making the decision.  there is another set of queries that are relevant to capturing and assessing the case solution.  when the decision is acted on in the game  a snapshot of the case is constructed before and after execution and stored in the game context.  this case snapshot is used both for analyzing the effects of actions and supporting later analogical transfer. 
 for retrieval purposes  cases are organized into case libraries from which mac/fac can extract the most structurally similar precedent.  as the planner attempts to learn decision tasks  it creates and populates libraries for each type of task.  after executing a plan  the current relevant facts are queried and stored as a temporal sub-abstraction in the game context.   when the result of the action is evaluated with respect to the performance goal  the case is added to the task-specific library of successful or failed cases  as appropriate.  a case is considered successful if it both improves the goal quantity and the quantity meets any threshold requirements.  by essentially  rewarding  actions that improve the goal quantity  this can be viewed as a type of learning by the method of temporal differences  sutton  1 .  however  the threshold provides an additional criterion that helps prevent the accumulation of low-values cases that tend to leave the system stuck in local maxima.  in other words  an action that improved a precedent from  terrible  to merely  bad  probably won't help much in the new situation.  for maximizing goals  this initial threshold is simply  greater than zero   while for minimizing and balance goals  it is undefined. 
 one complication with mapping prior solutions to the new problem is that particular choices may not have a clear correspondence in the new problem.  when this happens  the mapping process produces an analogy skolem denoting an unmapped entity.  we resolve such skolems by collecting the facts that mention that entity in the base case  and treating them as constraints in the problem.  we then pick randomly from those choices that satisfy the constraints.   an important insight was that it is not always necessary to resolve all skolems in a previous plan  but only those corresponding to the current choice.  so for example  if a previous plan was to move a worker from one tile to another  but the current problem is to allocate an available worker  then it is not necessary to resolve the worker's prior location.  
1 experimentation
while analogy can be a powerful technique for learning  there must first be cases to retrieve and compare.  to bootstrap a case library and ensure a variety of cases  we guide empirical learning via explicit learning goals  ram and leake  1 .   some learning goals are typically provided at the beginning of a game  such as a goal to learn the effect of the action of allocating workers.  other learning goals may be posted as a byproduct of explaining failures   such as learning the ranges of quantities.  these goals may persist across games. 
 the goal for learning the effects of actions determines how a decision task will be solved when there are insufficient analogical precedents or canned plans.  the experimentation strategy it uses makes decisions randomly in order to generate the requisite variation and provide cases that better cover the decision space.  this strategy is typically accompanied by additional learning goals to control parameters  by suppressing decisions  in order to try to learn one effect at a time.   such a trial and error approach tends to work best for simple low-level decisions.   
 within the context of a single game  poor choices are not revisited blindly  but are recorded as nogoods.  farming the desert typically results in that tile labeled as a nogood.  the problem with this is that for some difficult cities  the system may run out of choices  at which point it must revisit the nogoods  but having tried them  it can now order them by performance and choose the best of what remains.  later  as successful cases accumulate  it becomes possible to solve problems analogically.  still  when analogical transfer fails or runs out of successful precedents  it falls back on experimentation. 
1 exploiting a qualitative model
a qualitative model is a partial domain theory that captures influences between quantities.   one of the outstanding features of a game like freeciv is the complexity of the quantitative relationships in the simulation engine.  understanding the relationships is a critical factor in playing the game well. figure 1 shows a small fraction of our model of cities in freeciv.   
 the primary way the qualitative model is currently used is to determine which changes incurred by an action correspond to goals and whether those changes signify success or failure.  the model allows the system to trace from local tasks to global goals to assign credit and blame.  the second role of the model is in identifying leaf quantities that could affect outputs  and spawning learning goals as described in 'overcoming local maxima'  below.  the greatest potential role for qualitative models will be in synthesizing higher-level strategies by proposing primitive actions that influence goal quantities.   this is on-going work that is outside the scope of this paper. 

figure 1: a portion of the city model
1 overcoming local maxima
one of the difficulties in applying analogical learning to optimization problems of this type is that it is easy to fall into a local maxima  where the system performance stops improving and keeps adopting the same precedents over and over.  we have two ways to overcome this: 
　first  when it fails to improve a goal  it attempts to explain why.  by traversing the qualitative model  it collects the leaf quantities that ultimately affect the goal.  it then posts a learning goal to learn the maximum or minimum values of these quantities  e.g.  the food produced on individual tiles .  over several iterations  this provides a simple expectation about what should be attainable.  the learner uses this information to estimate the improvement that should be possible by moving a worker from the least productive tile to a maximally productive tile.  then  depending on its current tolerance for risk  it sets the minimally acceptable threshold for its goal quantity.  by raising the bar this way  it forces the learner to experiment some more to find a better solution.  the tolerance for risk mentioned above is a function of the penalty that will be incurred on a bad decision.  for example  when a city grows  its granary starts out empty.  moving a worker to a less productive location can lead to a catastrophic famine if the granary is empty  but can be easily corrected if the granary is almost full. 
　the second method  not yet implemented  is by explicitly recognizing the lack of improvement.  in the same way that a person can look at a learning graph and recognize a local maxima  so should the system.  if the recent trend across games is flat or declining  this could serve as a spur to experiment more and be less satisfied with the existing cases. 
1 transfer learning experiments
in order to measure the generality of the learning mechanism  we performed a number of transfer learning experiments. transfer learning involves training a performance system on one set of tasks and conditions and measuring its effect on learning in a different but related set of tasks and conditions.  three types of improvement are possible: higher initial performance  the  y intercept    faster learning rate  and/or higher final  asymptotic  performance.   in the experiments we describe here  the system learned to allocate workers to productive tiles  while holding other potentially conflating decisions constant. 
 to understand the worker allocation task better  it is necessary to understand that cities in freeciv can only benefit from working the 1 tiles in their immediate region  as shown in figure 1.  some tiles are more productive than others  by virtue of their terrain type and randomly placed resource  specials .   when a city is founded  it has one  worker  citizen available to work the land and produce food.  if more food is produced than consumed  then the city's granary slowly fills over a number of turns.  when it is full  the city grows by producing another worker that may be assigned to another tile.  if  on the other hand  more food is consumed than produced  then when the granary is empty the city experiences a famine and loses a worker.  the task we have set for the learner is to learn which tiles will most improve food production.  the performance objective is to maximize the total food production at the end of 1 turns.   note that this is strongly affected by how fast the city grows  which in turn depends on making good decisions early and avoiding famine. 
	philadelphia 	new york 

figure 1: freeciv cities 
 in order to simplify evaluation  we controlled the city production queues to produce only coinage  and constrained the technology research agenda to work towards therepublic.  beyond this  it was also necessary to control for the game's unfortunate tendency to allocate workers for you whenever a city grows or shrinks.  we did this by intercepting these changes in the low-level packet handler routines and undoing the game's interference before the learning agent ever sees it.  in these experiments  we ran 1 training  games on one city   philadelphia   followed by 1 games on a structurally different city   new york   see figure 1 .  each game stopped after 1 turns  which is approximately long enough to show some variation in performance.  figure 1 shows the learning curves for the trials with prior training and those without.  in this case  the trials with prior training have flatlined  because they are essentially at ceiling.  the trials without prior training start out at 1 food points  the baseline performance  and rise to 1 after 1 games.  this could be viewed as a 1% improvement in yintercept. 
 we next ran the same experiment with the cities reversed in order to determine how sensitive it is to the order of presentation.  here  there is an improvement from 1 to 1 food points  but games without prior training remain fixed at 1 points  while after the third trial  the games with training start to oscillate between 1 and 1 points.  what is going on   when we looked at the saved game cases  we saw that successful cases were transferred and applied both within 

figure 1: learning from failure 
 average food production in new york after 1 turns  
the same city and across cities.  however  in cases with prior training  the system kept making the same mistakes over again.  specifically what was happening was that as it gained experience  it transferred successful cases early in the game  reaching a population of 1 or 1.  yet  there aren't typically more than 1 or 1 uniquely mappable highly productive tiles in a city.  so above this size  the system fell back on random choice.  this in itself is not terrible  except that the system had no facility for learning from prior failed cases  and so would continue to try farming the desert and suffer famine.  since this was not a case of negative transfer  we modified the experimentation strategy to first retrieve the most similar failed cases and map their choices into nogoods in the current case.  we ran 1 sequences of 1 games in each condition and plot the average learning curves in figure 1.  these results show a 1% improvement in initial  y-intercept  performance  with a p-value of 1 and a 1% confidence interval between 1 and 1  and are therefore statistically significant.  we further analyzed the saved cases and determined that the incidence of famine dropped in half.   avoiding poor random decisions has the effect of converging more rapidly to an asymptote and reducing the variability in performance.  this suggests that future learning experiments will require more complex  open-ended tasks. 
1 related work
this work is part of a government-funded program on transfer learning.  other groups are pursuing similar goals in somewhat different ways.  icarus  langley et al.  1  and soar  nason and laird  1  have both been applied to learning in real-time game environments  using markov logic networks and bayesian techniques  respectively.  icarus has also been extended to include an htn planner to generate hierarchical skills that can be executed in the architecture.  unlike icarus  our system does not learn hierarchical skills  but instead employs analogy to transfer instances of decisions. 
　the type of learning our system is doing could be viewed as a kind of reinforcement learning  kaelbling et al.  1   in so far as it involves unsupervised online learning and requires exploration of the space of actions.  in reinforcement learning  successful actions are rewarded so that they will be preferred in the future.  here  successful actions are rewarded by adding them to the library of successful cases.  yet the emphasis is clearly different  because the process we have described is knowledge intensive  model-driven  and instance based.   derek bridge  has also explored the relation between reinforcement learning and cbr  in the context of recommender systems. 
 qualitative models have been used in planning previously  notably hogge's  tplan  which compiled a qualitative domain theory into planning operators  forbus'  action-augmented envisionments  which integrated actions into an envisioner  and drabble's  excalibur planning and execution system that used qp theory with a hierarchical partial-order planner.   the strategy game domain is more complex than any of the domains tackled in previous efforts.  our use of htns was inspired by mu oz-avila & aha   who used htn planning in a real-time strategy game. 
 prodigy/analogy tightly integrated analogy with problem solving  veloso  1 .  prodigy's core means-ends analysis strategy is perhaps more amenable to inferential tasks than the kinds of  optimization goals we face.  liu and stone  also apply structure mapping to transfer learning in the domain of robocup soccer. 
　 other researchers have also used freeciv as a learning domain.  ashok goel's group at georgia tech has applied model-based self-adaptation in conjunction with reinforcement learning  ulam et al  1 .  we believe that analogy will better support distant transfer learning  and that qualitative models will ultimately permit strategic reasoning in ways that their tkml models will not.  
1 summary
this paper has presented initial results on integrating analogy  experimentation  and qualitative modeling in a system for planning  executing  and learning in strategy games.  we suggest that qualitative models provide an intermediate level of domain knowledge  comparable to what a novice human player might start with.  we described the use of analogy to compare before and after snapshots in order to extract the effects of actions.  on the basis of experimental results  we implemented a minor change to the plans to enable learning from failures.  this led to a pronounced improvement in behavior. 
　clearly  learning resource allocation decisions is only a first step to learning and transferring abstractions and highlevel strategies.   we are investigating the role of qualitative models for composing new plans.  we also intend to develop more elaborate strategies for pursuing learning goals.  this system does not currently construct explicit generalizations.  consequently  this can be viewed as a kind of transfer by reoperationalization  krulwich et al.  1   though by spawning learning goals in response to failures  we are part way to explanatory transfer.  a next step for us will be to employ seql to construct generalizations and rules once the conceptual distinctions are sufficiently captured in the case base. 
 another important near-term goal is to extract and reason about trends  deadlines and inflection points.  this is a critical requirement for learning the early warning signs of impending problems and learning how to compensate for them before they become severe. 
 ultimately  we expect the techniques developed from this effort to be applicable to the reflective control of agents in a companion cognitive system  forbus and hinrichs  1 . strategizing and directing agents in a game is similar to the problem of coordinating computational resources  learning from interacting with a human partner  and maintaining an episodic memory of previous reasoning sessions. 
acknowledgments
this research was supported by darpa under the transfer learning program.  we thank phil houk  jon sorg  jeff usher  and greg dunham for their programming contributions. 
