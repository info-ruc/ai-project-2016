
in this paper we investigate methods to detect and repair concavities in roc curves by manipulating model predictions. the basic idea is that  if a point or a set of points lies below the line spanned by two other points in roc space  we can use this information to repair the concavity. this effectively builds a hybrid model combining the two better models with an inversion of the poorer models; in the case of ranking classifiers  it means that certain intervals of the scores are identified as unreliable and candidates for inversion. we report very encouraging results on 1 uci data sets  particularly for naive bayes where the use of two validation folds yielded significant improvements on more than half of them  with only one loss.
1 introduction
there is an increasing amount of work on model selection and model combination in the machine learning and data mining literature: for instance  model selection based on roc space  provost and fawcett  1   model combination by means of bagging  breiman  1   boosting  freund and schapire  1   arcing  breiman  1   the mixture of experts method  jacobs et al.  1   to name just a few. a review on ensembles of learning machines can be found in  valentini and masulli  1 .
¡¡typically  these methods assume a set of given models with fixed performance  and the issue is how best to combine these models to obtain a better ensemble model. there is no attempt to analyse the performance of the given models to determine a region where performance is sub-standard. this paper investigates methods to improve given models using roc analysis.
¡¡roc  receiver operating characteristic  analysis is usually associated with classifier selection when both class and misclassification cost distribution are unknown at training time. however  roc analysis has a much broader scope that is not limited to cost-sensitive classification. a categorical classifier is mapped to a point in roc space by means of its false positive rate on the x-axis and its true positive rate on the y-axis. a probabilistic classifier results in a roc curve  which aggregates its behaviour for all possible decision thresholds. the quality of a probabilistic classifier can be measured by the area under the roc curve  auc   which measures how well the classifier separates the two classes without reference to a decision threshold. a good classifier should have a large auc  and auc=1 means that there is a decision threshold such that the corresponding categorical classifier has 1% accuracy.
¡¡we use the term model repair to denote approaches that modify given models in order to obtain better models. in contrast  ensemble methods produce hybrid models that leave the original models intact. an approach to model construction using roc space is given in  blockeel and struyf  1   where the authors identify and assemble parts of a decision tree that perform well in different areas of roc space. our approach in this paper is to identify 'bad' areas  or concavities  in a roc curve and repair them by manipulating the corresponding low-quality predictions. the approach is experimentally validated using both naive bayes and decision tree  but the approach has much wider scope as it can be applied to any classifier that computes class scores.
¡¡to illustrate the approach  we describe in section 1 the repairpoint algorithm that combines three models based on different thresholds of the same probabilistic classifier  and creates a new model which theoretically should improve upon the worst of the three models. in section 1 we introduce the main algorithm repairsection  that mirrors an entire concave region  a region of the curve that is below its convex hull . in section 1 we present experimental results on 1 data sets from the uci repository. section 1 reviews some related work on model ensembles  gives the main conclusion and suggests further work.
1 basics of repairing classifiers in roc space
assume that the confusion matrix of a classifier evaluated on a test set is as in table 1. then the true positive rate of the classifier is a/ a+b  and the false positive rate of the classifier is c/ c+d . the point  c/ c+d  a/ a+b   in the xy plane  i.e.  roc space  will be used to represent the performance of this classifier.
¡¡if a model is under the ascending diagonal in roc space  this means that it performs worse than random. models a and b in figure 1 are such worse-than-random models.
predicted positivepredicted negativeactual positiveabactual negativecdtable 1: a confusion matrix.
however  there is a very useful trick to obtain better-thanrandom models: simply invert all predictions of the original model. this corresponds to exchanging the columns in the contingency table  leading to a new true positive rate of b/ a +b  = 1  a/ a +b   i.e. one minus the original true positive rate; similarly we obtain a new false positive rate of d/ c+d = 1 c/ c+d . geometrically  this corresponds to mirroring the original roc point through the midpoint on the ascending diagonal.

figure 1: by inverting their predictions  worse-than-random models a and b below the diagonal can be transformed into better-than-random models -a and -b above the diagonal.
¡¡notice that the ascending diagonal really connects two classifiers: the classifier which always predicts negative in  1   and the classifier which always predicts positive in  1 . this suggests that the above repair procedure can be generalised to line segments connecting arbitrary classifiers. for instance  consider figure 1. denote the sets of true and false positives of model i by tpi and fpi  then we can construct model 1 under the condition that tp1   tp1   tp1 and fp1   fp1   fp1. in particular  these inclusion constraints are satisfied if models 1  1 and 1 are obtained by setting thresholds on the same probabilistic model  which is what we assume throughout the paper.
¡¡model 1 operates as indicated in table 1. the inclusion constraints guarantee that the geometric configuration of figure 1 holds. this is formally stated in the following theorem.
theorem1 assuming that tp1   tp1   tp1 and fp1   fp1   fp1  the model produced by algorithm repairpoint has true and false positive rates tpr1= tpr1+tpr1 tpr1 and fpr1 = fpr1+fpr1 fpr1  where tpri and fpri denote true and false positive rates of model i.
proof. under the inclusion constraints expressed in the theorem  there are four disjoint groups of examples: those classified positive by models 1  1 and 1  tp1¡Èfp1 ; those clas-
given three models model 1  model 1 and model 1  output model 1 that operates as follows:
1. if both model 1 and model 1 predict negative  then predict negative;
1. if both model 1 and model 1 predict positive  then predict positive;
1. if model 1 predicts negative and model 1 predicts positive  then predict the opposite of what model 1 predicts;
1. otherwise  predict what model 1 predicts.

table 1: algorithm repairpoint. the last clause does not apply under the inclusion constraints and is only added for completeness.
sified negative by model 1 and positive by models 1 and 1   tp1  tp1 ¡È fp1  fp1  ; those classified negative by models 1 and 1 and positive by model 1   tp1  tp1 ¡È
 fp1 fp1  ; and those classified negative by all three models   pos tp1 ¡È neg fp1   where pos and neg are the sets of all positive and all negative examples  respectively . by construction  model 1 classifies the first group as positive  the second group as negative  the third group as positive  and the fourth group as negative. the true positives of model 1 are thus tp1¡È tp1 tp1 ; because of the inclusion constraints the result follows  analogous for false positives .

figure 1: model 1 is mirrored to model 1 with the help of models 1 and 1.
¡¡an equivalent construction is the following. remove from the test set all instances classified positive by model 1  and all instances classified negative by model 1. we can imagine this as a smaller nested roc space in which model 1 represents  1  and model 1 represents  1 ; because of the inclusion constraints the position of model 1 remains unchanged. note that model 1 performs worse than a random model in this nested roc space. we then construct model 1 as in figure 1  by inverting the predictions of model 1 on the remaining test examples.
¡¡we end this section by noting that the true and false positive rates derived for model 1 only hold for the same test set on which models 1  1 and 1 were evaluated. on a second  independent test set the roc locations of the 1 classifiers may be different - in particular  model 1 may not be below the line connecting models 1 and 1  in which case model 1 will be evaluated worse than model 1. in our experiments  we therefore use validation sets to decide whether concavities are stable across different samples. this will be further discussed in section 1.
1 identifying and repairing concavities in a roc curve
the previous section outlined the main ideas underlying repairing concavities in roc curves. however  preliminary experiments indicated that the three-point approach is too crude to work well in practice. in this section we introduce our main algorithm  which manipulates a whole section of a roc curve. a roc curve is obtained by evaluating a probabilistic classifier on a test set and varying the decision threshold  resulting in a step curve  hand and till  1 . an efficient way of constructing this curve is by ranking the instances corresponding to their predicted probability of being positive  fawcett  1 . figure 1 shows both the roc curve for a probabilistic model evaluated on a small test set1 and its convex hull  provost and fawcett  1 .

figure 1: a probabilistic roc curve and its convex hull.
¡¡four points of the roc curve  point a  b  c and d  are located on the convex hull. each of these points corresponds to a probability threshold  and thus each segment of the convex hull corresponds to a probability interval. for instance  the convex hull in figure 1 has three segments corresponding to three disjoint probability intervals. three out of these five segments delineate concave regions of the roc curve  indicated as a  b and c. for example  area a is delineated by line ab and the roc curve between point a and point b. in general  a concave area means that the ranking obtained from the probabilistic model in this probability interval is worse than
random. for instance  consider area c which is the largest concavity. one way to repair this concavity is by ignoring the score calculated by the probabilistic model in this interval  and output a constant score  e.g.  the mid-point of the interval . assuming that ties are broken by assigning a random rank  this would replace the concave region of the roc curve with the line segment cd. it is interesting to note that if this procedure is followed for all concavities  this corresponds to constructing the convex hull by discretising the probability scores.
¡¡however  in theory we should be able to do better than that: we can invert the ranking of the instances in the probability interval  which can be seen as applying algorithm repairpoint to all thresholds in this interval. for instance  by applying this algorithm to the model corresponding to threshold c1  this point would be point-mirrored through the midpoint on line segment cd to the other side of the convex hull. the same can be done for the other thresholds. the resulting roc curve is shown in figure 1. we can note that the area c under the curve has been replaced by an equally large area c1 above the curve. the auc of the repaired curve is therefore larger than both the auc of the original curve and the auc of its convex hull.

figure 1: mirroring a concave part of the roc curve.
¡¡the algorithm to produce the model with the repaired roc curve is given in table 1. the procedure works for any model that calculates a score; a probabilistic model is a special case.

given a scoring model m and two thresholds t1  t1  construct a scoring model m1 predicting scores as follows. let s be the score predicted by m: 1. if s   t1  then predict s;
1.	if s   t1  then predict s; 1.	otherwise  predict t1+t1 s.

table 1: algorithm repairsection. the algorithm effectively inverts the ranking of all instances whose score as predicted by m falls in the interval t1   s   t1.
1 experimental evaluation
we describe a number of experiments to evaluate our approach. we used 1 two-class data sets from the uci repository  blake and merz  1 . table 1 shows their numbers of attributes  numbers of examples  and relative size of the majority class.
dataset#attrs#exs%majclassaustralia11sonar11glass11german11car11anneal11monk11.1monk11.1monk11.1hepatitis11house11tic-tac-toe11heart11ionosphere11breast cancer11lymphography11primary tumor11soybean-large11solar-flare11hayes-roth11credit11balance11bridges11table 1: uci data sets used in our experiments.
¡¡the experimental procedure for the first experiment was as follows. we split a data set into ten folds and use eight of them for training  one for validation and one for testing. we trained a naive bayes model and a decision tree model1on the training data  and chose two thresholds that delineate a concavity. we then produced a new model by repairing the probabilities between the thresholds; we only use the new model if it improves auc on the validation set. the detailed procedure is given in table 1.
¡¡we ran experiment repairsection ten times and obtained m pairs of auc values. since we use a validation set  we are able to decide whether or not repair resulted in a better model - if not  we discard it and use the original  unrepaired model. for this reason  we only report results on the non-discarded models  so m may be smaller than 1. the average aucs of the unrepaired curve and the repaired curve for each data set are given in tables 1 and 1. we also performed a paired t-test with m   1 degrees of freedom and level of confidence 1 to test the significance of the average difference in auc. the results are favourable: the significance tests yield 1 wins and 1 losses for repaired naive bayes models and 1 wins and 1 losses for repaired decision tree models.
¡¡experiments without using a validation set yielded worse results  so the use of a validation set appears crucial. we
1. train a naive bayes or decision tree model m on the training data; construct a roc curve c and its convex hull h on the training data.
1. find adjacent points on h such that in this interval the area between c and h is largest. let t1 and t1 be the corresponding score thresholds.
1. produce a new probabilistic model m1 by calling repairsection t1  t1 .
1. evaluate m and m1 on the validation set  construct their roc curves and calculate their aucs. if auc m1  ¡Ü auc m  then go to 1.
1. evaluate m and m1 on the test set  construct their roc curves and calculate their aucs.
1. go to 1. until each fold has been used as a test set.

table 1: experiment repairsection.
datasetaucaucbetter original  repaired  australia1 ¡À 11 ¡À 1¡Ásonar1 ¡À 11 ¡À 1glass1 ¡À 11 ¡À 1¡Ågerman1 ¡À 11 ¡À 1¡Ácar1 ¡À 11 ¡À 1anneal1 ¡À 11 ¡À 1¡Åmonk1.1 ¡À 11 ¡À 1¡Åmonk1.1 ¡À 11 ¡À 1¡Åmonk1.1 ¡À 11 ¡À 1¡Åhepatitis1 ¡À 11 ¡À 1house1 ¡À 11 ¡À 1tic-tac-toe1 ¡À 11 ¡À 1¡Åheart1 ¡À 11 ¡À 1¡Áionosphere1 ¡À 11 ¡À 1breast cancer1 ¡À 11 ¡À 1¡Ålymphography1 ¡À 11 ¡À 1¡Åprimary tumor1 ¡À 11 ¡À 1¡Åsoybean-large1 ¡À 11 ¡À 1solar-flare1 ¡À 11 ¡À 1hayes-roth1 ¡À 11 ¡À 1¡Åcredit1 ¡À 11 ¡À 1balance1 ¡À 11 ¡À 1bridges*1 ¡À 11 ¡À 1average11table 1: results of experiment repairsection with naive bayes. *for the bridges data set we used 1-fold crossvalidation.
therefore conducted an experiment with two validation folds: we would only use the repaired model if its auc was higher on both validation folds. the results for naive bayes are shown in table 1. we now obtain 1 significant wins and only 1 significant loss  and the average increase in accuracy is more than a percentage-point. interestingly  two validation folds didn't work well for decision trees.
¡¡finally  we conducted an experiment with naive bayes whereby we selected all concavities  and repaired those that occurred on two validation sets. the results were similar to the results in the first experiment  1 significant wins and three losses   with some of the wins and losses occurring on
datasetaucaucbetter original  repaired  australia1 ¡À11 ¡À1sonar1 ¡À11 ¡À1¡Åglass1 ¡À11 ¡À1¡Ågerman1 ¡À11 ¡À1¡Åcar1 ¡À11 ¡À1anneal1 ¡À11 ¡À1¡Åmonk1.1 ¡À11 ¡À1¡Åmonk1.1 ¡À11 ¡À1¡Åmonk1.1 ¡À11 ¡À1¡Åhepatitis1 ¡À11 ¡À1house1 ¡À11 ¡À1¡Åtic-tac-toe1 ¡À11 ¡À1¡Åheart1 ¡À11 ¡À1¡Áionosphere1 ¡À11 ¡À1breast cancer1 ¡À11 ¡À1lymphography1 ¡À11 ¡À1¡Åprimary tumor1 ¡À11 ¡À1¡Ásoybean-large1 ¡À11 ¡À1¡Ásolar-flare1 ¡À11 ¡À1¡Áhayes-roth1 ¡À11 ¡À1¡Åcredit1 ¡À11 ¡À1balance1 ¡À11 ¡À1bridges1 ¡À11 ¡À1¡Áaverage11table 1: results of experiment repairsection with decision trees.
datasetaucaucbetter original  swapped  australia1¡À11¡À1¡Ásonar1¡À11¡À1¡Åglass1¡À11¡À1¡Ågerman1¡À11¡À1¡Åcar1¡À11¡À1anneal1¡À11¡À1¡Åmonk1.1¡À11¡À1¡Åmonk1.1¡À11¡À1¡Åmonk1.1¡À11¡À1¡Åhepatitis1¡À11¡À1house1¡À11¡À1¡Åtic-tac-toe1¡À11¡À1¡Åheart1¡À11¡À1ionosphere1¡À11¡À1breast cancer1¡À11¡À1lymphography1¡À11¡À1¡Åprimary tumor1¡À11¡À1¡Åsoybean-large1¡À11¡À1solar-flare1¡À11¡À1hayes-roth1¡À11¡À1¡Åcredit1¡À11 ¡À1balance1¡À11¡À1average11table 1: results with naive bayes using two validation folds.
different data sets. it appears that repairing only the largest concavity and using two validation sets is the best strategy  at least for naive bayes .
1 discussion and conclusions
the work reported in this paper bears some similarity with ensemble methods. bagging and boosting are two well-known ensemble approaches. both approaches are implemented by re-sampling methods. in bagging  breiman  1   the ensemble is formed by making bootstrap replicates of the training data sets and then multiple generated hypotheses are used to get an aggregated predictor. boosting algorithms  freund and schapire  1  assign different weights to training instances depending on whether they are correctly classified. the approaches presented in this paper do not make use of re-sampling techniques.
¡¡another relevant ensemble method is majority voting  kimura and shridar  1; lam and sue  1   in which the class predicted by the ensemble is the most predicted class among the base classifiers. algorithm repairpoint in this paper uses a kind of voting: when both model 1 and model 1  see figure 1  agree on the classification of an instance  then we choose that class. otherwise  we choose the class not predicted by model 1. in majority voting  on the other hand  we would choose the class predicted by model 1. the difference is that majority voting does not take the quality of the different models into account  whereas our repair scheme knows that model 1 is sub-optimal and therefore corrects it predictions in the relevant region.
¡¡roc curves contain a wealth of information about the performance of one or more classifiers  which can be utilised to construct better models. they have been used to find optimal labelling of decision trees  ferri et al.  1  and to find good decision thresholds for probabilistic classifiers  lachiche and flach  1 . in this paper we have proposed a novel approach to construct new models by repairing concavities in a roc curve. the first method  repairpoint  works on a probabilistic classifier with three probability thresholds  and tries to improve the poorest model with help of the other two. preliminary experimental results  not reported  showed that this didn't work too well  but this may be due to the fact that the selection of the threshold for model 1 is not easy. the threshold is a point chosen based on the roc curve of the training data set; this point  see point c1 in figure 1  has the farthest distance to the convex hull. if this threshold is not optimal on the test data  for instance  the position c1 in the roc curve on the test data set unfortunately is located in the position c1   the auc after repair becomes worse. still  we believe that the idea of mirroring models around lines in roc space will prove to be very useful. an interesting investigation for future work is whether a similar method can be made to work if the models are not obtained from a single scoring model  this would invalidate theorem 1  i.e.  the position of model 1 may be different from the point obtained by pointmirroring .
¡¡the second method  repairsection  locates and repairs an entire concave region of a roc curve. experimental results were very encouraging for both naive bayes and decision trees. for naive bayes we were able to improve results even further by using two validation folds  but this didn't work for decision trees. we are currently investigating why this is so. one possible explanation is that roc curves obtained from decision trees have lower resolution  because all instances in a leaf receive the same predicted probability   which may mean that concavities are less stable across samples. pruning may be another factor  as it has been shown that pruning is detrimental for probability prediction  provost and domingos  1; ferri et al.  1 .
¡¡there are several other ways in which this work could be taken further. one is to investigate how much repair is possible  by concentrating on roc curves with large concavities  possibly from artificial data sets. another is to work with averaged roc curves that are obtained by cross-validation  since each instance occurs in the test fold exactly once  an averaged roc curve can be simply constructed by combining all instances with their predicted probabilities .
acknowledgments
a preliminary version of this paper  without the experimental results with decision trees and two validation folds  appeared as  flach and wu  1 . we gratefully acknowledge the constructive comments made by the anonymous reviewers. we would also like to thank rich roberts for performing additional experiments.
