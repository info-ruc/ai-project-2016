
markov models have been a keystone in artificial intelligence for many decades. however  they remain unsatisfactory when the environment modelled is partially observable. there are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. on the other hand  working with a hidden state  like in hidden markov models or partially observable markov decision processes  has a high computational cost. in order to circumvent this problem  we suggest the use of a context-based model. our approach replaces strict transition probabilities by influences on transitions. the method proposed provides a trade-off between a fully and partially observable model. we also discuss the capacity of our framework to model hierarchical knowledge and abstraction. simple examples are given in order to show the advantages of the algorithm.
1 introduction
the ability to predict future events is a necessary component of intelligent agents  as it facilitates accurate planning. a standard approach is to predict the future solely through observations  for example using a fixed-order model markov chain. unfortunately  a history of any fixed length might not be sufficient to accurately predict the next observation. methods using a variable history window  e.g. mccallum  1  work well in practice but are largely heuristic. a different approach for making predictions through time is to introduce a notion of latent or hidden state of the environment  as in hidden markov models  rabiner  1  and partially observable markov decision processes  kaelbling et al.  1 . such models clearly separate observations from hidden states  and keep track of the current state through a belief vector. however  assuming a hidden state requires knowledge about it  and often the state transition probabilities are assumed to be known. for learning agents  this approach appears restrictive: knowledge is necessarily bounded by the state representation. furthermore  the true state of the system may involve many elements which are not observable by the agent for long periods of time. one may imagine  for example  an environment divided into regions where certain observations only appear in certain regions. objects which evolve in the world also involve keeping track of additional state information about them. recent work on predictive representations  sutton and tanner  1; littman et al.  1  aims to bridge this gap by explicitly constructing a state  which is a sufficient statistic for predicting the future  from the experience of the agent.
¡¡the goal of our work is also to build a predictive representation which is not based on a fixed length of history and can be learned incrementally from data. specifically  our work is geared towards environments where observations are subjective. in such systems  we can hope that the only state the agent needs to be concerned about is based on what is directly accessible to it. more formally  this would be observations that lead to better predictions of the immediate future. our work is based on hetero-associative memories  such as sparse distributed memories  kanerva  1   but puts the emphasis on predicting future events rather than as a way of palliating to noisy inputs. our model can also handle new observations and can generalize to a certain extent  as there is no necessity for model specification in advance beyond defining the observation features. in this paper we do not explicitly discuss planning and rewards  but assume that predictions can be used to determine actions.
¡¡similar problems have been explored in the connectionist literature. for example   elman  1  considers the notion of context in a neural network as being closely related to the previous state of the system. through simple tasks  he shows that a recurrent neural network can learn to base its predictions on past inputs summarized in its hidden units' activation. research on a symbol prediction task close to ours has been described in  bose et al.  1 . however  they take a neural network approach to the problem and focus on a particular kind of network architecture and implementation. recently  boltzmann machines have been used for prediction through time as well  taylor et al.  1 . our proposed algorithm is significantly different from those mentioned above: we focus on weighting past observations in a direct fashion rather than through an internal state of the agent. a similar prediction task is addressed in  gopalratnam and cook  1   in the context of text parsing. however  in their approach all data structures are geared towards dealing with symbols. our approach works both for discrete and continuous observations  although our experiments contain discrete observations only .
¡¡the paper is organized as follows. in section 1 we briefly review sparse distributed memories  on which we based our ideas. we also discuss hopfield networks and boltzmann machines and the benefits of sparse distributed memories over them. in section 1  we formally define our framework. following this  we discuss a simple algorithm for learning from observations in section 1. we then give examples showing the predictive power of the algorithm in section 1. finally  section 1 discusses the sort of environments for which our method is suitable  its current failings compared to existing models and how it may be used to obtain abstraction.
1 background
sparse distributed memories  sdms  were developed in order to model long-term memory  kanerva  1 . of interest is the capacity of such memories to both retrieve a noise-free version of an input vector  auto-association  and to retrieve an associated output vector  hetero-association . as opposed to other types of associative memories  sdms do not require an iterative process in order to converge to the desired answer. in that respect  they resemble feed-forward neural networks  and have been modelled as such. sdms have been used in prediction  for example for robot navigation  rao and fuentes  1 .
¡¡a sdm is divided into two parts: the hard locations and the output words. a hard location is simply a binary vector of the same length as the input vector. to each hard location corresponds an output word which is composed of an integervalued vector  possibly of different length.
¡¡when a vector is given as input to the memory  its distance di to each hard location i is computed; in the original implementation  di is simply a hamming distance. the output of the system is then obtained in the following way: for each hard location with distance di ¡Ü ¦Ä  where ¦Ä is a threshold value  its corresponding word is added to the output sum s. the actual output o is found by thresholding the sum such that oi = 1 if si   1  and oi = 1 otherwise.
¡¡learning in sdms is fairly straightforward. when we wish to map an input v to a target word z  we determine which hard locations are activated  di ¡Ü ¦Ä  by v. for each of these  we add z to their corresponding output word.
¡¡if the hard locations are orthogonal  we obtain something very close to a linear approximator. such systems have been studied extensively  for example as associative search networks  barto et al.  1 . if the hard locations are not orthogonal  however  the memory potentially gains in generalization capacity by producing more complex combinations of mappings. various works on sdms have experimented with extending the system to handle real values  computing its capacity  and deciding how to craft the hard locations to better suit a given task. presented as above  sdms are interesting as they propose an intuitive coverage of the input space through actual potential inputs  work on this idea in reinforcement learning was done in  ratitch and precup  1  . their hetero-associative capacity also makes them promising for prediction purposes.
¡¡other associative memory frameworks have been proposed before sdms. among those still used today  we find hopfield networks  hopfield  1  and boltzmann machines  fahlman et al.  1 . both rely on the idea of neuronlike elements that share undirected connections with their neighbors. in the simplest version  each element has two states. with each undirected connection is associated a weight  which influences the corresponding neighbor into being in a certain state  based on the weight sign. usually  a positive weight indicates that both units should be in the same state  whereas a negative weight indicates they should take different states. the system then attempts to minimize its energy. this energy increases when units of the system take states that violate the influence of their connection weight.
¡¡inputs can be given to these algorithms by forcing certain elements to stay in a given state  for example by explicitly defining input units with fixed values. since both algorithms are undirected by nature  obtaining a minimum energy state  a solution  requires iterating over the unit states until the energy stagnates. unfortunately  this process can be slow. learning in such a model is also computationally expensive  although restricted boltzmann machines have recently been used to achieve good results  hinton et al.  1 .
1 framework
unfortunately  sdms suffer from a big disadvantage. they are  by nature  deterministically noise-correcting and do not allow us to predict events that may occur. before we discuss our proposed framework  we must make a few assumptions regarding the environment that we are interested in modelling and define a few terms. first  let us define a percept as a real-valued vector representing an observation. secondly  we assume that association is not done on a one-to-one basis. rather  a given percept may be associated to many other percepts  with different strengths of association for each. we formalize this by proposing that an input percept p maps to a distribution over associated percepts.
¡¡we define the memory as a set of cells  c. each of these cells acts as a hard location  and therefore has a single percept associated with it  which we denote ci through notational abuse. each cell also has a saliency value si associated with it and a vector of output weights  wi. wi represents directed associations between cells  and so wi has the same size as c. in general  we denote the weight matrix w and the saliency vector corresponding to c  s.
¡¡when the system is presented with an input percept p  it computes an activation vector ¦Á similar to the activation in sdms. here however  ¦Á is a real-valued vector with elements taking values between 1 and 1  where ¦Ái = 1 indicates a perfect match between p and ci and ¦Ái = 1 indicates no match. usually  we would like. in our work  we use the simplest activation function  namely ¦Ái = 1 if ci = p  and ¦Ái = 1 otherwise. in effect  we are assuming that percepts are actually symbolic and not subject to noise; this need not be the case in general.
¡¡by themselves  sdms do not allow any association based on past observations. to circumvent this  we use the saliency value of cells as an activation trace. more formally  at every time step we set the saliency vector to
s ¡û ¦Ãs+ ¦Á
¡¡this is similar to the cumulative eligibility trace in reinforcement learning  sutton  1   where ¦Ã is a decay factor. a restricted form of this type of encoding has also been used for prediction in  bose et al.  1; furber et al.  1 . if ¦Ái = 1 for exactly one percept and 1 everywhere else  s represents the past sequence of observations  provided no percept has been observed twice. note that for the purposes of the system  s does not need to be an exponentially decaying trace of observations; its goal is to serve as context to obtain better predictions. in section 1 we will discuss one possible way of improving the above equation.
¡¡our algorithm diverges from sdms as it attempts to predict percepts that match its hard locations  rather than separating them from the output words. we define secondary activation  denoted by ¦Â  as a vector representing a prediction weight for each cell. we are interested in predicting which of the percepts  represented by cells  may be observed. we assume here that the hard locations represent actual percepts. we then compute ¦Â as
¦Â = ws
from this equation one can notice that the weight matrix indeed acts as a set of associations; experiencing a percept leads to related percepts being predicted.
¡¡at the very least the values of ¦Â should be in the correct prediction order and significantly different. any function of ¦Â which preserves this ordering and results in a valid probability distribution may be used to predict the next time step. we chose to use a simple boltzmann distribution using ¦Â and given by:

the distribution's entropy is controlled by ¦Ó  a standard temperature parameter between 1 and ¡Þ. in the experiments we used ¦Ó = 1 but other values may yield better results.
1 learning
after having discussed how the algorithm predicts events  we now describe how learning can be accomplished. logically  since we use the saliency vector s as contextual hints which allow us to make a better prediction  we should modify weights based on s.
¡¡for now  assume that we already have a known set of hard locations. let pt be the percept observed at time t  and similarly ct  the set of cells  wt the weight matrix and st the saliency vector. we denote the activation due to pt by ¦Á pt .
¡¡assuming that we want to predict pt in the future when s is present  we should modify wt to produce a probability distribution similar to ¦Á pt . formally  let ¦Ð be our probability distribution on c; we define the prediction error e as:

where ¦Ái is the ith component of ¦Á pt . we then compute the gradient of the error with respect to wi j  the jth weight
1. initialize hard locations
1. w ¡û 1
1. for each episode do
1. s ¡û 1
1. repeat until done
1. compute ¦Ð from s
1. observe the next percept p
1. update
1.table 1: our context-based prediction algorithm.
of cell i. here wi j represents how strongly j influences the prediction of i.
¡¡let   and recall that. first note that:

let   be the vector of errors  with. from the above equation we obtain:

we can then modify the output weights through a standard update rule with learning rate c ¡Ê  1 :

¡¡usually  probability-producing systems are trained using a likelihood-based gradient. here  however  there are two reasons why we might prefer to use the sum of squared errors to compute the gradient. first  it explicitly allows us to train the system to output a combination of hard locations  through the ¦Á vector. this can be interesting if many percepts activate two or three hard locations due to noise. also  we are interested in good generalization capabilities. experiments in which we used maximum likelihood gave worse results. we believe that is might due to the fact that there is no 'ground truth' distribution which we are approximating; instead  we are constructing an appropriate distribution through association.
¡¡there is a second learning problem  which we ignore here  but is of interest. the hard locations do not have to be predefined  or fixed. in a way  learning to recognize a percept can be just as hard as prediction. we discuss this issue further in section 1 below. the whole algorithm is presented in table
1.
episodes11p 1 =1p 1 1111p 1 1111p 1 =1p 1 1111p 1 1111p 1 =1p 1 1111p 1 1111table 1: predicted observation frequencies based on the number of training episodes.
1 examples
in this section we give examples in order to show that our proposed algorithm can indeed perform prediction in a similar fashion to that of a strict markovian model  without being restricted by a fixed history length or states. for all of the examples  we use sequences of numbers as observations. each number is encoded as a n-sized vector with the corresponding bit set to 1 and all others set to 1; n is the maximum number of observations. note that in a more natural task a percept vector may represent sensory input  and so its structure would be well-defined. to simplify matters  we assume that the agent always receives the percept 1  first bit set to 1  at the beginning of each episode  and never receives it during the episode. this is similar to defining a special start symbol when learning a kth order markov model  and we would expect this to have little impact in a system that learns continuously. here we are using a learning rate of 1  which gave sufficiently stable results. the decay factor ¦Ã was also arbitrarily set to
1.
¡¡the first experiment is the simplest one  and shows that the system can approximate symbol frequencies. we simply produce two separate one-observation episodes with certain frequencies. one of these episodes produces a 1  while the other produces a 1. in order to avoid variance in the results  we chose to use a fixed sequence of episodes. these always start with 1  end in 1 and episodes containing 1's are experienced at regular intervals in-between. estimated frequencies are given in table 1  where we show the probability of each event after a certain number of training episodes. actual frequencies are given on the left-hand side. note that the given probabilities do not sum to one due to the boltzmann distribution: it assigns a non-zero probability to all events  and here 1 is predicted as unlikely but not impossible. we can see that with sufficient training  all estimates converge towards their true values. the learning rate here prevents us from obtaining the exact probabilities as the output weights oscillate between episodes.
¡¡the second experiment is aimed at showing that the system  despite having no explicit indication of ordering  can in fact discriminate based on recency. the example strings that we use are respectively 1 and 1. the goal here is to show that we can predict with high probability which of the two end symbols will appear based on ordering. results are given in table 1.
episodes11context 1p 1 1111p 1 1111context 1p 1 1111p 1 1111¡¡clearly as the number of training iterations increases the system becomes able to more precisely predict the true symbol based on context. again here  the symbols 1  1 and 1 |
table 1: predicted observation based on order of past observations.
episodes111sequence pairs1.1.1.1.1.1111111.1.1.1.1.1111111.1.1.1.1.1111111.1.1.1.1.1111111.1.1.1.1.111111table 1: predicted observation based on long-term context. values in italic show when the actual observation was not predicted with the highest probability.
are also predicted with probabilities that decrease as training increases. if we were only interested in predicting the end symbol  we could increase ¦Ó to obtain higher probabilities. with our choice of parameters  the initial symbol  1 or 1  is predicted to occur with roughly 1% chance  as in the first experiment.
¡¡the third set of sequences that we looked at shows that the system can learn to use context from any point in the past to differentiate two predictions. more specifically  we have two target symbols  1 and 1  which can only be predicted based on the first symbol that occurs  which is either a 1 or a 1. the training strings and the predicted probability of the correct symbol are reported in table 1.
¡¡as can be seen from this table  the system follows a slow degradation in predicting the correct event as the differentiating observation becomes more remote. the fact that 1 is systematically predicted with higher probability than 1 is an artefact of our experiment  due to the sequence containing 1 being presented after the sequence containing 1. it is interesting to note that for the longest example given here  the saliency of the context percept at the time of prediction is 1. yet it can be seen that as the number of iterations increases  the algorithm learns to correctly distinguish between the two events.
1 discussion
the framework presented seems to provide us with a way of predicting events which  if not perfectly accurate  is not subject to history length constraints and does not require explicit state knowledge. of chief interest is the fact that the algorithm  as shown above  can handle temporal ordering  which can be key to many predictions. however  it can also handle predicting observations from unordered sequences. as a brief example  we can imagine an environment where we obtain clues about the 'state' of the world  which is represented by a separate percept  possibly one that occurs often while in that 'state' . markov models relying on a strict order of observations will need many samples to produce clear predictions. our algorithm  on the other hand  can infer from many weak clues a more general percept.
¡¡of a technical nature  both the saliency vector and the weight updating scheme are flexible and may be modified to fit different situations. for example  we experimented with modifying the saliency vector to keep relevant percepts in context and remove more quickly irrelevant ones. this can be easily done by considering the gradient of the error on the current prediction with respect to the saliency of each cell; the update rule is then very similar to the one used for the output weights. the weight updating scheme may also be improved by preventing negative gradients. currently  our weight update rule reduces the probability of all the events that did not occur  ¦Ái = 1  at the same time as it increases the probability of the actual observation. this might hinder later learning; a purely additive approach  closer to a frequency-based model  might perform better.
¡¡obviously  being able to predict events more or less accurately is not sufficient for an agent; we also need correct control. from a reinforcement learning point of view  we can naturally incorporate reward into our framework in three specific ways. first  we can explicitly attempt to assign a value to each observation  and compute the predicted expected value based on context. a similar idea was developed in  ratitch and precup  1   but their algorithm did not explicitly model observation prediction  and was done in a fully observable framework. we can also modify the weight update rule to use the magnitude of rewards to focus learning on important parts of the system. more interestingly  though  we can modify the saliency vector based on reward information  so that a percept which is known to be associated with high reward or penalty might be kept in context longer.
¡¡in the case of a truly stochastic event  similar to what we presented in the first experiment of section 1  our prediction will  by definition  never be accurate. this is in some sense a drawback of the algorithm: transitions are expected be fully determined by some past event. there are many ways to address this problem. first  we can assume that no event is truly stochastic in nature  and that stochasticity only appears through lack of contextual information. in such a case  we could try inferring missing past events by increasing the saliency of percepts which usually cause the current observation. another approach would be to explicitly consider the variance of a prediction. qualitatively  large output weights suggest strong evidence towards a prediction. if more than one event is strongly activated by context  then there is reason to believe that stochasticity will appear in the observations.
¡¡one question that has been largely ignored so far in this paper is that of handling percepts which occur more than once in a short interval of time. since the algorithm has no way of specifying that an observation has occurred twice  we lose the capacity of a kth order model to make a separate prediction when this happens. this may be seen as a failure of the framework  and indeed it is not suited for environments where there are very few observations. however  if we consider that the algorithm builds a causality link from a percept to another  then the problem may be solved in the following way. the repeated occurrence of a percept does not provide additional information; it instead becomes a question of the duration of the observation. our framework implicitly handles duration if we do not allow non-zero output weights from a cell to itself. indeed  the output weights to the context should become larger in order to accurately predict an event which occurs for a longer period of time  and therefore such events should be predicted even when the relevant contextual information is further in the past.
¡¡we purposefully left out the recognition problem in our presentation of the algorithm. constructing suitable hard locations might not be simple. however  our framework implicitly proposes a different approach to recognition. a set of temporally or spatially related percepts may all be associated together to form a single  more abstract object. this can be the case in computer vision for example  where multiple poses are mapped to be same object.
¡¡in a similar fashion  the algorithm is not restricted to a single modality. we have extended it to include multiple modalities which are associated together. although a separate prediction is made for each modality  context is constituted of all modalities. such a capacity opens many opportunities  such as building knowledge through associations across modalities. among the most striking is the possibility to link perceptual knowledge to actions if both are represented in the associative framework proposed.
¡¡the discussion above suggests the use of actual percepts as abstraction  ie. strong contextual clues. an abstract percept may be one that activates its instances  for example the idea of a tree may simply be a cartoon tree  which then maps to all other kinds of trees. what is interesting here is that considering an abstract object as a regular percept allows us to manipulate it atomically  without regard for its abstract nature. having atomic abstractions in turn allows us to abstract them  and gives us the capacity to build hierarchical knowledge without any additional apparatus.
¡¡past research on association and prediction has mainly focused on obtaining averages of noisy observations. the novelty in our approach comes from the fact that we put the emphasis on the importance of associations between incomplete observations. as such  our framework takes a radically different view of perceptions: noise can be overcome through the association of noisy signals. all the associative memories discussed above share the fault that they are geared towards producing an ideal output. rather  we hope that our framework can build many  possibly noisy  associations which  when combined  yield a correct answer. this is something that can only be done if we can produce a probability distribution over associations; we also need to be able to use events from any time in the past. our algorithm  by achieving both of these  seems promising for predictive and knowledge-building purposes.
1 conclusion
in this paper we presented a novel framework which uses past observations as contextual clues for predicting future events. at the core of our algorithm lies the concept of saliency  which causes past  important events to be used as context. we described both how to produce predictions using associative links between observations and how to learn such associations. simple examples show that the algorithm preserves basic properties of markov models  such as the capacity to distinguish between differently ordered observations. we also exemplified the potential of the algorithm to use contextual information from anywhere in the past towards current predictions  which is the main failing of history-based models. however  our algorithm does not require knowledge of the true state of the world in order to make predictions  therefore reducing its complexity and making it more applicable to general tasks. although much remains to be done in order to discover the advantages and flaws of the algorithm  we have sketched out it potential uses. we propose a slightly different view of observations  abstraction and contextual information  which we hope might lead to improved performance of algorithms.
¡¡we are currently in the process of testing the algorithm on larger domains which may help understand its strengths and weaknesses. although it is not suited for environments where very few observations are available and where they repeat often  natural environments provide a large variety of perceptions. the goal in such tasks is not necessarily perfectly accurate prediction  but rather coherent actions. our framework may help in this respect by using context to drive prediction and action  and by allowing for abstraction of knowledge and the construction of high-level actions through association.
acknowledgments
this work was supported in part by funding from nserc and cfi.
