
a key problem in reinforcement learning is finding a good balance between the need to explore the environment and the need to gain rewards by exploiting existing knowledge. much research has been devoted to this topic  and many of the proposed methods are aimed simply at ensuring that enough samples are gathered to estimate well the value function. in contrast   bellman and kalaba  1  proposed constructing a representation in which the states of the original system are paired with knowledge about the current model. hence  knowledge about the possible markov models of the environment is represented and maintained explicitly. unfortunately  this approach is intractable except for bandit problems  where it gives rise to gittins indices  an optimal exploration method . in this paper  we explore ideas for making this method computationally tractable. we maintain a model of the environmentas a markov decision process. we sample finite-length trajectories from the infinite tree using ideas based on sparse sampling. finding the values of the nodes of this sparse subtree can then be expressed as an optimization problem  which we solve using linear programming. we illustrate this approach on a few domains and compare it with other exploration algorithms.
1 introduction
a key problem in reinforcement learning is posed by the need to explore the environment sufficiently in order to discover the sources of reward  while at the same time exploiting the knowledge that the agent has already  by taking actions that yield high return. the most popular techniques in the current literature  e.g. -greedy or boltzmann exploration  are not aimed at efficient exploration; instead  they ensure that action choices are randomized  which enables the agent to try out all actions in all states. this approach guarantees the convergence of reinforcement learning algorithms  but is not efficient from the point of view of the number of samples gathered  and may cause the agent to repeatedly try harmful actions. an extensive line of research has been devoted to directed exploration methods  most of which are based on heuristics that help the agent get data while trying to protect it from  danger   e.g.  thrun  1; meuleau and bourgine  1 . recently  several algorithms have been proposed which carry guarantees in terms of the number of samples necessary for the agent to attain almost optimal performance  kearns and singh  1; brafman and tennenholtz  1; strehl and littman  1; strehl et al.  1 .
　in this paper we explore a bayesian approach to exploration. the initial idea was due to bellman  who suggested keeping information about the agent's current state of knowledge  in addition to the model being learned by the agent. this method is bayes-optimal from the point of view of decision making  but its complexity grows exponentially with the horizon considered. this makes it inapplicable except for the case of bandit problems  where it gives rise to gittins indices. recently  a body of work on bayesian reinforcement learning has developed method for approximating the bayes-optimal procedure for the case of general mdps  dearden et al.  1; duff  1; wang et al.  1 . in this paper  we pursue a similar idea  but with two important differences. first  we propose to use linear programming in order to approximate the value function during this procedure. second  we use a sampling approximation based on the value function to expand the horizon of the decision making process.
　the paper is organized as follows. in section 1 we present the necessary background. in section 1 we present the linear programming approach to this problem. in section 1 we present empirical results in three domains.
1 background
a finite markov decision process  mdp  is a tuple
  where s is the finite state space  a is the finite action space  abilities  and
 by rmax. if the agent is in state s （ s and performs action a （ a  t  s a ，  is the distribution over next possible states and r s a  is the expected immediate reward. a deterministic stationary policy π : s ★ a is a function that determines what action to take depending on the current state. one of the goals of reinforcement learning is to find the policy that maximizes the expected discounted return received  given by   where rt is the reward received at time step t and γ （  1  is a discount factor. the value of a state v π s  is given by the expected discounted return received when executing policy π starting from state s:
		 1 
note that the upper bound on the value of any policy from any state is rmax/ 1   γ 
1 hyperstate mdps
in this paper we consider mdps in which the transition probabilities t and rewards r are unknown. in this situation  the agent maintains an estimate of the mdp model based on prior knowledge and its observations. for the transition probabilities  it is convenient to use a distribution that is closed under updates performed after observing a state transition. for mdps with finite state and action sets  the estimate of the transition probabilities can be maintained in a matrix p of size |s| 〜 |a| 〜 |s|. as shown in  martin  1   the matrix beta distribution is closed under updates and is a natural choice of distribution for this case. we denote by the current indexing parameters of the distribution  where maij is simply the number of observed transitions from state i to state j under action a. the expected transition probabilities based on this model are given
. letting the matrix
 denote an additional number of observed transitions from state i to state j under action a  in  martin  1  it is shown that the posterior distribution p with parameters is also a matrix beta distribution. in what follows  we will denote by dija  m  the new distribution obtained from m  through this operation  after observing on transition from state i to state j under action a.
　the estimated reward received when performing action a from state i can then be estimated by   where r ija is the average reward obtained when going for state i to state j under action a. these rewards can be summarized in a matrix r．.
　the matrix beta distribution can be viewed as a summary of the information that the agent has accumulated so far. this information  together with the original mdp states and actions  will form a new  hyperstate-mdp  hmdp . the actions in the hmdp are the same as in the original mdp. the states of the hmdp consist of pairings of states from s with possible information states m. a hyperstate  i m r．  contains an mdp state i  all the counts summarizing the agent's experience so far m  and all the estimates of the expected rewards r．. the counts define a distribution over all mdp models consistent with the data observed so far.
　at any given time step when action a is taken in the original mdp from state i  precisely one transition is observed  to some state j. so  from any given hyperstate  i m r．   we can consider taking all possible actions a （ a. if such an action results in a state j  the new hyperstate will be  j dija  m  dija  r．    where we have updated the innformation model as described above to reflect the new transition. we note that this update only affects the parameters for state i; all parameters for other states do not change. assuming that there are a fixed number of rewards r that can be observed  each hyperstatein the hmdp has at most |a|〜|s|〜r successors. moreover  each successor hyperstate is uniquely indexed. hence  the hmdp is an infinite tree. note also that the hmdp takes into account all possible transitions. hence  it is fully known  even though the mdp itself is not. one can express the bellman optimality equations for an hmdp as:

 solving this set of equations exactly would yield an optimal policy  bellman and kalaba  1; martin  1 . however  it is clear that the number of states in the hmdp is infinite  so an exact solution is intractable. it is also clear that the number of hyperstates increases exponentially with the depth of the tree  so an exact solution of a finite-horizon subset of the tree would be limited to very shallow depths. the focus of this paper is to present an algorithm for computing an empirically good learning policy from a finite subset of the tree. theoretical guarantees will be left for future work and we will focus on comparing the performance of our algorithm to other well-known exploration techniques.
1 related work
optimal learning has seen an increase in interest in recent years. much of the recent work has been spurred by  kearns and singh  1   where the authors introduce e1  an algorithm that is guaranteed to converge to an optimal learning policy in polynomial time. the drawbacks with this algorithm are its difficulty of implementation  and the intuition that it does not necessarily scale well to large state spaces. a practical model-based algorithm with similar guarantees was given in  brafman and tennenholtz  1 . more advanced model-based algorithms with pac guarantees  based on realtime dynamic programming  have been recently proposed in  strehl and littman  1; strehl et al.  1 . these methods explore by assigning a reward for exploratory actions. the drawback of both  kearns and singh  1  and  strehl et al.  1  is that their strategies are myopic - they do not consider the long term effects their actions may have on the total reward.
　using hyperstates as the model for exploration overcomes this myopic behavior. the concept of hyperstates was first introduced in  bellman and kalaba  1   which refers to this model as an adaptive control process. although mathematically rich  the paper presents no algorithmic approach for this idea. in  duff  1   various heuristic methods are presented for approximating an exact solution to the adaptive control process. these produce empirically good results  but with no general theoretical guarantees.
　wang et al  propose sampling from the infinite hypertree to produce a small  more manageable tree. they solve exactly the mdps along the path they are sampling  which gives them estimates for the values of different actions. then  they use thompson sampling to expand the tree locally under promising actions. the authors estimate unsampled regions by effectively 'copying' the last sampled node down to the current planning horizon. finally  sparse sampling  kearns et al.  1  is used to decide what action is optimal. a correction procedure is used when the desired horizon has not been reached. empirical results demonstrate that the algorithm performs well in comparison to other algorithms. furthermore  the sparse sampling technique enables the algorithm to be able to handle infinite state and action spaces.
1 solving the hyperstate mdp
in this paper we take an approach similar to  wang et al.  1  but with two key differences. first  action-value estimates are maintained for the state-action pairs of the original mdp  in addition to hyperstate values. the action-values are updated from the obtained samples using standard qlearning  sutton and barto  1 . hence  we always have an easy estimate of how good different actions are. second  we compute the value of the hyperstates using linear programming. linear programming  lp  is a technique that has been around for a long time  is well understood and has an elegant theory  puterman  1 . furthermore  recent work in approximate linear programming  de farias and roy  1; 1; hauskrecht and kveton  1   suggests that this approach can be used to work with continuous states using linear function approximation. we are hopeful that such techniques could eventually be used to generalize our approach to continuous mdps. however  for now we limit ourselves to discrete mdps.
　in a manner similar to  schweitzer and seidmann  1   the optimality equations for the hyperstate mdp can be formulated as a linear program as follows:
　　　　　　minimize	such that

 for all states i （ s  all actions a （ a and all information states m. we will refer to this formulation as the exact lp.
however  at a depth d  the number of hyperstates is at least
  more if we consider several possible reward levels . this means that for depth d the linear program has at leastvariables and constraints  a prohibitive amount.
　in  kearns et al.  1  the authors overcome a similar obstacle by constructing a sparse sample tree. we apply a similar idea here and construct a sampling tree incrementally. more precisely  we sample from the hyperstate mdp using our current estimates of the action values to decide what to do. at each node  we use the corresponding estimate of the dynamics of the system to sample the next node in the trajectory. each trajectory is completed to a desired horizon. we continue constructing trajectories in this way until a maximum desired number of samples has been reached.
　if we encounter a hyperstate for which a value was computed already using an lp  we use the lp-based estimate  which  in the limit  approaches the true optimal value of the corresponding mdp state . if such a value has not been computed  the action-value function for the mdp state is used instead. note that any unsampled regions constitute a subtree of our hyperstate mdp. these unsampled subtrees are estimated by setting the value of the hyperstate at the root of the subtree   i m r  to the value of the correspondinng mdp state i. with this method  we can effectively choose how many variables and constraints we want to use in the linear program.
　once the sampled hyper tree is built  we can solve it using linear programming. then  in order to gather more samples  we will choose action greedily based on the values computed by the linear program. if we enter an unsampled region  the action choice becomes greedy with respect to the value estimates attached to the original mdp states  until we decide to construct a new hyper tree and compute new values.

algorithm1explore levels maxsamples numsteps maxepochs 
1: initialize the matrix of counts to all 1s  uniform distribution 
1: while epochs ＋ maxepochs do1:construct a hyper-tree of a depth of levels using maxsamples sampled trajectories1:solve the lp of the sample hypertree1:choose action greedily based on hyperstate values1:observe transition and update hyper parameter for observed transition1:for i = 1 to numsteps do1:choose action greedily based either on current hyperstate value or current estimate of state values1:observe transition and update hyper parameters based on the observed transition1:end for1:epochs ○ epochs + numsteps1: end while
　algorithm 1 presents an outline for our approach. the maxepochs parameter defines how many total samples will be gathered from the environment. the levels parameter describes the depth to which we will expand the sampled hypertree. the maxsamples parameter controls how many trajectories are sampled  i.e.  the width of the tree . together  these two parameters control how large each linear program is. the numsteps parameter defines how many steps in the environment are taken before the lp solution is recomputed. this allows us to trade off the precision of the computation against time. obviously  if we re-compute the lp after every sample  we always generate samples based on the action that is truly believed to be best. if we wait longer  we may start selecting actions based on imperfect estimates of the values of the underlying mdp. however  this speeds up the computation significantly.
1 empirical results
we tested our algorithm on three different problems. for all domains  results are averaged over 1 independent runs. we compared our approach with a q-learning agent  with α =

figure 1: two-state  three-action mdp: dynamics  left  comparison with the other algorithm  center  and effect of parameters1  using an -greedy approach  with varying values of     right 
and with the algorithm described in  wang et al.  1 . we also experimented with a myopic agent  acting only based on immediate reward   and against an agent which chooses actions randomly. these last two algorithms did far worse than all the others  and so are omitted here for clarity. for our algorithm we experimentedwith differentparametersettings  as explained in more detail below. because the primal lp has more constraints than variables  the dual was solved instead.
　we also considered two different possible uses of the model that is constructed by our algorithm. in the first version  once the desired number of epochs has been completed  we perform dynamic programming using the acquired model  to determine a value function for the original mdp. in the second setting  we just use the values determined by the lp as value estimates. obviously  this latter approach is faster  since no extra computation is required  but it is also more imprecise.
small mdp
the first problem was a two-state  three-action mdp  described in the left panel of figure 1. intuitively  the best policy is to perform action 1 until state 1 is reached  then do action 1 and keep in the same state.
　the center graph compares our algorithm  with parameters levels = 1 numsteps = 1 maxsamples = 1  with qlearning using three different values of   1  1 and 1  and the approach by wang et al. for all algorithms we plot average return per episode. note that the implementation of the algorithm described in  wang et al.  1  was not able to handle more than 1 samples  which is why we performed the comparison at this level. although q-learning with outperformed all the other algorithms in this graph  we can see that if we allow our algorithm 1 samples to construct the hypertree  it performs better than the rest. interestingly  in this case  using the values from the lp also gives better performance than doing one last step of dynamic programming.
bandit problem
the second problem is similar to a bandit problem with two slot machines  bandits  and three actions. this problem was modeled using three states and three actions  with dynamics as given in figure 1. the dynamics are shown in the left panel for action 1  solid   action 1  dashed  and action 1  solid grey . the leftmost state corresponds to not winning  the middle state corresponds to winning under machine 1 and the rightmost state corresponds to winning under machine 1. there is no cost associated with gambling.
　for the comparisonbetween algorithms  we use parameters levels = 1 numsteps = 1 maxsamples = 1. for this problem  wang et al.  1 's algorithm was able to handle up to 1 samples as well. our algorithm outperformed all the others. in figure 1 we can see that even with only 1 samples  our algorithm does almost as well as the best qlearning algorithm.
grid world
the third problem is a 1 gridworld with a reward of +1 in the top right state and 1 everywhere else. there are four actions  north  south  west and east  with a 1 probability of remaining in the same state. if the agent tries to move into a wall it will deterministically stay in the same state. the parameter values used are levels = 1 numsteps = 1 maxsamples = 1.
　this is a larger problem than the first two  and all the other algorithms are 'stuck' with low reward  while our algorithm is able to find a good policy. in figure    it is clear that using dynamic programming to obtain state value estimates is advantageous for this task.
　it is interesting to see that in the first two problems using the values returned by the lp to estimate the state values yields better returns initially. this is probably because of the 'lookahead'nature of hmdp. however  for the gridworld problem the performance is much better when using the dp solution to estimate the state values. this behavior is most probably due to the size of the hypertree created. as the state and action space get bigger  more samples are needed to obtain state value estimates that are closer to the true values. it should be noted that even when the dp was not solved exactly  in all cases the performance when using hmdps was superior to the other algorithms. we note that we also experimented with boltzmann exploration algorithms  omitted from the plots for clarity   but in all cases their performance was similar to the q-learning algorithms.

figure 1: bandit problem: dynamics  left   comparison of different algorithms  center  and parameter influence  right 

figure 1: comparison of algorithms  left  and influence of parameters  right  in the gridworld taskrunning time
the results demonstrate that our approach has an advantage compared to the other methods in terms of accuracy. however  in terms of computation time  using hyperstates is obviously much slower than q-learning. table 1 plots the average running time per episode when solving the dp to estimate the state values versus using the values returned by the lp for the different domains  as well as the running time of  wang et al.  1 's algorithm. it is worth observing the difference in performance/runningtime depending on how the state values are estimated  solving with dp or using the values returned by the lp . although our algorithm is considerably slower when solving with dp  it is faster than  wang et al.  1 's algorithm when using the values returned by the lp solution. however  for the gridworld problem  although solving with hmdp is slower than  wang et al.  1 's algorithm  the savings when using  wang et al.  1 's algorithm are insignificant when compared against the superior performance demonstrated in figure 1.
hmdpwangusing dpno dpsmall mdp11e-1.1e-1bandit11e-1.1e-1grid world111table 1: comparison of running times
1 conclusions and future work
the aim of this paper was to demonstrate the performance of bayesian learning using sparse sampling and linear programming. it was observed that in general using hyperstates does lead to greater average returns when exploring an unknown environment than if we used simpler exploration techniques or even state-of-the-art techniques such as the algorithm presented in  wang et al.  1 . the drawback of this technique is that it may leave certain areas of the environment unexplored  but this could be the desired behavior. if you consider a physical agent  such as a robot  placed with the task of exploring a new environment  you would want the agent to avoid areas which could cause harm to its mechanical parts or even to other people around it  yet still explore sufficiently to try to reach states with high reward. a situation like that would justify the increase in computation. furthermore  the method of sparse sampling could allow the hypertree to have a greater depth  and thus  the agent could see states that other myopic methods may not be able to. this ability to value actions not only on their immediate but long-term effect makes bayesian learning a good candidate for exploratory agents.
　the next step is to obtain theoretical guarantees for our algorithm. we would also like to implement the algorithms presented in  strehl et al.  1  and possibly  duff  1  to empirically compare our algorithm against more of the latest advances in bayesian decision theory. it would also be interesting to try our algorithm against problems with continuous state and/or action spaces. placing a gaussian over sampled points could assist us in approximating the full hypertree better. further work could be to apply approximation techniques to the full hyper-tree  in a manner similar to what was done in  de farias and roy  1  and  hauskrecht and kveton  1  for mdps with very large state spaces.
1	acknowledgments
this work was supported in part by nserc and fqrnt.
