
decision making is particularly important for emergency managers as they often need to make quick and high quality decisions under stress based on scratch and inadequate information; and to follow expert knowledge or past experience.  the potential release of radioactive material from the guangdong nuclear power station  gnps  at daya bay  though is highly unlikely  could perhaps be the most dreaded disaster which would cause drastic damages to lives and properties.  the government of the hong kong special administrative region government  hksar  has therefore in 1 completed the daya bay contingency plan  dbcp  to prepare for such disasters.  to supplement the experts in assisting disaster managers with a useful tool to make better quality decisions based on well-structured  accurate  sufficient expert knowledge  a prototype expert system has been developed to cover two major areas of the plan  namely:  a  determination of activation level of the dbcp and provision of an action checklist and   b  recommendation on counter-measures.
1. introduction
decision making is often a challenging job for disaster managers as they often need to make quick and high quality decisions under stress based on scratch  inadequate  unstructured information  turban  1 .  the seriousness of consequences in the event of an accident in gnps resulting release of radioactive materials impose great pressure on disaster managers in the government of hksar .  unlike some large countries such as canada  russia  france  british etc which have long history of developing nuclear industry and some have experienced different scale of accidents e.g. the well-known chernobyl accident  1  and the three mile island accident  1   hk has little experience and relative low public awareness in nuclear industry.  even though we have a comprehensive contingency plan with detailed rules and procedures in hardcopy format and human experts  it is difficult and timeconsuming for disaster managers to retrieve  study and organise such information and expert knowledge to cope with real emergencies under great pressure.
1. the daya bay contingency plan
the gnps at daya bay is located at about 1 km from the northeast coast of hk. it began commercial operation in 1. the pressurized water reactors used in the plant adopt a successful french reactor design and the safety review has been conducted by the international atomic energy agency  iaea  which found to be in accordance with international standards and operated to international practices. nevertheless  as part of the emergency planning system  the government of hksar has prepared a comprehensive contingency plan  hksar  1 . the main purpose of emergency planning is to ensure that proper and prompt actions are taken in an accident to protect the health and safety of the general public.
1. an intelligent expert system
according to martin  and medsker   expert system  es    reproduced the reasoning process a human decision maker would go through in reaching a decision  diagnosing a problem  or suggesting a course of action . the components of an es include:  a  knowledge base which contains heuristic and judgmental programs;  b  inference engine consisting of the reasoning logic; and  c  user interface where the user supplies data and receives an answer  and often with the reasoning supporting the answer. exsys professional  version 1.1  by the exsys inc.  note  has been selected to build the prototype because it is user-friendly shell and easy to understand and use.  domain knowledge is captured in a set of rules entered in the system's knowledge base. the system along with other information contained in the working memory to solve a problem. active research of es on process planning and scheduling can be found in kingston et al.   boutilier et al.  and liu .
1. development of dbcp advisor
1 overall system architecture


           figure 1 - overall system architecture note: exsys professional is a product of the exsys inc. founded in 1  renamed as multilogic afterwards  in usa .the company is one of the longest lived expert system companies in the market.
1 design structure
the system is divided into two parts    a  activation level and action checklist; and  b  recommended countermeasures. an overview of the design structure of the prototype is as follows:
activation level of dbcp and provision of an action checklist
the system will guide managers through the stipulated procedures under the existing dbcp according to different sources of notification.

figure 1 - overview of system structure
source of notification
there are six major sources of notification under the dbcp.  they are:  1  from the guangdong authority   1  from the hong kong nuclear investment company  hknic    1  from the international atomic energy agency  iaea    1  hong kong observatory  hko 's radiation monitory network  rmn    1  water supplies department's water contamination monitoring system  wcms   and  1  any other possible sources.  for example  when the source of notification is from the guangdong authority  the system will ask the disaster managers to supply information on whether it is a request for assistance and advise appropriate action. expert rule involved will look like this:
for example :  
if the notification message from the chinese authority received at hko and request for assistance is yes
then hko refers message to security bureau  sb 
emergency level of accident
after confirming the notification message  the system will guide the disaster managers to clarify the emergency level of the accident in order to determine the activation level of the dbcp. the management of the gnps has adopted the iaea's four-category system for classifying nuclear power emergencies:  i  emergency standby  ii  plant emergency  iii  site emergency and  iv  off-site emergency.
activation level of dbcp and action checklist
under the dbcp  according to information on the emergency level and other relevant information  the sb will have to determine the final activation level of on recommendation by operational departments. there are 1 levels of activation  namely:  1  observation level   1  ready level   1  partial activation level and  1  full activation level  which is corresponding to the emergency level as illustrated in figure 1.
   on recommendation of the activation level of the dbcp  dbcp advisor will provide an action checklist as follows:
for example:
if the notification message from the chinese authority received at
hko
and request for assistance is no
and the notified emergency level is off-site emergency
then hko alerts sb - confidence=1
and recommends full activation of dbcp - confidence=1 and hko initiates cascade calls according to activation level decided by sb - confidence=1
and hko consults dh and advises emsd immediately with respect to ping chau/mirs bay  monitoring centres and border controls - confidence=1

figure 1 - determination of activation level of dbcp
recommendation on counter-measures during emergencies in gnps
this part of the system advises disaster managers of the appropriate counter-measures. it is a very complicated process in determining what are the appropriate countermeasures to be taken during different levels of emergency in gnps. it involves expert advice after numerous discussion on various risk factors among technical departments.
initially  experts have to assess the  a  initial risk of hong kong  hk  by referencing to the  a  meteorological risk and the  b  plant status; and finally to determine the total risk when assessment results of  b  radiological risk is available at a later stage. depending on the degree on total risk to hk  experts will recommend corresponding counter-measures.
 a  initial risk - at the early stage of activation of the dbcp  initial risk is assessed by looking at  a  meteorological risk and  b  plant risk from different sources.
 a  meteorological risk level  mrl  - two main attributes:  i  plume track coming from a particular quadrant of gnps and  ii  plume arrival time will be considered. plume is the released radioactive fission products carried by wind which behave in a way similar to a cloud of smoke dispersing into the atmosphere and depositing some of its content on the ground. the concentration of radioactive materials in the plume decreases with distance from the site.
 i  plume track coming from a particular quadrant ofgnps - it is measured in terms of degrees within 1 degrees  1 degree=due north . since gnps is located at the north-east  ne  direction from hk  wind blowing from sw  i.e. 1 to 1 degrees  will bring the plume to hk which is very unfavourable to hk. though plumes towards nw and se quadrants of gnps are not directly threatening hk  there is the possibility of change in direction  so it is still unfavourable to hk.   if the plume is towards ne quadrant of gnps which will blow the radioactive materials away from hk towards the opposite direction  it will therefore be favourable to hk. hko will get meteorological information from the chinese authority during emergencies which will be verified against measurement by hko.
 ii  plume arrival time - plume movement indicates whether sufficient tie is available for preparation and implementation of counter-measures. it is measured in terms of minutes  according to the hko's guideline  plume arrived within 1 minutes will threaten hk because it takes at least two hours to evacuate residents/visitors from the ping chau region and clearance of vessels from mirs bay; and carry out other counter-measures.  hko will get information on plume movement from the chinese authority during emergencies. the accident consequence assessment system  acas  which is a computerized system operated by hko will base on meteorological data and radiological information  models the transport and dispersion rates of the released radioactive materials. fuzzy logic - the process to derive the mrl involves expert advice on plume track direction and plume arrival time using linguistic terms such as very favourable  very fast  which are descriptive in nature. the resulted mrl is also description in linguistic terms: high/medium/low. exsys professional can handle inexact reasoning by fuzzy logic which deals with uncertain knowledge. fuzzy logic is primarily concerned with quantifying and reasoning about vague or fuzzy terms  called fuzzy variables  that appear in our natural language. it provides a rigorous mathematical method to handle parameters that are defined subjectively. it does this by associating membership grades to different values of such parameters and carrying out calculations on those membership grades  altrock  1 .
table 1 -  linguistic terms of gnps quadrant which the plume track comes from
classdescription 1 
  	1 	and
 1very unfavourable  - since gnps is located at the north-east direction from hk  plume comes from the south-west quadrant of gnps    i.e.
1 to 1 degrees  means towards hk. 1 
 =1 and
 =1    or  =1 and
 =1unfavourable - though plume moving northwest and south-east quadrants of gnps are not directly threatening hk  there is the possibility of change in direction. so  it is still unfavourable. 1 
  1 and  1favourable - plume moving north-east quadrant of gnps will be away from hk  so it is favourable to hk.the fuzzy inference can identify the rules that apply to the current situation and can compute the values of the output linguistic variable. we first establish rules with linguistic terms according to expert advice  quadrant of gnps which the plume track is coming from and plume arrival time are associated with the output variable i.e. mrl in linguistic terms  high/medium/low  as shown in the table 1.
table 1 -  linguistic terms of  plume arrival time
classdescription 1   1 min.very fast - plume moving quickly and will arrive hk within one hour implying very little time to arrange counter-measures. 1 
 = 1 and  =1 min.fast - plume moving quickly and will arrive hk within two hours implying the arrangement of counter-measures must be taken timely. 1   1 min.slow - plume moving relatively slow with sufficient time to prepare for /implementing counter-measures.table 1 - determination of mrl
gnps quadrantplume arrival timemeteorological risk level  mrl very unfavourablevery fast  or fasthighvery unfavourableslowmediumunfavourablevery fast or fastmediumunfavourableslowmediumfavourablevery fasthighfavourablefastmediumfavourableslowlowat the end of fuzzy inference  the result for mrl is given as a linguistic variable. the system will translate the resulted mrl into mathematical values i.e. defuzzification. exsys professional follows the following procedures to determine the fuzzy  mrl  defuzzified  :
 fuzzy mrl  = #   weighting factor assigned to each possible result of mrl  i.e. high/medium/low   * corresponding confidence level of rules 
 b  plant status   ps  - in order to derive the initial risk  we have also to derive the level of plant risk from the plant status at gnps. it is measured by the level of defence-in-depth degradation which is classified into three levels with reference to the international nuclear event scale. the higher the level of defence-in-depth degradation  the lower the safety level of the plant  hence the higher the risk.
table 1- classification of the plant status
p sdefence-in-depth degradationpoorserious incident - near accident with no safety layers remaining.badincident - incidents with significant failures in safety provisions.fairanomaly 	- 	anomaly 	beyond 	the 	authorized operation regime.in the system  the weighting assigned for different status of the plant to indicate its relative importance are set as below:
table 1 - relative weighting of the plant status
psrelative weightingpoor1bad1fair1taking into account the fuzzy scores and weighting assigned by experts of  a  mrl and  b  ps  the initial risk score can be derived as shown below:
table 1 - example on determination of initial risk score
factorsfuzzy score
  si weighting  wi initial risk score
 1 
 si x wi mrl*111p s*111뫉 wi1뫉  si x wi 1* fuzzy score is derived by system on mrl & relative weighting of specific ps assigned by experts
* weighting is assigned by experts for mrl & ps
* assumed that the fuzzy mrl is 1 which is normalized to be 1 andplant  status is bad  i.e. relative weighting=1  which is normalized to be 1.
table 1 - the classification of environmental gamma dose-rate
class  in msv descriptionx  1the level of x will not impose any harmful effect on public health.1  = x  = 1the level of x will impose certain harmful effect on public health  sheltering is recommended.x   1the level of x will impose serious harmful effect on public health  evacuation is recommended.radiological risk - it is assessed by three main attributes:  i  environmental gamma dose-rate;  ii  radionuclide concentration activity in food; and  iii  radionuclide concentration activity in water.
taking into account the three attributes  the  b  radiological risk can be derived as below:
table 1- determination of radiological risk
environmental
gamma doserate  x radionuclide
concentration
activity in food  y radionuclide
concentration
activity in water  z radiological riskx   1yes or noyes or nohigh1 =  x =  1yesyes or nohigh1 =  x =  1noyeshigh1 =  x =  1nonomedium1   xyesyeshigh1   xyesnomedium1   xnoyesmedium1   xnonolowin the system  the relative weighting assigned for different levels of radiological risk are set as below:
table 1 -  relative weighting of the radiological risk
radiological riskrelative weightinghigh1medium1low1taking into account the fuzzy scores and weighting assigned by experts of  a  initial risk score and the  b  radiological risk score  the total risk
score can be derived. an example is shown below:
table 1 - example on determination of total risk score
factors scores  st weighting   wt total risk score  1   st x wt initial risk score  irs  뫉  si x wi *11.1radiological risk score  rrs 11뫉 wt1뫉  st x wt 1  * weighting factor refers to the weighting on irs & rrs assigned by experts. assumed that the radiological risk is medium  i.e. relative weighting =1  which is normalized to be 1.
1 user interface

figure 1 - question screen on part a
activation level of dbcp
question screens - when user starts the exsys program on   activation level and action checklist    dbcp advisor  will pop-up a series of  screens to ask for the required information in multiple choice format as follows: advisory screen -  at the end  dbcp advisor will popup an advisory screen with recommended actions as below:

figure 1 - advisory screen on part a
recommended counter-measures
data file - when the user starts the exsys program on  recommended counter-measures   dbcp advisor will get information on relevant parameters to derive the total risk level by calling from external databases in relevant government departments through  .dat files. an example is as follows:
table 1 - parameters in the data files
gnps quadrant1 degreesplume arrival time1 minutesdefence-in-depth degradation1 - significant failure of safety provisionenvironmental gamma doserate  x 1   i.e.  1 =x =1 radionuclide concentration activity  rca  in food1   i.e.  rca in food exceeds the intervention level = yes radionuclide concentration activity  rac  in water1  i.e.  rca in water exceeds the intervention level = no advisory screen - at the end  dbcp advisor will derive the total risk score  trs . an example is as follows:
 initial risk level irs  * .1 +  radiological risk score
 rrs *.1  = 1*.1 + 1 *.1 = 1
relevant rule will finally be fired to advise user of the appropriate counter measures as follows:
if  irs *.1 +  rrs  *.1*.1  1
then run  trial1.bat /b 
and immediate evacuation of residents/visitors from ping chau and clearance of vessels from mirs bay area if sufficient time is available or temporary sheltering if running out of time
and radiological screening at the border controls points for returnees from area within 1 km of the gnps be implemented
and monitoring of foodstuff and livestock as well as water be initiated and
and all monitoring centres be opened for evacuees from ping
chau and mirs bay  and for the returnees via lo wu  man kam to and sha tau kok. provided that these centres are unlikely affected by the plume
and distribution of iodinetablets when situation required
1. knowledge 	verification 	and 	system evaluation
1 knowledge verification
knowledge must be verified for  redundancy  inconsistency  incompleteness  circularity  and other errors   tepandi  1  quoted in  h. o. nourse et.al.  1 . verification has also been characterised as the process of determining  if the system was built right    o'keefe et. al.  1 quoted in  h. o. nourse et.al.  1 . while t. j. o'leary  suggested that verification is the   authentication that the formulated problem contains the actual problem in its entirety and is sufficiently well structured to permit the derivation of a sufficiently credible solution .
   exsys has a self-contained verification function which help identify utilisation  frequency of usage  of rules  qualifiers  variables and choices  and which of them have never been used during the verification. this function are frequently performed during the development of the prototype until no error is identified and rules which have never been used are picked up for testing by case simulation. besides  testing cases have been conducted: 1 cases for part a and 1 cases for part b. these are critical cases and testing results are quite satisfactory.
1 system evaluation
t. l. o'leary et. al.  defined evaluation as  the process of examining an expert system's ability to solve real-world problems in a particular problem domain. evaluation focuses on the expert system and the real world. 
   validation - according to t. j. o'leary et. al.   turing test is to test the system's ability to supply response comparable to an expert's decision  wallace  1 . in part b  data from a total of four important exercises were used for validation. the experts considered that the recommendation by dbcp advisor were very close to the human experts' recommendations.
   user evaluation - a user survey has also been conducted in the form of a user survey to measures  1  usefulness  1  logic  1  system design  1  user-friendliness and  1  potential further development of the system. twelve colleagues in the emergency support unit of the sb and hko participated in the survey. results of the survey indicated that dbcp advisor is well received. the mean scores of colleagues strongly agree  1%  and agree  1%  in terms of the system's usefulness  system logic  design  user-friendliness as well as the potential for further development are very satisfactory.
1. limitations
the main difficulty involved is that the process to arrive at final consensus on recommended counter-measures is a very complicated process involving much discussion and exchange of view and opinion among many experts in different fields. the prototype can only base on limited scope of knowledge provided by the nuclear plant experts in a short period of time and numerous determining factors that leading to the recommendations have been simplified. in real situation  sometimes experts have to make decision base on personal experience and intuitive judgement.
1. recommendation and further study
given the contributions of the prototype and positive user feedback  the usefulness of the system can be further enhanced by developing it into a full version  of course enriching the knowledge-base is required. as the system has very high compatibility with other external programs and can accept inputs from various kinds of database files  it can be further enhanced through system integration. on the output end  the system's capability to link to the internet provides great flexibility to enrich the system with supplementary information and explanation to the users. nevertheless the system currently lacks learning capability  further study to develop a hybrid system with the introduction of the neuro network technique  or to build up historical databases from previous exercises may enable learning through case-base reasoning  zhu and yang  1; eyke hullermeier  1   data mining approaches  liu and sin  1  and other intelligent business advisory systems  liu  1; liu et al  1; liu and jane  1 . finally  further study on the feasibility to apply the system to other contingency plans such as the plans on natural disaster and aircraft crash etc. will also generate benefits in ensuring security in the society.
references
 boutilier et al.  1  c. boutilier  r. brafman  c. geib. processes: towards a synthesis of classical and decision
theoretic planning. proceedings of ijcai' 1  pages 1  1.
 constantin  1  constantin von altrock. fuzzy logic and neurofuzzy applications explained. prentice hall  englewood cliffs  new jersey  1.
 hullermeier  1  e. hullermeier. toward a probabilistic formalization of case-based inference. proceedings of ijcai' 1  1.
 kingston et al.  1  j. kingston  a. griffith and t. lydiard. multi-perspective modelling of the air compaign planning process. proceedings of ijcai' 1  pages 1  1.
 liu  1  liu  n.k. an intelligent business advisory system for stock investment. encyclopedia of microcomputers  1: 1  1.
 liu and jane  1  james n.k. liu and jane you. an agentbased intelligent system for e-commerce applications. proceedings of international icsc symposium on multiagents and mobile agents in virtual organizations and ecommerce  mama'1   december 1  1 
wollongong  australia.
 liu and sin  1  liu  j.n.k. and sin  d.k.y. a data mining approach for maintenance scheduling. international journal of engineering intelligent systems  1 : 1  1.
 liu et al.  1  liu  j.  mak  t.  tang  b  ma  k. and tsang  z. an intelligent job counseling system. hing-yan lee and hiroshi motoda  eds.  springer-verlag  lecture notes in artificial intelligence 1: 1  1.
 martin  1  martin james. building expert systems : a tutorial. englewood cliffs  n. j. :prentice hall  1.
 medsker  1  medsker larry. design and development of expert systems and neural networks. new york: macmillan; toronto: maxwell macmillan canada; new york: maxwell macmillan international  1.
 nourse  1  nourse hugh o.  watson hugh j.  bostrom robert p.  and gatewood robert d. evaluation of a generic expert system for corporate real estate disposition. proceedings of the twenty-seventh annual hawaii international conference on system sciences. edited by nunamaker jay f. jr. and sprague ralph h.  jr.  vol iii  ieee computer society press  1.
 leary  1  o'leary timothy j.  goul michael  moffitt kathleen e.  radwan a. essam validating expert systems. ieee expert  1  1 : pages1  1.
 hksar  1  the government of hong kong special
administrative region daya bay contingency plan  1.
 turban  1  turban efraim. decision support and expert systems: management support systems. new york: macmillan; toronto: maxwell macmillan canada; new york: maxwell macmillan international  1.
 wallace  1  wallace william a. and de balogh frank. decision support systems for disaster management. public administration review    special issue   1: 1  jan.1.
 zhu and yang  1  j. zhu and q. yang. remembering to add: competence-preserving case-addition policies for case base maintenance. proceedings of ijcai' 1  1.
expertclerk: navigating shoppers' buying process with the combination of asking and proposing
hideo shimazu
nec corporation
1 takayama  ikoma  nara  japan shimazu ccm.cl.nec.co.jpabstract
this paper analyzes conversation models of human salesclerks interacting with customers. the goal of a salesclerk is to effectively match a customer's buying points and a product's selling points. to achieve this  the salesclerk alternates among asking questions  proposing sample goods  and observing the customer's responses. based on this analysis  we developed expertclerk  an agent system that imitates a human salesclerk and navigates web shoppers in merchandise databases. in the system  a character agent talks with a shopper in a natural language and consolidates the shopper's request by narrowing down a list of many matching goods by asking effective questions using entropy  navigation by asking . then  it shows three contrasting samples with explanations of their selling points  navigation by proposing . this cycle is repeated until the shopper finds an appropriate good. evaluations show that the combination of navigation by asking and navigation by proposing works as most effectively as human salesclerks.
1	introduction
this paper describes a salesclerk conversation model that interacts with customers. it is used in e-tailer web sites to help customers easily find  compare  and decide on appropriate goods from among a lot of merchandise goods.
모specifically  we have implemented expertclerk as an agent system that imitates a human salesclerk and navigates web shoppers in merchandise databases. in the system  a character agent talks with a shopper in a natural language and consolidates the shopper's request by narrowing down a list of many matching goods by asking effective questions using entropy  navigation by asking . then  expertclerk shows three contrasting samples with explanations of their selling points  navigation by proposing . this cycle is repeated until the shopper finds an appropriate good.
모the motivation behind this research was two-fold. first  we have developed several knowledge retrieval systems for customer support using conversational case-based reasoning techniques  aha & breslow  1 . recently  requests have been increasing by internal and external customers to apply these techniques into e-tailer web sites to help shoppers find and decide on goods in large-scale merchandise databases. this prompted us to develop prototype systems and show them to our clients. however  their responses were not good. this is because the system designs were based on conversations between customers of products and customer support agents. most complaints centered around the conversations in the shops being different from those in customer supports. other complaints were heard about the knowledge retrieval systems not having a different conversation structure and control.
모second  the patricia seybold group reported the top 1 e-tailer ranking of the 1 pre-christmas season  seybold  1   and lands' end  landsend.com  earned the numberone spot. according to their survey  it was the company's superb customer service that made it first. the web site provides the ability to press a button and talk to a customer service representative from the web site via live chat or phone for shopping help  professional advice  or gift suggestions. we were surprised that even the top e-tailer site relies on the conversation skills of human representatives.
모we surveyed various conversation skill's manuals for human salesclerks. the survey taught us that salesclerks effectively match customers' buying points and products' selling points. today's web shop sites including lands' end do not have such human-like salesclerk-customer conversational interfaces. however  expertclerk was designed and developed to imitate the conversation techniques of actual salesclerks.
1	conversations between salesclerks and shoppers
in a conversation between a shopper and a salesclerk  the shopper plays the role of a decision-maker and the salesclerk plays the role of an adviser. the conversation is a clarifying process of the subconscious desire of the decision-maker. the adviser is a catalyst that promotes the shopper's decisionmaking. the following sequence shows a typical conversation between a shopper and a salesclerk  s: shopper  c:
clerk .
s: please show me that blouse.
c: ok. we have a similar color that may also suit you. and  this is a blouse of the same type but with a different design.
s: well  the design is fine  but the neck looks very tight.
c: how about this one if you prefer a loose neckband  that one also has a loose neckband and an interesting design.
s: i like this one. how much is it 
c: it is $1.
s: wow. $1 is a bit too expensive. c: how about this one  it has a similar design and color  but the price is only $1. the material is polyester.
s: ok  i'll take it. c: thank you.

figure 1: aidca model
모figure 1 show a typical mental process of shoppers  called aidca model  shigeta  1 . a shopper first turns his/her attention towards a specific good and becomes interested in the good. the shopper then starts to desire the good and becomes perplexed about the decision  i.e.  in the perplexity cycle of desire  comparison and association. the shopper eventually decides to buy the good with conviction  and moves to the purchasing action.
모a salesclerk follows the shopper's aidca process to promote each step. the salesclerk watches for a chance and approaches the new shopper. the approach must be done at the appropriate time. if it is too soon  the shopper guards against the salesclerk. if it is too late  the salesclerk is regarded as a dull clerk. when the conversation starts  the salesclerk asks a few questions to identify the shopper's necessary conditions  buying points . then  the salesclerk picks a few sample goods and shows them to the shopper and explains their selling points.
모we analyzed a real conversation corpus between shoppers and salesclerks and interviewed senior salesclerks. in order to understand a customer's buying points  a salesclerk alternates between asking questions and proposing sample goods. many salesclerks said the best approach is to present three sample goods at the same time. according to these salesclerks  shoppers become frustrated if they are shown only one or two samples. on the other hand  if four or more samples are presented  they become puzzled as to which to choose. each of these three samples must have clear selling points that differentiate itself from the other two. the salesclerk explains the selling points  like for example   this is twice as expensive as those because it is made of silk and the other two are made of polyester . while hearing the explanations  the shopper can more easily exclude one of the three proposed goods with a specific reason  like  this one is too dark for me compared with the other two . the salesclerk excludes it and chooses a new one satisfying the shopper's comment. the salesclerk repeats picking sample goods  explaining their selling points  and modifying his/her sample picking strategy by observing the customer's responses.
모it is also very important that the salesclerk presents appropriate sample goods. if a proposed sample is opposite to a shopper's preference  the shopper may feel distrust towards the opinion of the salesclerk. in order to avoid this  the salesclerk must first ask a few effective questions to infer the shopper's preference.
1	design decision
1	modeling a salesclerk's action sequence
expertclerk was designed to model the typical action sequence of human salesclerks. it has the following steps:
1. approach: expertclerk approaches a shopper.
1. navigate by asking appropriate questions: expertclerkfinds a set of goods matching the shopper's request. if too many matching goods exist  expertclerk narrows them down by asking a few questions using entropy which effectively discriminates the shopper's intention.
1. navigate by proposing three sample goods: if the shopper can not identify his/her own intention  expertclerk shows three contrasting samples with explanations of their selling points.
1. observe: expertclerk observes the shopper's reactionson likes/dislikes and why.
1. repair: expertclerk modifies the proposal to fit the shopper's responses.
모the process repeats until the shopper finds an appropriate good. the key to expertclerk effectiveness is its combination of navigation by asking and navigation by proposing. the following sections explain these features.
1	merchandise database
expertclerk is a front-end system for a merchandise database. we assume that merchandise records are represented as a flat record of n-ary fields  and stored in a table in a commercial rdbms. each field stores an attribute of merchandise  such as price  color  material  the country of origin  brand  and so on. for each record attribute  a conceptual hierarchy is defined by domain experts.
모this hierarchy represents a classification and is defined as a discrimination tree. each node in the hierarchy is a question node that subdivides the set of nodes stored underneath it. each child node represents a different answer to the question posed by its parent. each leaf node refers to a set of merchandise records which satisfy the above discrimination conditions. for example  the  price  attribute may have a question  which price range are you interested in  . the possible answers may be divided into  very expensive    expensive    reasonable    fairly cheap   or  very cheap . they can be subdivided into more detailed categories.  very expensive  may be divided into  over $1   and  between $1 and $1  . each merchandise record is linked from several different leaf nodes of different hierarchies.
모these questions and possible answers are used to discriminate merchandise records. the discrimination capability of each question is different. it depends on the set of remaining merchandise records. for example   which price range are you interested in   is useful if the remaining goods have various price ranges. however  the same question is meaningless if all of the remaining goods are over $1.
1	navigation by asking
navigation by asking calculates the information gain of possible questions and sorts them according to their statistical efficiency. it works as follows:
 1  extracting questions. within each question hierarchy having leaf-nodes pointing to merchandise records  the paths from each of these leaf-nodes are traversed in the direction of the root  and the first question node reached that is common to all of these leaves is extracted as a question node. then  the nodes on the level just below this question node are extracted as its answer nodes.
 1  calculating the information amount of question nodes. questions are determined from their calculated information amount. the algorithm is based on id1's  quinlan  1  information gain approach. let c = {r1  r1  ...  rk} be the set of retrieved question nodes and let hj be the retrieval counts of rj 1 뫞 j 뫞 k . then  the occurrence probability pj of rj is calculated by equation  1  . the entropy m c  of c is given by equation  1 . when c is divided into subsets c1  c1  ...  cn by answer nodes a1  a1  ... an of question node a  the expected information b c a  is given by equation  1 . the information gain c a  gained by question node a is given by equation  1 . dividing c into subsets by using question node a which maximizes gain c a  should narrow down the number of result sets efficiently.
		 1 
	 1  	 1 
	gain c a 	=	m c    b c a 	 1 
1	navigation by proposing
it is very annoying for shoppers to be asked many questions. after merchandise records are narrowed down to a pre-defined threshold number after several questions  expertclerk changes its conversation mode from navigation by asking to navigation by proposing. in the navigation by proposing mode  three goods as the most contrasting among remaining goods are selected. they are selected by the following algorithm.
  the first sample good  1st-sg  is the goods record closest to the center point of the set. its selling points directly reflect the customer's request.
  the second sample good  1nd-sg  is the goods record positioned most distantly from the center point of the set. the selling points of 1nd-sg clearly differentiate itself from 1st-sg.
  the third sample goods  1rd-sg  is the goods record positioned most distantly from 1nd-sg. 1rd-sg has selling points which differentiate itself from 1st-sg and 1ndsg.
모let g = {g1  g1  ...  gk} be the set of retrieved goods records and let gi = {fi1 fi1 ....fin} be the set of attribute values. then the median gmed of g is calculated by equation  1  and the standard deviation of the ith attribute value set gdev i  is calculated by equation  1 

모the distance  d  between gmed and a sample gi is given by equation  1 .

where gmed j  is the j-th attribute of gmed  gi j  is the j-th attribute of gi  and wj is the j-th attribute weight.
모1st-sg is a record whose distance from gmed d gmed gi  is the shortest among gj 1 뫞 j 뫞 k . 1nd-sg gmed g gi 1ndis the sg  is a record whose distance from gmed d largest among gj 1 뫞 j 뫞 k . 1rd-sg is a record whose
distance from 1 뫞 j 뫞 kg . figure 1 shows the dot frequency diagram1nd sg d g1nd sg gi  is the largest among gj
of a set of retrieved goods records. three points represent the median and range of the set. 1st-sg is located at the center of the diagram  1nd-sg is located at the right edge of the diagram  and 1rd-sg is at the left edge of the diagram. a salesclerk's proposing strategy is modified by the shopper's response to the proposal. a shopper's response is typically expressed with  i don't like this sample because of this attribute value a. i want the attribute value to be more/less than this value v  . the salesclerk's next proposal reflects this response.


figure 1: the expertclerk systemfigure 1: narrowing down records by manipulating three samples
1	the expertclerk system
figure 1 shows the expertclerk system structure. the user interaction module uses a natural language dialog system  shimazu et al.  1  with a character animation. it translates a user's request into a corresponding sql query and issues the query to a backward relational dbms. the module is integrated with the navigation by proposing module and the navigation by asking module and calls these modules when either asking questions to a user or proposing three samples to the user. the navigation by asking module refers to conceptual hierarchies each of which corresponds to each attribute of the backward relational dbms.
모the selection strategy of the two navigation modules is defined and can be changed by a few parameters. a system administrator defines the parameters  such as  if the number of matching records becomes less than seven  change to navigation by proposing  or  use both narrowing modules at random . figure 1 shows the screen image of expertclerk.

figure 1: the expertclerk screen image
1	evaluation
we evaluated the efficiency and precision of expertclerk. we used a small wine database of 1 wine records. this database is used to recommend wines meeting a user's specific conditions. all of the wine records have the same structure and each record has ten attribute values. for each attribute  a similarity measure and weight are defined.
모since many of the records in the database have unique solutions  we could not evaluate the retrieval precision based on the correctness of the retrieved merchandise record. instead  we tested expertclerk using aha and breslow's leave-one-in testing methodology  aha & breslow  1  in which each merchandise record is used once as a test record  but without removing it from the merchandise record database during testing. the leave-one-in method randomly selects a test record from the database. a conversation starts by activating retrieval methods in expertclerk. a user interacts with expertclerk by entering a query or excluding one of three sample wine records proposed by expertclerk. the conversation ends after the interaction exceeds m times or after retrieved records are narrowed down to the test record. the efficiency is measured by the degree of narrowing down after n  n 뫞 m  interactions. the precision is measured by examining whether the retrieved records match the test record.

figure 1: degree of narrowing down records at each interaction
모figure 1 shows how effectively records are narrowed down per interaction by navigation by asking  nba   navigation by proposing  nbp  and their combination  nba+nbp  . the first and second interaction of nba+nbp is nba and the following interactions are nbp.
모nba narrows down possible records to between one third and one half at each interaction  but it can narrow down little at the third  fourth  or fifth interaction. nbp with no customer input can not narrow down records effectively. after a few interactions with a user's response  however  it can work well and the remaining records can decrease rapidly. nba+nbp takes the best parts of nba and nbp. at early interactions  nba works better than nbp. at later interactions  nbp works better than nba. in addition  because navigation by proposing sounds more gentle than navigation by asking as the style of conversation  the combination of nba and nbp is the best among the three.
모figure 1 shows how precisely records are narrowed down per interaction. the figure shows the similarity between a test record and each retrieved record set  1st-sg  1nd-sg  and 1rd-sg  per interaction. the higher border shows the best similarity values for a test record among 1st-sg  1nd-sg  and 1rd-sg and the lower border shows the worst similarity values for a test record among them. the dashed line shows the similarity value between a test record and the center of each retrieved record set. after the first and second interactions  the precision value of three samples are refined  and at the

figure 1: precision during narrowing down records at each
interaction
third interaction  the three samples shown to the customer are nearly appropriate with higher than 1% accuracy.

figure 1: generated selling points at each interaction
모figure 1 shows how many selling points can be presented to a customer per interaction. at early interactions  each of 1st-sg  1nd-sg  and 1rd-sg has its own selling points. the number of selling point decreases at later interactions because the retrieved records eventually becomes homogeneous and the three samples also become similar.
1	related research
several research projects have focused on user-interface issues of conversational case-based reasoning  cbr . our research was inspired by findme  hammond et al.  1; burke et al.  1  and its enhanced version the wasabi personal shopper  wps   burke et al.  1 . findme combines instance-based browsing and tweaking by difference. the system shows a user an example of retrieved results. by critiquing the example  the user then indicates his/her preferences and controls the system's retrieval strategies. nacodae  aha et al.  1  has a dialogue-inference mechanism to increase the conversational efficiency. it infers the details of a user's problem from his/her incomplete text description by using model-based reasoning. it is especially useful in front office systems like help-desk or sales-support systems. wilke  lenz and wess  wilke et al.  1  surveyed various research projects that applied cbr into sales support solutions on the internet. they pointed out the importance of negotiation  a process in which two parties bargain resources for an intended gain. they classify negotiation into competitive negotiation and cooperative negotiation. the former is used in various internet auction sites. expertclerk can be categorized under the latter.
모the incremental query modification technique of navigation by proposing is influenced by the relevance feedback technique used in the smart system  salton  1 . relevance feedback incrementally shifts the query vector to the center of a related document cluster. the navigation by asking module is based on id1  quinlan  1  and was first described in our previous work  expertguide  shimazu et al.  1 . selling points finding can be regarded as a salient feature extraction problem and is discussed in many cbr research projects  kolodner  1 . kasbah  chavez & maes  1  is an intelligent agent with negotiation capabilities and has a similar idea to selling point. negotiation rules are defined for each attribute of goods and activated during every negotiation step.
모several cbr projects have applied cbr techniques into commercial rdbms  shimazu et al.  1; watson & gardingen  1; burke et al.  1 . it is important to design an architecture in a simple and scalable manner if it is to be used in large-scale web shopping sites.
1	conclusion
this paper analyzed conversation models of salesclerks interacting with customers. a salesclerk alternates among asking questions  proposing sample goods  and observing the customer's responses. based on this analysis  we developed expertclerk  an agent system that imitates a human salesclerk and navigates web shoppers in merchandise databases. in the system  a character agent talks with a shopper in a natural language and consolidates the shopper's request by narrowing down a list of many matching goods by asking effective questions using entropy  navigation by asking . then  it shows three contrasting samples with explanations of their selling points  navigation by proposing . this cycle is repeated until the shopper finds an appropriate good.
모evaluations showed that the combination of the two navigation modes works most effectively as human salesclerks do. finally  although the human salesclerks we surveyed insisted that it is the best approach to propose three goods at the same time  we have not examined any subjective evaluation on this. it is a future issue to justify the expertclerk conversation model.
references
 aha & breslow  1  aha  d.w. and breslow  l.a.: 1  refining conversational case libraries  proceedings of the second international conference on case-based reasoning  pp. 1 - 1.
 aha et al.  1  aha  d.w.  maney  t.  and breslow  l.a.: 1  supporting dialogue inferencing in conversational case-based reasoning  proceedings of the 1th european workshop on case-based reasoning  pp. 1 - 1.
 burke et al.  1  burke  r.d.  hammond  k.j.  and young  b.c.: 1  the findme approach to assisted browsing  journal of ieee expert  vol. 1  1  pp. 1 -
1.
 burke et al.  1  burke  r.: 1  the wasabi personal shopper: a case-based recommender system. proceedings of the seventeenth national conference on artificial intelligence  menlo park  ca: aaai press.
 chavez & maes  1  chavez  a. and maes  p. : 1  kasbah: an agent marketplace for buying and selling goods. proceedings of the first international conference on the practical application of intelligent agents and multi agent technology  london.
 hammond et al.  1  hammond  k.j.  burke  r.  and schumitt  k.: 1  a case-based approach to knowledge navigation  in: leake  d.b.  eds. : case-based reasoning experiences  lessons  & future directions  pp. 1 - 1  menlo park  calif.: aaai press.
 kolodner  1  kolodner  j.: 1  case-based reasoning  san francisco  calif.: morgan kaufmann.
 quinlan  1  quinlan  j.r.: 1  induction of decision trees. journal of machine learning  vol. 1  pp 1 - 1.
 seybold  1  seybold  p.b. and miller  j.: 1  top 1 pre-holiday e-tailing picks  customers.com / perspective  www.customers.com
 salton  1  salton  g.: 1  an introduction to modern information retrieval new york: mcgraw-hill
 shigeta  1  shigeta  t.: 1  1 lessons on how to be an excellent salesclerk.  in japanese  keirin shobo: tokyo.
 shimazu et al.  1  shimazu  h.  arita  s.  and takashima  y.: 1  design tool combining keyword analyzer and case-based parser for developing natural language database interfaces proceedings of the fourteenth international conference on computational linguistics.
 shimazu et al.  1  shimazu  h.  kitano  h.  and shibata  a.: 1  retrieving cases from relational data base: another stride towards corporate-wide case-based systems  proceedings of the thirteenth international joint conference on artificial intelligence  pp. 1 - 1.
 shimazu et al.  1  shimazu  h.  shibata  a.  and nihei  k.: 1  expertguide: a conversational case-based reasoning tool for developing mentors in knowledge spaces. applied intelligence  1  pp 1 - 1  kluwer academic publishers  january 1.
 watson & gardingen  1  watson  i. and gardingen  d.: 1  a distributed case-based reasoning application for engineering sales support  proceedings of the 1th international joint conference on artificial intelligence  morgan kaufmann.
 wilke et al.  1  wilke  w.  lenz  and m.  wess  s.: 1  intelligent sales support with cbr  in: lenz  m.  bartsch-spo몮rl  b.  burkhard  h-d.  and wess s.  eds. : case-based reasoning technology from foundations to applications  pp. 1 - 1  lecture notes in artificial intelligence 1  springer.

web applications
web search

preference-based configuration of web page content
	carmel domshlak	ronen i. brafman	solomon e. shimony
department of computer science
ben-gurion university of the negev
모모모beer-sheva 1  israel dcarmel brafman shimony  cs.bgu.ac.il

abstract
we present a new approach for personalized presentation of web-page content. this approach is based on preference-based constrained optimization techniques rooted in qualitative decisiontheory. in our approach  web-page personalization is viewed as a configuration problem whose goal is to determine the optimal presentation of a webpage while taking into account the preferences of the web author  layout constraints  and viewer interaction with the browser. the preferences of the web-page author are represented by a cp-network  a graphical  qualitative preferencemodel developed in  boutilier et al.  1 . the layout constraints are represented as geometric constraints. we discuss the theoretical basis of this approach and its implementation within the cpml system.
1	introduction
an important goal for web-page design is to provide vieweroriented personalization of web-page content. designers may strive to condition web-page content and appearance on the current preferences of the viewer  and probably on some underlying structure of the web-page content. for example  when the user is viewing an article about the consequences of a traffic accident in an on-line newspaper  the designer may wish to present a volvo ad  as well.
모much of the content personalization literature focuses on learning user profiles. although this technique is useful  it generally suffers from low availability  and tends to address only long-term user preferences. these schemes are thus applicable only to frequent viewers  that are  in addition  amiable to having information about their behavior managed by an external agent.
모we propose a new model for representing web-page content. this model is unique in two ways. first  it emphasizes the role of the author in the process  viewing her as a content expert whose taste is an important factor in how the web-page

모모partially supported by an infrastructure grant from the israeli science ministry  and by the paul ivanier center for robotics and production management.
will be presented. the resulting model exhibits dynamic response to user preferences but does not requirelearning longterm user profiles. second  to accomplish this behavior  we use well-founded tools for preferences elicitation and preference optimization grounded in qualitative decision theory. these tools help the designer structure her preferences over web-page content off-line  in an intuitive manner  and support fast algorithms for optimal configuration determination.
모a web-page is typically composed of several components. the informationcontent of each componentcan be either presented or not. in our model  the first step is for the web-page designer to express her preferencesregardingthe presentation of the web-page content. for example  the author may prefer some material to be presented if and only if some other material is not presented. this is done in an intuitive yet expressive manner using the cp-network  short for ceteris paribus networks   boutilier et al.  1  which are an intuitive  qualitative  graphical model of preferences that captures statements of conditional preferential independence. the description of these preferences  as captured by the cp-net  becomes a static part of the web document  and sets the parameters of its initial presentation. then  for each particular session  the actual presentation changes dynamically based on the user's actual choices. these choices exhibit the user's content preferences. they are monitored and reasoned about during each session. no long-term learning of a user profile is required  although it can be supported.
모using this approach  we achieve content personalization through dynamic preference-based reconfiguration of the web-page. whenever new user input is obtained  e.g.  a click indicating his desire to view some item   the configuration algorithm attempts to determine the best presentation of all web-page components with respect to the web-page designer's preferences that satisfies the user's viewing choices. this process is based on an algorithm for constrained optimization in the context of a cp-network.
모determining only preferred content is generally insufficient - the chosen configuration should be presentable w.r.t. the layout constraints of the viewer's browser. likewise  a web-page designer may wish to specify expectations of the exact appearance of the document  in addition to preferences
about content. declarative specification of the desired layout of a web-page are well known: cascading style sheets were introduced as a part of the html 1 standard  lie and bos  1   constraint style sheets were proposed in  borning et al.  1 . we extend our content personalization approach to handle different layout constraints on web-page rendering  providing the viewer with a preferentially optimal feasible presentation of the web-page components.
모we implement our approach in the framework of the ceteris paribus markup language  cpml  system  which consist of an authoring tool for the preference-based web-pages  and a corresponding viewing tool  which is implemented as a browser plugin.
모the paper is organized as follows: section 1 presents the frameworkof web-pagepreference-basedconfigurationin the context of qualitative decision making. section 1 discusses relevant preference representation issues  and describes the cp-network model. section 1 describes the basic architecture and implementation of cpml. section 1 illustrates the approach by example. section 1 extends cpml by integrating layout constraints into the web-page optimization process.
1	configuration and qualitative dt
any web-page can be considered as a set of components . each component is associated with its content. for example  the content of a component may be a block of text  an image  etc. in our work  each component may be either presented to the viewer or hidden  and these options for are denotedby   . however the modelcan be expanded to handle more options for components' content presentation.
the web-page components define a configuration space
	. each element	in this space
is a possible configuration of the web-page content. our task will be to determine the preferentially optimal configurations  and to present one of them to the viewer. in terms of decision theory  the set of components of web-page is a set of features  the optional presentations of component's content are the values of the corresponding feature  and configurations are outcomes  over which a preference ranking can be defined.
모first we define a preference order over the configuration space: means that the decision maker views configuration as equal or more preferred than . this preference ranking is a total preorder  and  of course  it will be different for different decision makers. given a preference order
over the configuration space  an optimal configuration is any such that for any .
모the preference order reflects the preferences of a decision maker. the typical decision maker in preference-based product configuration is the consumer. however  in our application the role of the decision makeris relegatedto anotheractor - the web author. the author is the content expert  and she is likely to have considerable knowledge about appropriate content combinations. we would like the web-page to reflect her expertise as much as possible.
모during the creationof the web-page  the designer describes her expectations regarding content presentation. therefore  the preference order represents the static subjective preferences of the web-page designer  not of its viewer. thus  preference elicitation is performed on the web-page designer

모모for a discussion on using constraints in user interfaces and interactive systems we refer to  borning et al.  1 .
off-line once for all subsequent accesses to the created webpage. the dynamic nature of the web-page stems from the interaction between the statically defined author preferences and the constantly changing content constraints imposed by recent viewer choices.
모the choice of a representation model for the preferences is crucial in any preferencedriven configurationprocess. in particular  one can adopt either a quantitative  utility-theoretic  model  or a qualitative model. we believe that qualitative models can form a good basis for the automated product configuration in general  and for the web-page configuration in particular. for a comprehensive overview of the field of qualitative decision theory see  doyle and thomason  1 . the main advantages of qualitative decision theory tools  as opposed to traditional decision theory  are compactness  intuitiveness - which can considerablyreduce the preferenceelicitation burden - and potentially reduced computational effort. thus  in our domain  designers are likely to find  quantitative  utility assessment of content configurations unintuitive. yet  the web-page designer is likely to be able to  qualitatively  compare and to rank alternative content presentations.
1	preference representation
although a qualitative representation of preferences is typically simpler to represent and manipulate  the preference elicitation stage can still be quite complicated. to perform real-life preference-based configuration  we must represent user preferences in a compact  yet expressive manner. even relatively small web-pages may consist of numerous components  making an explicit  exponential size  ranking table for all alternative content configurations impractical. likewise  we wish to capture conditional preference dependencies between different components. for example  the designer may prefer to expose a short ijcai call for papers  cfp  if the viewer explicitly examined the content of an article about a new book on ai  but not to expose the cfp otherwise.
모an appropriate representation of preferences is insufficient on its own; we need to be able to reason about them efficiently. reasoning tasks include finding preferentially optimal configurations  comparing two configurations. likewise  the representation model should be intuitive in order to simplify the web-page designer's task.
모because of these requirements  in our work we decided to exploit the advantages of the cp-network model developed in  boutilier et al.  1 . this is an intuitive  qualitative  graphical model  that represents statements of conditional preference under a ceteris paribus  all else equal  assumption. in terms of our domain  this conditional ceteris paribus semantics requires the web-page designer to specify  for any specific component of interest  content presentation of which other components can impact her preferences over the presentation options of . for each content configuration of   the designer must specify her preference ordering over the presentation options of given . for example  suppose that the designer determines that
and that is preferred to given and all else being equal. this means that given any two configurations that agree on all components other than and in which is in and is out  the configuration in which is out is preferred to the configuration in which is in.
모cp-networks bear a surface similarity to bayesian networks: their representation is by a directed acyclic graph  with additional information - for each possible state of each node's predecessors. a node in the graph stands for a feature  which here represents a component of the web-page. the immediate predecessors of in the graph are associated with . formally  if

then and are conditionally preferentially independent given . this standard notion of multi-attribute utility theory can be defined as follows: let     and be nonempty sets that form a partition of feature set . and are conditionally preferentially independent given   if for each
assignment	on	and for each	 	 	 	we have that
each node of the cp-net contains a table  which describes the preferences about the values of the corresponding feature given all possible combinations of .
모the acyclicity requirement is necessary  as introduction of cycles may cause an inconsistent preference structure. an effective algorithm for detecting such a problem does not yet exist  and we believe that the problem is np-hard.
모an example cp-network with the corresponding preference table is shown in figure 1. we see that the designer specifies unconditional preference for presenting the content of component  denoted in figure by  . however  if content is presented and content is not  then the designer prefers not to expose the content of  denoted by
 .
모in our domain  the interactive construction of a cpnetwork for web-page's content consists of two stages: for each component of interest  asking the web-page designer to identify   and to specify the 's preference table. note that each complete assignment for the nodes of the network represents a configuration of the web-page content.
모suppose that there are no constraints on web-page rendering. then all content configurations are feasible. in this case  given a cp-network representation of the preferences  finding preferentially optimal configuration is straightforward: traverse the nodes of the network according to a topological ordering and set the value of the processed node to its preferred value  given the  already fixed  values of its predecessors. the more complicated case of constrained configuration space will be discussed later in this paper.
1	cpml system description
this section presents a framework for preference-based webpage configuration  and shows how decision theoretic tools provide a basis for this application. past work has dealt with general preference-based configuration  e.g.   boutilier et al.  1; d'ambrosio and birmingham  1   but its adaptation to information personalization has not been explored. a prototype system  cpml  has been developed at our university  and here we describe some of its implementation issues.
모the process of a partly unsupervised  preference-based web-page content configuration can be divided into 1 stages: 	
		



		
		
figure 1: an example cp-network
1. identify the components on whose content presentationthe designer has a preference.
1. apply interactive elicitation of the designer's preferences for the presentation of the selected components.
1. reason about the specified preferencesduring viewer interaction with the web-page.
the cpml prototype system for this process consists of two modules - the authoring tool  and the viewing tool. the central part of the authoring tool is a specification of the cpnetwork for the created web-page  steps 1 and 1 . given such a specification  the viewing tool is responsible for reasoning about the preferences  i.e. for an optimal content reconfiguration after an interaction of the viewer with the web-page. likewise  the authoring tool allows an optional specification of layout constraints on the web-page rendering. the nature of these constraints and their integration into the process of content optimization is described in details in section 1. the outline of the system is presented in figure 1.
모first  the designer creates an html web-page. this task is standard for most web-page authoring tools. however  html document component contents and their representation are indivisible. therefore  during step 1  see below   the specified html components are automatically wrapped by javascripts  rule and rule  1 . this will allow presentation of components in different manners. recall that this

authoring
tool
web browser1. reconfiguration of the web page.
figure 1: framework of the cpml system
prototype system is restricted to only two options: presenting  or hiding  the component contents. the whole process of html expansion is completely transparent to the designer.
모next  step 1 in our approach is the definition of a cpnetwork over the specified components. for this purpose  an editor for cp-networks is integrated with the authoring tool. the result of preference-based web-page design  is a document specifying both what to present and how to present.
모in turn  the viewing tool on the client side consists of a standard browser expanded by a cpml agent  implemented by a browser-compatible plugin. accessing a preferencebased web document in our system results in shipping the web-page to the browser and the embedded cp-network to the agent. upon downloadingthe document  the agent sets all the components to their values in the optimal configuration  given no evidences on the viewer's current interests. from this point the agent acts in event-driven fashion  and waits for a viewer interaction with the browser. the user may either choose to expose or hide a component. the agent is responsible for reacting to viewer actions by reconfiguringthe content presentation of the web-page. more specifically  the new content presentation should be a pareto-optimal configuration of the web-page that contains  or omits  the item selected by the user for presentation  or for omission  respectively .
모determining the best configuration is done by the procedure presented in figure 1. the simplicity of this algorithm
procedure reconfiguration
- cp-network of the web page's components
- queue of the recent events  component  value 
let be the recent event: an interaction of the viewer with component  setting on/off . then
1. switch the value of and add into . if is bigger than some threshold - remove the oldest event from .
1. project on  specify values of recently observed components w.r.t.  .
1. traverse in a topological order and set each unspecified component to its most preferred value w.r.t. to the values of its predecessors in .
figure 1: preference-based reconfiguration
stems from an important property that emerges from the semantics of cp-networks: parent preferences are more important than children preferences. this sanctions a fast top-down traversal process for generating optimal configurations.
모as the cpml prototype is restricted to components with only two presentation options  in step 1 of reconfiguration  we switch the value of the observed component  and then add it to the list of the recent viewer's choices. the maximal size of is specified by the designer of the web page and is passed as a part of the document. in step 1  we specify the values of the recently observed components as constraints on the required optimal configuration of the web-page. this is done in order to personalize the content to the viewer by reflecting her recent choices in content configuration. finally  in step 1  giventhe constrainedcomponents we determine the optimal configuration of the web-page. due to the semantics of the cp-network model  this process can be performed in time linear in the number of components.
1	example
we illustrate the process through the following example. the designed web-page consists of seven components: four short articles  and three commercials. the articles are about the ongoing elections  elections   a traffic accident  traffic accident   a new car airbag  new airbag   and the results of the recent nba games  nba . the commercials are for new-york times magazine  ny times   volvo cars  volvo   and nike shoes  nike . after specification of the web-page content  the designer expresses her preferences about the content presentation:
1. by default  presenting the central article elections     is preferred to hiding it  i.e. on is preferred to off. for the secondary article traffic accident  off is preferred to on.
1. article new airbag    : on is preferred only if traffic accident is on and elections is off.
1. article nba     on is preferred only if traffic accident is not presented.
1. commercial ny times     is preferred only if both elections and traffic accident are presented.
1. commercial volvo     on is preferred if either new airbag or traffic accident are presented.
1. the commercial nike  	  on is preferred only if nba is on.

figure 1: example cp-network
모the corresponding graph of the cp-network is presented on figure 1 and the cp-tables are as follows:
모upon downloading this web document  the initial presentation of its content  depicted in figure 1 a   is determined by the reconfiguration procedure. in the figure shaded nodes stand for presented components  and the rest for hidden components. suppose that the viewer clicks on the link to the traffic accident article. recall that such an interaction involves not just the presentation of the directly selected component  but the entire web-page presentation is reconsidered. the result of the reconfiguration is shown in figure 1 b . now  suppose that the viewer consequently clicks on the elections component  trying to express that she is not interested in this topic . if the event queue was specified by the designer to contain only one  most-recent event  then previous interaction with the traffic accident component is removed from the queue  and the result of the subsequent reconfiguration is presented in figure 1 c . otherwise  reconfiguration is performed w.r.t. the last two interactions  figure 1 d  .

	 a 	 b 

	 c 	 d 
figure 1: reconfigurations of the content
1	adding layout constraints
as mentioned above  the designer may specify expectations about layout of the presented components  as well as preferences about content presentation. the designer is allowed to specify layout constraints on web-page components  and we integrate these constraints into our preference-based optimization. we begin by discussing adopted layout constraints issues  followed by the modified reconfiguration process.

모모obviously  clicking on a component as expression of loss of interest is just one type of possible interaction - other kinds of interaction can be considered.
모using the cpml authoring tool  a designer can specify different constraints on the components of a web-page. each component is considered to be rendered in a rectangular area   with neither its size nor its actual placement specified a priori. in turn  the designer is able to restrict the size and the form of each   and to specify their relative placement and alignment. all these constraints are expressed as linear equalities and inequalities over the coordinates of the corners of the s. the required solution for them should minimize an objective function that expresses the size of the rendering area . this problem is an instance of linear programming  and thus can be solved in polynomial time.
모unfortunately  the designer may specify only a few layout constraints  thus we expect our problems to be underconstrained. in conjunction with the frequently used objective function that attempts to minimize the rendering area of the web-page  this will result in a spatial overlap of some of the components  an unacceptable situation in our domain. general overlap prevention for the unconstrained rectangular areas cannot be expressed as a linear program. we express the non-overlap constraint as a set of linear inequalities over integers as follows. for each pair of rectangles	and	of undetermined relative position  denote their spatial extents by and	respectively. here
and stand for left  right  top and bottom respectively. the non-overlap of and is expressed by the disjunction:
we rewrite this disjunction as the following set of inequalities  by introducing four new integer variables: where is a  sufficiently large  constant  and
for . these inequalities are added to other linear constraints specified by the web-page designer  and solved by a general purposeilp solver. the additionalconstraints make the general layout optimization problem np-hard. however  we do not expect that to cause intractability in practice  due to the usually restricted number of components in a web-page. during the preliminary evaluation of the cpml prototype  no significant response time degradation was observed.
모as with cp-networks creation  our authoring tool provides a simple user interface that supports icon-based layout constraint specification. the web-page designer is not aware of the underlying linear expressions  which are generated automatically. these linear expressions automatically become a part of the created web document  together with the actual web-page and the cp-network's description. upon document download  these embedded constraints are passed to the cpml agent  together with the embedded cp-network   and become a part of the optimization process.
모during user interaction  optimal layout w.r.t. the objective functionis chosenfor the configurationdeterminedby the reconfiguration procedure. the obvious question now is: what

other objective functions are also supported.
happens if the constraint system is unsatisfiable for a given  or even maximally possible  width and height of the viewer's browser  the first possibility is to use scroll bars. the second option is to try to compromise a little in the preferential optimality of the presented content. if we could determine a content presentation  which is both close to optimal w.r.t. the preferences of the designer and the viewer  and feasible w.r.t. the layout constraints  it may be a better solution.
모one of the properties of the cp-network model is that given an outcome on can easily determine a set of outcomes such that  for   and there is no other outcome such that . in other words  is a set of all outcomes which are less preferred than but with a minimal loss of preference. the corresponding worsening search procedure is presented in  boutilier et al.  1 . informally  each outcome is reached by changing value of a single feature in   such that given the same values of in and .
procedure constrainedreconfiguration  	 
- cp-network of the web page's components
- queue of the recent events  component  value 
- set of layout constraints
1. = reconfiguration
1. if an = ilpsolve       exist  then present according to the layout specification . otherwise:
 a  using restricted worsening search starting from   find the frontier of preferentially pareto-optimal configurations consistent with that satisfy .
 b  if then remove the oldest event from and continue from 1. otherwise  present a .
figure 1: constrained reconfiguration
모the extended version of the reconfiguration procedure is shown in figure 1. note that the number of preferentially optimal  feasible configurations may be exponential in the number of features  thus in step 1a we explicitly restrict ourself in the worsening search. consequently  if such a compromise in the designer's preferences does not result in a feasible configuration  we can allow ourself less attention to the preferences of the viewer. this is done in step 1b by explicitly ignoring some of the oldest viewer's choices. naturally  all these parameters of the reconfiguration process can be specified by the web-page designer.
1	summary and future work
a framework for preference-based configuration of the web page content was presented. our approach is based on qualitative decision theory  and in particular on the cp-network graphical model for preference representation. the choice of qualitative  well-founded tools leads to an application that is both intuitive and fast. the configuration process is performed according to the preferences of the web author  as well as to the current interests of the viewer. preferences of the web-page author are encoded as a cp-network. given this cp-network and the recent actions of the viewer  the optimal configuration of the web page content is determined.
likewise  geometric constraints on web-page layout are integrated within the optimization process  and different aspects of this integrated optimization were discussed. a prototype system was implemented  consisting of an authoring tool on the web-page author's side  and a decision making agent on the client side.
모in future work  we plan to deal with a number of issues. first  preferred presentation of a web component may depend on the presentation of its neighbors  whose preferred presentation depend on its presentation  in turn. the resulting cyclic preference graphs are not well understood. the cp-network model presented in  boutilier et al.  1  requires an acyclic graph  as the consistency of cyclic preference graphs is not guaranteed and depends on actual values in the preference tables. thus  we plan to investigate the computational aspects of consistency verification in cyclic preference graphs.
모second  we wish to exploit our domain-specific knowledge in order to achieve an efficient ilp solver for our specific domain  in a manner similar to  badros and borning  1 . likewise  we will consider translating some geometric constraints into actual constraints on the content presentation options of the web components.
모finally  specifying the behavior of intelligent user interfaces and autonomous multimedia objects is another potentially important application of qualitative preference models. here as well  qualitative constraint-based optimization techniques seem useful.
references
 badros and borning  1  greg j. badros and alan borning. the cassowary linear arithmetic constraint solving algorithm: interface and implementation. technical report 1-1  university of washington  1.
 borning et al.  1  alan borning  richard lin  and kim marriott. constraint-based documentlayoutfor the web. acm multimedia systems journal  1 :1  1.
 boutilier et al.  1  c. boutilier  r. brafman  c. geib  and d. poole. a constraint-based approach to preference elicitation and decision making. in aaai spring symposium on qualitative decision theory  stanford  1.
 boutilier et al.  1  c. boutilier  r. brafman  h. hoos  and d. poole. reasoning with conditional ceteris paribus preference statements. in proceedings of the fifteenth annual conference on uncertainty in artificial intelligence  uai-1   1.
 d'ambrosio and birmingham  1  joseph d'ambrosio and william birmingham. preference-directed design. journal of artificial intelligence in engineering design  analysis  and manufacturing  1-1  1.
 doyle and thomason  1  j. doyle and r.h. thomason. background to qualitative decision theory. ai magazine  1 :1  1.
 lie and bos  1  h.w. lie and b. bos. cascading style sheets. addison-wesley  1.
 rule and rule  1  jeff rule and jeffrey s. rule. dynamic html: the html developer's guide. addisonwesley  november 1.
keyword spices: a new method for building domain-specific web search engines
satoshi oyama  takashi kokubo	and toru ishida
department of social informatics
 kyoto university  kyoto 1  japan oyama  t-kokubo  ishida  kuis.kyoto-u.ac.jpteruhiro yamada
laboratories of image information
science and technology yamateru kuis.kyoto-u.ac.jp yasuhiko kitamura
department of information and
communication engineering
osaka city university  osaka 1  japan

abstract
this paper presents a new method for building domain-specific web search engines. previous methods eliminate irrelevant documents from the pages accessed using heuristics based on human knowledge about the domain in question. accordingly  they are hard to build and can not be applied to other domains. the keyword spice method  in contrast  improves search performance by adding domain-specific keywords  called keyword spices  to the user's input query; the modified query is then forwardedto ageneral-purposesearchengine. keyword spicescanbeeffectively discoveredautomatically from webdocuments allowing us to build high quality domain-specific search engines in various domains without requiring the collection of heuristic knowledge. we describe a machine learning algorithm  which is a type of decision-tree learning algorithm  that can extract keyword spices. to demonstrate the value of the proposed approach  we conduct experiments in the domain of cooking. the results confirm the excellent performance of our method in terms of both precision and recall.
1	introduction
the expansion of the internet and the number of its users has raised many new problems in information retrieval and artificial intelligence. gathering information from the web is a difficult task for a novice user even if he uses a search engine. the user must have experience and skill to find the relevant pagesfromthelargenumberofdocumentsreturned  whichoften cover a wide variety of topics. one solution is to build a

presently with ntt docomo  inc.
presently with sanyo electric co. ltd.
kitamura info.eng.osaka-cu.ac.jp
domain-specificsearchengine mccallum etal.  1 ; anengine that returns only those web pages relevant to the topic in question.
모this paper proposes a new method for building domainspecific search engines automatically that it is based on applying machine learning technologies to determine keyword occurrence in web documents.
모when oneof the authors used a popular japanesesearchengine  goo1  to find some beef recipes  he input the obvious keyword gyuniku  beef   but only 1 of the top 1 returned pages  1%  pertained to recipes. he hit on the idea of adding another keyword shio  salt  to the query  at which point all but one of the returned pages 1%  contained recipes. surprised at this enhancement  he used the same approach for other ingredients such as pork and chicken... the same improvement in searchperformancewasseen. this indicatedthe possibility of making a domain-specific search engine simply by adding a few keywords to the user's query and forwarding the modified query to a general-purpose search engine. our keyword spice method is a generalization of this finding.
모several research papers have described domain-specific web search services. a straightforward approach to building a domain-specific web search engine is to makeindices to domain documents by running web-crawling spiders that collect only relevant pages. cora1 mccallum et al.  1  is a domain-specific search engine for computer science research papers. its web-crawling spiders effectively explore the web by using reinforcement learning techniques. spiral cohen  1  or webkb craven et al.  1  also use crawlers. these systems offer sophisticated search functions because they establish their own local databases and can apply various machinelearning or knowledge representation techniques to the data. unfortunately  domains such as personal homefigure 1: filtering model for building domain-specific web search engines
pagesor cooking pages  which aredispersed acrossmany web sites  are not well handled by spiders since the time and network bandwidth consumed are excessive. accordingly  such types of systemsare suitable only for those domains that have few web sites.
모reusing thelargeindices ofgeneral-purposesearchengines to build domain-specific ones is a clever idea etzioni  1 . for example  ahoy!1  shakes et al.  1  is a search engine specialized for finding personal homepages. it forwards the user's query to general-purpose search engines and sifts out irrelevant documents fromthe returned ones to increaseprecision by domain-specificfilters. we call this the filtering model for building domain-specificsearchengines figure 1 . ahoy! has a learning mechanism to assess the patterns of relevant urlsfrom previous successfulsearches but overallaccuracy basically depends on human knowledge.
모one solution to the above problem is to make domain filters automatically from sample documents. automatic text filtering  which classifies documents into relevant and nonrelevantones hasbeenamajorresearchtopic in both information retrieval baeza-yates and ribeiro-neto  1  and machine learning mitchell  1 .
모we can use various machine learning algorithms to find such filters if the training examples  which consist of documents randomly sampled from the web together with their manual classification  are available. unfortunately  making such training examples is the real barrier because the web is very large  and randomly sampling the web will provide only a small likelihood of encountering the domain in question. in fact  most studies on text classification have been applied to e-mail  net news  or web documents at limited sites where the ratio of positive examplesis rather high. thus previous methodsoftext classificationcannotbedirectly applied totheproblem of building domain-specific web search engines.
thekeyword-spice method considers only those web pages figure 1: the keyword spice model of building domainspecific web search engines

figure 1: sampling with input keywords to increase the ratio of positive examples
that contain theuser'sinput query keyword  not all webpages. this eliminates the problem of finding positive examples and enables us to make domain-specific search engines at low cost.
모the remainder of this paper is organized as follows: section 1 presents the idea of building domain-specificsearchengines using keyword spices. section 1 describes a machine learning algorithm for discovering keyword spices. section 1 evaluatesour methodand our conclusions aregiven in section
1.
1 the keyword spice model of building domain-specific web search engines
hereweintroduce somenotations to definethe machinelearning problem. we let denote the set of all web documents; denotes the set of documents relevant to a certain domain. the target function  an ideal domain filter  that correctly classifies any document is given as
if
otherwise
모we let be the set of all keywords in the domain and let be the hypothesis space composed of all boolean expressions where any keyword is regarded as a boolean variable. we adopt the boolean hypothesis space because most commercial search engines can accept queries written in boolean expressions.
모a boolean expression of keywords can be regarded as a function from to when we assign 1 true  to a keyword  boolean variable  if the keyword is contained in the document and 1 false  otherwise. in the filtering model  the problem of building a domain filter is equal to finding hypothesis that minimizes the error rate

note: quantity	is 1 if	  1 otherwise.
모the keyword spice model does not filter documents returned by a general-purpose search engine. instead  it extends the user's input query with a domain-specific boolean expression  keyword spice   which better classifies the domain documents  and passes the extended query to a general-purpose search engine  figure 1 . this model is just the reverse of the filtering model.
모our method is based on the idea that when we build a domain-specific web search engine  we need consider only those web pagesthat contain the user's input query keywords; not all web pages.
모as described in figure 1  the scope of sampling is reduced from set   all web documents  to   the set of web pages that contain input keyword ; this increases the ratio of positive examples . thisidea makesit easierto create training sets and it becomespossible to build a domain filter  which is not possible with random sampling. by using domain filter   we modify the user's input query to   so the returned documents contain and are included in the domain. in short  is the keyword spice for the domain.
1	algorithm for extracting keyword spices
1	identifying keyword spices
it is rather easyto findgood keyword spicesfor any input keyword  for example  beef  . the problem is to find that the keyword spices that provide enough generalization to handle all future user keywords.
모we let denote the probability of that a user will input keyword to a domain-specific search engine. then

is the expectation of the error rate when users try to locate domain documents using this system.

figure 1: an example of decision tree that classifies documents
tablespoon tablespoonrecipehometoptablespoonrecipepepperpanfigure 1: an example of boolean expression converted from the tree in figure 1
모the boolean expression that minimizes the above expectation value is the most effective keyword spice. it would be best to maketraining examplesusing but wedo not know beforehand. obviously  we have to start with some reasonable value of   and modify the value as statistics on input keywords are collected.
모in this paper  we choose several input keyword candidates in the cooking domain. we assume that all candidates have the same probability of occurrenceand collect the same number of documents for each keyword as described in section 1. we then split the examples into two disjoint subsets  the training set  used for identifying initial keyword spices   and the validation set to simplify the keyword spices described in section 1.
모we apply a decision tree learning algorithm to discover keyword spices because it is easy to convert a tree into boolean expressions which areacceptedby most commercial search engines. in this decision tree learning step  each keyword is used as an attribute whose value is 1 when the document contains this keyword  or 1 otherwise . figure 1 shows an example of simple decision tree that classifies documents.
모the node indicates attribute  the value of branch indicates the value of the attribute  and the leaf indicates the class. in order to classify a document  we start at the root of the tree  examine whether the document contains the attribute  keyword  or not and take the corresponding branch. the process continues until it reaches a leaf and the document is asserted to belong to the class corresponding to the value of the leaf. this tree classifies web documents into  domain documents  and  the others   and the web document  for example  that does not include  tablespoon   does  recipe   does

figure 1: a decision tree induced from web documents
not  home   and does not  top  belongs to class	.
모we make the initial decision tree using an information gain measure quinlan  1  for greedy search without using any pruning technique. in our real case  the number of attributes  keywords  is large enough  several thousands  to make a tree that can correctly classify all examples in the training set
모모모모. then for each path in the induced tree that ends in a positive result  we make a boolean expression that conjoins all keywords  a keyword is treated as a positive literal when its value is and a negative literal otherwise  on the path. our aim is to make a boolean expression query that specifies the domain documents and that can be entered into search engines; accordingly  we consider only positive paths. we make a boolean expression by making a disjunction of all these conjunctions   i.e. we make a disjunctive normal form of a boolean expression . this is the initial form of keyword spices. figure 1 provides an example of a boolean expression converted from the tree in figure 1.
1	simplifying keyword spices
figure 1 shows a decision tree induced from collected web document in the experiments described in the next section1. decision trees usually grow very large which triggers the over-fitting problem. furthermore  too-complex queries cannot be acceptedby commercialsearchengines and so we have to simplify the induced boolean expression. we developed a two-stage simplification algorithm  described below  that is like rule post-pruning quinlan  1 .
1. for each conjunction	in	we remove keywords
	 boolean literals  from	to simplify it.
1. we remove conjunctions from disjunctive normal form to simplify it.
모in information retrieval research  we normally use precision and recall for query evaluation. precision is the ratio of number of relevant documents to the number of returned documents and recall is the ratio of the number of relevant documents returned to the number of relevant documents in existence.
모in this section  precision and recall are defined over validation set as follows:


where is the set of relevant documents classified by humansand isthe setof documentsthat theboolean expression identifies as being relevant in the validation set.
모in our case  we use the harmonic mean of precision	and recall	 shaw jr. et al.  1 


as the criterion for removal. the harmonic mean weights low values more heavily than high values. high values of occur only when both precision and recall are high. so if we simplify keyword spices in the way that results in high value of   we canobtain the keyword spices that arewell-balanced in terms of precision and recall.
모in the first stage of simplification we treat each conjunction as if it is an independent boolean expression. we calculate the conjunction's harmonic mean of recall and precision over the validation set. for each conjunction  we remove the keyword  boolean literal  if it results in the maximum improvement in this harmonic mean and repeat this process until there isno keywordthat canbe removedwithout decreasingthe harmonic mean.
모when we remove a keyword from conjunction recall either increases or remains unchanged. before the simplification  each conjunction usually yields high precision and low recall. accordingly  we can remove the keyword that results in improvement in recall in exchange for some decrease in precision  because the harmonic mean weights lower recall values more heavily. the removal of the keywords from the conjunction by the harmonic mean may appear to cause some problems. if the initial conjunction contains only a few relevant documents  the algorithm makes conjunctions that contain very large numbers of irrelevant documents. however  we can remove the conjunction from the keyword spices by the algorithm for simplifying a disjunction as is described below.
모in the second stage of simplification  we try to remove conjunctions from the disjunctive normal form to simplify the keyword spices. we remove the conjunctions so as to maximizethe increasein harmonic mean . we repeatthis process until there is no conjunction that can be removed without decreasing the harmonic mean .
1. generate input keywords according to some estimate of distribution	and collect web pages that contain keyword	and classify them into positive and negative examples by hand.
1. split the examples into two disjoint subsets  the training set 
 for generating the initial decision tree  and the val-
	idation set 	 for simplifying the tree .
1. make the initial decision tree from using an information gain measure without any pruning technique.
1. convert the tree so learned into a set of positive conjunctions by creating one conjunction for each path from the root node to each leaf node: this classifies positive examples.
1. make a disjunctive normal form of boolean expression by making a disjunction of all positive conjunctions.
1. for each conjunction	in	do
repeat
remove the keyword  boolean literal  from the conjunction that results in the maximum increase in the harmonic mean


of precision measure and recall measure of over the validation set.
모모until there is no keyword that can be removed without decreasing . end
1. repeat
remove the conjunctive component from the disjunctive normal form that results in the maximum increase in the harmonic mean


of precision measure and recall measure of over the validation set.
until there is no conjunction that can be removed without decreasing .
returnfigure 1: the keyword spice extraction algorithm
모after the first stage of simplification  each conjunction is generalized and changed to cover many examples. as a result  the recall of becomes rather high  but some conjunctions may cover many irrelevant documents. we can remove the conjunctions that cause the large improvement in the precision with aslight reduction in recall. thosecomponents that cover many irrelevant documents are removed in this stage  becausethe other conjunctions covermost ofthe relevantdocumentsand theremoval ofthe defectiveconjunctions doesnot cause a large reduction in recall. this yields simple keyword spices composed of a few conjunctions.
모after the above simplification processes is returned as the keyword spices for this domain. our algorithm for extracting keyword spices is summarized in figure 1.
table 1: collected web documents in the cooking domain
keywordrelevantirrelevanttotalbeef11chicken11paprika11potato11pumpkin11radish11salmon11tofu11tomato11whitefish11total11table 1: pruning results
trials111initialconjunctions111keywords111step 1conjunctions111keywords111step 1onjunctions111keywords111	evaluation in the cooking domain
1	experimental settings
as described in the previous section  we gathered two thousand sample pages of the cooking domain that contained human-entered keywords in japanese: gyuniku  beef   toriniku  chicken   piman  paprika   jagaimo  potato   kabocha  pumpkin   daikon  radish   sake  salmon   tofu  tofu   tomato  tomato   and shiromizakana  whitefish . we used a japanese general-purpose search engine goo to find and download web pages containing the above input keywords. we collected two hundred sample pages for each initial keyword. we examined the pages collected and classified them as either relevant or irrelevant by hand  table
1 .
모in splitting the collected documents into the training set and validation set  we paid no attention to which keywords were input. thus each set was randomly composed of documents containing the input keywords. we performed 1 trials in which the samplepagesweresplit randomly in this fashion. table 1 shows the pruning results after each step. in the early steps  induced trees are very large and after translating trees to conjunctions  we have more than 1 conjunctions; the number of keywords in these conjunctions exceeded1. this number istoo large to permit entry into commercialsearchengines. after step 1 the number of keywords was reduced to one third. step 1 removed redundant conjunctions and keyword number was reduced again to 1 to 1. this number of keywords can be accepted by commercial search engines.
different trials yielded different keyword spices. figure 1
 ingredients tablespoonspecialitygoodsfigure 1: extracted keyword spices
table 1: average precision of the queries over the index of a general-purpose search engine
querythe input querythe query with keyword spicespork11spinach11shrimp11table 1: estimated recall of the queries with keyword spices over the index of a general-purpose search engine
queryestimated recallpork11spinach11shrimp11shows  as an example  the keyword spices discovered in the first trial. we used these keyword spices in subsequent experiments.
모to conduct realistic tests with external commercial search engines  we choose the keywords of butaniku  pork   horenso  spinach  and ebi  shrimp  which were not used to generate the keyword spices.
1	precision
figure 1 compares the precision values for the queries containing only keywords and the queries with keyword spices for the three input keywords. we checked up to the top 1 pages as ranked by the search engine goo. in general  as the number of pages viewed increases  the precision with queryonly input decreases while the precision of queries with keyword spices stays high. table 1 lists the average precision of the top 1 returned results. precision is higher than1% for all queries.
1	estimated recall
it is easyto achieve high precision if we do not address recall  but keeping both high is rather difficult. the recall of a query is much harder to calculate than the precision because   the set of all relevant documents in the web  is unknown. we estimated from the results returned from a general-purpose search engine. most search engines show the total number of documents that matched the query. we can calculate the estimatednumberofrelevantdocumentsin thesearchengine'sindex     by using the averageprecision of thequery for the top 1 returned documents.

figure 1: precision of queries forwarded to a general-purpose search engine
 the number of document found with the input query 
 average precision of the input query 
모the number of relevant documents found with the spiceextended query can be calculated in the same way.
 the number of document found with the query with keyword spices  

figure 1: precision of the query  pork and salt  forwarded to goo
 average precision of the query with keyword spices 
모it is reasonable to use because we have no consistent way of finding web pages that are not linked to any general-purpose search engine. we estimate the recall of a spice-extended query as follows

모table 1 shows the estimatedrecall values of different spiceextended queries over the index of goo. the high value of recall  higher than 1%  indicates that our method filters out only non-relevant documents and does not drop any useful information in the search process.
모to compare these results with the example in the introduction  figure1 showsthe resultsofsubmitting the query pork and salt  to goo. the average precision and estimated recall for the top 1 returned documents are 1 and 1  respectively. this shows that our systematic method yields a great improvement in search performance.
1	conclusion
we have proposed a novel method for domain specific web searchesthat is based on the idea of keyword spices; boolean expressions that are added to the user's input query to improve the search performance of commercial search engines. this method allows us to build domain-specific search engines without any domain heuristics. we described a practical learning algorithm to extract powerful but comprehensive keyword spices. this algorithm turns complicated initial decision trees to small boolean expressions that can be accepted by search engines. our experiments with an external generalpurpose search engine yielded good results. for two different keywords in the field of cooking  precision was higher than 1%. high estimated recall higher than 1%  over the search engine's index was also confirmed.
모we used the domain of cooking as an example  and we are now developing search services for other domains such as restaurant pages and personal homepages.
모in this paper  we used input keywords selected by humans to make training examples. to be more comprehensive  we need some criteria with which input keywords can be selected. as discussed in section 1  it is sufficient to make examples based on the distribution of user's input query . we are planning to open our recipe search system to the public through the web and we will obtain the value of afterwards. in future work we will study how the input keywords used to form the training examples affect the performance of the system.
acknowledgments
this research was partially supported by laboratories of image information science and technology and by the ministry of education  science  sports and culture  grant-in-aid for scientific research a   1  1.
