ly  we will content ourselves with following in detail how the process might work in a particular example. this form of exposition has its perils  of course. the example 
probably would not work as described. it is sometimes difficult to distinguish properties of the particular example from the general behavior of the algorithm. and it is often not clear why  at a given point  one thing is done next and not another. for these deficiencies we can only beg the reader's indulgence. with respect to the last objection we may note that in cognitive processes 

-1-

the factors which determine precisely what is  cyrus wilfred  is justified by 	sad to operated on next need not be closely relevant to say  the kernels 	 time   thursday  	of f1 and the operation i t s e l f  nor very interesting:  map neither into each e.g. the precise connectivity of the semantic other nor into anything else. memory at the time the process takes place . when a kernel is not immediately matched in b. 	example of generalization-over-cases w h a t is  an other wise promising analogy  a search is for the purpose of c l a r i t y in our example  we instituted for information that w i l l correspond w i l l not indicate node-criterialities except where with    we would for exanple search memory 
	we have particular interest in them. 	where they 	if 	possible  	the 	dtermine 	whether 
are omitted  the reader may assume they have the 	or 	not 	f1 	w a s 	ocurring 	on a thursday. 	if such conventional value 1 . 	information is unavailable and the kernel must 
remain unmatched  this fact must duly be taken 
	phase i: the f i r s t structuring of information. 	. . 	. . 	the 	analogy 	n  
	  	into account in scoring the analogy  recall 
synopsis: three f a c t s a n d exist in section . . iii.b even   if the analogy .  analogy. is .accepted     
	section i i i . b   . 	even if the analogy is accepted  
is found to be analogous to this failure ... to  matchc   will affect   further . to this failure to match w i l l ffect further 
	. 	 sten 	1.  	let 	us 	. 	that 
	processing  step 	1 . 	let us assume that in our 
example these two kernels go unmatched  but the analogy is accepted anyhow. 
steps 1-1: at various times the following the mere construction   . of   the analogy does   the mere construction of the analogy does not produce anvthiing that  would . willing 
i t s e l f produce anything that we would be w i l l i n g to call the  understanding  of the new situation. what is significant is the structuring of the four previously unrelated facts into a path of analogy: 
 1  
step 1: our analogy  in relating f to suggests that these two facts about wilfred may somehow be  relevant  to each other. it seems 
reasonable to commemorate this relationship at 
 1  least by recording a new fact which is their 

	it would be possible simply to record this new fact conjuction . . 	 - 	. 	.. 	. 	that  
	possible  	syntactically f1 is merely the union of f1 and 
	without analyzing it farther  but we seldom record 	and 	so 	w e   just 	p e r f o r m e d t h e 	a p m t ＼ m 	o f 
input without trying to  understand  i t . one   c o n 1   r e s s i o n d e f i n e d s e c t i o n iiii c. aspect of understanding a novel situation is to written explicitly  relate it to something already known. in our 
model  this means to find an analogy with some situation already in memory. 
step 1: we seek an analogic match for  in 
　　　 1  memory  and propose as a candidate  with m = { bb aa ; cyrus wilfred   . the correspond- ence  bb aa  is j u s t i f i e d by member bb suspenders phase; t h e analogy perpetuates i t s e l f . 
and  member aa suspenders  also by the shared synopsis: a n e w f a c t  is encountered and found property of redness   vhile the identification   synopis a new   f o r m a t l o n o f t h e n e w 
-1-
analogy is facilitated by a remnant of the old analogy  namely the copy of  embedded in f    which allows the prediction of a fact  which 
serves as justifying information. 	a new path 
compression forms phase i analogy. thus the old analogy has greatly facilitated the formation of a new analogy similar 
to i t . 
step 1: a new path compression gives us yet another fact: step 1 . 1 : at some point we record that: 
phase i i i ; conjunction is restructured into 
  member rupert fireman  :1  . implication. 	synopsis: the successful prediction 
step 1: later  at the firemen's ball  we are of f 	triggers a re-examination of the facts 
keeping tabs on rupert when we suddenly notice that: 
which are found to be mutuallythe conjunction is split 
up and reorganized as two tentative implications 
	 rules  r.. and r1  one going each way. 	the 

presumably this observation is sufficiently striking that we w i l l attempt to  understand  i t . step 1: a search is made through semantic memory for situations resembling f1 . let us suppose that f1. happens to present i t s e l f as a candidate for matching. 
step 1: in constructing an analogy between f1 and f1 there are three unmatched kernels: 
 dances-with rupert maude  in f1  and  time   thursday  and member wilfred fireman  in f1 . as noted in step 1  each of these induces a search through memory for corresponding information. in this case  the f i r s t two l e f t overs presumably find no match  but happily the kernel  mnember wilfred fireman  does retrieve an item from memory  namely  the path which 
justifies the identification  rupert wilfred  is therefore: 

there is a significant improvement in the way in which the  firemanhood  justifying information was 
found here  in contrast with step 1   in  the phase i analogy  the facts relating cyrus and wilfred were turned up by a poorly-guided or nonguided search. in this new analogy  on the other hand  the kernel  member wilfred fireman  served as a clue to search for a specific item  namely the corresponding fact { member rupert fireman } . 
recall that this kernel was descended of f1 via the criterialities within r1 and r1 are based on information provided by the analogies among 

step 1 . 1 : the successful search for  in step may be regarded as a  prediction  that rupert is a fireman. since  rupert  and  fireman  are both low-frequency concepts  the success of this prediction may be so surprising as to cause us to re-examine the conjunctions which represent. in particular  we search memory for other similar  analogous  items  and turn up f1 . 
step 1: we reconstruct the three analogies among 
 the three are parallel to each 
other  thus: 

step 1  picking one of these facts as typical  say f1  we now convert i t s conjunction into a pair 
1 
of implications  thus implicitly following the reasoning of section iv.a   these implications w i l l of course be represented as rules. we w i l l 
have basically  but not exactly: 		and 
 we must now consider the modifications to be made to before they are 
-  the arbitrariness of this choice  and of many other aspects of the process we describe here  reflect the almost t o t a l lack of psychological data on which to base the algorithm. hopefully  future studies along the lines of posner and keele  1k  w i l l eventually enable us to make far more accurate models of when and how generalization takes place. 

-1-

combined into r1..  the story is identical for r 1  . 	step 1: the data structure requires that the new 
   analogies  as we have said  are a superb source rules r1 and r1 be assigned subjective probabilities  of information  and there is a great deal of infor since we have as yet no reason to prefer one of 
mation left in the analogies made in step 1 that these rules over the other  their initial s.p.'s can contribute to the content of r.  in particular  should be equal. we might assign initial values 

examination of these analogies will determine the criterialities of the various parts o f a s follows. 
   node-criterialities: the three facts f  1' f 1 ' and fg differ significantly only in the two triplets of corresponding nodes: wilfred  cyrus  rupert; and aa  bb  cc . clearly these are the cases 
which the analogies generalize over. it appears that the presence of one or another of these nodes is not crucial to the general law which unites the three facts. therefore  by definition these nodes are less criterial to than those nodes which are constant over all three facts. 
   kernel-criterialities: the kernels which never found analogic matches have shown themselves to  be dispensable in  or perhaps even irrelevant to the generalization which underlies the three facts. they thus are less criterial to r  than those 
kernels which found mates in all three analogies. 
   thus  in assembling r from f and fg we increase the criterialities of those structures 
which through their constancy give evidence of being relevant to the rule  and decrease the criterialities of those structures whose presence shows signs of being inessential. given that all of the node-criterialities were originally 1   the new rule r will be as shown in figure 1  above . 
   we should mention that these adjustments of criterialities  like everything else in life  are fallible. it might have happened  for instance  that by coincidence both wilfred and rupert were observed to wear red suspenders on a thursday. we assume that with the accumulation of evidence  as in phase iv below   such coincidences will wash out of the inductions. 
of 1   and thereafter treat the s.p. as the ratio 
of number of successes when the rule is used number of predictions 
predictively  as below . this particular treatment of s.p.'s  and the particular scheme we use in adjusting criterialities  were concocted for illustrative purposes in presenting this example; the actual manipulations must of course be more subtle. 
phase iv; new evidence argues for a generalization. synopsis: a new fact f is matched analogically with the left half of r1 . the inverse of the analogy is applied to the right half of r1 to obtain a predicted situation. this situation is in fact observed  and r1 is rewarded for its success. step 1: at some arbitrary time after phase i i i has taken place  a new fact comes to our attention: f : 
step 1: in an attempt to understand f1  we seek an analogic match for it in memory. suppose that the left half of r1 comes up as a candidate for matching. although we have no justifying information for the identification  otis rupert   the situations are otherwise identical  so we may assume that the analogy is accepted. 
     we are now in a position to follow the  prediction paradigm  of figure 1. that is  we may apply the inverse of our analogy  namely the mapping 
 to the right half of  to 
obtain a situation which we may expect to observe or to find already recorded in memory. step 1. the application of to the right half of  encounters an interesting difficulty. this right half contains a node  cc  whose low criteriality indicates that it has been generalized over. we would expect the analogy  again to map cc 

-1-

into some other node  but in fact provides no such node. hence in this and similar cases we are led to predict the existence of an entity in the new situation  corresponding in this case to cc.  note that the translations of quantifiers in terms of dummies  figure 1  arise from just this sort of argument.  we may optimistically create a name for our new entity  say xx. we thus predict a situation of the form: 

     the kemel-criterialities here  which are taken from those in the right half of  assume a new role: they indicate the zeal with which we 
should seek a realization of the given kernel. 
that is  the dominant criteriality of  wears otis  tells us that our main job is to look for something otis wears; the low criteriality of  dances-with otis maude  suggests that we should not give much concern to finding such a condition  since this kernel is under suspicion of being irrelevant to the prediction. step 1: we may search for the predicted situation in our semantic memory or in the real world. if it is in fact true that all firemen wear red suspenders  then we will indeed find such a situation. we will find an object to which we can attach the name xx  and in a l l likelihood there will be no match for the kernel  dances-with otis maude  . step 1: this successful prediction gives us valuable information with which we may adjust the rule r1 . we may raise and lower node- and 
kernel-criterialities in accordance with the 
analogy formed between the right half of r1 and the predicted situation. in particular  the unmatched kernel  dances-with rupert maude  attains a criteriality of 1 and disappears  since a kernel of zero relevance has no place in a situation. the successful prediction of course increases the subjective probability that the rule is a valid one. thus from r1 we derive a new rule  shown in figure 1  below . 
steps 1+: after enough recurrences of steps  our successful induction will 
approach the form shown in figure 1  below . 	here the zero criterialities of the nodes  rupert  and  cc  indicate that these occurrences have completely lost their identities and become dummies. 	like dummy variables in mathematical notation  these nodes could be replaced  consistently  of course  by any symbols  e.g. 	in view of the translation between dummies and quantifiers given 
in figure expresses precisely the proposition: v 	a 
xgfireman yesuspenders 
which is to say   firemen wear red suspenders.  
phase v: new evidence may argue against a generalization. we recall that a rule was formed along with and is its converse. the statement made by is that  if a person wears red 
suspenders  then he is a fireman . although this 
proposition is false in the absolute  it will be worth retaining if its statistical validity is significantly greater than zero. therefore we will 
reward this rule as per phase iv when it succeeds  simply adjust its subjective probability when it fails  and expunge it if the subjective probability 

f a l l s below some threshold. in this way our model becomes capable of retaining  half-truths  - a capacity which is very valuable to a semantic 
memory  among other things  it allows the storage of contradictory information . 
	v. 	discussion 
a. 	the relation between analogy and generalization 
　　　if we examine the generalization-over-cases process closely  we find that an interesting statement can be made of the relation between analogy and generalization. in step 1 we saw how a great deal of information supplied by analogies was incorporated into the representation of the generalized rule. thus  analogy takes part in generalization. but in phase iv we found that 
the induced rule led to the search for a situation which was in fact analogous to previously-known facts  and which might have gone unnoticed if the inductive generalization had not existed. thus  generalizations f a c i l i t a t e the finding of new analogies. in fact  the search for the new analogy was guided by precisely that information which had been contributed to the rule by the old analogy   i . e . c r i t e r i a l i t i e s   see steps 1 and 1   so we might say that analogies perpetuate themselves via generalizations. on the other hand  we could also summarize phase iv by saying that generalizations 
perpetuate themselves via new analogies. in any case  we have certainly shown that analogy and generalization are mutually reinforcing processes which can hardly be separated from each other. 
b. 	problems in modeling cognitive processes 
　　　we are at present attempting to construct a computer implementation  in the lisp language  of the processes outlined in this paper. such an effort necessarily leaves one sadder but wiser 
with regard to the prospects for formulating and testing explicit cognitive models. in this section we w i l l discuss some of the more forbidding obstacles we have encountered. we feel that the 
problems brought out below correspond not to deficiencies in our particular model  although there are enough of those  heaven knows   but rather to major dilemmas attending the construction of any general cognitive model in a large semantic memory system. 
　　　we have already mentioned that the choice of  what to do next  in a cognitive process is often poorly specified. because of the extreme scarcity of psychological data regarding such choices  the model builder is confronted with a small i n f i n i t y of arbitrary decisions in designing an algorithm. the cumulative effect of these low-level choices may well wash out the central theoretical propositions that the model was designed to test. 
　　　there are other factors which complicate the issue of what should be done when and for how long. in the f i r s t place  many cognitive processes contain no inherent termination condition. like memory search or the construction of analogies according to a recursive definition  they are bounded only by the size of semantic memory. in the second place  a cognitive operation is seldom t o t a l l y successful or t o t a l l y unsuccessful. as in our discussion of analogies  success must be defined by a scoring function and threshold. these considerations would seem to imply that a general cognitive process cannot be represented as an orderly succession of tidy operations  but instead must be couched in a welter of effort-limiting and evaluation heuristics. 
　　　of a l l of the issues we have sidestepped in sections i i i and iv  certainly none is more worrisome than the problem of memory search. not only must relevant information be brought forth  but this must be done without exhaustive search   i . e . rapidly   despite the fact that 1% of the contents of memory w i l l be irrelevant to the given search. moreover  the phenomena of  set  and  effect of context  show us that in human memory the memory structure  or  equivalently  the means of search access into i t   is continuously adapting in response to ongoing cognitive activity. certainly no process involving a large  general semantic memory can be adequately modeled u n t i l some progress is made on this most refractory set 
of problems. 
　　　the necessary size and intricacy of a semantic memory create a host of methodological problems in validating the algorithms which operate in such a 

1-

system. it becomes extremely time-consuming to construct a suitable data-base on which to test an algorithm - especially if that data-base is to be realistic in being 1# irrelevant to the test problem. sometimes it becomes very d i f f i c u l t to distinguish which properties of a program's behavior are inherent in i t s algorithm  and which stem from i t s interaction with the particular test data-base used. in addition  any algorithm w i l l 
contain dozens of arbitrarily-set parameters and arbitrarily-made decisions. ideally  one would evaluate these by varying them one at a time  using 
a large number of test data-bases  but such a procedure is out of the question in practice. 
　　　the ultimate problems in validation arise when one strives  as we have striven  to characterize general  subroutines  of cognitive behavior  rather than attempting to build a beginning-to-end model of a particular type of performance in a well-defined cognitive task. the processes of analogy formation  generalization-over-cases  and the prediction paradigm of section iii.c are not by themselves sufficient to model any particular cognitive behavior. they are intended rather to represent elementary sub-processes which may be observed to participate in a very wide range of psychological phenomena  from sensory perception to natural language understanding. our approach is in accord with the venerable programming dogma that the best and often the only way to come to grips with a complex process is to decompose it into easily-conceptualized subroutines. certainly this law must apply to that most complex of processes  human cognition. but the question immediately arises of how one is to validate a 
proposed algorithm for a  cognitive subroutine'1. it is essentially impossible for experimental techniques to provide data on a single cognitive sub-process taken in isolation from a l l others. but with no data as to how the subroutine is supposed to perform  one cannot even debug a 
proposed algorithm  much less validate it i 
　　　is it not intolerable  in a scientific investigation  to be asked to consider models for which empirical validation is next to impossible  we think not. consider the situation in linguistics. the linguist  not to be confused with the metatheorist  or prophet  spends his time trying to model particular aspects of a particular language  e.g. negation or nominalization. he does not have a complete grammar of the language available to him  nor does he attempt to construct one. he knows that his limited model is guaranteed not to be f u l l y consistent with empirical observations of language  because in language too it is impossible in reality to isolate one aspect from a l l the others. in the face of a host of counterexamples  exceptions  and phenomena not covered by his model  the linguist calmly decides to judge the worth of his theory by subjective criteria such as internal elegance and explanatory power. he is happy with a model if it gives him a better understanding than he had before. 
　　　we hope that the model presented in this paper gives the reader a better understanding than he had before. 
acknowledgment s 
　　　many of the ideas put forth in this paper r i g h t f u l l y belong either to jerome feldman of stanford university  or to ross quillian of bolt beranek and newman inc. i would also like to acknowledge my great personal indebtedness to these two gentlemen. this research was originally inspired by kenneth colby of stanford  and he has contributed much to the model. i am most grateful to daniel bobrow of bolt beranek and newman  and to nils nilsson  bertram raphael  and charles rosen of stanford research institute for their encouragement and for the opportunity to use their computing machines in developing the lisp implementation of the model. 
