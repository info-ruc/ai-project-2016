
document clustering is traditionally tackled from the perspective of grouping documents that are topically similar. however  many other criteria for clustering documents can be considered: for example  documents' genre or the author's mood. we propose an interactive scheme for clustering document collections  based on any criterion of the user's preference. the user holds an active position in the clustering process: first  she chooses the types of features suitable to the underlying task  leading to a task-specific document representation. she can then provideexamplesof features- if such examples are emerging  e.g.  when clustering by the author's sentiment  words like 'perfect'  'mediocre'  'awful' are intuitively good features. the algorithm proceeds iteratively  and the user can fix errors made by the clustering system at the end of each iteration. such an interactive clustering method demonstratesexcellent results on clustering by sentiment  substantially outperforming an svm trained on a large amount of labeled data. even if features are not provided because they are not intuitively obvious to the user-e.g.  what would be good features for clustering by genre using partof-speech trigrams -our multi-modal clustering method performs significantly better than k-means and latent dirichlet allocation  lda .
1 introduction
the problem of data clustering is generally underspecified unless criteria for clustering are explicitly provided. for example  given a set of objects of various colors and shapes  it is unclear whether clustering should be performed according to the objects color  shape  or both. in the absence of labeled instances  a clustering criterion might be expressed in terms of the data representation: e.g.  if only shapes of objects are known  there is no more doubt about the clustering criterion.
모when we talk about clustering text documents  we usually assume that the clustering will be by topic and we typically approach it using a bag-of-words  bow  representation that ignores word order  willett  1 . however  there is no reason that text documents must be clustered in that way: there are numerous non-topical criteria that could be considered- e.g.  clustering by sentiment  turney  1   style  genre  author's mood  and so on. other criteria may be esoteric or application-specific: e.g.  clustering by the author's age  by the age of the documents  by their credibility  expressiveness  readability  etc. it is unlikely that a simple bow representation will be sufficient for all of those purposes  meaning that most will require specific document representations. intuitively  some of these representations will be based primarily on syntax  while others are likely to have a semantic nature.
모this study proposes a unified framework for clustering document collections according to nearly any criterion of the users choice.  we restrict ourselves to hard clustering-i.e.  partitioning-of a document collection.  the user is first asked to choose types of features suitable for clustering by the desired criterion. for example  genres may be represented by sequences of part-of-speech  pos  tags  by a particular focus on punctuation  stopwords  as well as by general words as captured in the standard bow representation. the user is next asked to provide a few examples of features  seed features  of the chosen types  if such examples are intuitive and can be obtained without much effort-e.g.  when clustering by authors mood  words like 'angry'  'happy'  'upset' might be easily suggested.
모the clustering system then represents documents based on the users choice and applies a multi-modal clustering method  bekkerman and sahami  1 . when seed features are provided  the system iteratively clusters documents represented over the chosen features and then enriches feature sets with other useful features. the user can choose to intervene  or not  after each iteration  in order to fix possible mistakes made by the system on the feature level  no document labeling is required .
모we illustrate the effectiveness of our approach on two domains: clustering by genre and clustering by author's sentiment. genre is a type of a domain where providing examples of features is non-trivial: it is not intuitive  e.g.  whether noun phrases are more effective than verbs. sentiment classification is one where words like 'brilliant' and so on are easily recognizable as useful  when using bow features.
모there has been work on interactive topical clustering where the user corrects clustering errors on a document basis  basu et al.  1   but that effort is more time consuming than feedback on features  raghavan et al.  1 . other recent work has had the user select important keywords for  supervised  categorization  thereby leveraging the user's prior knowledge  dayanik et al.  1; raghavan et al.  1 - approaches that are more like that of our framework. raghavan et al.  further support this direction in the finding that users can identify useful features with reasonable accuracy as compared to an oracle. liu et al.  experiment with labeling words instead of documents for text classification  providing the user with a list of candidate words from which to select potentially good seed words  based on which a training set is constructed from a set of unlabeled documents. a classifier is then constructed given this training set. liu et al.'s documentrepresentation is the standard bow  which has strong topical flavor  and therefore cannot be used for clustering by any criterion  for example  our preliminary experiments show that bow is not appropriate for clustering by author's mood . in addition  liu et al.'s method involves the user only at the initial step  selecting seed words   limiting the user's control of the classification process.
모in summary  we propose a new interactive learning framework for clustering by user-determined criteria  section 1 . our multi-modal clustering method based on combinatorial mrfs  section 1  neatly incorporates multiple feature types as well as user prior knowledge into clustering presented as a combinatorial optimization problem. we demonstrate the effectiveness of our system by testing it on genre clustering  section 1  and multi-class clustering by author's sentiment when seed features are provided  section 1 . to our knowledge  this study is the first in which clustering  as opposed to classification  by genre is discussed and the first to perform multi-class clustering of documents by sentiment. we show that our interactive clustering outperforms state-of-theart methods  svm and lda  on real-world data collections.
1 interactive clustering scenario
we provide a step-by-step recipe for clustering documents by a particular criterion that the user has in mind:
모1. specify the number of clusters: learning the natural number of clusters still remains an open problem. we do not attempt to solve it in this paper  instead the user is asked to specify the desired number of clusters.
모1. specify feature types: a list of various feature types is provided to the user. examples of such types are: bag of words or word n-grams  pos tags or pos tag n-grams  punctuation  parse subtrees and other types of syntactic and semantic patterns that can be extracted from text. such a list can hypothetically include a large variety of feature types that would respond to everyone's needs. from this list the user is asked to choose one or more types that best serve the particular clustering criterion.
모1. give examples of features: for each feature type chosen  the user should attempt to construct  small  sets of seed features that correspond to each category of documents. sometimes this task is easy: e.g.  if the clustering criterion is authors' sentiments  then words such as 'excellent'  'brilliant' etc. would correspond to the category of positive documents  while 'terrible'  'awful' etc. would correspond to the negative category. however  when such sets cannot be easily constructed  e.g. it is non-trivial to come up with good feature examples for clustering by genre-see section 1   the user can skip this step and go to 1.
모1. default clustering: if m feature types are chosen  but no seed features are provided by the user  documents are represented as m distributions  each of which is over the  entire  feature sets of the corresponding type and then multi-modal distributional clustering  bekkerman and sahami  1  is applied.
모1. interactive clustering: for the cases when the user has provided seed features for some of the feature types  we propose a new model for multi-modal clustering  which combines regular clustering of non-seeded variables with an incremental  bootstrapping procedure for seeded variables:
1. represent documents as distributions over the sets ofseed features. ignore documents with zero probability given the seed features. cluster the remaining documents using the distributional clustering method.
1. stop if most documents have been clustered  see section 1 for details .
1. represent all features of the clustered documents as distributions over the document clusters. ignore features that have zero probabilitygiventhe clustered documents. cluster the remaining features using the distributional clustering method.
1. select feature clusters that contain the original seedwords. let the user revise the selected clusters: noisy features can be deleted; misplaced features can be relocated; new features can be added. the revised clusters of features are the new sets of seed features. go to 1.
1 combinatorial mrfs for clustering
a combinatorial markov random field  comraf   bekkerman and sahami  1 is a new framework for multi-modallearning in general  and for multi-modal clustering in particular. multi-modal  hard  clustering is a problem of simultaneously constructing m partitionings of m data modalities  e.g. of documents  their words  authors  titles etc. while clustering modalities simultaneously  one would overcome the statistical sparseness of the data representation  leading to a dense  smoothed joint distribution of the modalities that would result in  hypothetically  more accurate clusterings than the ones obtained when each modality is clustered separately. bekkerman et al.  empirically justify this hypothesis.
모a comraf model for multi-modal clustering is an undirected graphical model in which each data modality xi : 1 뫞 i 뫞 m corresponds to one discrete random variable  r.v. . this r.v. is defined over all possible clusterings of xi  which implies that the support of this r.v. is exponentiallylarge in the size of xi. we call such an r.v. a combinatorial r.v. let xi be an r.v. with an empirical distribution over xi  e.g. over documents in the dataset ; let x ij be an r.v. defined over clusters in the j-th clustering of xi; let x ic be a combinatorial r.v. defined over all the possible clusterings of xi. edges in the comraf graph g correspond to interactions between modalities  the graph is not necessarily fully connected . examples of comraf graphs are shown in figure 1.
모the objective is to construct clusterings of modalities  or  in other words  to find values of combinatorial r.v.'s  such that the sum of pairwise mutual information between the clusterings of the interacting modalities is maximized:
	x c  = argmax .	 1 
x c
j e
this objective function naturally factorizes over g  so that an efficient inference algorithm  such as iterative conditional mode-icm  besag  1   can be directly applied. the icm algorithm iterates over each node in g  which is optimized with respect to the current values of its neighbors.
모in the comraf case  the optimization of each node is a resource-consuming process. each clustering x cij can be represented as a point  cj1 cj1 ... cjni  in an ni-dimensional hypercube hi of all the possible clusterings  where ni is the number of elements of the i-th modality   meaning that element 1 belongs to cluster cj1  element 1 belongs to cluster cj1 etc. we apply the simplest combinatorial optimization algorithm-hill climbing  where the procedure starts at some point on hi and greedily searches for a nearby point that satisfies equation  1 . since the problem is non-convex random restarts are used to overcome local optima.
모in this paper  we propose an interactive learning approach  in which the user assists the clustering algorithm to avoid local optima. first  by selecting seed features  the user specifies a potentially good starting point on the hypercube hi. second  by correcting the constructed clustering after each iteration  the user causes a controlled jump from one region of hi to another  in which potentially better clusterings are located.
1 evaluation methodology
in this paper we use clustering accuracy as a quality measure of document clustering. let t be the set of ground truth categories. for each cluster d   let 붺t d   be the maximal number of elements of d that belong to one category. then  the precision prec d t    of d  with respect to t  is defined as prec d t    = 붺t d  /|d |. the micro-averaged precision of the entire clustering  which is the portion of documents appearing in the dominant categories. for all our experiments we fix the number of clusters to be equal to the number of categories. in this case 
prec d t 	  equals clustering accuracy.
모in our experiment with clustering by sentiment  we compare comraf clustering results with svm classification results. bekkerman and sahami  show that the clustering accuracy can be directly compared with the  standard  classification accuracy if a constructed clustering is well-balanced  meaning that each category prevails exactly in one cluster. it appears that all our clusterings obtained using the comraf model are well-balanced.
1 clustering by genre
according to the scenario proposed in section 1  let us set up an experiment of clustering documents by their genre. after fixing the number of clusters to be equal to the number
	~c	~c
	d	d
	~c	~c
	s	w
	 a 	 b 	 c 	 d 
figure 1: comraf graphs for:  a  1-way document clustering with pos unigrams as an observed r.v.  shaded node ;  b  1way clustering of documents and pos bigrams  same as for pos 1-grams or 1-grams ;  c  1-way clustering with bow;  d  1-way clustering with pos bigrams and bow.
of categories in our dataset  see section 1   we decide about feature types which would best match the task of clustering by genre. documents are labeled with genres on the basis of external criteria such as intended audience  purpose and activity type  lee  1 . the notion of genre can be described in terms of the syntax/semantics duality of text: documents of different genres use different syntactic constructions and/or different vocabulary. it is not obvious whether syntactic or semantic features play a major role in clustering documents by genre. we propose to take advantage of both. we represent documentsovertwo sets of features: words  that correspond to documents' vocabularies  and part-of-speech  pos  n-grams  that correspond to the syntactic structure of text . pos n-grams are extracted from sentences in an incremental manner: the first n-gram starts with the pos tag of the first word in the sentence  the second one starts with the tag of the second word etc.
모intuitively  one cannot come up with particular features that best capture documents' genres  e.g. it is hard to say whether a word 'clouds' is more often used in fiction  poetry or weather reports . to the contrary  document distributions over the entire set of features would be different for documents of different genres and are then the most appropriate representation of documents for clustering by genre. thus  we apply the multi-modal clustering method described in section 1  without the interactive learning component.
모given a document collection  let d be a random variable over its documents  w be a random variable over its words  and s be a random variable over the pos n-grams of its words. we apply a multi-modal comraf model  section 1  for constructing a clustering d  of documents  a clustering w  of words and/or a clustering s  of pos n-grams  by maximizing the objectivefrom equation  1 . in this paper  we consider four comraf models for clustering by genre:
모1. pos unigrams: since the number of pos tags in any tagging system is relatively small  it makes no sense to cluster pos unigrams. therefore  we apply a 1-way model for clustering documents using the comraf graph shown in figure 1 a . the objective function from equation  1  in this simple case has the form of i d ;s .
모1. pos n-grams  where n   1. the number of unique pos n-grams of order higher than 1 is exponential in n  so clustering them would be necessary. we perform a 1-way clustering with the comraf graph from figure 1 b  and the objective i d ;s  .
모1. bag-of-words: the number of unique words in our table 1: clustering by genre. clustering accuracy on the bnc corpus  averaged over four independent runs. standard error of the mean is shown after the  sign. comraf results with other pos tuples  besides bigrams  are in figure 1 left . the bow+pos hybrid setup is only applicable in comrafs.
dataset is comparable with the number of pos trigrams  so in analogy to the previous model  we perform a 1-way clustering with the comraf graph of figure 1 c  and the objective i d ;w   .
모1. bow+pos hybrid: we combine contextual information of bow and stylistic information of pos n-grams into a 1-way clustering model  where we simultaneously cluster documents  words and bigrams of pos tags. over the comraf graph of figure 1 d   we maximize the sum i d ;s   + i d ;w   .
1 dataset
we evaluate our models on the british national corpus  bnc   burnard  1 . we employ david lee's ontology of bnc genres  lee  1  with 1 genres covering most aspects of modern literature such as fiction prose  biography  technical report  news script and others. to perform fair evaluation using clustering accuracy  section 1   we choose 1 largest categories  for each of which we uniformly at random choose 1 documents  so our resulting dataset consists of 1 documents. the bnc texts are represented in sgml. we remove all markup  lowercase the text  and delete stopwords and low frequency words. all words in the bnc corpus are semi-manually tagged using 1 pos tags  four of which refer to punctuation. the resulting dataset has 1 unique words; and 1 pos bigrams. since the overall number of unique pos trigrams and fourgrams is prohibitively large  we apply more aggressive term filtering: we consider trigrams that appear in at least 1 documents  1 trigrams overall  and fourgrams that appear in between 1 and 1 documents  1 fourgrams .
1 results
we compare the results of our clustering model with the results of k-means  weka implementation   as well as of latent dirichlet allocation  lda -a popular generative model for unsupervised learning. we use xuerui wang's lda implementation  mccallum et al.  1  that performs gibbs sampling with 1 sampling iterations. table 1 summarizes the results which appear to be surprisingly good for an unsupervised method  given that the result of a random assignment of documents into 1 clusters would be about 1% accuracy. as shown  the 1-way comrafmodel significantly outperforms other  1-way  models. figure 1 shows results of stability tests of 1-way comraf models:  left  the pos n-gramsetup;  right  the bow setup. as shown on the left figure  the pos bigrams setup is preferable over the other pos tuples: it is more ef-
1 1 1 1 ngram size threshold on low frequent words
figure 1: clustering by genre. comraf clustering accuracy as a function of:  left  size of pos n-gram 1-grams  1-grams  1-grams and 1-grams ;  right  threshold on low frequency words-a point i on the x axis means that in this experiment words that appear in less than i documents are removed.
fective than unigrams  and almost as effective as trigrams and fourgrams  while being much more efficient.
1 clustering by sentiment
in clustering by authors' sentiment  data categories correspond to different levels of the authors' attitude to the discussed topic  e.g. liked/disliked  satisfied/unsatisfied etc. . the categories can be finer grained  strongly liked / somewhat liked etc. -as long as it is possible to distinguish between two adjacent categories.
모following the procedure described in section 1  after choosing the number of clusters and particular feature types  the user is asked to select a few seed features for each category. for clustering by sentiment  as well as for close tasks of clustering by authors' mood or by familiarity with the topic  relevant feature types may be words or word n-grams  i.e. semantic features . however  for other quite similar tasks  e.g. clustering by authors' age  not only semantics but also syntax can matter: children  for instance  use certain words more often than adults do; children also tend to use primitive  and sometimes erroneous  syntactic constructions   me going bye-bye  etc. . in this paper  for simplicity  we experiment with word features only.
모the task of selecting seed words has two issues. first  it is easier to come up with words that correspond to extreme sentimental categories  'spectacular'  'horrible'   but it is difficult to choose seed words for intermediate  mild categories. nevertheless  as we will see in section 1 users usually succeed in accomplishing this task. second  in our early experiments  users consistently tended to choose words that were out of the vocabulary of a given dataset. inspired by liu et al.   we decided to provide the users with a word list  to narrow her search only to the dataset vocabulary. unlike liu et al.   whose task is topical clustering  we cannot automatically predict which words would be relevant. instead  we employ zipf's law and provide the user with a list of words from the interior of the frequency spectrum. we anticipate such a list to contain the most relevant seed words.
모we then perform an iterative process of clustering that allows user's involvement in between clustering iterations. we apply a 1-way comraf model  see figure 1c : we first cluster documents that contain the selected seed words and then we cluster all words of these documents. in the latter step  our seed word groups are enriched with new words that have been clustered together with the original seed words. the user is then asked to edit the new seed word groups  in order to correct possible mistakes made by the system  word removal  relocation and addition is allowed . by this  a clustering iteration is completed and the next iteration can be executed.
모since the seed word groups have been enlarged  we can expect that a set of documents that contain these seed words is now larger as well  so that the clustering process will cover more and more documents from iteration to iteration. the process stops when no more documents are added to the pool. documents that have never been covered  the ones that contain no seed words from the largest seed word groups  are considered to be clustered incorrectly. an alternative approach to guarantee the algorithm's convergence would be to require enlargement of seed word groups such that at least one document is added to the clustering at each iteration. the algorithm would then stop when the entire dataset is covered. we choose the former approach because  a  we do not want to put additional constraints either on the user or on the comraf clustering model;  b  in each real-world dataset there can be documents whose sentimental flavor is hard to identify - it would not be beneficial to force such documents into any of the sentimental clusters.
1 experimental setup
we evaluate our interactive clustering system on a dataset of movie reviews. our dataset consists of 1 reviews written on  harry potter and the goblet of fire  1   that we downloaded from imdb.com in may 1 the data was preprocessed exactly as the bnc corpus. we ignore reviews that do not have rating scores assigned by the user. the imdb's scoring system is from 1  the worst  to 1  the best . based on our extensive experience with imdb.com  we translate these scores into four categories as follows: scores 1 to 1 are translated into the category strongly disliked  1 documents   scores 1 to 1 are translated into somewhat disliked  1 documents   scores 1 and 1 into somewhat liked  1 documents   and score 1 is translated into the category strongly liked  1 documents . we do not introduce a neutral category because there are very few neutral reviews on
imdb.com.
모on the task of clustering by sentiment  we compare our method's performance with that of an svm classifier trained on 1 movie reviews. the training data for the svm consisted of reviews of 1 popular hollywood movies released in 1  of the same genre as harry potter. the reviews and genre labels of movies are obtained from imdb.com. again  we ignore reviews without user-assigned rating.
모the system is evaluated on five users who are familiar with the task of document clustering. the users were explained
doc repres.	k-means	lda	comraf	svm
bow1111111sentim. list1111111interactive clustering  oracle 11n/asimulated classification  oracle 11table 1: clustering by sentiment. clustering accuracy vs. classification accuracy. standard error of the mean is shown after the  sign.
the idea behind interactive clustering and provided a brief description of the dataset. they were given a list of 1 words that appeared in 1 뫞 n   1 documents in our dataset. the users proceeded as described in section 1. also  we construct an oracle as follows: for each category t we select 1 most frequent words that belong to a given list of sentimental words1 and their distribution over the categories has a peak at t. unlike human users  the oracle does not provide feedback between clustering iterations. to some extent  the oracle's performance can be considered as an upper bound to results obtained in practice  when a human user is involved.
모we perform a simulated classification  sc  experiment analogous to the one of liu et al.   see a description in section 1   where the seed words are provided by our oracle. we replace an ad-hoc knn-like clustering in liu et al.'s implementation by our effective comraf clustering  and a naive bayes classifier by an svm.
1 results
table 1 summarizes our observations. surprisingly  with bow features  our comraf clustering method performs as well as an svm trained on a large amount of data  row 1 . a good performance of our unsupervised method  with bow  indicates that the constructed topical clustering sheds some light on reviewers' sentiments  which can occur when the reviewers have a consensus on certain aspects of the movie  e.g. liked the actors but disliked the plot etc.
모after feature selection according to our list of sentimental words  the comraf achieves a significant boost in accuracy surpassing the svm  row 1 . using an oracle in our interactive clustering setup  row 1  improves the performance even further  while the sc result  row 1  is only slightly  but significantly  inferior. these two results are close because the training set of sc is identical to the clustering constructed at the first iteration of the comraf algorithm. as its size appears to be over 1 of the entire dataset  there is almost no room for the actual diversity in performance of the two methods.
1 user 1	user 1	user 1	user 1	user 1	user 1 user 1 user 1 user 1 user 1	svm
figure 1: interactive clustering by sentiment. clustering accuracy over various users:  left  over interactive learning iterations  with original seed words only  after one correction step and after two correction steps . the horizontal line is svm performance  after feature extraction using a given list of sentimental words  and after training on over 1k documents ;  right 모figure 1  left  shows the accuracy  micro-averaged over the classes  for each user and each iteration. for three of the five users  selection of the initial seed words is sufficient to obtain significantly higher accuracy than the best result of the svm. user 1 has significantly lower accuracy than the baseline to begin with  but over the two correction steps is able to provide the necessary feedback so as to obtain an improvement in accuracy  equalling the baseline. we found that user 1 was fairly conservative in her assessment of terms in the over categories of the dataset after two correction steps.
beginning marking only 1 terms  while user 1  the one with the best average performance  marked 1 terms  1 of which were in common with user 1. user 1 reported that she aggressively removed words at the first correction step  which caused a noticeable drop in the performance.
모figure 1  right  shows the accuracy per class  per user at the end of 1 iterations. user 1 and user 1 have near identical accuracies on the two extreme categories  strongly liked and strongly disliked   but user 1 has higher accuracies on the intermediate categories  resulting in higher micro-averaged accuracy. it is apparent from this figure that users are able to come up with good features for the two extreme categories  but have difficulty with the intermediate categories. the figure also shows the performance of svm  with sentiment features . it is interesting to note that the svm's pattern of behavior is almost identical to the interactive comraf's.
1 conclusion
we have introduced an interactive clustering method that allows on-the-fly clustering of text collections according to any  especially non-topical  criterion the user can come up with. in our method  the user's prior knowledge on the importance of features is incorporated into the multi-modal clustering. we apply our method to clustering movie reviews by sentiment. it takes the user less than 1 minutes to choose initial seed words using which our system significantly outperforms an svm trained on a large amount of data. the subsequent correction steps however are often unnecessary. we also test our system on clustering by genre where seed features cannot be chosen. instead  we cluster documents together with their contextual and stylistic features and achieve good results.
acknowledgements
this work was supported in part by the center for intelligent information retrieval  in part by the defense advanced research projects agency  darpa  under contract number hr1-c1  and in part by the ministry of education  culture  sports  science and technology of japan under grant number kakenhi1.
