 
finding optimal solutions for job shop scheduling problems requires high computational effort  especially under consideration of uncertainty and frequent replanning. in contrast to computational solutions  domain experts are often able to derive good local dispatching heuristics by looking at typical problem instances. they can be efficiently applied by looking at few relevant features. however  these rules are usually not optimal  especially in complex decision situations. here we describe an approach that tries to combine both worlds. a neural network based agent autonomously optimizes its local dispatching policy with respect to a global optimization goal  defined for the overall plant. on two benchmark scheduling problems  we show both learning and generalization abilities of the proposed approach. 
1 	introduction 
production scheduling is the allocation of limited resources to tasks over time  while one or more objectives have to be optimized. many variants of the basic problem formulation exist  and most of them are np-hard to solve  pinedo  1   meaning that exact solution algorithms suffer from a non-polynomial increase of computation time. this constitutes a problem not only if the problem to solve surmounts a certain size  but also in moderately complex domains  where the occurrence of new or unexpected events - the arrival of new jobs or the breakdown of machines - makes frequent replanning necessary. even more  technological changes like semiconductor fabrication or thin film production are posing additional challenges  since new problem structures - like conditional loops in the production process - occur  for which conventional optimization techniques may not be applicable. 
an alternative and far less time-consuming way is the application of simple heuristic dispatch-
1 	machine learning 
m a r t i n riedmiller 
computer science department 
 ilkd  
university of karlsruhe d-1 karlsruhe 
riedml ira.uka.de 
ing rules that select the job to process next on an idle resource depending on the current situation. however  these dispatching rules only reflect heuristic knowledge and do not guarantee to lead to the optimal behaviour of the overall system. even for experienced human experts it may become arbitrarily difficult to decide which dispatching rule to apply and how to time it in a certain scenario  since the effects on the dynamics of the overall system can hardly be predicted. 
	1 	general idea 
here we propose an alternative way that allows to combine the desire for  nearly  optimal solutions with a time-efficient computation  provided by resource-coupled dispatching rules. the idea is to have learning agents  that are associated to each resource and determine the local dispatching policy. this policy is not fixed  but instead is autonomously learned by getting feedback of the overall dynamic behaviour of the production system. in contrast to common dispatching rules which typically only consider few characteristic features of the current situation to make the decision  a learning agent can deal with more state information and therefore figure out more sophisticated policies  which are better tailored to the process. the appearance of the proposed training method does not depend on the optimization goal or constraints posed by the production process. therefore  it is applicable to a wide range of problem instances. as an example  in this article we focus on job shop scheduling problems with the goal to reduce the summed tardiness. the following summarizes the main characteristics of the proposed approach: 
  autonomous 	acquisition 	of 
 approximately  optimal dispatching policies  including the adaptation to the plant structure and the inherent consideration of constraints  

  reusability of acquired knowledge 
  fast situation dependent decision making  ability to do reactive scheduling 
1 	related work 
recently  several reinforcement learning approaches have been proposed to solve certain aspects of scheduling problems. they vary in the type and their view of the scheduling problem and - as a consequence - the type of control decisions. zhang and dietterich  dietterich and zhang  1  propose an rl approach that learns a neural value-function to guide a repairbased scheduler. an action in this approach is the decision for a certain repair operation. schneider  boyan and moore  schneider et al.  
1  present a value-function-based approach for the problem of demand based scheduling. the learning scheduler decides over a set of possible factory configurations to maximize expected production profit in the presence of varying demand curves. in contrast to these global approaches  a local multi-agent view of a production scheduling problem is taken in  brauer and weiss  1 . each machine does make a local decision which job to process next based on the estimated completion time of the candidate jobs. the learning rule is based on the propagation of these estimates along the production line of a job  similar to the qrouting algorithm  boyan and littman  1 . an application of average-reward rl is presented by mahadevan and theocharous  mahadevan and theocharous  1  for the control of a transfer-line. 
our approach follows the idea of local decision making. the neural network based agent considers the current situation  represented by a set of relevant  local  features  to make its decision which job to process next. it learns a 
local decision policy  similar to heuristic dispatching rules  with the ability to adapt to the user-defined optimization goal. this is done autonomously  meaning that the policy is selfimproved by repeating a certain number of typical training cases. 
1 	task description 
the task considered in the following is to process a set of m jobs on the n resources of a factory. each job j  1 . . . m  consists of a certain 
number of lj basic operations  a basic operation must be performed on a certain resource k  1... .n and has a certain processing time. a job is finished after completion of its last operation. if the completion time of job j is larger than a certain due-date   then the job is said to be tardy. the tardiness of a job is zero  if it is finished before or at its duedate; otherwise it is in general scheduling objectives to be optimized all relate to the completion time of the jobs. various variants of the basic optimization problem exist which can be classified within a complexity hierarchy  pinedo  1 . in the following  we look at the problem of minimizing the summed tardiness over all jobs    solving this deterministic problem subsumizes also the total completion time problem as well as the maximum lateness or maximum makespan problems. being np-hard  it is not possible to solve this problem with a polynomial time algorithm. 
1 	description of the solution approach 
1 optimization problem and decision making 
the global production scheduling problem can be described as a markov decision process  mdp : the system's state s t  is described by the current situation of the n resources and the processing state of the to jobs  a decision a t  describes which job is processed next on a waiting resource. the goal of scheduling is to find an optimal policy  such that production costs r s  a  t  accumulated over time are minimized 
 1  
where denotes the time after which the last job is finished. costs r s a t  may depend both on the current situation and on the selected decision and express the desired optimization goal. for example  costs may arise due to the tardiness of a job  due to a resource waiting idle  or due to costs caused by the need to change a tool before continuing processing  and so on. 
in our approach  the global decision a t  is a vector of single decisions  made by distributed agents each associated with one of the n resources. if a resource is ready to process a 
new operation  then its agent chooses one job out of a set ak of jobs  where ak is the set of jobs for which the next operation oji must be processed on resource k. the resource is then occupied for the duration of oji. after this time  a new decision is made. 
1 	learning algorithm 
each agent makes its decision based on a local 
view  of the global plant situation at time t. sk represents the local view 
riedmiller and riedmiller 

of the agent. it can be thought of compressing the huge amount of global state information into features that are relevant for making the local decision at resource k. learning here means iteratively improving the decision policy with respect to the optimization of the global costs  1 . this is done by a q-learning related learning-rule adapted to the local decision process: 

the learning rule relates the local decision process as experienced by agent k to the global optimization goal by considering the global direct costs r . . since the time between the decisions varies depending on the duration of the currently processed operation  the 'reinforcement term' accumulates the global costs between t and 
mated accumulated global costs  if in situation sk the job denoted by  would be processed next. a policy greedy with respect to   thus will choose that job next  that will lead to an optimization of the performance of the overall plant. the policy of the agent is determined by greedily exploiting the value function  

during learning  a random exploration strategy is performed that deviates from time to time from the current policy. 
the agent's learning rule varies from the qlearning assumptions in two important issues: a  no central global decision is made  but instead  the global decision is composed by individual decisions of time-varying policies. unless all policies are stable  this makes the process seen by the local agent non-stationary  in contrast  a single q-learning agent assumes to experience a stationary environment   b  an agent does only use compressed information of the complete state. with an agent's local state information  the observed system behaviour may become stochastic or even unpredictable. however  there is empirical evidence that q-learning works in this scenario  barto and crites  1 . in the experiments in section 1 the problems mentioned in a  are circumvented  since in this paper only situations with a single local learning agent are examined. 
1 	machine learning 
1 	choice of r .  
since it is our objective to minimize the summed tardiness of all jobs  we have to choose r    such  that the minimization of  1  is equivalent to the minimization of the tardiness t. first  r .  is the sum of the costs  associated with the jobs 

two formulations of  are possible. the first is to compute the tardiness after the job is finished 

                                            1  the second possibility is to have costs in each time step during processing  if the job is currently too late but not ready yet: 

                                            1  although both formulations are equivalent with respect to the general problem formulation  the latter choice has the advantage  that the cost function directly reflects the tardiness when it actually occurs  which may help the learning system. therefore it is used in the experiments in section 1. 
1 	learning system 
we choose a multilayer-perceptron neural network for representing the value function q .  for two reasons: a  the state space is continuous and therefore no finite scheme  like a lookuptable  can represent all states  b  we want to exploit the generalization ability of the neural network to find general policies  i.e. the q function should generalize to unknown situations. input for the function approximator is an adequate description of the current decision situation by a feature vector. the features have to comply with the following requirements: they should relate to the future expected costs and should be characteristic for the present situation. features should represent characteristics of typical problem classes rather than of individual instances  such that the acquired knowledge can be generalized to new problem instances. for this reason  few attributes are considered  mainly describing the local situation at the machine. with respect to practical applicability  features should be computable 

out of data available from common commercial ppc-systems. in order to keep the network input small  redundant information should be avoided. dealing with a time-dependent problem  besides static properties dynamic features should be concerned. they are not only dependent on the job or resource properties  e.g. processing time  but also on time progress  e.g. slack . input to the neural network are local state features sk t  plus features coding the available decision  possible features are: 
state features sk ¡ê : 
  describing general characteristics of the problem: 'tightness' r  1  and 'distribution' r  1  of the jobs with respect to their due-dates  pinedo  1  
  describing the current situation: estimated tardiness  estimated makespan c  1   average slack 
decision features/ job characteristics: 
  describing characteristics of job j with respect to the present situation in ak'. e.g. due date index  edd   1   relative slack  slack index  ms   1   relative waiting time  fifo   1   relative processing time  processing time index  spt  lpt  
  describing the immediate consequences  
i.e. the properties of the remaining operations if job j was selected: e.g. average remaining slack 
  describing the relationship job/ operation  
i.e. the significance of operation oij for job j: e.g. relative work in process  relative 
buffer 
1 e m p i r i c a l e v a l u a t i o n 
1 	research objectives 
the following experiments show the behaviour of the proposed learning approach in comparison to heuristic dispatching rules. in detail  we examine the following issues: 
  how does the choice of input features in-fluence the optimality of the policy found by the learning system  
  is the learned policy general  i.e. does the policy show good performance when applied to untrained situations  
  does the proposed learning scheme work  i.e. is it possible to improve the decision policy of the local learning agent with respect to the global goal autonomously  
  how does the learned policy perform com-pared to heuristic dispatching rules  
1 description of the experiments 
we examine two cases: a single resource case  as a demonstration for the principle working and performance capabilities  and a multi resource case  to show the capability of the agent to work within a multi-agent scenario. in both cases  1 different production scenarios are used during the training phase. each production scenario has a random number of jobs  1 to 1  with different processing times and different due-dates. experiments were based on a random generation of problems with different problem characteristics  number of jobs  loads  tightness of jobs  due-date-range  ... . in the single resource case  each job has one operation  whereas in the multi-resource case  each job has a random number of basic operations. each operation has a random duration and must be processed on a certain resource. in the multiresource case we also allow circles - i.e. a job may have to visit one resource multiple times - which constitutes an additional difficulty for conventional solution algorithms. the mean process duration and the mean due-date were chosen such that 'interesting' scenarios are created  i.e. that arbitrary policies are not likely to produce acceptable solutions. as mentioned above  the production objective considered here is to reduce the overall tardiness of all the jobs. to test the generalization ability  1 test scenarios were generated  which vary from the ones considered in the learning phase. 
for learning  we used a multi-layer perceptron with up to 1 inputs  1 hidden neurons and one output neuron. the learning rate was set to 1  since it was not our goal to optimize the learning speed  not much effort was done to find an optimal parameter here . during learning  the jobs are selected randomly  exploration factor = 1 . the performance is reported in terms of the average tardiness of the jobs when acting greedily with respect to the current value function. 
1 t h e one resource case 
table 1 shows the performance of some typical heuristic dispatching rules. the lpt-policy chooses the job with the longest processing time first  the minimum-slack  ms -policy chooses the job with the minimum time between the expected termination and the due-date  and the edd-policy chooses the job with the most urgent due-date. the average tardiness per job varies considerably on the 1 training scenarios. lpt performs worst with an average tardiness of 1  even worse than a random policy  1 . ms works considerably better showing an average tardiness of 1  and edd perfoms 
riedmiller and riedmiller 

best with an average tardiness of 1. 

table 1: average tardiness on the training set for different heuristic dispatching policies 
to test the learning capability of the neural dispatching agent  several combinations of input features were tested  the feature numbers in table 1 correspond to the numbering in section 1 . in general  with a sensitive choice of input features the performance of the learning system did improve considerably with the number of production runs  remember that the average tardiness of a random policy is 1 . not surprisingly  the final performance depends crucially on the provided input information. we observed that the performance of the system with features that are also considered by the 
edd-rule has the same final performance as the 
edd-rule  column 1 . this means  that the learning system was able to extract this rule automatically out of the experience it made. analogously  the same was true for the msrule  column 1 . when we gave the combination of both features to the learning system  it was able to find a new policy  that is better than both edd and ms  column 1 . actually  this is the effect we are expecting the learning agent to exploit: considering a combination of features that are of different importance in different situations and acquiring a new  probably very complicated  policy based on the input. however  adding more features not always improves the performance here  column 1 . 

table 1: learning agent: average tardiness on the training set for different input feature combinations 
figure 1 gives an impression of the learning process. the bold line shows the performance on the training set. after about 1 production runs  the system has a performance of 1 and thus already beats the edd-policy  1 . in course of learning  the performance is further improved. 
	1 	generalization ability 
besides the principle learning and optimization capabilities  one major effect we expect to observe is the generalization ability of the learning system. to test it  the trained neural agent is applied to situations not included in the training set. the results are shown in table 1. 
1 	machine learning 
figure 1: improvement of the performance of the learning system with an increasing number of production runs. the average performance of a random policy was 1  and the best heuristic policy  edd  achieved an average tardiness of 1. the learning agent beats edd after only 1 production runs  bold line . 


table 1: average tardiness for different dispatching policies on the test set  test for generalization  
again  the neural agent shows the best performance 1 and beats the best heuristic rule considerably  which is the spt-rule here with 1. this shows  that the learning agent is not only able to optimize its performance on a certain set of training cases  but also is able to generalize this knowledge to new  previously unknown cases without retraining. it may also be derived  that the selected features fulfill the requirement of problem independency. 
	1 	t h e multi-resource case 
in the multi-resource benchmark we consider a plant consisting of 1 resources. here we examine the ability of one learning agent to adapt to the behaviour of a complex process. this behaviour is determined by the job profile and the structure of the plant  but in contrast to the previous scenario  also dispatching policies for the other resources play an important role. while we are examining the case of one learning agent and the other policies being fixed  in future experiments we will examine situations of multiple agents learning simultaneously. 
in the training scenario  for example  when two fixed agents are acting according to the fifo  first-in-first-out  rule  than the performance of the third agent can be improved from 1  the case of also acting according to the fifo 


table 1: average tardiness for different dispatching policies in the multi-resource case. the horizontal row denotes the fixed policies of resource 1 and 1  the vertical row compares the policy of resource 1 for a fixed policy and a neural learning policy   
principle  to 1  when a learning policy is applied . in order to get an optimal behaviour  the learning agent has to consider the future processing policy of a candidate job  too. as an additional difficulty  since circles may occur during the lifetime of a job  the current decision now also determines the future development of the candidate set. as can be seen in table 1  the learning agent is capable to deal with the described difficulties. in all training cases  left side  and all test cases  right side  the learning policy outperformed the fixed policy. the agent has autonomously acquired a local policy based on few relevant decision features  that is able to perform well in a complex environment with complex dynamics. 
1 conclusions 
the paper describes a neural network based local learning approach to job shop scheduling problems. it is based on local learning agents  associated to a resource. the agent has a restricted view on the complete factory's state  representing the most important features that are needed for the local decision. the learning rule relates a local value function with costs depending on the overall performance of the global plant. doing so  the acquired local decision policy is coordinated with the global optimization goal. the experiments on a oneresource and a three-resource production plant show the capability to learn local policies to optimize global behaviour from experience. furthermore  the agent's policy can be generalized to unknown situations without retraining. therefore  the learned policies are more tailored to the actual task than comparable heuristic dispatching rules  but still are general enough to be valid in a wide range of untrained situations. in case of major changes in the organizational structure the proposed learning architecture allows an easy reconfiguration of the reactive scheduling policy. 
1 acknowledgments 
a major part of this work was carried out during a research visit at the robotics institute at carnegie mellon university  pittsburgh  pa. the authors gratefully thank stephen smith  andrew moore and jeff schneider for valuable discussions. special thanks to dieter spath  hartmut weule  juergen schmidt and wolfram 
menzel for giving us the opportunity and the support for this stay. 
