 
in this paper  we are mainly concerned with logical compilations of propositional knowledge bases. we propose a new approach to equivalence-preserving knowledge compilation based on a generalization of the standard notion of prime implicate  the theory 
prime implicates. our approach consists in taking advantage of tractable theories implied by the knowledge base to make local by computing the theory prime implicates of w.r.t. . as a main result  query answering from theory prime implicates compilations can be done in time polynomial in their sizes. while there is no guarantee that the size of a compilation is not exponentially greater than the size of the original knowledge base  we show that it is smaller than the size of the prime implicates compilation in the general case and it can even be exponentially smaller. additionally  we present some experimental results providing evidence for the substantial space savings achievable with our compilation technique. 
1 introduction 
query answering from a propositional knowledge base is a central concern in artificial intelligence. unfortunately  its computational complexity is high  both theoretically  conp-complete   and practically since every existing algorithm runs in time exponential in the size of the knowledge base in the worst case. 
   to improve query answering and deal with its intractability  several approaches have been proposed so far. a first approach consists in restricting the representation language  e.g. considering only horn formulas  or the inference mechanisms  e.g. involving resource-bounded reasoning . another approach consists in approximating the knowledge base  selman and kautz  1; kautz and selman  1  or the logical entailment relation  cadoli and schaerf  1 . 
   a third family gathers equivalence-preserving compilation techniques. compiling a knowledge base consists in translating  compiling  the base during a preprocessing phase into an equivalent compiled form  a compilation  from which query answering is tractable. in contrast to the two other approaches  equivalence-preserving compilation does not give up expressiveness or equivalence but simply speeds inference by shifting computational costs from the on-line query answering process to an off-line compilation process. 
relevant to this last approach is  among others  e.g.  del 
val  1; mathieu and delahaye  1; dechter and rish  
1    reiter and de kleer's technique which consists in turning the knowledge base  into the prime implicates normal form  reiter and de kleer  1 . 
　interestingly  this last technique relies on a more general and computationally valuable principle of locality. let us say that a knowledge base is local for query answering when  for every formula is a logical consequence of iff there exists a formula in s.t. is a logical consequence of now  let us assume that contains n formulas of size m and that is of size p. the time cost for answering query from is  in practice  exponential in in the worst case. when is local  the cost comes down to which can easily be exponentially smaller than additionally  query answering from formulas rt can be computationally easier than query answering from the whole knowledge base. this is exactly what happens with prime implicates knowledge bases: they are local for clausal query answering 
 i.e. considering queries limited to clauses  and  while  is exponential in the size of is polynomial in the size of  checking whether a clause entails a clause can be done in time 
   in this paper  we propose a new approach to equivalencepreserving knowledge compilation which extends reiter and de kleer's technique. our approach is based on both: 
 l the idea of expressiveness restrictions and/or compilations that make inferences tractable. thus  we propose a compilation function which takes advantage of tractable theories  among those implied by this leads us to generalize the standard logical entailment relation to the relation where is built-in. 
 1  the idea of locality for clausal query answering. to be more precise  our compilation function aims at making local for clausal query answering by computing the theory prime implicates of w.r.t. theory prime implicates are to *= j  as prime implicates are to 
   the rest of this paper is organized as follows. some formal preliminaries are given in section 1. in section 1  the concept of theory prime implicate is introduced. some metatheoretic properties and a procedure for generating theory prime implicates are successively pointed out. in section 1  we present our compilation function which basically consists in computing the theory prime implicates of 
w.r.t. a tractable theory implied by  as a central result  
	marquis 	1 


1 	knowledge representation 

	marquis 	1 


1 main characteristics of theory prime implicates compilations 
our approach to knowledge compilation possesses several interesting features. a strong point is that it is modeltheoretic. in particular  the number of clauses of compo l  does not depend on any particular ordering of symbols but on the models of o only. this is not true for many approaches to knowledge compilation  particularly fpio  del val  1  and ed  dechter and rish  1 . we can also prove that fpio and fpi1 compilations depend as well on a particular ordering of symbols; particularly  the way in which implicates are generated in fpio and fpi1 may dramatically bear on the size of the resulting compilation  i.e. by an exponential factor . 
　　another strong point of our approach is that it is generic in essence. it may give rise to many compilations  taking advantage of many tractable classes of theories in propositional logic. for instance  the class of theories which are unit-refutation complete  including the horn and the reverse horn theories  and the class of theories consisting of binary clauses  krom formulas  are polynomial for sat  see  jones and laaser  1    cook  1  . it is easy to prove that they are also stable by expansion with unit clauses  i.e. if o belongs to such a class c then o u     ln   where lj are unit clauses  belongs to c . consequently  all these classes are tractable. additionally  the classes of theories which result from any equivalence-preserving compilation technique  including fpio  fpi   fpi1  ed compilations and all the techniques that remain to be discovered  are tractable  by definition . all these classes of theories can be used to generate theory prime implicates compilations. incidentally  this shows that our approach can be considered complementary as well as supplementary to the existing  and future!  knowledge compilation techniques. 

that the set of all the literals of propps is an efficient basis for the set of all the conjunctions of literals -i1c* of propps  see  moses and tennenholtz  1  for details . 
　　another valuable consequence offered by locality is the possibility to organize the theory prime implicates of i w.r.t. o. thus  attaching to each minimal implicate the frequency with which it is used to answer a query  we can order such implicates in a decreasing way. since the ordering may vary dynamically  the organization of the compilation evolves with the queries and this may result in a an improvement of query answering with time. interestingly  this is fully compatible with the common point of view stating that every piece of knowledge which is often used must be easy to remember. clearly enough  such a behaviour is not guaranteed by the existing approaches to compilation  with  reiter and de kleer  1  as a notable exception  since they do not suggest any way to organize  the knowledge. 
   interestingly  our compilation technique can also be shown incremental and anytime. in fact  the results pointed out in  del val  1  can be easily adaptated to our framework  the structure of t p i is close to the structure of fpio . thus  provided that t p i uses an incremental prime implicate procedure inside  each time the knowledge base is revised  i.e. a new clause is added   recompiling the base from scratch is not mandatory. note however that a small change in the knowledge base may result in a significant  and computationally expensive  change in the compilation  thus  from a computational point of view  incremental prime implicate algorithms do not behave better than nonincremental ones in the worst case  kean and tsiknis  1  . a further advantage of our compilation technique is that it is anytime; it means that the knowledge base can be queried before compiling ends up. to be more precise  t p i can be interrupted at any time and produce some useful intermediate results nevertheless: while time passes  the number of queries that can be answered successfully increases and when the compiling process ends  every query can be answered. as  del val  1  underlines it  the interest of this  convergent approximation  view of compilation is that it has the potential to greatly decrease the inconvenience of using off-line computation. 
   despite many advantages  our approach to knowledge compilation does not overcome the main impediment of 

1 	knowledge representation 

existing compilation techniques: there is no guarantee that the size of our compilation is not exponentially greater than the size of the original knowledge base. for instance  let us consider the empty theory o =def { . the empty theory is tractable but theory prime implicates compilations with empty theory reduce to prime implicates compilations 
 cf. ′ 1 . hence  their size can easily be exponential in l♀l  chandra and markowsky  1; kean and tsiknis  1 . in this situation  query answering  while tractable w.r.t. comp j  i   is still exponential in the size of i. 
   however  our approach does not behave computationally worse than its predecessors in this respect since  for all the compilation techniques pointed out so far  an exponential growth of the knowledge base may result from compiling. moreover  due to the complexity considerations pointed out in  kautz and selman  1   it is unlikely that a given compilation technique definitely outperforms by an 


1 pointing out  good   tractable theories 
since all the theory prime implicates compilations of i do not behave identically w.r.t. query answering from a computational point of view  a way to compare such compilations and a notion of optimal compilation of a knowledge base must be defined. intuitively  an optimal compilation is one that makes query answering the most efficient. however  we cannot define a unique criterion  significant and computable in practice  for characterizing the notion of optimal compilation in full generality. 
   in order to assess  w.r.t. query answering in the worst case  a possible criterion c is the time complexity of 1 w.r.t. query answering per the size of 
unfortunately  given a knowledge base we did not find a 
procedure to point out  in time polynomial in a tractable theory leading to an optimal w.r.t. c  we suspect that there is no such polytime procedure . 
　instead  we propose a time-bounded heuristic search strategy through the space of tractable theories. thus  our strategy basically consists in pointing out a potentially good tractable theory from compile w.r.t.    then retain if it is better than every compilation encountered so far  reject it otherwise. the search terminates when a satisfying compilation has been found or when the time devoted to search is wasted. clearly enough  our strategy is fully compatible with del val's view of compilation as a process of increasing the efficiency and the quality of query answering over the life cycle of the knowledge base  del val  1 . 
　applying this strategy requires to deai with the size of the space of tractable theories. it may exist a large number of tractable theories which are logical consequences of a given knowledge base   and we surely do not know all of them! . consequently  we need some criteria to prune the search space and focus on promising candidates only. to this end  we mainly consider the time complexity of w.r.t. query answering and an estimation of the computational resources to be spent to point out in particular  tractable theories that cannot be put forward in time polynomial in are considered as a last resort. the logical strength of is taken into account as an additional criterion  when it does not conflict with the two previous criteria . indeed  in the light of corollary 1  we know that it is sufficient to consider logically strongest theories only in order to minimize the number of clauses in the corresponding compilations. since checking logical entailment is intractable in the general case  we focus in practice on computationally easier relationships between theories  particularly on set-inclusion. 
   thus  given a knowledge base  we will successively take into account as potentially good theories: 
1/ the set 	of all the horn clauses of 
1/ the set 	of all the binary clauses of 
1/ subsets of such that the tied chain graph is acyclic  if so  is unit-refutation complete  del val  1  . 1/ sets ed    where   is a subset of such that the induced width of 1 is lower or equal to 1 or the induced diversity of o is lower or equal to 1  if so   can be compiled under the form of a directional extension without an exponential growth  dechter and rish  1  . 
1/ sets l where cf is an equivalence-preserving compilation function  or cf is horn lub  selman and kautz  1   and is a subset of  provided that does not exceed from more than around one order of magnitude. 
   when several choices are possible  steps 1/ 1/ 1/   we prefer maximal sets  w.r.t. set-inclusion  if we have a way to point them out in polynomial time in . otherwise  we try to approximate them. if it is not possible  we pick up randomly any possible choice. 
   to restrict the search space further  an additional heuristic can be used; it consists in setting aside every theory 
is lower than a given threshold. this 
may prevent us from taking into account unpromising theories  e.g. if there is no horn clause in we jump directly to the set of all the binary clauses of and we do not generate 
   clearly enough  our strategy is very inefficient in the general case since an experimental post-hoc evaluation is necessary to check the choices that have been made. however  existing compilation techniques do not behave better here: in the general case  there is no way to estimate the size of a compilation before computing it1. 
1 a comparison with some other approaches to knowledge compilation 
before comparing our approach with some existing techniques  we must define the way in which the comparison will be done. as a matter of fact  a crucial complexity factor of pi  fpi    fpi1  fpi1 and ed compilations w.r.t. query answering is the size of the compilation  the number of 
1  this is not possible in the general case. for instance  determining maximal subsets of  that do not contain a tiedchain by focusing on maximal acyclic subgraphs of cannot be done in time polynomial in  in the general case since this last problem is np-complete. 
1  this is not exact for the approach of  dechter and rish  
1  since the size of a directional extension is related to its induced width and to its induced diversity. these factors can be determined without computing the compilation but this does not help so much since determining a best ordering -i.e. one leading to a smallest induced width or a smallest induced diversity- is not tractable in the general case. 
	marquis 	1 


1 	knowledge representation 

1 conclusion 
in this paper  we have been concerned with equivalencepreserving logical compilation of propositional knowledge bases. the main contribution of this paper is a new approach to compilation  based on the concept of theory prime implicates  a generalization of the standard notion of prime implicates. our approach can be viewed as an additional element in the panoply of techniques designed to deal with intractability of propositional query answering. interestingly  our approach can also be viewed as complementary to its predecessors. analytically and experimentally  we have shown that theory prime implicates compilation can achieve substantial space savings w.r.t. some other compilation techniques. 
   this work must be extended in several directions. a first direction concerns the empirical validation of the approach. more experiments are clearly needed to assess its practical applicability. a second direction is related to the choice of a good theory. we have only a very limited understanding of how to point out such good theories. accordingly  the naive strategy we have proposed must be improved. more generally  the key problem of predicting whether a given compilation technique is well-suited to a given knowledge base should be addressed in the future. 
acknowledgements 
the author would like to thank alvaro del val for several fruitful discussions and the anonymous reviewers for their helpful comments. 
