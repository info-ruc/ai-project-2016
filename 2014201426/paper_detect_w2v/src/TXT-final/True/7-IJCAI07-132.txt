
an  active learning system  will sequentially decide which unlabeled instance to label  with the goal of efficiently gathering the information necessary to produce a good classifier. some such systems greedily select the next instance based only on properties of that instance and the few currently labeled points - e.g.  selecting the one closest to the current classification boundary. unfortunately  these approaches ignore the valuable information contained in the other unlabeled instances  which can help identify a good classifier much faster. for the previous approaches that do exploit this unlabeled data  this information is mostly used in a conservative way. one common property of the approaches in the literature is that the active learner sticks to one single query selection criterion in the whole process. we propose a system  mm+m  that selects the query instance that is able to provide the maximum conditional mutual information about the labels of the unlabeled instances  given the labeled data  in an optimistic way. this approach implicitly exploits the discriminative partition information contained in the unlabeled data. instead of using one selection criterion  mm+m also employs a simple on-line method that changes its selection rule when it encountersan  unexpected label . our empirical results demonstrate that this new approach works effectively.
1 introduction
there are many situations where unlabeled instances are plentiful and cheap  but it is expensive to label these instances. for example  consider the challenge of learning a classifier that can determine which webpages contain job ads. one can easily grab literally billions of webpages at essentially no cost. however  to produce an effective training set  we first need labels for a sufficient number of these pages; unfortunately  this typically requires paying a person to produce each such label. it is therefore useful to find a small set of pages that  when labeled  will produce a high quality classifier. an  active learning  system will sequentially select the most informative pages to label from a pool of unlabeled pages  then produce a classifier from these labeled pages. an active learner is good if it produces the best classifier from a small number of labeled pages.
모this paper presents an effective active learner   mm+m   based on two ideas:  1  in general  select the query instance that provides the maximum conditional mutual information about the labels of the unlabeled instances  given the labeled data. there is a subtlety here that we resolve by using an  optimistic guess  about the query's label.  1  it can be helpful to allow the selection criterion to depend on the outcomes of previous selections. in our case  if this optimistic guess is wrong for the selected query  mm+m uses a differentstrategy to identify the next query  then returns to use the maximum information strategy for the next selection.1
모section 1 presents related works  to help position  and motivate  our approach. section 1 then introduces our specific mm+m approach  and section 1 presents experimental results based on our implementation of these ideas. the webpage http://www.cs.ualberta.ca/ greiner/ research/optimisticactivelearning provides more information about our approach  and the experiments.
1 related work
many previous researchers have addressed this  active learning  task in various different ways. some of their systems use a very simple heuristic to determine which instance to label next: select the most uncertain instance  based on the classifier produced using the current set of labeled instances. freund et al.  employed a committee of classifiers and choose the instance on which the committee members disagree. lewis and gale  used a probabilistic classifier  over binary classes; here this most-uncertain approach would select the instance whose conditional probability p yi = 1|xi   is closest to 1. the same principle is also used in active learning with support vector machines  tong and koller  1; schohn and cohn  1; campbell et al.  1   where it suggests choosing the instance closest to the classification boundary. tong and koller  analyzed this active learning as a version space reduction process  while schohn and cohn  used a heuristic search view. our mm+m algorithm incorporates this approach as one of its components. while this  most uncertain  approach often works well  we provide empirical results that the complete mm+m system typically works better.
모these  most uncertain  approaches decide which instance to select based only on how that instance relates to the current classifier s   which in turn is based on only a few labeled instances; notice in particular this selection does not depend on the remaining unlabeled instances. as the goal is producing a classifier that has a good generalization performance  it makes sense to use  at least  the marginal distribution p x  over this unlabeled data. this motivates a second class of approaches  which use this unlabeled data. cohn et al.  and zhang and chen  employed the unlabeled data by using the prior density p x  as weights. roy and mccallum  selected instances to optimize expected generalization error over the unlabeled data. others also used the clustering distribution of the unlabeled instances: xu et al.  proposed a representative sampling approach that selected the cluster centers of the instances lying within the margin of the support vector machine. nguyen and smeulders  presented a mathematical model that explicitly combines clustering and active learning together. mccallum and nigam  used an em approach to integrate the information from unlabeled data  while muslea et al.  combined active learning with semi-supervised learning.
모our mm+m uses an  optimistic  information theoretic way to use the unlabeled instances: seek the instance whose optimistic label  i.e.  the  best  of its possible labels  leads to the maximum mutual information about the labels of the remaining unlabeled instances. this approach implicitly exploits the clustering information contained in the unlabeled data  in an optimistic way.
모while this optimistic heuristic typically works  it is occasionally misled. when this happens  mm+m employs a different rule to select the next instance to label. this means our selection process is not as myopic as the other approaches  as it is based on the outcome of the previous instance. our empirical results show that this component is critical to mm+m's success.
1 the mm+m active learner
we let x denote the input features of an instance  y 뫍 {1 ... k} denote the class label  l = { x1 y1  ...  xn yn } denote the set of labeled instances and u denote the index set for the unlabeled data {x1 ... xm}. here  xu refers to the set of unlabeled instances.
모our approachuses probabilisticclassifiers that computethe posterior distribution of the class label y  conditioned on the input x; our mm+m implementation uses logistic regression.
1 using mutual information
as the goal of active learning is to learn the best classifier with the least number of labeled instances  it may make sense to select the instance whose label provides the most informa-
		 
		 
	 	  a	 
figure 1: problem with optimizing  1 
tion; i.e.  the one we currently know least about:
	argmaxh yi |xi l  .	 1 
i뫍u
where

represents the conditional entropy of the unknown label yi wrt the instance xi given the labeled data l. for binary classes  this measure prefers instances whose conditional distribution p y = 1|x l  is closest to 1. we refer to this as the  mu-instance   for  most uncertain  .
모unfortunately  this mu approach is limited in that its assessment of an instance involves only the small set of currently-labeled instances  that produce the classifier used in this step  but not the distribution of the other unlabeled instances. to see why this is problematic  consider figure 1. the mu-instance here will be the one nearest the bisector -  a . notice  however  this label does not provide as much information about the remaining unlabeled instances as b: as b is in the midst of a cluster of points  its value will significantly increase our confidence in the labels of the neighbors.
모based on this observation  we propose using a mutual information criterion for instance selection: select the instance whose label will provide maximum mutual information about the labels of the remaining unlabeled instances  given the labeled data:
argmax{h yu |xu l    h yu |xu l  xi yi  }  1 
i뫍u
as the first term in  1  does not depend on the instance i selected  we can rewrite  1  as:
	argminh yu |xu l  xi yi  .	 1 
i뫍u
assuming we use a parametric probabilistic conditional model p y |x 붿   for the classification task  and use maximum likelihood for parameter 붿 estimation  then the labeled data l +  xi yi  will produce a classifier parameterized by 붿l+ xi yi . this allows us to approximate criterion  1  as:
	 	 1 
u
since here. therefore  maximizing conditional mutual information corresponds minimizing the classification uncertainty  i.e.  minimizing entropy  on unlabeled data set. minimizing entropy on the unlabeled data has been showed useful in semi-supervised learning  grandvalet and bengio  1 . a smaller classification uncertainty usually indicates a larger classification

	+	+

	 	 
figure 1: why should  1  use the optimistic assignment 
margin  therefore leading to a better classification performance. as this means our criterion is directly related to optimizing the classification performance  we can view  1  as a discriminative information criterion for selecting query instances.
모unfortunately   1  can not be used directly since yi  the true value of xi  is unknown. one obvious approach to resolve this problem is to use the apparently most-likely value of yi based on the conditional model 붿l trained on the labeled data l - i.e.  use
	yi 	=	argmaxp y |xi 붿l   .
y
alternatively  roy and mccallum  chose to take the expectation wrt yi
	argmin	p	h y
y
 we will later refer to this as the  mcmi avg -instance .  this corresponds to the  self-conf  approach in  baram et al.  1 . notice both of these approaches use the p y |xi 붿l   information determined by the labeled data; unfortunately  in the typical active learning situations where there are very few labeled instances l  the labeled data might lead to a bad classifier  therefore the p y |xi 붿l   will not be helpful. this concern is verified in our empirical study.
모we propose instead using an  optimistic  strategy  which takes the y value that minimizes the entropy term. using this optimistic strategy  our mcmi min  approach would compute  1  as argminf i 
모모모모i뫍u where
	.	 1 
 notice this measure will give each candidate xi instance its best possible score  over the set of y labels. for instance  if the classifier is seeking a linear separator  and if there is a setting y for which the classifier based on l +  xi y  nicely separates the unlabeled data with a wide margin  then  1  will give the score associated with this large margin. we will refer to this optimal i  = argmini뫍u f i  as the  mcmi min  instance .
모this differs from roy and mccallum   which chooses y in a supervised way  as our approach can be viewed as choosing y in an unsupervised way. this approach is motivated by the  clustering assumption   sindhwani et al.  1  line 1

  line 1
 


 line 1



		 

		 
figure 1: the mcmi min  criterion can guess wrong.  lines 1 and 1 are not shown  to avoid cluttering the image. 
used in many unsupervised and semi-supervised learning algorithms: instances separated by a wide margin usually belong to different classes. this means minimizing classification uncertainty is usually consistent with the underlying clustering partitions.
example: to make this concrete  consider figure 1  where each  and  is a labeled instance  and the  's are unlabeled. assigning a the label  would mean the data is not linearly separable  which would add a great deal of uncertainty to the remaining unlabeled instances; hence the value of  for each unlabeled instance xu would be high. by contrast  the  label would not change the support vectors  meaning h yu |xu 붿l+ a     would be essentially unchanged; i.e.  h yu |xu 붿l+ a     뫘 h yu |xu 붿l  . hence mcmi min  would use the  more informative   label  if it decides to use a. now consider b  and observe both  and  are consistent with some linear separator. notice  however  that while the  label will well specify the label of the points below immediately below it  the  label will not. this is why mcmi min  will use the  label for b.
모this assignment clearly depends on the other unlabeled instances; e.g.  if those 1 instances were just above b  then mutatis mutandis  mcmi min  would use the  label. hence  the min part of mcmi min  is exploiting relevant nuances of the p x  density over the instances themselves  as it seeks the labels with the largest separation margin.
모given the choice between a versus b  our mcmi min  criterion will prefer b  as neither  a   nor  will significantly increase the information about the labels for the instances  beyond the information from the other current labels   while b could be very informative - especially if its label is .
1 on-line adjustment
potential problem: there is a potential problem with this approach: at the beginning  when we only have a few labeled data  there may be many consistent parameter settings  read  classifiers  ; here  any method  including ours  may well  guess  wrong. to illustrate this  consider seeking a linear separator within the data shown in figure 1. which point should be selected next 
모recall that mcmi min  will seek the instance that could lead to the most certainty over the remaining unlabeled instances. if a is labeled   then the resulting set of 1 labeled

mm+m  u: indices of unlabeled instances; l: labeled instances  
repeat
for each i 뫍 u  compute y i  := argmin f i  := u h yu |xu 붿l+ xi y i    
% ie  score based on this minimum y i  value
let i  := argmini f i  % instance with optimal mcmi min  score
purchase true label wi  for xi  remove i  from u; add  xi  wi   to l.
if  then
let i  := argmaxi뫍u h yi |xi l 
purchase true label wi  for xi 
모모remove i  from u; add  xi  wi   to l. until bored.
end mm+m
figure 1: the mm+m algorithm
instances would suggest a vertical dividing line   line 1  ; in fact  it is easy to see that this leads to the largest margin  which means the highest mcmi min  score. our algorithm will therefore select instance a  anticipating this  label. now imagine the  unknown  true class dividing line is line 1  which means our mcmi min  approach was misled - that is  the optimistic prediction of a's label was different from its true label.
모at this point  given these 1 labeled instances  mcmi min  would next select c  anticipating that it would be labeled   consistent with a horizontal separating line  line 1   unfortunately  this too is wrong: c's label is   meaning mcmi min  again guessed wrong. this examplesuggests it might be helpful to consider a different selection strategy after mcmi min  has been misled.
approach: fortunately  our algorithm can easily detect this  guessed wrong situation in the immediate next step  by simply comparing the actual label for a with its optimistically predicted label. when they disagree  we propose switching from mcmi min  to another criterion; here we choose mu. this is done in our mm+m algorithm  shown in figure 1 if mm+m guesses correctly  i.e.  if the label returned for the selected instance is the one producing the minimum entropy   then mm+m continues to use the mcmi min  approach - i.e.  if it is  converging  to the correct separator  it should keep going. otherwise  it will select a single mu-instance  to help it locate a more appropriate  split  in the space  before returning to the mcmi min  criterion.
모for our example  once mm+m has seen that it was wrong about a  i.e.  a was labeled  rather than the anticipated    mm+m will then select a mu-instance - i.e.  an instance near the current boundary  line 1  - which means it will select b. when b is labeled   we can come closer to the true separator  line 1. notice this means we will be confident that c's label is   which means we will no longer need to query this instance. in this case  the information produced by the mu-instance is more relevant than the mcmi min -instance.

	1
모모this requires o |u|   time to make each selection: first iterating over each i 뫍 u  then for each such i  iterating over each u 뫍 u when computing the f i  score. for larger datasets  would could use sampling  for both loops.
모section 1 provides empirical evidence that this approach works well.
1 approximating logistic regression
recall our system uses logistic regression  which in general takes a set of labeled instances l = { xi yi }  and seeking the parameters 붣 that best fit the logistic function1
 .
 we use the obvious variant for multiclass classification  schein and ungar  1 .  we use a regularized version  seeking the 붣 that minimizes1

there are a number of iterative algorithms for this computation  minka  1 ; we use newton's method: 붣new := 	where g
모모모모모모모모모 x y 뫍l is the gradient based on the labeled dataset l and
hi
 x y 뫍l
is the hessian. if there are n instances of d dimensions  this requires o nd1  time per iteration. to compute each mcmi min  instance  our mm+m algorithm  figure 1  will have to compute parameters |u| 뫄 k times  as this requires computing 붣l+ xi y  for each i 뫍 u and each of the k classes yi.
모in order to save computational cost  we use a simple way to approximate these parameters: assuming we have a good estimate of 붣l  based on the gl and hl  we can then approximate the values of 붣l+ xi yi   by starting with that 붣l value and performing just one update iteration  based on the new values of gl+ xi yi  and hl+ xi yi :

moreover 	there	are	easy	ways	to	approximate
h based on h.
1 experiments
to investigate the empirical performance of our mm+m algorithm  we conducted a set of experiments on many uci datasets  uci  1   comparing mm+m with several other active learning algorithms. the four primary algorithms we considered are:
1. mcmi min +mu: current mm+m algorithm
1. mcmi min : always use the mcmi min -instance

1
모모of course  we include x1 = 1 within each xi to avoid the need to explicitly separate out the constant bias term. 1
모모in our experiments  we used 뷂 = 1 in binary classes  and 뷂 = 1 for others.

figure 1: comparing active learners on various uci datasets1. mcmi avg : differs from mcmi min  by averaging over the y values  1  rather than taking the minimum  1   note  this is same as the self-conf in  baram et al. 
1  
1. mu:  most uncertain ; based on  1 
모we consider the following 1 uci datasets  we show the name  followed by its number of classes  number of instances and the number of attributes : australian 1;1   breast 1;1 ; cleve 1;1 ; crx 1;1   diabetes 1;1   flare 1;1   german 1;1   glass1;1   heart 1;1   hepatitis 1;1   mofn 1;1   pima 1;1  
vote 1;1  1 iris 1;1   lymphography 1; 1  and vehicle 1;1 . note each of the last 1 datasets has strictly more than 1 classes.
모for each dataset  we ran 1 trials. for each trial  we first randomly selected 1 of the instances from each class to serve as the test data. from the remaining 1  we randomly picked k labeled instances from each class to form the initial labeled set l  where k = 1 for binary-class databases  and k = 1 for multiclass databases 1 and left the remaining instances as the unlabeled pool. each of the active learners then sequentially selects 1 instances1 from the unlabeled pool to add to the labeled set. every time a new instance is labeled  that active learner then retrains a new classifier on the increased labeled set and evaluates its performance on the test set. in general  let acci d  a m   be the accuracy of the classifier learned after the a active learner has added m labels to the d database  on the ith run  and let acc d  a m   be the average of this accuracy values  over the 1 runs  i = 1..1.
모our mm+m   mcmi min +mu   wins on most datasets. figure 1 shows these averaged acc d  a m   results for
table 1: comparing mm+m to other active learners.  see text for descriptions of notation. 
databasevs muvs mcmi min vs mcmi avg vs randomvs mu-svmaustralian 	1/	1  + 1 /1  1 1/1  + 1 / 1  - 1/ 1  + breast 	1 /	1  - 1/1  + 1/1  + 	1/	1  + 1 /1  - cleve 	1/	1  + 1/1  + 1/1  + 	1/	1  + 1/ 1  + corral1 / 1  1 1/1  + 1 /1  1 	1/	1  + 1 /1  - crx1/1  + 1 /1  - 1/1  + 	1 /	1  - 1/ 1  + diabetes1/1  + 1 /1  1 1/1  + 1 / 1  + 1/ 1  + flare1/1  + 1/1  + 1/1  + 1/1  + 1/ 1  + german 1/1  + 1/1  + 1/1  + 1 /1  + 1/ 1  + glass1 1/1  + 1 /1  + 1/1  + 1/1  + 1/ 1  + heart 1/1  + 1/1  + 1/1  + 1 /1  + 1/ 1  + hepatitis 1/1  + 1 /1  1 1/1  + 1 /1  1 1/ 1  + mofn1 / 1  - 1/1  + 1/1  + 1/1  1 1 /1  1 pima 	1/	1  + 1 /1  + 1/1  + 1 /1  1 1  + vote1 / 1  - 1/1  + 1 /1  - 1 /1  + 1 /1  - iris	1/	1  + 1/1  + 1 /1  1 1/1  + - /	-   vehicle 	1/	1  + 1/1  + 1/1  + 1 / 1  - - /	-   lymphography	1 /	1  1 1 /1  - 1 /1  - 	1 /	1  - - /	-   total w/l/t1 / 1 / 1/ 1 / 1/ 1 / 1 / 1 / 1 / 1 / 1signed rank test1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 11 datasets. the pima graph shows that mm+m can do well even when mu does relatively poorly. the fact that mcmi min   which did not include the  mu-correction   does comparably to mm+m shows that mm+m did not need this correction here. now consider breast  and notice that mcmi min  was the worst performer here  while mm+m was able to match mu's performance - i.e.  here the mucorrection was essential.  we observed here that mm+m used this mu-correction as often as possible - i.e.  for essentially every other query.  that is  when mcmi min  is working well  mm+m can capitalize on it; but when mcmi min  is misled  mm+m can then fall back on the alternative mu approach. in the third database  vehicle  we see that mm+m in fact does better than both mu and mcmi min .  this dataset also shows that mm+m system can performwell even when there are more than 1 classes.  in all of these examples  and many others   we see that mm+m - a composition of mcmi min  and mu - typically does at least as well as either of these two approaches individually  and often better.
모even though the mcmi avg  approach  of averaging over the possible y values  is intuitive  the pima graph shows that it can work worse than the  at first counter-intuitive  mcmi min  system. but not always: mcmi avg  is better than mcmi min  for breast.
모our mm+m did not always win: the final graph shows that mu is best for the vote database. we see  however  that mm+m is still close.
모of course  these 1 graphs are only anecdotal. moreover  even here we find that different active-learners are best for some  but not all  of the 1 evaluation points  where each corresponds to a specific number of additional labeled instances. we present two evaluations to help quantify this comparison. first  we quantify the quality of the different learners by counting the number of times that one algorithm was significantly better than another over the 1 evaluation points  at the p   1 level based on a 1-sided paired t-test.  we did not not bother with a bonferroni correction  as this is just to quickly determine which differences  at each specific evaluation point  appears significant.  we used this to compare mm+m with each other active learner
a 뫍 { mu  mcmi min   mcmi avg }  over all 1 databases. the results are shown in table 1  whose entries are of the form  #mm+m-wins / #a-wins  s    where a is the algorithm associated with the column.  we will explain the parenthesized   s   value below.  an entry is bold if mm+m was statistically better at least 1 moretimes than it was statistically worse. each entry in the  total w/l/t  row shows the number of datasets where mm+m  dominated  by this    1 measure   when it lost by this same quantity  versus tied. we also note that  wrt these 1 active learners  our mm+m was the best active learner for 1 of the 1 databases  in that it was at least as good as the other 1 approaches  and strictly better than at least one ; we marked each such database with a    .
모to describe our second set of evaluations  note that the 1 numbers {acc d  a m  }1m=1 characterize the performance of a on d. we therefore ran a wilcoxon signed rank test to determine whether these scores were significantly better for mm+m  or for the  challenger  associated with the column. this produced the parenthesized value   s   in each entry in
table 1  which is  +  if this signed rank test suggests that
mm+m is statistically better than the alternative algorithm a at the p   1 level  is  -  if this test suggests that a is statistically better at p   1  and is  1  otherwise.  hence  this test suggests that mm+m was statistically better than mu on the australian database  but it was statistically worse on the breast database  and is comparable for corral. 
모the numbers at the very bottom of table 1 are the totals for this signed rank test; they are in a similar form as the first evaluation  i.e.  the number of datasets where mm+m was better/worse/tied. hence  the wilcoxon signed rank test shows that mm+m is statistically better than mu for 1 of the 1 datasets  worse on 1  and tied in the remaining 1. notice the results of this ranked sign test is very similar to our other  ad hoc  scoring measure. we also computed 1-sided t-tests over this data  and found that it also produced very similar results; see http://www.cs.ualberta.ca/ greiner/ research/optimisticactivelearning .
we also considered two other active learners. the  ran-
dom  learner simply selected instances at random. the final learner   mu-svm   uses a  linear kernel  svm  and selects the instance closest to the classification boundary - i.e.  this is a variant of the  most uncertain  selection criterion.  note we only consider the 1 binary databases here.  using the evaluation measures shown above  the results of our experiments appear in the final two columns of table 1.
모these results confirm that our mm+m algorithm works effectively - and appears better than the other algorithms considered here. we note  in particular  that under either test considered  our mm+m is better than any of the other active learners  in at least twice as many databases.
1 conclusion
future work: section 1 motivated why our mm+m algorithm should work  and section 1 provided evidence that it does. it would be useful to augment this with a more formal theoretical analysis  one that could precisely characterize those domains where mm+m is guaranteed to work effectively  or better  optimally. this may depend on both the instance distribution p x  as well as the nature of the conditional distribution p y |x    e.g.  linearly separable or not   and also perhaps on the initially provided labeled instances. investigating better on-line adjustment strategies is another direction.
contributions: this paper explores the insight that an active learner should identify instances whose label will help determine the labels of the other unlabeled instances. this leads to a novel approach: select the instance that provides the maximum conditional mutual information about the labels of the unlabeled instances  given the labeled data  based on an optimistic assumption about the label for each instance - the  min  in  1 . when this optimistic assumption fails for an instance  i.e.  when the real label does not match the predicted optimistic one   our mm+m selects one mu-instance before reverting back to mcmi-instances.
모our empirical results demonstrate first that our mm+m works effectively  and second that its effectiveness depends on its two major design decisions - the counter-intuitive  min  in  1  and the on-line  mu-correction .
모for more details about the experiments  as well as other insights about the mm+m algorithm  see http://www.cs.ualberta.ca/ greiner/ research/optimisticactivelearning .
acknowledgments
we gratefully acknowledge the insightful comments of the four anonymous reviewers  and of our colleagues  w bishof and d schuurmans. this work was partially funded by nserc and the alberta ingenuity centre for machine learning.
