 
compilability is a fundamental property of knowledge representation formalisms which captures how succinctly information can be expressed. although many results concerning compilability have been obtained  they are all  worst-case  results. we develop a theory of average-case compilability which allows for the formal comparison and classification of knowledge representation formalisms 
 on average.  
1 introduction 
by now  a multitude of knowledge representation formalisms have been proposed and studied in the literature  for example  propositional logic  default logic  and circumscription - to name a few. the comparison of these formalisms has been a major research theme over the past decade. in particular  many results have been obtained on the computational complexity of inference and model checking - core reasoning tasks associated with each formalism. the fundamental property of compilability  which measures how efficiently or succinctly a formalism represents knowledge  has also been studied.1 there is now a rich body of results concerning compilability; however  all of these results address the  worst-case   and are thus susceptible to the complaint that they do not address  average  or  typical  compilability. the main contribution of this paper is a theoretical framework providing a language and tools for comparing and classifying the compilability of formalisms on average. our framework is built on notions and insights from the theory of compilability clases due to cadoli et al. 1a; 1b  and the theory of average-case time complexity due to levin 1 . 
1 	background 
compilability. informally  a formalism a is compilable to a formalism b if for every knowledge base x in formalism a  there is a knowledge base in formalism d representing the same information as x with size polynomial in the length 
¡¡¡¡in previous work  compilability has also been called space efficiency and succinctness. 
of xr intuitively  this means that formalism b is at least as space efficient as formalism a: whatever can be expressed in formalism a can be expressed  with about the same level of succinctness  in formalism b. 
¡¡a number of papers compared the compilability of different formalisms  kautz et ai  1; gogic el al  1; khardon and roth  1; cadoli et al 1; 1; 1; 
1a; 1b . in many cases  these papers rigorously prove that one formalism b is strictly more succinct than another formalism a - that is  a is compilable to b  but b is not compilable to a. this means that there is no way to translate knowledge bases in b to knowledge bases in a  unless the translation is allowed to increase the size of knowledge bases by a super-polynomial amount. in other words  any translation of knowledge bases in b to knowledge bases in a is inherently exponential in size.1 observe that this statement has the same flavor as the famous  p does not equal np  conjecture from classical complexity theory. this conjecture holds that no algorithm can solve an np-complete problem  unless the algorithm is allowed to take super-polynomial time on some inputs - or  put differently  any algorithm solving a np-complete problem is inherently exponential in time. 
¡¡notice that the above definition of  formalism a is compilable to formalism b  docs not take into account the difficulty or complexity of computing the translation between knowledge bases in a and knowledge bases in b. this nonuniformity is a key feature of the definition: the existence of a succinct translation is sufficient; an efficiently computable translation is not necessary. it turns out that many proofs of non-compilability - that is  proofs of statements of the form  formalism b is not compilable to formalism a  - rely on results from non-uniform complexity theory. such proofs often are not unconditional  but are contingent upon the widely believed complexity-theoretic assumption that the polynomial hierarchy does not collapse.1 
   1 ln the technical portion of this paper  this definition will be captured formally by the  rcducibility. 
knowledge representation 	1    1 here  by  exponential  we mean exceeding every polynomial infinitely often. 1 the polynomial hierarchy is a collection of complexity classes which includes p  np  co-np  and other classes which are in essence generalizations of these three classes. for the intents and purposes of this paper  this assumption can be thought of as being similar to the  perhaps better known  assumption that p does not equal np  in compilability classes. the initial papers that demonstrated non-compilability results  kautz et at.  1; gogic et al  
1; khardon and roth  1; cadoli et al.  1; 1; 1  were based on ad hoc proofs  each of which isolated two particular formalisms and then demonstrated noncompilability of one to the other. in  cadoli et al  1a; 1b   new complexity classes measuring compilability  called compilability classes  were introduced; for every classical complexity class c  it is possible to define a compilability class analog of c. the theory of compilability classes made it possible to systematize proofs of non-compilability much in the way classical complexity theory makes it possible to systematize proofs of intractability. a proof that a language is np-complete is demonstration that there is no polynomial-time algorithm for the language  and that the language has the same time complexity  up to a polynomial  as all other np-complete languages. likewise  a proof that a formalism is complete for the compilability class analog of np is demonstration that it is not compilable to any formalism in the compilability class analog of p  and that the language is compilable to and from all other formalisms complete for the compilability class analog of np.1  more generally  when c is a class from the polynomial hierarchy  a proof that a formalism is complete for the compilability class analog of c is demonstration that it is not compilable to any formalism in the compilability class analog of c   if c  is below c in the polynomial hierarchy.  
¡¡in addition to providing a methodology for comparing formalisms with respect to compilability  the compilability classes capture formally the notion of off-line preprocessing. preprocessing a knowledge base off-line can be of great utility if the resulting  processed knowledge base is in a form that allows for quick  on-line response to queries  and if many queries are expected . membership of a formalism a in the compilability class analog of p will mean that any knowledge base x of a can be preprocessed into a form that does not unreasonably increase the size of x  but permits queries to x to be processed efficiently  that is  in polynomial time . 
¡¡the compilability classes are part of a formal framework for discussing compilability  which includes robust notions of reduction and completeness. not only is this framework extremely appealing from a theoretical point of view  but the compilability classes are rife with natural complete problems - the sine qua non of complexity classes purporting to be useful in performing problem classification. indeed  there do not appear to be any knowledge representation formalisms which defy classification as complete for a compilability class. on the downside  many of the existing classification results are quite negative  showing that a formalism is complete for  the compilability class analog of  np  conp  or a higher level of the polynomial hierarchy. 
fact  if the polynomial hierarchy does not collapse  then p does not equal np.  1 our discussion presumes that the polynomial hierarchy does not collapse. 
1 motivations and approach 
worst-case versus average-case. although crisp theoretical results can be obtained concerning compilability and non-compilability  these are worst-case notions: noncompilability of formalism b to formalism a implies that there is an infinite family of knowledge bases in b that cannot be succinctly translated into knowledge bases in a. noncompilability does not say anything about the density or frequency of instances from the family of untranslatable knowledge bases. this observation calls into question the realworld utility of studying compilability  as defined above ; if formalism b is compilable to formalism a for all but a pathological family of knowledge bases arising infrequently in practice  then formalism b is  for pragmatic purposes  compilable to formalism a. therefore  a theory which permits formal results of compilability on average is necessary. this paper lays the foundations for a theory of average-case compilability. 
¡¡notice that our objection to the worst-case nature of noncompilability is not truly novel. it has long been observed that np-completeness of a language l does not imply hardness of l on typical or real-world instances. our objection is really this old observation  masquerading in the new context of compilability. 
¡¡one theory that was developed in response to this old observation is the theory of average-case time complexity  actc   initiated by levin . our theory of averagecase compilability will be built on a key notion of actc that of  polynomial on average.  moreover  there are useful analogies between our theory and the theory of actc. consequently  we provide a brief overview of actc. 
average-case time complexity. by definition  a language l  consisting of strings  is in p if there is an algorithm deciding membership for l in polynomial time. the idea behind actc is to relax the requirement in this definition that a suitable algorithm is one that always runs in polynomial time; this is done by placing a probability distribution on all strings and allowing an algorithm to take super-polynomial times on unlikely strings. 
¡¡a language paired with a probability distribution over all strings is called a distributional language. while languages are the objects classified by classical complexity theory  distributional languages are the objects classified by actc. a distributional language  is in the average-case version of p  average-p  if there is an algorithm deciding membership for l in time polynomial on - a concept to be discussed formally later in this paper. 
¡¡there is a vast literature on actc; for more information  we recommend the overviews/surveys  johnson  1; gurevich  1; 1a; 1b; impagliazzo  1; wang  1; goldreich  1  as starting points. 
average-case compilability. in laying down a theory for average-case compilability  we want to  soften  the definition of  formalism a is compilable to formalism j1  by relaxing the requirement that there needs to be a translation of knowledge bases of strictly polynomial size. this is done roughly in analogy to the described development of actc. we first define a distributional formalism to be a formalism paired with a probability distribution over all knowledge bases. then  a distributional formalism  is  informally  compilable on average to a formalism b if there is a translation of knowledge bases of size polynomial on 
¡¡using the notion of  compilable on average   we define average-case analogs of compilability classes. these average-case analogs contain distributional formalisms  in contrast to the compilability classes themselves  which contain pure formalisms. intuitively  membership of a distributional formalism  in the average-case compilability class analog of p will mean the following: any knowledge base x of a can be preprocessed in a way that tends not to increase the size of x by more than a polynomial  with respect to u   and that allows for rapid processing of queries to x. 
1 preliminaries 
in this section  we present notation and assumptions that will be used throughout the paper. 
¡¡we assume to be a fixed finite alphabet which is used to form strings. we will at times assume that pairs of strings  that is  elements of are represented as strings  that is  elements of ; when this assumption is made  we assume that the representation is via a pairing function such that the length of  x  y  is linear in  for a string  denote  that is  the length of x written 
in unary notation. 
¡¡we assume that the reader has familiarity with basic notions of computational complexity theory - in particular  the classes of the polynomial hierarchy  p  np  co-np  etc.  and the polynomial many-one reduction  balcdzar et al.  1 . a language is a subset of   that is  a set of strings. a complexity class is a set of languages. when c is a complexity class and is a reduction  we say that c is compatible with if for all languages a and b  a  b and b  c imply that a c. we will assume throughout that every complexity class c is compatible with the polynomial many-one reduction we say that a language b is complete for a complexity class c under reductions if b c and for all 
	a function 	is polynomial-size if there exists 
a polynomial p such that for all 
a function / : is polynomial-time computable if there exist a polynomial p and a turing machine m such that for all   the turing machine m  on input x  produces f x  in time less than 
definition 1 a knowledge representation formalism  krf  is a subset of 	when f is a krf and 	we let 
fx denote the set 
¡¡intuitively  each can be thought of as a knowledge base  kb   representing the information the following 
   1 in the technical portion of this paper  this definition will be captured formally by the  reducibility. 
   1 notice that every polynomial-time computable function is polynomial-size  but not every polynomial-size function is polynomial-time computable. 
are examples of krfs which capture model checking and inference for 1-sat formulas. 
propositional-logic-mc 
{ x  y  : x is a 1-sat formula and y is a model of x} 
clause-inference 
 : x is a 1-sat formula  y is a 1-clause  and 
¡¡notice the generality of the definition of a krf; the knowledge represented  the various  may be models  formulas  or some altogether different combinatorial structures. 
1 compilability classes and reductions 
this section reviews the theory of compilability classes. there are two different types of compilability classes  uniform and non-uniform  but our focus is on the latter  for reasons discussed below. we emphasize that none of the definitions  theorems  or insights in this section are our own  but rather  are due to  cadoli et al.  1a; 1b   on which our 
presentation is based. 
¡¡the formal definition of non-uniform compilability may by itself look non-intuitive  so we first attempt to describe some of the ideas behind this definition  before giving the actual definition. intuitively  we want to say that a krf f is compilable if membership queries  can be decided efficiently after the kb x is preprocessed into a new kb / x . we want to constrain the size of the new ; otherwise  it may be prohibitively large to store. we arrive at a candidate definition of compilability: say that a krf f is in comp-c if for some polynomial-size function / and a second krf the property 
	for all pairs 	if and only if 
holds. notice that the class c constrains the difficulty of deciding a post-processing query  of course  such queries can be decided efficiently when c = p. however  to permit fine classification of krfs  we leave c as a parameter to the definition of comp-c. 
¡¡the above definition misses one important detail - nonuniformity. if f is a krf such that fx is always a finite set  f can never be complete for comp-c when c is a class of the polynomial hierarchy above p. at the same time  there are krfs with this  finiteness  property that are provably not in comp-p. as an example  clause-inference is in comp-co-np  but is neither comp-co-np-complete nor in comp-p.1 hence  the classes comp-c fail to capture the compilability of some very natural krfs.  for more information on these matters  as well as proofs of the claims  we refer the reader to  cadoli et al  1a .  to resolve this issue  we allow the length of a query to be known to the compilation mapping /. 
definition 1  nu-comp-c  let c be a complexity class. 
a krf f belongs to nu-comp-c if there exists a binary polynomial-size function  and a krf f' in 
c such that the following property holds: 
knowledge representation 	1 ¡¡¡¡1 this discussion presumes that the polynomial hierarchy does not collapse. 

¡¡the fact that the compilation mapping may use the length of y may seem a bit strange; after all  we wish to capture  after preprocessing on a knowledge base x  the difficulty of answering queries of the form y  fx. however  it is reasonable to assume that we will never be interested in such a query when the length of y greatly exceeds that of x since such a query requires a large amount of time to even write down. under this assumption  definition 1 is equivalent to the  uniform  definition given above.  formally  if there exists a polynomial p such that  x  y  f implies that  y   p  x    then f is in nu-comp-c if and only if f is in comp-c.  
¡¡the following notion of reduction  associated with the nu-comp-c classes  allows one to compare the compilability of different krfs. 
definition 1  nu-comp reducibility  a krf f is nu-comp reducible to a krf f'  denoted by if there 
exist binary polynomial-size functions fi f'1   and a binary polynomial-time computable function g : such that the following property' holds: 
	for all pairs 	if and only if 
theorem 1 the nu-comp reduction is transitive and is compatible with the class nu-comp-c  for every complexity class c . 
¡¡we note that clause-inference is nu-comp-co-npcomplete  under nu-comp reductions. 
¡¡finally  we observe that the nu-comp-c analog of the polynomial hierarchy does not collapse. 
theorem 1 lfc  and c l are classes of the polynomial hierarchy such that c1 is higher than c1  then nu-comp-c  is properly contained in nu-comp-c1  under the assumption that the polynomial hierarchy does not collapse . 
1 average-case compilability 
in this section  we present our new theory of average-case compilability. 
definition 1 a probability distribution is a realvalued function from such that 
the nu-comp-c classes are used to classify and compare 
krfs; our new classes will be used to classify and compare what we call distributional krfs. 
definition 1 a distributional krf  dkrf  is a pair  f  u  consisting of a krf f and a probability distribution u. 
¡¡we now define the notion of  polynomial on average.1' this definition is exactly that used in the theory of average-case time complexity  up to appropriate changes of the domain and range of the functions whose size we wish to measure . 
¡¡we are now ready to define our new  averagecompilability  classes; formally  each class is a set of dkrfs. the definition can be viewed as a relaxation of definition 1. 
the difference is that the translation mapping  need no longer be of strictly polynomial size  but is now 
only required to be of size polynomial on average. 
definition 1  avg-nu-comp-c  let c be a complexity class. a dkrf  f  p  belongs to avg-nu-comp-c if there exists a binary function of size polynomial on p-average and a krf f' in c such that the property of definition 1 holds. 
¡¡there is an alternative way to define avg-nu-comp-c  in terms of the following type of reduction  which relates dkrfs to krfs. 
definition 1  dist-nu-comp reducibility  a dkrf  f  p  is dist-nu-comp reducible to a krf f'  denoted by f i f there exist binary polynomial-size f u n c t i o n s a n d a binary polynomialtime computable function such that is of size polynomial on -average*1 and the property 
of definition 1 holds. 
theorem 1 suppose that is a dkrf and that c is a complexity class. the dkrf is in avg-nu-comp-c if and only if f' for some f' in nu-comp-c. 
¡¡this theorem has an important corollary: if nu-comp-c has a complete krf f' then the dkrfs contained in avg-nu-comp-c are exactly those which reduce to f'. in order to establish the corollary  the following lemma is needed. 
lemma 1 suppose that 	is a dkrf and that f' and 
f  are krfs. if 
f  then 
¡¡roughly  lemma 1 can be viewed as a proof of transitivity: i f r e d u c e s to f' and f' reduces to f   then reduces to  under the right notions of reduction . 
corollary 1 suppose that is a dkrf c is a complexity class and the krf ff is nu-comp-c-complete under nu-comp reductions. is in avg-nu-comp-c if and only if 
¡¡it is worth noting here that the class nu-comp-c has a complete krf under nu-comp reductions whenever the underlying complexity class c has a complete language under reductions fcadoli et al  1a . 
¡¡we now give a notion of reduction for the comparison of dkrfs. when u and v are probability distributions  we say that v dominates p if there exists a polynomial p such that for all 
definition 1  avg-nu-comp reducibility  a dkrf is avg-nu-comp reducible to a dkrf if there exist a nucomp reduction  /1  f1  g from f to ff and a probability distribution dominating p such that where the sum is over all  x  /  such that  f  x  i  = y and there exists 
theorem 1 	the avg-nu-comp reduction is transitive and is compatible with the class avg-nu-comp-c  for every com-
plexity class c . 
1 discussion 
we now discuss some of the considerations behind the definitions in the previous section. when is a probability distribution and s is a subset of let denote the conditional distribution of on 1  that is  the function defined on   with value equal to 

it is well known that this notion of expected polynomial is not closed under polynomials  see for example  goldreich  
1  . for instance  there exist functions  and a distribution  such that 
/ has expected polynomial size  but /' fails to have expected polynomial size. 
¡¡in our average-case compilability theory  this  lack of closure under polynomials  problem manifests itself in the following way. suppose that  using the alternative definition of avg-nu-comp-c given in theorem 1  the krf f' is witness to the membership of dkrf in avg-nu-comp-c  that is  '. surely  if f  is at least as space efficient as f'  that is    then one expects that f  would also witness the membership of  f  ¦Ì  in avg-nu-comp-c; this is the content of lemma 1  on which corollary 1 relies. however  if expected polynomial size is used instead of polynomial on average in definition 1  lemma 1 breaks down - then  there would exist a dkrf 
 f  ¦Ì  and krfs f' and f  such that f  f' reduce to f . 
¡¡  why does a dkrf have one probability distribution over all elements of - as opposed to an ensemble of distributions  each of which is over some finite subset of 

¡¡defining a dkrf to be a krf paired with an ensemble of distributions  each of which is  say  defined on a different string length  may seem more natural than the given definition  where there is only one distribution  on all strings. 
   however  the literature on actc contains many alternative formulations of  polynomial on average   equivalent to that given in definition 1  and sufficient conditions for a function to be polynomial on average.  for example  there is a different formulation of definition 1 in terms of ensembles of distributions  each having finite support  in  impagliazzo  1 .  these give rise to alternative formulations of avg-nu-comp-c  and so the particular characterizations we give for avg-nu-comp-c  in definition 1 and theorem 1  were  in some sense  chosen over provably equal characterizations on purely aesthetic grounds. 
     why is a probability distribution over e* x 1 * as opposed to over e*   after all  the knowledge bases are represented by elements of   and an element of 'e* x 1* is just a  slice  of a knowledge base. 
   let f be a krf conforming to the  bounded-querylength  assumption discussed in section 1  that is  suppose that there exists a polynomial p such that  x  y  e f implies |y|   p ixi - then  a distribution over e* naturally induces a distribution over e* x 1* for the padded version of f defined as f' = { x z  : z - ydk   z  = p  x     x y  € f   where ¡õ is an extra padding symbol   which has the property that all strings in fx. have the same length  for any x. note that all of the example krfs in this paper already have this property. 
1 example: model checking for circumscription 
in this section  we illustrate the use of our new theory by showing that model checking for circumscription on 1-sat formulas  formalized below and denoted by circumscription-mc d   is in avg-nu-comp-p  under a natural probability distribution where formulas are generated by including each clause independently with identical probability. since circumscription-mc d  is nu-comp-co-nphard 1 this result gives a natural dkrf which is contained in avg-nu-comp-p  but which has a krf which is provably not in nu-comp-p  theorem 1 . 
	suppose that 	is a 1-sat formula over 
{v1 ...  vn   and 1 is a subset of vn. we say that a model a:vn -  	if for all other all s s  implies that in addition  a model a : s is said to be a minimal model if it is the restriction of a s-minimal model a : vn --  {1}  of 

we define circumscription-mc d  to be the set 
is a 1-sat formula on variable set on vn and is a minimal model of 
let wn denote the is a 1-sat formula on vn and a : -  
{1} is an assignment }  which is the set of all syntactically well-formed pairs that may or may not be in circumscription-mc d -
theorem 1 let vn c denote the distribution on 1-sat formulas over vn where a formula is generated by including each of the 1 n/1  1-clauses independently with probability  all real numbers c   1 and natural numbers 
 with c   1 and d   1   1 ; be any dis-
tribution such that 
  for all n  and 
  i f i s a 1-sat formula on 
 dn . 
1 this can be shown using a result in  gogic et al  1 . 
knowledge representation 	1 1  here  denotes the usual total ordering on {1} where 

¡¡this theorem is established using techniques from probabilistic combinatorics. it is worth noting that at c = 1  the expected number of satisfying assignments that a random 1-sat formula has is exponential. 
1 conclusions and future work 
in  papadimitriou  1   papadimitriou writes: 
the ultimate and most conclusive criterion for comparing knowledge representation formalisms is to compare their expressive power not on arbitrary sets of models  but on the  interesting  sets of models  the ones that come up in the  real world.  we still hope that a meaningful and convincing formulation of this important problem may be possible. 
¡¡in this paper  we presented a robust and flexible theory of average-case compilability in which the intuitive notion of  interesting sets of models  can be formalized as a dkrf  definition 1   dkrfs can be classified  definition 1   and dkrfs can be compared  definition 1 . we illustrated the use of this theory by showing that  under a natural probability distribution  model checking for circumscription is compilable to the analog of the complexity class p  in our theory. 
¡¡there are many open questions for future investigation; to conclude the paper  we list a few. 
  can additional dkrfs be classified as being inside avg-nu-comp-p  we conjecture in particular that circumscription-mc d  - along with the  standard  1sat probabilistic model with for a sufficiently low c - is in avg-nu-comp-p. 
  are there dkrfs complete for avg-nu-comp-c  where c is from the polynomial hierarchy  under the avg-nucomp reduction  or some other notion of reduction compatible with the avg-nu-comp-c classes  
  can any questions concerning the new avg-nu-comp-c classes defined here be related to better known complexity-theoretic hypotheses  
