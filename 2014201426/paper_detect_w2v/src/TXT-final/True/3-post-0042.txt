a universal measure of intelligence for artificial agents1
shane legg and marcus hutter
idsia  galleria 1  manno-lugano 1  switzerland
{shane marcus} idsia.ch
모
1 the concept of intelligence
a fundamental difficulty in artificial intelligence is that nobody really knows what intelligence is  especially for systems with senses  environments  motivations and cognitive capacities which are very different to our own. if we look to definitions of human intelligence given by experts  we see that although there is no consensus  most views cluster around a few common perspectives and share many key features.
모in all cases  intelligence is a property of an entity  which we will call the agent  that interacts with an external problem or situation  which we will call the environment. an agent's intelligence is typically related to its ability to succeed in environments  which implies that there is some kind of objective  which we will call the goal. the emphasis on learning  adaptation and flexibility common to many definitions implies that the environment is not fully known to the agent. thus intelligence is the ability to deal with a wide range of possibilities  not just a few specific situations. putting these things together gives us our informal definition of intelligence:
intelligence measures an agent's general ability to achieve goals in a wide range of environments.
we are confident that this definition captures the essence of many common perspectives on intelligence. it also describes what we want to achieve in machines: a very general capacity to adapt and perform well in a wide range of situations. in this paper we will try to formalise this view of intelligence.
1 a formal framework
we refer to the signals sent from the agent to the environment as actions  and the signals sent back as perceptions. our definition requires there to be a goal for the agent to try to achieve which implies that the agent knows what it is. if the goal was known in advance it could be built into the agent  however this would limit each agent to just one goal. the alternative is to inform the agent of what the goal is. unfortunately  the possession of a high level of language is too strong an assumption to make. thus we define a communication channel with very simple semantics: a signal that indicates how good the agent's current situation is  called the reward. the agent then simply tries to maximise the amount of reward it receives by learning about the structure of the environment and what it needs to accomplish in order to receive large rewards.
모this is the extremely flexible reinforcement learning framework commonly used in ai  and is equivalent to the controller-plant framework in control theory. in our formulation we will include the reward signal as part of the perception generated by the environment. thus  as the agent's goal is implicitly defined by the environment  to test an agent in any given way it is sufficient to fully define its environment.
모formally  the agent sends information to the environment by sending symbols from some finite set called the action space  for example  a := {up down left right}. the environment responds with symbols from a finite set called the perception space  denoted p. the reward space  denoted by r  is a finite subset of  1  뫌 q. every perception consists of two separate parts; a reward and a non-reward part called an observation. for symbols being sent we will use the lower case variable names o  r and a for observations  rewards and actions respectively. we index these in the order in which they occur  thus a1 is the agent's third action. the agent and the environment take turns at sending symbols  starting with the environment. this produces an increasing history of observations  rewards and actions  o1a1r1o1a1 ....
모the agent is a function  denoted by 뷇  which takes the current history as input and chooses the next action as output. we represent this as a probability measure over actions conditioned on the current history  for example  뷇 a1|o1a1r1 . the workings of the agent is unspecified  though in artificial intelligence the agent will be a machine and thus 뷇 will be a computable function. the environment  denoted 뷃  is similarly defined:  k 뫍 n the probability of okrk  given the current history is 뷃 okrk|o1a1r1 ...ok 1rk 1ak 1 .
모the agent must try to maximise the total reward it receives  however what this means depends on how we value rewards at different points in the future. the standard way of expressing this is to weight the future reward at time i by a factor 붺i.
thus the future value is    where ri is the reward in cycle i of a given history  and the expected value is taken over all possible interaction histories of 뷇 and 뷃. the choice of 붺i is a subtle issue that controls how greedy or far sighted the agent should be. here we use the near-harmonic 붺i := 1/i1 as this produces an agent with increasing farsightedness of the order of its current age  hutter  1 .
모as we desire an extremely general definition of intelligence for arbitrary systems  our space of environments should be as large as possible. an obvious choice is the space of all probability measures  however this causes serious problems as we cannot even describe some of these measures in a finite way. the solution is to require the measures to be computable. this allows for an infinite space of possible environments with no bound on their complexity. it also permits environments which are non-deterministic as it is only their distributions which need to be computable. this space  denoted e  appears to be the largest useful space of environments.
1 a formal measure of intelligence
we want to compute the general performance of an agent in unknown environments. as there are an infinite number of environments  we cannot simply take a uniform distribution over them. if we consider the agent's perspective on the problem  this is the same as asking: given several different hypotheses which are consistent with the data  which hypothesis should be considered the most likely  this is a standard problem in inductive inference for which the usual solution is to invoke occam's razor: given multiple hypotheses which are consistent with the data  the simplest should be preferred. as this is generally considered the most intelligent thing to do  we should test agents in such a way that they are  at least on average  rewarded for correctly applying occam's razor. this means that our a priori distribution over environments should be weighted towards simpler environments.
모as each environment is described by a computable measure  one way of measuring the complexity of an environment is by taking its kolmogorov complexity. if u is a prefix-free universal turing machine then the kolmogorov complexity of an environment 뷃 is the length of the shortest program on u that computes 뷃  formally k 뷃  := minp{l p  : u p  = 뷃}. unfortunately  k is not computable and is provably difficult to approximate. for the purposes of occam's razor  it also seems philosophically unnatural to consider short programs which require an enormous amount of time to compute to be  simple . we can address both of these problems by using a notion of complexity that takes execution time into account  such as kt complexity  levin  1 . formally  kt 뷃  := minp{l p  + logt p  : u p  = 뷃} where t p  is the number of steps required to compute 뷃 on u. this gives us a computable distribution 1 kt 뷃  over our space of possible environments which is consistent with the notion that very simple algorithms should be short and fast to compute. another alternative is the speed prior  schmidhuber  1 .
모we can now define the universal intelligence of an agent 뷇 to simply be its expected performance when faced with an unknown environment sampled from this distribution 
붮 뷇  := x 1 kt 뷃 v 뷇뷃. 뷃뫍e
1 properties of universal intelligence
it is clear by construction that universal intelligence measures the general ability of an agent to perform well in a very wide range of environments  as required by our informal definition of intelligence given earlier. the definition places no restrictions on the internal workings of the agent; it only requires that the agent is capable of generating output and receiving input which includes a reward signal. universal intelligence also reflects occam's razor in a natural way that respects both the minimal description and computation time of an environment. indeed it is similar to intelligence tests for humans which usually define the correct answer to a question to be the simplest consistent with the given information. although our space of possible environments e is infinite  as kt is computable  붮 뷇  can also be computed.
모by considering v 뷇뷃 for a number of basic environments  such as small mdps  and agents with simple but very general optimisation strategies  it is clear that 붮 correctly orders the relative intelligence of these agents in a natural way. if we consider a highly specialised agent  for example ibm's deepblue chess super computer  then we can see that this agent will be ineffective outside of one very specific and complex environment  and thus would have a very low universal intelligence value. this is consistent with our view of intelligence as being a highly adaptable and general ability.
모a very high value of 붮 would imply that an agent was able to perform well in many environments. such a machine would obviously be of large practical significance. if we replace near-harmonic discounting with a finite length horizon and ignore computation time  it is possible to define an order relation between agents over interaction histories  known as the intelligence order relation  ior   hutter  1 . the maximal agent with respect to this order relation is aixi  and with minor adjustments  it would also be maximal with respect to 붮. aixi has been shown to have many optimality properties  including the ability to be self-optimising in environments in which this is at all possible  hutter  1 . these results demonstrate the power of agents which rate highly with respect to the ior and the related 붮 defined here.
모clearly 붮 spans simple adaptive agents right up to super intelligent agents like aixi  unlike the pass-fail turing test which is useful only for agents with near human intelligence. moreover  the turing test is highly anthropomorphic  with many suggesting that it is a test of humanness rather than intelligence. we have avoided this problem of human bias  as well as the need for human judges  by basing our definition on the fundamentals of information and computation theory.
모the only related work to ours is the c-test  hernandez-뫣 orallo  1 . while 붮 is an interactive test  the c-test is a static sequence prediction test which always ensures that each question has an unambiguous answer. we believe that these are unrealistic and unnecessary assumptions. the c-test was able to compute a number of usable test problems which were shown to correlate with real iq test scores for humans.
