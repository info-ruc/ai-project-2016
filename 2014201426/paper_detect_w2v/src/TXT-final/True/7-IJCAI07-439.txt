
named entity recognition  ner  is the task of locating and classifying names in text. in previous work  ner was limited to a small number of predefined entity classes  e.g.  people  locations  and organizations . however  ner on the web is a far more challenging problem. complex names  e.g.  film or book titles  can be very difficult to pick out precisely from text. further  the web contains a wide variety of entity classes  which are not known in advance. thus  hand-tagging examples of each entity class is impractical.
this paper investigates a novel approach to the first step in web ner: locating complex named entities in web text. our key observation is that named entities can be viewed as a species of multiword units  which can be detected by accumulating n-gram statistics over the web corpus. we show that this statistical method's f1 score is 1% higher than that of supervised techniques including conditional random fields  crfs  and conditional markov models  cmms  when applied to complex names. the method also outperforms cmms and crfs by 1% on entity classes absent from the training data. finally  our method outperforms a semi-supervised crf by 1%.
1 introduction
named entity recognition  ner  is the task of identifying and classifying names in text. in previous work  ner was carried out over a small number of pre-defined classes of entities-for example  the named entity task definition from the message understanding conference identified the three classes person  organization  and location  grishman and sundheim  1 . however  the web contains a variety of named entities that fall outside these categories  in-
cluding products  e.g.  books  films   diseases  programming languages  nationalities  and events  to name a few.
모ner requires first locating entity names in text. some named entities are easy to detect in english because each word in the name is capitalized  e.g.   united kingdom   but others are far more difficult. consider the two phrases:
 i  ...companies such as intel and microsoft...
 ii  ...companies such as procter and gamble...
모the first phrase names two entities   intel  and  microsoft    whereas the second names a single entity   procter and gamble  . however  this distinction is not readily apparent from the text itself. similar situations occur in film titles   dumb and dumber    book titles   gone with the wind    and elsewhere  the  war of 1  .1 the challenging problem is to precisely determine the boundaries of the entity names in the text. we refer to this problem as the entity delimitation problem. since entity delimitation does not determine the entity's class  it is a subproblem whose solution is necessary but not sufficient for successful ner.
모another fundamental challenge for standard  supervised ner techniques is that the set of entity classes is not known in advance for many web applications  including information extraction  etzioni et al.  1   question answering  banko  1   and search  pasca  1 . thus  it is impractical to hand tag elements of each entity class to train the supervised techniques. instead  we are forced to create a training corpus where entities of any type are labeled  entity   and nonentities are labeled as such. this solution is problematic because ner techniques rely on orthographic and contextual features that can vary widely across entity classes. we refer to this problem as the unseen classes problem.
모this paper introduces the lex method  which addresses both the delimitation and unseen classes problems by utilizing n-gram statistics computed over a massive web corpus. lex does not address the entity classification problem. lex is based on the observation that complex names tend to be multi-word units  mwus   that is  sequences of words whose meaning cannot be determined by examining their constituent parts  da silva et al.  1 ; mwus can be identified with high accuracy using lexical statistics  the frequency of words and phrases in a corpus  da silva and lopes  1 .
모lex requires only a single threshold that is easily estimated using a small number of hand-tagged examples drawn from a few entity classes. however  lex does require an untagged corpus that contains the relevant lexical items a handful of times in order to compute accurate lexical statistics. thus  lex is well-suited to the web where entity classes are not known in advance  many entity names are complex  but a massive untagged corpus is readily available.
모lex is a semi-supervised learning method  so in addition to conditional random fields  crfs   lafferty et al.  1  and conditional markov models  cmms   mccallum et al.  1   we compare it with crfs augmented with untagged data using co-training and self-training. we find that lex outperforms these semi-supervised methods by more than 1%.
our contributions are as follows:
1. we introduce the insight that statistical techniques forfinding mwus can be employed to detect entity names of any class  even if the classes are unknown in advance.
1. we demonstrate experimentally that lex  a surprisingly simple technique based on the above insight  substantially outperforms standard supervised and semisupervised approaches on complex names rarely considered in ner such as the titles of films and books.1
1. we show preliminary results demonstrating that lex can be leveraged to improve web applications  reducing the error rate of a web information extraction system by 1% in one case.
1. based on an analysis of the limitations of lex  we propose an enhanced algorithm  lex++  which attaches common prefixes and suffixes to names  and uses additional lexical statistics to detect spurious collocations. lex++ offers an additional 1% performance improvement over lex.
모section 1 describes lex. section 1 presents our experimental results comparing lex with previous supervised and semi-supervised methods. section 1 presents an analysis lex's limitations  along with an experimental evaluation of
lex++. section 1 considers related work  and the paper concludes with a discussion of future work.
1 the lex method for locating names
the lexical statistics-based approach we use for locating names is based on the key intuition that names are a type of multi-word unit  mwu . broadly  our algorithm assumes that contiguous capitalized words1 are part of the same name  and that a mixed case phrase beginning and ending with capitalized words is a single name if and only if it is a mwu-that is  if it appears in the corpus sufficiently more frequently than chance. capitalized words at the start of sentences are only considered names if they appear capitalized sufficiently often when not at the beginning of a sentence.
모more formally  the lexical method  denoted by lex  is defined by two thresholds 뷉 and 붻  and a function f s1 s1 s1  mapping three strings to a numerical value. the function f s1 s1 s1  measures the degree to which the concatenated string s1s1 occurs  more than chance  - several such collocation identifiers exist in the mwu literature  and we detail the two measures we experiment with below. the threshold 뷉 is the value f s1 s1 s1  must exceed in order for the string s1s1 to be considered a single name. lastly  if a capitalized word's appearances come at the beginning of a sentence more than 붻 of the time  we assume it is not a name. we heuristically set the value of 붻 to 1  and set 뷉 using training data as described in section 1.
모given a sentence s  a function f s1 s1 s1   and thresholds 뷉 and 붻  lex proceeds as follows:
1. initialize a sequence of names n =  n1 n1 ... nm  equal to the maximal contiguous substrings of s that consist entirely of capitalized words  where elements of n are ordered from left to right in s. if when the first word of s appears capitalized in the corpus  it is at the beginning of a sentence more than 붻 of the time  omit it from n.
1. until all consecutive name pairs in n have been evaluated:
 a  choose the unevaluated pair of names  ni ni+1  with minimum i.
 b  if f ni wi ni+1    뷉  where wi is the uncapitalized phrase separating ni and ni+1 in s  and wi is at most three words in length 
  replace ni and ni+1 in n with the single name niwini+1.
1 measures of collocation
the function f s1 s1 s1  is intended to measure whether a string s1s1 occurring in text is a single name  or if instead s1 and s1 are distinct names separated by the string s1. the functions f we consider are based on two standard collocation measures  pointwise mutual information  pmi  and symmetric conditional probability  scp   da silva and lopes  1 . define the collocation function ck a b  for two strings
 1 
where p s  is the probability of the string s occurring in
the corpus.
모in the above  c1 gives the scp between a and b  and c1 is  for our purposes  equivalent to pmi.1 pmi and scp are defined for only two arguments; a variety of techniques have been employed to extend these binary collocation measures to the n-ary case  for example by averaging over all binary partitions  da silva and lopes  1  or by computing the measure for only the split of highest probability  schone and jurafsky  1 . here  we generalize the measures in the following simplistic fashion:
		 1 
모we take g1 as our pmi function for three variables  and g1 as our scp function for three variables.
1 experimental results
this section describes our experiments on web text  which compare lex's performance with both supervised and semisupervised learning methods. experiments over the web corpus can be tricky due to both scale and the diversity of entity classes encountered  so we begin by carefully describing the experimental methodology used to address these difficulties.
1 experimental methodology
many classes of named entities are difficult to delimit precisely. for example  nested entities like the phrase  director of microsoft  could be considered a single entity  a job title   two entities  a job title and a company   or both. to make evaluation as objective as possible  we performed our experiments on entities from four classes where the boundaries of names are relatively unambiguous:  actor   book  company  and film .
모the lex method requires a massive corpus in order to accurately compute its n-gram statistics. to make our experiments tractable  we approximated this corpus with 1 sentences automatically selected as likely to contain named entities from the above four classes.1
모we formed a training set containing a total of 1 sentences  evenly distributed among the four classes. in these training sets  we manually tagged all of the entities 1 resulting in a total of 1 tagged examples. we formed a test set by hand-tagging a sample of 1 entities from each class; because the lexical approach requires estimates of the probabilities of different phrases  we ignored any entities that appeared fewer than five times in our corpus. to ensure that this was a reasonable restriction  we queried google and found that over 1% of our original test entities appeared on at least 1 distinct web pages. thus  the requirement that entities appear at least five times in the corpus is a minor restriction. given the massive size of the web corpus  a lexical statistics method like lex is likely to be effective for most entities.
모we further separated our test set into  easy  and  difficult  cases. precisely  the easy cases are those that are correctly identified by a baseline method that simply finds maximal substrings of contiguous capitalized words. the difficult cases are those on which this simple baseline fails. our final test set contained 1 difficult cases  and 1 easy cases  suggesting that difficult entity names are quite common on the web.
모in the test phase  we ran each of our methods on the sentences from the test set. recall is defined as the fraction of test set entities for which an algorithm correctly identifies both the
f1recallprecisionlex-pmi111lex-scp1  1% 11table 1: performance of lex using collocation measures pmi and scp. scp outperforms pmi by 1% in f1.
left and right boundaries. precision is defined as the fraction of putative entities suggested by the algorithm that are indeed bona fide entities. because our test set contains only a proper subset of the entities contained in the test text  for the purpose of computing precision we ignore any entities identified by the algorithm that do not overlap with an element of the test set.
1 pmi versus scp
our first experiment investigates which of the candidate collocation functions  pmi or scp  is most effective for use in the lex method. in these experiments and those that follow  the threshold 뷉 is set to the value that maximizes performance over the training set  found using simple linear search.
모the performance of each metric on difficult cases is shown in table 1. the scp metric outperforms pmi by a considerable margin. this performance difference is not merely a consequence of our method for acquiring thresholds; the performance discrepancy is essentially unchanged if each threshold is chosen to maximize performance on the test set. further  although we are focusing on difficult cases  the scp metric also outperforms pmi on the non-difficult cases as well  1 vs. 1 in f1 .
the large performance difference between lex-pmi and
lex-scp is a more extreme version of similar results exhibited for those measures in mwu identification  schone and jurafsky  1 . in our case  pmi performs poorly on our task due to its well-known property of returning disproportionately low values for more frequent items  manning and schutze  1몮  . pmi fails to correctly join together several of the more frequent names in the test set. in fact  of the names that pmi fails to join together  1% appear in the corpus more than 1 times - versus 1% for scp.
모in the remainder of our experiments the lexical method employs the scp function and is referred to simply as lex.
1 comparison with ner approaches
we experimented with the following ner methods:
  svmcmm - a supervised conditional markov model trained with a probabilistic support vector machine.
  crf - a supervised conditional random field approach.
  caps - a baseline method that locates all maximal contiguous substrings of capitalized words.
  man - an unsupervised name recognizer  based on manually-specified rules. this method is taken from the name location subcomponent of the knowitall web information extraction system  etzioni et al.  1 .
모both conditional random fields  crf   lafferty et al.  1  and conditional markov models  cmm   mccallum et al.  1  are state of the art supervised approaches for
f1recallprecisionsvmcmm111crf111man111lex1  1% 11table 1: performance on difficult cases lex's f1 score is 1% higher than the nearest competitor  svmcmm.
f1recallprecisionsvmcmm111crf111man111lex111caps1  1% 11table 1: performance on easy cases all methods perform comparably near the perfect performance of the caps baseline; caps outperforms lex and man by 1%.
text delimitation tasks. our crf and cmm implementations were taken from the minorthird text classification package  cohen  1 . we obtained settings for each algorithm via cross-validation on the training set. we chose to train the cmm using a support vector machine  svmcmm  rather than with the maximum entropy approach in  mccallum et al.  1  because the svmcmm substantially outperformed the maximum-entropy classifier in cross-validation. the supervised classifiers create feature vectors for each word; these features include the lower-case version of the word  the orthographic pattern for the word  e.g.  xx+  for  bill  or  1+  for  1   etc   and analogous features for tokens in a small window around the word  of size selected by cross validation .
모by definition  the caps baseline performs perfectly on easy entity names  but does not correctly delimit any difficult entity names. caps is equivalent to step 1 in the lex algorithm in section 1. although lex includes caps as an initial step to handle easy names  the two algorithms will differ on easy names when the lexical statistics  computed in step 1 of lex  indicate that two easy names delimited by caps ought to be concatenated. consider  for example  the phrase  ...as intel and microsoft have... . caps will always delimit this phrase's two names correctly  but lex could potentially delimit them incorrectly  by  for example  outputting the phrase  intel and microsoft  as a single name . in practice  this situation occurs infrequently  see table 1 .
모the performance of the different methods on difficult cases is shown in table 1. overall  lex's f1 score of 1 is 1% higher than that of its nearest competitor  svmcmm  at 1 . the precision differences between lex and svmcmm are significant at p   1  and the recall differences are significant at p   1  fisher exact test . of course  performance on the easy cases is also important. as shown in table 1  all methods perform comparably well on easy cases  with f1 scores in the 1 and higher range.
모the performance of lex is fairly robust to parameter changes. a sensitivity analysis revealed that altering the
f1recallprecisionsvmcmm111crf111man111lex1  1% 11table 1: performance on unseen entity classes  difficult cases  lex outperforms its nearest competitor  svmcmm  by 1%.
f1recallprecisionsvmcmm111crf111man111lex111caps1  1% 11table 1: performance on unseen entity classes  easy cases  caps outperforms all methods by a small margin  performing 1% better than its nearest competitor  man .
learned value of 뷉 by a factor of 1 in either direction resulted in lex still outperforming svmcmm by at least 1% on difficult cases. also  altering the value of 붻 to either 1 or 1 resulted in lex outperforming svmcmm by at least 1% on difficult cases.
1 unseen entity classes
as argued in the introduction  in many web applications  the target entity classes are not known in advance. this poses particular problems for standard ner approaches  which rely on textual cues that can vary widely among entity classes. in this experiment  we simulate the performance of each of our entity delimitation methods on unseen entity classes. specifically  when evaluating performance on an entity class  we remove from the training set any sentences containing entities of that class. the results of these experiments on difficult cases are shown in table 1. lex outperforms the other approaches  with f1 performance more than 1% higher than the best competing method  svmcmm . the precision and recall differences between lex and svmcmm on difficult cases are each statistically significant at p   1  fisher exact test . as in the previous experiment  all methods perform relatively well on easy cases  table 1 .
1 semi-supervised comparison
lex outperforms supervised ner methods by leveraging lexical statistics computed over a large  untagged corpus. augmenting tagged examples with untagged data is known as semi-supervised learning  an increasingly popular approach for text processing tasks  in which untagged data is plentiful. in this section  we compare lex with previous semisupervised algorithms.
모to create a semi-supervised baseline for our task  we augmented the supervised classifiers from our previous experiments to employ untagged data. as in other recent work on semi-supervised text segmentation  ando and zhang  1   we experimented with two semi-supervised approaches: cotraining and self-training.
모one of the first semi-supervised algorithms for named entity classification  collins and singer  1  was based on semi-supervised co-training  blum and mitchell  1 . cotraining relies on a partition of the feature set to produce independent  views  of each example to be classified  where it is assumed each individual view is sufficient to build a relatively effective classifier. in co-training  separate classifiers are trained for each view. then each classifier is applied to the untagged examples  and the examples whose classifications are most certain are added as new training examples. by iteratively bootstrapping information from distinct views  co-training algorithms are able to create new training examples from untagged data.
모the co-training approach developed in  collins and singer  1  does not actually perform entity delimitation. instead  it assumes the entity delimitation problem is solved  and addresses the task of classifying a given set of entities into categories. in our attempts to utilize a similar co-training algorithm for our entity delimitation task  we found the algorithm's partitioning of the feature set to be ill suited to our task. the approach in  collins and singer  1  employed two views to classify named entities: the context surrounding a named entity  and the orthographic features of the entity itself. while the context surrounding a known entity is a good indicator of that entity's class  in our experiments we found that context was not effective for determining an unknown entity's boundaries. the co-training classifier trained on our training set using only the  context  view achieved f1 performance of 1 on difficult cases  and 1 on easy cases  versus 1 and 1 for the classifier trained on all features . this violates the central assumption in co-training  that each view be independently capable of producing a relatively effective classifier.
모because co-training was unsuited to our task  we experimented with  single-view  co-training  also known as selftraining. this approach involved training a supervised classifier on the training set  and then iteratively applying the classifier to the untagged data  adding to the training set the r sentences on which the classifier is most confident.
모our experimental results are shown in table 1. because only the crf classifier returned reliable probability estimates for sentences  it is the only classifier tested in a semisupervised configuration. in these experiments  r = 1  and the algorithm was executed for a total of 1 self-training iterations. as shown in the table  the bootstrapped training examples make positive but insignificant impacts on the performance of the baseline supervised classifier. lex's f1 performance of 1 is 1% higher than that of the best semisupervised crf in table 1  1 .
1 information extraction performance
one important application of web named entity delimitation is found in unsupervised information extraction systems  e.g.  etzioni et al.  1 . these systems use generic patterns  e.g.  films such as   to extract entities  but require some method to identify the boundaries of the entities to extract. given a particular set of patterns  a more accurate name delimitation system should result in better extraction accuracy. in this experiment  we tested a pattern-based information ex-
self-tagged sentencesf1recallprecision1.1.1.11111.1.1.1table 1: performance of semi-supervised crf  difficult cases  the additional training sentences generated by the semi-supervised algorithm   self-tagged sentences   do not significantly improve performance over the original 1 sentences from the training set.

figure 1: information extraction performance for the film class  difficult cases . lex's precision is 1% higher than its closest competitor  svmcmm .
traction system on our evaluation corpus  using each of our methods to delimit entities. we ranked all extracted entities in order of decreasing frequency to obtain precision/recall curves for each class. figure 1 shows the precision/recall curves for the 1 most frequent extractions of difficult cases in the film class. as the graph shows  lex enables significantly higher precision than the other methods  as before  all methods perform comparably well on easy extractions . additional experiments are necessary to assess information extraction performance on other classes.
1 error analysis and enhancements
lex is a simple algorithm  and there are ways in which its simplistic assumptions about the structure of names hinder its performance. in this section  we detail three of lex's primary error types  and we introduce lex++  an enhanced algorithm which addresses lex's limitations. lex++ follows the lex algorithm given in section 1  with modifications for each of lex's error types as detailed below.
numbers and punctuation at name boundaries
lex's assumption that names begin and end with capitalized words is violated by any names that begin or end with numbers or punctuation-for example  the film  1 mile  or a company name including trailing punctuation such as  dell inc.  this limitation of lex accounted for 1% of its false negatives on difficult cases in the experiments in section 1.
모lex++ addresses this limitation of lex by optionally extending names to the left or right  or both  to include numbers or punctuation. formally  if a name ni  output by lex  is immediately preceded by a single number or punctuation mark ti 1  lex++ prepends ti 1 to ni whenever c1 ti 1 ni  exceeds a threshold 붺. similarly  a number or punctuation mark ti immediately following ni is appended to ni whenever c1 ni ti    붺.
common prefixes and suffixes
lex's assumption that named entities correspond to lexical collocations does not always hold in practice. for example  lex rarely attaches common prefixes and suffixes to names. in an entity like  cisco systems  inc.  the statistical association between the suffix  inc  and the company name  cisco systems  is relatively small  because  inc  occurs so frequently in the corpus as a part of other entity names. the situation is further complicated by the fact that most prefixes and suffixes should be considered part of the entity name  as in the  inc.  example above   but others should not  e.g.  honorifics like  sir  or  mr.  .1 mis-applying prefixes and suffixes is responsible for 1% of lex's false negatives.
모lex++ attempts to attach common prefixes and suffixes to names. to determine whether a given phrase is a prefix or a suffix  we would like to measure how frequently the phrase appears as a constituent of other entity names. as an approximation to this quantity  we use the fraction of times the phrase is followed or preceded by a capitalized word. formally  we consider a phrase r = niwini+1  where ni and ni+1 are distinct names output by lex  to be a single name including a prefix niwi if the phrase niwi is followed by a capitalized word in the corpus more than a threshold 뷋 of the time. similarly  r is a single name including a suffix if wini+1 is preceded by a capitalized word more than 뷋 of the time.1
related entities
lex also makes errors due to the high statistical association between distinct but related entities. phrases containing multiple related entities  e.g.  conjunctions like  intel and amd   occur frequently relative to their constituent parts  and therefore appear to lex to be mwus. thus  these phrases are often erroneously output as single names by lex.
모lex++ attempts to identify phrases containing distinct names which happen to be mentioned together frequently. specifically  lex++ exploits the fact a named entity requires a unique order for its constituent terms  whereas a phrase containing distinct entities often does not. for example  consider that the phrase  intel and amd  appears with similar frequency in its reversed form   amd and intel    whereas the same is not true for the single entity  pride and prejudice. 
모formally  consider a phrase r = niwini+1 which would be concatenated into a single name by lex  that is  g1 ni wi ni+1    뷉. lex++ outputs r as a single name only if p ni+1wini   the probability of the reverse of r  is sufficiently small. in our experiments  we use a heuristic of
f1recallprecisionlex111lex++1  +1% 11table 1: performance of lex++ versus lex  difficult cases . lex++ outperforms lex by 1%.
outputting r as a single name only when the reverse of r appears at most once in our corpus.
1 experiments with lex++
we compared the performance of lex++ to lex under the experimental configuration given in section 1. the values of 뷉 and 붻 were taken to be the same for both algorithms  and the thresholds 뷋 and 붺 were set using linear search over the training set.
모the results of our experiments are shown in table 1. for difficult entity names  the enhancements in lex++ offer a 1% improvement in f1 over the original lex. the performance of both algorithms on easy cases is similar-lex++ has a slightly lower f1 score  1  than that of lex  1 .
1 related work
the majority of named entity recognition  ner  research has focused on identifying and classifying entities for a small number of pre-defined entity classes  in which many entities can be identified using capitalization. our focus  by contrast  is on delimiting names in cases where entity classes may be unknown  and when names are complex  involving a mixture of case .
모several supervised learning methods have been used previously for ner  including hidden markov models  and more recently maximum entropy markov models  mccallum et al.  1  and conditional random field  crf  models  lafferty et al.  1; mccallum and li  1 . our experiments show that lex outperforms these methods on our task. semisupervised methods for ner  which leverage untagged data  were investigated specifically in the ner component of the conll 1 shared task  tjong kim sang and de meulder  1 . while this task was primarily aimed at typical ner categories  person  location  and organization   it also encompassed other  miscellaneous  entities  including nationalities and events. the evaluation corpus consisted of high quality newswire text  and offered a relatively small untagged corpus  roughly six times the size of the tagged corpus . our focus is on leveraging the web  a vastly larger  and lower quality  untagged corpus. further  our experiments show that lex outperforms canonical semi-supervised methods on our task. these two distinctions are likely responsible for the fact that none of the top performing systems in the conll task made profitable use of untagged data  in sharp contrast to our experiments in which lex outperforms supervised methods by making crucial use of untagged data.
our experiments compared lex with previous semi-
supervised methods for ner  including co-training  collins and singer  1; blum and mitchell  1  and single-view bootstrapping. in recent work  li and mccallum used untagged data to devise context-based word clusters  which were added as features to a supervised crf algorithm. the untagged data was shown to improve performance in part of speech tagging and chinese word segmentation  li and mccallum  1 . also  recently ando and zhang developed a semi-supervised method for text chunking  based on training thousands of classifiers on untagged data  and using the commonalities of the classifiers to devise relevant features for a particular task  ando and zhang  1 . this method was shown to improve performance in named entity delimitation on the conll 1 corpus mentioned above. in contrast to lex  which is simple  the methods in  li and mccallum  1  and  ando and zhang  1  are complex. further  neither of these methods have been evaluated on tasks such as ours  where entity names are difficult and the target entity classes may not be known in advance. comparing lex with these semi-supervised techniques is an item of future work.
모lastly  the basic insight used in lex - that named entities are a type of mwu - was also employed in  da silva et al.  1  for an unsupervised entity extraction and clustering task. in contrast to that work  lex is a technique for semi-supervised named entity delimitation  and we provide experiments demonstrating its efficacy over previous work.
1 conclusions
this paper shows that lex  a simple technique employing capitalization cues and lexical statistics  is capable of locating names of a variety of classes in web text. our experiments showed that this simple technique outperforms standard supervised and semi-supervised approaches for named entity recognition in cases where entity names are complex  and entity classes are not known in advance.
모google's recent release to the research community of a trillion-word n-gram model makes lex practical at web scale. evaluating lex in a large-scale experiment is an important item of future work. further  although the statistical approach performed well in our experiments  it remains the case that textual features are valuable clues in identifying names-investigating ways to utilize both lexical statistics and textual features simultaneously is a natural next step. finally  further investigation into leveraging improved web entity location to enhance applications such as web search  information extraction  and question answering is an important item of future work.
acknowledgments
we thank michele banko  charles downey  jonathan pool  and stephen soderland for helpful comments. this research was supported in part by nsf grants iis-1 and iis-1  darpa contract nbchd1  onr grant n1-1 as well as gifts from google  and carried out at the university of washington's turing center. the first author was supported by a microsoft research fellowship sponsored by microsoft live labs.
