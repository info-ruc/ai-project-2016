
constraint satisfaction problems  csps  are ubiquitous in artificial intelligence. the backtrack algorithms that maintain some local consistency during search have become the de facto standard to solve csps. maintaining higher levels of consistency  generally  reduces the search effort. however  due to ineffectiveconstraint propagation it often penalises the search algorithm in terms of time. if we can reduce ineffective constraint propagation  then the effectiveness of a search algorithm can be enhanced significantly. in order to do so  we use a probabilistic approach to resolve when to propagate and when not to. the idea is to perform only the useful consistency checking by not seeking a support when there is a high probability that a support exists. the idea of probabilistic support inference is general and can be applied to any kind of local consistency algorithm. however  we shall study its impact with respect to arc consistency and singleton arc consistency  sac . experimental results demonstrate that enforcing probabilistic sac almost always enforces sac  but it requires significantly less time than sac. likewise  maintaining probabilistic arc consistency and maintaining probabilistic sac require significantly less time than maintaining arc consistency and maintaining sac.
1 introduction
constraint satisfaction problems  csps  are ubiquitous in artificial intelligence. they involve finding values for problem variables subject to constraints. for simplicity  we restrict our attention to binary csps. backtrack algorithms that maintain some local consistency during search have become the de facto standard to solve csps. maintaining a higher level of local consistency before and/or during search usually reduces the thrashing behaviourof a backtrackalgorithm. however  the amount of ineffective constraint propagationalso increases  which can penalise the algorithm in terms of time.
모arc consistency  ac  is the most widely used local consistency technique to reduce the search space of csps. coarse-

모모  the first author is supported by the boole centre for research in informatics.grained arc consistency algorithmssuch as ac-1  mackworth  1  and ac-1  bessie`re and r뫣egin  1  are competitive. these algorithms repeatedly carry out revisions  which require support checks for identifying and deleting all unsupported values from the domain of a variable. however  for difficult problems  in many revisions  some or all values successfully find some support  that is to say  ineffective constraint propagation occurs. for example  when rlfap 1 is solved using either mac-1 or mac-1 equipped with dom/deg as a variable ordering heuristic  1% of the total revisions are ineffective. if we can avoid this ineffective propagation  then a considerable amount of work can be saved.
모recently  singleton arc consistency  sac   debruyne and bessie`re  1  has been getting much attention. it is a stronger consistency than ac. therefore  it can avoid much unfruitful exploration in the search-tree. however  as pointed out earlier  as the strength of local consistency increases  so does the amount of ineffective propagation. hence  applying it before search can be expensive in terms of checks and time  and maintaining it during search can be even more expensive. at the cpai'1 workshop   mehta and van dongen  1a  presented a probabilistic approach to reduce ineffective propagation and studied it with respect to arc consistency on random problems. this probabilistic approach is to avoid the process of seeking a support  when the probability of its existence is above some  carefully chosen  threshold. this way a significant amount of work in terms of support checks and time can be saved.
모in this paper  we present a more extensive investigation. we study the impact of using the probabilistic approach not only in ac but also in sac and lsac  limited version of sac proposed in this paper  on a variety of problems. we call the resulting algorithms probabilistic arc consistency 1  pac   probabilistic singleton arc consistency  psac   and probabilistic lsac  plsac . our four main contributions  then  are as follows: first  we investigate the threshold at which maintaining pac  mpac  performs the best on different classes of randomproblems. second  we carry out experimentsto determine how well mpac does on a variety of known problems including real-world problems. our findings suggest that mpac usually requires significantly less time than mac. next  we examine the performances of psac and plsac when used as a preprocessor before search. experimental results demonstrate that enforcing probabilistic sac and lsac almost always enforces sac and lsac but usually require significantly less time. finally  we investigate the impact of maintaining psac and plsac during search on various problems. results show a significant gain in terms of time on quasi-group problems. overall  empirical results presented in this paper demonstrate that the original algorithms are outperformed by their probabilistic counterparts.
모the remainder of this paper is organised as follows: section 1 gives an introduction to constraint satisfaction and consistency algorithms. section 1 introduces the notions of a probabilistic supportcondition and probabilistic revision condition. experimental results are presented in section 1 followed by conclusions in section 1.
1 preliminaries
a constraint satisfaction problem  v d c  is defined as a set v of n variables  a non-empty domain d x  뫍 d  for all x 뫍 v and a set c of e constraints among subsets of variables of v. the density of a csp is defined as 1e/ n1   n . a binary constraint cxy between variables x and y is a subset of the cartesian product of d x  and d y  that specifies the allowed pairs of values for x and y. the tightness of the constraint cxy is defined as 1   |cxy |/|d x  뫄 d y |. a value b 뫍 d y   also denoted as  y b   is called a support for  x a  if  a b  뫍 cxy. similarly   x a  is called a support for  y b  if  a b  뫍 cxy. a support check is a test to find if two values support each other. the directed constraint graph of a csp is a graph having an arc  x y  for each combination of two mutually constraining variables x and y. we will use g to denote the directed constraint graph of the input csp.
모a value a 뫍 d x  is arc consistent if  y 뫍 v constraining x the value a is supported by some value in d y . a csp is arc consistent if and only if
d x    x a  is arc consistent. we denote the csp p obtained after enforcing arc consistency as ac p . if there is a variable with an empty domain in ac p   we denote it as ac p  = 뫐. usually  an input csp is transformed into its arc consistent equivalent  before starting search. we call the domain of x in this initial arc consistent equivalent of the input csp the first arc consistent domain of x. we use dac x  for the first arc consistent domain of x  and d x  for the current domain of x. the csp obtained from p by assigning a value a to the variable x is denoted by p|x=a. the value a 뫍 d x  is singleton arc consistent if and only if. a csp is singleton arc consistent if and only if each value of each variable is singleton arc consistent.
모mac is a backtrack algorithm that maintains arc consistency after every variable assignment. forward checking  fc  can be considered a degenerated form of mac. it ensures that each value in the domain of each future variable is fc consistent  i.e. supported by the value assigned to every past and current variable by which it is constrained. msac is a backtrack algorithm that maintains singleton arc consistency. throughout this paper  familiarity with the arc consistency algorithm ac-1  mackworth  1  is assumed.
1 probabilistic support inference
the traditional approach to infer the existence of a support for a value a 뫍 d x  in d y  is to identify some b 뫍 d y  that supports a. this usually results in a sequence of support checks. identifying the support is more than needed: knowing that a support exists is enough. the notions of a support condition  sc  and a revision condition  rc  were introduced in  mehta and van dongen  1b  to reduce the task of identifying a support up to some extent for arc consistency algorithms. if sc holds for  x a  with respect to y  then it guarantees that  x a  has some support in d y . if rc holds for an arc   x y   then it guarantees that all values in d x  have some support in d y . in the following paragraphs  we describe the special versions of sc and rc which facilitates the introduction of their probabilistic versions.
모let cxy be the constraint between x and y  let a 뫍 d x   and let r y  = dac y    d y  be the values removed from the first arc consistent domain of y. the support count of  x a  with respect to y  denoted sc x y a   is the number of values in dac y  supporting a. note that |r y | is an upper bound on the number of lost supports of  x a  in y. if sc x y a    |r y | then  x a  is supported by y. this condition is called a special version of a support condition. for example  if |dac y | = 1  sc x y a  = 1  and |r y | = 1  i.e. 1 value is removed from dac y   then sc holds and  x a  has a support in d y  with a probability of 1. hence  there is no need to seek support for a in d y .
모for a given arc   x y   the support count of x with respect to y  denoted sc x y   is defined by sc x y  = min {sc x y a  : a 뫍 d x } . if sc x y     |r y |  then each value in d x  is supported by y. this condition is a special version of what is called a revision condition in  mehta and van dongen  1b . for example  if |dac y | = 1  sc x y  = 1 and |r y | = 1 then each value a 뫍 d x  is supported by some value of d y  with a probability of 1. hence  there is no need to revise d x  against d y .
모in the examples discussed above  if |r y | = 1  then sc and rc will fail. despite of having a high probability of the support existence  the algorithm will be forced to search for it in d y . avoiding the process of seeking a support with a high probability can also be worthwhile. in order to do so  the notions of a probabilistic support condition  psc  and a probabilistic revision condition  prc  were introduced in  mehta and van dongen  1a . the psc holds for  x a  with respect to y  if the probability that a support exists for  x a  in d y  is above some  carefully chosen  threshold. the prc holds for an arc  x y   if the probability of having some support for each value a 뫍 d x  in d y   is above some  carefully chosen  threshold.
모let ps x y a  be the probability that there exists some support for  x a  in d y . if we assume that each value in dac y  is equally likely to be removed during search  then it follows that
ps .
let t  1 뫞 t 뫞 1  be some desired threshold. if ps x y a  뫟
t then  x a  has some support in d y  with a probability of t or more. this condition is called a probabilistic support condition  psc  in  mehta and van dongen  1a . if it holds  then the process of seeking a support for  x a  in d y  is avoided. for example  if t = 1  |dac y | = 1  sc x y a  = 1  and this time |r y | = 1  then psc holds and  x a  has a support in d y  with a probability of 1.
모let ps x y  be the least probability of the values of dac x  that there exists some support in y. if ps x y  뫟 t then  each value in d x  is supported by y with a probability of t or more. this condition is called a probabilistic revision condition. if it holds then the revision of d x  against d y  is skipped. the interested reader is referred to  mehta and van dongen  1a  for details.
1 pac-1
psc and prc can be embodied in any coarse-grained ac algorithm. figure 1 depicts the pseudocode of pac-1  the result of incorporating psc and prc into ac-1  mackworth  1 . depending on the threshold  sometimes it may achieve less than full arc consistency. if psc holds then the process of identifying a support is avoided. this is depicted in figure 1. if prc holds then it is exploited before adding the arcs to the queue  in which case the correspondingarc  x y  is not added to the queue. this is depicted in figure 1. note that coarsegrained arc consistency algorithms require o ed  revisions in the worst-case to make the problem arc consistent. nevertheless  the maximum number of effective revisions  that is when at least a single value is deleted from a domain  cannot exceed o nd   irrespective of whether the algorithm used is optimal or non-optimal. thus  in the worst case  it can perform o ed   nd  ineffective revisions. in order to use psc and prc  the support count for each arc-value pair must be computed prior to search. if t = 1 then pac-1 makes the problem fc consistent. if t = 1 then pac-1 makes the problem arc consistent. the support counters are represented in o ed   which exceeds the space-complexity of ac-1. thus  the space-complexity of pac-1 becomes o ed . the worstcase time complexityof pac-1 is o ed1 . the use of psc and prc in pac-1 is presented in such a way that the idea is made as clear as possible. this should not be taken as the real implementation.  mehta and van dongen  1a  describes how to implement psc and prc efficiently.
1 experimental results
1 introduction
overview
we present some empirical results demonstrating the practical use of psc and prc. we investigate several probabilistic consistencies  particularly  probabilistic arc consistency  pac   probabilistic singleton arc consistency  psac   and a limited version of psac  plsac . first  we find out the threshold at which maintaining pac  mpac  performs the best by experimenting on model b random problems. next  we carry out experiments to determine how well mpac does when compared to mac and fc. the results for fc are also included to show that mpac is not only better than mac on problems on which fc is better than mac but as well as on the problems on which mac is better than fc. finally  we
function pac-1 currentvar  :boolean;
begin
q := g
while q not empty do begin select any x from {x :  x y  뫍 q} effective revisions := 1 for each y such that  x y  뫍 q do remove  x y  from q if y = currentvar then
   revise x y changex  else
revisep x y changex 
if d x  =   then return false
else if changex then
effective revisions := effective revisions + 1

if
:
else if effective
return true;
end;	figure 1: pac-1
function revisep x y  var changex 
begin
changex := false for each a 뫍 d x  do if ps x y a  뫟 t then
   do nothing   
else
	if	supports a then
	    :=	   
changex := true
end;
figure 1: algorithm revisep
examine the usefulness of psac and plsac when used as a preprocessor and when maintained during search.
problems studied
we have used model b  gent et al.  1  random problems and several problems from the literature to evaluate the competence of probabilistic consistencies. in model b  a random csp instance is represented as where n is the number of variables  d is the uniform domain size  c is the number of constraints  and t is the number of forbidden pairs of values. for each combination of parameter  1 instances are generated and their mean performances is reported. the remaining problems  except odd even n 1 have all been used as benchmarks for the first international csp solver competition and are described in  boussemart et al.  1 . informally  the odd-even n problem is an undirected constraint graph with a cycle with n variables. the domain of each variable is {1 1}. for each constraint cxy  if a  x a  is odd  even  then it is supported by even  odd  values of d y . the problem is unsatisfiable if n is odd.
모throughout  it has been our intention to compare generalpurpose propagation algorithms  and not special-purpose algorithms  which make use of the semantics of the constraints. some readers may argue that the probabilistic constraint propagation algorithms should have been compared with special-purpose propagation algorithms. for example  it is well known that for anti-functional constraints  there is no need to look for a support unless there is only one value left.
table 1: comparison between fc  mac  and mpac  with t=1  on random problems.
algorithm#chkstime#vnfc1 1.1 1mac1 1.1 1mpac1 1.1 1fc1 1.1 1mac1 111 1mpac1 1.1 1fc1 1.1 1mac1 1.1 1mpac1 1.1 1fc1 111 1mac1 1.1 1mpac1 1.1 1fc1 1.1 1mac1 1.1 1mpac1 1.1 1fc1 1.1 1mac1 1.1 1mpac1 1.1 1indeed  this should improve constraint propagation. however  probabilistic algorithms can be improved similarly.
implementation details
ac-1 is used to implement the arc consistency component of mac and sac. the reason why ac-1 is chosen is that it is easier to implement and is also efficient. for example  the best solvers in the binary and overall category of the first international csp solver competition were based on ac-1. similarly  pac-1 is used to implement the probabilistic arc consistency component of the probabilistic versions of mac and sac. sac-1 is used to implement singleton arc consistency. all algorithms were equipped with a dom/wdeg  boussemart et al.  1  conflict-directed variable ordering heuristic. the performance of the algorithms is measured in terms of checks  #chks   time in seconds  time   the number of revisions  #rev   and the number of visited nodes  #vn . the experiments were carried out on a pc pentium iii having 1 mb of ram running at 1 ghz processor with linux. all algorithms were written in c.
1 probabilistic arc consistency
maintaining probabilistic arc consistency in such a way that the amount of ineffective constraint propagation is minimised and simultaneously the least amount of effective propagation is avoided depends heavily on the threshold value t. therefore  a natural question is for which values of t  mpac resolves the trade-off  in terms of time  between the effort required to search and that required to detect inconsistent values. to find this out  experiments were designed to examine the behaviour of mpac with different thresholds ranging from 1 to 1 in steps of 1 on random problems. note that at t = 1  mpac maintains full arc consistency and at t = 1  it becomes forward checking. it is well known that on hard dense  loosely constrained random csps  fc performs better than mac and on hard sparse  tightly constrained random csps  mac performsbetter than fc  chmeiss and sa몮 s  1 .
therefore  we studied mpac with these classes of problems.
random problems
in our investigations  we found that inferring the existence of a support with a likelihood  roughly between 1 and 1  enables mpac to outperformboth mac and fc on both classes of problems. table 1 presents results on hard dense  loosely table 1: comparison between fc  mac  and mpac  with t = 1  on a variety of known problems.
problemalgorithm#chkstime#vn#rev

1111111 1.1 1 1odd even11.11뇸11 1	111 1 q11 1	111 1constrainedproblems first 1 rows   on which fc is better than mac  and on hard sparse  tightly constrained problems  last 1 rows  on which mac is better than fc. results demonstrate that mpac  t = 1  is better than the best of fc and mac on both classes of random problems. also  the number of nodes visited by mpac  t = 1  is nearer to mac than those visited by fc. this is the first time an algorithm has been presented that outperforms mac and fc on both classes of problems. seeing the good performance of mpac for threshold values ranging between 1 and 1  we decided to choose t = 1 for the remainder of the experiments.
problems from the literature
table 1 shows the results obtained on some instances of variety of known problems:  1  forced random binary problems frb-1 and frb-1  1  rlfap instances scen1 and scen1   1  modified rlfap instances scen1 and scen1   1  average of 1 satisfiable instances of balanced quasigroup with holes problems bqwh1 and bqwh1  1  average of 1  1 satisfiable  1 satisfiable  instances of geometric problem geom  1  two instances of attacking prime queen problem qa-1 and qa-1   1  one instance of odd-even problem odd-even 1   1  two instances of queen-knights problem k1뇸1 and k1 q1.
모one can observe in table 1  that in terms of time  on some problems  fc is better than mac  while on some  mac is better than fc. it is surprising that fc is not much inferior than mac as anticipated. this is partially because of the robustness of the conflict directed variable ordering heuristic. for easy problems  due to the expense entailed by computing the number of supports for each arc-value pair  mpac may not be beneficial. however  the time required to initialise the support counters is not much. furthermore  for all the harder instances  that require at least 1 second to solve  mpac generally pays off  by avoiding much ineffective propagation. in summary  by visiting few extra nodes than mac  mpac is able to save much ineffective propagation and solves the problem more quickly than both mac and fc.
모note that  if the domain size is small or if the values have only a few supports  then keeping the threshold high  fails psc and prc quickly  since the likelihood of support existence decreases rapidly with respect to the number of values removed from the domain and the prospect of effective propagation increases. this in turn permits mpac to act like mac. for example  in case of oddeven 1  the domain size of each variable is 1 and each arc-value pair is supported by 1 values. for this problem  fc visits exactly 1n+1   1 nodes  while mac visits only 1 nodes and for t = 1 mpac also visits only 1 nodes. the probability of support existence for a value  x a  in d y  becomes 1  as soon as 1 values are removed from y  and since the threshold is set to 1  both psc and prc fails  enabling mpac to maintain full arc consistency. this again shows that mpac with the right threshold is able to resolve when to propagate and when not to.
모we also experimented with mac-1 which uses an optimal arc consistency algorithm ac-1. however  for almost all the problems  mac-1 was consuming more time than mac-1  since there is a huge overhead of maintaining auxiliary data structures  van dongen  1 .
1 probabilistic singleton arc consistency
although there are stronger consistencies than arc consistency  the standard is to make the problemfull/partial arc consistent before and/or during search. however  recently  there has been a surge of interest in sac  bessie`re and debruyne  1; lecoutre and cardon  1  as a preprocessor of mac. the advantage of sac is that it improves the filtering power of ac without changing the structure of the problem as opposed to other stronger consistencies such as k-consistency  k   1  and so on. but  establishing sac can be expensive  and can be a huge overhead  especially for loose problems  lecoutre and cardon  1 . we investigate the advantages and disadvantages of applying psc and prc to sac.
모enforcing sac in sac-1  debruyne and bessie`re  1  style works by having an outer loop consisting of variablevalue pairs. for each  x a  if ac p|x=a  = 뫐  then it deletes a from d x . then it enforces ac. should this fail then the problem is not sac. the main problem with sac-1 is that deleting a single value triggers the addition of all variablevalue pairs in the outer loop. the restricted sac  rsac  algorithm  prosser et al.  1  avoids this triggering by considering each variable-value pair only once. we propose limited sac  lsac  which lies between restricted sac and sac. the idea is that if a variable-value pair  x a  is found arcinconsistent then  only the pairs involving neighbours of x as a variable are added in the outer-loop. our experience is that lsac is more effective than rsac.
table 1: comparison between sac  lsac  psac and plsac.
problemsaclsacpsacplsac11111111111뇸111111 q1

co-1-1
co-1-11111the algorithms sac  lsac  psac  probabilistic sac   and
plsac  probabilistic lsac  were applied to forced random problems  rlfap  modified rlfap  quasi-group with holes  queens-knights  attacking prime queen  job-shop instances  composed random problems. table 1 presents results for only a few instances of the above problems  due to space restriction. again  the value of threshold used for psc and prc is 1. in table 1 #rem denotes the number of removed values. the intention is not to test if the preprocessing by sac has any effect in the global cost of solving the problem  but to see if the same results can be achieved by doing considerably less computation by using probabilistic support inference. when the input problem is already singleton arc consistent  psac and plsac avoid most of the unnecessary work. for example  for job-shop instances js-1 and js-1  both psac and plsac spend at least 1 times less time than their counterparts. even when the input problem is not singleton arc consistent  probabilistic versions of the algorithms are as efficient as the original versions. for most of the problems  they remove exactly the same number of values as removed by sac and lsac  but consume significantly less time. for example  in case of attacking queen problems  all the algorithms remove the same number of values. however  psac and plsac are quicker by an order of at least 1.
모seeing the good performance of psac and plsac  the immediate question arises: can we afford to maintain them during search  so far sac has been used only as a preprocessor. maintaining sac can reduce the number of branches significantly but at the cost of much constraint propagation  which may consume a lot of time. maintaining it even for depth 1 table 1: comparison of mac  msac  mlsac against their probabilistic versions  t = 1  on structured problems.
 checks are in terms of 1s. 
problemmacmpacmsac mpsac mlsacmplsac#chks1111	11qwh-1time1111	11 easy #vn111	11#chks1111 11qwh-1time1111	11 hard #vn1111	11#chks1111 11qwh-1time1111	11 easy #vn	1 1 1	11	11#chks	1	1 1 1 1 1qcp-1time	1.1 1 1.1 1 1.1.1 easy #vn1 1 1 1 1 1 1within search has been found very expensivein  prosser et al.  1 . we investigate if psac and plsac can be maintained within search economically. table 1 shows the comparison of mac  mpac  msac  maintaining sac   mlsac  maintaining
lsac   mpsac  maintaining psac   and mplsac  maintaining plsac  on structured problems. mean results are shown only for quasigroup with holes  qwh  and quasi-completion problems  qcp  categorised as easy and hard. note that here easy does not mean easy in the real sense. the results are first of their kind and highlight the following points:  1  the probabilistic version of the algorithm is better than its corresponding original version   1  maintaining full or probabilistic  l sac reduces the branches of the search tree drastically   1  though mlsac and mplsac visit a few nodes more than msac and mpsac  their run-times are low   1  mplsac is the best in terms of checks and solution time.
in our experiments  mpsac/mplsac outperformed msac/
mlsac for almost all the problems which we have considered. but  when compared to mac and mpac  they were found to be expensive for most of the problems except for quasi-group problems. however  this observation is made only for threshold value 1. thorough testing remains to be done with different values of t.
1 conclusions and future work
this paper investigates the use of probabilistic approach to reduce ineffective constraint propagation. the central idea is to avoid the process of seeking a support when there is a high probability of its existence. inferring the existence of a support with a high probability allows an algorithm to save a lot of checks and time and slightly affects its ability to prune values. for example  by visiting a few nodes more than mac  mpac is able to outperform both mac and fc on a variety of different problems. similarly  enforcing probabilistic sac almost always enforces sac  but it requires significantly less time than sac. overall  experiments highlight the good performance of probabilistic support condition and probabilistic revision condition. we believe that the idea of probabilistic support inference deserves further investigation. the notions of psc and prc can be improved further by taking into account the semantics of the constraints.
