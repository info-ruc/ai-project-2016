
we present a formal framework for treating both incomplete information in the initial database and possible failures during an agent's execution of a course of actions. these two aspects of uncertainty are formalized by two different notions of probability. we introduce also a concept of expected probability  which is obtained by combining the two previous notions. expected probability accounts for the probability of a sentence on the hypothesis that the sequence of actions needed to make it true might have failed. expected probability leads to the possibility of comparing courses of actions and verifying which is more safe.
1	introduction
uncertainty  both about the domain in which a robot has to operate and about the effective sequence of actions it is executing  is one of the most crucial problem that cognitive robotics has to face in robot control. in this paper we present a formal framework for dealing with uncertainty both in the initial database and during the execution of a course of actions: actions can fail. the failure of an action can lead to exogenous events  e.g. if the agent fails to pick up a block then the exogenous event that the block falls to the floor can take place. the complete framework is designed for autonomous manipulators endowed with perception. perception  in fact  can be used to diagnose what really happened during a failed execution  a similar approach is taken in  . here  leaving perception aside  we shall concentrate on the different aspects concerning uncertainty:
1. incomplete initial information. the most prominent logical approaches to probability are  1; 1  and  1; 1; 1; 1; 1; 1 . we introduce in the situation calculus  1; 1  a formal theory  which turns out to be equivalent to the possible world approach  under certain restrictions  e.g.  . instead of possible worlds  we propose a suitable axiomatization inducing possible state descriptions  therefore relying on a classical semantics. probability of sentences is entailed by the axiomatization. we do not provide nested probabilities.
1. incomplete information for a course of actions  and exogenous events. a logical approach to failures has been presented in . in general  the problem of failures during the execution of plans is thoroughly treated in the planning community  see e.g.  1; 1 . to cope with the non deterministic effect of actions during the execution of a plan  say  we extend the classical language and axiomatization of the situation calculus with stochastic actions and events so that different possible runs  for a given sequence of actions  can be obtained. however we preserve the core ontology of the situation calculus in which actions are deterministic and the whole dynamic of change is idealized. in fact  each run of stochastic actions is obtained as an expansion of a fixed sequence of deterministic actions. an expansion is formed by considering both probability of success and failure for each action  and exogenous events. we make a closed world hypothesis about exogenous events: the exogenous events are all and only those fixed in the theory. probability of events is empirical  it accounts for the relative frequency of properties of events like the usual failures of an agent  due to its normal behavior or the circumstances under which it operates.
모the probability of a sentence depends only on the amount of information available in the initial database  i.e. with state descriptions. probability of actions  on the other hand  depends on properties of events  like a robot behavior. with the notion of expected probability we combine the two former notions  the logical and frequency ones  into a new probability that can account for the probability of a sentence after the execution of a course of actions  in the hypothesis that this sequence can fail in several different ways and exogenous actions can occur. this idea captures in some sense the dynamic execution under incomplete information   . an example is the probability that the screwdriver is on the floor  given that the action the robot has to execute is to pick it up  and its goal is holding it in its hand. expected probability  combining the two previous probabilities  gives a measure of safety of a sequence of actions chosen to achieve a goal. therefore it can be used to determine whether a course of actions is more safe than another one. the notion of safety can be suitably extended to treat decision theory in the situation calculus  see e.g.  1;
1  .
1	preliminaries
we consider the situation calculus  sc    1; 1   as the core of our logic and language  and we suitably extend it to include new sorts  new symbols for its alphabet and new axioms. we refer the interested reader to  for a full presentation of the core logic and language and  for more details related to the extensions concerning probability on events. the core consists of the three sorts: action  situation and object = worldentities 뫋 genericobjects. here the sort genericobject is a catch-all that can include any possible additional sorts needed in an application  like the reals or sets. we have extended the core with three new

sorts: outcome  stochasticaction  event. the sort event consists of two sub-sorts  event = condevent 뫋 seqevent. observe that the sorted domain d could be infinite or uncountable even if some of its subsets is finite. for example we are interested in imposing a finite domain for world entities  while we would require probabilities to range over the reals. we shall use the following alphabet  with subscripts and superscripts  to denote terms: a  for variables of sort action; s  for variables of sort situation; sta  for variables of sort stochastic action; e  for variables of sort event; w for variables of the sub-sort seqevent and u for variables of the sub-sort condevent; p  for variables of sort object denoting probabilities and x y z...  for variables of sort object  these might be further distinguished  e.g for terms of sort worldentity . the family of terms over sc and the set of formulae lsc over sc are inductively defined as usual. in particular a basic sentence is one formed only by fluents  e.g.  x y p x s1 뫸q x y s1    in which fluents take as argument only elements of sort worldentities  we . a sentence uniform in  is one in which no variable  free or quan-
tified  of sort situation is mentioned. e.g.  sp t s  is not uniform and	  is uniform in  =	1 .
omitted quantifiers are always universal quantifiers.
1	the basic theory of actions
the notions related to a basic theory of action with its set of foundational axioms and domain description axioms  are presented in detail in  and in . these axioms represent an agent's prior knowledge about its domain of application  and its behavior  and are denoted by the following:
ds1 뫋 dssa 뫋 duna 뫋 dap 뫋 .
here  ds1 is the initial database  describing what properties hold initially about the objects in the domain. dssa 뫋 dap specify  respectively  the successor state axioms and the action precondition axioms.  is the set of foundational axioms for situations  and duna ensures the uniqueness of each action. we present in the following a simple example to illustrate the basic ontology.
1	the basic ontology: a dispatching domain
an agent has to dispatch a string of bits from a source to a receiver and the information it has concerns some of the bits and the constrains that the string has to satisfy. the transmission can fail and thus an exogenous action that distorts the signal can take place.  a robotic oriented example is given in
 .
fluents	source x n s : at the source there is a bit x 뫍
{1} in position n in situation s. e.g. source 1 s1  뫇 source 1 s1  specifies that the string 1 is at the source in the initial situation s1.
receiver x n s : at the receiver there is a bit x 뫍 {1} in position n in situation s.
control and exogenous actions transmit n . this is a control action with the following effect: the n-th bit at the source is transmitted.
distort n . this is an exogenous action with the following effect: the complement of the n-th bit at the source is transmitted.
the initial database ds1 there are properties of the domain that the initial database has to represent. we list some of them  where x is complement of x:

모the first axiom says that no string of bits is at the receiver in s1. the second says that at the source there is always one fixed bit in position n  the last specifies that the length of the string at the source is a priori fixed. here for example we can fix  x source x 1 s1 . the following constrains to the strings in source are also in ds1.

c1. if a bit b is in position n   1  b is either in position n 1 or in position n   1:

c1. if a bit b is in position n = 1  b is in position n = 1: source x n s1  뫇 n = 1 뫸 source x n   1 s1 
for example: 1 is a legal string. 1 and 1 are not. finally some axioms will state that bits  which we denote by b are limited to 1 and 1  and positions  which we denote by n  constitute a finite set of natural number. both the sets are ordered and pairwise disjoint.
successor state axioms the successor state axiom for receiver is:

the initial database. observe that our approach is  however  possible just in case the domain of sort we has a fixed finite cardinality.
모we shall now give few details about the formalization. let ds1 be the initial database  which we assume is a finite set of basic sentences  uniform in s1. the set of basic sentences uniform in s1 is denoted  for short  by lwe.
모given ds1 we shall consider a set of state description axioms  sd . sd is defined by the following set:

and

 1 
here n is the cardinality of the domain of sort we  and q 뫍 i is an index which will be specified below  1 . 붯i s1   that appears in  1  is constructed as follows.
definition 1 let x be a set of variables. a function fx is a variable renaming isomorphism iff fx maps x onto x and is one-to-one. let a and b be two sets of m-tuple of variables from
fx induces an equivalence relation between sets of m-tuples. an equivalence class of m-tuples is:
example 1 and x1 is a variable renaming isomorphism  and a =fx b.
모let 뷋 be a basic atom uniform in s1  of the form f -x y  or  with -x possibly empty  y = s1 if it appears  terms of sort we. let m 뫍 { a | 뷋a 뫍 lwe a the arity
of 뷋}. let x = {x1 ... xn}:
c
 
bm   cm  only one representative is in{am}i뫍i
  is bijective and such that
m i뫍i
모the above definitions are used to form all the possible m-tuples out of n variables and ensure to gather only one representative for each equivalence class  under the renaming isomorphism. let us order all the atoms 뷋s  as defined above. let 뷋k be the k-th element in such ordering ord  with |ord| = 붸. 붯i s1  is defined as follows:

observe that  given n   1 the number of variables  and m   1 the arity of the tuples  and 뷋k 뫍 ord 
		 1 
is the number of renaming isomorphisms and i =
.
모to show that each 붯i s1  and  consequently each  is uniquely defined  many lemmata have to be used. moreover an equivalence relation  actually an isomorphism on structures  based only on the domain of sort we  and mirroring the renaming isomorphism  has to be defined. we skip all these details and leave only the main theorem.
 theorem 1 let m be a structure of lsc with domain of sort with cardinality n. there exists a unique  such that m |= pi .
모the above theorem tells us that each p  is a possible state description and  by the uniqueness  there is a one-to-one correspondence between the p s and the classes of structures of lsc  with domain of sort we of fixed cardinality n.
theorem 1 let 뷋 be a basic sentence  uniform in s1  such that sd뫋{뷋} is consistent. there is an ordering on the p s  such that:

to count probabilities we need to extend lsc with a number p1 ... pq of parameters.
lemma 1 let p1 ... pq be new parameters of the language  and let us add the following to ds1:
		 1 
then there is an ordering on the p s such that ds1 뫋 sd entails both:

모observe that the role of  1  is to assign to each model m of ds1  which is also a model of a uniquea value 1 for the associated pi. on the other if mi is not a model of ds1 the pi associated with its unique state description   will have everywhere value 1. we can  now  count the models of ds1 as follows  where 1m is a new parameter:

모we are  now  ready to introduce the probability of sentences based on the equivalence classes of models of ds1 뫋 sd. this can be done at the object level just by operating on the p 's. first we define:
definition 1 let ds1 be equivalent  for some ordering on the
p s  to:

it	follows 	by	lemma	1	and	definition	1 	that
	.
모the following definition will give us the probability of a sentence with respect to the state descriptions.
definition 1 for any basic formula 뷋  uniform in s1:

theorem 1 the following properties are satisfied for each basic sentence 뷋  either uniform in s1  or equivalent to a sentence uniform in s1  in all models of ds1 뫋 sd:

it follows that  if 뷍 is uniform in s1 then:
ds1 뫋 sd |= 뷍 iff prob 뷍  = 1.
모observe that differently from bacchus and halpern we do not allow any nesting of probabilities. in other words we do not admit sentences of the form: prob prob 뷋  뫟 1 . however  given the above limitations  and the strict constraint that the domain of sort world entities has a fixed  finite  cardinality  the above theorem shows that our logic is equivalent to halpern's logic  when to halpern's axiomatization the finiteness axiom fin is added.
example 1  continued  let dwe = {{1} {1 1}}. we assume that no information about the three bits in the source's string in s1 is given  but the string is three bits and it satisfies c1 and c1. the possible strings are: st1 = 1 st1 = 1 st1 = 1 st1 = 1
each string corresponds to a state description. e.g. 1 is captured by with the following 붯1:
here 뷌1 states the ordering defined respectively on bits and positions.can be defined analogously. ds1 뫖  where the pj are all the
other state descriptions admitted by the 1-structures of lwe.
hence. we want to evaluate the probability of the following sentence:
붱s s1  뫖 source x n s1  뫇 source x n +1 s1 

stating that each bit b is followed by b  which is not entailed by ds1. as  then
.
1	a model for failures
the nice behavior of the situation calculus as a logic of change  is that once we have suitably defined truth and probabilities in the initial database  everything about uniform sentences in    s1  i.e. about the future of the initial database  can be easily computed in ds1  using regression. regression  is a proof method that  given a sentence uniform in a situation  reduces such a sentence to an equivalent sentence in which the only situation mentioned is s1. regression amounts to the following equivalence  given a sentence 뷍  : d |=   뷍   뫖 r 뷍    
here r 뷍    is a sentence uniform in s1. now  by the properties of probability shown in theorem 1  regression allows us to compute the probability for any sentence uniform in .

figure 1: the tree tree do transmit 1  transmit 1  s1   . the dotted arrow means the execution of an exogenous action.
observe however that we do not have a method to compute the probability of any sentence  what so ever. for example we would not be able to compute the probability of a sentence of the form.
and since the definition of prob is based on state descriptions  which are closed sentences  we cannot either express sentences of the form:  xprob 뷋 x  .
모independently of probabilities  we have so far given an idealized representation of the world in which the effects of actions are exactly those intended  i.e. actions  even exogenous actions  are deterministic. however  due to various circumstances  e.g. noise in the channel   an agent might fail in its intended execution of an action and probabilities on sentences cannot capture this fact. to address this problem  we have extended the specification of an action theory with stochastic actions and events  therefore we introduce a complementary notion of probability as a relative frequency of events. we introduce in the sequel the functions  constants and relations of the extended lsc that we use to model stochastic actions and probability of events  a more detailed description is in .
outcome  e.g. v a  = 1.
a stochastic action  taking values in the outcome space outcome = {1}  where 1 means that a failed and 1 that it succeeded. given a sequence of deterministic actions  this sequence expands to a tree of events due to all the possible outcomes of each action. see figure 1.
모to model the tree and the probability distribution we introduce the following notion of event.
event..
|	:
and the special constant e1.	e.g.
 is a sequence of stochastic actions  i.e. is a term of sort seqevent  hence a term of sort event. an event e is either a sequence event w of sort seqevent or a conditional event u of sort condevent  according to the following definition: sequence event : 1. e1 : the empty event.
1.  the event obtained as a result of executing a with value v a  뫍 {1}  after the sequence event w. conditional event :
1.  the event obtained as a a result of executing a with value v a  뫍 {1}  given the sequence event w probability of events. 
object. e.g. p. probability on events  is a functional event fluent p which takes as arguments events and returns their probability: p maps events into  1 .
모finally the following functions and predicates are used for the theory of events.
measure. .	e.g. 뷃source bit pos e1  = 1. for each fluent p there is a belief measure	p.
 : seqevent 뫄 seqevent	.
	.	e.g.
.
 axioms for probability p we shall not treat here axioms for events  we refer the reader to . these axioms mirror the foundational axioms for situations  and for all events of sort sequence the analogous theorems as for situations  are derivable. the properties of p are:
1. p e  뫟 1  for any event e.
1. 1.
1..
we define the following function:

let 뷄 뫍 {1}. the following sets can be defined  with exact a predicate that sorts out the exogenous actions:

given the above axioms and the above functions  the following properties hold:
1. p sta|e  = p sta   e  p e 
.
1	treating preconditions for stochastic actions
when a stochastic action sta is taken into account we need to know what holds at a given event in order to decide if sta is executable. in fact  we shall use the conditional probabilities to state that  if the preconditions are not satisfied  then the probability of success of the action will be 1. we need  therefore  suitable action preconditions axioms and successor state axioms concerning the dynamic of stochastic actions. consider the dispatching domain and the control and exogenous actions given in paragraph 1: transmit and distort. the corresponding stochastic actions are 
and  where x is a random variable on the values 1 or 1. preconditions are defined as follows:

the conditional probabilities for non exogenous action are:
the preconditions for the success of an exogenous action like distort depends on the result of a previous action. therefore the probability of an exogenous actions is conditioned on the random effect of stochastic actions.
p

모모모모모모모모모모모모모모모모모모모모모모모 1  observe that  by the axioms for p  we get a conditional probability for failure  for each of the above stochastic actions:
p
successor state axioms for fluent measures. since in the core language of sc fluents can take as argument only sequences of deterministic actions  by a principle of preservation  we prefer to extend the language with event fluents  namely 뷃-fluents  which we call measures of fluents. for each fluent f -t s  we add a functional event fluent 뷃f -t e   with the same arity as f and such that all arguments which are not of sort situation are the same as f and the argument of sort situation is replaced by an argument of sort event. the value of a 뷃-fluent is given as follows:
뷃f -t e1  = prob f -t s1  
the successor state axioms for fluent measures capture the dynamic of the domain  in terms of measures  under the circumstances that stochastic actions occurred. these successor state axioms are strictly tied to the successor state axioms for fluents and they have to preserve the following property:
property 1 for any 뷃-fluent 뷃f -t e :
뷃f -t e  = prob r f -t trans e    
모according to the above property a simple method can transform a successor state axiom for a fluent into a successor state axiom for the corresponding 뷃-fluent. in particular  as a successor state axiom has the form f -x do a s   뫖 붱  then we get prob f -x do a s    = prob 붱 . by suitable manipulations we get the required successor state axioms.

1	expected probability and safety
we want to combine the logical probability on sentences and the event probability defined on stochastic actions to get a third probability  see theorem 1  that accounts for both incomplete information and possible failures of actions during execution. we are interested to know  given that g   is what an agent should achieve  what is the probability that g will hold at . we also want to know the probability that a sentence g will hold at   given the evidence that g was to be achieved at . to capture these ideas we introduce the notion of expected probability. let
i
definition 1  expected probability in s1  let 뷋 and 뷍 be sentences uniform in . the expected probability  in s1  of 뷋  given evidence 뷍 is:
p ei  
theorem 1 ep satisfies the properties of theorem 1.
the above defined expected probability allows us to compare two sequences of actions and decide which one is safer:
definition 1  safety  let 뷋 1  and 뷋 1  be two sentence uniform  respectively  in 1 and 1. let be a ordering relation:

example 1  continued  let 붱r   be the sentence  receiver x n   뫇 receiver -x n + 1    where  = do transmit 1  do transmit 1  s1  . we consider the stochastic tree expansion of . considering the set
  we get the following
possible executions of   see figure 1.
1 = do transmit 1  do transmit 1  s1   1 = do transmit 1  s1   1 = do transmit 1  s1  1 = do transmit 1  do distort 1  s1    1 = s1 1 = do distort 1  do transmit 1  s1   1 = do distort 1  s1   1 = do distort 1  s1  1 = do distort 1  do distort 1  s1  
모by regression  prob r 붱r 횰    = 1  for i 뫍 {1 1} and prob r 붱r 횰    = 1 for i = {1 1 1}.
considering the probability of events  from  1  and  1  we get:
p
p
	p e1  = 1	p e1  = 1 p e1  = 1
p e1  = 1 p e1  = 1 p e1  = 1
   p e1  = 1 p e1  = 1 p e1  = 1 by definition 1 we get ep 붱r    = 1.
1	discussion
the notion of probability that we are concerned with  comes from the tradition of the logical interpretations of probability   . the basic idea is that when assertions are not entailed it is always possible to conclude something about these assertions  a logical proximity as a degree of deducibility  treated as a probability on statements. the idea of attributing probabilities to logical statements was earlier introduced by keynes's and illustrated in . carnap  in  addresses a distinction between degree of confirmation and relative frequency. in  halpern proposes both a logic for statistical measure statements and probability statements. the former provides a probabilistic measure of events defined on a generic object:  the probability that a bird flies is 1 . the latter is about probabilities of logical assertions:  the probability that tweety flies is.. . following the tradition of probability logic  we have considered the carnaps'notion of degree of confirmation to declare a probability prob on sentences and the relative frequency idea to declare probability p on actions. bacchus and halpern in  1; 1  presents two systems for both the statistical and confirmative approaches. a unified theory is presented in . in particular  in  1; 1  axioms and rules of inference are presented and the logic is proved sound and complete  under given restrictions   w.r.t. a semantics based on possible worlds. differently from halpern's and bacchus one  our theory is based on a classical tarskian semantics. by suitably axiomatizing the initial database we get a semantical definition of probabilities analogous to the possible worlds approach. we introduce axioms for state description  typical extension formulas used in finite model theory  1; 1 . finally we have introduced a third probability  the expected probability  which can be used to compute the probability of a sentence given both incomplete knowledge and failures during the execution of a course of actions.
references
 f. bacchus. representing and reasoning with probabilistic knowledge. mit press  cambridge  mass.   1.
 f. bacchus  j. halpern  and h. levesque. reasoning about noisy sensors in the situation calculus. artificial intelligence  1-1  1.
 c. boutilier  r. reiter  m. soutchanski  and s. thrun. decision-theoretic  high level agent programming in the situation calculus. in aaai-1  pages 1. aaai press  1.
 r. carnap. logical foundations of probability. university of chicago press  chicago  1.
 r. fagin. probabilities on finite models. journal of symbolic logic  1 :1  1.
 r. fagin. finite-model theory: a personal perspective. theoretical computer science  1-1  1.
 r. fagin and j. halpern. reasoning about knowledge and probability. acm  1 :1  1.
 y. a. feldman and d. harel. a probabilistic dynamic logic. journal of computer and system sciences  1-1  1.
 a. finzi  f. pirri  m. pirrone  m. romano  and m. vaccaro. autonomous mobile manipulators managing perception and failures. in agents'1  the fifth international conference on autonomous agents  1.
 a. finzi  f. pirri  and r. reiter. open world planning in the situation calculus. in proceedings of aaai-1  seventeenth national conference on artificial intelligence  1.
 h. gaifman. concerning measures in first-order calculi. israel journal of mathematics  1-1  1.
 k. z. haigh and m. m. veloso. interleaving planning and robot execution for asynchronous user requests. autonomous robots  1 :1  1.
 j. halpern. an analysis of first-order logics of probability. artificial intelligence  1-1  1.
 j. m. keynes. a treatise on probability. macmillan  london  1.
 j. mccarthy and p. hayes. some philosophical problems from the standpoint of artificial intelligence. machine intelligence  1-1  1.
 d. mcdermott. probabilistic projection in planning. in spatial and temporal reasoning. dordrecht. dordrecht: kluwer academic  1.
 j. pinto  a. sernadas  c. sernadas  and p. mateus. nondeterminism and uncertainty in the situation calculus. in proceedings of the flairs'1 - the 1th international florida ai research symposium  pages 1. aaai press  1.
 f. pirri and r. reiter. some contributions to the metatheory of the situation calculus. acm  1 :1  1.
 d. poole. decision theory  the situation calculus  and conditional plans. linkoping electronic articles in computer and몮 information science  1   1.
 r. reiter. knowledge in action. mit press. to appear  www.utoronto.ca/ cogrobo  1.
 p. roeper and h. leblanc. probability theory and probability semantics. university of toronto press  1.
heterogeneity in the coevolved behaviors of mobile robots: the emergence of specialists
	mitchell a. potter  lisa a. meeden	and alan c. schultz
navy center for applied research in artificial intelligence
naval research laboratory  washington  dc 1 usa mpotter schultz  aic.nrl.navy.mil
computer science department  swarthmore college swarthmore  pa 1 usa meeden cs.swarthmore.eduabstract
many mobile robot tasks can be most efficiently solved when a group of robots is utilized. the type of organization  and the level of coordination and communication within a team of robots affects the type of tasks that can be solved. this paper examines the tradeoff of homogeneity versus heterogeneity in the control systems by allowinga team of robots to coevolve their high-levelcontrollers given different levelsof difficultyof the task. our hypothesis is that simply increasing the difficulty of a task is not enough to induce a team of robots to create specialists. the key factor is not difficulty per se  but the number of skill sets necessary to successfully solve the task. as the number of skills needed increases  the more beneficial and necessary heterogeneity becomes. we demonstrate this in the task domain of herding  where one or more robots must herd another robot into a confined space.
1	introduction
many mobile robot tasks can be more efficiently solved when a group of robots is utilized. some tasks cannot be solved at all without multiple robots. the type of organization  and the level of coordination and communication within a team of robots affects the type of tasks that can be solved.
모in swarm approaches   usually large  groups of robots execute the same simple strategies with no explicit communication. complex group behaviors emerge from the simple interactions among the robots. examples of this include flocking behaviors. in cooperative approaches   usually smaller  groups of robots can have different strategies  allowing some of the robots to become specialists in solving parts of the task-that is  robots can assume roles. recently  the term collaborative has been used to indicate cooperative approaches where the robots explicitly communicate their intent to one another. one important issue in multi-agent robotics is to understand when a particular approach is appropriate for a given task  that is to understandthe relative powerof each approach.
모this paper will examine the tradeoff of homogeneity versus heterogeneity in the control systems by allowing a team of robots to coevolve their high-level controllers given different levels of difficulty of the task. in the homogeneous case  we will restrict the robots to using the same control structure  i.e. only one high-level controller is evolved  which all robots will use. in the heterogeneous case  robots will be allowed to coevolve separate high-level controllers  thus enabling the emergence of specialists. our hypothesis is that simply increasing the difficulty of a task is not enough to induce a team of robots to create specialists. the key factor is not difficulty per se  but the number of skill sets necessary to successfully solve the task. as the number of skills needed increases  the more beneficial and necessary heterogeneity becomes.
모experiments were conducted within a simulation model of the task domain. the task chosen for these experiments is herding  where a group of robots must force another robot into a confined space. the robots are nomad 1s which are modeled using the teambots system  balch  1b . we show that simply increasing the difficulty of the task alone does not necessarily require heterogeneous control systems  but that introducing a predator into the environment induces the evolution of specialists for defending against the predator.
모in the next section we will describe related work. in section 1  we will describe the task domain and how the complexity of the task can affect coevolved behaviors. the use of a neural network as a high-level controller  and the evolutionary algorithm used for learning will be described in section 1. we will then describe the experimental methodology and the results in section 1  followed by our conclusions and a description of future work.
1	related work
evolution of robotic shepherding behaviors was first described in  schultz et al.  1   although only one shepherd was involved  and therefore multi-robot coordination was not required. much has been written about evolution  and coevolution  of robot behaviors  see  mataric and cliff  1; meeden and kumar  1  for an overview . several articles have attempted to lay out taxonomies and general issues in multi-robot coordination  dudek et al.  1; cao et al.  1 .
모a number of researchers have explored heterogeneity at the hardware level by equipping members of a robot team with different sets of sensors or effectors  cao et al.  1; parker  1 . other researchers have utilized teams of similar agents  and have instead explored heterogeneity at the behavior level  balch  1a; bongard  1; good  1 . our research takes the latter approach in that each herding agent has the same physical capabilities  but can develop unique control strategies through coevolution.
모it remains an open question as to what kinds of tasks warrant a heterogeneous approach. homogeneous teams have one clear advantage-there is built-in redundancy. if a team member fails for any reason  the rest of the team can still go on and be successful. however  as a task increases in difficulty  division of labor or specialization may become essential for success.
모in doing reinforcement learning studies  balch found that diversitywithin a team is not always desirable  balch  1a . certain kinds of tasks  such as foraging  were solved more easily with a homogeneous approach. balch speculated that any domain in which an individual agent could reasonably perform the task alone  is well suited for a homogeneous approach. other domains  such as soccer which seems to require a variety of agent types  were more easily solved with a heterogeneous approach.
모one result which is somewhat inconsistent with balch's conclusions  is luke's experiences developing a genetic programming based softbot soccer team for robocup1  luke  1 . the main goal of his work was to produce the best team possible in a limited amount of time. luke developed both homogeneous and heterogeneous teams. however  the heterogeneous teams had a larger search space and thus converged much more slowly. by competition time  the homogeneous teams had the advantage and were entered. luke predicts that given enough evolution time though  the heterogeneous teams might ultimately win out.
모balch has suggested that tasks that cannot be reasonably solved by a single agent should lead to heterogeneity. bongard focuses more on the domain rather than the agent and argues that decomposable domains should lend themselves more readily to heterogeneity  bongard  1 . like balch  he found that a homogeneous team was more successful at foraging than a heterogeneous one. yet  it is not clear that this supports his hypothesis. foraging can be easily decomposed as balch did in his hand-coded control cases. for example  the domain can be decomposed into separate territories or the targets can be decomposed into particular types.
모our hypothesis is that the need for heterogeneity depends on the number of skill sets required by the domain. the herding domain is a good environment for testing this hypothesis as well as those of balch and bongard. we can increase the difficulty of the task along a number of dimensions without requiring new skill sets. we can also increase the number of skill sets required by adding a predator to the environment.

figure 1: herding domain
1	task domain and complexity
the herding environment shown in figure 1 consists of a foot pasture that is fenced on three sides  with a smaller enclosed corral on the right. the herdingtask requires that a group of robots  the shepherds  force another robot  the sheep  into the corral. the sheep does not want to enter the corral  but instead wants to escape through the unfenced side of the pasture. to further complicate matters  in some experiments there is a predator robot  the fox  that attempts to kill the sheep by approaching within a certain distance.
모the sheep's behavior is a fixed strategy that causes the sheep to avoid the approach or contact of other robots and obstacles  with additional drives to avoid the corral and to seek escape through the unfenced side of the pasture. the fox's behavior is also fixed  and causes it to attempt to approach the sheep while avoiding the shepherds and other obstacles. if the fox is able to get within a certain distance of the sheep  the sheep dies and the trial is over. the strategies of the shepherds are implemented via high-level neural network controllers which are evolved as described in section 1.
모the complexityof this domain can be controlled along several dimensions. for example  the degree to which the sheep avoids the corral and seeks an escape from the fenced pasture can be increased  the predatory fox can be included  the radius around the sheep in which the fox kills the sheep can be increased  and we can increase the number of sheep. given the speed and turning rates of the sheep and shepherds  a single shepherd alone can force the sheep into the corral if the sheep only avoids obstacles  including the shepherds . however  if the sheep aggressivelyavoids the corral and seeks escape then a minimum of two shepherds are required to accomplish this task. by introducing a fox into the environment  a minimum of three shepherds is required.
모as a performance task  success is measured by the ability of the shepherds to get the sheep into the corral. the task has failed if the sheep escapes from the pasture  is killed by the fox  or if a time limit is exceeded. the learning task is for the shepherds to evolve neural controllers that allow them to succeed in the performance task. the fitness measure as used by the evolutionary learning method is given in section 1.

figure 1: derivation of sensors used by the neural controller
1	evolution of neural controllers
the high-level controller is implemented with a feed-forward neural network with one hidden layer. the weights of the neural network are evolved using a particular evolutionary algorithm known as an evolution strategy  rechenberg  1 . as will be seen in a moment  the strategy being evolved represents a high-level behavior  specifically  the neural network generates a goal point at each time step to which the robot will navigate. the lower-level behaviors such as collision avoidance and local navigation are built into the behavior and are not learned.
1	sensors and actions
each shepherd is controlled by a neural network that maps its current sensors to a new position to be obtained by the shepherd. this mapping occurs at a 1 hz rate. each shepherd has vision sensors that represent the range and bearing to other agents in the environment. in particular  a shepherd can detect the sheep  the closest other shepherd  assuming there is at least one additional shepherd in the environment  and the fox  if present.
모these shepherd-based sensor values are translated to ranges and bearing egocentric to the sheep as illustrated in figure 1. the bearings are relative to the sheep's angle to the corral. specifically  a polar coordinate system is used in which the pole is centered on the sheep and the polar axis passes through a point at the center of the opening of the corral. all angular measurements are relative to this polar axis. an agent's position is then defined as   that is  the range is the length of the radius vector from the sheep to the agent  and the bearing is the supplement of the polar angle of the radius vector. the supplement of the polar angle is used so that if an agent is directly behind the sheep with respect to the corral  its bearing will be 1 degrees.
the input to the neural controller includes the following:
1. shepherd b: the bearing from the sheep to the shepherd under control. present in all experiments.
1. shepherd r: the range from sheep to the shepherd under control. present in all experiments.
1. othershep b: the bearing from the the sheep to the closest other shepherd. present in all experiments with two or more shepherds.
1. othershep r: the range from the sheep to the closest other shepherd. present in all experiments with two or more shepherds.
1. fox b: the bearing from the sheep to the fox. present in experiments with a predator.
1. fox r: the range from the sheep to the fox. present in experiments with a predator.
모in addition to the vision sensors used by the neural controller  the shepherd also has sonar sensors which are used by the fixed  lower-level behaviors to avoid collisions with other objects  although the distance in which the shepherd will avoid the sheep is less than the distance at which the sheep will avoid the shepherd  allowing the shepherd to be able to herd the sheep.
모the output of the neural controller is the range and bearing to the new position to which the robot is to navigate in the same coordinate system as described above. these values are translated to a coordinate system egocentric to the shepherd under control  and input into a motor schema  see  arkin  1   to produce a linear attraction to the target position. this motor schema is combined with motor schema for obstacle avoidance and stochastic noise into an assemblage which controls the robot via turn and translation rate commands.
1	neural network controller
we use a simple two-layer feed-forward neural network topology as shown in figure 1. nodes are implemented using a standard sigmoid centered at 1 as follows:
		 1 
 1 
where is the output of node   and is the weight on the connection from node to node . all input nodes have weighted connections to all hidden nodes  and all hidden nodes have weighted connections to all output nodes. the network shown is the most complex case  which is used in the experiments with two or more shepherds and a predator. in the experiments with two shepherds and no predator  the number of input nodes is reduced to 1  and in the experiment with one shepherd and no predator  the network is further reduced to 1 input nodes and 1 hidden nodes.
모the network accepts real-valued inputs corresponding to the sensors described in the previous section. an additional input is clamped to the value 1 in order to provide a learnable bias for each node in the hidden and output layers. the target range output is converted to a value between 1 and 1 units in simulation  which corresponds to the full 1 foot width of the pasture in the real world  and the target bearing output is converted to a value in the range .

figure 1: neural network used as high-level controller
1	evolution of controllers
the evolutionary algorithm we use to evolve the connection weights for the neural controllers is a evolution strategy  es  as described by ba몮ck and schwefel   with and . each individual consists of a realvalued vector of connection weights and a companion vector of standard deviations used by the mutation operator as described below. this class of evolutionary algorithm was introduced in germany by rechenberg  for numerical optimization  and variants such as the -es  originally developed by schwefel   are a good choice when the problem solution is naturally represented as a vector of realvalued numbers  as is the case when evolving neural network connection weights.
모a -es begins with a population of 1 individuals  and generates 1 children by selecting uniformly from this populationand mutating each child. the 1 children and their 1 parents are then evaluated by applying each of them in turn to the target problem  and the 1 individuals with the highest fitness become the next generation of parents. mutation of an individual with genes and a companion vector of standard deviations consists of tweaking each gene   for   according to a normal distribution with mean zero and standard deviation as follows:
 1 
furthermore  the standard deviations are themselves adapted as follows:
 1 
where
		 1 
		 1 
the elements of are initialized to 1 and are restricted to the range . in practice  the standard deviations approach the lower limit of this range over time  which has an effect much like that of simulated annealing.
모in the experiments where heterogeneous control systems are evolved  we use the architecture for coevolution developed by potter and de jong . this architecture models an ecosystem consisting of two or more species. as in nature  the species are genetically isolated-meaning that individuals only mate with other members of their species. mating restrictions are enforced simply by evolving the species in separate populations. the species interact within the herding domain as described below and have a cooperative relationship.
모to evaluate an individual from one of the coevolving species given heterogeneous control systems  we construct a neural control system using the individual's genes as connection weights  and assign the resulting control system to one of the shepherds. we then select the current best individual from each of the other species and similarly construct neural control systems from them for assignment to the other shepherds. the shepherds are then set to work on the herding task. alternatively  we could organize the best shepherds into multiagent squads and assign each squad to a different neural control system. in the experiments where purely homogeneous control systems are evolved  the ecosystem consists of only one species. to evaluate one of these individuals  a neural control system is constructed from that individual's genes and assigned to all the shepherds  that is  each shepherd will be controlled by an identical neural network.
모in this current study  evaluations are done in simulation. each evaluation consists of 1 trials in which the shepherds herd the sheep until it is corralled  killed by the fox  escapes from the pasture  or 1 simulated minutes have expired. the cumulative distance of the sheep from the corral is measured at the rate of 1 hz throughout the trial. if the trial ends early due to the sheep escaping or being killed  we continue to accumulate the full pasture-width distance until the clock expires. the fitness of an individual is taken to be the final cumulative distance averaged over the 1 trials. the worst possible fitness is 1  which would result if the sheep immediately initiated a turn towards the left side of the pasture and escaped without any interference from the shepherds. the best possible fitness is 1  which would result if the sheep set a course directly towards the corral at maximum speed. the es will seek to minimize this measure.
1	results
in order to test our hypothesis  we vary the complexity of the task in several dimensions using both homogeneous and heterogeneous multi-agent approaches.
모we begin with the simplest case of one shepherd herding one passive sheep that just avoids obstacles. this is essentially a reimplementation  in simulation  of earlier work done by schultz et al. . we repeat this earlier experiment to validate the design of our neural controller and evolutionary learning method. the results are shown in figure 1 along with results from a second experiment in which the sheep

figure 1: one shepherd herding one sheep
actively seeks escape from the pasture and avoids the corral. the curves in the graph represent the mean fitness of the best individual seen so far averaged over 1 separate runs. overlaid on the curves at increments of 1 generations  1 evaluations  are 1-percent confidence intervals on the mean. although these control systems were evolved for 1 generations  1 evaluations   a controller with near optimal behavior on this simple task was easily evolved in as little as 1 generations. we also observed simulated robots solving this task using the best neural control system from the final generation of evolution  and it is clear that they are exhibiting behavior very much like the behavior evolved in the earlier work by schultz et al.. specifically  the robot positions itself directly behind the sheep with respect to the corral  and herds the sheep by moving towards it and triggering its obstacle avoidance behavior. the shepherd makes subtle swings from left to right to counter irregularities in the movement of the sheep. in contrast  the more complex case involving the sheep seeking freedom  hereby referred to as the active sheep  does not appear solvable with a single shepherd  as indicated by the learning curve flattening out at a very poor level of fitness.
모the result of adding a second shepherd to the more difficult task of herding an active is shown in figure 1. we compare both the evolution of two shepherds using homogeneous control systems  and two shepherds using heterogeneous control systems. although it took slightly fewer evaluations to evolve good homogeneous control systems  adequate behavior was also evolved in the case of heterogeneous control. due to the significantly smaller search space of homogeneous controllers  it is not surprising that when good homogeneous solutions exist it is easier to find them. when observing robots solving this task using the best neural control systems from the final generation of evolution  we see that in both cases  heterogeneous and homogeneous  the pair of robots assume positions behind the sheep  but slightly to the left and right. this counters the strong tendency of the sheep to slip by the shepherds and escape from the pasture. this is clearly a cooperative approach on the part of the shepherds  and it shows

figure 1: two shepherds herding one sheep

figure 1: three shepherds herding one sheep with fox
the need for cooperation is not alone sufficient to warrant the use of heterogeneous control.
controlshepherdssheepfoxfitnesssuccess ratiomeanminimummaximumhomogeneous1passiveno11111homogeneous1activeno11111homogeneous1activeno11111heterogeneous1activeno11111homogeneous1activeyes11111heterogeneous1activeyes11111table 1: comparison of the mean fitness and success rate of the best individual from each experiment  evaluated over an모finally  we add a predator to the environment  in the form of a fox that seeks to kill the sheep. now the shepherds require two skills-the ability to herd the sheep and the ability to keep the fox a safe distance away from the sheep. to encourage specialization  we initially position two shepherds behind the sheep  and a third shepherd is positioned between the sheep and the fox. as before  we compare the evolution of shepherds using homogeneous control systems with the evolution of shepherds using heterogeneous control systems. in the case of heterogeneous control  the two shepherds positioned behind the sheep are controlled by one neural network and the shepherd positioned closer to the fox is controlled by a second neural network. the results from this experiment are shown in figure 1. this task is much more difficult than the previous ones  so we evolved these control systems for 1 generations  1 evaluations . the graph clearly shows the superiority of heterogeneous control on this task. additional 1 trials
모when we observe the heterogeneous control systems performing this task  we see that the shepherd that is initially positioned between the sheep and fox does indeed exhibit specialized blocking behavior  while learning to herd as well. it begins by moving towards the sheep  keeping its body between the sheep and fox. when it nears the sheep  it sometimes turns to face the fox and performs a quick deflection maneuver before it joins the other two shepherds begins herding. however  most of the time its blocking behavior is more subtle. close observation reveals that the blocking shepherd maintains slightly more distance between itself and the sheep than the other two shepherds  which enables it to keep the fox at a safe distance from the sheep while still providing some help in herding. without having to concern themselves as much with the fox  the other two shepherds are able to herd the sheep more directly towards the corral. the homogeneous control systems rely much more on herding the sheep away from the fox  which is not as effective as blocking because it gives the sheep more opportunities to escape from the pasture.
모it should be noted that there is a difference in complexity between the heterogeneous and homogeneous control systems. since the heterogeneous control system utilizes two separate neural networks-one for the pair of shepherds initially positioned behind the sheep  and one for the shepherd positioned between the sheep and fox-a total of 1 connection weights are being evolved  while only 1 connection weights are evolved for the single-network homogeneous control system. to verify that the observed difference in performance is not simply due to this difference in complexity  we evolved a homogeneous control system with a complexity on the order of the heterogeneous system by increasing the number of hidden units to 1  which produced a neural network with 1 connection weights. as expected  this homogeneous control system performed much more poorly than the homogeneous control system with only 1 hidden units. specifically  averaged over 1 runs to 1 generations  the final mean fitness of the more complex homogeneous control system was 1  compared with an average fitness of 1 for the simpler homogeneous control system.
모the results from the previous three graphs are summarized and further supported by table 1. here we take the single best individual from the 1 runs of each experiment and apply it to the herding task for an additional 1 trials. we report the mean fitness  along with 1-percent confidence intervals on this mean  the maximum and minimum fitness achieved  and the success ratio  that is  the percentage of trials in which the sheep was actually corralled. the table reinforces our earlier observation that a single shepherd is not capable of herding an active sheep into the corral. however  two cooperating shepherds are sufficient to accomplish this mission. heterogeneous control systems are not an advantage here  in fact  they perform slightly worse  although a t-test on the means of the two-shepherd homogeneous and heterogeneous trials produced a p-value of 1  indicating a lack of statistical significance in their difference. only when the task requires multiple skills  e.g.  herding the sheep and blocking the predator  does heterogeneous control perform better than homogeneous control  as indicated by the trials with three shepherds. a t-test on the means of the three-shepherd trials produced a p-value of   clearly showing a statistically significant advantage to using heterogeneous control.
1	conclusion
in this paper  we have tried to demonstrate that simply increasing the difficulty of a task is not enough to induce a team of robots to create specialists. the key factor is not difficulty per se  but the number of skill sets necessary to successfully solve the task. as the number of skills needed increases- in this study by adding the response to a predator-the more beneficial and necessary heterogeneity becomes.
모although heterogeneous control systems can promote better solutions for many tasks  there is a trade off. learning  coevolving  a team of homogeneous agents can take much less time  since each evaluation of an individual in the population goes towards all individuals' progress and the search space is smaller. in a heterogeneous group  the available cpu time during evolution must be divided among the different skill sets.
모ongoing experiments are attempting to more generally determine the properties that dictate the type of approach that is appropriate. in addition  results will be duplicated on the physical nomad 1 robots to show that the simulation results hold on the actual robots.
acknowledgments
this work was supported by the office of naval research under work request n1wx1.
references
 arkin  1  ronald c. arkin. motor schema-based mobile robot navigation. the international journal of robotics research  1 :1  1.
 ba몮ck and schwefel  1  thomas ba몮ck and hans-paul schwefel. an overview of evolutionary algorithms for parameter optimization. evolutionary computation  1 :1- 1  1.
 balch  1a  tuckerbalch. behavioraldiversity in learning robot teams. phd thesis  georgiainstitute of technology  1.
 balch  1b  tucker balch. integrating robotics research with javabots. in working notes of the aaai-1 spring symposium  stanford  ca  1.
 bongard  1  josh c. bongard. the legion system: a novel approach to evolving heterogeneity for collective problem solving. in genetic programming: third european conference  pages 1. springer-verlag  1.
 cao et al.  1  y. u. cao  a. s. fukunaga  and a. b. kahng. cooperative mobile robotics: antecedents and directions. autonomous robots  1-1  1.
 dudek et al.  1  g. dudek  m. jenkin  e. milios  and d. wilkes. a taxonomy for swarm robots. in ieee/rsj international conference on intelligent robots and systems  pages 1  1.
 good  1  benjamin mcgee good. evolving multi-agent systems: comparing existing approaches and suggesting new directions. master's thesis  university of sussex  1.
 luke  1  sean luke. genetic programming produced competitive soccer softbot teams for robocup1. in john koza  editor  proceedings of the third annual genetic programming conference  pages 1. morgan kaufmann  1.
 mataric and cliff  1  m. mataric and d. cliff. challenges in evolving controllersfor physical robots. robotics and autonomous systems  1-1  1.
 meeden and kumar  1  lisa a. meeden and deepak kumar. trends in evolutionary robotics. in l.c. jain and t. fukuda  editors  soft computing for intelligent robotic systems  pages 1. physica-verlag  new york  ny  1.
 parker  1  l. e. parker. adaptive heterogeneous multirobot teams. neurocomputing  1-1  1.
 potter and de jong  1  mitchell a. potter and kenneth a. de jong. cooperative coevolution: an architecture for evolving coadapted subcomponents. evolutionary computation  1 :1  1.
 rechenberg  1  ingo rechenberg. cybernetic solution path of an experimental problem. library translation 1  august 1. farnborough hants: royal aircraft establishment. english translation of lecture given at the annual conference of the wglr  berlin  1.
 schultz et al.  1  a. c. schultz  j. j. grefenstette  and w. adams. robo-shepherd: learning complex robotic behaviors. in m. jamshidi  f. pin  and p. dauchez  editors  robotics and manufacturing: recent trends in research and applications  volume 1  pages 1. asme press  1.
 schwefel  1  h.-p. schwefel. numerical optimizaion of computer models. chichester: wiley  1.

agent-based control for object manipulation with modular self-reconfigurable robots
jeremy kubica and arancha casal and tad hogg
xerox palo alto research center
palo alto  ca 1abstract
we demonstrate multiagent control of modular selfreconfigurable  msr  robots for object manipulation tasks and show how it provides a useful programming abstraction. such robots consist of many modules that can move relative to each other and change their connectivity  thereby changing the robot's overall shape to suit different tasks. we illustrate this approach through simulation experiments of the telecube msr robot system.
1	introduction
modular self-reconfigurable  msr  robots  1  1  1  1  1  1  1  1  consist of many identical modules that can attach and detach from one another to change their overall connectivity. each module has processing  sensing and actuation capabilities. the main characteristic of msr robots is their ability to change shape to suit their task. while such shapes are unlikely to be as effective at particular tasks as special purpose tools  e.g.  the modules are unlikely to have the same strength as  say  a hammer   they provide adaptability for a wide range of tasks in unpredictable environments. for instance  changing shape can provide a variety of gaits for locomotion  e.g.  wheels for flat surfaces  snakes for small tunnels  etc. robot shapes matching object contours can also help provide precise manipulation.
모beyond the challenge of building many such modules  their tight physical interactions and many degrees of freedom pose a difficult control problem. moreover  modules constructed of micromachines  mems   or even smaller devices  1  1  1  are likely to have limited capabilities and be prone to some failures or incorrect assembly. thus the robust functioning of the robot requires a control architecture that tolerates such limitations  as has also been proposed for computing with molecular-scale devices .
모one control approach would be to precisely specify the desired locations of all the modules  and perform a combinatorial search to find appropriate module motions according to some criterion  such as minimizing moves or power consumption. with many modules  such searches are generally intractable  but can be solved approximately using heuristics  1  1  1  1  1  1  1 . even so  a high-levelcontroller often lacks enough information on the modules' statusor the task environment to define a precise shape  e.g.  when grasping an object whose detailed size or shape is unknown.
모instead  a structure with the general properties required for the task is often sufficient. when the properties are determined mainly by the local environment for each module  agent-based local control often finds a suitable shape without any need to precisely specify each module's position . a multiagent approach to general behaviors  such as locomotion and object manipulation  simplifies the design of higher-level control by presenting a useful programming abstraction to a centralized planner or within levels of a sequence of increasingly complex behaviors . that is  higher level control programs can determine the choice of local rules for the agents without detailed concern of the particular motion primitives of the modules or situation of each agent individually. for example  by switching among a set of available behaviors or modifying parameters used by the agents' rules.
모agent-based architectures readily match control to physical phenomena dominant at the different scales relevant to msr robots with many modules. for example  micromachined robots  are dominated by friction and other surface forces rather than gravity. even smaller structures  are subject to randomly fluctuating forces  i.e.  brownian motion. biological structures provide numerous other examples . in such cases  different agent types could be responsible for individual modules  small groups of modules and so on  forming a hierarchical or multihierarchical correspondence between the robot's physical structures and its environment  task and the controlling software .
모in related work  multiagent control helps teams of robots cooperating to achieve a common task  1  1  1  1  1 . these teams usually consist of independently mobile robots that are not physically connected and have little or infrequent physical contact. in most current modular robot systems the modules remain attached to one another forming a single connected whole  and giving rise to a number of tight physical motion constraints that do not apply to teams of separate robots. on the other hand  the physical contact between modules allows them to locate their neighbors without complex sensory processing as would be required  for example  to visually identify a physically disconnected member of a team. hence the techniques for coordinating teams address somewhat different requirements than those of msr robots.
the key issue of relating local rules to global behavior also

figure 1: the telecube hardware: two modules shown with arms retracted and expanded. the current modules are about 1cm in diameter when fully contracted.
arises in artificial life  which concentrates on how complex natural organisms achieve sophisticated crowd behaviors or deal with abstract agents that currently can not be physically constructed  1  1  1 . alternatively  biological and chemical techniques  1  1  give complex shapes from local behaviors  but can not yet produce programmable robots.
모thus while multiagent systems have been applied to several related tasks  identifying appropriate agent behaviors for msr robots remains an open problem. in the remainder of this paper  we describe the telecube modular robot and simple agent behaviors that provide basic object manipulation capabilities. these agent behaviors  in turn  abstract away the detailed nature of the module motion capabilities  providing a simpler programming model for higher level control.
1	the telecube modular robot
this section describes the modular robot we used  its simulator and the agent-based control technique.
1	hardware
telecube is a new msr robot where each module is a cube that can prismatically extend each of its six faces independently up to a factor of two times its fully retracted configuration  fig. 1 . a 1d analogue was previously developed at dartmouth . each module can communicate with its immediate neighbors in all six directions. when a module extends an arm in contact with an object  it can exert a force on the object. this design allows modules bound inside a group to move  so telecube can perform tasks inside the aggregate  such as internal object manipulation and density changes.
1	simulator
to examine behavior with more modules than currently feasible to construct  we simulated the telecube system. the simulator  written in java  accounted for the motions of the modules in three dimensions and typically ran with hundreds of modules. the simulator performs a series of steps. for each step  a random ordering of the modules is selected  thereby simulating asynchronous module actions. using this ordering  each module is given a  turn  in which to decide its behavior and perform any motion it selects.
모for vertical motion  the simulator included the effects of gravity  as appropriate for relatively large scale objects such as the current telecube modules. for smaller scale modules  viscous and frictional forces will become more important and could be included instead of gravity.
모the simulator makes several simplifying assumptions. first  the module arms have only two states: fully expanded or contracted. second  the arms are infinitely rigid. both of these assumptions simplify alignment of modules after a move. in practice  such alignment can be done with lowlevel feedback control or engineering the connectors on the module arms to guide slightly misaligned parts into alignment. lastly  the simulator assumed enough friction between the module and the floor to eliminate any sliding. we use a motion primitive to ensure correct module movement by including required low-levelactions such as disconnection from modules perpendicular to the motion direction  connection with any new neighbors and possible arm contraction while a neighbor extends its arm . a global connectivity check insures disconnecting from a neighbor does not separate the ensemble. this check may be easily preformed in hardware  and was simulated with a depth-first search of the modules.
1	agent-based control
our approach views each module as an independent agent executing a simple program  consisting of a finite-state machine  fsm . this approach uses the distributed  homogenous and networked nature of msr robots and scales well as the number of modules increases. there is no central control processor or designated leader module. global motion results  or  emerges   from purely local control rules executed at every module and through communication between neighboring modules alone. these behaviors can then be turned on and off by a higher-level system to give overall control for a sequence of tasks.
모the fsm control consists of simple if-then rules which may act probabilistically. modules switch among  states  that determine their behaviors and communicate by sending and receiving  messages . randomization helps prevent modules from becoming indefinitely stuck in unproductive configurations. it also prevents all modules from moving at once  potentially disconnecting the structure or leading to oscillations due to synchronous activity in which modules respond based on a system state that no longer holds if too many neighbors move simultaneously .
모messages are the means of global communication among modules. messages are propagated through the system in a distributed breadth-first fashion and can be modified as they move from one module to another. a  scent  is a message

figure 1: message passing from a column module. help messages are also sent in the other two directions: into and out of the page.
consisting of a numerical value that is slightly adjusted up or down as it passes through the modules . propagating scents form global gradients that can guide module motions. for instance  a scent indicating the distance to some location  e.g.  a module under high stress  could consist of a single integer value a module sends to its neighbors. a module increments the minimum value of all such messages it receives and sends the new value to its neighbors. using the minimum function to combine values from neighbors allows the scent to provide a useful gradient in spite of many cycles in the graph of links among the modules. these messages can have a maximum number of iterations to prevent them passing among modules indefinitely. once these messages have passed through a region of modules  the difference in recorded values between one module and a neighbor is a local scent gradient.
모biological concepts  such as scents and hormones  have been applied to msr robots for reconfiguration  grouping  synchronization and locomotion  1  1  1 . extending these concepts to consider gradients with telecube's ability to move modules within a structure allows object manipulation as described below.
모we assume the modules are small compared to the size of objects in their environment  allowing many modules to act on them at once. furthermore  we take module motions to be fast compared to the other object speeds  so numerous module updates take place with little change in the objects' positions. since modules move slower than they communicate  the response to messages does not introduce oscillating behavior  or irregular growth  and the intermediate configurations are well-balanced at all times.
1	results
this section presents the rules and resulting behaviors for two tasks involved in object manipulation.
1	growing toward an object
our first task is a dynamic supporting structure in which modules under pressure signal others to add their support  reducing the weight pressure  i.e.  force per supporting module  and providing more uniform support for the object. this application requires a pressure sensor in each module. control is handled through four different states: relaxed  column  expanding  and reset and three different messages: stimulus  help  and reset.

figure 1: growing up toward an arbitrary-shaped object. this example required 1 simulation steps.
모all modules begin in a relaxed state. when a module receives an external force stimulus  it sends a stimulus message to the modules along the direction of the force  one of the module's six faces . upon receiving the message the module transitions to the column state and effectively forms a rigid support column. this column sends help messages to neighboring modules  which ask them to expand toward the object and provide support. the remainder of this section provides details of the behaviors.
behavior for one turn in the relaxed state is:
 if a stimulus message is received  transition to column state and end turn.
 if a help message is received  record the direction from which the message came  the  help direction    transition to expanding state  and end turn.
 if contact with an object is detected  transition to column state and end turn.
 otherwise  when no message is received  adjust module density in all six directions as follows: for each direction  if the module is farther from or closer to a neighbor than one fully extended arm length  move towards or away from the neighbor  respectively  with a probability of 1%. this motion tends to restore a uniform density.
모modules in the expanding state move in the direction where additional support is required  the  help direction  . however  to insure that the bottom level remains connected  the last module in each column  as determined by the direction of the original force stimulus  does not move or disconnect from its neighbors. behavior of the expanding state is:  if a help message is received  update help direction.
 if a stimulus message is received  transition to column state and end turn.
 if the force stimulus is removed  transition to reset state and end turn.
 if contact with an object is detected  transition to column state and end turn.
 otherwise  send help messages in all six directions and  with 1% probability  move in the direction of required help  unless the opposite direction has no neighbor.
모modules in the column state remain rigid and stationary  providing support for an object. they emit help and stimulus scents to the appropriate neighbors  effectively calling for help and telling the others in the column to hold fast. behavior of the column state is:
 if the force stimulus is removed  transition to reset state and end turn.
 send stimulus messages to the modules above and below as determined by the direction of stimulus.
 send help messages to modules perpendicular to the direction of stimulus  as shown in fig. 1.
모the reset state is used when the object is removed so modules return to their original configuration. a module in the reset state sends the reset message to all neighbors in the direction along which the force previously acted and transitions to the relaxed state.
1	manipulating an object
the telecube architecture allows manipulating objects inside a group of modules. this can be accomplished by growing around a supported object or  more generally  opening gaps within the structure and pushing an object into them.
모internal object manipulation requires translational and rotational motions. in both cases  a module that is not in direct contact with an object moves based on the gradient of messages it receives from its neighbors  determined by comparing the relative numerical values of the  scent  messages. namely  if possible  a module will move in the direction of the positive gradient. if the module does not sense a gradient  it will try to move so as to restore an  optimal  density of modules in its neighborhood. this means moving away from a neighbor that is too close or toward a distant neighbor.
모internal manipulation uses shell and tissue states  and one stimulus message propagated among neighbors to form a gradient scent. the process starts with a broadcast of the desired

figure 1: module motions to move  left  and rotate  right  an object in 1d. only a few of the modules surrounding the object are shown.
movement direction. modules on the outside of the structure enter the shell state  and do not move under the influence of gradients or density adjustments. thus  these modules form a rigid support structure off which inside modules can push  i.e.  msr robots can not only form tools shaped to their task  but can also provide dynamic support structures for using those tools. interior modules enter the tissue state.
모translational 1d motion uses a simple stimulus-based response. given the desired direction of motion  modules in contact with the object use the following rules:
 if a module is in front of the object  i.e.  touching it with an arm extending from the module in a direction going against the desired direction of object movement   it releases a negative stimulus message and then tries to move away from the object. if possible  it moves in the direction opposite to the arm touching the object. if that direction is blocked by other modules  it tries to move instead perpendicular to that direction. either motion tends to move the module out of the object's way. furthermore  the negative stimulus message creates a scent gradient among neighbors causing them to also move away and make room for the module to move.
 if a module is behind the object  i.e.  touching it with an arm in a direction aligned with the desired direction of object movement   it releases a positive stimulus and tries to move toward and push the object. if the module has more than one arm in contact with the object  this rule applies to the arm most closely aligned with the desired direction of motion.
these behaviors are illustrated in fig. 1 left  and result in modules pushing the object in the desired direction.
모rotation requires the approximate location of the object's center to allow relating individual module forces to the overall torque imposed on the object. estimating this location via a distributed local algorithm can be difficult if the modules are much smaller than the object and  on the scale of the modules  the object is not smooth. instead  we assume the higher level controller broadcasts the approximate location of the object's center. another possibility is to use open-loop force fields to position the object in a known starting configuration  as demonstrated on a micromachined surface . in either case  for the behavior described here we suppose knowledge of the approximate center. that is  any error in the center's specified location is small compared to the object's size  but not necessarily small compared to the size of the modules. the modules act in the same way as the translation modules  except they move away from or towards the object if they are in front of a corner that should be turned

figure 1: internally manipulating an object. the modules form a  structure  and are shown simply as points to allow seeing the internally manipulated object. the full sequence  consisting of 1 simulation steps  shows the object gradually shifted from its initial to final positions in response to instructions to move the object in an  l  shaped path. the figure shows steps 1 and 1.
towards or away from them  respectively  as determined by their relative location to the specified center and the desired direction of rotation. again  they release positive and negative stimulus messages when moving towards and away from the object  respectively  and push the object when they move towards it. this behvaior  shown in fig. 1 right   gives 1d rotation in a plane.
모fig. 1 includes both types of motion. first the modules are told to move the object in one direction  then to rotate it  and finally move in the perpendicular direction. these instructions cause modules to switch between translation and rotation rules  giving desired motions without the higher-level control specifying individual module motions. we found the local rules could produce a variety of trajectories. the combination of translation and rotation rules allows achieving arbitrary planar position and orientation of the object.
모modules move the object by exerting forces on it. they could also exert compressive forces on the object by simply replacing the negative stimulus messages with positive ones so modules on all sides move toward the object. this behavior could be used to push parts together. shear forces could arise from tangential motion  provided friction between the arms of the modules and the object was sufficient. the modules then can be viewed as a material surrounding the object with forces under programmed control  changing from rigid support to fluid-like flow as needed. this behavior is a useful programming astraction for higher levels of control  similar to programmable force fields in two dimensions . by giving different instructions to different regions  a higher-level control could use the modules to move multiple objects along different paths or  through feedback based on the position of the objects  rather than details of the much larger number of modules   bring objects together.
1	discussion
this paper described local rules for two object-handling behaviors for telecube robots  dynamic support and internal manipulation. local rules can also reconfigure them for locomotion and navigation .
모our results extend prior distributed control  of proteo   another msr robot whose motions differ in two important ways from telecube. first  proteo modules move independently. by contrast  a telecube module usually requires cooperative actions from neighbors to push or pull it  as shown in fig. 1. second  proteo modules are rigid and can only move on the external surface created by other modules. thus  proteo can not internally manipulate objects unless the control creates holes inside the structure. telecube modules can deform and move inside a structure of other modules. the different hardware capabilities of proteo and telecube affect the design and complexity of the local control rules that are appropriate to consider. thus hardware designs should balance the control complexity and the manufacturing difficulty. ideally  the hardware should support simple highlevel programming abstractions while the control simplifies the required hardware capabilities. schemes based on local  module-level  control help with this since they are simple and can be easily modified to apply to a wide range of module designs  including proteo and prismatic robots  such as telecube and the crystalline robot at dartmouth .
모our approach could be combined with other methods  as part of an overallhierarchical control scheme. moreover testing a population of agents against variations in the desired task can evolve better behaviors . for instance  with the primitives presented in this paper  genetic techniques could identify methods to modulate the rule parameters and switch among rules to achieve higher-level goals.
모as robots with many modules are built  one important question is how module failures affect system behavior. scaling is another important issue  i.e.  increasing the number of modules or decreasing their size  or both. more modules allow greater variety of interactions and statistically more robust randomized behaviors. smaller modules allow finer scale object manipulation. however  smaller modules take smaller steps resulting in slower manipulation unless they complete each step more rapidly. this observation highlights the importance of balancing the scaling of speed  strength and other module properties  1  1  as  for example  with protein motors that carry weights much larger than their own  1  1 .
multiagent control can balance competing goals to help such robots achieve robust behaviors.
references
 harold abelson et al. amorphous computing. technical report 1  mit artificial intelligence lab  august 1.
 forrest h. bennett iii and eleanor g. rieffel. design of decentralized controllers for self-reconfigurable modular robots using genetic programming. in proc. of the 1nd nasa/dod workshop on evolvable hardware  july 1.
 karl f. bohringer et al. computational methods for design and control of mems micromanipulator arrays. computational science and engineering  1 :1  january-march 1.
 hristo bojinov  arancha casal  and tad hogg. multiagent control of modular self-reconfigurable robots. in proc. of intl. conf. on multiagent systems  icmas1   1. an extended version is los alamos preprint cs.ro/1.
 ned bowden  andreas terfort  jeff carbeck  and george m. whitesides. self-assembly of mesoscale objects into ordered two-dimensional arrays. science  1-1  1.
 rodney a. brooks. new approaches to robotics. science  1-1  september 1.
 janusz bryzek  kurt petersen  and wendell mcculley. micromachines on the march. ieee spectrum  pages 1  may 1.
 p. caloud et al. indoor automation with many mobile robots. in proc. of the intl. workshop on intelligent robots and systems. ieee  1.
 arancha casal and mark yim. self-reconfiguration planning for a class of modular robots. in spie symposium on intelligent systems and advanced manufacturing: sensor fusion and decentralized control in robotic systems  pages 1  1.
 h. g. craighead. nanoelectromechanical systems. science  1-1  1.
 k. eric drexler. nanosystems: molecular machinery  manufacturing  and computation. john wiley  ny  1.
 thorbjo몮rn ebefors et al. a walking silicon micro-robot. in proc. of the 1th intl. conf. on solid-state sensors and actuators  transducers1   pages 1  1.
 joshua m. epstein and robert axtell. growing artificial societies. mit press  cambridge  ma  1.
 robert a. freitas jr. nanomedicine  volume 1. landes bioscience  1.
 s. hackwood and g. beni. self-organization of sensors for swarm intelligence. in proc. of the conference on robotics and automation  icra1 . ieee  1.
 j. storrs hall. utility fog: the stuff that dreams are made of. in b. c. crandall  editor  nanotechnology  pages 1. mit press  cambridge  ma  1.
 brosl hasslacher and mark w. tilden. living machines. in l. steels  editor  robotics and autonomous systems: the biology and technology of intelligent autonomous agents. elsivier  1.
 james r. heath  philip j. kuekes  gregory s. snider  and r. stanley williams. a defect-tolerant computer architecture: opportunities for nanotechnology. science  1-1  1.
 tad hogg and bernardo a. huberman. controlling smart matter. smart materials and structures  1:r1-r1  1. los alamos preprint cond-mat/1.
 k. hosokawa et al. self-organizing collective robots with morphogenesis in a vertical plane. in proc. of the conference on robotics and automation  icra1 . ieee  1.
 joe howard. molecular motors: structural adaptations to cellular functions. nature  1-1  1.
 bernardo a. huberman and natalie s. glance. evolutionary games and computer simulations. proceedings of the national academy of science usa  1-1  august 1.
 j. o. kephart  t. hogg  and b. a. huberman. dynamics of computational ecosystems. physical review a  1-1  1.
 hiroaki kitano  editor. robocup-1: robot soccer world cup i  volume 1 of lecture notes in computer science. springer  berlin  1.
 keith kotay  daniela rus  marsette vona  and craig mcgray. the self-reconfiguring robotic molecule: design and control algorithms. algorithmic foundations of robotics  1.
 jeremy kubica  arancha casal  and tad hogg. complex behaviors from local rules in modular self-reconfigurable robots. in proc. of icra1  1.
 ross j. metzger and mark a. krasnow. genetic control of branching morphogenesis. science  1-1  1.
 carlo montemagno and george bachand. constructing nanomechanical devices powered by biomolecular motors. nanotechnology  1-1  1.
 satoshi murata et al. a 1-d self-reconfigurable structure. in proc. of the conference on robotics and automation  icra1   page 1. ieee  1.
 satoshi murata  haruhisa kurokawa  and shigeru kokaji. self-assembling machine. in proc. of the conference on robotics and automation  icra1   pages 1  los alamitos  ca  1. ieee.
 amit	pamecha 	imme	ebert-uphoff 	and	gregory	s.
chirikjian. useful metrics for modular robot motion planning. ieee transactions on robotics and automation  1-1  1.
 d. rus and m. vona. self-reconfiguration planning with compressible unit modules. in proc. of the conference on robotics and automation  icra1 . ieee  1.
 j. r. rush  a. p. fraser  and d. p. barnes. evolving cooperation in autonomous robotic systems. in proceedings of the ieee international conference on control  march 1 1.
 b. salemi  w.-m. shen  and p. will. hormone controlled metamorphic robots. in proc. of the intl. conf. on robotics and automation  icra1   1.
 luc steels. cooperation between distributed agents through self-organization. journal on robotics and autonomous systems  1.
 darcy wentworth thompson. on growth and form. cambridge university press  cambridge  1.
 mark yim. locomotion with a unit-modular reconfigurable robot. phd thesis  stanford university  1.
 mark yim  john lamping  eric mao  and j. geoffrey chase. rhombic dodecahedron shape for self-assembling robots. technical report p1  xerox parc  1.

robotics and perception
vision

learning iterative image reconstruction
sven behnke
freie universita몮t berlin  institut f몮ur informatik
takustr. 1  1 berlin  germany  behnke inf.fu-berlin.de

abstract
successful image reconstructionrequires the recognition of a scene and the generation of a clean image of that scene. we propose to use recurrent neural networks for both analysis and synthesis.
the networks have a hierarchical architecture that represents images in multiple scales with different degrees of abstraction. the mapping between these representations is mediated by a local connection structure. we supply the networks with degraded images and train them to reconstruct the originals iteratively. this iterative reconstruction makes it possible to use partial results as contextinformation to resolve ambiguities.
we demonstrate the power of the approach using three examples: superresolution  fill in of occluded parts  and noise removal / contrast enhancement.
1	introduction
the quality of captured real world images is frequently not sufficient for the application at hand. the reasons for this can be found in the image formation process  e.g. occlusions  and in the capturing device  e.g. low resolution  sensor noise .
모goal of the reconstruction process is to improve the quality of measured images  e.g. by suppressing the noise. to separate noise from objects  models of the noise and the objects present in the images are needed. then  the scene can be recognized and a clean image of that scene can be generated.
모hierarchical image decompositions using wavelets have been successfully applied to image denoising  simoncelli and adelson  1; donoho and johnstone  1 . the image is transformed into a multiscale representation and the statistics of the coefficients of this representation are used to threshold them. the back-projected images are then less noisy. problematic with these approachesis that the choice of the wavelet transformation is usually fixed and the thresholding ignores dependencies between neighboring locations within a scale and between scales.
모the recently proposed vista approach  freeman and pasztor  1  to learning low-level vision uses markov random fields to model images and scenes. the parameters of

figure 1: iterative image reconstruction.
these graphical models can be trained  e.g. for a superresolution task. however  the models have no hidden variables and the inference via belief propagation is only approximate.
모continuous attractor networks have been proposed to complete images with occlusions  seung  1 . for digits belonging to a common class  a two-layer recurrent network was trained using gradient descent to reconstruct the original. the network had many adaptable parameters  since no weight sharing was used. further  it was not demonstratedthat the reconstruction is possible  if the digit class is unknown. we extend the approach by adding lateral connections  weight sharing  and more layers to the network and train it to reconstruct digits from all classes without presenting the class label.
모a common problem with image reconstruction is that it is difficult to decide locally about the interpretation of an image part. for example in a digit binarization task  it might be impossible to decide whether or not a pixel belongs to the foreground by looking only at the pixel's intensity. if contrast is low and noise is present  it could be necessary to bias this decision with the output of a line-detector for that location.
모in general  to resolve such local ambiguities  a large context is needed  but feed-forward models that consider such a large context have many free parameters. they are therefore expensive to compute and difficult to train.
모we propose to iteratively transform the image into a hierarchical representation and to use partial results as context. figure 1 illustrates the propagation of information from regions that are interpreted easily to ambiguous regions. further  we describe the reconstruction problem using examples of degraded images and desired output images and train a recurrent neural network of suitable structure to do the job.
모the remainder of the paper is organized as follows: in the next section  the hierarchical architecture of the proposed recurrent networks is introduced. section 1 discusses the supervised training of such networks. experiments on three image reconstruction tasks are presented in section 1.

figure 1: sketch of the recurrent network.
1	hierarchical architecture
the neural abstraction pyramid architecture  introduced in  behnke and rojas  1   is a suitable framework for iterative image reconstruction.
the main features of the architecture are:
pyramidal shape: layers of columns are arranged vertically to form a pyramid  see fig. 1 . each column consists of a set of neural processing elements  nodes  with overlapping receptive fields. the number of nodes per column increases and the number of columns per layer decreases towards the top of the pyramid.
analog representation: each layer describes an image in a two-dimensional representation where the level of abstraction increases with height  while spatial resolution decreases. the bottom layer stores the given image  a signal . subsymbolic representations are present in intermediate layers  while the highest layers contain almost symbolic descriptions of the image content. these representations consist of quantities that have an activity value from a finite interval for each column.
local interaction: each node is connected to some nodes from its neighborhoodvia directed weighted links. the shared weights of all nodes in a layer that represent the same quantity are described by a common template.
the links can be classified as:
- feed-forward links: perform feature extraction 
- lateral links: for consistent interpretation 
- feedback links: provide interpretation hypotheses.
discrete time computation: the update of a node's value for time step dependsonly on the input values at . all nodes are updated in parallel at each time step.
모we use -units as neural processing elements that compute the weighted sum of their inputs and apply a nonlinear output function. the update of the value of a unit at column in layer for quantity is done as follows:
the template is associated with quantity at layer . is the set of links of that template and is the template bias.	describe location and quantity of the input value for link   and is the link weight. the output function is here a sigmoid function that limits the values to the interval . in addition to the weights and the bias a start value for initialization at is needed for each template. the value of input nodes is set to a copy of the corresponding component of the input vector of the current example :
모the feed-forward inputs of a node come from all quantities in a small window at the corresponding position in the layer directly below that node. lateral connections link to all quantities in its neighborhood including the node itself. feedback links originate from the units in the layer above that correspond to the same position.
1	training recurrent networks
in  behnke  1  an unsupervised learning algorithm for the neural abstraction pyramid architecture has been proposed. it learns a hierarchy of increasingly abstract representations of the image content that could be used to improve the quality of the images. here  we apply supervised training to achieve the desired image reconstruction.
모training of recurrent neural networks is difficult due to the non-linear dynamics of the system. several supervised training methods have been proposed in the literature. realtime recurrent learning  rtrl   williams and zipser  1  is suitable for continuously running networks  but very resource intensive. the backpropagation through time algorithm  bptt   williams and peng  1 unfolds the network in time and applies the backpropagation idea to compute the gradient of the error function. its computational costs are linear in the number of time steps the error is propagated back.
모for image reconstruction  we present a static input to the network and train it to quickly reach a fixed point that coincides with the desired output . thus  the network runs only a few iterations and the gradient can be computed efficiently. no artificial truncation of history is necessary.
1	objective function
the goal of the training is to produce the desired output as quickly as possible. to achieve this  the network is updated for a fixed number of iterations. the output vector collects the output units of the network in an appropriate order.
모the output error   the difference between the activity of the output units and the desired output is not only computed at the end of the sequence  but after every update step. in the error function we weight the squared differences progressively  as the number of iterations increases:
모a quadratic weight has proven to give the later differences a large enough advantage over the earlier differences  such that the network prefers a longer approximation phase  if the final approximation to the desired output is closer.
모the contribution of intermediate output values to the error function makes a slight modification to the original backpropagation rule necessary. at all copies of the output units for the difference is computed and added to the backpropagated component of the gradient.
1	robust gradient descent
minimizing the error function with gradient descent faces the problem that the gradient in recurrent networks either vanishes or grows exponentially in time  depending on the magnitude of gains in loops  bengio et al.  1 . it is therefore very difficult to determine a learning constant that allows for both stability and fast convergence.
모for that reason  we decided to employ the rprop algorithm  riedmiller and braun  1   that maintains a learning constant for each weight and uses only the sign of the gradient to determine the weight change. the learning rates are initialized to a moderate value  increased when consecutive steps have the same direction  and decreased otherwise. we modify not only the weights in this way  but adapt the bias and start values as well.
모the rprop training method proved experimentally to be much more stable than gradient descent with a fixed learning rate. however  to compute the gradient  all training examples have to be presented to the network  which is slow for large training sets. to accelerate the training we implemented the following modification. we use as batch only a small working set of training examples. this set is initialized at random. after each weight update  a small fraction of the examples is replaced with randomly chosen examples to ensure a stable estimate for the gradient that takes over time all training examples into account. with a working set of of 1 training examples we realized a speedup of two orders of magnitude  as compared to the batch method  without compromising convergence.
1	experimental results
we conducted a series of experiments with images of handwritten digits to demonstrate the power of the proposed approach for iterative image reconstruction. the reason for choosing digits was that large datasets are publicly available and that the images contain multiscale structure which can be exploited by the learning algorithm. clearly  if there were no structure to learn  the training would not help.
모we degraded the digits by subsampling  occlusion  or noise and trained recurrent networks to reconstruct the originals.
1	superresolution
for our first experiment we used the original nist images of segmented binarized handwritten digits  garris and wilkinson  1 . the digits are given in a window  but their bounding box is typically much smaller. for this reason  we centered the bounding box in a window to produce the desired output . the input to the network consists of subsampled versions of the digits that have been pro-
duced by averaging	pixels.
모the superresolution network has three layers  as shown in figure 1. the low resolution image is input to the rightmost output	input

figure 1: network for superresolution.
layer. four quantities represent the digit in the middle layer. they are connected to their -neighborhoods to windows of the output units  and to a single input node. the leftmost layer contains only the output units of the network. they are connected to four nodes in the middle layer and to their -neighborhoods.
모we initialized the 1 free parameters of the network randomly and trained the network for ten time steps using 1 randomly chosen examples. as test set we used 1 different randomly chosen examples. figure 1 shows for the first five test digits  how the output of the network develops over time. after two iterations the input can influence the output  but no further interactions are possible yet. in the following iterations the initial reconstruction is refined.

input	1	1	target
figure 1: iterative superresolution.
모the network tries to concentrate the gray that is present in the input images at black lines with smooth borders. to illustrate this behavior  we presented uniform pixel noise to the network. the stable response after ten time steps is shown in:
out:
figure 1: response of the superresolution network to uniform noise.
in figure 1. the network hallucinates smooth black lines at positions where many dark pixels are present.
모we also trained a larger version of the recurrent network  rnn   that had eight hidden quantities in the middle layer as well as two feed forward neural networks  ffnn  with four and eight quantities. the units of the ffnns looked at windows of the previous layer such that the networks had a similar number of adjustable parameters as the corresponding rnns. figure 1 shows for the next five test digits the output of these four networks after 1 iterations. in general  the reconstructions are good approximations to the high resolution targets  given the low resolution inputs. the rnn outputs appear to be sharper than the responses of the ffnns.
모in figure 1 the mean square error of the networks is displayed. the test set reconstruction error of the recurrent networks decreases quickly and remains below the error of the corresponding ffnn after six time steps. at iterations 1 and 1 the small rnn outperforms even the large ffnn.
input	rnn	ffnn	rnn	ffnn	target small	large
figure 1: outputs of different superresolution networks.

	1	1	1	1
 b 	iteration
figure 1: mean square error of superresolution:  a  the recurrent network on the training set and the test set;  b  detailed view of the test set performance  compared to ffnn.
1	fill in of occluded parts
for the second reconstruction experiment we used the mnist database of handwritten digits  lecun  1 . the nist digits have been scaled to the size and centered in an image. we set an square to the value 1
 light gray  to simulate an occlusion. the square was placed randomly at one of positions  leaving a 1 pixel wide border that was never modified. while in the top layer the resolution of the 1 quantities is reduced to and the feed-forward weights are connected to all nodes of the third layer. the first three layers are surrounded by a one pixel wide border that is set to zero. in these layers the nodes have lateral connections. in the fourth layer the lateral weights contactall 1 nodes. the feedback links are non-overlapping and have thus the size between the first three layers and between the third and the topmost layer.
모training is done with a working set of 1 of the 1 examples for twelve time steps. figure 1 displays the reconstruction process for the first ten digits of the test set. one can observe that the images change mostly at occluded pixels. this shows that the network recognized the occluding square. further  the change is such that a reasonable guess is produced  how the digit could look like behind the square. the network connects lines again that have been interrupted by the square. it is also able to extend shortened lines and to close opened loops. in most cases  the reconstructions are very similar to the original digits.
1	noise removal and contrast enhancement
the last experiment uses the same network architecture and the same mnist digits  but degrades the input images as follows. we scaled the pixel intensities to   added a random background level that was uniformly distributed in the range   and added uniform pixel noise in the range . finally  we clipped the pixel values at
모모. the first column of figure 1 shows the first ten digits of the test set that have been corrupted in this way. the network was trained on a working set of 1 out of 1 digits for twelve time steps.
모the reconstruction process is also shown in figure 1. one can observe that the network is able to detect the dark lines  to complete them  to remove the background clutter  and to enhance the contrast. the interpretation of most locations is decided quickly by the network. ambiguous locations are kept for some iterations at intermediate values  such that the decision can be influenced by neighboring nodes. the reconstructed digits are very similar to the originals.
1	discussion
the experiments demonstrated that difficult non-linear image reconstruction tasks can be learned by hierarchical neural networks with local connectivity. supervised training of the networks was done by a combination of bptt and rprop.
모the networks reconstruct images iteratively and are able to integrate partial results as context information for the resolution of local ambiguities. this is similar to the recently demonstrated belief propagation in graphical networks with cycles. the difference is that the proposed approach learns horizontal and vertical feedback loops that produce rich multiscale representations to model the images where current belief propagation approaches use either trees or arrays to represent the vertical or horizontal dependencies  respectively.
모further  the proposed network can be trained to compute an objective function directly  while inference in belief networks with cycles is only approximate due to multiple counting of the same evidence.
모recently  generalized belief propagation has been proposed  yedidia et al.  1  that allows for better approximations of the inference process. it would be interesting to investigate the relationship between this approach and the proposed hierarchical recurrent neural networks.
모the iterative reconstruction is not restricted to static images. the training method allows for a change of input and/or desired output at each time step. thus  the networks should be able to integrate information over time  which would help to reconstruct video sequences.
references
 behnke and rojas  1  sven behnke and rau뫣l rojas. neural abstraction pyramid: a hierarchical image understanding architecture. in proceedings ijcnn'1- anchorage  volume 1  pages 1  1.
 behnke  1  sven behnke. hebbian learning and competition in the neural abstraction pyramid. in proceedings ijcnn'1 - washington  dc  paper #1  1.
 bengio et al.  1  y. bengio  p. simard  and p. frasconi. learning long-term dependencies with gradient descent is difficult. ieee transactions on neural networks  1 :1  1.
 donoho and johnstone  1  d.l. donoho and i.m. johnstone. adapting to unknown smoothness via wavelet shrinkage. journal of the american statistical association  1 :1  1.
 freeman and pasztor  1  w. freeman and e. pasztor. learning low-level vision. in proceedings of iccv1  pages 1  1.
 garris and wilkinson  1  m. d. garris and r. a. wilkinson. nist special database 1 - handwritten segmented characters. technical report hwsc  nist  1.
 lecun  1  yann lecun. the mnist database of handwritten digits. http://www.research.att.com/ yann/exdb/mnist  at&t labs  1.
 riedmiller and braun  1  martin riedmiller and heinrich braun. a direct adaptive method for faster backpropagation learning: the rprop algorithm. in proceedings of the international conference on neural networks - san francisco  ca  pages 1. ieee  1.
 seung  1  h. sebastian seung. learning continuous attractors in recurrent networks. in advances in neural information processing systems 1  pages 1  1.
 simoncelli and adelson  1  e. simoncelli and e. adelson. noise removal via bayesian wavelet coring. in proceedings of ieee int. conf. on image processing - icip'1  switzerland   1.
 williams and peng  1  r. williams and j. peng. an efficient gradient-based algorithm for on-line training of recurrent network trajectories. neural computation  1 :1  1.
 williams and zipser  1  r. j. williams and d. zipser. a learning algorithm for continually running fully recurrent neural networks. neural computation  1 :1  1.
 yedidia et al.  1  j. yedidia  w. t. freeman  and y. weiss. generalized belief propagation. in advances in neural information processing systems 1  to appear   1.


input	1	1	target	input	1	1	target
	figure 1: fill in of occluded parts.	figure 1: noise removal and contrast enhancement.
a hierarchy of boundary-based shape descriptors
richard meathrel and antony galton
school of engineering and computer science
university of exeter
exeter ex1pt  uk
	e-mail:	r.c.meathrel  a.p.galton  ex.ac.uk

abstract
in this paper we extend previous work on the boundary-based approach to describing shape  by deriving an unbounded hierarchy of  atomic  shape descriptors  called tokens  based on tangent bearing and its successive derivatives  and incorporating angle and cusp curve features. both open and closed curves have token-string descriptions at all levels in the hierarchy. we provide a pair of compatibility matrices for generating transition tables for any level  from which level-specific token ordering graphs that encode basic string syntax can be systematically constructed.
1	introduction
in the development of conceptual tools for spatial representation and reasoning  the category of shape has provedto be one of the most problematic areas. some important earlier work  representative of the boundary-based approach to shape  is exemplified by the contour codons of hoffman and richards  and the extremum primitives of leyton . these two approaches  although differing in motivation and detail  both used the idea of characterising the shape of an outline by means of a string of tokens  recording salient curvaturebased features encountered during a traversal of the outline. a more recent approach  using yet another set of primitives  is that of  galton and meathrel  1 . in  meathrel and galton  1   we attempted to define the boundary-based approach to shape in a more general and systematic way. by considering variations in curvature as a starting point  we derived two sets of atomic tokens for describing curves  and presented token ordering graphs for verifying the syntax of atomic curve descriptions.
모the present paper extends the work described in  meathrel and galton  1  in three significant respects:  i  by systematically investigatingthe structure of sets of atomic curvaturebased tokens  resulting in an unbounded hierarchy of atomic tokens   ii  by incorporating kink points  angles and cusps  into the framework of tokens  and  iii  by providing a pair of compatibility matrices  and an algorithm  for generating transition tables for any level in the hierarchy  from which level-specific token ordering graphs can be constructed.
1	deriving the hierarchy of atomic tokens
at each point on a curve  the tangent to the curve at the point is either defined or undefined. the rate of change of the tangent bearing with distance along a curve gives us curvature  the first derivative of tangent bearing with respect to arclength . we can also consider the rate of change of curvature      which is the second derivative of tangent bearing. for each curve we can think of there being an infinite number of associated plots. our hierarchy of descriptors is based on a discretisation of and its derivatives. for tangent bearing we are interested in whether is defined     or undefined      so we use the quantity space . points where the tangent bearing is undefined correspond to angles and cusps  and may be referred to as kink points. for all of the derivatives of tangent bearing  i.e.     we use the quantity space
모모모모모  since a derivative may be positive  zero  negative  or undefined.
1	curve states
associated with each point on a curve is a sequence of qualitative values  representing       etc. the complete curve state at corresponds to an infinite sequence of values. we write to denote the th component of the curve state . not all componentsequencesgive rise to valid curvestates. if a component has a value other than   then the value of the next component is unconstrained. if  however  a component does have the value   then all subsequent components must also be . more formally  a curve state must satisfy the following constraint:
a partial curve state at is any initial -tuple of the complete curve state. so if   for example  then
모모모모    and is a partial curve state that is assigned to points on a curve where the tangent bearing is defined and the curvature is positive.
모we refer to a partial curve state with components as a level- state. given the complete set of states for some level
  it is straightforward to generate the set of states for level
모모. each state   at level   generates a set of levelstates    as follows  where  :
if
otherwise
there are two partial curve states with one component:
모모and . from these two states we can generate the five states of level 1 and then  from those  the fourteen states of level 1:
모for each point on a curve we can assign a partial curve state of components  so we could theoretically represent a curve by providing a mapping between curve points and partial curve states. however  such a mapping would be infinite and therefore of no practical use. because our state components take qualitative values  however  certain states may persist over intervals of curve and  therefore  support mappings that are finite. a curve state may have an interval and/or a point interpretation. a state that has an interval interpretation may persist over an interval of curve  and a state that has a point interpretation may hold at a single curve point  without holding on any interval adjoining that point. an atomic token identifies a particular interpretation of a partial curve state.
1	interval and point interpretation
for each set of level- curve states  we obtain a corresponding set of level- atomic tokens by considering the allowable interpretations of each state. a state may support an interval interpretation  a point interpretation  or both. an atomic token  or  atom   is a particular interpretation of a particular state  and is identified by a signature consisting of a sequence of qualitative component values. a signature that is underlined indicates an interval interpretation; a non-underlined signature indicates a point interpretation. the atom d+  for example  is identified with the interval interpretation of the state   and the atom d+-1 is identified with the point interpretation of the state .
모a curve state may hold at a single point iff one of its components is either zero or undefined. this is because if all components are defined  then all of them are continuous  and hence the values ' ' and ' ' can only hold over intervals. a curve state may persist over an interval iff none of its components are undefined and  whenever a component has the value zero  the next component also has the value zero. the following predicates  therefore  can be used to determine the interpretations that are supported by a curve state:
-
-
1	atomic hierarchy
for each set of partial curve states  each level  we can use the predicates - and - to obtain the corresponding set of atoms. the partial curve states and atoms for level 1 are as follows:
d++
d+1 d+1
d+-
d+u
d1+
d1 d1
d1-d1u
d-+
d-1 d-1
d--
d-u
duu
uuu모the first four levels of the hierarchy of atomic tokens are shown in figure 1. each atom in the hierarchy  except the atoms d and u at level 1  is a child of a single parent atom at the previous level  i.e.  each atom at level is derivable from one  and only one  atom at level . we call parent atoms that have more than one child expansive  and those with only one child non-expansive. an interval atom is expansive iff its last component is either ' ' or ' '  otherwise it is nonexpansive. a point atom is expansive iff its last component is not ' '  otherwise it is non-expansive. at level 1  then  there is just one non-expansive atom  u  which propagates through the hierarchy as uu  uuu  and so on. the other level-1 atom  d  expands into five atoms at level 1; two of which are nonexpansive: d1 and du.
모any atom is either an interval  i  or a point  p   and either expansive  e  or non-expansive  n   giving us four kinds of atoms: ie  in  pe  and pn. each ie atom yields five children: two ie atoms  one in atom  one pe atom  and one pn atom. each pe atom yields four children: three pe atoms and one pn atom. the non-expansiveatoms  in and pn  yield a single in atom and a single pn atom  respectively.
모the recursive equations for calculating the numbers of each kind of atom at level   and the actual figures for levels 1 to 1  are as follows:
ieieininiepepeiepnpnpeietotalieinpepnlevelieinpepntotal1111111111111111111	atom labelling
level 1	d	u

figure 1: the first four levels of the atomic hierarchyannotating curves with signatures and describing curves with strings of signatures is not ideal. therefore  for notational convenience  we assign each atom at level two or above a suitable descriptive label  according to the following convention1: the label begins with ' '  ' '  ' '  or ' '  depending on whether the curvature component is positive  zero  negative  or undefined. the remaining components     are denoted by a superscripted string  whose elements are values from the set . the label is underlined if the atom it is symbolising is an interval. to distinguish between the two distinct kinds of atoms where the curvature component is   we use the label ' ' for atoms where the tangent bearing component is also   and ' ' for atoms where the tangent bearing is . using the new notation  the first three sets of atomic tokens are written as follows:
	level 1	
	level 1	
	level 1			

1	kink points
at each level in the hierarchy there exists a single kink-point atom labelled   each component of which takes the value . such atoms represent points on a curve where the tangent bearing  and therefore all of its derivatives  are undefined  and correspond perceptually to angles and cusps. clearly  we would like to be able to distinguish between different kinds of kink points  otherwise we are losing important curve information. in particular  the two most relevant aspects of a kink point we would like to preserve are its orientation  whether it is inward-pointing or outward-pointing  and its type  i.e.  whether it is an angle or a cusp. in order to distinguish between the different kinds of kink points  we introduce the following kink tokens into our representation: and
 for inward and outward pointing angles  respectively   and and  for inward and outward pointing cusps  respectively . these tokens have the same status as   in that they can be thought of as appearing at every level in the hierarchy. note  however  that kink tokens are not atomic  since the process by which the atomic tokens are derived does not produce them. instead  we need to introduce them explicitly.
1	atomic description
a curve is described by a string of atomic tokens taken from a particular level in the hierarchy. by prefixing descriptions with a symbol indicating curve type  both open and closed curves can be represented. we use ' ' for open curves and ' ' for closed ones. our convention for relating changes in tangent bearing to curvature is that a clockwise change in tangent bearing indicates positive curvature  while an anticlockwise change indicates negative curvature. thus positive curvature is associated with convex curve segments and negative curvature with concave segments. a curve can be traversed in one of two directions. for a closed curve  the direction is that which preserves the intended figure/ground relationship. for open curves  consistent description dictates that one of the two directions is chosen as the  default  one.
모shown in figure 1 are two curves that have been annotated with level-1 atoms. note that  refers to a convex circular arc  constant positive curvature over an interval   whereas refers to a positive curvatureextremum; and analogouslywith  and . given a closed curve there will be  in general  a number of different token strings that describe it. however  we can easily transform one string into another by cyclically shifting it by a certain amount. in this sense  all of the strings may be considered equivalent. any one of sixteen level-1 strings may be used to describe the closed curve in figure 1  two of which are obtained by either starting at the or starting at the uppermost :


모the importance of specifying a particular direction for open curves is illustrated by the open curvein figure 1  where a reversal of direction yields the description .
note  however  that under a system where  mirror-reflected  curves are considered equivalent  the direction chosen for traversal is unimportant.
1	levels of description
a curve has a description at every atomic level. the most coarse-grained description is at level 1  where only the atoms  and are available. by moving down the hierarchy  and incorporating more qualitative components  we get finergrained descriptions that reflect the increase in discriminatory power that extra components provide.

figure 1: example curves labelled with level-1 atoms
모given the description of a curve at some level   we can derive all of the coarser-grained descriptions for the curve  i.e.  from the description at level 1 through to the description at level . if denotes the description of a curve   at level   then the following straightforward three-step procedure yields the description of at level :
1. replace each atom of	with its parent.
1. iteratively replace substrings of the form  where is a single atom  with   until there are no more substrings of the form .
1. if is a closed curve  and the string resulting from the previous step is of length greater than two  and begins and ends with atoms of the same type  then remove the last atom in the string.
모by applying the procedure to the ellipse shown in figure 1  we get the level-1 description   by re-applying the procedure with  as input  we get the most coarse-grained description for an ellipse: .

figure 1: an ellipse described at level 1
모although we can derive all coarser-grained descriptions for a curve given its description at level   it is not possible  in general  to derive any of its finer-grained descriptions. the occasion when it is possible to derive a finer-grained description is when the description given consists entirely of atoms that are non-expansive  in which case all finer-grained descriptions are accessible. consider the description of a square at level 1:  . because both  and are non-expansive  we can get the  finer-grained  description at level 1 by replacing each atom in the string with its single child atom. the level-1 description  for example  is  . another example would be a shape that has the level-1 description  i.e.  a convexcircular arc segment . for most shapes it would appear that there exists some minimal value of such that its description at level contains only non-expansiveatoms. for the square  ; descriptions at levels are unnecessary  leading to no increase in  qualitative precision . for a circle  we need to go to level 1 to achieve this. in this sense  a circle might be regarded as a more complex shape than a square.
1	token ordering graphs
a curve is described by stringing together those atoms that correspond to the  atomic  qualitative boundary features of the curve. clearly  some features can occurtogether and some cannot. a tokenorderinggraph tog  of the kind introduced in  meathrel and galton  1   encodes the constraints that determine the basic string syntax for a set of atoms. in this section  we show how the transition tables that underlie such graphs can be systematically constructed for each level of the atomic hierarchy.
모given a set of atoms  a tog is a way of pictorially representing the two complementary transition tables that specify which atoms may follow which other atoms in a string. the interval-interval   i-i   table tells us which interval atoms can follow which other interval atoms. the interval-pointinterval   i-p-i   table tells us which point atoms can occur in between each pair of interval atoms. it is the construction of these tables  then  that we are primarily interested in. the i-i and i-p-i tables for level 1 are given later  in figure 1.
1	compatibility matrices
we make use of two compatibility matrices when constructing i-i and i-p-i tables. first  consider the construction of an i-i table for a given level in the hierarchy. each cell row column in an i-i table contains a tick iff the interval atom can directly follow the interval atom . given interval atoms and   at level   we can determine whether ot not can directly follow by making use of the matrix given in table 1. the matrix tells us whether or not the th qualitative components of the atoms and are compatible.
table 1: compatibility matrix for i-i tables
table 1: compatibility matrix for i-p-i tables모the th componentof is compatible with the th component of   and holds  iff the condition in the corresponding cell is satisfied. if and are of opposing signs then they are definitely incompatible  signified by a in the appropriate cell   because a derivative cannot change from positive to negative  or vice versa  without taking the value or in between. if an interval over which has the value is followed immediately by an interval over which is positive  then in the latter interval it must initially be increasing  i.e.  must be positive. this explains the entry for cell . analogous explanations hold for the other non-trivial entries in the table. given interval atoms and at level   then  can directly follow iff - holds:
-
in other words  and must be distinct interval atoms  to ensure an underlying state change   and the components of must be compatible with the components of . note that  whenever a condition in the matrix refers to an  unavailable  componentof an atom   i.e.  when   the condition is satisfied because the valueof is unconstrained since it is not specified . as an example  consider the level-1 interval atoms and   and whether  can directly follow . we start by comparing the second component of each atom  since all interval atoms have the same value     for their first component. because both  and  take the value ' '  the compatibility condition is   and therefore satisfied. the condition of compatibility for the final component pair evaluates to    since  and  . the condition is satisfied because  has no fourth component   is a generalisation of a set of child atoms that includes the level-1 atom  .
모next  consider the construction of an i-p-i table for a given level in the hierarchy. each cell row column in an i-p-i table contains a list of point atoms that can occur in between interval atoms and . for i-p-i tables  we make use of the compatibility matrix given in table 1  which differs from the matrix for i-i tables in that the notion of compatibility concerns three components rather than just two. given a point atom and interval atoms and   at level   the component is compatible with the components and   and holds  iff the condition in the corresponding cell is satisfied. a point atom   then  can occur in between two interval atoms and  after and before   iff - - holds:
	-	-
in other words  the underlying curve state of must be distinct from the underlying curve states of and   and the qualitative components of must be compatible with those of and . as before  conditions that refer to  unavailable  components are automatically satisfied. to illustrate we shall use the table to determine which point atoms can come between  and . we have and
. we want to know the possible values for
	 	and	. since the definition of	-	-
imposes no constraints on the first level components  can be either or . for   the top-left cell of the matrix states that may be any of     and   but is ruled out because it requires that and   which contradicts . moving on to   we consult the middle cell in the bottom row of the matrix. we see that may be either or   the additional constraints on the latter value being irrelevant in the present case  as . remembering that if any component is   all subsequent components must be too  we derive the following candidate component sequences for :
corresponding to the level-1 atoms	 	 	  and	. the
first condition of - - rules out   as its underlying curve state is not distinct from that of . thus we conclude that only the point atoms     and can occur in between the interval atoms and .
1	transition tables
the set of atoms at level is the union of a set of interval atoms    and a set of point atoms    e.g.  at level 1 we have  and . the transition tables at level   i-i and i-p-i   each contain cells  one for each ordered pair of interval atoms. we refer to a cell row column in the i-i and i-p-i tables by writing i-i and i-p-i   respectively. the following algorithm populates the i-i and i-p-i tables for any level in the hierarchy1:

for each	do if - then i-i ; for each do
	if	-	-	then add	to i-p-i	;

모owing to space constraints  only the i-i and i-p-i transition tables for level 1 are given here  see figure 1 . as described in 1  we can substitute the single kink-point atom with a set of kink tokens: . in order to do this  we need to replace each in cell   of the i-p-i table with the correct subset of kink tokens  as follows:  i  replace with and   and  ii  add if or   and add if or   i.e.  inward-pointing cusps must be flanked by an interval of positive curvature and outward-pointing cusps by an interval of negative curvature  cf.  galton and meathrel  1  1  .

figure 1: the i-i and i-p-i tables for level 1
1	graph correspondence
a tog consists of nodes representing atomic tokens and edges representing token ordering constraints. the nodes for interval atoms are shown as rectangles and those for point atoms as circles. any path through a tog  which starts and finishes at a node representing an interval atom  yields a syntactically valid string of tokens that is instantiable as an open curve and that may or may not be instantiable as a closed curve. a tog that encodes the ordering constraints for the atoms of level 1  incorporating the set of kink tokens  is shown in figure 1. we have seen that we can construct i-i and i-p-i tables for any level of the atomic hierarchy. from the tables for a particular level  a corresponding tog can be constructed that visually encodes the constraints given by the table contents. we do not here provide an algorithm for constructing togs from i-i and i-p-i tables; details relevant to such an algorithm may be found in  meathrel and galton  1  1 . the tog in figure 1  then  corresponds to the i-i and i-p-i tables given in figure 1  with replaced with the kink tokens  as described at the end of 1 . the togs in figures 1 and 1 of  meathrel and galton  1  correspond  respectively  to the i-i and i-p-i tables of levels 1 and 1 of the atomic hierarchy1.

figure 1: level-1 tog with distinct angle and cusp tokens
1	conclusion
we have generalised the work described in  meathrel and galton  1   by deriving an unboundedhierarchy of atomic tokens and incorporating distinct kink tokens for inward and outward pointing angles and cusps. both open and closed curves have qualitative descriptions  consisting of strings of atoms  at every level of the hierarchy  with the most coarsegrained descriptionat level 1. higher-numberedlevels  where more qualitative components are included  possess greater discriminatory power and allow for finer-grained descriptions. we showed how  for each level of the hierarchy  i-i and i-p-i transition tables  encoding the ordering constraints for the set of atoms at that level  can be constructed by a simple algorithm that makes use of a pair of compatibility matrices. from a pair of i-i and i-p-i tables  a corresponding tog can be constructed that visually encodes their constraints.
모in  meathrel and galton  1   we showedhow sequences of atomic tokens  with  contexts   can be used to model boundary-based schemes such as those of leyton  and hoffman and richards . we believe the theory presented here provides an adequate basis for constructing any boundary-based scheme of qualitative shape descriptors.
references
 galton and meathrel  1  a. galton and r. meathrel. qualitative outline theory. in t. dean  ed.  proc. of 1th ijcai  pp. 1. morgan kaufmann  inc.  1.
 hoffman and richards  1  d. d. hoffman and w. a. richards. representing smooth plane curves for recognition: implications for figure-ground reversal. in proc. of aaai-1  pp. 1  1.
 leyton  1  michael leyton. a process-grammar for shape. artificial intelligence  1-1  1.
 meathrel and galton  1  r. meathrel and a. galton. qualitative representation of planar outlines. in w. horn  ed.  proc. of 1th ecai  pp. 1. ios press  1.
resolving ambiguities to create a natural computer-based sketching environment
christine alvarado  randall davis
mit artificial intelligence laboratoryabstract
current computer-based design tools for mechanical engineers are not tailored to the early stages of design. most designs start as pencil and paper sketches  and are entered into cad systems only when nearly complete. our goal is to create a kind of  magic paper  capable of bridging the gap between these two stages. we want to create a computer-based sketching environment that feels as natural as sketching on paper  but unlike paper  understands a mechanical engineer's sketch as it is drawn. one important step toward realizing this goal is resolving ambiguities in the sketch- determining  for example  whether a circle is intended to indicate a wheel or a pin joint-and doing this as the user draws  so that it doesn't interfere with the design process. we present a method and an implemented program that does this for freehand sketches of simple 1-d mechanical devices.
1	sketching conceptual designs
engineers typically make several drawings in the course of a design  ranging from informal sketches to the formal manufacturing drawings created with drafting tools. drawing is far more than an artifact of the design process; it has been shown to be essential at all stages of the design process  ullman et al.  1 . yet almost all early drawings are still done using pencil and paper. only after a design is relatively stable do engineers take the time to use computer aided design or drafting tools  typically because existing tools are too difficult to use for the meager payoff they provide at this early stage.
모our aim is to allow designers to sketch just as they would on paper  e.g.  without specifyingin advance what component they are drawing  yet have the system understand what has been sketched. we want to have the input be as unconstrained as possible  in order to make interaction easy and natural; our route to accomplishing this is to build a sufficiently powerful sketch recognizer.
모it is not yet obvious that a freehand sketching interface will be more effective in real use than a carefully designed menubased system. in order to do the comparison experiments  however  we must first build powerful sketch-based systems.
it is the construction of such a system that is the focus of this paper.
모the value of sketching as an interface and the utility of intelligent sketch understanding has gained increasing attention in recent years  e.g.   hearst  1  . some early research was concerned with single stroke classification   rubine  1    while more recent work   gross  1; landay and myers  1   puts groups of strokes together to form larger components. a number of efforts  e.g.   gross and do  1    mankoff et al.  1   have acknowledged the necessity of representing ambiguities that arise in interpreting strokes  but have not substantially addressed how to resolve those ambiguities.
모given the frequency of ambiguities in a sketch  a tool that constantly interrupts the designer to ask for a choice between multiple alternatives would be cumbersome. our work is thus focused  in part  on creating a framework in which to both represent and use contextual  top-down  knowledge to resolve the ambiguities. we built a program called assist  a shrewd sketch interpretationand simulation tool  that interprets and understands a user's sketch as it is being drawn  providing a natural-feeling environment for mechanical engineering sketches.
the program has a number of interesting capabilities.   the basic input to the program is a sketch  i.e.  a sequence of strokes drawn  while the system watches   not a finished drawing to be interpreted only after it is complete.
  sketch interpretation happens in real time  as the sketch is being created.
  the programallows the user to draw mechanicalcomponents just as on paper  i.e.  as informal sketches  without having to pre-select icons or explicitly identify the components.
  the program uses a general architecture for both representing ambiguities and adding contextual knowledge to resolve the ambiguities.
  the program employs a variety of knowledge sources to resolve ambiguity  including knowledge of drawing style and of mechanical engineering design.   the program understands the sketch  in the sense that it recognizes patterns of strokes as depicting particular

figure 1: a car on a hill  as drawn by the user in assist.

figure 1: the sketch as displayed by assist.
components  and illustrates its understandingby running a simulation of the device  giving designers a way to simulate their designs as they sketch them.
모we describe the system and report on a pilot user study evaluating the naturalness of the program's interface and the effectiveness of its interpretations.
1	designing with assist
figure 1 shows a session in which the user has drawn a simple car on a hill. the user might begin by drawing the body of the car  a free-form closed polygon. as the user completes the polygon  the system displays its interpretation by replacing the hand-drawn lines  shown in figure 1  with straight blue lines. next the user might add the wheels of the car  which also turn blue as they are recognized as circular bodies. the user can then  attach  the wheels with pin joints that connect wheels to the car body and allow them to rotate. the user might then draw a surface for the car to roll down  and anchor it to the background  the  x  indicates anchoring; anything not anchored can fall . finally  the user can add gravity by drawing a downward pointing arrow not attached to any object. the user's drawing as re-displayed by assist is shown in figure 1.
모the system recognizes the various components in the drawing by their form and context; when the  run  button is tapped  it transfers the design to a two-dimensional mechani-

figure 1: the sketch simulated  showing the consequences.
cal simulator which shows what will happen  figure 1 .1
모note that the user drew the device without using icons  menu commands  or other means of pre-specifying the components being drawn. note  too  that there are ambiguities in the sketch  e.g.  both the wheels of the car and pin joints are drawn using circles  yet the system was able to select the correct interpretation despite these ambiguities  by using the knowledge and techniques discussed below. the automatic disambiguation allowed the user to sketch without interruption.
모figure 1 shows a session in which the user has drawn a more interesting device  a circuit breaker  and run a simulation of its behavior.
모note that assist deals only with recognizing the mechanical components in the drawing and is  purposely  literalminded in doing so. components are assembled just as the user drew them  and component parameters  e.g. spring constants  magnitudes of forces  etc빡  are set to default values. the car in figures 1  for example  wobbles as it runs down the hill because the axles were not drawn in the center of the wheels. the combination of literal-minded interpretation and default parameter values can produce device behavior other than what the user had in mind. other work in our group has exploredthe interesting and difficult problem of communicating and understanding the intended behavior of a device once it has been drawn using assist  oltmans  1 .
1	embedding intelligent assistance
we created a model for sketch understanding and ambiguity resolution inspired by the behavior of an informed human observer  one that recognizes the sketch by relying on both low-level  i.e.  purely geometric  routines and domain specific knowledge.
모one interesting behavior of an informedobserver is that interpretation begins as soon as the designer begins sketching. while not a requiredstrategy-peoplecan obviouslyinterpret a finished sketch-there are advantages in ease of use and in speed from having the program do its interpretation in parallel with drawing. ease of use arises because the program

figure 1: a sketch of a circuit breaker  left  and its simulation
 right .
can provide an indication of its interpretation of parts of the sketch as soon as they are drawn  making it easier for the user to correct a misinterpretation. interpretation is faster because incremental interpretation effects a divide and conquer strategy: parts of the drawing interpreted correctly can provide useful context when interpreting parts drawn subsequently.1
모a second interesting behavior of an informed observer is the ability to accumulate multiple interpretations and defer commitment. consider for example the objects in figure 1. are the strokes in 1a going to become part of a ball and socket mechanism  1b   or are they the beginning of a gear  1c   committing too soon to one interpretation precludes the other. hence interpretation must be capable of revision in the face of new information.
모there is clearly a need to balance out the desire for interpretation occurring in parallel with drawing  and the need to avoid premature commitment. we discuss below how our system accomplishes this.
모third  while commitment should be deferred  it must of course be made eventually  and determining when to make that commitment is not easy. timing information can assist. consider the case of circles: because circles are low-level structures  it is likely that they will be used in higher-level structures  as for example when a circle turns out to be part of a pulley system. one way of dealing with this is to use timing

figure 1: an example of ambiguity: the bold strokes in  b  and  c  are identical to the strokes in  a .
data: the system gets to  watch  the sketch being drawn and knows when each stroke was made. if  some time after the circle has been drawn  it has still not been used in any other structure  the observer can plausibly guess that it will not be incorporated into another piece and should be interpreted as an independent circular body.1
모finally  parts may remain ambiguous even when a piece of the drawing is finished. to resolve these residual ambiguities  the observer uses his knowledge of mechanical engineering components and how they combine. consider  for example  the small circles inside the larger circles in figure 1; assist determines that these are more likely to be pivot joints than additional circular bodies  both because small circles typically indicate pin joints and because bodies do not typically overlap without some means of interconnection  i.e.  the pin joint .
모our system incorporates each of these observations: it begins interpreting the sketch as soon as the user starts drawing; it accumulates multiple interpretations  deferring commitment until sufficient evidence  e.g.  stroke timing  accumulates to suggest a component has been finished  and it resolves ambiguities by relying on knowledge from the domain about how components combine.
1	assist's interpretation and disambiguation process
assist's overall control structure is a hierarchical templatematching process  implemented in a way that produces continual  incremental interpretation and re-evaluation as each new stroke is added to the sketch. each new stroke triggers a three stage process of recognition  reasoning and resolution. recognition generates all possible interpretations of the sketch in its current state  reasoning scores each interpretation  and resolution selects the current best consistent interpretation. after each pass through the three stages the system displays its current best interpretation by redrawing the sketch.
1	recognition
in the recognition stage  assist uses a body of recognizers  small routines that parse the sketch  accumulating all possible interpretations as the user draws each stroke. a recognizer takes as input raw strokes and previously recognized objects  and if the input fits its template  produces a new object. for example  the circle recognizer reports a circle if all the points on a stroke lie at roughly the same distance from the average x and y coordinate of the stroke.1 the circle is then available to other recognizers  e.g.  the pulley recognizer.
1	reasoning
in the second stage the system scores each interpretation using a variety of different sources of knowledge that embody heuristics about how people draw and how mechanical parts combine.
temporal evidence
people tend to draw all of one object before moving to a new one. our system considers interpretations that were drawn with consecutive strokes to be more likely than those drawn with non-consecutive strokes.
모additional evidence comes from  longevity:  the longer a figure stays unchanged  the stronger its interpretation becomes  because as time passes it becomes more likely that the figure is not going to be turned into anything else by additional strokes.
simpler is better
we apply occam's razor and prefer to fit the fewest parts possible to a given set of strokes. for example  any polygonal body  e.g.  the car body in figure 1  could have been interpreted as a set of  connected  individual rods  but the system prefers the interpretation  body  because it fits many strokes into a single interpretation.
more specific is better
our system favors the most specific interpretation. circles  for example   currently  have three interpretations: circular bodies  pin joints  and the  select  editing gesture. the selection gesture is the most specific interpretation  in the sense that every circle can be a circular body or pin joint  but not every circle can be a selection gesture  e.g.  if it does not encircle any objects . hence when a circle contains objects inside of it  the system prefers to interpret it as a selection gesture.
domain knowledge
assist uses basic knowledge about how mechanical components combine. for example  a small circle drawn on top of a body is more likely to be a pin joint than a circular body.
user feedback
user feedback also supplies guidance. the  try again  button  see the bottom of figure 1  permits the user to indicate that something was recognized incorrectly  at which point the system discards that interpretation and offers the user an ordered list of alternative interpretations. conversely the system can be relatively sure an interpretation is correct if the user implicitly accepts it by continuing to draw.
combining evidence
the heuristics described above all independently provide evidence concerning which interpretation is likely to be correct. our method of combiningthese independentsources involves distinguishing between two categories of evidence: categorical and situational.
모categorical evidence ranks interpretations relative to one another based on the first four knowledge sources described above. each source is implemented in the system as a set of rules that takes two interpretations as input  and outputs an ordering between them. in processing figure 1  for example  the interpretation  body  is ranked higher than the interpretation  connected rods   based on the  simpler is better  heuristic.
모situational evidence comes from implicit and explicit feedback from the user. explicit feedback is provided by use of the  try again  button; implicit feedback arises when the user keeps drawing after the system displays an interpretation  suggesting that the user is satisfied that the system has understood what has been drawn so far.
모the system gives each interpretation two numeric scores  one from each category of evidence. the categorical score is an integer from 1 to 1; the situational score is an integer from -1 to 1. these values are chosen so that the situational dominates the categorical  because we want user feedback to dominate general ranking rules. an interpretation's total score is simply the sum of its two scores.
모to convert categorical evidence to a numerical score  so it can be combined it with the situational score   we generate a total ordering of all the interpretations consistent with the partial orders imposed by the categorical evidence. we do a topological sort of the graph of partial orders produced by the evidence and distribute scores evenly  from 1 to 1  over all the interpretations in the sorted graph.1
모situational scores start out at 1 and are strengthened or weakened by evidence that can raise of lower the current value by 1 or by 1. situational evidence thus either modifies an interpretation's value by a small amount  1 unit  or declares it to be certainly correct or certainly incorrect. the system declares an interpretation to be certainly correct or certainly incorrect when the user explicitly accepts or rejects the interpretation using the  try again  dialog box. the system strengthens an interpretationby a small amount each time strokes added by the user are consistent with that interpreta-
tion.1
모we developed this approach to accumulating and combining evidence  and implemented our knowledge sources as a rule based system  in order to provide a degree of modularity
	a	b

figure 1: a recognition graph for four strokes; scores are shown at the left of each interpretation.
to the system. our overall approach to the problem is to take into account as many sources of knowledge as prove useful in interpreting the sketch. we knew that it would be impossible to identify and implement them all at the outset  hence our design put a high premium on the ability to add and remove sources of evidence easily.
1	resolution
the third stage in the interpretation process involves deciding which interpretation is currently the most likely. our system uses a greedy algorithm  choosing the interpretation with the highest total score  eliminating all interpretations inconsistent with that choice  and repeating these two steps until no more interpretations remain to be selected.
모the process is illustrated by the interpretationgraph in figure 1  which shows in graphical form all of the possible interpretations of four strokes  the top row of ovals : 1 separate lines  1 rods  a quadrilateral  rectangle  or square. the rod on the left has the highest score  so it is chosen as a correct interpretation for stroke a. choosing that interpretation eliminates the interpretations of quadrilateral  rectangle or square  because stroke a is needed in any of these interpretations. in this context the other strokes are interpreted as rods because that interpretation has the highest score of any remaining interpretation.
모recall that our interpretation process is continuous: all three stages of processing occur after every new stroke is added to the sketch  and the current best interpretation as selected by the greedy algorithm is presented to the user. the process tends to settle down reasonably quickly  in part because  as noted  we reward longevity. hence once an interpretation has been presented to the user and unchanged for some period of time  it becomes increasingly unlikely to change.

figure 1: a scale.

figure 1: a rube-goldberg machine. the ball rolling down the incline sets in motion a sequence of events that eventually pushes the block at the right into the receptacle at bottom right. the device is an adaptation of the one found in  narayanan  1 .
1	evaluation and results
our initial evaluation of assist has focused on its naturalness and effectiveness. we asked subjects to sketch both on paper and using assist. we observed their behavior and asked them to describe how assist felt natural and what was awkward about using it.
모we tested the system on eleven people from our the laboratory  two of whom had mechanical engineering design experience. all were asked first to draw a number of devices on paper  figures 1  1  1   to give them a point of comparison and to allow use to observe differences in using the two media.
모they were then asked to draw the same systems using assist  they drew with a wacom pl-1 tablet  an active matrix lcd display that allows users to sketch and see their strokes appear directly under the stylus . we asked them how often they felt the system got the correct interpretation and how reasonable the misinterpretations were  and asked them to compareusing our system to drawing on paper and to using a menu-based interface.
모the system was successful at interpreting the drawings despite substantial degrees of ambiguity  largely eliminating the

figure 1: a circuit breaker.
need for the user to specify what he was drawing. as a consequence  a user's drawing style appeared to be only mildly more constrained than when drawing on paper.
모people reported that the system usually got the correct interpretation of their sketch. where the system did err  examination of its performance indicated that in many cases the correct interpretationhad neverbeen generatedat the recognition step  suggesting that our reasoning heuristics are sound  but we must improve the low-level recognizers. this work is currently under way.
모users tended to draw more slowly and more precisely with assist than they did on paper. the most common complaint was that it was difficult to do an accurate drawing because the system changed the input strokes slightly when it re-drew them  to indicate its interpretations . users felt that the feedback given by assist was effective but at times intrusive. our next generation of the system leaves the path of the strokes unchanged  changing only their color to indicate the interpretation.
모for a more complete discussion responses to the system from a user interface perspective  see  alvarado and davis  1 .
1	related work
the electronic cocktail napkin  ecn  project  do and gross  1; gross  1  attacks a similar problem of sketch understanding and has a method for representing ambiguity. our system takes a more aggressive approach to ambiguity resolution and as a result can interpret more complicated interactions between parts. in order for ecn to to resolve ambiguity  the user must either inform the system explicitly of the correct interpretation  or the system must find a specific higher-level pattern that would provide the context to disambiguate the interpretation of the stroke. our system  in contrast  takes into account both drawing patterns and knowledge of drawing style.
모 mankoff et al.  1  presents a general framework for representing ambiguity in recognition-based interfaces. this work is similar in using a tree-like structure for representing ambiguity  but touches only briefly on ambiguity resolution. our work pushes these ideas one step further within the domain of mechanical engineering by providing a framework and set of heuristics for ambiguity resolution.
모silk  landayand myers  1 allows a user to sketch out rough graphical user interface designs  then transform them into more polished versions. silk addresses the notion of ambiguity  but limits its handling of it to single parts  e.g.  is this group of strokes a radio button or a check box  this does not in general affect the interpretation of the other strokes in the sketch. in contrast  our system can resolve ambiguities that affect the interpretation of many pieces of the sketch.
모a theoretical motivation to our work was providedby work in  saund and moran  1   which outlines several goals in interpreting ambiguous sketches. our work implements many of the multiple representation and disambiguation techniques suggested in their work.
모we have also been motivated by work in mechanical system behavior analysis  especially in the field of qualitative behavior extraction and representation  sacks  1; stahovich et al.  1 . the work by stahovich aims to extract the important design constraints from the designer's rough sketch and is less focused on the interface or sketch recognition process. it was nevertheless the inspiration for our work in this area.
1	future work
the work presented in this paper is a first step toward creating a natural interface. it can usefully be expanded in several areas.
모first  our current formulation of recognition and evidential reasoning is of course quite informal. this is a consequence of our focus at this stage on the knowledgelevel  i.e.  trying to determine what the program should know and use to evaluate interpretations. once the content has become more stable and better understood  a more formal process of evaluation and control  e.g.  bayes' nets  may prove useful both for speed and scaling.
모second  in our efforts to combine the best properties of paper and the digital medium we have yet to find many of the appropriate trade-off points. how aggressive should the system be in its interpretations  forcing the user to correct the system immediately when it makes a mistake greatly aids recognition  but may distract the designer by forcing her to focus on the system's recognition process rather than on the design. in addition  some ambiguities are resolved as more of the sketch is drawn  yet if the system waits for the sketch to be finished  unraveling an incorrect interpretations can be a great deal of work.
모in the same vein  it will be important to calibrate how important true freehand sketching is to designers. the obvious alternativeis a icon-basedsystem with graphicaleditingcapabilities  e.g.  moving and resizing the standard components . freehand drawing can be powerful  but alternative interface styles need to be considered as well.
모the system should also adapt to new users and their sketching style. for example  one of our heuristics was that people draw all of one object before moving onto the next  but there are of course exceptions. the system should be able to adjust to this type of behavior and learn to override its default heuristic.
1	conclusion
cad systems are rarely used in early design because they do not allow for quick and natural sketching of ideas. to be useful here  computers must allow the designer to sketch as on paper  yet provide benefits not available with paper  such as the ability to simulate the system.
모to provide an interface that feels natural yet interprets sketches as the user draws  the system must be able to resolve ambiguities without interrupting the user. this work provides one solution to problem of ambiguity resolution in a framework of reasonable generality.
acknowledgments
luke weisman and mike oltmans helped extensively in the development of these ideas and this system. the work was supported in part by the national science foundation  the mit/ford motor company collaboration  and mit's project oxygen.
references
 alvarado and davis  1  christine alvarado and randall davis. preserving the freedom of sketching to create a natural computer-based sketch tool. in human computer interaction international proceedings  1.
 do and gross  1  ellen yi-luen do and mark d. gross. drawing as a means to design reasoning. ai and design  1.
 gross and do  1  mark gross and ellen yi-luen do. ambiguous intentions: a paper-like interface for creative design. in proceedings of uist 1  pages 1  1.
 gross  1  mark d. gross. recognizing and interpreting diagrams in design. in 1nd annual international conference on image processing  pages 1  1.
 gross  1  mark d. gross. the electronic cocktail napkin - a computational environmentfor working with design diagrams. design studies  1-1  1.
 hearst  1  marti hearst. sketching intelligent systems. ieee intelligent systems  pages 1  may/june 1.
 landay and myers  1  james a. landay and brad a. myers. sketching interfaces: toward more human interface design. ieee computer  1 :1  march 1.
 mankoff et al.  1  jennifer mankoff  scott e hudson  and grefory d. abowd. providing intergrated toolkit-level support for ambiguity in recogntiion-based interfaces. in proceedings of the chi 1 conference on human factors in computing systems  pages 1  1.
 narayanan et al.  1  n. hari narayanan  masaki suwa  and hiroshi motoda. behavior hypothesis from schematic diagrams  chapter 1  pages 1. the mit press  cambridge  massachusetts  1.
 oltmans  1  michael oltmans. understanding natually conveyedexplanations of device behavior. master's thesis  massachusetts institute of technology  1.
 rubine  1  dean rubine. specifying gestures by example. computer graphics  pages 1  july 1.
 sacks  1  elisha sacks. automated modeling and kinematic simulationof mechanisms. computer-aideddesign  1 :1  1.
 saund and moran  1  eric saund and thomas p. moran. perceptual organization in an interactive sketch editing application. in iccv 1  1.
 sezgin    metin sezgin. early processing in sketch understanding. unpublished master's thesis  massachusetts institute of technology.
 stahovich et al.  1  t. stahovich  r. davis  and h.shrobe. generalting multiple new designs from a sketch. artificial intelligence  1-1 :1  1.
 ullman et al.  1  david g. ullman  stephan wood  and david craig. the importance of drawing in mechanical design process. computer & graphics  1 :1  1.

robotics and perception
vision i

vambam: view and motion -based aspect models                         for distributed omnidirectional vision systems 
hiroshi ishiguro* and takuichi nishimura** 
*department of computer & communication sciences  wakayama university  japan 
** cyber assist research center   
national institute for advanced industrial science and technology  aist   *ishiguro sys.wakayama-u.ac.jp  ** t nishimura aist.go.jp 

abstract 
this paper proposes a new model for gesture recognition. the model  called view and motion -based aspect models  vambam   is an omnidirectional view-based aspect model based on motion-based segmentation. this model realizes location-free and rotation-free gesture recognition with a distributed omnidirectional vision system  dovs . the distributed vision system consisting of multiple omnidirectional cameras is a prototype of a perceptual information infrastructure for monitoring and recognizing the real world. in addition to the concept of vabam  this paper shows how the model realizes robust and real-time visual recognition of the dovs. 
1 introduction 
an open problem of visual recognition is  segmentation  of sensory data. based on marr's paradigm  marr  1   many researchers have tried to build recognition systems using 1-d models as the intermediate representations. there are mainly two methods. one is to detect visual features such as line segments and surfaces bounded by the lines and then to directly fit wire-frame models to the detected features or by estimating 1-d positions of the features by using multiple views. another more powerful method is to detect targets by multiple-camera stereo  kanade  xxxx . the target segmentation by cameras surrounding the target is robust against lighting conditions. a problem of the former method is in the feature segmentation in the image. it has a serious problem for deformable targets  such as humans. a problem of the later method is in the computational const. even if we use the most powerful computer  it is difficult to compute in real-time. 
모모모모모the purpose of this paper  is to develop a real-time vision system that tracks and 
fig. 1: omnidirectional 	recognizes human behaviors camera 
 
fig. 1: distributed omnidirectional vision system 
in a wide area. therefore  the methods based on the 1-d models are not suitable for our purpose. we  rather  take a new but well-known memory-based approach for the segmentation problem. recent progress of computer hardware enabled us to use a large volume of memory and to access in real time. this paper follows the basic idea and proposes a general framework of visual recognition in a wide environment.  
 
모모the system introduced in this paper consists of omnidirectional cameras. each camera has an omnidirectional visual field and observes a wide range. fig. 1 shows a compact omnidirectional camera  ishiguro  1 . by using multiple omnidirectional cameras  the system redundantly covers the wide environment as shown in fig. 1. we call it distributed omnidirectional vision system  dovs . the goal of our research is to develop a perceptual information infrastructure  ishiguro  1  by using the distributed omnidirectional vision system as an extension of computer and sensor networking. 
모모the key function of the dovs is to track walking humans and to recognize the gestures in real time. that is  it is location-free and rotation-free gesture recognition. this paper proposes a new model of visual recognition for realizing the function. we call the model view and motion -based aspect model  vambam . the model has two features as follows: 

fig. 1: modeling and recognition 
1. omnidirectional model of a gesture 
1. motion-based segmentation of the gesture 
모모as shown in fig. 1  the multiple cameras observe a human from various viewing point to build the vambam. then  the system recognizes human gestures in the distributed vision system that consists of multiple cameras distributed in the environment. a vambam maintains the visual information in a form of view-based aspect model. in the modeling  a gesture is observed as image sequences. then  the image sequence is transformed to a feature vector that represents an aspect of the gesture based on the motion information.  thus  vambam maintains omnidirectional and motion-based gesture information. this gesture recognition not using 1-d models has not been developed so far  pavlovic  1 . 
 
모모in the following sections  section 1 explains the design policies of vambam. to utilize vambam  we have developed a 1-d dynamic programming matching. section 1 describes the detail. finally  section 1 reports experimental results and effectiveness of vambam.  
1 vambam 
1 omnidirectional aspect model 
the system has two phases for recognizing human gestures: modeling phase and recognition phase  see fig. 1 . in the modeling phase  the system acquires vambams by using 1 cameras surrounding a human. the human makes several gestures and the system memorizes each gesture as an individual vambam.  in the recognition phase  the system consisting of multiple omnidirectional cameras finds a human  tracks  and then matches acquired images from several cameras with the stored vambams. this section explains the modeling phase. 
 
fig. 1: weighted 1 feature vectors 
 
모모an ideal model for object recognition is to have all visual information from all directions like a complete 1d model. if there are an infinite number of cameras surrounding an object  we can directly acquire such an ideal view-based model. vambam is an approximation of such an ideal model. 
모모a problem is how to reduce the number of cameras surrounding a modeling target in the modeling phase. it depends on complexity of the target. since the purpose of the dovs is to recognize coarse human gestures  we have decided the number as 1 based on a preparatory experiment.  
모모by using 1 cameras  the system acquires 1 view sequences and transforms them to 1 feature vectors. in fig. 1  the sphere shows a feature vector. here  we suppose all of the cameras are located at the same height. of course  we can extend the camera arrangement  but it is not needed for our current gesture recognition. 
모모all 1 cameras do not have the same priority. for recognizing a particular gesture  for example breathing gesture  the front camera is more important than the side cameras. therefore  each feature vector has a weight estimated from the motion information. if the camera observes more moving pixels by background subtraction than the others  it has a bigger weight. fig. 1 shows the conceptual figure of the weight vectors. the weights define aspect changes in vambam. fig. 1 shows 1 feature vectors that have positive weights  and it represents 1 aspects. the system efficiently refers to the weight for searching the target in the recognition phase. 
 
fig. 1: 1 feature vectors 	fig. 1: full vambam 모모the vambam shown in fig. 1 is obtained by observing the target with a constant distance. if the camera is distant from the object  the aspect also changes. bobick and bolles  bobick  1  reported an important idea that the model structure changes according to the resolution of a vision sensor. we follow the idea and 

 
fig. 1: several approaches for visual modeling 
extend a vambam to a full vambam as shown in fig. 1. the full vambam has several layers to handle different resolutions. in fig. 1  the lateral circle represents the coarsest resolution  or distant observation . although we have not developed the system using the full vambam  this concept is very important for our future extension of the basic model. 
 
모모here  let us state vambam as an aspect model. aspect models represent omnidirectional visual information surrounding the target. koenderink  koenderink  1  proposed the original concept based on 1-d structure of the object. ikeuchi  ikeuchi  1  modified for passive vision systems and defined the aspect change by the number of observable surfaces of the object. in their works  the important point is that the aspect model maintains omnidirectional visual information for the generality. on the other hand  view/appearance-based methods and parametric methods are also a kind of aspect model.  the view-based method defines the aspect change as a difference among views by template matching or featurebased comparison. the parametric method defines the aspect change as a distance in the principle component parameter space. they  however  do not maintain the omnidirectional visual information in the previous works. vambam is a view-based approach as described in the next subsection  but it is also an aspect model that maintains omnidirectional visual information. fig. 1 shows relationships among the existing methods and our method. 
1 motion-based segmentation 
another important feature of vambam is motion-based segmentation of visual information. as mentioned before  the motion-based segmentation reduces the amount of memory and enables us to utilize omnidirectional view-based aspect models for practical systems.  
모모in fig. 1  each feature vector is given as shown in fig. 1. the camera finds a moving object by background subtraction and tracks it as a candidate of a human. then  it detects the center of gravity by making histograms along vertical and horizontal axes. the histograms give the height and width of the human. based on the parameters  the system defines 1 regions that cover the 
 
fig. 1: acquisition of a feature vector 
human as shown in the left of fig. 1  tracks the human fitting the regions. in each region  the system recodes the moving pixels detected by background subtraction and acquires 1 motion vectors in 1 sec. observation. here  the observation length is adjusted according to the gesture. the acquired 1 motion vectors define a feature vector as represented with a sphere in fig. 1. as mentioned before  several feature vectors do not contains motion information because of the viewing direction. by referring to the average and variance of the 1 motion vectors  the system assigns a weight to the feature vector. 
모모the motion-based segmentation by background subtraction is one of the most robust and stable feature extractions and many practical systems use background subtraction. if we consider that  motion  represents relationships between static objects and events and gives meaning to the static objects  it is natural to build a recognition system based on the motion-based feature extraction. 
모모further  this representation of a gesture is compact. suppose to record a 1 sec. gesture. a motion vector consists of 1뫄1 integers and 1 feature vectors have 1뫄1뫄1뫄1 = 1 integers and the quarter is selectively memorized by the weights. if we represent an integer with 1bit  the necessary memory size will be 1 bit and it is not large for recognizing a dozen of gestures in real time. 
1 two techniques to support vambam 
1 real-time tracking of walking humans 
for realizing the location-free and direction-free gesture recognition  the system needs two more functions: one is tracking of walking humans in real time  the other is a robust matching method with gesture models. 
모모by utilizing redundant visual information of the dovs  we can build a robust and real-time multiple camera stereo system. this stereo  called n-ocular stereo  is an extension of trinocular stereo  kitamura  1 . the basic process of n-ocular stereo is as follows: 
 
fig. 1: n-ocular stereo by a dovs 
1. detect moving regions on the images by back-
 
fig. 1: matching with a vambam 

ground subtraction. 
1. for all combination of two moving regions on different omnidirectional cameras  estimate the distance to the region and apply a circle as a human model  the circles in fig. 1 . 
1. check the overlapping of the circles and determine human locations. 
모모as described above  we implement the n-ocular stereo as a model-based stereo method. generally  the model-based stereo is more stable than the feature-based stereo in the case where the target is known. however  appearance of a human body frequently deforms on the image. therefore  we have approximated the human body with a circle  the diameter is 1 cm  on the horizontal plane that is parallel to the floor. when three or more circles overlap each other  the system decides a human exists in the center of gravity of the circles.  sogo  1  sogo  1 . 
모모this n-ocular stereo for tracking humans in a dovs is executed in real-time with a standard computer and it robustly tracks up to 1 humans simultaneously with four omnidirectional cameras. the reason of the robustness is in the redundant observation by multiple omnidirectional vision sensors. the method based on background subtraction is  generally speaking  noisy against change of lighting condition. however  the multiple observations from different viewing angles suppress the noise.  
모모another issue in the n-ocular stereo is precise localization and identification of the omnidirectional cameras. for this issue  we have already developed a method that automatically performs the localization and identification  kato  1 . 
 
1 1-d dynamic programming for matching 
the recognition system needs to handle multiple gesture models simultaneously. therefore  a robust and real-time matching technique is required.  
모모the basic idea is to use conventional continuous dynamic programming  cdp . we have extended this cdp to interpolate discrete observation by 1 cameras  which is called spatio-temporal continuous dynamic programming  stcdp  see fig. 1 . in the recognition phase  a stored model does not exactly match to the observations by the dovs as shown in fig. 1. the stcdp searches the best match by swiveling the model within 1 degrees. in addition to this  the stcdp finds the direction of target by rotating the models 1 degrees. thus  this 1-d matching realizes the rotationfree gesture recognition by using the discrete model. 
모모by taking account of all conditions in the matching  we formalize the stcdp as follows. let us denote the cumulative distance of a point  iq t  t  for evaluation of the dp matching as s iq t  t  . suppose the q th feature vector is rotated iq up to nq and the model length is t. the stcdp calculates s iq t  t  by using the following recursive equations. 
 
fig. 1: stcdp 
 
fig. 1: attention control in the matching 
boundary conditions 1뫞iq 뫞 nq  1뫞t 뫞t 1 뫞 t : s iq  1 t  = 
s 1 t  t  = s nq +1 t  t  = 
s iq t   1  = s iq t  1  =  
recursive equations  1뫞 t  : s iq 1 t  = 1 d iq 1 t  
s iq t  t  = 
	 	s iq t  1 t  1 +1 d iq t  t  1 +d iq t  t 
	  	s iq t  1 t  1 +1 +d iq t  t 
	 	s iq t  1 t  1 +1 d iq t  1 t +1 d iq t  t 
min     s iq  1 ts  iq1  t1  t1  +1t  d1  iq+ 1  +td t i q1 t  +t d iq t  t 
  s iq  1 t  1 t  1 +1 d iq  1 t  1 t +1 d iq t  t 
  s iq +1 t  1 t  1 +1 d iq +1 t  t  1 +d iq t  t 
	  	s iq +1 t  1 t  1 +1 +d iq t  t 
  s iq +1 t  1 t  1 +1 d iq +1 t  1 t +1 d iq t  t 
 1 뫞t 뫞t  
 
모모in the current implementation  the stcdp sequentially matches the input with 1 stored models. however  the stcdp needs to be modified to handle a large number of models. the weights assigned to the vambam reduce the computational time drastically. by referring to the weights  the stcdp can dynamically select possible feature vectors and ignore redundant search for impossible models in the early stage as shown in fig. 1. in other words  we can see attention control in the recognition phase using vambam. 
1 experimental results  
for the modeling  we have arranged 1 cameras along a circle of 1m diameter and acquired ten model gestures:  a  deep-breathing   b  turning around the upper body   c  kicking   d  throwing   e  jumping   f  opening two legs   g  picking up   h  sitting down   i  bowing   j  sumo-stomping. fig. 1 shows the modeling system.  
모모then  we have verified the performance of the proposed method by using 1 omnidirectional cameras. fig. 1 shows the top view of 1 cameras tracking a walking human and fig. 1 shows a part of the trajectory. fig. 1 shows an image sequence while the system recognizes  kick  gesture. fig. 1 shows the monitor output of the system. 
모모as shown in table 1  we could verify good performance of the system. in the recognition phase  we have prepared different humans with the modeling phase and they have randomly iterated the 1 gestures 1 times. 
 
fig. 1: modeling system 
 
fig. 1: tracking of a walking human 
 
fig. 1: trajectory of the walking human  
when the human behaves a gesture at an arbitrary location with an arbitrary orientation  the recognition results using stcdp was 1% with 1 cameras as shown in table 1 a . further  a human can behave similar behavior even if he/she is moving. in the case  the performance was 1%. in this experimentation  the system has recognized in real time. 
모모the purpose of this system is to realize gesture recognition in a wide area. although the experimental results are not sufficient  we could verify the basic function. 
 a  recognition results when the human stops 
 cdp stcdp 1 camera 1% 1% 1 cameras 1% 1%  b  recognition results when the human moves 
 cdp tscdp 1 camera 1% 1% 1 cameras 1% 1% table 1: recognition results  
1 conclusions 
this paper has proposed a new model for gesture recognition  called vambam. by using the model for the distributed omnidirectional vision system  we have developed a location-free and rotation-free gesture recognition system.  
모모there are several remained problems yet. one is resolution of the gesture recognition. the current system handles only large gestures using arms and legs. however  we consider high-resolution cameras for the omnidirectional cameras can solve this problem. another problem is the discrete omnidirectional models using 1 cameras. a better method is to acquire continuous models from the discrete observation and search by 1-d dynamic programming. we are now improving the algorithm. 
모모the model  vambam  is simple but general. therefore  we consider this can be extended and realize more sophisticated recognition systems that can recognize more complex scenes  such as interactions among humans. our final goal is to develop a perceptual information infrastructure to support human activities based on the proposed method. 
references 
 bobick  1  a. bobick and r. c. bolles  the representation space paradigm of concurrent evolving scene descriptions  ieee trans. pami  vol. 1  no. 
1  pp. 1  1.  
 ikeuchi  1  k. ikeuchi  automatic generation of object recognition program  proc of ieee  vol. 1  no. 1  1. 
 ishiguro  1  h. ishiguro  distributed vision system: a perceptual information infrastructure for robot navigation  proc. ijcai  pp. 1  1. 
 ishiguro  1  h. ishiguro  development of low-cost and compact omnidirectional vision sensors and their applications  proc. int. conf. information systems  analysis and synthesis  pp. 1  1.  
 kato  1  k. kato  h. ishiguro and m. barth  identifying and localizing robots in a multi-robot system environment  proc. iros  1. 
 kitamura  1  y. kitamura and m. yachida  threedimensional data acquisition by trinocular vision  advanced robotics  vol.1  no.1  pp. 1  1.  
 
fig. 1: an image sequence of   kick  gesture  

fig. 1: monitor output of the system 
 koenderink  1  j.j. koenderink and a.j. van doorn   the internal representation of solid shape with respect to vision  biological cybernetics  1-1  
1 
 marr  1  d. marr  vision: a computational investigation into the human representation and processing of visual information  freeman  w. h. and company  1. 
  pavlovic  1  v.i. pavlovic  r. sharma and t.s. huang  visual interpretation of hand gestures for human-computer interaction: a review  ieee trans. pami  vol. 1  no. 1  pp. 1  1.   
 sogo  1  t. sogo  h. ishiguro and m. trivedi  real-time target localization and tracking by n-ocular stereo  ieee workshop on omni. vision  pp. 1  1. 
 sogo  1  t. sogo  h. ishiguro and m. trivedi  n-ocular stereo for real-time human tracking  in r. benosman and s. b. kang eds.  panoramic vision: sensors  theory and applications  springer-verlag  berlin  1.  to appear . 
efficient interpretation policiesramana isukapalli
1 crawfords corner road lucent technologies
holmdel  nj 1 usa ramana research.bell-labs.com
russell greiner
department of computing science university of alberta edmonton  ab t1g 1 canada greiner cs.ualberta.ca

abstract
many imaging systems seek a good interpretation of the scene presented - i.e.  a plausible  perhaps optimal  mapping from aspects of the scene to real-world objects. this paper addresses the issue of finding such likely mappings efficiently. in general  an   interpretation  policy  specifies when to apply which  imaging operators   which can range from low-level edge-detectors and region-growers through highlevel token-combination-rules and expectation-driven objectdetectors. given the costs of these operators and the distribution of possible images  we can determine both the expected cost and expected accuracy of any such policy. our task is to find a maximally effective policy - typically one with sufficient accuracy  whose cost is minimal. we explore this framework in several contexts  including the eigenface approach to face recognition. our results show  in particular  that policies which select the operators that maximize information gain per unit cost work more effectively than other policies  including ones that  at each stage  simply try to establish the putative most-likely interpretation.
keywords: vision  decision theory  real time systems
1	introduction
interpretation i.e.  assigning semantically meaningful labels to relevant regions of an image  is the core process underlying a number of imaging tasks  including recognition   is objectx in the image    and identification   which object is in the image     as well as several forms of tracking   find all moving objects of typex in this sequence of images    etc.  pl1; hr1 . of course  it is critical that interpretation systems be accurate. it is typically important that the interpretation process also be fast: for example  to work in real-time  an interpreter examining the frames of a motion picture will have only 1 of a second to produce an interpretation. or consider a web-searcher that is asked to find images of  say  aircraft. here again speed is critical - and most searchers do in fact sacrifice some accuracy to gain efficiency  i.e.  they quickly return a large number of  hits   only some of which are relevant . this paper addresses the challenge of producing an interpreter that is both sufficiently accurate and sufficiently efficient.
모section 1 provides the framework  showing how our framework generalizes the standard  classical  approaches to image interpretation  then providing a formal description of our task: given a distribution of possible images and an inventory of  operators   produce a  policy  that specifies when to apply which operator  towards optimizing some user-specified objective function. it describes three different policies that could be used  using a simple blocksworld example to illustrate these terms. the rest of this paper demonstrates that one of these policies   infogain   which uses information gain to myopically decide which operator is most useful at each step   is more effective than the other obvious contenders. section 1 provides an empirical comparison of these approaches in the context of the simple blocks-world situation. section 1 extends these ideas to deal with face recognition  using the modern eigenvector approach.  this complements section 1's classical approach to interpretation.  section 1 quickly surveys some related work. while the results in this paper demonstrate the potential for this approach  they are still fairly preliminary. section 1 discusses some additional issues that have to be addressed towards scaling up this system.
1	framework
1	standard approaches
there are many approaches to scene interpretation. a strictly  bottom-up  classical approach performs a series of passes over all of the information in the scene  perhaps going first from the pixels to edgels  then from edgels to lines  then to regions boundaries and then to descriptions  until finally producing a consistent interpretation for the entire scene. most  top down   or  model driven   systems likewise begin by performing several bottom-up  sweeps  of the image - applying various low-level processes to the scene to producean assortmentof higher-leveltokens  which are then combined to form some plausible hypothesis  e.g.  that the scene contains a person  etc. . these systems differ from strictly bottom-up schemes by then switching to a  top-down  mode: given sufficient evidence to support one interpretation  they seek scene elements that correspond to the parts of the proposed real-world object that have yet to be found  lab1 .
모notice the model-based systems have more prior knowledge of the scene contents than the strictly bottom-up schemes - in particular  they have some notion of  models  - which they can exploit to be more efficient. we propose going one step further  by using additional prior knowledge to further increase the efficiency of an interpretation system. consider a trivial situation in which we only have to determine whether or not a  red fire-engine  appears in an image; and imagine  moreover we knew that the only red object that might appear in our images is a fire-engine. here it is clearly foolish to worry about line-detection or region-growing;it is sufficient  instead  to simply sweep the image with an inexpensive  red  detector. moreover  if we knew that the fireengine would only appear in the bottom third of the image  we could apply this operator only to that region.
모this illustrates the general idea of exploiting prior knowledge  e.g.  which objects are we seeking  as well as the distribution over the objects and views where they might appear  to produce an effective interpretation process. in general  we will assume our interpretation system also has access to the inventory of possible imaging operators. given this collection of operators  an  interpretation policy  specifies when and how to apply which operator  to produce an appropriate interpretation of an image.
모our objective is to produce an effective interpretation policy - e.g.  one that efficiently returns a sufficiently accurate interpretation  where accuracy and efficiency are each measured with respect to the underlying task and the distribution of images that will be encountered. such policies must  of course  specify the details: perhaps by specifying exactly which bottom-up operators to use  and over what portion of the image  if and when to switch from bottomup to top-down  which aspects of the model to seek  etc. these policies can include  conditionals ; e.g.   terminate on finding a red object in the scene; otherwise run procedure  . they may also specify applying a particular operator only to specified regions of the image  e.g.  seek only near-horizontal edges  only in the upper left quadrant of the image . based on the information available  this interpretation policy could then use other operators  perhaps on other portions of the image to further combine the tokens found.
1	input
we assume that our interpretation system     is given the following information:
   the distribution of images that the will encounter  encoded in terms of the distribution of objects and views that will be seen  etc.  here we assume this information is explicitly given to the algorithm; we later consider acquiring this by sampling over a given set of training images. 
모as a trivial example  we may know that each scene will contain exactly sub-objects  each occupying a cell in a grid; see figure 1. each of these cells has a specified  color    texture  and  shape   and each of these properties ranges over values.  hence  we can identify each image with a tuple of values  where each value is from .  moreover  our knows the distribution over these possible images; see below.
   the task includes two parts: first  what objects the should seek  and what it should return - e.g.   is there an airplane in image  or  is there a dc1 centered at in the image   etc. in our trivial blocks-world case  we simply want to know which of the images is being examined.
모second  the task specification should also specify the  evaluation criteria  for any policy  which is based on both the expected  accuracy  and its expected  cost . in general  this will be a constrained optimization task  combining both hard constraints and an optimization criteria  e.g.  minimize some linear combination of accuracy and cost  or perhaps maximize the likelihood of a correct interpretation  for
        모모모figure 1: a simple image of 1 sub-objects a given bound on the expected cost .
모for this blocks-world task  we want to minimize the expected cost and also have correctness  assuming the operators are perfect.
   the set of possible  operators  includes  say  various edge detectors  region growers  graph matchers  etc. for each operator  we must specify
its input and output  of the form:  given a set of pixel intensities  returns tokens representing the regions of the same color ; its  effectiveness   which specifies the accuracy of the output  as a function of the input. this may be a simple  success probability   or could be of the form:  assuming noise-type   can expect a certain roc curve   rh1 ; its  cost   as a function of  the size of  its input and parameter setting.
when used  each operator may be given some arguments  perhaps identifying the subregion of the image to consider.
모here  we consider three operators:  resp.      for detecting the  value  of color  resp.   texture    shape  ; each mapping a location of the current image to a value in .  note that location is an argument to the operator.  we assume that each operator  when pointed at a particular  cell   will reliably determine the actual value of that property at the specified location  and will do so with unit cost.  section 1 considers less trivial operators. 
모for each situation  we assume ourinterpreterwill be given a series of scenes  but will always have the same objective and criteria; e.g.  it is expected to look for the same objects in each image  and has a single objective function.  it is easy to generalize this to deal with environments that can ask different questions for different images  and impose different costs. 
모at each stage  our will use its current knowledge  both prior information - e.g.  associated with the distribution and the operators - and information obtained by earlier probes  to decide whether to terminate  returning some interpretation; or to perform some operation  which involves specifying both the appropriate operator and the relevant arguments to this operator  and then recur.
1	policies
in general  we could represent an explicitly as a large decision tree  whose leaf nodes each represent a complete interpretation  which is returned as the result of the    and whose internal nodes each correspond to a sequence of zero or more operator applications  followed by a test on the original data and/or some set of inferred tokens. each arc descending from this node is labeled with a possible result of this test  and descends to new node  containing other operators and tests  appropriate for this outcome.
	 	: taskspecification 
모모모: distribution 	: operators 	: image  initialize cost	evidence
	while  	obj	  do
	select	operator	arguments	 based on policy	 
	 note	may specify the region to consider 
	apply	to	  yielding
extend
	update	  based on result
	update	cost
	return best interpretation:	obj figure 1: identification algorithm 
for policy	randpol  besthyp  infogain
모given that such explicit strategy-trees can be enormous  we instead represent strategies implicitly  in terms of a  policy  that specifies how to decide  at run-time  which operator to use. figure 1 shows a general interpretation strategy using any of the policies. we will consider the following three policies:1
policy randpol: selects an operator1 randomly.
policy besthyp: first identifies the object that is most likely to be in the scene  given the evidence seen so far  weighted by the priors  etc.  and then selects the operator that can best verify this object  lhd 1  p1 : that is  after gathering information from previous operators	  it computes the posterior probabilities of each possible interpretation   . to select the next operator 
besthyp will first determine which of the scenes is most likely - i.e. 
- and then determines which operator has the potential of increasing the probability of this interpretation the most: assume the operator returns a value in
모모모모모모; then might increase the probability of to best	. here 
besthyp will use the operator
best
policy infogain: selects the operator that provides the largest information gain  per unit cost  at each time. this policy computes  for each possible operator and argument combination   the expected information gained by performing this operation: ig

figure 1: tail lights of chevrelot
where is the entropy of the distribution over the interpretations given the evidence   for or .
infogain then uses the operator
ig
that maximizes ig   where is the cost of applying the operator.
1	simple experiments: blocks world
this section presents some experiments using the simple blocks world situation presented above. they are designed simply to illustrate the basic ideas  and to help us compare the three policies described earlier. section 1 below considers a more realistic situation.
모we first generated a set of images  each with 1 subobjects  by uniformly assigning values for color  texture and shape to each of the sub-objects randomly  for each of images; we also assigned each a  prior distribution  to these images  this corresponds to taking an empirical sample  with replacement . for each run  we randomly select one of the images to serve as a target for identification  then used each of the three policies to identify the image.
모after observing from the operators in the first iterations  randpol randomly selects a cell and an operator to probe the value for a property  color  texture etc   insisting only that was not tried earlier on in any previous iterations of this run. besthyp chooses a cell and an operator to maximize the posterior probability of the most likely image  as explained earlier.
finally  infogain chooses and such that is the maximum over all possible
cell and operator combinations. for each of these policies  the posterior probabilityis updatedafter applyingthe chosen operator on the cell. the process is repeated until the image is identified - i.e.  all other contenders are eliminated.
we considered 1 set-ups  each with its own objects and
's   and performed 1 runs for each set-up.	over these
1 runs  randpol required on average	probes 
besthyp	probes and infogain	.
infogain is statistically better than the other two policies  at the level.
모we then performed a variety of other experiments in this domain  to help quantify the relative merits of the different policies - e.g.  in terms of the  average hamming distances  between the images. see  ig1  for details.
모while this particular task is quite simplistic  we were able to use the same ideas for the more interesting task of identifying the make and model of a car  e.g.  toyota corolla  nissan sentra  honda civic  etc.  given an image of the

figure 1: histogram of values; total height  left-size scale  reflects number of pairs in bucket .   feature; .  also shows   using right-size scale. car that shows its  rear tail lights assembly ; see figure 1. again see  ig1  for details.
1	scaling up: face recognition
we next investigate the efficiency and accuracy of the three policies in the more complicated domain of  face recognition   tp1; pms1; pwhr1; ec1 . this section first discusses the prominent  eigenface  technique of face recognition that forms the basis of our approach; then presents our framework  describing the representation and the operators we use to identify faces; then presents our face interpretation algorithm; and finally shows our empirical results.
1 eigenface  and eigenfeature  method many of today's face recognition systems use principal component analysis  pca   tp1 : given a set of training images of faces  the system first forms the covariance matrix of the images  then computes the main eigenvectors of   called  eigenfaces . every training face is then projected into this coordinate space   facespace    producing a vector 	.
모during recognition  any test face is similarly projected into the facespace  producing the vector   which is then compared with each of the training faces. the best matching training face is taken to be the interpretation  tp1 .
모following  pms1   we extend this method to recognize facial features - eyes  nose  mouth  etc. - which we then use to help identify the individual in a given test image: we first partition the training data into two sets 
 for constructing the eigenfeatures  and  for collecting statistics - see below   which each contains at least one face of each of the people. using id to denote the person whose face is given by   we have id
id for ; each remaining and     also maps to .
모we use pca on  say  the mouth regions of each image  to produce a set of eigenvectors; here eigen-mouths. for each face image   let be  feature space  encoding of 's mouth-region. we will later compare the feature space encoding of a new image against these vectors  with the assumption that suggests that is really person - i.e.  finding that

figure 1: training images  top ; test images  bottom 
모모모is small should suggest that id .  note refers to the norm  aka euclidean distance. 
to quantify how strong this belief should be  we com-
pute	values	where each
is the euclidean distance between the
 eigen-mouth encodings  of 's and 's . we considered buckets for these values:
	 	  ... 	 
	. then  for each bucket	  we estimate
	id	as
	id	id

	id	id
where id id is the number of pairs where is the same person as .  we used the obvious laplacian correction to avoid using s here  mit1 .  we also compute

to estimate .  note this is the average over all images in the test set .  figure 1 shows a histogram of the values  using the 1 buckets for the left eye feature  see below ; it also shows the values of for . we use these and values to interpret a new test image of a person's face .  while the specific image is not in   it is another face of someone who has other faces in  .  we first project
모모's mouth region onto the  eigen-mouth s space  forming the vector   then compare with the stored eigen-mouth projections  from   - computing the values for each in . this will be in some bucket  say . we then use bayes
rule to compute the probability that this face is person :
id
id
	id	id	 1 


 here  we assume the faces are drawnfrom the	individuals
uniformly; hence	id	. 
모so far  we considered only a single feature - here   mouth projections   as indicated by the superscript.
we similarly compute       values associated with the nose  left-eye and right-eye  as well as the
	 	 	values.
모we then used the na몮 ve-bayes assumption  dh1   that features are independent  given the specified person  to essentially simply multiply the associated probabilities: assume we observed  for each feature     then id
id
id
	id	id
		 1 
where	are scaling constants  as the prior is uniform .
모 note: we had also tried computing individual values  specific to each training face . however  we found this was too noisy  as the number of relevant instances was too small. 
1	framework
the distribution is the set of all people who can be seen  which varies over race  gender and age  as well as poses and sizes; we approximate this using the images given in the training set. we assume that any test face-image belongs to one of the people in the training set  but probably with a different facial expression or in a slightly different view  and perhaps with some external features not in the training image  like glasses  hat  etc.   or vice versa. figure 1 shows three training images  top  and four test images  bottom .
모our task is to identify the person from his/her given test image  wrt the people included in the training set   subject to the minimum acceptable accuracy     and the maximum total cost of identification    .
we use four classes of operators  to detect respec-
tively  left eye    right eye    nose  and  mouth . each specific operator also takes a parameter which specifies the size of the feature space to consider; here we consider
모모모모모모모모모모. as discussed above  each instantiated operator takes an input the test image of a face   and returns a probabilistic distribution over the individuals.
	each operator	 associated with the feature
               performs three subtasks: subtask#1 locates the feature from within the entire face . here we use a simple template matching technique in which we search in a fixed  window  of size pixels in for any given feature  of size by pixels.
subtask#1 then projects the relevant region of the test image into the feature space - computing of dimension . subtask#1 uses this to compute first the values for each person   then place each value into the appropriate bucket  and finally compute the probability id for each person   using equation 1  possibly augmented with equation 1 to update the distribution when considering the 1nd and subsequent features ; see section 1. for each eigenspace dimension   we empirically identified the cost
 in seconds  of the four classes of operators -
 	  and
모모모모모모모모모.while increasing the dimensionality of the feature space should improve the accuracy of the result  here we see explicitly how this will also increase the cost.
1	interpretation phase
during interpretation  each current policy  randpol 
besthyp and infogain  iteratively selects an operator
모모모모. randpol chooses an operator and a value randomly  subject only to the condition that had not been tried before on the image; besthyp first identifies the most likely person id   then choses the instantiated operator that best confirms this hypothesis  that the image belongs to person   provided this had not been used earlier for this
image; and infogain chooses an instantiated operator that has the maximum ig value.
in each case  the operator is applied to the appropriate region in the given test face image and the distribution is updated...until one face is identified with sufficiently high probability     or the system fails  by exhausting all the possible operators  or having cost  ; see figure 1.
1	face recognition experiments
we used face images of different people  each pixels  from which we placed images into   another images into   and are used in the training phase to collect statistics  and used the remaining as test images. as shown in figure 1  the faces are more or less in the same pose  facing front   with some small variation in size and orientation.1 we considered all operators based
on the four features listed aboveand for each feature.
basic experiment: we set	and
 i.e.  no upper limit on identification cost . in each  set-up   we assigned a random probability to each person. on each run  we picked one face randomly from the test set as the target  and identified it using each of the three policies. we repeated this for a total of runs per set-up  then changed the probability distribution and repeated the entire process again  for a total of set-ups. the cost of recognition on the average was   and

	figure 1:  a  cost vs. accuracy	 b  min. accuracy vs. error	 c  min. accuracy vs. costseconds for randpol  besthyp and infogain respectively. infogain is statistically better than the other two policies  at the	level.  as expected  these policies had comparable identification accuracy here:	  and	  respectively. 
bounding the cost: in many situations  we need to impose a hard restriction on the total cost; we therefore considered seconds. we then picked one face
randomly from the test set  and identified the test image for each of these maximal costs  using each of the three policies. as always  we terminate whenever the probability of any person exceeds or if the cost exceeds   and return the most likely interpretation.
모we repeated this experiment for a total of set-ups  each with a different distribution over the people  and with random runs  target face images  per set-up. the accuracy  the percentage of correct identifications  for each policy is shown for various values of in figure 1 a . infogain has better accuracy than both besthyp and rand-
pol. randpol trailed these two policies significantly for low   seconds  cost.
varying the minimum accuracy: in this experiment  we varied from to . for each of these values  we chose a face randomly from the test set as the target and identified it using each of the three policies. during the process  the first person in the training set for which id is returned  or if cost   the most probable face is returned . we repeated this for different target faces  runs  per set-up  and repeated the entire process for a total of different set-ups.
모we evaluated the results in two different ways. first  figure 1 b  compares the percentage of wrong identifications of each policy  for each value. infogain has fewer wrongidentifications than besthyp and randpol for low accuracy. as expected  for sufficiently high accuracy  all three policies have comparable number of wrong identifications. secondly  figure 1 c  compares the average cost of each policy  for each value. again  infogain has lower cost than besthyp and randpol  while randpol trails the other two policies significantly.
1	literature survey
our work formally investigates the use of decision theory in image interpretation  explicitly addressing accuracy versus efficiency tradeoffs  ge1; rsp1 . geman and jedynak  gj1  used information theoretic approaches to find the  optimal sequence of questions  for recognizing objects - hand-written numerals  1   and highways from satellite images. our work also uses information theoretic methods to compute the expected information gain  but we differ as we are seeking the most cost-effective sequence of interpretation operators  rather than the shortest sequence of questions; this forces us to explicitly address the cost vs accuracy tradeoffs. sengupta and boyer  sb1  presented a hierarchically structured approach to organizing large structural model-bases using an information theoretic criterion. we do not have explicit model-bases  but consider various interpretation policies that decide  at run time  which operators to apply to what regions of an image  based on expected information gain of the operators.
모we used the domain of face recognition to test our approach. while there are several approaches to face recognition  tp1; pms1; pwhr1; ec1   none explicitly address the issues of efficiency. we used one of the popular and successful methods as the basis to our approach and performed a systematic study of the various efficiency and accuracy related issues.
모levitt et al.  lab1; blm1; lhd 1  have applied bayesian inference methods and influence diagrams to image interpretation. we however provide a way to adjust the optimization function.  our work also further motivates the use of  maximum expected utility  in such systems. 
모as our system is seeking a policy that maps the state  here current distribution over possible interpretations  to an appropriate action  it can be viewed as solving a markov decision problem  mdp   which puts it in the realm of reinforcement learning; cf.   dra1 . our research objective differs as we are considering a range of reward functions  which can be various combinations of accuracy and efficiency  some of which may be difficult to word within a mdp framework . we anticipate being able to use many of the reinforcement learning techniques as we begin to consider interactions between the actions  and going beyond our current myopic approach.
모finally  there is a growing body of work on providing precise characteristics of various imaging operators  which quantify how they should work  har1; rh1 . we hope to use these results to quantify the effectiveness of our operators  to help our algorithms decide when to use each. there is also work on building platforms that allow a user to manually assemble these operators  fua1; pl1   oftenusing an expert-systemstyle approach mat1 .
here  we are taking a step towards automating this process  wrt some given task. in particular  our approach suggests a way to automatically assemble the appropriate imaging operators  i.e.  without human intervention   as required to effectively interpret a range of images.
1	conclusions
future work: while our face recognition results show that our ideas can be applied to a complex domain  there are a number of extensions that would further scale up our approach. some are relatively straightforward - e.g.  extending the set of operators to cover more features; this will help deal with larger number of faces in the training set  with better accuracy and lower interpretation costs. in other contexts  we will need to deal with thornier issues  such as operators that rely on one another. this may be because one operator requires  as input  the output of another operator  e.g.  a line-segmenter produces a set of tokens  which are then used by a line-grower - notice this precondition-situation leads to various planning issues  cfml1    or because the actual data obtained from one operator may be critical in deciding which next operator  or parameter setting  to consider next: e.g.  finding the fuselage at some position helps determine where to look for the airplane's wings.
모clearly we will need to re-think our current myopic approach to cope with these multi-step issues; especially as we expect heuristics will be essential  as this task is clearly np-hard  sri1 . finally  all of this assumes we have the required distribution information. an important challenge is more efficient ways to acquire such information from a set of training images - perhaps using something like qlearning  sb1 .
contributions: this paper has three main contributions: first  it provides a formal foundation for investigating efficient image interpretation  by outlining the criteria to consider  and suggesting some approaches. secondly  our implementation is a step towards automating the construction of effective image interpretation systems  as it will automatically decide on the appropriate policies for operator applications  as a function of the user's  explicitly provided  task and the available inventory of operators. finally  it presents some results related to these approaches - in particular  our results confirm the obvious point that information gain  as embodied in the infogain policy  is clearly the appropriate measure to use here - and in particular  it is better than the besthyp approach. this observation is useful  as there are deployed imaging systems that use this besthyp approach  lhd 1 .
acknowledgements
rg gratefully acknowledges support from nserc for this project. ri thanks sastry isukapalli for several discussions  general comments and help on the paper. both authors thank ramesh visvanathan  and the anonymous reviewers  for their many helpful and insightful comments.
references
 blm1  t binford  t levitt  and w mann. bayesian inference in model based machine vision. in uai  1.
 cfml1  s chien  f fisher  h mortensen  and e lo. using ai planning techniques to automatically reconfigure software modules. in lecture notes in cs  1.
 dh1  r. duda and p. hart. pattern classification and scene analysis. wiley  new york  1.
 dra1  b. draper. learning control strategies for object recognition. in ikeuchi and veloso  editors  symbolic visual learning. oxford university press  1.
 ec1  k etemad and r chellappa. discriminant analysis for recognition of human faces. j. optical society of america  1.  fua1  p fua. image understanding for intelligence imagery  chapter model-based optimization: an approach to fast  accurate  and consistent site modeling from imagery. morgan kaufmann  1.
 ge1  r. greiner and c. elkan. measuring and improving the effectiveness of representations. in ijcai1  pages 1  august 1.
 gj1  g geman and b jedynak. an active testing model for tracking roads in satellite images. ieee pami  1.
 har1  r haralick. overview: computer vision performance characterization. in arpa iu  1.
 hr1  r huang and s russell. object identification: a bayesian analysis with application to traffic surveillance. artificial intelligence  1.
 ig1  r. isukapalli and r. greiner. efficient car recognition policies. in icra  1.
 lab1  t s levitt  j m agosta  and t o binford. model-based influence diagrams for machine vision. in uai  1.
 lhd 1  t. levitt  m. hedgecock  j. dye  s. johnston  v. shadle  and d. vosky. bayesian inference for model-based segmentation of computed radiographs of the hand. artificial intelligence in medicine  1.
 mat1  t. matsuyama. expert systems for image processing: knowledge-based composition of image analysis processes. computer vision  graphics  and image processing  1 :1- 1  1.
 mit1  t. mitchell. machine learning. mcgraw-hill  1.
 pl1  a. pope and d. lowe. vista: a software environment for computer vision research. in ieee cvpr  1.
 pl1  a. pope and d. lowe. learning object recognition models from images. in early visual learning  1.
 pms1  a pentland  b moghaddam  and t starner. view-based and modular eigenspaces for face recognition. in ieee cvpr  1.
 pwhr1  p phillips  h wechsler  j huang  and p rauss. the feret database and evaluation procedure for face recognition algorithms. image and vision computing  1.
 rh1  v ramesh and r m haralick. performance characterization of edge operators. in machine vision and robotics conference  1.
 rsp1  s. russell  d. subramanian  and r. parr. provably bounded optimal agents. in ijcai1  august 1.
 sb1  k sengupta and k boyer. information theoretic clustering of large structural modelbases. in ieee cvpr  1.
 sb1  richard s. sutton and andrew g. barto. reinforcement learning: an introduction. mit press  1.
 sri1  s srinivas. a polynomial algorithm for computing the optimal repair strategy in a system with independent component failures. in uai  1.
 tp1  m turk and a pentland. eigenfaces for recognition. journal of cognitive neuroscience  1.

robotics and perception
vision ii

perceptual texture space improves
perceptual consistency of computational features
huizhong long and wee kheng leow
school of computing  national university of singapore
 1 science drive 1  singapore 1 longhuiz  leowwk comp.nus.edu.sg

abstract
perceptual consistency is important in many computer vision applications. unfortunately  except for color  computational features and similarity measurements for other visual features are not necessarily consistent with human's perception. this paper addresses three critical issues regarding perceptually consistent texture analysis:  1  development of perceptual texture space   1  assessment of how consistent computational features are to human perception  and  1  mapping computational features to perceptual space. it demonstrates the construction of a reliable perceptual texture space  which can be used as a yardstick for assessing the perceptual consistency of computational features and similarity measurements. moreover  it is found that commonly used computational texture features are not very consistent with human perception  and mapping them to the perceptual space improves their perceptual consistency.
1	introduction
perceptual consistency is important in many computer vision applications. for example  a computer image understanding system should analyze and interpret an image in a manner that is consistent with human's perception of the image. a content-based image retrieval system should compare images in its database with the query in a manner that is consistent with human's perception of image similarity. to this end  many systems and algorithms measure color similarity in the cie l*u*v* space which has been shown to be quite consistent with human's perception  meyer and greenberg  1 . unfortunately  computational features and similarity measurements for other visual features are not necessarily consistent with human's perception  with the possible exception of santini and jain's fuzzy features contrast model  santini and jain  1 .
모several perceptual spaces that capture human's perception of texture exist. however  the mapping from computational

모모this research is supported by nus arf r-1-1 and joint nstb and nus arf rp1.
features to these perceptual spaces has not been derived. another difficulty with using these perceptual spaces is that the scales and orientations of the textures used to construct the spaces are not normalized. therefore  texture similarity measured in these spaces are confounded by scale and orientation variations. this problem makes it very difficult to use existing texture spaces in practical applications.
모this paper addresses three critical issues regarding perceptually consistent image analysis  with specific application to texture analysis:
1. development of a perceptual texture space  pts  based on texture images with canonical scales and orientations  section 1 . once the reliability of pts is established  it can be used as a yardstick for assessing the perceptual consistency of computational features and similarity measurements.
1. assessment of how consistent commonly used computational texture features and similarity measurements are to human's perception  section 1 .
1. assessment of the accuracy of mapping computationaltexture features to pts  section 1 . once an accurate mapping is accomplished  texture similarity can be measured in pts to achieve perceptual consistency.
1	perceptual consistency
there are many ways of defining perceptual consistency. this section discusses some definitions which will be used throughout the paper. let denote the perceptual distance between samples and   and denote the corresponding measured or computational distance. a simple notion of perceptual consistency is that is proportional to . that is  there exists a linear function such that
 1 
then  perceptual consistency can be measured in terms of the mean squared error  mse  of linear regression:
		 1 
where	is the number of sample pairs. the smaller the mse 
the better is the consistency. a perfect consistency has an mse of 1.
모a less stringent notion of perceptual consistency is to require that be a monotonic function which can be nonlinear. the difficulty with this definition is that it is difficult to determine the best nonlinear function to use in practice.
모an alternative definition is to require that be statistically correlated to . in this case  it is useful to transform the populations and to equivalent zero-mean unitvariance populations and :
			 1 
where and are the means and and are the standard deviations of the populations. then  perceptual consistency can be measured in terms of the correlation :
		 1 
substituting eq. 1 into eq. 1 yields the pearson's correlation coefficient:
		 1 
the coefficient ranges from to  for perfect correlation . in the following sections  we will use both mse and pearson's correlation coefficient to measure two different aspects of perceptual consistency. note that  with perfect consistency   or    we obtain the following condition: for any textures  1 
that is  if perfect consistency is achieved  computational similarity would imply perceptual similarity. this condition is especially useful in practical applications  section 1 .
1	development of perceptual texture space
this section addresses the first critical issue put forth in section 1: development of perceptual texture space  pts . first  let us briefly review existing perceptual texture models.
1	existing perceptual texture models
the earliest study of human's perception of texture similarity was conducted by tamura et al.  tamura et al.  1  in their experiments  1 human subjects were asked to judge the similarity of texture pairs according to six visual properties  namely  coarseness  contrast  directionality  line-likeness  regularity  and roughness. similarity judgments were measured and each texture was assigned a perceptual rating value along each of the six visual scales. due to the combinatorial nature of the task  only 1 textures were used. amadasun and king  amadasun and king  1  conducted similar ranking experiments to measure similarity judgments according to various visual properties.
모the major difficulty with these studies is that the subjects were asked to judge texture similarity according to subjective visual properties. unfortunately  the subjects' interpretations of the meaning of these visual properties are expected to vary from one person to the next. therefore  it is uncertain whether the individual ranking results can be combined into group ranking results that represent the perception of a typical person. the second difficulty is that the ranking results were measured according to individual visual properties. but  the relative scale between two visual properties is unknown. for example  one unit difference in coarseness may not be perceptually equal to one unit difference in regularity. so  the different visual dimensions cannot be easily combined to form a perceptual texture space.
모to avoid these difficulties  rao and lohse  rao and lohse.  1  performed an experiment in which 1 subjects were asked to sort 1 textures into as many groups as the subjects wished such that the textures in each group were perceptually similar. the textures were sorted based on the subjects' perception of overall texture similarity instead of individual visual properties. a co-occurrence matrix of the sorting results was computed and multidimensional scaling  hair et al.  1  was performed to derive a 1d perceptual space. rao and lohse concluded that the 1 dimensions of the space strongly correlate with the visual properties of repetitiveness  orientation  and complexity.
모heaps and handel  heaps and handel  1  conducted further studies using the same methodology. however  they arrived at different conclusions than those of rao and lohse. they concluded that it is not possible to reliably associate a single visual property to each dimension of the texture space because each dimension appears to be correlated with a combination of more than one visual property. in addition  perception of texture similarity depends on the context in which the similarity is judged. that is  how similar two textures appear to humans depends not only on the two textures being judged  but also on the whole set of textures with which pairwise judgments are made.
모in our development of perceptual texture space  we do not assign visual properties to the dimensions of the space. moreover  the influence of the context problem is reduced by  1  selecting most  if not all  texture patterns that are expected to be present in images of natural scenes  and  1  normalizing the intensity  contrast  scale  and orientation of the textures used in the psychological experiment. it is well known  for example  that human's sensitivity of perceiving spatial frequency  i.e.  spatial patterns  is dependent on contrast  schiffman  1 . therefore  these confounding factors should be removed from the experiment.
1	texture image preparation
a rich set of texture images are collected to construct the perceptual texture space  pts . fifty texture images are selected from the brodatz album. the remaining images in the album are omitted for various reasons. for example  some textures are scaled versions of each others  and only one of them is required. another reason is that the contents of some images  e.g.  flowers in lace  appear very striking but their texture patterns are perceptually weak. it is very difficult for the subjects to avoid classifying the images based on the content instead of the texture alone. in addition to these brodatz textures  ten images of natural textures such as foliage and roof tiles are included because they are not collected in the brodatz

	 a 	 b 	 c 	 d 
figure 1: four types of dft patterns  bottom  and the corresponding representative textures  top :  a  structured   b  directional   c  granular   d  random.
album. these 1 texture images constitute a good set of representative textures that appear in images of natural scenes. these images are first normalized before they are used in the psychological experiments.
모four image features  namely  intensity  contrast  scale  and orientation  that can confound the subject's judgment are normalized. the intensity and contrast of a texture image is measured as the mean and standard deviation of the intensity of the pixels in the image. a texture's intensity and contrast are normalized by a simple linear transformation to a canonical mean intensity and standard deviation.
모our analysis reveals that there is no single scaling method that fits all the textures. however  by categorizing the textures into four different groups based on their 1d discrete fourier transform  dft  patterns  each group of textures can be scaled in the same way. the four groups consist of structured  directional  granular  and random textures  fig. 1 .
모structured textures have very well defined dft patterns which contain a small number of highly localized  prominent frequency components with large amplitudes  fig. 1a . the textures in this group are scaled and rotated so that the frequencies of the 1 most prominent components in the dfts of various textures are identical. the dft patterns of oriented textures have many frequency components and well defined orientations  fig. 1b . they lack prominent frequency components which can be used for scale normalization. however  their orientation can be normalized so that their dft patterns have the same orientations.
모granular textures have well defined frequencies and many orientations. their dft patterns appear in the form of small disks  fig. 1c . their scales are normalized so that their dft patterns have approximately the same size. random textures have many frequencies and orientations  fig. 1d . they lack well defined scales and orientations. so  they are not subjected to scale and orientation normalization.
1	measurement of human perception
a direct way of measuring the dissimilarity between textures and is to perform an exhaustive comparison of all sample pairs. when the number of samples
is large  e.g. 1   the number of pairs becomes very large  in this case  1 . it is practically impossible for a subject to perform such a large number of judgments. another commonly used method is to sort the samples into as many groups as the subjects wish such that the samples in each group are perceptually similar. this method sorts samples into groups  requiring at most judgments  which is far smaller than . this method is also adopted in  rao and lohse.  1  and  heaps and handel  1 .
모in our experiment  sixty subjects were asked to freely sort the images according to texture similarity. the sorting results were used to construct distance matrices based on two similarity measurements: co-occurrence matrix and information measurement  donderi  1 .
모let denote the number of times textures and are grouped into the same group. then  the distance measured by the co-occurrence method is:
 1 
where	is the number of subjects in the experiment.
모donderi's distance measurement is based on shannon's information theory :

 1 
where is the conditional entropy or independent information of after knowing the grouping of . let denote the set of textures used in the experiment and denote the group in which belongs. then  the independent information is computed as follows:
if if
모모모모모모모모모모모모모모모모모모모모모모모 1  according to eq. 1  informationally independent stimuli are dissimilar and informationally redundant stimuli are similar. it has been demonstrated that information measurements derived from the free sorting task are empirically and theoretically equivalent to the distance measurements generated by direct pairwise comparison  donderi  1 .
1	construction of perceptual space
after obtaining the distance matrices  multidimensional scaling  mds  was applied to derive a perceptual texture space  pts . mds  also known as perceptual mapping  maps the original dissimilarity judgments into points in a multidimensional space such that each point corresponds to a texture  and the euclidean distance between two textures and matches as well as possible the original dissimilarity . the multidimensional space would correspond to a perceptual space  and the euclidean distance measured in the space would correspond to perceptual distance.
모the appropriate number of dimensions of the perceptual space is determined by the stress measure which indicates the proportion of the variance of the data that is not accounted for  hair et al.  1 :

	stress	 1 
the larger the number of dimensions  the better is the fitting of the data and the lower is the stress. however  a space

figure 1: stress plot of the mds fitting results. optimal trade-off between accurate fitting and small number of dimensions is obtained with four dimensions. the space constructed with donderi's information measurement  infor  has a better fit than that constructed using co-occurrence  co-oc .
table 1: comparison of heaps and handels's space and our perceptual texture space  pts  with rao and lohse's space. pearson's correlation coefficients show that the spaces are consistent with each others.
perceptual space1d1d1dheaps & handel1--pts  co-occurrence 111pts  info. measure 111with too many dimensions is difficult to visualize and to use. so  a trade-off is made to find a small number of dimensions that offer good data fitting. rao and lohse  rao and lohse.  1   as well as heaps and handel  heaps and handel  1   concluded that a texture space with three dimensions is appropriate. our mds fitting results show that increasing the number of dimensions beyond 1 does not significantly reduce the stress. thus  a four-dimensional texture space provides a better trade-off  fig. 1 .
모the constructed spaces are verified by comparing them with existing perceptual spaces. this comparison is performed by computing the pearson's correlation  eq. 1  between the distances measured in the spaces. table 1 summarizes the comparison results. heaps and handel reported a good correlation     with rao and lohse's data  heaps and handel  1 . our pts constructed using cooccurrence has a better correlation with rao and lohse's space compared to that using donderi's information measurement. this is expected because rao and lohse's space was developed using co-occurrence as well. we do not have heaps and handel's data for direct comparison. nevertheless  table 1 shows that the spaces are mutually consistent but not the same. the difference between the space of rao and lohse and our pts can be attributed to the normalization of texture images. thus  the pts can be regarded as a reliable measurement of human's perception of texture similarity. in the following sections  the 1d pts constructed based on information measurement will be used as the standard perceptual space because it has a better fit to the psychological data than that developed using co-occurrence.
1	consistency of computational features
this section addresses the second critical issue: assessment of the perceptual consistency of computational texture features and similarity measurements. first  let us review commonly used texture features and similarity measurements.
1	computational texture features
statistical and spectral texture models are commonly used in existing image retrieval systems. statistical models categorize textures according to statistical measurements of visual qualities such as coarseness and granularity. spectral models characterize textures based on fourier spectrum or filtering results. the statistical features proposed by tamura et al.  tamura et al.  1  are used in ibm's qbic system. the wold model  liu and picard  1  based on fourier transform  spectral model  and multiresolution simultaneous autoregressive model  mrsar  statistical model  is used in mit's photobook. gabor features  spectral model  are used in ma and manjunath's netra  ma and manjunath  1  and leow and lai's invariant texture space  leow and lai  1 .
모the above features are easy to compute  but the texture similarity computed based on these features are not necessarily consistent to human's perception. on the other hand  santini and jain developed the fuzzy features contrast model  ffc   santini and jain  1  which has the potential of being perceptually consistent. their similarity measurement is based on tversky's feature contrast model  tversky  1  which can account for various peculiarities of human's perceptual similarity. they have applied ffc to measure texture similarity based on gabor features  and have obtained encouraging results. unfortunately  due to combinatorial problem  they have compared ffc's measurements with only a small number of human ranking data  and are not able to perform more thorough comparison with human perception.
모ffc defines a fuzzy membership function for each gabor feature of texture by a sigmoid function:
		 1 
where is the mean feature of the textures along the -th dimension  is the corresponding standard deviation  and is a constant parameter  which is fixed at 1 in  santini and jain  1  . the similarity between two gabor features and is then defined as
 1 
where and are parameters. various ranges of parameter values were tried. the values that produced the best match between ffc and pts are   .
모the major difficulty with using ffc is that the values of the parameters must be carefully selected to match human's perception. the authors acknowledge that the problem of determining the parameter values has not been adequately addressed  santini and jain  1 .
1	perceptual consistency tests
perceptual consistency is assessed by comparing the texture distances measured by various features with that measured table 1: comparison of various computational features and similarity measurements with pts. mahalanobis distance is used for mrsar in the wold model. = pearson's correlation coefficient  = mean squared error.
featuredistancetamuraeuclidean11gaboreuclidean11gaborffc11mrsareuclidean11mrsarmahalanobis11invarianteuclidean11in the 1d pts. both pearson's correlation coefficient  eq. 1  and the mean squared error  mse  of linear regression  eq. 1  are used as the measurement of consistency. the larger the pearson's coefficient  or the smaller the mse  the more consistent computational features are to pts  section 1 .
모the following features are assessed: tamura's features  gabor  mrsar  and invariant texture space. we could not assess the wold model directly because it defines only a ranking order of the textures without explicit measurement of texture similarity. therefore  only the mrsar features used in the wold model are assessed. the invariant texture space method maps gabor features to a space that is invariant to texture scales and orientations  leow and lai  1   whereas the other features are not invariant. so  it is interesting to see how consistent is the invariant space to pts.
모table 1 summarizes the results  and shows that gabor feature and gabor with ffc are most consistent with pts. in particular  measuring gabor similarity with ffc does improve gabor feature's consistency. measuring mrsar similarity with euclidean distance is perceptually more consistent than measuring with mahalanobis distance. the degrees of consistency of computational features     are  however  not very high compared to those between various perceptual spaces  table 1   . therefore  it can be concluded that these computational features and similarity measurements are not very consistent with human's perception.
1	mapping features to perceptual space
the previous section demonstrated that computational texture features are not very consistent with human's perception as measured in the 1d pts. this section addresses the third critical issue: assessment of the accuracy of mapping computational texture features to pts.
the feature mapping problem is to find a set of functions
모    that map the computational features of a texture into the coordinates in pts. that is  . our study reveals that the functions are typically nonlinear  and the form of the nonlinearity cannot be readily determined. therefore  both multilayer neural networks and support vector machine  svm  regression were tested. svm regression was found to give more accurate results. in addition  it has fewer parameters to tune than do neural networks. so  only the results of svm regression are reported here.
모svm regression maps a multidimensional input to a single output value. thus  four separate svms are required to table 1: testing errors of feature mapping. columns 1 to 1 are the average testing errors of -fold cross-validation. 1d denotes the mean squared error  mse  along one of the four dimensions of pts. 1d denotes the overall mse of the 1d coordinates in pts. columns 1 and 1 are the results of comparing the distances in pts with those measured by the features after they are mapped to pts.
unknownunknowndistancefeaturesinstancetexturecomparison1d1d1d1dtamura111111gabor111111mrsar111111invariant111111map computational features to the four coordinates   of pts:
 1 
where and are the lagrange multipliers  analogous to the weights of neural networks   is a kernel function  are the support vectors  and is the bias term. we experimented with various kernel functions and found that the gaussian kernel consistently produced more accurate results. various parameter values of the gaussian kernel were tried  and it was found that different computational features require different parameter values for optimal mapping.
모the assessment of perceptual consistency used the 1 normalized texture images described in section 1 as the data set. each image was cropped into 1 subimages of size pixels  resulting in a total of 1 texture samples. to better assess the performance of svm  two different training and testing experiments were conducted:
1. unknown instance of known texture.in this experiment  1 of the 1 samples of each texture were used for training  and the remaining 1 sample was used for testing. although the testing samples were different from the training samples  they were perceptually similar to some of the training samples. therefore  the testing error was expected to be small. 1-fold crossvalidation tests were conducted.
1. unknown texture.
this experiment tests svm's performance on unknown texture. it is an important test because new textures can appear in real applications. in this experiment  all the samples of 1  1%  of the 1 textures were used for training  and all the samples of the other 1  1%  textures were used for testing. since the testing samples were perceptually different from the training samples  the testing error was expected to be larger than that of the first experiment. 1-fold cross-validation tests were conducted.
모table 1 illustrates the mapping results obtained by svm regression. as expected  for all the features  testing errors for unknown instance is smaller than those for unknown texture.
moreover  being most consistent with pts  table 1   gabor features can be mapped to pts more accurately than other features can. in particular  gabor features' testing error for unknown instance is much smaller than those of other features and those for unknown texture tests. therefore  it can be concluded that accurate mapping to pts can be achieved  at least for gabor features and under the condition that some samples of all texture types appear in the training set.
모after mapping computational features to pts  one would expect the mapped coordinates to be more consistent with pts. a comparison is performed between pts and the computational features mapped by svm models trained under the second condition  unknown texture . the distance correlation results are shown in table 1. comparing table 1 with table 1 shows that mapping computational features to pts does improve the perceptual consistency of the features.
1	application examples
given an accurate mapping  it is now possible to use pts for various applications  such as  retrieval  classification  and classification with uncertainty.
1. in retrieval  we like to sort a set of textures into an ordered list such that  with respect to the query texture   for any   i.e.  textures that are perceptually similar to are placed at the beginning of the list. this problem can be solved by mapping to pts and ordering the textures according to .
1. in classification  we like to classify a texture to a class such that for any and any   .
this can be achieved by classifying to the class  say  of its nearest neighbor  say    i.e.  for any . since the distance between textures in the same class is smaller than the distance between textures
in different classes  by perceptual consistency  for any and any .
1. the distance measurement	in pts can be used to represent uncertainty of classification.	it is computed in terms of the independent information	and
모모모모 eq. 1   which represents the uncertainty of classifying	given	and vice versa.
1	conclusion
this paper has addressed three critical issues about perceptually consistent texture analysis:
1. a 1d perceptual texture space  pts  has been developed. by comparing with existing perceptual spaces  it is verified that the pts provides a reliable measurement of human's perception of texture similarity.
1. it is shown  by correlating with the distances measuredin pts  that commonly used computational texture features and similarity measurements are not very consistent with pts. among the various features  gabor features and gabor with ffc give the highest correlation.
1. it is shown that mapping computational features to ptsimproves the features' perceptual consistency. in particular  gabor features can be most accurately mapped to pts  especially when some samples of all texture types appear in the training set.
this paper thus establishes that pts can be used as a yardstick for assessing the perceptual consistency of computational features and similarity measurements.
모for practical applications  it might be necessary to find a method that can map computational features to pts accurately under the more stringent condition of unknown texture types. this would be necessary if some rare textures that are distinct from the known textures used in the training process appear during actual run. in addition  it would be necessary to perform the mapping in a manner that is invariant to texture scale and orientation.
references
 amadasun and king  1  m. amadasun and r. king. textural features corresponding to textural properties. ieee trans. smc  1 :1  1.
 donderi  1  d. c. donderi. information measurement of distinctiveness and similarity. perception and psychophysics  1 :1  1.
 hair et al.  1  joseph. f. hair  r. e. anderson  and r. l. tatham. multivariate data analysis. prentice hall  1.
 heaps and handel  1  c. heaps and s. handel. similarity and features of natural textures. j. expt. psycho.: human perception and performance  1 :1  1.
 leow and lai  1  w. k. leow and s. y. lai. scale and orientation-invariant texture matching for image retrieval. in m. k. pietika몮inen  editor  texture analysis in machine vision. world scientific  1.
 liu and picard  1  f. liu and r.w. picard. periodicity  directionality  and randomness: wold features for image modeling and retrieval. ieee trans. pami  1 :1- 1  1.
 ma and manjunath  1  w. y. ma and b. s. manjunath. texture features and learning similarity. in proc. of ieee conf. cvpr  pages 1  1.
 meyer and greenberg  1  g. w. meyer and d. p. greenberg. perceptual color spaces for computer graphics. in h. j. durett  editor  color and the computer. academic press  lodon  1.
 rao and lohse.  1  a. r. rao and g. l. lohse. towards a texture naming system: identifying relevant dimensions of texture. in ieee conf. on visualization  pages 1  1.
 santini and jain  1  s. santini and r. jain. similarity measures. ieee trans. pami  1 :1  1.
 schiffman  1  h. r. schiffman. sensation and perception: an integrated approach. john wiley & sons  1 edition  1.
 tamura et al.  1  h. tamura  s. mori  and t. yamawaki. textural features corresponding to visual perception. ieee trans. smc  1 :1  1.
 tversky  1  a. tversky. features of similarity. psychological review  1 :1  1.
fuzzy conceptual graphs for matching images of natural scenes
philippe mulhem   wee kheng leow   and yoong keok lee
ipal-cnrs  national university of singapore
school of computing  national university of singapore
모모science drive 1  singapore 1 mulhem  leowwk  leeyoong comp.nus.edu.sgabstract
conceptual graphs are very useful for representing structured knowledge. however  existing formulations of fuzzy conceptual graphs are not suitable for matching images of natural scenes. this paper presents a new variation of fuzzy conceptual graphs that is more suited to image matching. this variant differentiates between a model graph that describes a known scene and an image graph which describes an input image. a new measurement is defined to measure how well a model graph matches an image graph. a fuzzy graph matching algorithm is developed based on error-tolerant subgraph isomorphism. test results show that the matching algorithm gives very good results for matching images to predefined scene models
1	introduction
conceptual graphs  sowa  1; 1; 1; 1  are expressive for structured knowledge representation and they can be converted to first-order logic expressions. hence  they are used with theoretical logic-based approaches in applications like hypertext modeling  kherbeik and chiaramella  1  and image representation and retrieval  mechkour  1; ounis and pasc a  1 . however  in these applications  uncertainty is not represented in the conceptual graphs.
모to incorporate uncertainty  morton  morton  1  extended sowa's conceptual graph theory to fuzzy conceptual graphs  which include fuzzy referents  fuzzy operations  and fuzzy open and closed worlds. wuwongse and colleagues  wuwongse and manzano  1; wuwongse and tru  1  extended morton's fuzzy conceptual graphs to incorporate fuzzy conceptual relations. in addition  maher  maher  1  proposed a similarity measurement for matching simple fuzzy conceptual graphs.
모existing formulations of conceptual graphs are important developments of the theory of fuzzy conceptual graphs. however  they are not suitable for matching uncertain information in images with model scenes. for example  in existing formulations  a concept has only one type. in practice  however  it is very difficult to accurately determine the concept

모모this research is supported by nus arf r-1-1 and joint nstb and nus arf rp1.
types of the regions in an image. in  petrakis and faloutsos  1   attributed relational graphs are used for image content representation  and  medasani and krishnapuram  1  proposed fuzzy matching of these graphs. but  both methods do not handle hierarchies of concept types and of relations.
모this paper presents a new variation of fuzzy conceptual graphs that is more suited to matching images of natural scenes. this variant has several distinct characteristics that distinguish it from existing formulations:
it associates a concept with multiple concept types so as to handle the uncertainty of classification.
it introduces a new component called relation attribute for capturing concepts such as the ratio between the sizes of two image regions. separating relation attributes from relations allows two or more correlated relations to share the same attribute explicitly. relationships between attributes can be represented as well. for example  we can explicitly represent the fact that the ratio between the sizes of two objects is larger than the size ratio of two other objects.
it differentiates between model graphs and image graphs. a model graph describes a model of a scene whereas an image graph describes the components and their relationships in an image. as will be described in more details in section 1  these two types of fuzzy graphs have different characteristics.
it defines a similarity measurement between a model graph and an image graph in terms of graph projection. in addition  an algorithm is implemented to compute the similarity based on subgraph isomorphism.
1	fuzzy conceptual graphs
this section first introduces and defines the basic elements of a fuzzy conceptual graph. next  the degree of match between conceptual graphs is defined and  finally  the graph matching algorithm is described.
1	basic elements
a fuzzy conceptual graph is a directed graph formed from three kinds of components: fuzzy concepts  fuzzy relations  and fuzzy relation attributes. the set is the set of
fuzzy concepts  the set of fuzzy relations  and the set of fuzzy relation attributes.
모a fuzzy concept is a 1-tuple that consists of a set of concept types   a referent   and a fuzzy membership function   for each . in  wuwongse and
manzano  1; wuwongse and tru  1   is called the measure assigned to the type . in practice  can be computed as the confidence of classifying a region as type . a crisp concept can be regarded as a special kind of fuzzy concept whose set consists of only one concept type with . for notational convenience  we denote a
crisp concept as a 1-tuple of the form	.
모a fuzzy relation is a 1-tuple that consists of a crisp relation type and a fuzziness value which represents the probability of occurrence of the relation in the images. a crisp relation is a special kind of fuzzy relation of the form   i.e.  which means that the relation is not fuzzy.
모a fuzzy relation attribute is a 1-tuple that consists of a crisp relation attribute type   a referent   and a fuzzy membership function such that . a crisp relation attribute is a special kind of fuzzy relation attribute of the form   where .
모the denotation of type  concept  relation  or relation attribute type  is the set of all possible individual referents of . a type is a specialization or subtype of type   denoted   if  sowa  1 . the reciprocity holds also. note that the specialization relationship is reflexive and transitive but not symmetric. the set of concept types together with the specialization relationship forms a finite lattice of concept types. relation attributes are special kinds of concepts. so  the set of relation attribute types form a sub-lattice in the lattice of concept types. in the same way  the set of relation types forms a finite lattice of relation types. a model graph represents a model of a known scene. in a model  the concept types are known. for example  a mountain-and-lake scene contains sky  mountain  water  tree  etc.  fig. 1  therefore  the concepts in a model graph are crisp concepts. however  the relations between the concepts may vary from one image to the next. moreover  to capture fuzzy linguistics such as  much smaller  and  slightly smaller   the relations and relation attributes have to be fuzzy. for example  the relation comp  represented as comp in fig. 1  from tree to image indicates that trees appear in 1% of the images of mountain-and-lake scene. the relation smaller  represented as smaller in fig. 1   with attribute ratio f1  represented as ratio f1 in fig. 1   from tree to mountain indicates that in 1% of the images of mountain-and-lake scene  the area occupied by trees is 1% that of the mountains. the symbol f1 denotes a fuzzy membership function that peaks at 1. if the ratio in an input image is not exactly 1  then f1    will be smaller than the maximum value of 1.
모several relations can connect the same two concepts in a model graph. among these relations  those that have the same relation type are regarded as forming a disjunctive group  i.e.  or  of relations  e.g.  the  smaller  relation between tree and mountain in fig. 1 . the various disjunctive groups then form a conjunction  i.e.  and  of relations.
모an image graph represents the structure of a particular input image. due to the inherent uncertainty in the algorithms that classify the image regions  the concepts in an image

figure 1: a  partial  model graph representing a mountainand-lake scene. relations are represented as ellipses. concepts and relation attributes are represented as rectangles. due to space limitation  only some of the concepts  relations  and relation attributes are shown.

figure 1: an example  partial  image graph of an input image. relations are represented as ellipses. concepts and relation attributes are represented as rectangles.
graph are fuzzy. however  the relations between the regions in an image can be computed exactly. therefore  the relations and their attributes are not fuzzy. for example  in fig. 1  due to the ambiguity between leaves texture and grass texture  concept #e1 has a confidence level of 1 of being a leave texture or a tree texture  and a lower confidence level of 1 of being a grass texture. the rectangular box for #e1 represents the fuzzy concept #e1 where = leave  grass  tree and  leave  = 1   grass  = 1  and  tree  = 1. the relation comp from concept #e1 to #e1 is crisp because the region #e1 is a component of the image #e1. similarly  the relation smaller and the relation attribute ratio are crisp because the ratio of 1 is measured according to the areas of the regions in the image. in summary  a model graph contains crisp concepts  fuzzy relations  and fuzzy relation attributes  while an image graph contains fuzzy concepts  crisp relations  and crisp relation attributes  table 1 .
1	fuzzy graph matching
the degree of match between a model graph and an image graph is defined in terms of graph projection  sowa  1  and the degrees of match between their concepts  relations  and relation attributes. the degree of match between a model concept and an image concept table 1: summary of the elements of fuzzy conceptual graphs for scene models and input images.
notationelementnotationin figuremodelcrisp conceptgraphfuzzy relation fuzzy relation attributeimagefuzzy conceptgraphcrisp relation crisp relation attributeis defined as
if	such that otherwise.
 1 
the degree of match between a model relation and an image relation is defined as
if
모모모모모모모모모 1  otherwise.
the degree of match between a model relation attribute and an image relation attribute is
defined as
if
모모모모모모모모모 1  otherwise.
모a projection from a fuzzy graph to another fuzzy graph is a mapping such that
	for each concept	in	 	is a concept in	and
 
	for each relation	in	 	is a relation in	and
 
	for each relation attribute	in	 	is a relation at-
	tribute in	and	 
for any two concepts or relation attributes and related by a relation in   and are related by in
.
in general  the projection	may not exist but the projection	from a subgraph	of	may exist. therefore  the matching problem becomes one of finding the best matching subgraph	of	that maximizes	.
모given a projection from a subgraph of model graph to an image graph   the degree of match between and is defined as

 1 
where is the sum of the number of concepts  the number of disjunctive groups of relations  and the sum of the maximum number of relation attributes in each disjunctive group of relation in the model graph	. the second summation in eq. 1 sums over the conjunctive groups of relations.	note that if	and	  then	. that is  a perfect match yields a matching value of 1.	we set in our test.
1	graph matching algorithm
to facilitate the matching process  a graph is first decomposed into a set of arches  each consisting of a source concept   a target concept   a relation from to   and zero or more relation attributes . thus  arches are actually subgraphs of the model graph or image graph. the match value between a model arch and an image arch is simply the normalized match values of the concepts  relation  and relation attributes of the arches  i.e.  eq. 1 .
모our fuzzy graph matching algorithm is a variant of messmer and bunke's error-tolerant subgraph isomorphism algorithm  messmer and bunke  1 . the main differences between these algorithms include the following:
in  messmer and bunke  1   the smallest unit of operation is a vertex. in our algorithm  the smallest unit is an arch.
for error-tolerant graph matching  our algorithm needs to consider only deletion of graph vertices. it does not require addition and replacement of vertices and edges  which are supported in  messmer and bunke  1 .
two queues are maintained  one for normal search  and the other one for backtracking to the first-level subgraphs  just below the root . this method improves the quality of the solution found by the algorithm.
the fuzzy graph matching algorithm is as follows:
convert model graph to arch set   image graph to arch set .
for each pair
	compute	.
if	threshold  then insert	into 1st-level queue	in decreasing order of	.
	repeat	times
	empty normal search queue	.
	remove first pair	from	and insert into	.
repeat
	remove first pair	from	.
if all arches in	have been considered for connecting to	  then break out of the inner repeat loop.
	for each pair	such that
1  can connect to   and can connect to connect to   to .
	compute	.
if threshold  then insert into in decreasing order of .
	save	and the match value as a candidate solution.
return the candidate solution with the highest match value.
the basic idea is to start off with a single arch in the model graph that best matches an arch in the image graph. this pair of arches forms the initial matching model and image subgraphs. subsequently  other model arches that have matching image arches are added to the matching subgraphs. for model arches that form a disjunctive group  only the best matching model arch is selected. this matching process repeats until all the arches in the model graph have been considered for addition to the matching subgraphs. then  the matching model and image subgraphs are saved as a candidate solution.
모due to possible ambiguity of the concepts  initial erroneous matches that happen to have large match values may lead the search algorithm down the wrong search paths. so  another queue is used to keep the first-level subgraphs. once a candidate solution is found and saved  the normal search queue is emptied and a new search from another firstlevel subgraph is initiated. this method allows the algorithm to escape from local maxima of the solution space. this procedure is repeated times  and the best solution among them is returned as the final matching result. in the tests reported in section 1  is set to 1.
모the threshold in the algorithm is used to prune away subgraphs that have poor matching values. this method reduces the amount of searches that need to be considered and helps to improve the efficiency of the algorithm. in the tests  the threshold is set to 1.
during the execution of the algorithm  the value
can be computed incrementally. let at   i.e.  connecting arch to model graph at the concept results in the new model graph . note that . similarly  let at . then  can be computed incrementally as follows:
		 1 
the time	complexity of the matching	algorithm is where	and	are the
number of model and image arches respectively  is the average number of relation attributes per relation  is the number of concept types in the concept lattice  is the number of relation types in the relation lattice. if each relation has at most one attribute  i.e.    then the time complexity reduces to	.
1	test results
in our current implementation  an input image is segmented using a color and texture segmentation algorithm. the color histogram and gabor texture features of each region are analyzed to classify the region into several possible classes  and the confidence level for each classification is measured. these measurements are used to generate the image graphs of images by a graph generation program. the model graphs of scene models are generated manually because  at present  we are still in the process of developing a learning algorithm for learning model graphs from example images.
모tests were conducted to match four scene models of different complexity  fig. 1  with 1 images. a sample of the images are shown in fig. 1. table 1 summarizes the test results. images that match the models well have higher match values than those that match the models poorly. model is

	 c 	 d 
figure 1: schematic drawings of four scene models with different complexity.
table 1: image matching results. numbers in brackets are the number of concepts  relations  and relation attributes of the respective conceptual graphs. the other numbers are the match values.
imagemodel 1 1  1 1  1 1  1 1 1  1 1 11111  1 1 11111  1 1 11111  1 1 11111  1 1 11111  1 1 11111  1 1 11111  1 1 11111  1 1 11111  1 1 1111the simplest among the four models. most images can fully match model successfully and thus have high match values. in particular  the first 1 images best match model and have the highest match values. model differs from model by having a small tree above the water. therefore  it best
matches images 1  1  and 1.
모model is the most complex model which has the most number of components. it is used to show that the algorithm can perform partial matching because none of the images can match model fully. as a result  the images have smaller match values than those for matching the other models. nevertheless  the ranking of the images is still consistent: images that match model better have higher match values. in particular  images 1 and 1 match model best.
모model is used to illustrate an extreme case in which a very specific model is specified. in this test  model is created according to image 1. the test results show that image 1 indeed matches model very well. image 1 differs a bit


 1 	 1 

 1 	 1 

 1 	 1 

 1 	 1 
figure 1: sample images used in the tests.

 1 	 1 
figure 1:  cont.  sample images used in the tests.

from model in that image 1 has tall trees on both sides of the image whereas model has tall trees on only one side. thus  image 1 has a lower match value than image 1. all other images cannot match model very well and have very small match values.
1	conclusion
this paper presented a new variant of fuzzy conceptual graphs that is more appropriate than existing formulations for representing and matching input images and model scenes. a new component called relation attribute is introduced into fuzzy conceptual graphs. moreover  model graphs that describe known scenes are distinguished from image graphs that represent input images. a similarity measure is defined to assess the degree of match between model and image graphs. a graph matching algorithm is developed based on errortolerant subgraph isomorphism algorithm. experimental tests show that the matching algorithm gives very good results for matching images to scene models.
모the fuzzy graphs can be input and modified manually using a graphical user interface. the image graphs of the images that match a model can be displayed by an image retrieval system for explaining how well the image matches the model. we are currently developing a method of learning model graphs given example images of the scenes.
references
 kherbeik and chiaramella  1  a. kherbeik and y. chiaramella. integrating hypermedia and information retrieval with conceptual graphs. in proc. int. conf. on hypermedia  information retrieval and multimedia  pages 1  1.
 maher  1  p. e. maher. conceptual graphs - a framework for uncertainty management. in proc. 1th annual nafips conference  pages 1  1.
 mechkour  1  m. mechkour. emir1: an extended model for image representation and retrieval. in proc. int. conf. on databases and expert systems applications  dexa '1   pages 1  1.
 medasani and krishnapuram  1  s. medasani and r. krishnapuram. a fuzzy approach to content-based image retrieval. in proceedings of the ieee conference on fuzzy systems  pages 1  seoul  1.
 messmer and bunke  1  b. t. messmer and h. bunke. a new algorithm for error-tolerant subgraph isomorphism detection. ieee trans. pami  1 :1  1.
 morton  1  s. morton. conceptual graphs and fuzziness in artificial intelligence. phd thesis  university of bristol  1.
 ounis and pasc a  1  i. ounis and m. pasc a. relief: combinig evidences and rapidity into a signle system. in proc. acm sigir '1  pages 1  1.
 petrakis and faloutsos  1  g. m. petrakis and c. faloutsos. similarity searching in large image databases. ieee transactions on knowledge and data engineering  1 :1  1.
 sowa  1  j. f. sowa. conceptual structures: information processing in mind and machine. addison-wesley  1.
 sowa  1  j. f. sowa. towards the expressive power of natural language. in proc. 1th annual workshop on conceptual graphs  1.
 sowa  1  j. sowa. relating diagrams to logic. in proc. int. conf. on conceptual structures  iccs '1    lnai 1   pages 1  1.
 sowa  1  j. sowa. knowledge representation: logical  philosophical  and computational foundations. brooks/cole publisher  1.
 wuwongse and manzano  1  v. wuwongse and m. manzano. fuzzy conceptual graph. in proc. int. conf. on conceptual structures  iccs '1   lnai 1   pages 1  1.
 wuwongse and tru  1  v. wuwongse and c. h. tru. towards fuzzy conceptual graph programs. in proc. int. conf. on conceptual structures  iccs '1   lnai 1   pages 1  1.

robotics and perception
perception

discriminating animate from inanimate visual stimuli
brian scassellati
mit artificial intelligence laboratory 1 technology square
cambridge  ma 1 scaz ai.mit.edu

abstract
from as early as 1 months of age  human children distinguish between motion patterns generated by animate objects from patterns generated by moving inanimate objects  even when the only stimulus that the child observes is a single point of light moving against a blank background. the mechanisms by which the animate/inanimate distinction are made are unknown  but have been shown to rely only upon the spatial and temporal properties of the movement. in this paper  i present both a multiagent architecture that performs this classification as well as detailed comparisons of the individual agent contributions against human baselines.
1	introduction
one of the most basic visual skills is the ability to distinguish animate from inanimate objects. we can easily distinguish between the movement of a clock pendulum that swings back and forth on the wall from the movement of a mouse running back and forth across the floor. michotte  first documented that adults have a natural tendency to describe the movement of animate objects in terms of intent and desire  while the movements of inanimate objects are described in terms of the physical forces that act upon them and the physical laws that govern them. furthermore  by using only single moving points of light on a blank background  michotte showed that these perceptions can be guided by even simple visual motion without any additional context.
모leslie  proposed that this distinction between animate and inanimate objects reflects a fundamental difference in how we reason about the causal properties of objects. according to leslie  people effortlessly classify stimuli into three different categories based on the types of causal explanations that can be applied to those objects  and different modules in the brain have evolved to deal with each of these types of causation. inanimate objects are described in terms of mechanical agency  that is  they can be explained by the rules of mechanics  and are processed by a special-purpose reasoning engine called the theory of body module  toby  which encapsulates the organism's intuitive knowledge about how objects move. this knowledge may not match the actual physical laws that govern the movement of objects  but rather is our intuitive understanding of physics. animate objects are described either by their actions or by their attitudes  and are processed by the theory of mind module which has sometimes been called an  intuitive psychology.  system 1 of the theory of mind module  tomm-1  explains events in terms of the intent and goals of agents  that is  their actions. for example  if you see me approaching a glass of water you might assume that i want the water because i am thirsty. system 1 of the theory of mind module  tomm-1  explains events in terms of the attitudes and beliefs of agents. if you see me approaching a glass of kerosene and lifting it to my lips  you might guess that i believe that the kerosene is actually water. leslie further proposed that this sensitivity to the spatiotemporal properties of events is innate  but more recent work from cohen and amsel  may show that it develops extremely rapidly in the first few months and is fully developed by 1 months.
모although many researchers have attempted to document the time course of the emergence of this skill  little effort has gone into identifying the mechanisms of how an adult or an infant performs this classification. this paper investigates a number of simple visual strategies that attempt to perform the classification of animate from inanimate stimuli based only on spatio-temporal properties without additional context. these strategies have been implemented on a humanoid robot called cog as part of an on-going effort to establish basic social skills and to provide mechanisms for social learning  scassellati  1 . a set of basic visual feature detectors and a context-sensitive attention system  described in section 1  select a sequence of visual targets  see figure 1 . the visual targets in each frame are linked together temporally to form spatio-temporal trajectories  section 1 . these trajectories are then processed by a multi-agent representation that mimics leslie's toby module by attempting to describe trajectories in terms of naive physical laws  section 1 . the results of the implemented system on real-world environments are introduced  and a comparison against human performance on describing identical data is discussed in section 1.
1	visual precursors
cog's visual system has been designed to mimic aspects of an infant's visual system. human infants show a preference for stimuli that exhibit certain low-level feature properties. for example  a four-month-old infant is more likely to look at a

visual input
figure 1: overall architecture for distinguishing animate from inanimate stimuli. visual input is processed by a set of simple feature detectors  each of which contributes to a visual attention process. salient objects in each frame are linked together to form spatio-temporal trajectories  which are then classified by the  theory of body   toby  module.
moving object than a static one  or a face-like object than one that has similar  but jumbled  features  fagan  1 . cog's perceptual system combines many low-level feature detectors that are ecologically relevant to an infant. three of these features are used in this work: color saliency analysis  motion detection  and skin color detection. these low-level features are then filtered through an attentional mechanism which determines the most salient objects in each camera frame.
1	pre-attentive visual routines
the color saturation filter is computed using an opponentprocess model that identifies saturated areas of red  green  blue  and yellow  itti et al.  1 . the color channels of the incoming video stream  r  g  and b  are normalized by the luminance l and transformed into four color-opponency channels  r1  g1  b1  and y1 :
 1 
 1 
 1 
 1 
the four opponent-color channels are thresholded and smoothed to produce the output color saliency feature map.
모in parallel with the color saliency computations  the motion detection module uses temporal differencing and region growing to obtain bounding boxes of moving objects. the incoming image is converted to grayscale and placed into a ring of frame buffers. a raw motion map is computed by passing the absolute difference between consecutive images through a threshold function t :
	mraw = t kit   it 1k 	 1 
this raw motion map is then smoothed to minimize point noise sources.
모the third pre-attentive feature detector identifies regions that have color values that are within the range of skin tones  breazeal et al.  1 . incoming images are first filtered by a mask that identifies candidate areas as those that satisfy the following criteria on the red  green  and blue pixel components:
	1g   r   1g	1b   r   1b	1   r   1
모모모모모모모모모모모모모모모모모모모모모모모 1  the final weighting of each region is determined by a learned classification function that was trained on hand-classified image regions. the output is again median filtered with a small support area to minimize noise.
1	visual attention
low-level perceptual inputs are combined with high-level influences from motivations and habituation effects by the attention system. this system is based upon models of adult human visual search and attention  wolfe  1   and has been reported previously  breazeal and scassellati  1 . the attention process constructs a linear combination of the input feature detectors and a time-decayed gaussian field which represents habituation effects. high areas of activation in this composite generate a saccade to that location and compensatory neck movement. the weights of the feature detectors can be influenced by the motivational and emotional state of the robot to preferentially bias certain stimuli. for example  if the robot is searching for a playmate  the weight of the skin detector can be increased to cause the robot to show a preference for attending to faces. the output of the attention system is a labeled set of targets for each camera frame that indicate the positions  and feature properties  of the k most salient targets. for the experiments presented here  k = 1.
1	computing motion trajectories
the attention system indicates the most salient objects at each time step  but does not give any indication of the temporal properties of those objects. trajectories are formed using the multiple hypothesis tracking algorithm proposed by reid  and implemented by cox and hingorani . the centroids of the attention targets form a stream of target locationswith a maximum of k targets present in each frame t. the objective is to produce a labeled trajectory which consists of a set of points  at most one from each frame  which identify a single object in the world as it moves through the field of view:
		 1 
however  because the existence of a target from one frame to the next is uncertain  we must introduce a mechanism to compensate for objects that enter and leave the field of view and to compensate for irregularities in the earlier processing modules. to address these problems  we introduce phantom points that have undefined locations within the image plane but which can be used to complete trajectories for objects that enter  exit  or are occluded within the visual field. as each new point is introduced  a set of hypotheses linking that point

figure 1: the last frame of a 1 frame sequence with five trajectories identified. four nearly stationary trajectories were found  one on the person's head  one on the person's hand  one on the couch in the background  and one on the door in the background . the final trajectory resulted from the chair being pushed across the floor.
to prior trajectories are generated. these hypotheses include representations for false alarms  non-detection events  extensions of prior trajectories  and beginnings of new trajectories. the set of all hypotheses is pruned at each time step based on statistical models of the system noise levels and based on the similarity between detected targets. this similarity measurement is based on similarities of object features such as color content  size  and visual moments. at any point  the system maintains a small set of overlapping hypotheses so that future data may be used to disambiguate the scene. of course  at any time step  the system can also produce the set of nonoverlapping hypotheses that are statistically most likely. figure 1 shows the last frame of a 1 frame sequence in which a chair was pushed across the floor and the five trajectories that were located.
1	the theory of body module
to implement the variety of naive physical laws encompassed by the theory of body module  a simple agent-based approach was chosen. each agent represents knowledge of a single theory about the behavior of inanimate physical objects. for every trajectory t  each agent a computes both an animacy vote 붸ta and a certainty 뷈ta. the animacy votes range from +1  indicating animacy  to  1  indicating inanimacy   and the certainties range from 1 to 1. for these initial tests  five agents were constructed: an insufficient data agent  a static object agent  a straight line agent  an acceleration sign change agent  and an energy agent. these agents were chosen to handle simple  common motion trajectories observed in natural environments  and do not represent a complete set. most notably missing is an agent to represent collisions  both elastic and inelastic.
모at each time step  all current trajectories receive a current animacy vote vt. three different voting algorithms were tested to produce the final vote vt for each trajectory t. the first voting method was a simple winner-take-all vote in which the winner was declared to be the agent with the greatest absolute value of the product: vt = maxa k붸ta 뫄 뷈tak
the second method was an average of all of the individual1 p  붸ta 뫄 뷈ta  where a is the numvote products: vt = a a
ber of agents voting. the third method was a weighted average of the products of the certainties and the animacy votes:1 p  wa 뫄 붸ta 뫄 뷈ta  where wa is the weight for
vt = a	a
agent a. weights were empirically chosen to maximize performance under normal  multi-object conditions in natural environments and were kept constant through out this experiment as 1 for all agents except the static object agent which had a weight of 1. the animacy vote at each time step is averaged with a time-decaying weight function to produce a sustained animacy measurement.
1	insufficient data agent
the purpose of the insufficient data agent is to quickly eliminate trajectories that contain too few data points to properly compute statistical information against the noise background. any trajectory with fewer than one-twentieth the maximum trajectory length or fewer than three data points is given an animacy vote 붸 = 1 with a certainty value of 1. in practice  maximum trajectory lengths of 1 were used  corresponding to trajectories spanning 1 seconds   so any trajectory of fewer than 1 data points was rejected.
1	static object agent
because the attention system still generates target points for objects that are stationary  there must be an agent that can classify objects that are not moving as inanimate. the static object agent rejects any trajectory that has an accumulated translation below a threshold value as inanimate. the certainty of the measurement is inversely proportional to the translated distance and is proportional to the length of the trajectory.
1	straight line agent
the straight line agent looks for constant  sustained velocities. this agent computes the deviations of the velocity profile from the average velocity vector. if the sum of these deviations fall below a threshold  as would result from a straight linear movement  then the agent casts a vote for inanimacy. below this threshold  the certainty is inversely proportional to the sum of the deviations. if the sum of the deviations is above a secondary threshold  indicating a trajectory with high curvature or multiple curvature changes  then the agent casts a vote for animacy. above this threshold  the certainty is proportional to the sum of the deviations.
1	acceleration sign change agent
one proposal for finding animacy is to look for changes in the sign of the acceleration. according to this proposal  anything that can alter the direction of its acceleration must be operating under its own power  excluding contact with other objects . the acceleration sign change agent looks for zerocrossings in the acceleration profile of a trajectory. anything with more than one zero-crossing is given an animacy vote with a certainty proportional to the number of zero crossings.
1
1
1
1
1
figure 1: thirty stimuli used in the evaluation of toby. stimuli were collected by recording the position of the most salient object detected by the attention system when the robot observed natural scenes similar to the one shown in figure 1. each image shown here is the collapsed sequence of video frames  with more recent points being brighter than older points. human subjects saw only a single bright point in each frame of the video sequence.1	energy agent
bingham  schmidt  and rosenblum  have proposed that human adults judge animacy based on models of potential and kinetic energy. to explore their hypothesis  a simple energy model agent was implemented. the energy model agent judges an object that gains energy to be animate. the energy model computes the total energy of the system e based on a simple model of kinetic and potential energies:
		 1 
where m is the mass of the object  vy the vertical velocity  g the gravity constant  and y the vertical position in the image. in practice  since the mass is a constant scale factor  it is not included in the calculations. this simple model assumes that an object higher in the image is further from the ground  and thus has more potential energy. the vertical distance and velocity are measured using the gravity vector from a three-axis inertial system as a guideline  allowing the robot to determine  up  even when its head is tilted. the certainty of the vote is proportional to the measured changes in energy.
1	comparing toby's performance to human performance
the performance of the individual agents was evaluated both on dynamic  real-world scenes at interactive rates and on more carefully controlled recorded video sequences.
모for interactive video tasks  at each time step five attention targets were produced. trajectories were allowed to grow to a length of sixty frames  but additional information on the longterm animacy scores for continuous trajectories were maintained as described in section 1. all three voting methods were tested. the winner-take-all and the weighted average voting methods produced extremely similar results  and eventually the winner-take-all strategy was employed for simplicity. the parameters of the toby module were tuned to match human judgments on long sequences of simple data structures  such as were produced by static objects or people moving back and forth throughout the room .
1	motion trajectory stimuli
to further evaluate the individual toby agents on controlled data sequences  video from the robot's cameras were recorded and processed by the attention system to produce only a single salient object in each frame.1 to remove all potential contextual cues  a new video sequence was created containing only a single moving dot representing the path taken by that object set against a black background  which in essence is the only data available to the toby system. thirty video segments of approximately 1 frames each were collected  see figure 1 . these trajectories included static objects  e.g. #1   swinging pendula  e.g. #1   objects that were thrown into the air  e.g. #1   as well as more complicated trajectories  e.g. #1 . figure 1 shows the trajectories grouped according to the category of movement  and can be matched to figure 1 using the stimulus number in the second column. the third column of figure 1 shows whether or not the stimulus was animate or inanimate.
1	human animacy judgments
thirty-two adult  volunteer subjects were recruited for this study. subjects ranged in age from 1 to 1  and included 1 women and 1 men. subjects participated in a web-based questionnaire and were informed that they would be seeing video sequences containing only a single moving dot  and that this dot represented the movement of a real object. they were asked to rank each of the thirty trajectories shown in figure 1 on a scale of 1  animate  to 1  inanimate . following initial pilot subjects  not included in this data   subjects were reminded that inanimate objects might still move  such as a boulder rolling down a hill  but should still be treated as inanimate. subjects were allowed to review each video sequence as often as they liked  and no time limit was used.
모the task facing subjects was inherently under-constrained  and the animacy judgments showed high variance  a typical variance for a single stimulus across all subjects was 1 . subjects tended to find multiple interpretations for a single stimulus  and there was never a case when all subjects agreed on the animacy/inanimacy of a trajectory. to simplify the analysis  and to remove some of the inter-subject variability  each response was re-coded from the 1 scale to a single animate  1  or inanimate  1  judgment. subjects made an average of approximately 1 decisions that disagreed with the ground truth values. this overall performance measurement of 1% correct implies that the task is difficult  but not impossible. column 1 of figure 1 shows the percentage of subjects who considered each stimulus to be animate. in two cases  stimuli #1 and #1   the majority of human subjects disagreed with the ground truth values. stimulus #1 showed a dot moving alternately up and down  repeating a cycle approximately every 1 msec. subjects reported seeing this movement as  too regular to be animate.  stimulus #1 may have been confusing to subjects in that it contained an inanimate trajectory  a ball being thrown and falling  that was obviously caused by an animate  but unseen  force.
1	toby animacy judgments
the identical video sequences shown to the human subjects were processed by the trajectory formation system and the toby system. trajectory lengths were allowed to grow to 1 frames to take advantage of all of the information available in each short video clip. a winner-take-all selection method was imposed on the toby agents to simplify the reporting of the results  but subsequent processing with both other voting methods produced identical results. the final animacy judgment was determined to by the winning agent on the final time step. columns 1 and 1 of figure 1 show the winning agent and that agent's animacy vote respectively.
모overall  toby agreed with the ground truth values on 1 of the 1 stimuli  and with the majority of human subjects on 1 of the 1 stimuli. on the static object categories  the circular movement stimuli  and the straight line movement stimuli  toby matched the ground truth values perfectly. this system also completely failed on all stimuli that had natural pendulum-like movements. while our original predictions indicated that the energy agent should be capable of dealing with this class of stimuli  human subjects seemed to be responding more to the repetitive nature of the stimulus rather than the transfer between kinetic and potential energy. toby also failed on one of the thrown objects  stimulus #1   which paused when it reached its apex  and on one other object  stimulus #1  which had a failure in the trajectory construction phase.
1	conclusion
the distinction between animate and inanimate is a fundamental classification that humans as young as 1 months readily perform. based on observations that humans can perform these judgments based purely on spatio-temporal signatures  this paper presented an implementation of a few simple naive rules for identifying animate objects. using only the impoverished stimuli from the attentional system  and without any additional context  adults were quite capable of classifying animate and inanimate stimuli. while the set of agents explored in this paper is certainly insufficient to capture all classes of stimuli  as the pendulum example illustrates  these five simple rules are sufficient to explain a relatively broad class of motion profiles. these simple algorithms  like the agents presented here  may provide a quick first step  but do not begin to make the same kinds of contextual judgments that humans use.
모in the future  we intend on extending this analysis to include comparisons against human performance for multitarget stimuli and for more complex object interactions including elastic and inelastic collisions.
acknowledgments
portions of this research were funded by darpa/ito under contract number dabt 1-1   natural tasking of robots based on human interaction cues. 
references
 bingham et al.  1  geoffrey p. bingham  richard c. schmidt  and lawrence d. rosenblum. dynamics and the orientation of kinematic forms in visual event recognition. journal of experimental psychology: human perception and performance  1 :1  1.
 breazeal and scassellati  1  cynthia breazeal and brian scassellati. a context-dependent attention system for a social robot. in 1 international joint conference on artificial intelligence  1.
stimulusgroundhumantobytobystimulus categorynumbertruthjudgmentjudgmentexpertnotesstatic objects1inanimate1%inanimatestatic object1inanimate1%inanimatestatic objectthrown objects1inanimate1%inanimateenergy1inanimate1%inanimateenergy1animate1%inanimatestraight linepause at apex1animate1%animateenergyvelocity increases near apexcircular1animate1%animateenergymovements1animate1%animateenergy1animate1%animatestraight line1animate1%animateacc. sign change1animate1%animateenergystraight line1inanimate1%inanimatestraight linemovements1inanimate1%inanimatestraight line1inanimate1%inanimatestraight line1animate1%animateenergymoving up1inanimate1%inanimatestraight line1animate1%animateenergymoving up and leftpendula1inanimate1%animateenergy1inanimate1%animateacc. sign change1inanimate1%animateacc. sign change1inanimate1%animateacc. sign change1inanimate1%animateacc. sign changeerratic1animate1%animateenergyrandom movementsmovements1animate1%animateacc. sign changeleft/right bouncing1animate1%animateacc. sign changeup/down bouncing1animate1%animateacc. sign changerepeated left/right hops1animate1%animatestraight linedelay at center point1animate1%inanimatelittle datafailure to track1animate1%animateenergyfigure-11animate1%animatestraight linedelay at center pointfigure 1: comparison of human animacy judgments with judgments produced by toby for each of the stimuli from figure 1. column 1 is the ground truth  that is  whether the trajectory actually came from an animate or inanimate source. column 1 shows the percentage of human subjects who considered the stimulus to be animate. column 1 shows the animacy judgment of toby  and column 1 shows the agent that contributed that decision. bold items in the human or toby judgment columnsindicate a disagreement with the ground truth.
 breazeal et al.  1  cynthia breazeal  aaron edsinger  paul fitzpatrick  brian scassellati  and paulina varchavskaia. social constraints on animate vision. ieee intelligent systems  july/august 1. to appear.
 cohen and amsel  1  leslie b. cohen and geoffrey amsel. precursors to infants' perception of the causality of a simple event. infant behavior and develoment  1 :1  1.
 cox and hingorani  1  ingemar j. cox and sunita l. hingorani. an efficient implementation of reid's multiple hypothesis tracking algorithm and its evaluation for the purpose of visual tracking. ieee transactions on pattern analysis and machine intelligence  pami   1 :1- 1  february 1.
 fagan  1  j. f. fagan. infants' recognition of invariant features of faces. child development  1-1  1.  itti et al.  1  l. itti  c. koch  and e. niebur. a model of saliency-based visual attention for rapid scene analysis. ieee transactions on pattern analysis and machine intelligence  pami   1 :1  1.
 leslie  1  alan m. leslie. the perception of causality in infants. perception  1-1  1.
 michotte  1  a. michotte. the perception of causality. methuen  andover  ma  1.
 reid  1  d. b. reid. an algorithm for tracking multiple targets. ieee transactions on automated control  ac1 :1  december 1.
 scassellati  1  brian scassellati. theory of mind for a humanoid robot. in proceedings of the first international ieee/rsj conference on humanoid robotics  1.
 wolfe  1  jeremy m. wolfe. guided search 1: a revised model of visual search. psychonomic bulletin & review  1 :1  1.
an hybrid approach to solve the global localization problem for indoor mobile robots considering sensor's perceptual limitations
leonardo romero  eduardo morales and enrique sucar
   itesm  campus morelos  temixco  morelos  1  mexico lromero zeus.umich.mx  emorales esucar  campus.mor.itesm.mxabstract
global localization is the problem of determining the position of a robot under global uncertainty. this problem can be divided in two phases: 1  from the sensor data  or sensor view   determine the set of locations where the robot can be; and 1  devise a strategy by which the robot can correctly eliminate all but the right location. the approach proposed in this paper is based on markov localization. it applies the principal component method to get rotation invariant features for each location of the map  a bayesian classification system to cluster the features  and polar correlations between the sensor view and the local map views to determine the locations where the robot can be. in order to solve efficiently the localization problem  as well as to consider the perceptual limitation of the sensors  the possible locations of the robot are restricted to be in a roadmap that keep the robot close to obstacles  and correlations between the possible local map views are pre-computed. the hypotheses are clustered and a greedy search determine the robot movements to reduce the number of clusters of hypotheses. this approach is tested using a simulated and a real mobile robot with promising results.
1	introduction
numerous tasks for a mobile robot require it to have a map of its environment and knowledge of where it is located in the map. determining the location  position and orientation  of the robot in the environment is known as the robot localization problem. a recent survey  borenstein et al.  1  illustrates the importance of the localization problem and illustrates the large number of existing approaches.
모traditionally  localization addresses two subproblems: position tracking and global localization  thrun et al.  1 . position tracking refers to the problem of estimating the location of the robot while it is moving. drift and slippage impose limits on the ability to estimate the location of the robot within its global map. the robot knows its initial location and tracks its location after small movements using sensor data. global localization is the problem of determining the location of the robot under global uncertainty. this problem arises  for example  when a robot uses a map that has been generated in a previous run  and it is not informed about its initial location within the map.
모the global localization problem can be seen as consisting of two phases: hypothesis generation and hypothesis elimination  dudek et al.  1 . the first phase is to determine the set of hypothetical locations that are consistent with the sensing data obtained by the robot at its initial location. the second phase is to determine  in the case that contains two or more hypotheses  which one is the true location of the robot  eliminating the incorrect hypotheses. ideally  the robot should travel the minimum distance necessary to determine its exact location.
모this paper presents an approach to solve the global localization problem in a known indoor environment modeled by an occupancy grid map  a two dimensional map where the environment is divided in square regions or cells of the same size. this approach is based on markov localization  see  gutmann et al.  1   and takes into account the perceptual limitations of sensors in two ways: 1  the robot tries to keep a fixed distance to obstacles while the robot is moving and 1  the sensors have a limited range of operation. following the approach described in  romero et al.  1  to build a roadmap using the first restriction  the number of possible locations where the robot can be is significantly reduced. taking into account the second restriction  local map views are computed from the map for each cell of the roadmap. the general form of each local map view can be compressed into two real numbers using the principal component method  gonzalez and wintz  1  with the advantage that these features are rotation independent. these compressed views are then clustered by autoclass  cheeseman and stutz  1   a bayesian classification system. the idea to generate the hypotheses is to compute the principal components of the sensor data  or sensor view   then use autoclass to predict the best class  or classes  and finally perform polar correlations between the sensor view and the predicted local map views. to eliminate hypotheses  a greedy search is performed upon the correlations of the local map views of the same class.
모the rest of this paper is organized as follows. section 1 reviews some related relevant literature. section 1 describes our approach to generate hypotheses. section 1 presents the framework of markov localization. section 1 explains the approach to eliminate hypotheses. experimental results using a mobile robot simulator and a mobile robot with a low cost laser range sensor  implemented by a laser line generator and a camera   are shown in section 1. finally  some conclusions are given in section 1.
1	related work
theoretical work on the subject has been reported in the literature. a solution to the hypothesis generation phase is given in  guibas et al.  1 . in  dudek et al.  1  it is shown that the problem of localizing a robot with minimum travel is np-hard. these works make strong assumptions: first  perfect knowledge about the orientation of the mobile robot is considered; second  they assume the availability of noise-free sensor data.
모practical work has also been reported  see  thrun  1; castellanos and tardos  1  . landmark methods rely on the recognition of landmarks to keep the robot localized geometrically. while landmark methods can achieve impressive geometric localization  they require efficient recognition of features to use as landmarks. in contrast  dense sensor methods attempt to use whatever sensor information is available to update the location of the robot. they do this by matching dense sensor scans against a surface map of the environment  without extracting landmark features. see  gutmann et al.  1  for an experimental comparison of two dense sensor matching methods: kalman filter and markov localization. some correlation techniques of laser range-finder scans are given in  weib et al.  1 . in  fox and burgard  1  a method to move the robot in order to eliminate hypotheses is given. the idea is to move the robot so as to minimize future expected uncertainty  measured by the entropy of future belief distributions.
1	hypothesis generation
in this section and the two following sections a simple occupancy grid map is used as an example to show the ideas behind the proposed approach. figure 1  a  shows this simple map built using a mobile robot simulator. figure 1  b  shows a local map view  extracted from the map for the position of the robot shown in  a   considering that the robot direction is aligned with a global fixed direction  pointing downwards in this case . figure 1  c  shows the sensor view  generated by the simulator  considering that the robot is aligned with the global direction. both views have an angular resolution of 1 degrees  a common value found in laser range sensors  and include the robot position for reference as a black cell. perceptual limitations are taken into account setting a maximum range of meters.
모the problem consist in estimating the set of possible locations that have local map views consistent with the sensor view.
1	polar correlation
to have a simple model of robot motion and hence a small state space in the markov localization  we assume that the robot should be in one of 1 possible directions   degrees     with respect to the global fixed direction  one for each adjacent cell. a polar correlation  using a sum of

figure 1: a simple environment. from left to right:     occupancy grid map.     local map view for the robot location showed in  a .     sensor view

figure 1: correlation results. angular displacements are from 1  left  to degrees  right .
absolute differences  can be used to find the match between a local map view and the sensor view. figure 1 shows the correlation results for all the possible angular displacements of the sensor view against the local map view shon in figure 1. from the minimum difference an angular displacement can be computed to align the robot with one of the directions . obviously the right angular displacement should be indicated considering the most probable position of the robot. in the case of figure 1 the angular displacement corresponds to degres  and the best estimated direction is degrees.
모as the markov localization needs a probabilistic value of perceiving a sensor view given that the robot is at location  cell with direction   a difference value can be computed for direction from the correlation results between the sensor view and the local map view at the cell   and then a probabilistic value can be obtained from . we compute as the minimum difference  in the correlation results  for an angular interval with center at . the desired probability is computed by where is positive real number.
모considering that this procedure is expensive  next sections show a fast procedure to find a small set of candidate cells to apply this procedure  instead of all the free cells in the map.
1	roadmap
following the ideas described in  romero et al.  1   the set of possible cells  where the robot is allowed to move  tries to keep a fixed distance to obstacles. it associates higher costs to free cells close to obstacles and the lowest costs to cells at distance from obstacles. figure 1  a  shows the full set of free cells where the robot can be as white pixels  while figure 1  b  shows the cells that form the roadmap. there is a significant reduction in the number of cells. let be the set of cells of the roadmap.
in the next section we are going to consider only the cells
.

figure 1: a roadmap. from left to right:     full set of free cells.     the roadmap for = 1 m.

figure 1: clusters found by autoclass. different gray level are associated to different classes
1	principal component analysis
we apply the principal component analysis described in  gonzalez and wintz  1  to find two rotation invariant features from a local map view or sensor view. the coordinates of each occupied cell of the view can be interpreted as two dimensional random variables with mean and covariance . it is noted that is a two dimensional vector and is a 1 matrix. the eigenvectors and of point in the directions of maximum variance  subject to the constraint that they are orthogonal  and the eigenvalues and are equal to the variance along eigenvectors and respectively. our approach uses the two eigenvalues as rotation invariant features for a local map view or sensor view.
1	bayesian clustering
since the eigenvalues associated to adjacent cells of the roadmap normally do not change a lot  it makes sense to cluster cells according to their eigenvalues. for this purpose we use autoclass  cheeseman and stutz  1 . figure 1 show with different gray level the three classes found for the cells of the roadmap shown in figure 1  b . autoclass found the same class for cells close to the end cells of the roadmap  another class for cells in the middle of the roadmap and the rest of cells in a third class.
모at the beginning of the localization process  to take the robot to the roadmap  we apply the algorithm described in  romero et al.  1  to learn a map of the environment. using that approach it is easy to know if the robot is in the roadmap or need to execute some movements. joining all these parts  the hypothesis generation does the following steps:
1. compute the eigenvalues	from the sensor view.
1. with	  use autoclass to predict the most probable classes.

figure 1: most probable locations for the robot
1. for all cells in the classes found in the previous step  apply the correlation technique to get the angular displacement that align the robot with one of the possible directions and compute the conditional probabilities
     . the conditional probability for other cells of the roadmap are set to a very small value.
모for the sensor view shown in figure 1  c  the conditional probabilities are shown in figure 1. if the probability of finding the robot at location of the roadmap is equal for all the cells in the roadmap  the same figure shows the most probable locations for the robot. the locations in the left top part of the figure point downwards and the locations in right bottom part of the figure point to the left. the following section describes the process to update the probability of hypotheses after the robot senses or moves.
1	markov localization
the key idea of markov localization is to compute a probability distribution over all possible locations in the environment. denotes the probability of finding the robot at location at time . here  is a location in space where and are cartesian coordinates of cells and is a valid orientation. reflects the initial state of knowledge and it is uniformly distributed to reflect the global uncertainty. is updated whenever ...
1. ... the robot moves. robot motion is modeled by a conditional probability  denoted by . denotes the probability that motion action   when executed at   carries the robot to . is used to update the belief upon robot motion:
		 1 
here is a normalizer that ensures that sums up to 1 over all . an example of is shown in
figure 1  considering that the robot moves in the thick roadmap  and the orientation of the robot is aligned to one of the possible 1 directions   as it was described in a previous section.
1. ... the robot senses. when sensing  

figure 1: robot motion for one direction

p
1
p
1
figure 1: hypotheses after a movement. from left to right:     one hypotheses.     two hypothesis
		 1 
here is the probability of perceiving at location . in order to get a efficient procedure to update the probability distribution  cells with probability below some threshold are set to zero.
모to get a robust and efficient procedure  our approach considers a thick roadmap which include the cells in the neighborhood of the roadmap  thin roadmap  besides the cells in the roadmap. the possible hypotheses for the robot are restricted to be in the thick roadmap.
1	hypothesis elimination
let be the probability of finding the robot at location at time . we consider as hypotheses only those locations with   where is a threshold value. let be the number of hypotheses.
모to eliminate hypotheses  the robot should move through the roadmap. after a robot move  there are two different scenarios for the cells that correspond to the different hypotheses: 1  the cells are in the roadmap under all the hypotheses  and 1  the cells are in the roadmap for less than hypotheses. in the first scenario  the movement does not help to eliminate hypotheses; in the second scenario the movement helps to decrease the number of hypotheses. figure 1 shows an example of both scenarios. if there are two hypotheses and then the movement  represented by a rotation followed by a translation  showed in figure 1  a  helps to reduce the number of hypotheses while the other movement is not useful.
모to get an efficient procedure  our approach considers that the mobile robot is at the most probable location and then considers all the cells of the thin roadmap as valid movements

figure 1: number of hypotheses associated to cells of the roadmap using the roadmap criteria. darker cells have lower number of hypotheses
for the robot. if we assign to locations the number of possible hypotheses  a good movement to eliminate hypotheses is to direct the robot toward the nearest cell with less than hypotheses.
모if we consider the hypotheses shown in figure 1  figure 1 shows the number of possible hypothesis as an image. darker cells have lower values for the number of hypotheses. in this case the best hypothesis is located in the right bottom part of the roadmap and it is pointing to the left. the robot should move to the nearest darker cell of the roadmap. let this cell be called the cell.
모if we have a criteria to consider two local map views as similar  we can use it to detect shorter movements than using only the previous criteria. the two scenarios discussed have two variants: a  the cells are similar considering its local map views  and b  not all the cells look similar. the robot should move to a cell with scenario  b .
모a similarity measure can be computed using the correlation technique previously presented. two cells look similar if the computed probability is abovesome threshold. if there are cells in the roadmap  let       denote the cells of the roadmap  and be the similarity between the local map view associated to cells and . for all form a similarity matrix and it can be computed from the map and the roadmap  before the localization process starts. to compute this matrix we can take advantage of the fact that this matrix is symmetric and have zero values in the main diagonal.
모figure 1 shows the similarity matrix for our example. in this case  the cells of the thin roadmap were ordered following a depth first search trying to keep cells with adjacent indices associated to adjacent cells in the roadmap. the image shows that there are only a few cells with significant similarities that are not close to each other. this makes sense if we consider the map shown in figure 1 and the limited range of the sensors.
모two cells and in the thick roadmap are considered similar if is above some threshold  where and are the closest cells in the thin roadmap to and respectively.
모if we consider the hypotheses shown in figure 1 and the similarity criteria  figure 1 shows the number of possible hypotheses as an image. in this case  the goal cell is closer to the starting location than the one in figure 1.
another improvement over the last criteria is to cluster the

figure 1: similarity matrix. darker pixels mean lower similarities

figure 1: number of hypotheses associated to cells of the roadmap using the roadmap and the similarity criteria. darker cells have lower number of hypotheses
hypotheses and then the goal cell is the cell that reduces the number of clusters. in other words  the best movement for the robot should reduce the number of clusters  not the total number of hypotheses. for instance  if there are two clusters
      and has 1 hypotheses and has only 1 hypothesis  and the most probable hypothesis is in cluster   then a movement that eliminates one hypothesis in cluster is better than one movement that eliminates one or two hypotheses in cluster .
모for the clustering process we use autoclass with the coordinates of the hypotheses. if there is only one cluster  then the goal cell is the nearest cell that reduces the number of hypotheses. the robot is localized if there is only one cluster and there is no cell that reduces the number of hypotheses.
모once a goal cell is found  the robot should perform some movements towards that cell  updating the probability distribution.
1	experimental results
this section presents preliminary results obtained using a mobile robot simulator and a real mobile robot. both robots use  or simulate  sonars and a low cost laser range sensor  implemented with a laser line generator and a camera. the laser sensor gives good measurements within a range of 1 m.
모figure 1  a  shows a more complex simulated environment. figure 1  b  shows the map built with its roadmap. the map is about 1 and the roadmap considers a distance to obstacles of 1m.
모figure 1 shows 1 clusters found by autoclass  and figure 1 shows the associated similarity matrix. the symmetric nature of the map is captured in the similarity matrix.

figure 1: a more complex map built using a mobile robot simulator. from left to right:     simulated environment.     map built with its roadmap

figure 1: clusters found by autoclass for environment of the figure 1.
모if the robot is located as it is shown in figure 1  a   then figure 1 shows the most probable locations for the robot.
autoclass found two clusters	 pointing downwards  and
    pointing upwards  and the location with the highest probability is in . the hypotheses elimination process found a goal cell pointed by in the figure. this result makes sense because the possible hypotheses after that movement have different sensor views.
모figure 1 shows a map built using the real mobile robot. autoclass found in this case 1 clusters and the similarity matrix is shown in figure 1. in this real environment the robot localization is easier than in the previous simulated environ-

figure 1: similarity matrix for environment of the figure
1. darker pixels mean lower similarities

figure 1: most probable locations for the robot for the environment showed in figure 1  a .

figure 1: a real mobile robot in a real environment. from left to right:     map and its roadmap.     similarity matrix
ment.
1	conclusions
an hybrid approach to solve the global localization problem in indoor environments has been presented. it can be seen as a merge of a landmark approach and a dense sensor method based on the markov localization. landmarks are generated and recognized by a bayesian clustering technique  autoclass  from rotation invariant features  principal components  of sensor or local map views. these landmarks are used to restrict the set of possible locations for the robot and so increase the speed of the markov localization process.
모in the hypothesis elimination phase a fast greedy search is introduced. this search uses a similarity matrix to compute a similarity criteria between the map views of cells. the search considers movements towards cells of the thin roadmap that reduce the number of clusters of hypotheses.
모to get an efficient localization algorithm  the approach uses the thick roadmap concept  instead of the full set of free cells; restricts the number of possible orientations of the robot  takes advantage of a correlation technique to align the robot and pre-compute the similarity matrix.
모we plan to explore a non deterministic kind of movement in the hypotheses elimination phase using the markov framework to perform sequence of  move-sense  for each cluster of hypotheses. we think this kind of movements will be able to predict a goal cell even when the map is not very accurate or in the case of robots with high odometric errors.
references
 borenstein et al.  1  j. borenstein  b. everett  and l. feng. navigating mobile robots: systems and techniques. a.k. peter  ltd.  wellesley  ma  1.
 castellanos and tardos  1  j.a. castellanos and j. d. tardos. mobile robot localization and map building: a multisensor fusion approach. kluwer academic publishers  1.
 cheeseman and stutz  1  p. cheeseman and j stutz. bayesian classification  autoclass : theory and results. in u.-m. fayyad  g. piatetsky-shapiro  p. smyth  and r. uthurusamy  editors  advances in knowledge discovery and data mining. aaai press/mit press  1.
 dudek et al.  1  g. dudek  k. romanik  and s. whitesides. localizing a robot with minimum travel. siam j. comput  1 :1  1.
 fox and burgard  1  d. fox and w. burgard. active markov localization for mobile robots. robotics and autonomous systems  1.
 gonzalez and wintz  1  r. c. gonzalez and p. wintz. digital image processing. addison-wesley  1 edition  1.
 guibas et al.  1  l. guibas  r. motwani  and p. raghavan. the robot localization problem. in k. goldberg  d. halperin  and j. c. latombe  editors  algorithmic foundations of robotics. a. k. peters  1.
 gutmann et al.  1  j.-s. gutmann  w. burgard  d. fox  and k. konolige. an experimental comparison of localization methods. in proc. international conference on intelligent robots and systems  iros'1   1.
 romero et al.  1  l. romero  e. morales  and e. sucar. a robust exploration and navigation approach for indoor mobile robots merging local and global strategies. in m. c. monard and j.s. sichman  editors  advances in artificial intelligence  lnai 1. springer  1.
 thrun et al.  1  s. thrun  a. bucken  w. burgar  et al. map learning and high-speed navigation in rhino. in d. kortenkamp  r. p. bonasso  and r murphy  editors  artificial intelligence and mobile robots. aaai press/the mit press  1.
 thrun  1  s. thrun. learning maps for indoor mobile robot navigation. artificial intelligence  1 :1  1.
 weib et al.  1  g. weib  c. wetzler  and e. puttkamer. keeping track of position and orientation of movingindoor systems by correlation of range-finder scans. in intelligent robots and systems  1.
 
multimodal integration - a biological view 
michael h. coen 
mit artificial intelligence lab 1 technology square 
cambridge  ma 1 mhcoen ai.mit.edu  
 
 
abstract 
we present a novel methodology for building highly integrated multimodal systems.  our approach is motivated by neurological and behavioral theories of sensory perception in humans and animals.  we argue that perceptual integration in multimodal systems needs to happen at all levels of the individual perceptual processes.  rather than treating each modality as a separately processed  increasingly abstracted pipeline - in which integration over abstract sensory representations occurs as the final step - we claim that integration and the sharing of perceptual information must also occur at the earliest stages of sensory processing.  this paper presents our methodology for constructing multimodal systems and examines its theoretic motivation.  we have followed this approach in creating the most recent version of a highly interactive environment called the intelligent room and we argue that doing so has provided the intelligent room with unique perceptual capabilities and gives insight into building similar complex multimodal systems. 
introduction   
this paper proposes a novel perceptual architecture motivated by surprising results about how the brain processes sensory information.  these results  gathered by the cognitive science community over the past 1 years  have challenged century long held notions about how the brain works and how we experience the world we live in.  we argue that current approaches to building multimodal systems that perceive and interact with the real  human world are flawed and based largely upon assumptions described by piaget  1  - although tracing back several hundred years - that are no longer believed to be particularly accurate or relevant. 
 instead  we present a biologically motivated methodology for designing interactive systems that reflects more of how the brain actually appears to process and merge sensory inputs.  this draws upon neuroanatomical  psychophysical  evolutionary  and phenomenological evidence  both to 
                                                 
 this material is based upon work supported by the advanced research projects agency of the department of defense under contract number daad1-c-1  monitored through rome laboratory.  additional support was provided by the microsoft corporation. 
critique modern approaches and to suggest an alternative for building artificial perceptual systems.  in particular  we argue against post-perceptual integration  which occurs in systems where the modalities are treated as separately processed  increasingly abstracted pipelines.  the outputs of these pipelines are then merged in a final integrative step  as in figure 1.  the main difficulty with this approach is that integration happens after the individual perceptions are generated.  integration occurs after each perceptual subsystem has already  decided  what it has perceived  when it is too late for intersensory influence to affect the individual  concurrent unimodal perceptions.  multimodal integration is thus an assembly rather than a perceptual process in most modern interactive systems.  in this  it is an abstraction mechanism whereby perceptual events are 
integration
	s1	s1	s1 	s1
	s1	s1	s1 	s1
	s1	s1	s1 	s1
 
figure 1 - post-perceptual integration in a multimodal system.  here  visual  auditory  and haptic inputs pass through specialized unimodal processing pathways and are combined in the integration mechanism on top  which creates multimodal perceptions by extracting and reconciling data from the individual inputs.  notice that multiple pathways may process a single sensory input. for example  the auditory signal in this system might be fed into separate speech recognition and spatial localization systems  designated s1 and s1 respectively. 
separated from the specific sensory mechanisms that generate them and then integrated into higher-level representations. 
 this paper examines the idea that multimodal integration is a fundamental component of perception itself and can shape individual unimodal perceptions as much as does actual sensory input.  this idea is well supported biologically  and we argue here for the benefits of building interactive systems that support cross-modal influence - systems in which sensory information is shared across all levels of perceptual processing and not just in a final integrative stage.  doing this  however  requires a specialized approach that differs in basic ways from how interactive systems are generally designed today.     
 this paper presents our methodology for constructing multimodal systems and examines its theoretic motivation.  we have followed this approach in creating the most recent version of a highly interactive environment called the intelligent room  and we argue that doing so has provided the intelligent room with unique perceptual capabilities and provides insight into building similar complex multimodal systems.  we specifically examine here how the intelligent room's vision and language systems share information to augment each other and why the traditional ways these systems are created made this sharing so difficult to implement.  our approach is similar in spirit to the work of  atkeson et al. 1  brooks et al. 1  cheng and kuniyoshi 1  ferrell 1  nakagawa 1  and sandini et al. 1 .  although they are primarily concerned with sensorimotor coordination  there is a common biological inspiration and long-term goal to use knowledge of human and animal neurophysiology to design more sophisticated artificial systems. 
background 
 who would question that our senses are distinct   we see  we feel  we hear  we smell  and we taste  and these are qualitatively such different experiences that there is no room for confusion among them.  even those affected with the peculiar syndrome synesthesia  in which real perceptions in one sense are accompanied by illusory ones in another  never lose awareness of the distinctiveness of the senses involved.  consider the woman described in  cytowic 1   for whom a particular taste always induced the sensation of a particular geometric object in her left hand.  a strange occurrence indeed  but nonetheless  the tasting and touching - however illusory - were never confused; they were never merged into a sensation the rest of us could not comprehend  as would be the case  for example  had the subject said something tasted octagonal.  even among those affected by synesthesia  the sensory channels remain extremely well defined. 
모given that our senses appear so unitary  how does the brain coordinate and combine information from different sensory modalities   this has become known as the binding problem  and the traditional assumption has been to assume that the sensory streams are abstracted  merged  and integrated in the cortex  at the highest levels of brain functioning.  this was the solution proposed by piaget  1  and in typical piagetian fashion  assumed a cognitive developmental process in which children slowly developed high-level mappings between innately distinct modalities through their interactions with the world. this position directly traces back to helmholtz  1   and even earlier  to berkeley  1  and locke  1   who believed that neonatal senses are congenitally separate and interrelated only through experience.  the interrelation does not diminish the distinctiveness of the senses themselves  it merely accounts for correspondences among them based on perceived cooccurrences.   
모the piagetian assumption underlies nearly all modern interactive  multimodal systems - it is the primary architectural metaphor for multimodal integration.  the unfortunate consequence of this has been making integration a post-perceptual process  which assembles and integrates sensory input after the fact  in a separate mechanism from perception itself.  in these multimodal systems  each perceptual component provides a generally high-level description of what it sees  hears  senses  etc.   some systems  such as sensor networks  provide low-level feature vectors  but the distinction is not relevant here.   this  for example  may consist of locations of people and how they are gesturing  what they are saying and how  e.g.  prosody data   and biometric data such as heart rates.  this information is conveyed in modal-specific representations that capture detail sufficient for higher-level manipulation of the perceptions  while omitting the actual signal data and any intermediate analytic representations.  typically  the perceptual subsystems are independently developed and trained on unimodal data; each system is designed to work in isolation.   see figure 1.   they are then interconnected through some fusive mechanism  as in figure 1  that combines temporally proximal  abstract unimodal inputs into an integrated event model.  the integration itself may be effected via a neural network  e.g.  waibel et al. 1   hidden markov models  e.g.  stork and hennecke 1   unification logics  e.g.  cohen et al. 1   or various ad hoc techniques  e.g.  wu et al. 1 .  the output of the integration process is then fed into some higher-level interpretative mechanism - the architectural equivalent of a cortex. 
 this post-perceptual approach to integration denies the possibility of cross-modal influence  which is pervasive in biological perception.  our visual  auditory  proprioceptive  somatosensory  and vestibular systems influence one another in a complex process from which perceptions emerge as an integrated product of a surprising diversity of components.   for surveys  see stein and meredith 1  lewkowicz and lickliter 1  and to a lesser extent  thelen and smith 1.   for example  consider the seminal work of mcgurk and macdonald  1 .  in preparing an experiment to determine how infants reconcile conflicting information in different sensory modalities  they had a lab technician dub the audio syllable /ba/ onto a video of someone saying the syllable /ga/.  much to their surprise  upon viewing the dubbed video  they repeatedly and distinctly heard the syllable /da/  alternatively  some hear /tha/   corresponding neither to the actual audio nor video sensory input.  initial assumptions that this was due to an error on the part of the technician were easily discounted simply by shutting their eyes while watching the video; immediately  the sound changed to a /ba/.  this surprising fused perception  subsequently verified in numerous redesigned experiments and now known as the mcgurk effect  is robust and persists even when subjects are aware of it. 
 the mcgurk effect is perhaps the most convincing demonstration of the intersensory nature of face-to-face spoken language and the undeniable ability of one modality to radically change perceptions in another.  it has been one of many components leading to the reexamination of the piagetian introspective approach to perception.  although it may seem reasonable to relegate intersensory processing to the cortex for the reasoning  as opposed to perceptual  processes that interested piaget  such as in cross modal matching  it becomes far more implausible in cases where different senses impinge upon each other in ways that locally change the perceptions in the sensory apparatus themselves.  one might object that the mcgurk effect is pathological - it describes a perceptual phenomenon outside of ordinary experience.  only within controlled  laboratory conditions do we expect to have such grossly conflicting sensory inputs; obviously  were these signals to co-occur naturally in the real world  we would not call them conflicting.  we can refute this objection both because the real world is filled with ambiguous  sometimes directly conflicting  perceptual events  and because the mcgurk effect is by no means the only example of its kind.  there is a large and growing body of evidence that the type of direct perceptual influence illustrated by the mcgurk effect is commonplace in much of ordinary human and more generally animal perception  and it strongly makes the case that our perceptual streams are far more interwoven than conscious experience tends to make us aware.  for example  the sight of someone's moving lips in an environment with significant background noise makes it easier to understand what the speaker is saying; visual cues - e.g.  the sight of lips - can alter the signal-to-noise ratio of an auditory stimulus by 1 decibels  sumby and pollack 1 .  thus  a decrease in auditory acuity can be offset by increased reliance on visual input.  although the neural substrate behind this interaction is unknown  it has been determined that just the sight of moving lips - without any audio component - modifies activity in the auditory cortex  sams et al 1 .  in fact  psycholinguistic evidence has long lead to the belief that lip-read and heard speech share a degree of common processing  notwithstanding the obvious differences in their sensory channels  dodd et al 1 . 
 perhaps the most dramatic cross-modal interactions were demonstrated in the landmark studies of meltzoff and moore  1   who showed that infants could imitate an investigator's facial expression within hours of birth.  for example  the investigator sticking out his tongue lead the infant to do the same  although the infant had never seen its own face.  somehow  the visual cue is matched with the proprioceptive sensation of tongue protrusion.  it is extraordinarily difficult to view this as a learned behavior; this will be quite relevant in considering below where the knowledge governing cross-modal influence should come from in computational systems.    
 we believe that the current approach to building multimodal interfaces is an artifact of how people like to build computational systems and not at all well-suited to dealing with the cross-modal interdependencies of perceptual understanding.  perception does not seem to be amenable to the clear-cut abstraction barriers that computer scientists find so valuable for solving other problems  and we claim this approach has lead to the fragility of so many multimodal systems.   we also dispute the notion that perception generally corresponds well to a discrete and symbolic event model.  although such a model is well suited to many types of computational applications  it is frequently more useful to view perception as a dynamic process  rather than an event occurring at some point in time  thelen and smith 1 .  this is all the more so during development  i.e. learning   where the space of events is fluid and subject to constant change  and during sensory fusion  where complex codependences and interactions among the different senses confound our simple event-biased predispositions.  a similar dynamic approach has been taken by  ullman 1  for explaining cross-feature influence in visual perception.  
 
	face detection	sentence structure
	oval detection	phrasal structure
	skin detection	word co-  occurrence
	depth mapping	lexical  segmentation
	motion detection	phonetic segmentation
   figure 1 - unimodal processing pathways.  individual modalities are processed in specialized pipelines.  the visual pathway on the left uses 1-dimensional depth and color maps to find candidate regions for locating faces.   this follows darrell et al. 1.   the auditory pathway on the right performs speech recognition. notice that higher-level syntactic constraints feed back into the lower-level morphological analysis. a multimodal system 
the examples for our discussion of multimodal interactions will be drawn from the intelligent room project  as described in  coen 1  1 .  the intelligent room has multiple perceptual user interfaces  supporting both visual and verbal interactions  which connect with some of the ordinary human-level events going on within it.  the goal of the room is to support people engaged in everyday  traditionally non-computational activity in both work and leisure contexts.  figure 1 contains a picture of the room  which contains nine video cameras  three of which the room can actively steer  several microphones  and a large number of computer-interfaced devices.  because the intelligent room is designed to support a range of activities that have never before explicitly involved computers  it in no way resembles a typical computer science laboratory.  its computational infrastructure is removed from view to enhance the room's appeal as a naturally interactive space with a decidedly understated  low-tech  atmosphere.  the room's computational infrastructure  coen et al. 1   consisting of over 1 software agents running on a network of ten workstations  is housed in an adjacent laboratory.   
 before exploring scenarios for new types of multimodal interactions  we first examine a traditionally explicit one in the resolution of a deictic reference  i.e.  use of a word such as this in referring to a member of some class of objects.  suppose  for example  someone in the room says   dim this lamp.   the room uses its ability to track its occupants  in conjunction with a map of its own layout  to dim the lamp closest to the speaker when the verbal command was issued.  this kind of interaction can be implemented with a simple post-perceptual integration mechanism that reconciles location information obtained from the person tracker with the output of a speech recognition system.  here  multimodal integration of positional and speech information allows straightforward disambiguation of the deictic lamp reference.   
motivations 
 given the simplicity of the above example  it seems far from obvious that a more complex integration mechanism is called for.  to motivate a more involved treatment  we start by examining some of the problems with current approaches. 
 despite many recent and significant advances  computer vision and speech understanding  along with many other perceptual interface research areas  picard 1  massie and salisbury 1   are still infant sciences.  the non-trivial perceptual components of multimodal systems are therefore never  perfect  and are subject to a wide variety of failure modes.  for example  the room may  lose  people while visually tracking them due to occlusion  coincidental color matches between fore and background objects  unfavorable lighting conditions  etc.  although the particular failure 

figure 1 - a view of the intelligent room with three of its nine cameras visible and circled.  the two lower of these in the picture can be panned and tilted under room control. 
modes of the modalities varies with them individually  it is a safe assumption that under a wide variety of conditions any one of them may temporarily stop working as desired.  how these systems manifest this undesired operation is itself highly idiosyncratic.  some may simply provide no information  for example  a speech recognition system confused by a foreign accent.  far more troublesome are those that continue to operate as if nothing were amiss but simply provide incorrect data  such as a vision-based tracking system that mistakes a floor lamp for a person and reports that he is standing remarkably still. 
 that perceptual systems have a variety of failure modes is not confined to their artificial instantiations.  biological systems also display a wide range of pathological conditions  many of which are so engrained that they are difficult to notice.  these include limitations in innate sensory capability  as with visual blind spots on the human retina  and limited resources while processing sensory input  as with our linguistic difficultly understanding nested embedded clauses  miller and chomsky 1 .  stein and meredith  1  argue for the evolutionary advantages of overlapping and reinforcing sensory abilities; they reduce dependence on specific environmental conditions and thereby provide clear survival advantages.   a striking example of this is seen in a phenomenon known as the  facial vision  of the blind.  in locating objects  blind people often have the impression of a slight touch on their forehead  cheeks  and sometimes chest  as though being touched by a fine veil or cobweb  james 1  p1 . the explanation for this extraordinary perceptory capability had long been a subject of fanciful debate.  james demonstrated  by stopping up the ears of blind subjects with putty  that audition was behind this sense  which is now known to be caused by intensity  direction  and frequency shifts of reflected sounds  arias 1 .  the auditory input is so successfully represented haptically in the case of facial vision that the perceiver himself cannot identify the source of his perceptions. 
 research on interactive systems has focused almost entirely on unimodal perception: the isolated analysis of auditory  linguistic  visual  haptic  or to a lesser degree biometric data.  it seems to put the proverbial cart before the horse to ponder how information from different modalities can be merged while the perceptory mechanisms in the sensory channels are themselves largely unknown.  is it not paradoxical to suggest we should or even could study integration without thoroughly understanding the individual systems to be integrated   nonetheless  that is the course taken here.  we argue that while trying to understand the processing performed within individual sensory channels  we must simultaneously ask how their intermediary results and final products are merged into an integrated perceptual system.  we believe that because perceptual systems within a given species coevolved to interoperate  compatibility pressures existed on their choices of internal representations and processing mechanisms.  in order to explain the types of intersensory influence that have been discovered experimentally  disparate perceptual mechanisms must have some degree of overall representational and algorithmic compatibility that makes this influence possible.  the approach taken here is entirely gestalt  not only from a gibsonian  1  perspective  but because we have no example of a complex unimodal sensory system evolving in isolation.  even the relatively simple perceptual mechanisms in paramecium  stein and meredith 1  chapter 1  and sponges  mackie and singla 1  have substantial cross-sensory influences.  it seems that perceptual interoperation is a prerequisite for the development of complex perceptual systems.  thus  rather than study any single perceptual system in depth - the traditional approach - we prefer to study them in breadth  by elucidating and analyzing interactions between different sensory systems.   
cross-modal influences 
how then might cross-modal influences be used in a system like the intelligent room   answering this question is a twostep process.  because the intelligent room is an engineered as opposed to evolved system  we first need to explicitly find potential synergies between its modalities that can be exploited.  once determined  these synergies must then somehow be engineered into the overall system  and this emerges as the primary obstacle to incorporating crossmodal influences into the intelligent room and more generally  to other types of interactive systems. 
 we begin with the following two empirical and complementary observations: 
 
1  people tend to talk about objects they are near.      
 figure 1a   
1  people tend to be near objects they talk about.  figure 
1b  
 
these heuristics reflect a relationship between a person's location and what that person is referring to when he speaks; knowing something about one of them provides some degree of information about the other.  for example  someone walking up to a video display of a map is potentially likely to speak about the map; here  person location data can inform a speech model.  conversely  someone who speaks about a displayed map is likely to be in a position to see it; here  speech data can inform a location model.  of course  it is easy to imagine situations where these heuristics would be wrong.  nonetheless  as observations they are frequently valid and it would be reasonable to somehow incorporate influences based on them into a system like the intelligent room.  mechanistically  we might imagine the person tracking system exchanging information with the speech recognition system.  for example  the tracking system might send data  which we will call a hint  to the speech recognition system to preferentially expect utterances involving objects the person is near  such as a map.   conversely  we can also imagine that the speech recognition 
a   
map
 
b   
map
모figure 1a - people talk about objects they are near. someone approaching a projected display showing  for example  a map  is more likely to make a geographical utterance.  here  location information can augment speech recognition. 
   figure 1b - people are near objects they talk about. someone speaking about the contents of a video display is more likely to be located somewhere  delineated by the triangle  from which the display is viewable to him.  here  speech information can augment person tracking. 
system would send hints to the person tracking system to be especially observant when looking for someone in indicated sections of the room  based on what that person is referring to in his speech. 
 this seems reasonable until we try to build a system that actually incorporates these influences.  there are both representational and algorithmic stumbling blocks that make this conceptually straightforward cross-modal information sharing difficult to implement.  these are due not only to post-perceptual architectural integration  but also to how the perceptual subsystems  such as those in figure 1  are themselves typically created.  we first examine issues of representational compatibility  namely what interlingua is used to represent shared information  and then address how the systems could incorporate hints they receive in this interlingua into their algorithmic models. 
 consider a person tracking system that provides the coordinates of people within a room in real-time  relative to some real-world origin - the system outputs the actual locations of the room's occupants.  we will refer to the tracking system in the intelligent room as a representative example of other such systems  e.g.  wren et al. 1  gross et al. 1 .  its only input is a stereo video camera and its sole output are sets of  x y z  tuples representing occupants' centroid head coordinates  which are generated at 1hz.  contrast this with the intelligent room's speech recognition system  which is based upon the java speech api  sun 1   built upon ibm's viavoice platform  and is typical of similar spoken language dialog systems  e.g.  zue et al. 1 .  its inputs are audio voice signals and a formal linguistic model of expected utterances  which are represented as probabilistically weighted context free grammars.   
 how then should these two systems exchange information   it does not seem plausible from an engineering perspective  whether in natural or artificial systems  to provide each modality with access to the internal representations of the others.   thus  we do not expect that the tracking system should know anything about linguistic models nor we do expect the language system should be skilled in spatial reasoning and representation.   even if we were to suppose the speech recognition system could somehow represent spatial coordinates  e.g. as  x y z  tuples  that it could communicate to the person tracking system  the example in figure 1b above involves regions of space  not isolated point coordinates.  from an external point of view  it is not obvious how the tracking system internally represents regions  presuming it even has that capability in the first place.  the complementary example of how the tracking system might refer to classes of linguistic utterances  as in figure 1a above  is similarly convoluted.    unfortunately  even if this interlingua problem were easily solvable and the subsystems had a common language for representing information  the way most perceptual subsystems are implemented would make incorporation of cross-modal data difficult or impossible.  for example  in the case of a person tracking system  the real-world body coordinates are generated via three-dimensional spatial reconstruction based on correspondences between sets of image coordinates.  the various techniques for computing the reconstructed coordinates  such as neural networks or fit polynomials  are in a sense closed - once the appropriate coordinate transform has been learned  there is generally no way to bias the transformation in favor of particular points or spatial regions.  thus  there is no way to incorporate the influence  even if the systems had a common way of encoding it.  here again  the complementary situation with influencing speech recognition from a tracking system can be similarly intractable.  for example  not all linguistic recognition models  e.g.  bigram-based  support dynamic preferential weighting for classes of commonly themed utterances.  so  even if the tracking system could somehow communicate what the speech recognition system should expect to hear  the speech recognition system might not be able to do anything useful with this information. 
 we see that not only are the individual modal representations incompatible  the perceptual algorithms  i.e.  the contents of the sensory pipelines  are incompatible as well.  this comes as no surprise given that these systems were engineered primarily for unimodal applications.  unlike natural perceptual systems within an individual species  artificial perceptual systems do not co-evolve  and therefore  have had no evolutionary pressure to force representational and algorithmic compatibility.  these engineered systems are intended to be data sources feeding into other systems  such as the ones performing multimodal integration  that are intended to be data sinks.  there is no reason to expect that these perceptual subsystems would or even could directly interoperate. 
designing for interaction 
모our solution was to redesign the intelligent room's perceptual systems with the explicit intension that they should interact with each other.  doing so required fundamental representational and algorithmic changes but has made possible subtle types of cross-modal interactions that were previously unworkable.  we first detail two different categories of intersensory function and then explain how the cross-modal influences described above were implemented in the intelligent room. 
모consider  for example  the effect of touching someone and having his head and eyes turn to determine the source of the stimulus.  this is clearly an example of cross-modal influence - the position of the touch determines the foveation of the eyes - but it is fundamentally different than the interaction described in the mcgurk effect above  where the influence is evidenced solely in perceptual channels.  the touch scenario leads to behavioral effects that center the stimulus with respect to the body and peripheral sensory organs.  the mcgurk effect is an example of sensory influence within perceptual channels and has no behavioral component.   
 motor influences - i.e.  ones that cause attentive and orientation behaviors - are by far the more understood of the 
 
figure 1 - a learned topographic map overlaid on top of the room's floor plan.  the dots represent locations in the room that were used as reference points to interactively train a multicamera vision system.  the arrows represent observed transitions between these reference locations.  once trained  the room can then locate people at and between these points. 
two.  the primary neurological substrate behind them is the superior colliculus  a small region of the brain that produces signals that orient peripheral sensory organs based on sensory stimuli.  the superior colliculus contains layered  topographic sensory and motor maps that are in register; that is  co-located positions in the real world - in the sensory case representing derived locations of perceptual inputs and in the motor case representing peripheral sensory organ motor coordinates that focus on those regions - are all essentially vertically overlapping.  the actual mechanisms that use these maps to effect intersensory influence are currently unknown - variants on spreading vertical activation are suspected - but there is little doubt the maps' organization is a fundamental component of that mechanism. 
모far less is known neurologically about purely semantic influences - i.e.  ones that have effects confined to perceptual channels.  the superior colliculus itself has been directly approachable from a research perspective because the brain has dedicated inner space  namely  the tissue of the topographic maps  to representing the outer space of the real-world; the representation is both isomorphic and perspicacious  and it has made the superior colliculus uniquely amenable to study.  the perceptual as opposed to spatial representations of the senses are far more elusive and are specialized to the individual modalities and the organs that perceive them.   
 we have used the notion of layered topographic maps to represent both motor and semantic information in the intelligent room.  even though the superior colliculus has not yet been identified as a substrate in non-behavioral cross-modal influences  its extensive intra-map connections to higher cortical areas - particularly the visual cortex - may indicate its role in other types of intersensory function that are confined to perceptual channels and have no behavioral component.    
   using a topographic organization  we created a new model for visually tracking people within a room  coen and wilson 1 .  the model takes advantage of the observation that much of the information needed in humancomputer interaction is qualitative in nature.  for example  it may be necessary to distinguish between a person sitting on a chair  a person standing in front of a bookcase  and a person standing in a doorway  but obtaining the actual realworld coordinates of these people is generally unimportant.  in our system  locations in a room that are likely to contain people are used as reference points  as in figure 1  to interactively train a multi-camera vision system  whose current implementation has three steerable and six fixed cameras.  the system learns to combine event predictions from the multiple video streams in order to locate people at these reference points in the future.  the system can also dynamically track people with the steerable cameras as they move between these locations.  because most rooms have natural attractors for human activity  such as doorways  furniture  and displays  the selection of training points is usually readily apparent from the layout of the room. 
   once this topographic map is created  the tracking system activates locations on the map to correspond to its observations of people in the room.  as we will see in a moment  other systems in the room can also weakly activate locations on the map  which causes the room to turn a steerable camera to view the corresponding real world location.  if a person is found there as a result of orienting the camera  the system then completely activates that location on the map.   
 the ability to interact with topographic representations of the room is not confined to the tracking system.  once the map is learned  the room builds corresponding maps to spatially categorize events in its other sensory systems  even if they have no explicit spatial component.  for example  speech recognition events are topographically organized on a map dedicated just for that purpose.  as utterances are heard for which the room has spatial information  either learned or explicitly provided by its programmers   it activates locations in the speech system's topographic map  which in turn activates locations in other modalities' topographic maps via vertical spreading activation  as shown in figure 1.  conversely  other systems can weakly activate locations on the speech system's topographic map  which causes the speech system to increase the expectation probabilities of utterances associated with that location.   
   thus  activations in the map are bi-directional: perceptual events in a given modality can directly activate its map locations.  maps locations can also be activated via spreading activation from corresponding positions in other system's topographic maps  which causes a corresponding change in that modality's perceptual state - here  these spreading activations cause the secondary system to either look or listen for something.  it is this bi-directional activation - through which the systems can react to intersensory stimuli - that has made possible the crossmodal influences that were presented in figure 1. 
conclusion 
this paper has described our approach to incorporating cross-modal influences into the perceptual processing of the 

intelligent room.  we simultaneously argued against conventional post-perceptual integration and have motivated this position with biological evidence that unimodal perceptions are themselves integrated products of multimodal sources.  our position has allowed us to explore representational and algorithmic issues in unimodal perception that can only be approached from an integrated  multimodal perspective.  it has also allowed us to investigate creating more sophisticated interactive systems by incorporating more subtle intersensory cues into the intelligent room.  future work is both exciting and promising.    
references 
1. atkeson cg  hale j  pollick f  riley m  kotosaka s  schaal s  shibata t  tevatia g  vijayakumar s  ude a  kawato m: using humanoid robots to study human behavior. ieee intelligent systems  special issue on humanoid robotics  1. 1. 
1. brooks  r.a.  c. breazeal  ferrell   r. irie  c. kemp  m. marjanovic  b. scassellati and m. williamson  alternate essences of intelligence. in proceedings of the fifteenth national conference on artificial intelligence.   aaai1 .  madison  wisconsin. 1. 
1. butterworth  g.  the origins of auditory-visual perception and visual proprioception in human development.  in intersensory perception and sensory integration  r.d. walk and l.h. pick  jr.  eds.  new york.  plenum.  1. 
1. cheng  g.  and kuniyoshi  y.  complex continuous meaningful humanoid interaction: a multi sensory-cue based approach proc. of ieee international conference on robotics and automation  icra 1   pp.1  san francisco  usa  april 1  1. 
1. coen  m. design principles for intelligent environments.  in proceedings of the fifteenth national conference on artificial intelligence.   aaai1 .  madison  wisconsin. 1. 
1. coen  m. the future of human-computer interaction or how i learned to stop worrying and love my intelligent room. ieee intelligent systems. march/april.  1. 
1. coen  m.  and wilson  k. learning spatial event models from multiple-camera perspectives in an intelligent room.  in proceedings of manse'1.  dublin  ireland.  1. 
1. coen  m.  phillips  b.  warshawsky  n.  weisman  l.  peters  s.  gajos  
k.  and finin  p.  meeting the computational needs of intelligent environments: the metaglue system.  in proceedings of manse'1.  dublin  ireland.  1. 
1. cohen  p. r.  johnston  m.  mcgee  d.  smith  i. oviatt  s.  pittman  j.  chen  l.  and clow  j. quickset: multimodal interaction for simulation set-up and control. 1  proceedings of the applied natural language conference  association for computational linguistics. 1. 
1. cytowic  r. e.  synesthesia: a union of senses.  new york.  springer-verlag.  1. 
1. darrell  t.  gordon  g.  harville  m.  and woodfill  j.  integrated person tracking using stereo  color  and pattern detection  proceedings of the conference on computer vision and pattern recognition  cvpr '1   pp. 1  santa barbara  june 1. 
1. ferrell  c. orientation behavior using registered topographic maps. in proceedings of the fourth international conference on simulation of adaptive behavior  sab-1 . society of adaptive behavior. 1. 
1. gross  r.  yang  j.  and waibel  a.  face recognition in a meeting room.  fourth ieee international conference on automatic face and gesture recognition  grenoble  france  march 1 
1. held  r.  shifts in binaural localization after prolonged exposures to atypical combinations of stimuli.  am. j. psychol.  1-1.  1. 
1. helmholtz  h. v. handbook of physiological optics. 1. as reprinted. in james p.c. southall.  ed.  1. 
1. james  h.  1.  principles of psychology.  vol. 1. dover. 1. 
1. kohler  i. the formation and transformation of the perceptual world.  psychological issues 1 :1.  1. 
1. mcgurk  h.  and macdonald  j. hearing lips and seeing voices. nature. 1-1. 1. 
1. meltzoff  a.n. and moore  m.k.  imitation of facial and manual gestures by human neonates.  science 1-1.  1. 
1. miller  george and noam chomsky  1 . finitary models of language users. in luce  r.; bush  r. and galanter  e.  eds.  handbook of mathematical psychology  vol 1. new york: wiley. 1. 
1. piaget  j. construction of reality in the child  london: routledge & kegan paul  1. 
1. sams  m.  aulanko  r.  hamalainen  m.  hari  r.  lounasmaa  o.  lu  s.  and simola  j.  seeing speech: visual information from lip movements modified activity in the human auditory cortex.  neurosci. lett. 1-1.  1. 
1. sandini g.  metta g. and konczak j.  human sensori-motor development and artificial systems . in: air&ihas '1  japan. 1. 
1. stein  b.  and meredith  m. a.  the merging of the senses.  cambridge  ma.  mit press.  1. 
1. stork  d.g.  and hennecke  m.  speechreading: an overview of image processing  feature extraction  sensory integration and pattern recognition techniques   proc. of the second int. conf. on auto. face and gesture recog. killington  vt pp. xvi--xxvi 1. 
1. sumby  w.h.  and pollack  i.  visual contribution to speech intelligibility in noise.  j. acoust. soc. am. 1-1.  1. 
1. sun.  http://www.javasoft.com/products/java-media/speech/. 1. 
1. thelen  e.  and smith  l.  a dynamic systems approach to the development of cognition and action.  cambridge   mit press.  1. 
1. ullman  shimon.  high-level vision: object recognition and visual cognition.   cambridge.  mit press.  1. 
1. waibel  a.  vo  m.t.  duchnowski  p.  and manke  s. multimodal interfaces.   artificial intelligence review.  1-1. p1. 1. 
1. wren  c.  azarbayejani  a.  darrell  t.  and pentland  p.   pfinder: real-time tracking of the human body    ieee transactions on pattern analysis and machine intelligence  july 1. 
1. wu  lizhong  oviatt  sharon l.  cohen  philip r.  multimodal integration -- a statistical view  ieee transactions on multimedia  vol. 1  no. 1  december 1  pp. 1. 
1. zue  v.  seneff  s.  glass  j.  polifroni  j.  pao  c.  hazen  t.  and hetherington  l.  jupiter: a telephone-based conversational interface for weather information   ieee transactions on speech and audio processing  vol. 1  no. 1  january 1. 
real-time auditory and visual multiple-object tracking for humanoids
kazuhiro nakadai   ken-ichi hidai   hiroshi mizoguchi   hiroshi g. okuno   and hiroaki kitano
kitano symbiotic systems project  erato  japan science and technology corp.
mansion 1 suite 1a  1-1 jingumae  shibuya-ku  tokyo 1  japan
department of information and computer science  saitama university  saitama 1  japan
department of intelligence science and technology  kyoto university  kyoto 1  japan
sony computer science laboratories  inc.  tokyo 1  japan nakadai  hidai  okuno  kitano  symbio.jst.go.jp  hm me.ics.saitama-u.ac.jpabstract
this paper presents a real-time auditory and visual tracking of multiple objects for humanoid under real-world environments. real-time processing is crucial for sensorimotor tasks in tracking  and multiple-object tracking is crucial for real-world applications. multiple sound source tracking needs perception of a mixture of sounds and cancellation of motor noises caused by body movements. however its real-time processing has not been reported yet. real-time tracking is attained by fusing information obtained by sound source localization  multiple face recognition  speaker tracking  focus of attention control  and motor control. auditory streams with sound source direction are extracted by active audition system with motor noise cancellation capability from 1khz sampling sounds. visual streams with face id and 1d-position are extracted by combining skincolor extraction  correlation-based matching  and multiple-scale image generation from a single camera. these auditory and visual streams are associated by comparing the spatial location  and associated streams are used to control focus of attention. auditory  visual  and association processing are performed asynchronously ondifferent pc's connected by tcp/ip network. the resulting system implemented on an upper-torso humanoid can track multiple objects with the delay of 1msec  which is forced by visual tracking and network latency.
1	introduction
humanoids and entertainment robots or at least mobile robots have attracted a lot of attention last year  e.g.  at ieee/rsj first humanoids-1 conference  and are expected to play a role of human partners in the 1st century. let us imagine the situation autonomous robots are used in social and home environment  such as a pet robot at living room  a service
robot for office  or a robot serving people at a party. the robot shall identify people in the room  pay attention to their voice and look at them to identify visually  and associate voice and visual images  so that highly robust event identification can be accomplished. these are minimum requirements for social interaction  brooks et al.  1 .
모some robots are equipped with improved robot-human interface. jijo-1  asoh et al.  1  can recognize a phrase command by speech-recognition system; amella  waldherr et al.  1  can recognize pose and motion gestures. kismet of mit ai lab  breazeal and scassellati  1  can recognize speeches by speech-recognition system and express various kinds of sensation. hadaly of waseda university  matsusaka et al.  1  can localize the speaker as well as recognize speeches by speech-recognition system.
모however  the technologies developed so far are still immature; in particular  auditory processing and integrated perception among vision  audition  and motor control. at robotic conferences such as iros  smc  and icra as well as airelated conferences  there were at most one or two papers related to auditory processing1  and most papers on robot perception is limited to vision-only and vision with ultrasonic  infra-red or laser range finders. this is unfortunate because integrated processing of auditory and visual processing combined with appropriate motor control is essential in social interaction of robot systems.
모for auditory and visual tracking  nakadai et al. presented the active audition for humanoids to improve sound source tracking by integrating audition  vision  and motor controls  nakadai et al.  1 . an active audition system is implemented in a upper-torso humanoid to demonstrate that the humanoid actively moves its head to improve localization by aligning microphones orthogonal to the sound source and by capturing the possible sound sources by vision. although such an active head movement inevitably creates motor noise  the system adaptively cancels motor noise using motor control signals. the experimental result demonstrates that the active audition by integration of audition  vision  and motor control enables sound source tracking in variety of conditions. one of crucial problems is a lack of real-time processing.
모matsuyama et al. presented an architecture for asynchronous coordination of sensorimotor control  matsuyama et al.  1  so that the camera moves smoothly to track the object in real-time. this architecture  called dynamic memory  is general  but they use it only for vision-based motor control. shafer et al. presented the software architecture of sensor fusion for an autonomous mobile robot  shafer et al.  1 . the architecture is based on a parallel blackboard system  and the sensors include vision  range finder  but not microphones. it exploits global consistency regarding position and orientation of the vehicle and sensors. murphy presented the sensor fusion system for mobile robots called the sensor fusion effects  sfx  architecture  which is based on the uncertainty management system by dempster-shafer theory  murphy  1 .
모other robots with microphones as ears for sound source localization or sound source separation have attained little in auditory tracking. kismet has a pair of omni-directional microphones outside the simplified pinnae  breazeal and scassellati  1 . since it is designed for one-to-one communication and its research focuses on social interaction based on visual attention  the auditory tracking has not been implemented so far. hadaly uses a microphone array to perform sound source localization  but the microphone array is mounted in the body and its absolute position is fixed during head movements  matsusaka et al.  1 . in the both cases  sound source separation is not exploited and a microphone for speech recognition is attached to the speaker.
모in the research of computational auditory scene analysis  casa  to understand a mixture of sounds  real-time processing is one of the main problems in applying casa to real-world applications  rosenthal and okuno  1 . realtime processing is important to take appropriate actions in daily environments where many people  robots and objects exist. auditory and visual tracking by nakadai et al. accepts sounds in daily environment  but does not run in real-time  nakadai et al.  1 . another auditory and visual tracking by nakagawa et al. does not run in real-time  nakagawa et al.  1 .
모two major issues that have not been done in the past are attacked in this paper  that is  association of multiple auditory and visual streams  and real-time processing of integrated auditory and visual scene analysis.
모the rest of the paper is organized as follows: section 1 explains the robot hardware which is used as a testbed and presents the issues in real-time tracking. section 1 describes the design of the system and the details of each module are described in section 1. section 1 demonstrates and evaluates the performance of the system. section 1 discusses the observations of the experiments and future work and concludes the paper.
1	issues in real-time tracking
1	robot hardware
as a testbed of real-time multiple-object tracking  we use a upper-torso humanoid called sig shown in fig. 1  nakadai et al.  1 . the cover of the mechanics is made of frp and discriminates internal and external world acoustically. sig has two microphones at the left and right ear positions to capture external sounds from outside of the body  and two microphones within the body to capture internal sounds mainly

figure 1: humanoid  sig
caused by motor movements. all the microphones are omnidirectional microphones of sony ecm-1s. sig's body has four dofs  degree of freedom   each of which is a dc motor controlled by a potentiometer. sig is equipped with a pair of ccd cameras of sony evi-g1  but the current vision module uses only one camera.
1	task and issues
the task in this paper is to track multiple objects in real-time with two kinds of sensors  a camera and two microphones  and one actuator to rotate the body. some important issues in this task are:
robustness in visual stream extraction  temporal sequences of face localization and face identification  against non-uniform environments due to lighting conditions or moving humans.
robustness in auditory stream extraction  temporal sequence of sound source localization and sound source separation  against dynamic environments because objects  people  move and the humanoid also moves.
association of visual and auditory streams by face identification and sound source localization to compensate missing or ambiguous data. common representation for both auditory and visual feature extractions is needed.
focus of attention control based on association to control actuators.
trade-off of processing speed vs quality of feature extractions for real-time processing.
method of synchronization between asynchronous auditory and visual processing that have different processing speeds. the frame rate of vision is 1hz  while the sampling rate of sound is 1khz. therefore  asynchronous processing is essential to exploit the full range of concurrency.
1	design of the system
from the viewpoint of functionality  the whole system can be decomposed into five layers - sig device layer  process layer  feature layer  event layer  and stream layer


figure 1: audition module extracts harmonic structures to localize and separate sound sources.figure 1: modules and layers of the system
 see fig. 1  . from the viewpoint of implementation  the whole system consists of six asynchronous modules - audition  vision  association  focus-of-attention  motor control  and viewer. this relation between two viewpoints is depicted in fig. 1. since vision module utilizes the full power of pentium-iii 1mhz cpu  the whole system is organized in the form of distributed processing with three linux nodes based on pentium-iii with redhat linux 1j. the first node with 1mhz is for audition  the second node with 1mhz for vision  and the third with 1mhz for the rest. they are connected by tcp/ip over fast ethernet 1base-tx.
모the estimated processing time of each module executed on a node is summarized below:
vision - 1msec for face localization and identification 
audition - 1msec for sound source localization 
motor control - 1msec
network latency - up to 1msec
therefore  we set the goal that the response time of the system should be 1msec of delay.
모audition and vision generate an event by feature extraction and organizea stream as a temporal sequenceof events. motor control also generates an event of motion. association fuses these events to make a higher level representation. this fusion associates auditory and visual streams to make an associated stream. focus-of-attention makes a planning of sig's movement based on the status of streams  that is  whetherthey are associated or not.
모motor control is activated by focus-of-attention module and generates pwm  pulse width modulation  signals to dc motors. it also sends a motor event consisting of motor direction  azimuth of the midsagittal plain  to association module. viewer shows the status of auditory  visual and associated streams in the radar and scrolling windows  see screen shots shown in fig. 1 . some modules are explained in details in the next section.
1	details of each module
1	active audition module
sound localization for a robot or an embedded system is usually solved by using interaural phase difference  ipd  and interaural intensity difference  iid . these values are calculated by using head-related transfer function  hrtf . however  hrtf depends on the shape of head and it also changes as environments change. for real-world applications  sound localization without hrtf is preferable. nakadai et al. proposed the method based on the auditory epipolar geometry  an extension of epipolar geometry in stereo vision to audition  nakadai et al.  1 . they also proposed active audition for sensorimotor task with canceling motor and mechanical noises. however  they failed in doing the jobs in real-time  because they stuck to pure-tone processing. in this paper  we extendtheir approach  1  by exploiting the harmonic structure to extract peaks precisely and  1  by solving the uncertainty in sound source localization by dempster-shafer theory.
모audition module equipped with active audition is depicted in fig. 1. the input signal  a mixture of sounds originating from different directions  is sampled with sampling frequency table 1: belief factor of iid 
+111-111of 1khz and 1-bit quantization  and its spectrogram is calculated by fast fourier transforms  fft . audition extracts pitches  fundamental frequency     separates and localizes sound sources.
peak extraction and sound source separation: first a peak is extracted by a band-pass filter  which passes a frequency between 1hz and 1khz if its power is a local maximum and more than the threshold. this threshold is automatically determined by the stable auditory conditions of the room. then  extracted peaks are clustered according to harmonicity. a frequency of is grouped as an overtone  integer multiple  of if the relation holds. the constant  1  is determined by trial and error. by applying inverse fft to a set of peaks in harmonicity  a harmonic sound is separated from a mixture of sounds.
sound source localization: once a harmonic structure is obtained  the direction of sound source is calculated by hypothetical reasoning for ipd  interaural phase difference  and iid  interaural intensity difference . the azimuth  horizontal direction  is quantized and represented by every discrete value in the range of . the front direction of sig is 1 .
모from the extracted harmonic structure of left and right channels  a pair of harmonic structures is obtained. then the ipd    is calculated. auditory epipolar geometry generates a hypothesis of ipd for each candidate   nakadai et al.  1 . since the ipd is ambiguous for frequencies of more than 1hz  the distance    in ipd between the data and a hypothesis is defined as follows:
1hz
			 1 
1hz
where is the number of overtones of which frequency is less than 1hz.
모the similar relation may hold for iid  but our experience with iid proves that it can discriminate at most the side  that is  left or right. suppose that is the iid for peak fre-
                               1hz quency . if the value of 1hz is nonnegative  the direction is decides as left  otherwise as right:
integration of ipd and iid by dempster-shafer theory
to determine the sound source direction  the belief factors of ipd and iid are calculated and then integrated by dempstershafer theory. the belief factor of ipd 	  is calculated by using probability density function defined by eq.  1 .
			 1 
where and are the average and variance of   respectively. is the number of .
	the belief factor of iid 	is defined by table 1.
모then  belief factors of ipd and iid  and   are integrated using dempster-shafer theoryas defined in eq.  1 .
 1 
for the maximum	is treated as the sound source
direction of the harmonics. finally  audition sends an auditory event consisting of pitch     and a list of 1-best directions     with reliability factor for each harmonics.
1	real-time multiple face tracking
multiple face detection and identification suffers more severely from frequent changes in the size  direction and brightness of face. to cope with this problem  hidai et al. combines skin-color extraction  correlation based matching  and multiple scale images generation  hidai et al.  1 .
모the requirements on multiple face tracking are the capability of discriminating face data of the same face id from others and on-line learning. the first requirement is a class concept. turk et al. proposed the eigenface matching technique as a kind of subspace method  turk and pentland  1 . a subspace for discrimination is created by principal component analysis  pca . pca  however  does not provide the means to group a data according to its face id  since such an id cannot be generated by pca. thus  the subspace obtained by pca is not always suitable to distinguish such classes.
모on the other hand  liner discriminant analysis  lda  can create an optimal subspace to distinguish classes. therefore  we use online lda  hiraoka et al.  1 . in addition  this method continuously updates a subspace on demand with a small amount of computation.
모the face identification module  see fig. 1  projects each extracted face into the discrimination space  and calculates its distance to each registered face. since this distance depends on the degree     the number of registered faces  of discrimination space  it is converted to a parameter-independent probability as follows.
		 1 

모the face localization module converts a face position in 1-d image plane into 1-d world coordinate. suppose that a face is pixels located in       in the image plane  whose width and height are and   respectively  see screen shots shown in fig. 1 . then the face position in the world coordinate is obtained in terms of distance   azimuth and elevation by the following equations.


where and are constants defined by the size of the image plane and the image angle of the camera.
모finally  vision module sends a visual event consisting of a list of 1-best face id  name  with its reliability and position  distance   azimuth	and elevation	  for each face.

	1	stream formation and association
association module forms auditory streams from auditory events and visual streams from visual events  and associates a pair of auditory and visual streams to create a higher level stream  which is called an associated stream  see fig. 1 . the flow of processing in stream formation and association is summarized as follows  figs. 1 a - d  :
1. events from audition  vision and motor modules are stored in the short-term memory.
1. direction information of events is converted into the absolute coordinate to treat them in the common coordinate.
1. events are grouped into an auditory or a visual streamaccording to a temporal sequence of events.
1. streams are synchronized by every 1msec to calculatethe distance between streams.
1. an auditory and a visual stream which are close formore than a constant time are associated as an associated stream.
모first  events are stored in the short-term memory and kept only for 1 seconds to attain incremental and real-time processing. in fig. 1 a   where s   v and m represent events created by audition  vision and motor modules  respectively. each module creates events at its own cycle  e.g. 1msec for audition  1msec for vision and 1msec for motion. then  motor events are synchronized with auditory and visual events. to put it concretely  a motor direction when an auditory or a visual event appeared is estimated from motor events in the short-term memory. a motor event with the estimated motor direction is shown as m . because visual events from vision module and auditory events from audition module are represented in robot coordinate  the directions of these events are converted to ones in the absolute coordinate by using estimated motor direction. these are represented as s or v in fig. 1 a . this synchronization process runs with a delay of 1msec as mentioned in sec. 1.
모auditory and visual streams are formed in fig. 1 b . xaxis indicates elapsed time from right to left  and y-axis indicates azimuth in the absolute coordinate. thin lines with small filled circles and a thick line with small rectangles represent auditory streams and a single visual stream. an auditory event is connected to the nearest auditory stream within the range of and with common . a visual event is connected to the nearest visual stream within 1cm and with a common face id. in either case  if there are multiple candidates  the most reliable one is selected. if any appropriate stream is found  such an event becomes a new stream. in case that no event is connected to an existing stream  such a stream remains alive for up to 1msec. the system cannot detect an auditory or a visual event when a person stops talking and looks away for a moment. this margin of 1msec is prepared to continue streams in case of the missing event extraction. after 1msec of keep-alive state  the stream terminates.
 d  association	when the distance between an auditory and a visual stream
is close for more than a constant time  they are regarded as
figure 1: association module forms streams

streams originating from the same object and integrated into an associated stream  which is a higher layer representation of a stream shown in fig. 1. because auditoryand visual streams consist of events with 1msec and 1msec cycles  respectively  it is difficult to evaluate the distance between these two streams without synchronization. then  they are synchronized with the same cycle  1msec. fig. 1 c  illustrates synchronized streams as lines with large circles and rectangles. if an event is not available in this case  linear regression is used for interpolation in the same way as synchronization with motor events.
모an auditory and a visual streams are associated if their direction difference is within the range of and this situation continues for more than 1% of the 1sec period shown in fig. 1 d .
모the visual direction is usually used for the direction of the associated stream because visual information is more accurate. however  when a tracking person is occluded  the system cannot use visual information. in this case  auditory information is used for the associated stream. this suggests an advantage of integration of audition and vision  i.e. auditory information is efficient not only for pre-attentive uses such as a trigger of attention but also for compensations of missing or ambiguous information as this case. if either auditory or visual event has not been found for more than 1sec  such an associated stream is deassociated and only existing auditory or visual stream remains. if the auditory and visual direction difference has been more than for 1sec  such an associated stream is deassociated to two separate streams.
1	focus of attention control
sig should pay attention for sounds from unseen objects to get further information. when such a sound does not exist  faces with sound  i.e. talking people  should have high priority because they are attractive even for human perception. the principle of focus-of-attention control hereby is as follows:
1. an auditory stream has the highest priority  1. an associated stream has the second priority  and
1. a visual stream has the third priority.
모the algorithm of focus-of-attention control is sketched by using an example shown in fig. 1  which depicts how auditory and visual streams are generated and associated.
1. focus of attention changes to a new association stream.  and of fig. 1 .
1. if one of the visual and auditory stream of an associatedstream terminates due to occlusion  disappearance  or end of speech  association continues   to of fig. 1 .
1. if this state continues for a particulartime  say 1seconds the focus of attention may change.
 a  focus of attention changes to one of associatedstreams.
 b  if no associated stream is found  focus of attentionchanges to one of auditory streams.  	of fig. 1 .
 c  otherwise  focus of attention changes to one of visual streams.
1. in turning the body to associate the auditory stream tovisual one  focus of attention keeps the same even if a new associated stream is generated.
1	experiments and evaluation
a 1-second scenario shown in fig. 1 is used as a benchmark. the performance of integrated auditory and visual tracking is shown in fig. 1  which shows that focus of attention changes twice. in the first half of the scenario up to sec  two speakers are apart  while in the second half they are close and viewed in the same camera view field. in both cases  the system can track the speakers well.
모the direction of sig's body is depicted in fig. 1  which shows that the motor control succeeds in giving correct pwm motor commands. to sum up  sensorimotor task in singleand multi-speaker tracking is well accomplished.
모the performance of visual tracking is shown in fig. 1. this timechart is generated by collecting the first candidate from the internal states of vision module. therefore  the motor movement is the same as the above. in the first half of the scenario  occlusion causes a gap of visual streams between and . from to   no person can be seen due to the limited range of camera view field. fig. 1 proves that occlusion and out-of-sight can be easily recovered by associated streams.
모the performance of auditory tracking is shown in fig. 1  which is generated in the same manner as fig. 1. the audition module can separate two auditory streams correctly from
to sec  and to   but generate erroneous streams around and . in addition  the directions of two speakers are not so correct from sec     to sec  because
mr. a moves and sig tracks him by rotating its body. that is  the reverberation  echo  due to this moving talker and motor noise deteriorates the quality of sound source localization.
1 limitations on the proposed system the room used in this experiment is about in width and length and in height  and sound absorbing materials are attached on walls  ceiling and floor. it is not anechoic  but has reverberation time of 1sec. because the value in a normal speech studio of the equivalent size is about 1sec  the room has less reverberation. acoustic conditions  however  depends on objects in the room. when we put a plastic partition of 1m 1m with strong reverberation in the room  the correctness of sound source localization is reduced remarkably.
모the background noise level of the room is 1dba on average. this is measured with the filter by a weighting  similar to human auditory characteristic. the value of 1dba corresponds to the noise level of a room in a quiet residence. we confirmed that the current system works well up to 1dba of background noise level  but we have not checked above 1dba.
is created. associated stream is created.	associated stream	on mr.b is created.	visual stream.	is created. on mr.a is created.	disappears.
figure 1: temporal sequence of auditory and visual tracking of two speakers: radar chart and stream chart are screen shots of the viewer. in radar chart  a wide-light and a narrow-dark sector indicate the camera view field and sound source direction  respectively. in stream chart  a thin line indicates auditory or visual stream  while a thick line indicates an associated stream.

figure 1: temporal sequence of body direction controlled by motor movement in the same scenario as fig. 1

figure 1: temporal sequence of visual tracking of two speakers in the same scenario as fig. 1

figure 1: temporal sequence of auditory tracking of two speakers in the same scenario as fig. 1모the room has six halogen lights with adjustment function of the intensity on the ceiling. the range of the light intensity is from 1 to 1 lux. because the light intensity from 1 to 1lux is recommended fora normal office by jis  japanese industrial standard   even the maximum light intensity of the room is weak. our face extraction and recognition method works well under the condition of more than 1 lux. visual processing is robust against the change of light intensity. in this experiment  the light intensity in the room is 1 lux.
모other benchmarks such as crossing of moving talkers  moving talkers without seeing any talkers  and alternative talking of four speakers prove that the resulting system succeeds in real-time sensorimotor tasks of tracking 1. 1
모모since the work is related to the real-time processing  the readers may be suggested to visit the following web site:
http://www.symbio.jst.go.jp/sig/
1	conclusion and future work
the key idea of real-time tracking is  for each processing  take it easy  and ambiguities will be resolved with the help of others.  this idea is obtained by the scrutiny of the behavior of each component of implementations of nakadai et al.'s work  nakadai et al.  1 . we do not stick to pure tones  but utilize the collective behavior of harmonic sounds; we prefer frequency resolution over the time resolution by increasing the points of fft. we give up the precise face localization and identification. instead  we associate auditory  visual  and motor direction information to localize the sound sources.
모some technical future work includes learning the adaptive association of different or dynamic environments. since lighting conditions and reverberation  echo  change drastically in such environments  vision and audition modules should adjust their parameters on demand. in addition  association should adapt its parameters for stream forming and association. baysian algorithm for resolving ambiguities in stream forming and association is a promising technique. another future work is incorporating stereo vision. even in a static environment robust auditory processing such as sound source separation and localization would be useful when a room is noisier  has objects with strong reverberation  or has many people.
모we believe that our result would open a new era of sound processing  in particular  cocktail party computer or  shotoku-taishi  computer that can listen to several things at once.
모auditory and visual tracking should be incorporated in a total system with robot-human interface. we have already built such a system comprising speech recognition  speaker identification  and speech synthesis based on the proposed system. once the application is fixed  the top-down stream separation may be exploited. some information that forces top-down stream separation includes speaker identification. the speaker information may reduce the search space of face recognition and speech recognition. for example  let us consider that crossing of two talking persons. in this case  the system may miss judging that they are approaching and then receding because speaker ids are lacking. thus  speaker identification can reduce this kind of ambiguity. its design and implementation will be reported by a separate paper  since this paper focuses on the real-time processing.
acknowledgments
we thank our colleagues of symbiotic intelligence group  kitano symbiotic systems project; mr. tatsuya matsui and dr. tino lourens  and prof. h. ishiguro of wakayama university for their discussions.
