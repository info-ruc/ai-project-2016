 
instance-based learning methods explicitly remember all the data that they receive they usually have no training phase and only at prediction time do they perform computation then they take a query search the database for similar datapoints and build an on-line local model  such as a local average or local regression  with which to predict an output value in this paper we review the advantages of instance based methods for autonomous systems but we also note the ensuing cost hopelessly slow computation as the database grows large we present and evaluate a new way of structuring a database and a new algorithm for accessing it that maintains the advantages ot instance-based learn ing earlier attempts to combat the cost of instancebased learning have sacrificed the explicit retention of all data or been applicable only to instancebased predictions based on a small number of near neighbors  or have had to re- ntrodtice an exp}tctt training phase in the form ot an interpolative data structure our approach builds a multiresolution data structure to summarize the database of experiences at all resolutions of interest simultaneously this permits us to query the database with the same flexibility as a conventional linear search but at 
greatly reduced computational cost 
1 introduction 
instance-based learning methods  stanhll et al 1 atkeson  1 ahaet al 1 moore 1  are highly flexible general purpose techniques tor making predictions from earlier data instance based methods  also known as memory-based methods or lazy-learning methods and closely related to case-based' methods  explicitly remember all the data they are shown only at prediction time do they perform non-tnvial amounts of computation this behavior differs from more conventional machine learning algorithms in which training occurs between the reception of data and prediction examples of instance based methods are nearest neighbor kernel regression and locally weighted linear regression example of non-instance-based techniques  they have a training phase  are neural networks and decision trees instance based methods can sometimes be a preferable form of function approximator there are three main rea sons for this 
flexible inductive bias 
with very little data a method such as nearest neighbor gives sensible conservative predictions it does not wildlv extrapolate but as the amount of data increases so does the complexity of the function that nearest neighbor can approximate this contrasts with for example multi-layer neural networks that do not by default have this property of representative power increasing locally according to the amount of local data in the limit very local methods can learn any piecewise continuous function to arbitrary preci sion  although with high dimensional uniformly distributed input the amount of data to do this can be enormous  for practical use in function approximation  much better instance-based methods than nearest neighbor are available that form local linear models and compute weighted averages of data to remove the noise from predictions  e g see atkeson 1 grosse i1   
 learning parameters need not be fixed in advanc e there are many learning parameters in instance-based algorithms one ot the most important concerns the extent 1 which the smoothing of noise is traded against goodness of fit others include  1  the parameters of a distance metric for determining the similarity between an input point and the query and  ii  the discrete decision of which attributes are relevant instance based methods do not need to decide on these learning parameters in advance they can use whichever parameters they desire tor one prediction and then have the option of using an entirely different sel for another prediction this is of immense use in an autono mous system that is both making new predictions online and tuning its learning parameters online as new data is arriving |moore el al 1  in contrast a non instancebased method must choose a parameter set and then train with it if a different parameter set is later needed then it is necessary for a non-instance-based method to retrain itself  and it is therefore also necessary for it to remember all previous data  
instance based can cover the global local spectrum instance based methods do not necessarily have to be local predictors based on a small handful of local datapoints this is particularly important lor large numbers of attributes  highly noisy data and for small data bases many of our own applications involve very noisy systems in which the underlying function is non linear but generally smooth in these cases the best instance-based function approximator might for example use the closest 1% of all datapoints to the query to form its prediction in 
	deng and moore 	1 

some extreme cases for example if the underlying function were nearly linear and local linear regression was in use  this figure might increase to 1% of the datapoints- essentially a global not local function approximator 
the properties described above make instance-based methods particularly desirable for autonomous systems that spend their lives in environments that are not known in advance and in which the designers will not be able to manually tweak the learning parameters during operation 
unfortunately instance-based methods have a serious problem as the database grows large it becomes increasingly expensive to make predictions each prediction involves searching the database to find similar earlier datapoints this can mean hopelessly slow performance after merely tens of thousands of predictions various researchers have attempted to deal with this problem  aha et at 1 grosse 1 moore 1 skalak 1  but as we will see in section 1  none in a manner that avoids sacrificing at least one of the benefits of instancebased methods described above this paper describes a new solution based on a multiresolulion hierarchical structuring of data  which retains all the above properties of instance-based learning while providing fast instancebased performance asymptotically it reduces the cost of a query from linear to logarithmic in the number of datapoints 
1 k e r n e l regression 
the approach taken in this paper can be applied to a wide variety of instance-based algorithms but here we will concentrate on one of the most well known examples kernel regression  see for example ifranke 1j  assume that datapoints consist of  input output  pairs such as 
 x1 y1   x1 y1   xnyn  where the inputs are d-element real-valued vectors and the outputs are scalars and to date we have observed n datapoints the prediction problem is given an input vector xq which is called the quer  how to predict the output ycsl xq 
the k.-nearest neighbor solution to this problem would be to find the k datapoints that have input vectors closest to the query and to take the average of the corresponding output values as yest xq  kernel regression uses a similar idea except that a weighted average of all the points in the database is used and the points closest to the query are assigned the largest weight thus 
y w i 
	* 	x     
where w  is the weight assigned to the *th datapoint in our memory and is large for points close to the query and almost zero for points far from tlie query it is calculated as a decreasing function of euclidean distance for example 
1 	learning 

the bigger the parameter k is the flatter the weight function curve is which means that many memory pointi contribute quite evenly to the regression as k tends to infinity the predictions approach the global average of all points in the database if the k is very small only closely neighboring datapoints make a significant contribution 
k  is an important smoothing parameter for kernel regression it the data is relatively noisy  we expect to obtain smaller prediction errors with a relatively large k if the data is noise free then a small k will avoid smearing away fine details in the function this is illustrated in figure 1 

figure l for the noiseless data in the top example a small k gives lhe besl regression  in terms of future predictive accuracy  for the noisy daia in lhe bollom example a larger k is preferable 
the drawback of kernel regression is the expense of enu merating all the distances and weights from the memory points to the query several methods have been proposed to address this problem reviewed in section 1 
1 	multiresolution structuring of datapoints 
the main idea of multiresolution instance-based regression is grouping the following figure shows a 1-d input space case given a query based on the distance from each point to the query we could calculate a weight for every point the input space 

figure 1 grouping data according to distance from query 


deng and moore 


	1 	learning 

tenon to satisfy both of these intuitions cutoff only if 

where nb is the number of datapoints below the current node simple algebra reveals that guarantees 

where g is the number of groups finally used in the search  and thus g   nt hopefully considerably less  notice that this cutoff rule requires us to know ew  in advance which of course we do not fortunately the sum of weights obtained so far in the search can be used as a valid lower bound and so the real algorithm makes a cutoff if 

where i is a system constant 
1 	experiments a n d results 
let us review the performance of the multiresolution method in comparison to kernel regression in the hrst experiment we use a trigonometric function of two inputs with added noise x  = uniformly generated random vector with all components between 1 and 1 and v  = a function of x   which ranges between 1 and 1 in height  with gaussian noise of standard deviation 1 
1 datapoints were generated experiments were run for different values of kernel width k in all experiments the cutoff threshold t was 1 figure 1a shows the testset error on 1 test points for both regular kernel regres sion   regular kr   and multiresolution kernel regression   multires kr   graphed for different values of k the values are very close indicating that multires kr is providing for a wide range of kernel widths a very close approximation to regular kr figure 1b shows the computational cost  in terms of the summations that dominate the cost of kr  of the two methods regular kr sums all points and so is a constant 1 in cost multires kr is substantially cheaper for all values of k but particularly so for very small and very large values 
figures 1a and 1b show corresponding figures for a similar trigonometric function of five inputs this still shows similar prediction performance as regular kr the cost is still always less than regular kr but in the worst case the computational saving is only a factor of three  when k = 1 multires kr cost = 1  this is not an especially impressive result however for any fixed dimensionality 
and kernel width costs rise sub linearly  in principle loganthmicallv  with the number of datapomts to check this we ran the same set of experiments for a datasel of ten times the size 1 points the results in figure 1 show that with this large increase in data the effectiveness 
deng and moore 

of multires kr becomes more apparent for example consider the k = 1 case with 1 datapoints instead of 1 the cost is only increased from 1 to 1 while the cost of regular kr  of course  increased from 
1 to 1 
rigun. 1  upper  the relative accuracy and  lower  the computational cost of multires kr againsl t the cutoff threshold 
real datasets 
in another experiment we ran multires kr on data from several real-world and robot-learning datasets further details of the datasets can be found in  maron and moore 1| thev include an industrial packaging process tor which the slowness of prediction had been a reasonable cause for concern encouragingly multires kr speeds up prediction by a factor of 1 with no discernible difference in prediction quality between multires and regular kr this and other results are tabulated below the costs and 
rms values given are averages taken over an independent test set nolabl  the datasets with the least savings were pool which had few datapoints and robot  which was high dimensional 

high dimensional non-uniform data 
our final experiment concerned the question of how well 
1 	learning 
the method performs it the number of input variables is relatively large but if the attributes are not independent for example a common scenario in robot learning is for the input vectors to be embedded on a lower-dimensionai manfold we performed two experiments  each with 1 inputs and 1 datapoints in the first experiment the components of the input vectors were distributed uniformly randomly in the second experiment the input vectors were distributed on a non-linear 1-d manifold of the 1 d input space the results were 
1 related w o r k and discussion 
there has not been room in this paper to discuss a number of additional flexibilities that multires kr provides once the kd-tree structure is built it is possible to make different queries with not only different kernel widths k but also different euclidean distance metrics with subsets of attributes ignored or with some other distance metrics such as manhattan it is also possible to apply the same 
technique with different weight functions and tor classification instead of regression 
it should be remembered that although we have succeeded in reducing the cost of instance-based learning there are other methods  outlined below  in the literature for doing so why might multires be preferable'' this depends upon the extent to which the application needs the following advantages of instance based learning described in section 1 
  flexibility to work throughout the local/global spectrum 
¡ö the ability to make predictions with different parameters without needing a retraining phase 
multires provides for both no other software method to our knowledge does there is however the simple hardware alternative of using a fast enough computer the standard kernel regression method can be parallelized with full efficiency 
kd-trees have frequently been used in instance-based learning methods for nearest neighbor searching or for range searching  preparata el al  1  the range-search solution to kernel regression is to find all points in the adtree that have a significant weight and then only sum 

together the weighted components ot those points this is only practical if the kernel width k is small if it is large all the datapoints may have a significant weight but only a small local variation in weight range searching would sum all the points individually multires would visit only relatively large intermediate nodes  because the weight variation is locally low  and so would still be cheap even in cases ot small kernel widths and large amounts of data the multiresolution method can be preferable lo the range search method because it may nol need to search all the way down to the leaf nodes 
another solution lo the cost ot instance-based learning is editing  or prototypes  most datapoints are forgotten and only particularly representative ones are used  e g  kibler and aha 1 skalak 1  kibler and aha extended this idea further by allowing datapoints to represent local averages ot sets of previous!  observed datapoints this can be effective and unlike range-searching can be appli cable even for wide kernel widths however the degree of local averaging must be decided in advance unlike multires queries cannot occur with different kernel widths without rebuilding the proiotvpes a second occasional problem is that if we require very local predictions  the prototypes must either lose local detail by averaging or else all the datapoints are stored as proiotvpes 
there are interesting parallels between prototypes and 
multires the intermediate nodes of the k.d tree can be thought ot as labncaled prototypes summarizing all the data below them 
decision trees and a.d-trees have been previously used lo cache local mappings in the tree leaves  grosse 1 moore 1 omohundro 1 quinlan 1 these algorithms provide fast access once the tree is built but a new structure needs to be built each time new learning parameters are required furthermore unlike the multiresolution method the resulting predictions from the tree have substantial discontinuities between boundaries only in  grosse 1  is continuity enforced bul at the cost of tree-sire tree-building-cosi and prediction-cost all being exponential in the number of input variables 
dimensionality is a weakness of multires diminishing returns set in above approximately 1 dimensions if the data is distributed uniformly this is an inherent problem for which no solution seems likely because uniform data in high dimensions will have almost all datapoints almost exactly the same distance apart and a useful notion of locality breaks down 
this paper discussed an efficient implementation of kernel regression we are applying exactly the same algorithm to locally weighted polynomial regression in which a prediction fits a local polynomial to minimize the locally weighted sum squared error the only difference is that each node of the id-tree stores the regression design matn ces of all points below it in the tree this permits fast prediction and also fasl compulation of confidence intervals and analysis of variance information 
multires as described here is only applicable to numeric features an interesting avenue for future work would be ways to extend the method to binary or symbolic features 
1 conclusion 
instance based methods make use ot a database of multidi mensional data multiresolution instance based methods provide a means for performing queries quickly even when the amount ot data is enormous an important message of this paper is that for efficient accessing ol large instance bases  or case bases or memory bases  it is not necessary to resort to throwing data away intelligent structuring is an alternative 
acknowledgement 
we wish to thank the ijcai reviewers for insightful and useful comments this work was suppon bv a research gift from 1m corporation and an nsf research initiation award 
