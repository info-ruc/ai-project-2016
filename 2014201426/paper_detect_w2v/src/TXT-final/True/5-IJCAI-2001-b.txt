
we consider the problem of how an agent creates a discrete spatial representation from its continuous interactions with the environment. such representation will be the minimal one that explains the experiences of the agent in the environment. in this paper we take the spatial semantic hierarchy as the agent's target spatial representation  and use a circumscriptive theory to specify the minimal models associated with this representation. we provide a logic program to calculate the models of the proposed theory. we also illustrate how the different levels of the representation assume different spatial properties about both the environment and the actions performed by the agent. these spatial properties play the role of  filters  the agent applies in order to distinguish the different environmentstates it has visited.
1	introduction
the problem of map building -how an agent creates a discrete spatial representation from its continuous interactions with the environment- can be stated formally as an abduction task where the actions and observations of the agent are explained by connectivity relations among places in the environment  shanahan  1  shanahan  1  remolina and kuipers  1 . in this paper we consider the spatial semantic hierarchy  ssh   kuipers  1 kuipers and byun  1  kuipers and byun  1  as the agent's target spatial representation. the ssh is a set of distinct representations for large scale space  each with its own ontology and each abstracted from the levels below it. the ssh describes the different states of knowledge that an agent uses in order to organize its sensorimotor experiences and create a spatial representation  i.e. a map . using the ssh representation  navigation among places is not dependent on the accuracy  or even the existence  of metrical knowledge of the environment.

　　this work has taken place in the intelligent robotics lab at the artificial intelligence laboratory  the university of texas at austin. research of the intelligent robotics lab is supported in part by nsf grants iri-1 and cda 1  and by funding from tivoli corporation.
　in order to define the preferred models associated with the experiences of the agent  we use a circumscriptive theory to specify the ssh's  minimal  models. different models can exist that explain the same set of experiences. this occurs because the agent could associate the same sensory description to different environment states  or because the agent has not completely explored the environment. the different ssh levels assume different spatial properties about the environment and the actions performed by the agent. these spatial properties play the role of  filters  the agent applies in order to distinguish the different environment states it has visited. for instance  at the ssh causal level two environment states are considered the same if any sequence of actions started at these states renders the same sequence of observations. at the ssh topological level  two environment states are considered the same if they are at the same place along the same paths. finally  at the ssh metrical level  two environment states are the same  if it is possible to assign to them the same coordinates in any frame of reference available to the agent. in sections 1 and 1 we make precise the claims above.
　finally  we use the ssh circumscriptivetheory as the specification for a logic program used to implement the abduction task. in the paper we provide the logic program for the ssh causal level theory and illustrate how to encode the minimality condition associated with this theory. we have implemented the program using smodels  niemela： and simons  1  and confirm that the theory yields the intended models.
1	related work
the ssh grew out of the tour model proposed in  kuipers  1  kuipers  1 . other computational theories of the cognitive map have been proposed:  kortenkamp et al.  1  mcdermott and davis  1  leiser and zilbershatz  1  yeap  1 . these theories share the same basic principles: the use of multiple frames of reference  qualitative representation of metrical information  and connectivity relations among landmarks. they differ in how they define what a landmark is  or the description  view  local 1d geometry  associated with a landmark. except for mcdermott and davis  none of the theories above has a formal account like the one presented in this paper for the ssh.
　considering map building as a formal abduction task has been proposed by shanahan  shanahan  1  shanahan  1 . he proposes a logic-based framework  based on the

circumscriptive event calculus  in which a robot constructs a model of the world through an abductive process whereby sensor data is explained by hypothesizing the existence  locations  and shapes of objects. in shanahan's work  space is considered a real-valued coordinate system. as pointed out in  shanahan  1   a problem of shanahan's approach is the existence of many minimal models  maps  that explain the agent's experiences. we have alleviated this problem by considering the ssh topological map instead of an euclidean space as the agent's target spatial representation.
　the problem of distinguishing environment states by outputs  views  and inputs  actions  has been studied in the framework of automata theory  basye et al.  1 . in this framework  the problem we address here is the one of finding the smallest automaton  w.r.t. the numberof states  consistent with a given set of input/output pairs. without any particular assumptions about the environment or the agent's perceptual abilities  the problem of finding this smallest automaton is np-complete  basye et al.  1 .
　the ssh  kuipers  1 kuipers and byun  1 kuipers and byun  1  abstracts the structure of an agent's spatial knowledge in a way that is relatively independent of its sensorimotor apparatus and the environment within which it moves. at the ssh control level  the agent and its environment are modeled as continuous dynamical systems whose equilibrium points are abstracted to a discrete set of distinctive states. a distinctive state has associated a view describing the sensory input obtained at that distinctive state. the control laws  whose executions define trajectories linking these distinctive states  are abstracted to actions  giving a discrete causal graph representation for the state space. the causal graph of states and actions can in turn be abstracted to a topological network of places  paths and regions  i.e. the topological map . local metrical models  such as occupancy grids  of neighborhoods of places and paths can then be built on the framework of the topological network while avoiding global metrical consistency problems. in the next sections we formally describe the ssh causal and topological levels.
1	ssh causal level
we use a first order sorted language in order to describe the ssh causal level. the sorts of this language include distinctive states  views  actions and schemas. the sort of distinctive states corresponds to the names given by the agent to the fixpoints of hill-climbing control strategies. it is possible for the agent to associate different distinctive state names with the same environment state. this is the case since the agent might not know at which of several environment states it is currently located. a distinctive state has an associated view.
we use the predicate to represent the fact that v is a view associated with distinctive state . we assume that a distinctive state has a unique view. however  we do not assume that views uniquely determine distinctive states  i.e.
 . this is the case
since the sensory capabilities of an agent may not be sufficient to distinguish distinctive states.
　an action has a unique type  either travel or turn  associated with it. we use the predicate 
to represent the fact that the type of action a is type. turn actions have associated a unique turn description  either turnleft  turnright or turnaround. we use the predicate  to indicate that desc is the turn descrip-
tion associated with the turn action a.
　a schema represents an action execution performed by the agent in the environment. an action execution is characterized in terms of the distinctive states the agent was at before and after the action was performed.1 we use the predicate to denote the fact that according to schema   action was performed starting at distinctive state and ending at distinctive state . while schemas are explicit objects of our theory  most of the time it is convenient to leave them implicit. we introduce the following convenient notation:


example 1
consider a robot moving in the environment depicted in figure 1. the robot moves from distinctive state a to distinctive state b by performing a follow-midline action  ml. then the robot performs the same action to move to distinctive state c. we assume that all corridor intersections look alike    . this set of experiences can be described by the formulae:


	 a 	 b 
figure 1:  a  distinctive states a  b and c are not distinguishable at the causal level. topological information is needed in order to distinguish them.  b  all distinctive states are distinguished at the causal level given the new information .

　given this set of experiences  at the ssh causal level distinctive states a  b and c are not distinguishable. any known sequence of actions renders the same set of views. however  at the ssh topological level all these distinctive states are distinguishable since the robot has traveled from a to b and then to c following the same path  see example 1 . should the robot continue the exploration and visit distinctive state   with view   then by relying just on known actions and views the agent can distinguish all distinctive states it has visited  see example 1 . end of example
　the agent's experiences in the environment are described in terms of cs  view  action type and turndesc atomic formulae. hereafter we use e to denote a particular agent's experience formulae. by we denote the formulae stating that the sorts of schemas  distinctive states  views and actions are completely defined by the sets of schema  distinctive states  view and action constant symbols occurring in e respectively.1 by we denote our domain theory  the formulae stating that:  -  the sets turn  travel   turnleft turnright turnaround  completely define the sorts of action types and turn descriptions;  -  an action has associated a unique action type ;  -  distinctive states have associated a unique view;  -  the description associated with an action is unique;  -  turn actions have associated a turn description;  -  the type of actions as well as the qualitative description of turn actions is the one specified in e. the ssh causal theory ct e  defines when two distinctive states are indistinguishable at the ssh causal level. we use the predicate to denote this fact. we will assume that actions are deterministic: 1
 1 
ct e  is the following nested abnormality theory  lifschitz  1 :

where ceq block is defined as
 1 
 1 
　it can be proved that the predicate ceq defines an equivalence relation on the sort of distinctive states. axiom 1 states that indistinguishable distinctive states have the same view. axiom 1 states that if distinctive states and are indistinguishable and action a has been performed for both and
     then the action links these states with indistinguishable states. by maximizing we identify distinctive states that cannot be distinguished by actions and/or views  and thereby minimize the set of states represented by the model.
　axioms 1 and 1 allow us to prove the following useful lemma:
lemma 1 let denote a sequence of action symbols. let denote the distinctive state symbol resulting of starting the sequence at distinctive state or if is not defined for .1 then 
example 1

1
that is completely defined by the constant symbols means that an interpretation for is the herbrand in-
terpretation defined by the set	. 1
　　throughout this paper we assume that formulas are universally quantified. 1
	given an action symbol	and distinctive state	 
if the schema	has been observed  otherwise 	.
moreover  . the definition is then extended to action sequences in the standard way. notice that being well-defined relies on our assumption that actions are deterministic  axiom 1 .
consider the situation depicted in figure 1b  with the corresponding schemas and views as in example 1. using lemma 1 one can conclude that all distinctive states a  b and c are distinguishable by actions and views alone.
for instance 	 	 
	 	  and con-
sequently 	.	end of example
　the herbrand models of are in a one to one correspondencewith the answer sets  gelfond and lifschitz  1  of the logic program in figure 1 in this program  the and variables range over distinctive states and the variable
ranges over views in . the sets of rules 1 and 1 are the facts corresponding to the agent's experiences. rules 1 require to be an equivalence class. rules 1 and 1 are the counterpart of axiom 1. rule 1 is the counterpart of axiom 1. in order to define the maximality condition of   the auxiliar predicate is introduced. this predicate reads as  if and were the same  then and would be the same . the predicate defines when dis-
tinctive states and are distinguishable. constraint 1 establishes the maximality condition on : should be the case unless and are distinguishable.1

 1 
 1 
 1 
 1 
 1 
 1 
 1 
 1 
 1 
figure 1: logic program associated with ct e .

1	ssh topological level
we are to define the ssh topological theory    associated with a set of experiences . the language of this theory is a sorted language with sorts for places  paths and path directions.1 the main purpose of is to minimize the set of paths and places consistent with the given experiences . a place can be a topological place  hereafter place  or a region. a place is a set of distinctive states linked by turn actions. a region is a set of places. we use the predicates tplace and isregion to identify these subsorts. a path defines an order relation among places connected by travel with no turn actions. they play the role of streets in a city layout. we use the predicate tpath to identify the sort of paths. by minimiz-
ing the extent of	 		and	we minimize
the sort of places and paths respectively.1 the language of the ssh topological level includes the following other predicates: teq ds ds'  - distinctive states and are topologically indistinguishable; at ds p  - distinctive state is at place ; along ds pa dir  -distinctive state is along path in direction ; onpath pa p  -place is on path ; po pa dir p q  -place is before place when facing direction on path  po stands for path order .
tt e   is the following nested abnormality theory:
		 1 


　the first line in axioms 1 says that topological places and regions are the two subsorts of places  and that the predicate represents the sort of paths. the block ct e  is the one defined in the previous section. the block tblock de-
fines the predicates	 	  and	such that
is the equivalence closure of the schemas	;
and	are the equivalence and transitive closure of the schemas	.
　the block atblock  figure 1  is the heart of our theory.1the purpose of this block is to define the extent of the predicates tpath  tplace  at  along  po and teq  while identifying a minimum set of places and paths that explain . the block has associated the circumscription policy1
circ	var
where stands for the tuple of predicates at  teq  travel eq  and turn eq.1 this circumscription policy states  among others  that a minimum set of paths is preferred over a minimum set of places. next we discuss the axioms in at block.

 1 
 1 
 1 
 1 
		 1 
		 1 


 1 
 1 
 1 
 1 
			 1 
 1 
 1 
 1 
 1 
 1 
 1 

		 1 


circ	var
figure 1: at block.
	predicate	is the equivalence relation defined by axiom
1. is the case whenever and cannot be distinguished by views and actions  i.e.   and it is consistent to group and into the same place. if we assume that views uniquely identify distinctive states  e.g.
   then predicates
	and	will reduce to equality. this is expected since all
that is required to identify a distinctive state is its view.
　every distinctive state is at a unique place  axiom 1 . whenever the agent   it stays at the same place  axiom
1 . distinctive states grouped into a topologicalplace should be connected  modulo    axiom 1 . travel actions among distinctive states are abstracted to topological paths connecting the places associated with those distinctive states  axiom 1 . a distinctive state is along at most one path  axiom 1 . at each place there is at most one distinctive state along a given path direction  axiom 1 . turn actions other
pa1 dir1
	ds1	
	a	pa pos	b	pa pos c
	 a 	 b 
figure 1: the environment in  a  illustrates a case where different paths intersect at more than one place.  b  depicts the topological map associated with this environment.
than turnaround change the path the initial and final distinctive states are at  axiom 1 . turnaround actions relate distinctive states being in the same path but opposite directions  axiom 1 . the order of places in a given path direction is the inverse of the order of places in the other path direction
 axiom 1 . axioms 1 and 1 require to be a non-reflexivetransitive order for the places on . places ordered by a path should belong to that path  axiom 1 . axiom 1 requires the agent to have traveled among the places on a same path.
　our theory does not assume a  rectilinear  environment where paths intersect at most in one place. it is possible for different paths to have the same order of places  see figure 1 . topological information can distinguish distinctive states not distinguishable by view and actions.
example 1
　consider the scenario of example 1. since the same view is experienced at a  b and c  the extent of is maximized by declaring . using the topological theory  from axiom 1 we conclude that there exist places and
　  such that and . since it is the case that   from axioms 1 and 1 we conclude  for instance  that . distinctive states and are topologically distinguishable though they are  causally indistinguishable   i.e.  . end of example
　given a minimal model of   the ssh topological map is defined by the extent in of tpath  tplace  along  po and at. since the positive and negative direction of a path are chosen arbitrarily  axiom 1   there is not a unique minimal model for . we will consider these  up to path direction isomorphic  models to be the same. however  it is still the case that the theory has minimal models that are not isomorphic up to path direction  see figure 1 .
1	ssh boundary regions
in addition to connectivity and order among places and paths  the topological map includes topological boundary relations: assertions that a place lies to the right of  to the left of  or on a path. in order to determine boundary relations we formally state the following default heuristic. suppose the agent is at an intersection on a given path  and it then turns right. if the agent now travels  any place it finds while traveling with no turns will be on the right of the starting path. when conflicting information exists about whether a place is to the right or

figure 1:  a  the robot goes around the block visiting places
　      in the order suggested in the figure. intersections and look alike to the agent. two minimal models can be associated with the set of experiences in  a   see  b  and  c  . topological information is not enough to decide whether the agent is back to or . notice that if the agent accumulates more information  by turning at and traveling to   then it can deduce that the topology of the environment is the one in  b . in addition  when available  metrical information can be used to refute the incorrect topology.

left of a path  we deduce no boundary relation  see figure 1 .

figure 1: different environments illustrating how our default to determine boundary relations works. in  a   we conclude by default that place c is to the left of the path from a to b. in  b  we conclude nothing about the location of place d with respect to the path from a to b. in  c   we conclude that place d is to the left of the path from a to b. this is the case since there is no information to conclude otherwise.
	we	use	the	predicates
to represent the facts that  i  p1 is a place on both paths  pa and pa1  and  ii  when the agent is at place p1 facing in the direction dir of pa  after executing a turn right  left  action  the agent will be facing on the direction dir1 of pa1  see figure 1 . the predicates totheleftof and totherightof are derived from the actions performed by the agent at a place:
 1 
we use the predicates and to denote that region     is
the left  right  region of path pa with respect to the path's direction dir. the left/right regions of a path are unique  disjoint  and related when changing the path direction  i.e
	 .	from the
relative orientation between paths at a place  we deduce the relative location of places with respect to a path  see figure 1 : 1
		 1 
pa  dir

figure 1: path pa1 is to the right of path pa at place p1. place p is after place p1 on path pa1. by default  we conclude that place p is to the right of path pa.

　the predicate ab is the standard  abnormality  predicate used to represent defaults in circumscriptive theories  lifschitz  1 . axiom 1 states that  normally   if at place p1 path pa1 is to the right of path pa  and place p is after p1 on path pa1  then it should be the case that p is on the
right of pa  figure 1 . in order to capture this default  boundary regions domain theory axioms1 are added to the block atblock  see figure 1 . since we are interested in the extent of the new predicates in region  leftof  rightof  totheleftof and totherightof  we allow them to vary in the circumscription policy. the new circumscription policy becomes
circ	ab isregion in region var
where	stands for the tuple of predicates
at  along  teq   travel eq  turn eq  leftof  rightof  totheleftof  and totherightof. the circumscription policy states that boundary relations should be established even at the expense of having more places on the map. in addition  by minimizing the predicates isregion and inregion  we require the models of our theory to have only the regions that are explicitly created by the agent  and not arbitrary ones.
example 1
　boundary relations determine distinctions among environment states that could not be derived from the connectivity of places alone. consider an agent visiting the different corners of a square room in the order suggested by figure 1a. in addition  suppose the agent defines views by characterizing the direction of walls and open space. accordingly  the agent experiences four different views  v1-v1  in this environment. the set of experiences e in the environment are:
　suppose that the agent does not use boundaryregions when building the topological map. then the minimal topological model associated with e has two paths1 and two places. in this model  is the case. the environment looks perfectly symmetric to the agent  figure 1b .!!
suppose now that the agent relies on boundary regions.
let p  q  r  be the topological places associated with

1
 the predicate inregion p r  states that place p is in region r. 1
in the spirit of axioms 1.
1
　　notice that from and axiom 1 we can deduce that in figure 1b.

p
sds1
papb　q ds1 ds1
ds1r	 a 	 b 	 c 
figure 1:  a  the figure shows the sequence of actions followed by an agent while navigating a square room. starting at distinctive state ds1  distinctive states are visited in the order suggested by their number. dashed lines indicate turn actions. solid lines indicate travel actions.  b  and  c  depict the topological map associated with the environment in  a  without and using boundary regions  respectively.

ds1  ds1 and ds1 respectively.	from axiom 1  let pa 
pb 	and	be such that	 
	 	 
　　　　　　　　 	 	and hold.	from axiom 1 we can conclude then	.	in the proposed model  the extent of ab is minimized by declaring	and consequently from axiom
1	we	conclude		where denotes the right region of	when facing
. finally  since a path and its regions are disjoint  and is the case  we conclude and so
             . the resulting topological map is depicted in figure 1c. end of example
　if the agent's sensory capabilities are so impoverished that many distinctive states are perceived to be similar  then metrical informationcould be used to distinguish differentenvironment states. figure 1 summarizes different representations an agent could build depending on the spatial properties it relies on.
1	conclusions
starting with an informal description of the ssh we have formally specified its intended models. these models correspond to the models of the circumscriptive theory tt e . the formal account of the theory allows us to illustrate the deductive power of the different ssh ontologies. for instance  example 1 shows how the use of boundary relations allows the agent to determine distinctions among environment states that could not be derived from the connectivity of places and paths alone.
　the theory tt e  is rather complex so it may be difficult to determine the effect of the different defaults in combination. however  it is possible to translate this theory into a logic program whose answer sets determine the models of tt e . we have illustrated the case for the ssh causal theory ct e   but the same techniques apply for tt e . the major subtleties in the translation are the minimality and maximality conditions associated with the theory. we have used smodels to calculate the models of tt e  and confirm that the theory yields
	travel	travel

	turn	travel
	 a 	 b 

	 c 	 d 
figure 1: consider the same environment and agent as in figure 1. assumes the agent keeps turning right and following the left wall until it is back to distinctive state   at place . only two kind of views and are observed by the agent. next we summarizes different maps the agent could build depending on the spatial properties it relies on.  a  if the agent only relies on causal information  the map consists of two states.  b  when topological information is used  but without boundary relations  the map consists of four states and two places.  c  when boundary relations are used  the map consists of six states and three places. there is no fixed correspondence between the three places in the map and the four indistinguishable places in the real world.  d  if metrical information is accurate enough to refute the hypothesis   the map will consist of eight states and four places.
the intended models. however  when the number of distinctive states is big  smodels may not be able to ground the theory as the number of rules associated with the program grows exponentially. we are still working on solving this problem.
acknowledgments
we are grateful to vladimir lifschitz for his valuable feedback during this work and for suggesting the use of smodels to implement the ideas proposed here. we also thank the anonymous referees for their valuable comments on this paper.
references
 basye et al.  1  k. basye  t. dean  and l. p. kaelbling. learning dynamics: system identification for perceptually challenged agents. artificial intelligence  1 :1  1.
 gelfond and lifschitz  1  m gelfond and v. lifschitz. classical negation in logic programs and disjunctive databases. new generation computing  1-1  1.
 kortenkamp et al.  1  d. kortenkamp  e. chown  and s. kaplan. prototypes  locations  and associative networks  plan : towards a unified theory of cognitive mapping. cognitive science  1-1  1.
 kuipers and byun  1  b. kuipers and y. t. byun. a robust qualitative method for spatial learning in unknownenvironments. in morgan kaufmann  editor  aaai-1  1.
 kuipers and byun  1  b. j. kuipers and y.-t. byun. a robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. journal of robotics and autonomous systems  1-1  1.
 kuipers  1  b. kuipers. representing knowledge of large-scalespace. phd thesis  artificial intelligencelaboratory  mit  1.
 kuipers  1  b. j. kuipers. modeling spatial knowledge. cognitive science  1-1  1.
 kuipers  1  b. kuipers. the spatial semantic hierarchy. artificial intelligence  1-1  1.
 leiser and zilbershatz  1  d. leiser and a. zilbershatz. the traveller: a computational model of spatial network learning. environment and behavior  1 :1- 1  1.
 lifschitz  1  v. lifschitz. circumscription. in handbookof logic in artificial intelligenceand logic programming  volume 1  pages 1. oxford university press  1.
 lifschitz  1  v. lifschitz. nested abnormality theories. artificial intelligence   1 :1  1.
 mcdermott and davis  1  d. v. mcdermott and e. davis. planning routes through uncertain territory. artificial intelligence  1-1  1.
 niemela： and simons  1  i. niemela： and p. simons. smodels - an implementation of the stable model and wellfounded semantics for normal logic programs. in 1th international conference on logic programming and nonmonotonic reasoning  number 1 in lncs  pages 1- 1. springer-verlag  1.
 remolina and kuipers  1  e. remolina and b. kuipers. towards a formalization of the spatial semantic hierarchy. in fourth symposium on logical formalizations of commonsense reasoning  london  january 1.
 remolina and kuipers  1  e. remolina and b. kuipers. a logical account of causal and topological maps. technical report ai1  the university of texas at austin  http://www.cs.utexas.edu/users/qr  april 1.
 shanahan  1  m. shanahan. noise and the common sense informatic situation for a mobile robot. in aaai-1  pages 1  1.
 shanahan  1  m. shanahan. noise  non-determinism and spatial uncertainty. in aaai-1  pages 1  1.
 yeap  1  w. k. yeap. towards a computationaltheory of cognitive maps. artificial intelligence  1-1  1.
on-line execution of cc-golog plans
henrik grosskreutz and gerhard lakemeyer
department of computer science v
aachen university of technology
　　　aachen  germany grosskreutz gerhard  cs.rwth-aachen.deabstract
previously  the plan language cc-golog was introduced for the purpose of specifying event-driven behavior typically found in robot controllers. so far  however  cc-golog is usable only for projecting the outcome of a plan and it is unclear how to actually execute plans on-line on a robot. in this paper  we provide such an execution model for ccgolog and  in addition  show how to interleave execution with a new kind of time-bounded projection. along the way we also demonstrate how a typical robot control architecture where a high-level controller communicates with low-level processes via messages can be directly modelled in cc-golog.
1	introduction

figure 1: actual trajectory and approximation
　consider a robot which  while engaged in a delivery task  is to move from room 1 to room 1  see figure 1 . in a modern robot architecture like xavier  simmons et al.  1  or rhino  burgard et al.  1   the controller would issue a message to a low-level navigation process telling it the new destination  rm1   and the navigation process would then move the robot to its desired location  making use of intermediate goal points like those outside doorways  nodes 1 and 1 in the figure  for robustness.
　besides executing such plans  an intelligent robot controller should be able to reason about its actions  that is  consider  for example  whether it is better to first charge the batteries or to continue the delivery  which would involve projecting the outcome of the delivery task and checking whether the batteries would still have enough power afterwards. moreover  projection is also valuable for the designer of robot controllers to find out whether a program under development is executable or whether it would satisfy certain goals after execution. in  grosskreutz and lakemeyer  1a   we proposed such a projection mechanism for ccgolog an extension of golog  levesque et al.  1  featuring a model of time  continuous change  and the ability to wait for conditions  like arriving in rm1  to become true. while tasks like the above can be modelled quite naturally in cc-golog  it so far remains unclear how to actually execute a cc-golog plan or program  in this paper we use plan and program interchangeably . to get a sense of the problem  consider the actions of initiating moving just outside of room 1 and waiting to arrive there. in cc-golog these actions would have the effect of setting the location of the robot to a continuous linear function of time to approximate the robot's trajectory and to advance time to the point when the goal location is reached  respectively. while this seems fine for the purpose of projection  which is sometimes also called off-line execution  de giacomo and levesque  1   those  effects  are simply inappropriate during actual on-line execution. for one  the robot has no control over the passage of time and  for another  the actual trajectory often follows a function quite different from the idealized approximation  see the curved vs. the straight  dotted  line in figure 1 . what is needed instead  it seems  are frequent sensor readings telling the robot about the current time and location  which should be used instead of the models of how time passes or how the robot moves. a waiting action would then simply reduce to a test whether the goal has been reached. another minor complication is that actions that are part of the model of a low-level process such as navigation need to be ignored during execution except for an action that activates the process. finally  while projection in cc-golog so far is limited to a complete program starting in the initial state  one would often like to project on the fly during execution  similar to the search operator of  de giacomo and levesque  1 . however  for reasons of efficiency  we would like to go beyond that and allow for a restricted projection of a program  which only searches up to a  temporally  limited horizon. for instance  if the robot is in the middle of a delivery but near the docking station  we want to enable it to find out whether the coming activities would allow it to operate for at least another 1 minutes and  if not  decide to charge the batteries first.
　in this paper  we show how all this can be done in ccgolog. for that we first show how a robot control architecture where a high-level controller communicates with lowlevel processes via messages can be modelled directly in ccgolog. the main advantage is that there is a clear separation of the actions of the high-level controller from those of the low-level processes. then we discuss the changes necessary to use the same cc-golog program both for on-line execution and projection. finally  we show how a time-bounded projection mechanism can be defined and interleaved with execution.
　the rest of the paper is organized as follows. in section 1  we briefly review cc-golog and the situation calculus on which cc-golog is based. then we describe how to model the robot control architecture and give an example of a model of a low-level navigation process. in sections 1 and 1 we discuss the changes needed to allow on-line execution of cc-golog programs and how on-line execution and projection can be interleaved. we end with a brief summary and discussion of related work.
1	preliminaries
the situation calculus the semantics of cc-golog is based on an extension of the situation calculus  mccarthy  1 . we will only go briefly over the basic situation calculus: all terms in the language are one of the following sorts: ordinary objects  actions  or situations; there is a special constant used to denote the initial situation  namely that situation in which no actions have yet occurred; there is a distinguished binary function symbol do where do denotes the successor situation to resulting from performing the action ; relations whose truth values vary from situation to situation are called relational fluents  and are denoted by predicate symbols taking a situation term as their last argument; similarly  functions varying across situations are called functional fluents and are denoted analogously; finally  there is a special predicate poss used to state that action is executable in situation
　within this language  we can formulate theories which describe how the world changes as the result of the available actions. one possibility is a basic action theory of the following form  levesque et al.  1 :
axioms describing the initial situation  . action precondition axioms  one for each primitive action   characterizing poss .
successor state axioms  ssas    one for each fluent   stating under what conditions do holds as a function of what holds in situation these take the place of the so-called effect axioms  but also provide a solution to the frame problem.
domain closure and unique name axioms for the actions. foundational  domain independent axioms.
　let us now consider extending this basic situation calculus by time and continuous change according to  grosskreutz and lakemeyer  1a . note that these extensions are intended for the purpose of projections only. in section 1 we will discuss the necessary changes to allow on-line executions as well. we begin by adding two new sorts real and time.1similar to pinto and reiter  pinto  1; reiter  1   we introduce a special unary functional fluent start connecting situations and time. the intuition is that start denotes the time when situation begins.  we defer the formal definition of start until after the discussion of the passage of time. 
　as in  pinto  1   continuous change is modelled based on the idea that a fluent like the position of a robot takes as value a function of time. for example  in the situation where a robot starts moving  its 1-dimensional location can be characterized  in a somewhat idealized fashion  by a linear function of time  starting at its actual position and moving toward its destination. we call functional fluents whose values are continuous functions continuous fluents. in order to represent the value of continuous fluents  we add to the language another sort t-function  whose elements are meant to be functions of time. we assume that there are only finitely many function symbols of type t-function and we require domain closure and unique names axioms for them  just as in the case of primitive actions. furthermore  one needs to specify what values these functions have at any particular time . this is done with the help of the special binary function val. the following axiom illustrates the use of val  where the values of constant functions and a special kind of linear functions are defined.
	val const	;
val linear
　while const always evaluates to the tuple   linear is a linear function intended to approximate the movement of a robot in 1-dimensional space  we ignore the details of representing tuples of reals in logic .
　motivated by the fact that during the execution of plans time passes merely when the high-level controller is waiting  the fluent start changes its value only as a result of the special primitive action waitfor . the intuition is as follows. normally  every action happens immediately  that is  the starting time of the situation after doing in is the same as the starting time of . the only exception is waitfor : whenever this action occurs  the starting time of the resulting situation is advanced to the earliest time in the future when becomes true.
　the arguments of waitfor are restricted to so-called t-forms  which are special formulas involving continuous fluents with the situation argument suppressed  see  grosskreutz and lakemeyer  1a  for details .an example is battlevel . given a t-form   denotes with every continuous fluent evaluated in situation at time . for example  battlevel becomes val battlevel . the following axiom defines the least time point after the start of where becomes true: ltp	start start
a waitfor	-action is possible iff	has a least time point:
	poss waitfor	ltp
　finally  the following successor state axiom for start captures the intuition that the starting time of a situation changes only as a result of a waitfor   in which case it advances to the earliest time in the future when holds.
start
waitfor ltp waitfor start .
example to illustrate the use of these definitions  let us model how a robot's location changes as a result of primitive actions. for simplicity  we ignore the robot's orientation and represent its position by the continuous fluent robotloc  which evaluates to a tuple of reals. there are two types of actions that affect the location of the robot: startgo   which initiates moving the robot towards position  1 and endgo which stops the movement of the robot. we assume that startgo is only possible if the destination differs from the robot's current location. then we obtain the following successor state axiom for robotloc.
robotloc
start	val robotloc startgo linear
endgo	const startgo	endgo
robotloc

　where 	is a normalizing factor needed in order to ensure that the total 1-dimensional velocity does not exceed 1.
　the variables and refer to the coordinates of the robot. after startgo   robotloc has as value a linear tfunction starting at the current position and moving toward
　　　. after endgo  it is const . finally  if is neither a startgo nor a endgo action  robotloc remains unchanged.
cc-golog we now turn to cc-golog  which offers constructs such as sequences  iteration and procedures to define complex actions. in addition  parallel actions are introduced with a conventional interleaving semantics.
primitive action
waitfor	wait until condition	becomes true test action sequence if	conditional while	loop withpol	prioritized execution until	ends1 proc	procedure definition
using cc-golog  it is possible to specify event-driven behavior quite naturally. for example  the following program specifies that the robot is to travel to room 1 and say  in hallway  if it ever enters the hallway on its way.
prg1	withpol waitfor	 in hallway  send	reg
　here the send action indicates that the robot assigns the room number to a register  which leads to an activation of the navigation process to head to that room  and then blocks execution until the register reached signals that the navigation process has completed successfully. the details of this architecture are found in section 1.
　the semantics of cc-golog is specified similar to that of congolog  giacomo et al.  1 . its main ingredients are a predicate   which defines single steps of

1
　　for simplicity  we model the robot as always traveling at speed 1m/s. in a more realistic model  we would also consider different velocities. 1
　　unprioritized concurrency can be defined as well  but is omitted since we do not use it in the paper.
computation  and another predicate . specifies which configurations are final  meaning that the computation can be considered completed. this is the case  roughly  when the remaining program is   but not if there is still a primitive action or test action to be executed. we leave out the details for reasons of space.
　intuitively  the predicate associates with a given program and situation a new situation that results from executing a primitive action in   and a new program that represents what remains of after having performed that action. for space reasons  we only list a few of the axioms for .
	poss	do
false
1
withpol withpol withpol
　a transition of a primitive action requires it to be possible in the current situation  leaving nothing     to be done afterwards. the execution of with policy means that one action of one of the programs is performed  whereby actions which can be executed earlier are always preferred. if both and are about to execute an action at the same time  the
policy	takes precedence.1
　a final situation reachable after a finite number of transitions from a starting situation is identified with the situation resulting from a possible execution trace of program
  starting in situation	; this is captured by the predicate
　　　　　  which is defined in terms of	  the transitive closure of	:
where the ellipsis stands for the universal closure of the
conjunction of the following formulas:
　given a program   proving that is executable in the initial situation then amounts to proving do   where consists of the above axioms for cc-golog together with a basic action theory in the situation calculus.
1	a robot control architecture
as indicated in the introduction  in modern robot architectures like xavier  simmons et al.  1  and rhino  burgard et al.  1   the high-level controller does not directly operate the robot's physical sensors and effectors. instead  it

1
　　here  stands for a situation calculus formula with all situation arguments suppressed. will denote the formula obtained by restoring situation variable to all fluents appearing in . 1
　　note that requires a reification of formulas and programs in the logical language. see  giacomo et al.  1  for how this can be done.
activates and deactivates specialized processes like a navigation process  to which we will refer as low-level processes. the job of the high-level controller is then to combine the activation and deactivation of these routines in a way to fulfill the overall goal. it turns out that cc-golog allows a logical reconstruction of this type of architecture in a fairly natural way. the overall architecture is illustrated in figure 1.

figure 1: robot control architecture
　the communication between the low-level processes and the high-level plan is achieved through the special fluent reg. the high-levelinterpreter can affect the value of reg by means of the special action send id val which assigns reg id the value val. the intuition is that in order to activate a low-level process  the high-level controller executes a send action. for example  the execution of send would tell the navigation process to start moving toward room 1.
　on the other hand  the low-level processes can provide the high-level controller with information by means of the  exogenous  action id . for example  in order to inform the high-level controller that it has reached its destination  the low-level process would cause an exogenous action. the following successor state
axiom specifies how reg is affected by send and reply. reg id	val
	send id val	id val	reg id	val
send
1	modeling low-level processes as cc-golog procedures
to describe complex low-level processes like a navigation process  we model low-level processes as cc-golog procedures.1 given a faithful characterization of the low-level processes in terms of cc-golog procedures  we can then project the effect of the activation of these processes using their corresponding cc-golog models. we stress that these procedures are not meant to be executed  but rather represent a model of the effects of the corresponding low-level process.
　as an example  let us consider a low-level navigation process. it is activated through a send action  which assigns reg the value and tells the process to travel to room . the process causes the robot to move towards until the destination is reached  where it informs the high-level controller of the arrival by means of an action.
　we model this behavior by the cc-golog procedure navproc. navproc makes use of the following functions:
          	and	.	maps a room name	to a location near the exit of room	  and to location within	.	currentroom is a function whose value in	is the name of the room the robot is in at the beginning of	.	we write	as an abbreviation for startgo	waitfor	and	for while true	.
navproc	reg
	if	hallway
reg
	reg	endgo
reg
　initially  the navigation process is blocked until reg is assigned a room name different from the one the robot is actually in. then navproc executes a sequence of startgo actions  approximating the trajectory towards the destination room by a polyline with an edge near every door the robot has to travel through  see figure 1 .
1	projection
we will now describe how to project a cc-golog plan  taking into account the cc-golog model of the low-level processes. let be a situation  a model of the low-level processes 1 and a cc-golog program. a projection can then be identified with the situation that results from the concurrent simulation of and starting in .
	proj	do withpol	.
　let ax be the set of foundational axioms of section 1 together with the domain closure and unique names axioms for t-functions  the axioms required for t-form's  the axioms defining val  the precondition axiom for waitfor  and axioms stating that the robot is in room 1 together with other appropriate descriptions of the environment like a definition of inhallway in terms of coordinates on the map. using
       we can project the plan prg1 from section 1. we write do a shorthand for do do .
ax	prg1 navproc
dosendstartgowaitforstartgowaitforsay  in hallway waitforstartgowaitforendgo　the projected execution trace includes a startgo and waitfor action for every node on the approximating trajectory  where the stand for coordinates corresponding to the nodes in figure 1. additionally  it includes a say  in hallway  which is executed immediately after the hallway has been reached  which is assumed to happen just after leaving . the execution trace ends with
  which completes the navigation task.
1	on-line execution of cc-golog plans
projections such as the above should be understood as a way of assessing whether a program is executable in principle. the resulting execution trace is not intended as input to the execution mechanism of the robot for several reasons. for one  many of the actions like startgo only serve to model the navigation process and are not meant to be executed by the high-level controller at all. for another  actions that do belong to the high-level controller like waitfor must be interpreted differently during on-line execution. this is because during projection waitfor advances time to the least point where the condition is satisfied according to an idealized model of the world  like piece-wise linear trajectories . during actual execution  however  a waitfor should intuitively be treated simply as a test  where a condition like being at a certain location is matched against sensor readings reflecting the actual state of affairs.
　in order to see how such sensor readings can be obtained  let us briefly look at how actual robots like the rhino system deal with it. there we find a tight update loop between the low-level system and the high-level controller. this update loop periodically provides the high-level controller with an update of the low-level processes' estimate of continuous properties like the batteries' voltage level or the robot's position  typically several times a second . the period of time between two subsequent updates is so small that for practical purposes the latest update can be regarded as providing not only the correct current time but also accurate values of the continuous fluents at the current time.
　our solution is then to represent the updates by means of a new exogenous action ccupdate and to treat waitfor's simply as special tests during on-line execution. intuitively the effect of ccupdate is to set the value of the continuous fluents to the latest estimates of the low-level processes. the actual arguments of ccupdate depend on the continuous fluents that are to be updated. in our example scenario  ccupdate has four arguments:     and   where and refer to the current position of the robot  to the current voltage level and to the current time. to get a sense of the effect of ccupdate  let us consider a new version of the successor state axiom for robotloc of section 1 suitable for both on-line execution and projection. for simplicity  it is assumed that the actions startgo and endgo only occur during projection  as part of the model of the low-level navigation process .
robotloc
start	val robotloc startgo linear
	endgo	const
ccupdate
const
	startgo	ccupdate	endgo
robotloc
　note that ccupdate simply  assigns  the constant function const   which reflects the idea that  during on-line execution  the linear approximation of the trajectory plays no role and that the actual values should be used instead. besides updating the value of the continuous fluents  ccupdate is of prime importance because it causes time or  better  the internal clock to advance during execution. therefore  we need to modify the successor state axiom of the fluent start. in order to account for the fact that time advances differently in projections and during on-line execution  we need to explicitly distinguish between the two modes of operation. for that purpose  we introduce a special fluent which can only change its truth value by the special actions setonline and cliponline  respectively.  specifying the ssa for is easy and left out.  the ssa for start then becomes
start
	waitfor	ltp
ccupdate
start	waitfor ccupdate	.
in other words  start is assigned the least time point when the waitfor-condition becomes true under off-line execution  or the time value returned by ccupdate under on-line execution  or it remains the same. we remark that we could equivalently write two separate successor state axioms for the on-line and off-line case using the idea of guarded successor state axioms proposed in  giacomo and levesque  1 . while this would enhance the readability and modularity of the axioms  we do not pursue this idea further here for space reasons.
　the precondition axiom for ccupdate ensures that the starting time of legal action sequences is monotonically nondecreasing:
	ccupdate	start
　a waitfor instruction is treated as a special kind of test during on-line execution: it succeeds immediately iff the condition is true at the beginning of the actual situation. to ensure this intended behavior  we modify the definition of for primitive actions:
	poss	do
waitfor
	waitfor	start
　finally  we remark that another difference between projection and on-line execution is that during actual execution  whenever the high-level controller executes a send action  the interpreter should check whether this signals an activation of a low-level process and  as a side-effect  activate the actual low-level process if necessary.
on-line execution the situations which can result from the on-line execution of a plan have a special structure. basically  they are made up of the sequence of robot actions as defined by the possible transitions  similar to the execution traces defined by    however with an arbitrary number of exogenous actions between any two robot actions. formally  the situations which can result from the execution of a program in situation can be characterized by the following predicate :
　where the ellipsis stands for the universal closure of the conjunction of the following formulas:
  where ccupdate
　note that this differs from the original in section 1 only in the last conjunct  which expresses that an arbitrary number of exogenous actions may occur between any ordinary transition. as an example  let us consider a possible on-line execution trace of prg1. prg1's first action is send   which results in an activation of the navigation process  which we then assume to provide the high-level controller with ccupdate actions every .1 sec. the execution trace results in situation   where the stand for appropriate -coordinates along the path of the robot  we have left out the 1rd argument  voltage level  of ccupdate .
do send
ccupdate	ccupdate ccupdate	ccupdate
	say  in hallway 	ccupdate
.
　it is not hard to see that is a legal on-line execution trace of the introductory cc-golog program prg1  provided the coordinates are the first to satisfy inhallway and is near the destination    . note that waitfor no longer occurs in the action sequence since it is now merely a test. let ax be similar to ax but with the new definitions of this section concerning   start  ccupdate and
robotloc. let	be	. then
prg1
1	interleaving projection and on-line execution
 de giacomo and levesque  1  suggest that it is often useful to interleave on-line execution and projection. with our model of time we can take this idea one step further and define projection during on-line execution with the possibility to explicitly put a time-bound on the projection.
1	projection in non-initial situations

figure 1: projection during execution
　so far  we have only considered projection of cc-golog in the initial situation. to see how things work in non-initial situation  let us again consider our example scenario  where projection makes use of a model of the navigation process. suppose we are in the situation illustrated in figure 1  resulting from the on-line execution of prg1. the controller begins execution by activating the navigation process through send   after which the low-level process provides the high-level controller with a sequence of ccupdates  the ccupdates are indicated as black dots in the figure . the controller is now in situation with
do send
ccupdate ccupdate ccupdate ccupdate .
　first  let us consider the value of the continuous fluent robotloc in	. let	be defined as before. then robotloc	const	.
　we can also infer that what remains from prg1 in differs from the initial plan  because send has already been executed .
prg1
withpol waitfor	 in hallway  reg
　given the updated value of robotloc  we can now correctly project that the remaining plan will cause the robot to directly travel to its destination. note the use of cliponline  which forces a switch to projection mode.
prg1
proj donavprocdostartgowaitforsay  in hallway waitforstartgowaitforendgo do　the reason why projections in non-initial situations of the example scenario are fairly straightforward is that we never have to remember the state the actual navigation process is in. more precisely  for any given position  the cc-golog model of the navigation process yields an appropriate approximation of the remaining trajectory of the robot  as illustrated in the above projection . while many low-level processes used in mobile robots seem to have this property  we remark that processes where one needs to keep track of their internal state during execution can be dealt with in a way similar to  grosskreutz  1 . the details  however  are outside the scope of this paper.
1	projection tests
in order to allow for a time-bounded lookahead  we define the predicate proj   which is true  roughly  if holds at time in the projected execution of a plan in situation . let refer to an appropriate model of the low-level processes  as before.
proj
　　　start withpol
start
　again  we use cliponline to switch to projection mode. the disjunct involving covers the case where the projected plan ends before time . using proj within test conditions  a cc-golog plan can check whether a possible behavior would lead to certain conditions  and thus deliberate over different possible subplans. in addition  the lookahead can be limited to a certain amount of time   which seems very useful in order to make quick decisions. a possible application of this test  illustrated by the following program  would be to check if the battery level is going to drop below 1v in the next 1 seconds if the robot comes close to the battery docking station.
withpol if proj delivermail
chargebatteries
1	implementation
although the definition of cc-golog requires second-order logic  it is easy to implement a prolog interpreter for ccgolog  just as in the case of the original congolog.1 in order to deal with the constraints implied by the waitfor instruction during projection  we have made use of the ecrc common logic programming system eclipse 1 and its built-in constraint solver library clpr  similar to reiter  reiter  1  .
　as for the on-line execution of cc-golog plans  we have implemented a prototype runtime system that couples the ccgolog interpreter to the beesoft execution system  burgard et al.  1 . the link between our prolog implementation and the beesoft execution system is based on the high-level interface hli  ha：hnel et al.  1  which provides a uniform prolog interface to the low-level processes of beesoft  along with a monitoring component which provides status update about the state of execution of the activated modules. in particular  our runtime-system handles high-level send actions  which are mapped to commands to the low-level processes  and periodically generates ccupdate actions reflecting the lattest status updates of the low-level processes.
1	summary and discussion
in summary  we have extended cc-golog so that it becomes suitable for both projections of plans and their on-line execution. in doing so  we also incorporated a model of a robot architecture typically found on modern mobile robots. finally  we showed how to interleave on-line execution and a form of time-bounded projection.
　as mentioned before  the idea of modeling both projection and on-line execution was first explored in  de giacomo and levesque  1 . however  the authors consider neither time nor the idea of low-level processes which interact with a highlevel controller in complex ways. most importantly  there is no distinction between the effects on fluents during projection and on-line execution  a distinction we feel is necessary when it comes to modeling essential features of mobile robots such as their location at a given time.  lesperance and ng  1  have extended de giacomo and levesque's ideas in a direction which bears some resemblances to ours. they propose that during projection one also needs to consider a simulated environment which  for example  produces exogenous actions to inform the high-level controller that the destination is reached. however  their notion of a simulated environment remains fairly simple since they are not able to model temporally extended processes  and they also do not distinguish between the effects on fluents during projection and on-line execution. we feel that our proposal addresses some of these shortcomings and brings logic-based robot controllers yet another step closer to real world robotics.
　the examples in this paper involving navigation tasks of a single robot were chosen mainly because of their simplicity and as such may not be too convincing as to why logicbased robot controllers with built-in projection mechanisms are beneficial or even necessary  especially at run-time. we believe that the advantages of projections will become much more apparent once robots engage in more complex tasks. consider  for example  a multi-robot delivery scenario where a user makes a request to have a letter delivered by one of the robots. then in order to determine which robot should deliver the letter  each robot might use projection to determine the cost that would arise if it were to carry out this job  and the task could then be assigned to the robot with a minimal cost estimate. on-the-fly projection becomes even more important when robots need to coordinate their activities. suppose that two robots agree to meet somewhere at a certain time in the future. until then they would probably want to carry out as much of their other duties as possible. since tasks may not be interruptible at arbitrary times  each robot would be well-advised to check how much of the current task can be completed before heading off to the meeting point. to do so  some form of projection seems necessary. we plan to investigate these and other multi-robot scenarios in the future.
references
 burgard et al.  1  w. burgard  a.b. cremers  d. fox  d. ha：hnel  g. lakemeyer  d. schulz  w. steiner  and s. thrun. experiences with an interactive museum tour-guide robot.
artificial intelligence  1-1   1.
 de giacomo and levesque  1  g.	de	giacomo	and	h.j.
levesque. an incremental interpreter for high-level programs with sensing. in h. levesque and f. pirri  editors  logical foundations for cognitive agents  pages 1. springer  1.
 giacomo and levesque  1  g. de giacomo and h. levesque. projection using regression and sensors. in proc. ijcai'1  1.
 giacomo et al.  1  guiseppe de giacomo  yves lesperance  and hector j levesque. congolog  a concurrent programming language based on the situation calculus. artificial intelligence  1-1  1.
 grosskreutz and lakemeyer  1a  h. grosskreutz and g. lakemeyer. cc-golog: towards more realistic logic-based robot controllers. in aaai'1  1.
 grosskreutz and lakemeyer  1b  h. grosskreutz and g. lakemeyer. turning high-level plans into robot programs in uncertain domains. in ecai'1  1.
 grosskreutz  1  h. grosskreutz. probabilistic projection and belief update in the pgolog framework. in second international cognitive robotics workshop  1.
 ha：hnel et al.  1  d. ha：hnel  w. burgard  and g. lakemeyer. golex - bridging the gap between logic  golog  and a real robot. in proceedings of the 1st german conference on artificial intelligence  ki 1   1.
 lesperance and ng  1  y. lesperance and h.-k. ng. integrating planning into reactive high-level robot programs. in second international cognitive robotics workshop  1.
 levesque et al.  1  hector j. levesque  raymond reiter  yves lesprance  fangzhen lin  and richard scherl. golog: a logic programming language for dynamic domains. journal of logic programming  1-1  1.
 levesque et al.  1  hector levesque  fiora pirri  and ray reiter. foundations for the situation calculus. linko：ping electronic articles in computer and information science  1   1. url: http://www.ep.liu.se/ea/cis/1/.
 mccarthy  1  j. mccarthy. situations  actions and causal laws. technical report  stanford university. reprinted 1 in semantic information processing  m.minske ed.   mit press  1.
 pinto  1  javier pinto. integrating discrete and continuous change in a logical framework. computational intelligence  1   1.
 reiter  1  ray reiter. natural actions  concurrency and continuous time in the situation calculus. in proc. kr'1  pages 1  1.
 reiter  1  r. reiter. sequential  temporal golog. in proc. kr'1  1.
 simmons et al.  1  r.g. simmons  r. goodwin  k.z. haigh  s. koenig  j. o'sullivan  and m.m. veloso. xavier: experience with a layered robot architecture. acm sigart bulletin intelligence  1  1   1.
an on-line decision-theoretic golog interpreter.
mikhail soutchanski
dept. of computer science
university of toronto
toronto  on m1s 1 mes cs.toronto.eduabstract
we consider an on-line decision-theoretic interpreter and incremental execution of golog programs. this new interpreter is intended to overcome some limitations of the off-line interpreter proposed in  boutilier et al.  1 . we introduce two new search control operators that can be mentioned in golog programs: the on-line interpreter takes advantage of one of them to save computational efforts. in addition to sensing actions designed to identify outcomes of stochastic actions  we consider a new representation for sensing actions that may return both binary and real valued data at the run time. programmers may use sensing actions explicitly in golog programs whenever results of sensing are required to evaluate tests. the representation for sensing actions that we introduce allows the use of regression  a computationally efficient mechanism for evaluation of tests. we describe an implementationof the on-line incremental decision-theoretic golog interpreter in prolog. the implementation was successfully tested on the b1 robot manufactured by rwi.
1	introduction
the aim of this paper is to provide a new on-line architecture of designing controllers for autonomous agents. specifically  we explore controllers for mobile robots programmed in dtgolog  an extension of the high-level programming language golog. dtgolog aims to provide a seamless integration of a decision-theoretic planner based on markov decision processes with an expressive set of program control structures available in golog. the motivation for this research is provided in  boutilier et al.  1   and we ask the reader to consult that paper for additional technical details and arguments why neither model-based planning  nor programming alone can manage the conceptual complexity of devising controllers for mobile robots. the dtgolog interpreter described in  boutilier et al.  1  is an off-line interpreter that computes the optimal conditional policy   the probability that can be executed successfully and the expected utility of the policy. the semantics of a dtgolog program is defined by the predicate   where is a starting situation and is a given finite horizon. the policy returned by the off-lineinterpreteris a gologprogramconsistingof the sequential composition of agent actions  senseeffect sensing actions  which serve to identify a real outcome of a stochastic action   and conditionals  if then else    where
is a situation calculus formula that provides a test condition to decide which branch of a policy the agent should take given the result of sensing. the interpreter looks ahead up to the last choice point in each branch of the program  say  between alternative primitive actions   makes the choice and then proceeds backwards recursively  deciding at each choice point what branch is optimal. it is assumed that once the off-line dtgolog interpreter has computed an optimal policy  the policy is given to the robotics software to control a real mobile robot.
　however this off-line architecture has a number of limitations. imagine that we are interested in executing a program
    where both sub-programs and are very large nondeterministicdtgolog programs designed to solve 'independent' decision-theoretic problems: is supposed to start in a certain set of states  but from the perspective of it is not important what policy will be used to reach one of those states. intuitively  in this case we are interested in computing first an optimal policy  that corresponds to    executing in the real world and then computing and executing an optimal policy . but the off-line interpreter can return only the optimal policy that corresponds to the whole program      spending on this computation more time than necessary: to compute and execute it is not relevant what decisions have to be made during the execution of . the second limitation becomes evident if in addition to senseeffect sensing actions serving to identify outcomes of stochastic actions  we need to explicitly include sensing actions in the program  and those sensing actions cannot be characterized as stochastic actions with a fixed finite set of outcomes . imagine that we are given a program
	if	then	else	 
where is a sensingaction that returns at time a measurement of a quantity  e.g.  the current coordinates  the battery voltage  and the condition depends on the real data that will be returned by sensors  in the program above are sub-programs  the choice operator binds variables and and the test grounds the current time . intuitively  we want to compute an optimal policy  corresponding to   off-line  execute in the real world  sense and then compute and execute an optimal policy that corresponds to the conditional golog program in brackets. but the off-line interpreter is not able to compute an optimal policy if a given programincludes explicit sensingactions andno information is available about the possible results of sensing. note that the nondeterministic choice operator  it occurs in front of the program  should not be confused with policies and computed by an interpreter.
　we propose to compute and execute optimal policies online using a new incremental decision theoretic interpreter. it works in a step-by-step mode. given a golog program and a starting situation   it computes an optimal policy and the program that remains when a first action in will be executed. at the next stage  the action is executed in the real world. the interpreter gets sensory information to identify which outcome of has actually occurred if is a stochastic action: this may require doing a sequence of sensing actions on-line. the action  and possibly sensing actions performed after that  results in a new situation. then the cycle of computing an optimal policy and remaining program  executing the first action and getting sensory information  if necessary  repeats until the program terminates or execution fails. in the context of the incremental on-line execution  we define a new programming operator that limits the scope of search performedby the interpreter. if is the whole program  then no computational efforts are saved when the interpreter computes an optimal policy from   but if the programmer writes   then the on-line incremental interpreter will compute and execute step-by-step the golog program without looking ahead to decisions that need to be made in . if the programmer knows that the sensing action is necessary to evaluate the condition   then
using the program
	if	then	else
the required information about will be obtained before the incremental interpreter will proceed to the execution of the conditional. if thesensingaction hasmany different outcomes  then this approach gives computational advantages over the off-line approach to computing an optimal policy.
　thus  the incremental interpretation of the decisiontheoretic golog programs needs an account of sensing  formulated in the situation calculus  that will satisfy several criteria which naturally arise in the robotics context. there are several accounts of sensing in the situation calculus that address different aspects of reasoning about sensory and physical actions  bacchus et al.  1; de giacomo and levesque  1a; 1b; funge  1; grosskreutz  1; lakemeyer  1; levesque  1; pirri and finzi  1; scherl and levesque  1 .
　belowwe proposea newrepresentationofsensingthat simplifies reasoning about results of sensing  does not require consistency of sensory information with the domain theory  leads to a natural and sound implementation and has connections with representation of knowledge and sensing considered in  reiter  1 .
　in section1 we recall the representationof the decision theoretic domainintroducedin  boutilieret al.  1 . in section 1 we propose a representationforsensingactionsand consider several examples. in section 1 we consider the on-line incrementaldecision-theoreticinterpreter. section1discussesconnections with previously published papers.
1	the decision theoretic problem representation
the paper  boutilier et al.  1  introduces the representation of problem domains that do not include sensing actions. the representation is based on the distinction between agent actions  which can be either deterministic or stochastic  and nature's actions which correspond to separate outcomes of a stochasticagent action. nature's actions are considered deterministic. they cannot be executed by the agent itself  therefore they never occur in policies which the agent executes. when the agent does a stochastic action in a situation   nature chooses one of the outcomes of that action and the situation is considered as one of resulting situations. in accordance with this perspective  the evolution of a stochastic transition system is specified by precondition and successor state axioms which never mention stochastic agent actions  but mention only deterministic agent actions and nature's actions. in  boutilier et al.  1   it is suggested to characterize the dtgolog problem domain by: 1  the predicate which holds if is an agent action  1  the predicate meaning that nature's action is one of outcomes of the stochastic agent action in the situation   1  the function symbol that denotes the probability of nature's action in situation   and 1  the predicate sensecond specifying the test condition serving to identify the outcome of the last stochastic action  1  the function symbol denotes rewards and costs as functions of the current situation   the action or both.
　as an example  imagine a robot moving between different locations: the process of going is initiated by a deterministic action but is terminated by a stochastic action that may have two different outcomes: successful arrival and unsuccessful stop in a place different from the destination  the robot gets stuck in the hall or cannotenter an office because its door is closed . we represent the process of moving between locations and by the relational fluent and represent a  symbolic  location of the robot by the relational fluent meaning that  the office of an employee  the hall or the main office  is the place where the robot is. the transitions in the stochasticdynamical system describing the robot'smotion are characterized by the following precondition and successor state axioms.
poss startgo	going	robotlocposs endgosgoingposs endgofgoinggoingstartgogoingendgosgoingendgof　the real outcome of a stochastic agent action can be identified only from sensory information. this information has to be obtained by executing sensing actions. in the following section we propose a new representation for sensing actions providing a seamless integration with the representation considered in this section.
1	sensing actions
in contrast to physical actions  sensing actions do not change any properties of the external world  but return values measured by sensors. despite this difference  it is convenient to treat both physical and sensing actions equally in successor state axioms. this approach is justifiable if fluents represent what the robot 'knows' about the world  see figure 1 . more specifically  let the high-level control module of the robot be provided with the internal logical model of the world  the set of precondition and successor state axioms  and the axiomatization of the initial situation. the programmer expresses in this axiomatization his  incomplete  knowledge of the initial situation and captures certain important effects of the robot'sactions  but some other effects and actions of other agentsmay remain unmodeled. whenthe robot does anaction in the real world  i.e.  the high-level control module sends a command to effectors and effectors execute this command   it does not know directly and immediately what effects in reality this action will produce: the high-level control module may only compute expected effects using the internal logical model. similarly  if the robot is informed about actions executed by external agents  the high-level control module may compute expected effects of those exogenous actions  if axioms account for them . thus  from the point of view of the programmer who designed the axioms  the high-level control modulemaintains the set of beliefs that the robot has about the real world. this set of beliefs needs feedback from the real world because of possible contingencies  incompleteness of the initial information and unobserved or unmodeled actions executedby other agents. to gain the feedback  the high-level controlmodulerequestsinformationfromsensors andtheyreturn required data. we find it convenient to represent information gathered from sensors by an argument of the sensing action: a value of the argument will be instantiated at the run time when the sensing action will be executed. then  all the high-level control module needs to know is the current situation  action log : expected effects of actions on fluents can be computed from the given sequence of physical and sensing actions.

figure 1: a high-level control module and low-level modules interacting with the environment.
　in the sequel  we consider only deterministic sensing actions  but noisy sensing actions can be represented as well.
　we suggest representing sensing actions by the functional symbol where is what to sense  is term representing a run time value returned by the sensors and is time; the predicate is true whenever is a sensing action. we proceed to consider several examples of successor state axioms that employ our representation.
　1. let be the sensing action that returns the pair of geometric coordinates of the current robot location on the two-dimensional grid and be a relational fluent that is true if
are the coordinates of the robot's location in . in this example we assume that all actions are deterministic  we do this only to simplify the exposition of this example . the process of moving  represented by the relational fluent   from the grid location	to
the point is initiated by the deterministic instantaneous action and is terminated by the deterministic action . the robot may also change its location if it is transported by an external agent from one place to another: the exogenous actions and account for this  and the fluent represents the process of moving the robot by an external agent . the followingsuccessor state and precondition axioms characterize aforementioned fluents and actions.
endmove
endmove
where denote  respectively  and components of the current robot location .
startmove endmove
poss startmove
poss endmove
poss
poss
 imagine that in the initial situation the robot stays at  1 ; later  at time 1  it starts moving from  1  to  1   but when the robot stops at time 1 and senses its coordinatesat time1  its sensors tell it that it is located at  1 . the sensory information is inconsistent with the expected location of the robot  butthis discrepancycan be attributed to unobservedactionsof an external agent who transported the robot. hence  the final situation is not
endmove
startmove
 as we originally expected  but the situation resulting from the execution of exogenous actions and
　　　　　  in the situation when the robot ends moving  followed by the sensing action. the exogenous actions occurred at unknown times  we may say about the actual history only that  .1
　1. the robot can determine its location using data from sonar and laser sensors. but if the last action was not sensing coordinates   then the currentlocationcan bedetermined from the previous location and the actions that the robot has executed. the process of going from to is initiated by the deterministic action and is terminated by the stochastic action  axiomatized in section 1 .
	office	inoffice
inoffice office
	where the predicate inoffice	is true if the pair
is inside of the office   the functional symbols bottomy   topy   rightx and leftx represent coordinates of top left and bottom right corners of a rectangle that contains the office inside: when the robot stops and senses coordinates  it determines its real location and the high-level control module can identify the outcome of the stochastic action : whetherthe robot stopped successfullyor failed to arrive at the intended destination.
　1. let be a stochastic action that has two different outcomes: - the robot gives successfully an to at time - and
- the action of giving an	to is unsuccessful.	let	be the ac-
tion of sensing whether delivery of the to was successful or not: if it was  then delivery is acknowledged and =1  if not - the result of sensing is =1. the following successor state axiom characterizes how the fluent hascoffee changes from situation to situation: whenever the robot is in the office of and it senses that one of its buttons was pressed  it is assumed that the pressed a button to acknowledge that she has a coffee. from this sensory information  the high-level control module can identify whether the outcome of was successful or not.
hascoffee
	coffee	hascoffee
office
　it is surprisingly straightforward to use regression in our setting to solve the forward projection task. let be a background axiomatization of the domain  a set of successor state axioms  precondition axioms  unique name axioms and a set of first order sentences whose only situation term is   and let be a situation calculus formula that has the free variable as the only situational argument. suppose we are given a ground situational term that may mention both physical and sensing actions. the forward projection task is to determine whether
　regressionis a computationallyefficient wayof solving the forward projection task  reiter  1; pirri and reiter  1  when does not mention any sensing actions. the representation introduced above allows us to use regression also in the case when mentions sensing actions explicitly. thus  we can use regression to evaluate tests in golog programs with sensing actions: no modifications are required. in addition  our approach allows us to use an implementation in prolog considered in  reiter  1 . there is also an interesting connection between our representation of 'beliefs' and sensing and the approach to knowledge-based programming  reiter  1 . in  soutchanski  1  we show that his approach andour approach to the solution of the forward projection task  with sensing actions  are equivalent.
1	the incremental on-line dtgolog interpreter
the incremental dtgolog interpreter uses the predicate incrbestdo and the predicate
　　　　　　　. the former predicate takes as input the golog program   starting situation   horizon and returns the optimal conditional policy   its expected utility   the probability of success   and the program that remains after executing the first action from the policy . the latter predicate tells when the execution of the policy completes: is true if either the program is  the null program  or  a zero cost action that takes the agent to an absorbing state meaning that the execution failed   or if the policy is or . in all these cases  is simply the reward associated with the situation . note that in comparison to bestdo  incrbestdo has an additional argument representing the program that remains to be executed.
　incrbestdo is defined inductively on the structure of a golog program . below we consider its definition in the case when the program begins with a deterministic agent action.
incrbestdo
	stop	reward
	incrbestdo	-
if a deterministic agent action is possible in situation   then we compute the optimal policy of the remaining program
　  its expected utility and the probability of successful termination . because is a deterministic action  the probability that the policy will complete successfully is the same as for the program itself; the expected utility is a sum of a reward for being in and the expected utility of continuation . if is not possible  then the remaining program is
     and the policy is the stop action  the expected utility is the reward in and the probability of success is 1.
　other cases are defined similarly to   e.g.  if the program begins with the finite nondeterministic choice
　　　  where is the finite set and the choice binds all free occurrences of in to one of the elements:
incrbestdo
incrbestdo
where means substitution of for all free occurrences of in . thus  the optimal policy corresponds to the element in that delivers the best execution. notethat the remaining
program	is the same on the both sides of the definition.
　recall that policies are golog programs as well. moreover  if is a golog program that contains many nondeterministic choices the optimalpolicy computedfrom is aconditional program that does not involve any nondeterministic choices. this observation suggests that programmers may wish to take advantage of the structure in a decision theoretic problem and use explicit search control operators that limit bounds where the search for an optimal policy has to concentrate. in addition to the standard set of golog programming constructs  we introduce two new operators:	and	.
intuitively  the program	means the following.
first  computethe optimalpolicy correspondingto the subprogram   then compute the optimal policy corresponding to the program . if both sub-programs and are highly nondeterministic  then using the operator the programmer indicates where the computational efforts can be saved: there is no need in looking ahead further than to compute . thus  in the case that a golog program begins with
incrbestdo
incrbestdo
incrbestdo
 the construct limits the search  but once the policy was computed and the first action in was executed  the remaining part of the program has no indication where the search may concentrate. for this reason  the programmer may find convenient to use another search control operator that once used persists in the remaining part of the program untilthe programinside the scopesof that operatorterminates. thisoperatoris called andis specifiedby the following abbreviation.
incrbestdo
incrbestdo
 according to this specification  an optimal policy can be computed without looking ahead to the program ; hence  using a programmer can express a domain specific procedural knowledge to save computational efforts. note that when is nil  i.e. there will be nothing in to execute after doing the only action in   the remaining program
	contains only	.
1	implementing the on-line interpreter
given the definitions of incrbestdo mentioned in the previous sub-section  we can consider now the on-line interpretation coupled with execution of golog programs. the definitions of incrbestdo translate directly into prolog clauses  we omit them here . the on-line interpretercallsthe off-lineincrbestdo e s er h pol1 u1 prob1  interpreter to compute an optimal policy from the given program expression   gets the first action of the optimal policy  commits to it  does it in the physical world  then repeats with the rest of the program. the following is such an interpreter implemented in prolog:
online e s h pol u  :incrbestdo e s er h pol1 u1 prob1  
  final er s h pol1 u1   pol=pol1  u=u1 ; reward r s  
pol1 =  a : rest  
  agentaction a   not stochastic a s l   doreally a   /*execute a in reality*/
　!  /* commit to the result */ online er do a s  h polfut ufut  
pol= a : polfut   u is r + ufut ; senseaction a   doreally a   /* do sensing */
　!  /*commit to results of sensing*/ online er do a s  h polfut ufut  
pol= a : polfut   u is r + ufut ;
agentaction a   stochastic a s l   doreally a   /*execute a in reality*/ !  /* commit to the result */ senseeffect a s seff   diagnose seff l sn   /*what happened */ online er sn h polfut ufut  
pol= a : polf   u is r + ufut
　   .
　the on-line interpreter uses the prolog cut  !  to prevent backtrackingto the predicate : we need this because once actions have been actually performed in the physical world  the robot cannot undo them.
　in addition to predicates mentioned in section 1  the on-line interpreter uses the predicate senseeffect   the predicate diffsequence and the predicate
. we describe below their
meaning and show their implementation in prolog.
　given the stochastic action and the situation   the predicate senseeffect holds if is the situation that results from doing a number of sensing actions necessary to differentiate between outcomes of the stochastic action . the predicate diffsequence holds if is the sequence of sensing actions     specified by the programmer in the domain problem representation: this sequence is differentiating if after doing all actions in the sequence the action chosen by 'nature' as the outcome of stochastic action can be uniquely identified.
senseeffect a s se  :- diffsequence a seq   getsensorinput s seq se .
getsensorinput s a do a s   :- senseaction a   doreally a . /*connect to sensors  get data for a free variable in a */
getsensorinput s1  a : rest  se  :doreally a   /* connect to sensors  get data	*/
getsensorinput do a s1  rest se .
　the predicate takes as its first argument the situation resulting from getting a sensory input: it contains 'enough' information to disambiguate between different possible outcomes of the last stochastic action . the second argument is the list of all outcomes that nature may choose if the agent executes the stochastic action in and the third argument is the situation that is the result of nature's action which actually occurred. we can identify which action naturehas chosen using the set of mutually exclusivetest conditions sensecond   where is a term representing a situation calculus formula: if holds in the current situation  then we know that nature has chosen the action   belongs to the list  .
diagnose se  n  do n se   :sensecond n c   holds c se .
diagnose se  n|outcomes  sn :- sensecond n c  
  holds c se   sn=do n se  ; not holds c se   diagnose se outcomes sn   .
　successful tests of the implementation described here were conducted in a real office environment on a mobile robot b1 manufactured by rwi. the low-level software was initially developedin the university of bonn to control rhino  another b1 robot  see  burgard et al.  1  for details. the tests of implementation demonstrated that using the expressive set of gologoperatorsitisstraightforwardtoencode domainknowledge as constraints on the given large mdp problem. the operator proved to be useful in providing heuristics whichallowedto computesub-policiesinreal time. these preliminary tests have brought several new important issues  e.g.  how the computationof a newpolicyoff-linecan proceed in parallel with executing actions from the policy on-line.
1	discussion
the incremental golog interpreter based on the singlestep -semantics is introduced in  de giacomo and levesque 1b . thegologprogramsconsideredtheremay include binary sensing actions. the interpreter considered in our paperis motivated by similarintuitions  but itis based on a different decision-theoretic semantics and employs more expressive representation of sensing. the paper  de giacomo and levesque  1a  introduces guarded sensed fluent axioms gsfa  andguardedsuccessor stateaxioms gssa  and assumes that there is a stream of sensor readings available at any time. these readings are represented by unary sensing functions syntactically theylooklike functionalfluents . becausewe introducethe representationfor sensingactions  they can be mentioned explicitly in golog programs or can be executed by the interpreter. the major advantage of our representation is that it does not require consistency of sensory readings with the action theory  this may prove useful in solving diagnostic tasks:  mcilraith  1  . the execution monitoring framework proposed in  de giacomo et al.  1  assumes that the feedback from the environment is provided in terms of actions executed by other agents. because in this paper we assume that the feedback is provided in terms of sensory readings  thismay leadtodevelopmentof a morerealistic framework. an approach to integrating planning and execution in stochastic domains  dearden and boutilier 1  is an alternative to the approach proposed here.
1	concluding remarks
several important issues are not covered in this paper. one of them is monitoring and rescheduling of policies. note that all actions in policies have time arguments which will be instantiated by moments of time: when the incremental interpreter computes an optimal policy  it also determines a schedule when actions have to be executed. but in realistic scenarios  when the robot is involved in ongoing processes extended over time  it may happen that a process will terminate earlier or later than it was expected.
　the diagnostic task that the current version of the on-line interpretersolvesis admittedlyoversimplified. we expectthat additional research integrating the on-line incremental interpreter with the approach proposed in  mcilraith  1  will allow us to formulate a more comprehensive version.
1	acknowledgments
thanks to ray reiter  maurice pagnucco  and anonymous reviewers for comments on preliminary versions of this paper. sam kaufman providedhelp with conducting tests on the mobile robot.
references
 bacchus et al.  1  fahiem bacchus  joseph y. halpern  and hector j. levesque. reasoning about noisy sensors in the situation calculus. in proceedings of the fourteenth international joint conference on artificial intelligence  pages 1  montreal  1.
 boutilier et al.  1  c. boutilier  r. reiter  m. soutchanski  and s. thrun. decision-theoretic  high-level robot programming in the situation calculus. in proc. of the 1th national conference on artificial intelligence  aaai'1   austin  texas  1.
 burgard et al.  1  w. burgard  a.b. cremers  d. fox  d. ha：hnel  g. lakemeyer  d. schulz  w. steiner  and s. thrun. experiences with an interactive museum tour-guide robot.
artificial intelligence  1-1   1.
 de giacomo and levesque  1a  g. de giacomo and h. levesque. projection using regression and sensors. in proceedings of the sixteenth international joint conference on artificial intelligence  stockholm  sweden  1.
 de giacomo and levesque  1b  g. de giacomo and h.j. levesque. an incremental interpreter for high-level programs with sensing. in levesqueand pirri  editors  logicalfoundations for cognitive agents: contributions in honor of ray reiter  pages 1. springer  1.
 de giacomo et al.  1  g. de giacomo  r. reiter  and m.e. soutchanski. execution monitoring of high-level robot programs. in principles of knowledge representation and reasoning: proc. of the 1th international conference  kr'1   pages 1  trento  italy  1.
 dearden and boutilier  1  richard dearden and craig boutilier. integrating planning and execution in stochastic domains. in proceedingsof the tenth conference on uncertainty in artificial intelligence  pages 1  1.
 funge  1  j. funge. making them behave: cognitive models for computer animation  ph.d. thesis. dept. of computer science  univ. of toronto  1.
 grosskreutz  1  h. grosskreutz. probabilistic projection and belief update in the pgolog framework. in the 1nd international cognitive robotics workshop  1th european conference on ai  pages 1  berlin  germany  1.
 lakemeyer  1  g. lakemeyer. on sensing and off-line interpreting in golog. in levesque and pirri  editors  logical foundations for cognitiveagents: contributions in honor of ray reiter  pages 1. springer  1.
 levesque  1  h.j. levesque. what is planning in the presence of sensing  inproceedingsof thethirteenthnationalconference on artificial intelligence  volume 1  pages 1  portland  oregon  1.
 mcilraith  1  s. mcilraith. explanatory diagnosis: conjecturing actions to explain obsevations. in principles of knowledge representation and reasoning: proc. of the 1th international conference  kr'1   pages 1  italy  1.
 pirri and finzi  1  f. pirri and a. finzi. an approachto perception in theory of actions: part 1. linko：ping electronic articles in computer and information science  1   1.
 pirri and reiter  1  f. pirri and r. reiter. some contributions to the metatheory of the situation calculus. journal of the acm  1 :1  1.
 reiter  1  r. reiter. knowledge in action: logical foundations for describing and implementing dynamical systems. http://www.cs.toronto.edu/ cogrobo/  1.
 scherl and levesque  1  r. scherl and h.j. levesque. the frame problem and knowledgeproducing actions. in proceedings of the eleventh national conference on artificial intelligence  pages 1  washington  dc  1.
 soutchanski  1  m soutchanski. a correspondence between two different solutions to the projection task with sensing. in the 1th symposiumon logical formalizations of commonsensereasoning  new york  usa  1.

knowledge representation
and reasoning
structure-based causality

causes and explanations: a structural-model approach- part ii: explanations

joseph y. halpern
cornell university
dept. of computer science
ithaca  ny 1 halpern cs.cornell.edu www.cs.cornell.edu/home/halpern judea pearl
dept. of computer science
university of california  los angeles los angeles  ca 1 judea cs.ucla.edu wwww.cs.ucla.edu/ judea

abstract
we propose a new definition of  causal  explanation  using structural equations to model counterfactuals. the definition is based on the notion of actual cause  as defined and motivated in a companion paper. essentially  an explanation is a fact that is not known for certain but  if found to be true  would constitute an actual cause of the fact to be explained  regardless of the agent's initial uncertainty. we show that the definition handles well a number of problematic examples from the literature.
1	introduction
the automatic generation of adequate explanations is a task essential in planning  diagnosis and natural language processing. a system doing inference must be able to explain its findings and recommendations to evoke a user's confidence. however  getting a good definition of explanation is a notoriously difficult problem  which has been studied for years.  see  chajewska and halpern  1; ga：rdenfors  1; hempel  1; pearl  1; salmon  1  and the references therein for an introduction to and discussion of the issues. 
　in  halpern and pearl  1   we give a definition of actual causality using structural equations. here we show how the ideas behind that definition can be used to give an elegant definition of  causal  explanation that deals well with many of the problematic examples discussed in the literature. the basic idea is that an explanation is a fact that is not known for certain but  if found to be true  would constitute an actual cause of the explanandum  the fact to be explained   regardless of the agent's initial uncertainty.
1	causal models: a review
to make this paper self-contained  this section repeats material from  halpern and pearl  1 ; we review the basic definitions of causal models  as defined in terms of structural equations  and the syntax and semantics of a language for

　　supported in part by nsf under grants iri-1 and iis1.
　　supported in part by grants from nsf  onr  afosr  and micro.
reasoning about causality and explanations. see  galles and pearl  1; halpern  1; pearl  1  for more details  motivation  and intuition.
causal models: the basic picture is that the world is described by random variables  some of which may have a causal influence on others. this influence is modeled by a set of structural equations each equation represents a distinct mechanism  or law  in the world  which may be modified  by external actions  without altering the others. in practice  it seems useful to split the random variables into two sets  the exogenous variables  whose values are determined by factors outside the model  and the endogenous variables  whose values are determined by the endogenous variables. it is these endogenous variables whose values are described by the structural equations.
　more formally  a signature is a tuple   where is a finite set of exogenous variables  is a finite set of endogenous variables  and associates with every variable a nonempty set of possible values for .
a causal  or structural  model over signature	is a tuple
           where	associates with each variable a function denoted	such that
	.	tells us the value of
given the values of all the other variables in	.
example 1: suppose that we want to reason about a forest fire that could be caused by either lightning or a match lit by an arsonist. then the causal model would have the following endogenous variables  and perhaps others :
	for fire  	if there is one 	otherwise 
	for lightning  	if lightning occurred 
otherwise 
ml for match lit  ml	if the match was lit and otherwise .
the set of exogenous variables includes conditions that suffice to make all relationships deterministic  such as whether the wood is dry  there is enough oxygen in the air  etc. . suppose that is a setting of the exogenous variables that
makes a forest fire possible  i.e.  the wood is sufficiently dry  there is oxygen in the air  and so on . then  for example 
	ml is such that	if either	or ml	.

　given a causal model   a  possibly empty  vector of variables in   and vectors and of values for the variables in and   respectively  we can define a new causal model denoted over the signature
. intuitively  this is the causal
model that results when the variables in are set to by external action  the cause of which is not modeled explicitly.
formally    where is obtained from by setting the values of the variables in to .
　we can describe  some salient features of  a causal model using a causal network. this is a graph with nodes corresponding to the random variables in and an edge from a node labeled to one labeled if depends on the value of . intuitively  variables can have a causal effect only on their descendants in the causal network; if is not a descendant of   then a change in the value of has no affect on the value of .
　we restrict attention to what are called recursive  or acyclic  equations; these are ones that can be described with a causal network that is a dag. it should be clear that if is a recursive causal model  then there is always a unique solution to the equations in   given a setting for the variables in . such a setting is called a context. contexts will play the role of possible worlds when we model uncertainty.
syntax and semantics: given a signature   a formula of the form   for and   is called a primitive event. a basic causal formula is one of the form   where is a boolean combination of primitive events; are variables in ; are distinct; ; and .
such a formula is abbreviated as . the special case where is abbreviated as . intuitively 
                 says that holds in the counterfactual world that would arise if is set to   . a causal formula is a boolean combination of basic causal formulas.
　a causal formula is true or false in a causal model  given a context. we write if is true in causal model given context .	if the variable has value in the unique  since we are dealing with recursive models  solution to the equations in in context  that is  the unique vector of values for the exogenous variables that simultaneously satisfies all equations  
　　　　　  with the variables in set to  . we extend the definition to arbitrary causal formulas in the obvious way.
　note that the structural equations are deterministic. we later add probability to the picture by putting a probability on the set of contexts  i.e.  on the possible worlds .
1	the definition of explanation
as we said in the introduction  many definitions of causal explanation have been given in the literature. the  classical  approaches in the philosophy literature  such as hempel's 1 deductive-nomological model and salmon's 1 statistical relevance model  as well as many other approaches  have a serious problem: they fail to exhibit the directionality inherent in common explanations. while it seems reasonable to say  the height of the flag pole explains the length of the shadow   it would sound awkward if one were to explain the former with the latter. despite all the examples in the philosophy literature on the need for taking causality and counterfactuals into account  and the extensive work on causality defined in terms of counterfactuals in the philosophy literature  as woodward 1 observes  philosophers have been reluctant to build a theory of explanation on top of a theory of causality. the concern seems to be one of circularity.
　in  halpern and pearl  1   we give a definition of causality that assumes that the causal model and all the relevant facts are given; the problem is to determine which of the given facts are causes.  we discuss this definition in more detail below.  we give a definition of explanation based on this defintion of causality. the role of explanation is to provide the information needed to establish causation. as discussed in the introduction  we view an explanation as a fact that is not known for certain but  if found to be true  would constitute a genuine cause of the explanandum  regardless of the agent's initial uncertainty. thus  what counts as an explanation depends on what you already know and  naturally  the definition of explanation is relative to the agent's epistemic state  as in ga：rdenfors 1 . it is also natural  from this viewpoint  that an explanation includes fragments of the causal model m  or reference to the physical laws which underly the connection between the cause and the effect. to borrow an example from  ga：rdenfors  1   if we want an explanation of why mr. johansson has been taken ill with lung cancer  the information that he worked in asbestos manufacturing for many years is not going to be a satisfactory explanation to someone who does not know anything about the effects of asbestos on people's health. in this case  the causal model  or relevant parts of it  must be part of the explanation. on the other hand  for someone who knows the causal model but does not know that mr. johansson worked in asbestos manufacturing  the explanation would involve mr. johansson's employment but would not mention the causal model.
　our definition of explanation is motivated by the following intuitions. an individual in a given epistemic state asks why holds. what constitutes a good answer to his question  a good answer must provide information that goes beyond and be such that the individual can see that it would  if true  be  or be very likely to be  a cause of . we may also want to require that be true  or at least probable . although our basic definition does not require this  but it is easy to do so.
　to make this precise  we must explain  1  what it means for to be a cause of and  1  how to capture the agent's epistemic state. in  halpern and pearl  1   we dealt with the first question. in the next subsection we review the definitions. the following subsections discuss the second question.
1	the definition of causality
we want to make sense of statements of the form  event is an actual cause of event in context of model  . note that we assume the context and model are given. intuitively  they encode the background knowledge. all the relevant facts are known. the only question is picking out which of them are the causes of	.
　the types of events that we allow as actual causes are ones of the form -that is  conjunctions of primitive events; we typically abbreviate this as . the events that can be caused are arbitrary boolean combinations of primitive events. we argue in  halpern and pearl  1  that it is reasonable to restrict causes to conjunctions  and  in particular  to disallow disjunctions . this restriction seems less reasonable in the case of explanation; we return to this point below. in any case  the definition of causality we give is restricted to conjunctive causes.
definition 1:  actual cause 	is an actual cause of in	if the following three conditions hold:
ac1.	.  that is  both
are true in the actual world. andac1. there exists a partition	of	withand	some setting	of the variables in
	that if	for	  thensuch a  . in words  changing from to changes from true to false;
 b  for all subsets of . in words  setting to should have no effect on as long as is kept at its current value   even if all the variables in an arbitrary subset of are set to their original values in the context .
ac1. is minimal; no subset of satisfies conditions ac1 and ac1. minimality ensures that only those elements of the conjunction that are essential for changing in ac1 a  are considered part of a cause; inessential
elements are pruned. 
for future reference  we say that is a weak cause of in if ac1 and ac1 hold  but not necessarily ac1.
　the core of this definition lies in ac1. informally  the variables in should be thought of as describing the  active causal process  from to . these are the variables that mediate between and . ac1 a  says that there exists a setting of that changes to   as long as the variables not involved in the causal process     take on value . ac1 a  is reminiscent of the traditional counterfactual criterion of lewis 1b  according to which should be false if it were not for being . however  ac1 a  is more permissive than the traditional criterion; it allows the dependence of on to be tested under special circumstances.
　ac1 b  is an attempt to counteract the  permissiveness  of ac1 a  with regard to structural contingencies. essentially  it ensures that alone suffices to bring about the change from to ; setting to merely eliminates spurious side effects that tend to mask the action of . it captures the fact that setting to should not affect the causal process  by requiring that changing from to has no effect on the value of .
　this definition is discussed and defended in much more detail in  halpern and pearl  1   where it is compared to other definitions of causality. in particular  it is shown to avoid a number of problems that have been identified with lewis's account  e.g.  see  pearl  1  chapter 1    such as commitment to transitivity of causes. for the purposes of this paper  we ask that the reader accept the definition. we note that  to some extent  our definition of explanation is modular in its use of causality  in that another definition of causality could be substituted for the one we use in the definition of explanation  provided it was given in the same framework .
　the following example will help to clarify the definition of both causality and explanation.
example 1: suppose that two arsonists drop lit matches in different parts of a dry forest; both cause trees to start burning. consider two scenarios. in the first  called  disjunctive   either match by itself suffices to burn down the whole forest. that is  even if only one match were lit  the forest would burn down. in the second scenario  called  conjunctive   both matches are necessary to burn down the forest; if only one match were lit  the fire would die down before the forest was consumed. we can describe the essential structure of these two scenarios using a causal model with four variables:
an exogenous variable which determines  among other things  the motivation and state of mind of the arsonists. for simplicity  assume that
　　　　　　　　; if   then the first arsonist intends to start a fire iff and the second arsonist intends to start a fire iff . in both scenarios .
endogenous variables ml and ml   each either 1 or 1  where ml if arsonist doesn't drop the match and
	ml	if he does  for	.
an endogenous variable fb for forest burns down  with values 1  it doesn't  and 1  it does .
both scenarios have the same causal network  see figure
1 ; they differ only in the equation for fb.	given
  for the disjunctive scenario we have
	and	;
for the conjunctive scenario we have	and
.
　in general  the causal model for reasoning about forest fires would involve many other variables; in particular  variables for other potential causes of forest fires such as lightning and unattended campfires. here we focus on that part of the causal model that involves forest fires started by arsonists. since for causality we assume that all the relevant facts are given  we can assume here that it is known that there were no unattended campfires and there was no lightning  which makes it safe to ignore that portion of the causal model.
　denote by and the  portion of the  causal models associated with the disjunctive and conjunctive scenarios  respectively. the causal network for the relevant portion of and is described in figure 1.
　despite the differences in the underlying models  it is not hard to show that each of ml and ml is a cause of fb in both scenarios. we present the argument for ml here. to show that ml is a cause in let
u
ml1ml1
fb
	figure 1: the causal network for	and	.
　　　ml fb   so ml . it is easy to see that the contingency ml satisfies the two conditions in ac1. ac1 a  is satisfied because  in the absence of the second arsonist  ml    the first arsonist is necessary and sufficient for the fire to occur fb . ac1 b  is satisfied because  if the first match is lit  ml   the contingencyml does
not prevent the fire from burning the forest. thus  ml is a cause of fb in .  note that we needed to set ml
to 1  contrary to facts  in order to reveal the latent dependence of fb on ml . such a setting constitutes a structural change in the original model  since it involves the removal of some structural equations.  the argument that ml is also a cause of fb in is similar.  again  taking
	ml	fb	and	ml	works. 
　this example also illustrates the need for the minimality condition . for example  if lighting a match qualifies as the cause of fire then lighting a match and sneezing would also pass the tests of ac1 and ac1  and awkwardly qualify as the cause of fire. minimality serves here to strip  sneezing  and other irrelevant  over-specific details from the cause.
　it might be argued that allowing disjunctive causes would be useful in this case to distinguish from as far as causality goes. a purely counterfactual definition of causality would make ml ml a cause of fb in  since  if ml ml were not true  then
fb would not be true   but would make neither ml nor ml individually a cause  for example  if ml were not true in   fb would still be true . clearly  our definition does not enforce this intuition. purely counterfactual definitions of causality have other well-known problems. we do not have a strong intuition as to the best way to deal with disjunction in the context of causality  and believe that disallowing it is reasonably consistent with intuitions. interestingly  as we shall see in section 1  our definition of explanation does distinguish from ; each of ml
and ml is an explanation of fb in under our definition of explanation  but neither is an explanation of fb in . in   the explanation of fb is ml ml : both matches being lit are necessary to explain the forest burning down. 
1	the basic definition of explanation
all that remains to do before giving the definition of explanation is to discuss how to capture the agent's epistemic state in our framework. for ease of exposition  we first consider the case where the causal model is known and the context is uncertain.  the minor modifications required to deal with the general case are described in section 1.  in that case  one way of describing an agent's epistemic state is by simply describing the set of contexts the agent considers possible. this choice is very much in the spirit of the standard  possible worlds  definitions of knowledge and belief.
definition 1:  explanation  given a structural model   is an explanation of relative to a set of contexts
if the following conditions hold:
ex1. for each context .  that is  must hold in all contexts the agent considers possible-the agent considers what she is trying to explain as an established fact 
ex1. is a weak cause of in  that is  ac1 and ac1 hold  but not necessarily ac1  for each such that	.
ex1.	is minimal; no subset of	satisfies ex1.
ex1.	for some	and
　　　　for some .  this just says that the agent considers a context possible where the explanation is false  so the explanation is not known to start with  and considers a context possible where the explanation is true  so that it is not vacuous.  
　our requirement ex1 that the explanation is not known may seem incompatible with linguistic usage. someone discovers some fact and says  aha! that explains why happened.  clearly  is not an explanation of why happened relative to the epistemic state after has been discovered  since at that point is known. however  can legitimately be considered an explanation of relative to the epistemic state before was discovered.
　consider the arsonists in example 1. if the causal model has only arsonists as the cause of the fire  there are two possible explanations in the disjunctive scenario: arsonist 1 did it or arsonist 1 did it  assuming consists of three contexts  where either 1  1  or both set the fire . in the conjunctive scenario  no explanation is necessary  since the agent knows that both arsonists must have lit a match if arson is the only possible cause of the fire  assuming that the agent considers these to be the only possible arsonists .
　perhaps more interesting is to consider a causal model with other possible causes  such as lightning and unattended campfires. since the agent knows that there was a fire  in each of the contexts in   at least one of the potential causes must have actually occurred. if we assume that there is a context where only arsonist 1 lit the fire  and  say  there was lightning  and another where only arsonist 1 lit the fire then  in the conjunctive scenario  ml ml is an explanation of fb   but neither ml nor ml by itself is an explanation  since neither by itself is a cause in all contexts in that satisfy the formula . on the other hand  in the disjunctive scenario  both ml and ml are explanations.
　it is worth noting here that the minimality clause ex1 applies to all contexts. this means that our rough gloss of being an explanation of relative to a set of contexts if is a cause of in each context in where holds is not quite correct. for example  although
ml ml is an explanation of fire in the conjunctive scenario  if includes contexts where there are other possible causes of fire   it is a cause of fire in none of the contexts in which it holds. the minimality condition ac1 would say that each of ml and ml is a cause  but their conjunction is not.
　note that  as for causes  we have disallowed disjunctive explanations. here the motivation is less clear cut. it does make perfect sense to say that the reason that happened is either or  but i don't know which . there are some technical difficulties with disjunctive explanations  which suggest philosophical problems. for example  consider the conjunctive scenario of the arsonist example again. suppose that the structural model is such that the only causes of fire are the arsonists  lightning  and unattended campfires and that consists of contexts where each of these possibilities is the actual cause of the fire. once we allow disjunctive explanations  what is the explanation of fire  one candidate is  either there were two arsonists or there was lightning or there was an unattended campfire  which got out of hand  . but this does not satisfy ex1  since the disjunction is true in every context in . on the other hand  if we do not allow the disjunction of all possible causes  which disjunction should be allowed as an explanation  as a technical matter  how should the minimality condition ex1 be rewritten  we could not see any reasonable way to allow some disjunctions in this case without allowing the disjunction of all causes  which will not in general satisfy ex1 .
　we believe that  in cases where disjunctive explanations seem appropriate  it is best to capture this directly in the causal model by having a variable that represents the disjunction.  essentially the same point is made in  chajewska and halpern  1 .  for example  consider the disjunctive scenario of the arsonist example  where there are other potential causes of the fire. if we want to allow  there was an arsonist  to be an explanation without specifically mentioning who the arsonist is  then it can be easily accomplished by replacing the variables ml and ml in the model by a variable ml which is 1 iff at least one arsonist drops a match. then ml becomes an explanation  without requiring disjunctive explanations.
　why not just add ml to the model rather than using it to replace ml and ml   we have implicitly assumed in our framework that all possible combinations of assignments to the variables are possible  i.e.  there is a structural contingency for any setting of the variables . if we add ml and view it as being logically equivalent to ml ml  that is  ml by definition iff at least one of ml and ml is 1  then  for example  it is logically impossible for there to be a structural contingency where ml   ml   and ml . thus  in the presence of logical dependences  it seems that we need to restrict the set of contingencies that can be considered to those that respect the dependencies. we have not yet considered the implications of such a change for our framework  so we do not pursue the matter here.
1	partial explanations and explanatory power
not all explanations are considered equally good. some explanations are more likely than others. an obvious way to define the  goodness  of an explanation is by bringing probability into the picture. suppose that the agent has a probability on the set of possible contexts. in this case  we can consider the probability of the set of contexts where the explanation is true. for example  if the agent has reason to believe that the first arsonist is extremely unlikely to have caused the fire  perhaps he had defective matches   then the set of contexts where ml holds would have higher probability than those where ml holds. thus  ml would be considered a better explanation of the fire in the disjunctive model than ml .
　but the probability of an explanation is only part of the story; the other part concerns the degree to which an explanation fulfills its role  relative to   in the various contexts considered. this becomes clearer when we consider partial explanations. the following example  taken from  ga：rdenfors  1   is one where partial explanations play a role.
example 1: suppose i see that victoria is tanned and i seek an explanation. suppose that the causal model includes variables for  victoria took a vacation in the canary islands    sunny in the canary islands   and  went to a tanning salon . the set includes contexts for all settings of these variables compatible with victoria being tanned. note that  in particular  there is a context where victoria both went to the canaries  and didn't get tanned there  since it wasn't sunny  and to a tanning salon. ga：rdenfors points out that we normally accept  victoria took a vacation in the canary islands  as a satisfactory explanation of victoria being tanned and  indeed  according to his definition  it is an explanation. victoria taking a vacation is not an explanation  relative to the context   in our framework  since there is a context where victoria went to the canary islands but it was not sunny  and in the actual cause of her tan is the tanning salon  not the vacation.
　for us  the explanation would have to be  victoria went to the canary islands and it was sunny.  in this case we can view  victoria went to the canary islands  as a partial explanation  in a formal sense to be defined below . 
　in example 1 the partial explanation can be extended to a full explanation by adding a conjunct. but not every partial explanation can be extended to a full explanation. roughly speaking  the full explanation may involve exogenous factors  which are not permitted in explanations. assume  for example  that going to a tanning salon was not considered an endogenous variable in our model but  rather  the model simply had an exogenous variable that could make victoria suntanned even in the absence of sun in canary islands. likewise  assume that the weather in canary island was also part of the background context. in this case  we would still consider victoria's vacation to provide a partial explanation of her sun tan  since the context where it fails to be a cause  no sun in the canary island  is fairly unlikely  but we cannot add conjuncts to this event to totally exclude that context from the agent's realm of possibilities.
　the situation actually is quite common  as the following example shows.
example 1: suppose that the sound on a television works but there is no picture. furthermore  the only cause of there being no picture that the agent is aware of is the picture tube being faulty. however  the agent is also aware that there are times when there is no picture even though the picture tube works perfectly well-intuitively   for inexplicable reasons . this is captured by the causal network described in figure 1  where describes whether or not the picture tube is working  1 if it is and 1 if it is not  and describes whether or not there is a picture  1 if there is and 1 if there is not . the ex-
u1
 u1 t
p
figure 1: the television with no picture.
ogenous variable	determines the status of the picture tube:
　　　. the exogenous variable is meant to represent the mysterious  other possible causes . if   then whether or not there is a picture depends solely on the status of the picture tube-that is  . on the other hand  if   then there is no picture     no matter what the status of the picture tube. thus  in contexts where   is not a cause of . now suppose that includes a context where . then it is easy to see that there is no explanation of . the only plausible explanation  that the picture tube is not working  is not a cause of in the context . on the other hand  is a cause of
in all other contexts in satisfying . if the probability of is small  capturing the intuition that it is unlikely that more than one thing goes wrong with a television at once   then we are entitled to view as a quite good partial explanation of . 
these examples motivate the following definition.
definition 1: let be the largest subset of such that is an explanation of relative to .
 it is easy to see that there is a largest such set.  then is a partial explanation of	with goodness
　. thus  the goodness of a partial explanation measures the extent to which it provides an explanation of .1 
　in example 1  if the agent believes that it is sunny in the canary islands with probability .1  that is  the probability that it was sunny given that victoria is suntanned and that she went to the canaries is .1   then victoria going to the canaries is a partial explanation of her being tanned with goodness .1. the relevant set consists of those contexts where it is sunny in the canaries. similarly  in example 1  if the agent believes that the probability of both the picture tube being faulty and the other mysterious causes being operative is .1  then is a partial explanation of with goodness .1  with consisting of all the contexts where  .
　a full explanation is clearly a partial explanation with goodness 1  but we are often satisfied with partial explanations that are not as good  especially if they have high probability  i.e.  if is high . in general  there is a tension between the goodness of an explanation and its probability.
　these ideas also lead to a definition of explanatory power. consider example 1 yet again  and suppose that there is an endogenous random variable corresponding to the presence of oxygen. now if holds in all the contexts that the agent considers possible  then is excluded as an explanation by ex1. but suppose that holds in one context that the agent considers possible  for example  there may be another combustible gas   albeit a very unlikely one. in that case  becomes a very good partial explanation of the fire. nevertheless  it is an explanation with  intuitively  very little explanatory power. how can we make this precise  suppose that there is a probability distribution on a set of contexts larger than that intuitively represents the agent's prior probability before the explanandum is discovered. that is  is the result of conditioning on and consists of the subset of that satisfies .
ga：rdenfors identifies the explanatory power of the  partial  explanation of with  see  chajewska and halpern  1; ga：rdenfors  1  . if this probability is higher than   then the explanation makes more likely. while this explanatory power  we would argue that a better measure of the explanatory power of is
　　　　　　　　　. according to either definition  under reasonable assumptions about   has much lower explanatory power than  say ml . moreover  the two definitions agree in the case that is a full explanation  since then is just   the set of contexts in where is true . the difference between the two definitions arises if there are contexts where and both happen to be true  but is not a cause of . such spurious correlations are exculded by our suggested definition.  see  pearl  1  for some examples showing that considering spurious
correlations leads to bad outcomes. 
　again   partial  explanations with higher explanatory power typically are more refined and  hence  less likely. than explanations with less explanatory power. there is no obvious way to resolve this tension.  see  chajewska and halpern  1  for more discussion of this issue. 
　as this discussion suggests  our definition shares some features with that of ga：rdenfors' 1. like him  we consider explanation relative to an agent's epistemic state. ga：rdenfors also considers a  contracted  epistemic state characterized by the distribution . intuitively  describes the agent's beliefs before discovering .  more accurately  it describes an epistemic state as close as possible to where the agent does not ascribe probability 1 to .  if the agent's current belief in came about as the result of an observation   then we can take to be the result of conditioning on   as we have done above. however  ga：rdenfors does necessarily assume such a connection between and . in any case  for ga：rdenfors  is an explanation of relative to if  1     1    and  1  .  1  is the probabilistic analogue of ex1. clearly   1  is the probabilistic analogue of ex1. finally   1  says that learning the explanation increases the likelihood of . ga：rdenfors focuses on the explanatory power of an explanation  but does not take into account its prior probability. as pointed out in  chajewska and halpern  1   ga：rdenfors' definition suffers from another defect: since there is no minimality requirement like ex1  if is an explanation of   so too is .
　in contrast to ga：rdenfors' definition  the dominant approach to explanation in the ai literature  the maximum a posteriori  map  approach  see  for example   henrion and druzdzel  1; pearl  1; shimony  1    focuses on the probabability of the explanation  given the explanandum
 i.e 	   but does not take explanatory power into account. the map approach is based on the intuition that the best explanation for an observation is the state of the world  in our setting  the context  that is most probable given the evidence. the most probable explanation for is then the context such that .
thus  an explanation is a  complete  context. this means that part of the explanation will include totally irrelevant facts  the agent sneezed . moreover  it is quite sensitive to the description of the context  see  chajewska and halpern  1  for details  and does not directly take causality into account.
　to some extent  these problems can be dealt with by limiting the set of candidate explanations to ancestors  of the explanandum  in the causal network; this also avoids many of the problems associated with non-causal approaches  although it requires there to be a causal network in the background . however  the map approach does not go far enough. one problem is that propositions with extremely high prior probabilities  e.g.  that oxygen is present in the room  will also receive high posterior probabilities  regardless of how relevant they are to the events explained. to remedy this problem  more intricate combinations of the quantities     and have been suggested to quantify the causal relevance of on but  as argued in  pearl  1  p. 1   without taking causality into account  no such combination of parameters can work.
1	the general definition
in general  an agent may be uncertain about the causal model  so an explanation will have to include information about it.  ga：rdenfors 1 and hempel 1 make similar observations . it is relatively straightforward to extend our definition of explanation to accommodate this. now an epistemic state consists not only of contexts  but of pairs consisting of a causal model and a context . call such a pair a situation. intuitively  now an explanation should consist of some causal information  such as  prayers do not cause fires   and the facts that are true. thus  a  general  explanation has the form   where is an arbitrary formula in our causal language and  as before  is a conjunction of primitive events. we think of the component as consisting of some causal information  such as  prayers do not cause fires   which corresponds to a conjunction of statements of the form   where is a random variable describing whether or not prayer takes place . the first component in a general explanation is viewed as restricting the set of causal models. to make this precise  given a causal model   we say is valid in   and write   if for all contexts consistent with . with this background  it is easy to state the general definition.
definition 1:	is an explanation of	relative to a set	of situations if the following conditions hold:
ex1.	for each situation	.
ex1. for all	such that	and
	 	is a weak cause of	in	.
ex1.	is minimal; there is no pair
satisfying ex1 such that
 
	where	for some	 
　  and is the restriction of to the variables in . roughly speaking  this says that no subset of provides a weak cause of in more contexts than those where is valid.
ex1. for some and for some	. 
note that  in ex1  we now restrict to situations that satisfy both parts of the explanation   in that and . furthermore  although both components of an explanation are formulas in our causal language  they play very different roles. the first component serves to restrict the set of causal models considered  to those with the appropriate structure ; the second describes a cause of in the resulting set of situations.
　clearly definition 1 is the special case of definition 1 where there is no uncertainty about the causal structure  i.e.  there is some such that if   then  .
in this case  it is clear that we can take in the explanation to be true.
　definition 1 can also be extended to deal naturally with statistical information of the kind considered by ga：rdenfors and hempel. let a probabilistic causal model be a tuple
                 where is a causal model and is a probability measure on the contexts defined by signature of . information like  with probability .1    is a restriction on probabilistic models  and thus can be captured using a formula in an appropriate extension of our language that allows such probabilistic reasoning. with this extended language  the definition of explanation using probabilistic causal models remains unchanged.
　as an orthogonal issue  there is also no difficulty considering a probability on the set of situations and defining partial explanation just as before.
example 1: using this general definition of explanation  consider scriven's 1 famous paresis example. paresis develops only in patients who have been syphilitic for a long time  but only a small number of patients who are syphilitic in fact develop paresis. furthermore  according to scriven  no other factor is known to be relevant in the development of paresis.1 this description is captured by a simple causal model . there are two endogenous variables   for syphilis  and  for paresis   and two exogenous variables 
　  the background factors that determine   and   which intuitively represents  disposition to paresis   i.e.  the factors that determine  in conjunction with syphilis  whether or not paresis actually develops. an agent who knows this causal model and that a patient has paresis does not need an explanation of why: the agent knows without being told that the patient must have syphilis and that . on the other hand  for an agent who does not know the causal model  i.e.  considers a number of causal models of paresis possible   is an explanation of paresis. 
1	discussion
we have given a formal definition of explanation in terms of causality. as we mentioned earlier  there are not too many formal definitions of explanation in terms of causality in the literature. one of the few exceptions is lewis 1a  who defends the thesis that  to explain an event is to provide some information about its causal history . while this view is compatible with our definition  there is no formal definition given to allow for a careful comparison between the approaches. in any case  if were to define causal history in terms of lewis's 1b definition of causality  we would inherit all the problems of that definition. as we said earlier  our definition avoids these problems.
　so what are the problems with our definition  for one thing  it inherits whatever problems our definition of causality has. as observed in  halpern and pearl  1   our definition at times declares certain events to be causes  and hence candidate explanations  that  intuitively  should not be causes because they should fail ac1 a . the only reason that they do not fail ac1 a  is because of extremely unlikely structural contingencies. to some extent  we can avoid this problem by simply ignoring structural contingencies that are extremely unlikely  this is essentially the solution suggested in  halpern and pearl  1  in the context of causality . of course  we can do this in the context of explanation too. another possibility is to take the probability of the structural contingency into account more directly when computing the probability of the explanation. we are currently exploring this option.
　we have mentioned the other significant problem of the definition already: dealing with disjunctive explanations. disjunctions cause problems in the definition of causality  which is why we do not deal with them in the context of explanation. as we pointed out earlier  it may be possible to modify the definition of causality so as to be able to deal with conjunctions without changing the structure of our definition of explanation. we are currently exploring this.
　finally  our definition gives no tools for dealing with the inherent tension between explanatory power  goodness of partial beliefs  and the probability of the explanation. clearly this is an area that requires further work.
acknowledgments:
thanks to riccardo pucella and vicky weissman for useful comments.
references
 chajewska and halpern  1  u. chajewska and j. y. halpern. defining explanation in probabilistic systems. in proc. uai '1  pages 1  1.
 galles and pearl  1  d. galles and j. pearl. axioms of causal relevance. artificial intelligence  1-1 :1  1.
 ga：rdenfors  1  p. ga：rdenfors. knowledge in flux. mit press  1.
 halpern  1  j. y. halpern. axiomatizing causal reasoning. journal of a.i. research  pages 1  1.
 halpern and pearl  1  j. y. halpern and j. pearl. causes and explanations: a structuralmodel approach- part i: causes. available at http://www.cs.cornell.edu/home/halpern  1.
 hempel  1  c. g. hempel. aspects of scientific explanation. free press  1.
 henrion and druzdzel  1  m. henrion and m. j. druzdzel. qualitative propagation and scenario-based approaches to explanation of probabilistic reasoning. in uncertainty in artificial intelligence 1  elsevier science  pages 1  1.
 lewis  1a  d. lewis. causal explanation. in philosophical papers  volume ii  pages 1. oxford university press  1.
 lewis  1b  d. lewis. causation. in philosophical papers  volume ii  pages 1. oxford university press  1. the original version of this paper  without numerous postscripts  appeared in the journal of philosophy 1  1  pp. 1.
 pearl  1  j. pearl. probabilistic reasoning in intelligent systems. morgan kaufmann  1.
 pearl  1  j. pearl. causality: models  reasining  and inference. cambridge university press  1.
 salmon  1  w. c. salmon. four decades of scientific explanation. university of minnesota press  1.
 scriven  1  m. j. scriven. explanation and prediction in evolutionary theory. science  1-1  1.
 shimony  1  s. e. shimony. explanation  irrelevance and statistical independence. in proc. aaai '1  pages 1  1.
 woodward  1  j. woodward. explanation. in the blackwell guide to the philosophy of science. basil blackwell  1. to appear.
complexity results for structure-based causality
thomas eiter and thomas lukasiewicz
institut und ludwig wittgenstein labor fu：r informationssysteme  tu wien
favoritenstra e 1  a-1 wien  austria eiter lukasiewicz  kr.tuwien.ac.atabstract
we analyze the computational complexity of causal relationships in pearl's structural models  where we focus on causality between variables  event causality  and probabilistic causality. in particular  we analyze the complexity of the sophisticated notions of weak and actual causality by halpern and pearl. in the course of this  we also prove an open conjecture by halpern and pearl  and establish other semantic results. to our knowledge  no complexity aspects of causal relationships have been considered so far  and our results shed light on this issue.
1	introduction
representing and reasoning with causal knowledge has received much attention in the recent decade. the existing approaches to causality in the ai literature can be roughly divided into those that have been developed as modal nonmonotonic logics  especially in the context of logic programming  and those that evolved from the area of bayesian networks.
　a representative of the former is geffner's modal nonmonotonic logic for handling causal knowledge 1; 1   which has been inspired by default reasoning from conditional knowledge bases. other more specialized formalisms play an important role in dealing with causal knowledge about actions and change; see especially the work by turner  and the references therein for an overview.
　a representative of the latter is pearl's approach to modeling causality by structural equations  balke and pearl  1; galles and pearl  1; pearl  1; 1   which is central to a number of recent research efforts. in particular  the evaluation of deterministic and probabilistic counterfactuals has been explored  which is at the core of problems in fault diagnosis  planning  decision making  and determination of liability  balke and pearl  1 .
　in a recent paper  halpern  gave an axiomatization of reasoning about causal formulas in the structural-model approach  and explored its computational aspects.
　it has been shown that the structural-model approach allows a precise modeling of many important causal relationships  which can especially be used in natural language processing  galles and pearl  1 . in particular  it allows an elegant definition of the important notions of actual causation and explanation  halpern and pearl  1; 1 .
　we give a simple example due to halpern and pearl   which illustrates the structural-model approach.
example 1 suppose that two arsonists lit matches in different parts of a dry forest  and both cause trees to start burning. assume now that either match by itself suffices to burn down the whole forest. in the structural-model framework  such a scenario may be modeled as follows. we assume two binary background variables and   which determine the motivation and the state of mind of the two arsonists  where
has the value 1 iff the arsonist intends to start a fire. moreover  we have three binary variables     and   which describe the observable situation  where has the value 1 iff the arsonist drops the match  and has the value 1 iff the whole forest burns down. the causal dependencies between these variables are expressed through functions  which say that the value of is given by the value of   and that has the value iff either or has the value 1. these dependencies can be graphically represented as in fig. 1.

figure 1: causal graph
　while the semantic aspects of causal relationships in the structural-model approach have been explored in depth  see especially the work by pearl    studies about their computational properties are missing so far. in this paper  we try to fill this gap by giving a precise account of the complexity of deciding causal relationships in structural models.
　note that halpern's work  is orthogonal to ours  as it focuses on the computational aspects of deciding whether a given causal formula has a causal model  while our work in this paper deals with the complexity of deciding whether a given causal relationship holds in a given causal model.
the main contributions can be summarized as follows:
   we analyze the complexity of deciding causal relationships between variables in structural causal models. we consider the notions of causal irrelevance  cause  cause in a context  direct cause  and indirect cause. it turns out that testing these notions has a complexity among	 	-	  and	.
hardness holds even in restricted cases.
   we analyze the complexity of deciding causal relationships between events. we consider the notions of necessary and possible causality  and the sophisticated notions of weak and actual causality by halpern and pearl . in particular  checking the latter is shown to be -complete in general  and -complete in the case of binary variables.
   we prove some semantic results related to the notions of actual and weak causality. more precisely  we prove an open conjecture by halpern and pearl   and we give a new characterization of weak causality for the binary case.
   as a representative for probabilistic causal relationships  we finally analyze the complexity of deciding the notion of probabilistic causal irrelevance  which is shown to be complete for the class c   and thus harder than - . note that few c -complete problems  and none in ai  are known.
　our results draw a clear picture of the complexity of structural causality  and give useful insight for exploiting it  e.g.  in counterfactual reasoning  see section 1 .
　for space reasons  we give only proof sketches of some results. proofs of all results are given in the extended paper  eiter and lukasiewicz  1 .
1	preliminaries
we assume a finite set of random variables. each variable may take on values from a finite domain . a value for a set of variables is a mapping such that
 where the empty mapping	is the unique value for	 . the domain of	  denoted	  is the set of all values for	.	for	and	  denote by	the restriction of	to	. for sets of variables	and values and	  we use	to denote the union of
and . as usual  we often identify singletons with   and their values with .
1	causal and probabilistic causal models
a causal model is a triple   where is a finite set of exogenous variables  is a finite set of endogenous variables such that   and is a set of functions that assign a value to for each value of the parents of .
　we focus here on the principal class of recursive causal models  as argued in  halpern and pearl 
1   we do not lose much generality by concentrating on recursive causal models  in which a total ordering on exists such that implies   for all . in such
models  every assignment to the exogenous variables determines a unique value for every set of endogenous variables   denoted  or simply  . in the sequel  is reserved for denoting a recursive causal model.
　for causal models     and   the causal model   where
  is a submodel
of . intuitively  is set to by an  external action . we abbreviate and by and   respectively. for   we abbreviate by .
　a probabilistic causal model consists of a causal model and a probability function on .
example 1 in our introductory example  the causal model is given by	 	  and	  where	 	  and iff	or	 fig. 1 shows the parent relationship between the variables . in a probabilistic causal model   we may use the uniform distribution	over	.
1	model representation for computation
we assume the following representation of causal models and probabilistic causal models  see the full paper for a discussion of these assumptions :
	the domain	of each variable
i.e. 	is enumerated.is explicit    each function	  putable in polynomial time.
	is given by a pair	  where  is com-is a polynomial-time computable function andis an in-teger  such that	for every
the following proposition is immediate..proposition 1 for every	and  the val-ues and   given   are computable in polynomial time.
　we say  resp.    is binary  if for all . furthermore   resp.    is bounded  if holds for each   i.e.  has at most parents  where is an arbitrary but fixed constant.
　note that in a bounded model  as for polynomiality  each is w.l.o.g. given by a table that lists the output values for all  which is similar to a conditional probability table in bayesian networks .
1	complexity classes
complexity classes that we encounter are shown in fig. 1  where arrows denote containment.     -     and are from the polynomial hierarchy  ph ;
	-	is the  conjunction  of	and	-	.
the class c is from the counting hierarchy  ch  of complexity classes  tora＞n  1 . informally  c contains all problems which can be expressed as deciding whether a given instance has at least many polynomial size  proofs   ...  that is a yes-instance  for some function   where computing and checking each proof is polynomial. the class c coincides with the famous class  probabilistic    papadimitriou  1   which contains the problems decidable by a polynomial-time turing machine which accepts an input iff the majority of its runs halt in an accepting state.

figure 1: containment between complexity classes
　the class c is a variant of c  where  exactly   replaces  at least  . while this difference seems marginal  c and c have quite different properties  tora＞n  1 . intuitively  c is an extension of -   and has many properties of this class. c and c are contained in pspace  and it is believed that they are not contained in ph.
1	causality between variables
we first focus on causal relationships between variables due to galles and pearl ; see also  pearl  1; 1 .
1	definitions
in the sequel  we assume a causal model and sets of variables with . we say
	is causally irrelevant to	given	iff for every
                         and   we have .
	is a cause of	iff values	and
exist such that	.
is a cause of in the context iff values and exist with .
	is a direct cause of	iff values	 
	  and	exist with	.
	is an indirect cause of	iff	is a cause of	and
is not a direct cause of	.
　we give some examples of such causal relationships. for a more detailed discussion of these concepts see  galles and pearl  1  and  pearl  1; 1 .
example 1 in our running example  is not causally irrelevant to . for instance  if we set to 1 and to 1 and 1  then has the values 1 and 1  respectively. informally the actions of arsonist 1 are not causally irrelevant to the state of the forest. in fact  is a cause of   but not a cause of in the context . informally  the actions of arsonist 1 are in general a cause of the state of the forest  but not when arsonist 1 starts a fire. finally  it is easy to verify that is in fact a direct cause of .
1	results
our complexity results for checking the above notions of causality are summarized in table 1. it is important to point out that for all these causal relationships  hardness holds even if is binary and bounded  and is a singleton.
table 1: complexity of causality between variables
is causally irrelevant to	given-	-completeis a cause of-completeis a cause of	in the context-completeis a direct cause of-completeis an indirect cause of-completeour first result shows that deciding causal irrelevance is - -complete. we sketch the main ideas of its proof.
theorem 1 given	and	with
         deciding whether is causally irrelevant to given is - -complete. hardness holds even if  1  is binary and bounded   1  is empty   1  is a singleton  and
 1a 	is a singleton or  1b 	.
proof  sketch . the problem is in	-	  as
　　　and values	 	 	  and such that	can be guessed and verified in polynomial time  see proposition 1 .
　we show - -hardness by a polynomial transformation from the - -complete problem of deciding whether a given propositional formula in 1dnf on the atoms   where w.l.o.g. and   is a tautology.
　we now construct	and	such that	is causally irrelevant to	given	iff	is a tautology. we define	 	  and	. moreover  and	. let all variables have the domain	  where 1 and 1 are identified with the truth values	and	  respectively. the values	and	of	are denoted by	and	  respectively. the functions	for	are as follows: and	 
	for all	 
.
　it can now be shown that is causally irrelevant to given iff is a tautology. informally  under any   if is set to   then becomes 1. whereas  if is set to   then is the truth value of under . that is  setting to and yields the same value of under any iff is a tautology. notice that assignments to some nonempty always yield the same value of .
　note that  1 - 1  and  1a  are satisfied. the proof of hardness in the case where  1 - 1  and  1b  holds is similar.
　the following result shows that deciding causes and causes in a contextis np-complete. here  np-hardness can be shown by a reduction from the np-complete problem of deciding whether a propositional formula is not a tautology.
theorem 1 given	and	 resp.  and	  with	  deciding whether is a cause of	 resp. 	is a cause of	in the context
       is -complete. hardness holds even if  1  is binary and bounded  and  1  and are singletons.
1	event causality
we now focus on causality between events. in particular  we consider causal relationships between events due to galles and pearl ; see also  pearl  1; 1   and the notions of weak and actual causality by halpern and pearl   which are inspired by pearl's causal beams .
1	definitions
a primitive event is an expression of the form   where is a variable and is a value for . the set of events is the closure of the set of primitive events under the boolean operations and  that is  every primitive event is an event  and if and are events  then also and  .
　the truth of an event	in a causal model under	  denoted	  is inductively given by: iff	 
iff does not hold  iff and .
we write	to abbreviate	.	for	and
  we write	to abbreviate	. for with	and	  we use
	to abbreviate	.
the following proposition is immediate.
proposition 1 let and . given and an event   deciding whether and  given   hold can be done in polynomial time.
　we are now ready to define causal relationships between events. we first define the notions of necessary and possible causality  which are slightly more general than in  galles and pearl  1  . let be a causal model.
let and . let be an event. we say always causes iff  i  for all   and
 ii  some	 	exist with	and	. may have caused	iff  i 	and	are observed  which implies that	and	for some
　　　　  and  ii  some	and	exist such that	 	 	  and	.
　we next define weak and actual causality. we say is a weak cause of under iff the following conditions hold:
ac1.	and	.
ac1. some set of variables and some values  and exist such that:
 a  	 
 b  for all	and	.
we say is an actual cause of under iff additionally the following condition is satisfied:
ac1. is minimal. that is  no proper subset of satisfies both ac1 and ac1.
we give some illustrative examples.
example 1 in our running example  each event among
	 	  and	always causes	.
for instance  let us show that	always causes	:
 i  if	is set to 1  then	has the value 1 under every
　　　　  and  ii  if is set to 1  and to 1  then has the value 1. informally  at least one arsonist starting a fire always has the effect that the whole forest burns down.
　consider now the background context in which both arsonists intend to start a fire. then  each among  
　　　  and is a weak cause of . for instance  let us show that is a weak cause of :  ac1  both and have the value 1 under    ac1  a   if both and are set to 1  then has the value 1  and  ac1  b   if is set to 1 and to 1  then has the value 1.
in fact  and are actual causes of   while is not an actual cause of .
1	results
our complexity results for the above causal relationships between events are summarized in table 1. we distinguish between the general and the binary case  in which we assume a syntactic restriction to binary causal models. we remark that for all these causal relationships between events  hardness holds even if is bounded and is a singleton.
table 1: complexity of event causality
problemgeneral casebinary casealways causes-complete-completemay cause-complete-completeis a weak cause of-complete-completeis an actual cause of-complete-complete　the following theorem shows that deciding weak causality is -complete in the general case. we sketch the main ideas behind the technically quite involved proof of this result.
theorem 1 given	 	 	 
　　　　  and an event   deciding whether is a weak cause of under is -complete. hardness holds even if
	is a singleton  and either	is bounded or	is primitive.
proof  sketch . as for membership in   recall that is a weak cause of under iff ac1 and ac1 hold. by proposition 1  verifying and is polynomial. moreover  some     and as in ac1 can be guessed and verified with an np-oracle  needed for  b   in polynomial time. in summary  checking ac1 and ac1 is in .
     -hardness is shown by a reduction from evaluating a quantified boolean formula   where is a propo-
sitional formula on the variables	and
               . we now define   etc. as stated such that is a weak cause of under iff is true  deciding this is -complete  papadimitriou  1  .
	we define	and	. the exogenous and
endogenous variables are	and
	  respectively. define	for all	 
and	for all	. we define
where	is obtained from	by replacing each	by  	 . the functions	with	are defined by: for all	  for all	  and	iff	is true.
　let and . let be . it can now be shown that is a weak cause of under iff
is true. more precisely  ac1 is trivially satisfied  and ac1 holds iff is true. roughly speaking  the existential and universal quantification over and in is reflected by the variables in and   respectively. we then especially have to ensure that  i  and  ii  no truth assignment to the variables in is ignored. in detail  to make false in ac1  a   must be set to and all must have the value . whereas  to make true in ac1  b   all must have a value from . this already ensures  i . since is set to 1 in ac1  b   and for all   each variable in has the value in ac1  b . as every has the value 1 in   this then ensures  ii .
　note that is a singleton and that is primitive. to show -hardness for a singleton and bounded   define	. then  is a weak cause of under iff evaluates to true.
　the just sketched proof of -hardness makes use of nonbinary causal models. thus  we may ask whether deciding weak causality in the binary case has a lower complexity.
　in fact  the following semantic result shows that in the binary case  ac1 can be expressed in a different way.
theorem 1 let	be binary. let	and
       . let be an event. then  is a weak cause of under iff ac1 holds  and  ac1'  some  
	  and	exist such that  a 		 
 b 	  and  c 	for	.
proof  sketch . notice that ac1'  a  is identical to ac1  a . moreover  ac1'  b  can be replaced by ac1  b   as ac1'  c  implies that setting to is immaterial in of ac1  b . thus  it is now sufficient to show that for binary   we can add ac1'  c  to ac1  a  and  b .
　roughly speaking  we can additionally satisfy ac1'  c  by iteratively moving variables from to the
-part in ac1  a  and  b . more precisely  any singleton with can be moved to the
 -part assigning them  . this is always feasible for  . if is binary  this is also feasible for
    which then implies  . this construction can now be iterated until ac1'  c  holds.
　based on this result  it can now be shown that deciding weak causality in the binary case is np-complete. this is more formally expressed by the following theorem.
theorem 1 given a binary	 	 
         and an event   deciding whether is a weak cause of under is -complete. hardness holds even if is bounded  is a singleton  and is primitive.
　we next focus on the problem of deciding actual causality. we first prove a conjecture by halpern and pearl   which says that only primitive events can be actual causes. note that the proof also goes through for their setting of possibly infinite domains and sets of endogenous variables.
theorem 1 let be a causal model. let and . let be an event. if is an actual cause of under   then is a singleton.
proof  sketch . we give a proof by contradiction. let be an actual cause of under . that is  ac1-ac1 hold. in particular  ac1  a  and  b  hold for some and some  and . suppose that is not a singleton. we consider two cases:
	case 1: there exists some nonempty	 	 
such that		for all	  where
	 	  	  and	. informally 
we can then move all variables in	to the	-part of
ac1 assigning them  . that is  set in ac1 	and 	where	is the respective	-part. then 	is another weak cause of	under	  which is smaller than	. this is a contradiction.
　case 1: for every nonempty     there exists some such that    where
            	  	  and	. informally  we can then take any such	and move the variables in	and	to the	-part of ac1 assigning them values and	.	that is  set in ac1
and	where	is the respective
　-part. then  is a weak cause of under  note that each instance of ac1 b  for amounts to an instance of ac1 b  for  . this is a contradiction.
　hence  is an actual cause of under iff  i  is a weak cause of under   and  ii  is primitive.
as an immediate corollary of theorems 1 and 1  it thus follows that deciding whether is an actual cause of under is -complete in the general case.
corollary 1 given	 	 	 
　　　　  and an event   deciding whether is an actual cause of under is -complete. hardness holds even if is a singleton  and is bounded or is primitive.
　furthermore  as an immediate corollary of theorems 1 and 1  we obtain that deciding whether is an actual cause of under is -complete in the binary case.
corollary 1 given a binary	 	 
         and an event   deciding whether is an actual cause of under is -complete. hardness holds even if is bounded  is a singleton  and is primitive.
1	probabilistic causality
as a representative of probabilistic causal relationships  we finally consider the notion of probabilistic causal irrelevance by galles and pearl ; see also  pearl  1; 1 .
	a counterfactual formula is an expression	  where
　　　　    and . given a probabilistic causal model   where   the probability of   denoted   is the sum of all
such that . for with   we say is probabilistically causally irrelevant to given   denoted   iff for all     and   it holds that . intuitively  once the value of is fixed at   the value of does not affect the probability of .
　the following theorem shows that deciding probabilistic causal irrelevance is complete for c .
theorem 1 given a probabilistic causal model   where   and   deciding whether is complete for c .
　theorem 1 is nontrivial and needs some explanations. firstly  the result means that  in a sense  testing probabilistic causal irrelevance is harder than -   and thus not polynomially reducible to sat-testing. moreover  it cannot be reduced to any problem solver for problems in the polynomial hierarchy. on the other hand  the problem is  easier  than
   -complete problems  which could perhaps help in finding polynomial time  randomized  approximation algorithms.
　the easier part of this result is the lower bound. hardness for c can be proved by a reduction from the following c -complete problem halfsat: given a propositional formula in 1dnf on the variables   decide whether exactly half of the assignments to satisfy . the construction is based on ideas in the proof of theorem 1  but more involved. in fact  it establishes hardness for the case where is binary and bounded  is empty  are singletons  and is the uniform distribution. thus  the problem shows its full complexity already in a minimalistic setting.
　the more difficult part is membership in c . recall that iff	for all values       and . checking that   for given       and   can be reduced to the following problem equalrun  which is in c : given two np turing machines and and an input string   decide whether and
have the same number of accepting runs on	.
　indeed  let be given by as described in section 1. let the np turing machines and have input and nondeterministically generate all . then  for each   let and generate paths. on each of these paths   resp.    computes deterministically
 resp.     and accepts if this value coincides with   otherwise it rejects. then 	iff and have the same numbers of accepting paths.
　obviously  and can be constructed in polynomial time from and . however  we actually need to test that for all values       and . what we obtain  by slight adaptations  is that the problem is in the class c   which is a generalization of c similar to for - : deciding whether is a yes-instance can be informally expressed as deciding whether for every polynomial size string   it holds that is a yes-instance of the same problem in c . since c is equal to c  cf.  green  1    this reduction actually proves membership in c .
1	conclusion and outlook
we analyzed the complexity of causal relationships in pearl's structural models. in particular  we considered causality between variables  event causality  and probabilistic causality. it turned out that all discussed notions of causality are intractable  where the sophisticated notions of weak and actual causality  and the notion of probabilistic causal irrelevance have the highest complexity   and c   respectively .
　our results give useful insight  and may be exploited e.g. in evaluating probabilistic counterfactuals as defined in  balke and pearl  1 . note that the evaluation of probabilistic counterfactuals can be polynomially reduced to standard inference tasks in bayesian networks  and thus has similar computational properties. it is easy to see that the complexity of computing conditional probabilities in bayesian networks  which is complete for  roth  1   carries over to computing probabilities of counterfactual statements. similar to independencies  pearl  1   deterministic and probabilistic causal relationships might now be used to simplify the evaluation of probabilistic counterfactuals. by our results  this seems reasonable  as the complexity of testing simple causal relationships  at most   c   is much lower than the complexity of evaluating probabilistic counterfactuals   -hard . moreover  our results may help to analyze the computational aspects of explanations and partial explanations as introduced by halpern and pearl   which are crucially based on the notions of weak and actual causality.
　an interesting topic of future research is to explore whether there are restricted cases in which testing causal relationships in the structural-model approach is tractable. for example  probabilistic causal irrelevance in stable causal models  galles and pearl  1  can be tested in polynomial time as it coincides with path interception in their causal graphs.
acknowledgments
this work has been partially supported by the austrian science fund project n z1-inf and by a dfg grant. we are very grateful to jacobo tora＞n for useful comments related to counting classes and for pointing out relevant literature. many thanks also to the reviewers for their useful comments.
references
 balke and pearl  1  a. balke and j. pearl. probabilistic evaluation of counterfactual queries. in proceedings aaai-1  pages 1  1.
 eiter and lukasiewicz  1  t. eiter and t. lukasiewicz. complexity results for structure-based causality. technical report infsys rr-1-1  institut fu：r informationssysteme  tu wien  1.
 galles and pearl  1  d. galles and j. pearl. axioms of causal relevance. artif. intell.  1-1  1.
 geffner  1  h. geffner. causal theories for nonmonotonic reasoning. in proc. aaai-1  pages 1  1.  geffner  1  h. geffner. default reasoning: causal and conditional theories. mit press  1.
 green  1  f. green. on the power of deterministic reductions to . math. syst. theory  1 :1  1.
 halpern and pearl  1  j. y. halpern and j. pearl. causes and explanations: a structural-model approach. technical report r-1  ucla cognitive systems lab  1.
 halpern and pearl  1  j. y. halpern and j. pearl. causes and explanations: a structural-model approach - part ii: explanation. in proceedings ijcai-1  1.
 halpern  1  j. y. halpern. axiomatizing causal reasoning. j. artif. intell. res.  1-1  1.  papadimitriou  1  c. h. papadimitriou. computational complexity. addison-wesley  reading  ma  1.  pearl  1  j. pearl. reasoning with cause and effect. in proceedings ijcai-1  pages 1  1.  pearl  1  j. pearl. causality: models  reasoning  and inference. cambridge university press  1.
 roth  1  d. roth. on the hardness of approximate reasoning. artif. intell.  1-1 :1  1.
 tora＞n  1  j. tora＞n. complexity classes defined by counting quantifiers. j. acm  1 :1  1.
 turner  1  h. turner. a logic of universal causation. artif. intell.  1-1  1.

knowledge representation
and reasoning
spatial reasoning

ambiguity-directed sampling for qualitative analysis of sparse data from spatially-distributed physical systems

chris bailey-kellogg dartmouth computer science dept.
1 sudikoff laboratory
hanover  nh 1 cbk cs.dartmouth.edu naren ramakrishnan
virginia tech dept. of computer science
1 mcbryde hall
virginia tech  va 1 naren cs.vt.edu

abstract
a number of important scientific and engineering applications  such as fluid dynamics simulation and aircraft design  require analysis of spatiallydistributed data from expensive experiments and complex simulations. in such data-scarce applications  it is advantageous to use models of given sparse data to identify promising regions for additional data collection. this paper presents a principled mechanism for applying domain-specific knowledge to design focused sampling strategies. in particular  our approach uses ambiguities identified in a multi-level qualitative analysis of sparse data to guide iterative data collection. two case studies demonstrate that this approach leads to highly effective sampling decisions that are also explainable in terms of problem structures and domain knowledge.
1	introduction
a number of important scientific and engineering applications  such as fluid dynamics simulation and aircraft design  require qualitative analysis of spatially-distributed data from expensive experiments and/or complex simulations demanding days  weeks  or even years on petaflops-class computing systems. for example  fig. 1 shows a cross-section of the design space for a multidisciplinary aircraft design problem involving 1 design variables with 1 constraints in a highly non-convex design space  knill et al.  1 . frequently  the designer will change some aspect of a nominal design point  and run a simulation to see how the change affects the objective function and various constraints dealing with aircraft geometry and performance/aerodynamics. this approach is inadequate for exploring such large high-dimensional design spaces  even at low fidelity. ideally  the design engineer would like a high-level mining system to identify the pockets that contain good designs and which merit further consideration; traditional tools from optimization and approximation theory can then be applied to fine-tune such preliminary analyses.
　two important characteristics distinguish these applications. first  they must deal not with an abundance of data  but rather with a scarcity of data  owing to the cost and time

figure 1: a pocket in an aircraft design space viewed as a slice through three design points  courtesy layne t. watson .
involved in conducting simulations. second  and more importantly  the computational scientist has complete control over the data acquisition process  e.g. regions of the design space where data can be collected   especially via computer simulations. it is natural therefore to focus data collection so as to maximize information content  minimize the number and expense of samples  and so forth.
　this paper presents a principled mechanism for applying domain-specific knowledge to focus sampling strategies for data-scarce applications. in particular  ambiguities identified by a multi-level qualitative analysis of data collected in one iteration guide succeeding iterations of data collection so as to improve the qualitative analysis. this approach leads to highly effective sampling decisions that are explainable in terms of problem structures and domain knowledge. we demonstrate the effectiveness of our approach by two case studies:  1  identification of pockets in n-dimensional space  and  1  decomposition of a field based on control influences.
1	qualitative analysis of spatially-distributed physical systems
the mechanism we develop for ambiguity-directed sampling is based on the spatial aggregation language  sal   bailey-
abstract description

input field
figure 1: sal multi-layer spatial aggregates  uncovered by a uniform vocabulary of operators utilizing domain knowledge.
kellogg et al.  1; yip and zhao  1   which supports construction of data interpretation and control design applications for spatially-distributed physical systems. sal programs uncover and manipulate multi-layer geometric and topological structures in spatially distributed data by using a small number of uniform operators and data types  parameterized by domain-specific knowledge. these operators and data types mediate increasingly abstract descriptions of the input data  as shown in fig. 1. they utilize knowledge of physical properties such as continuity and locality  based on specified metrics  adjacency relations  and equivalence predicates  to uncover regions of uniformity in spatially distributed data.
　as an example  see fig. 1   consider a sal program for bundling the vectors in a given vector field  e.g. wind velocity or temperature gradient  into a set of streamlines  paths through the field following the vector directions :
1. aggregate vectors into a neighborhood graph  say 1adjacency   localizing computation.
1. filter edges in the graph  ensuring edge direction is similar enough to vector direction.
1. cluster into equivalence classes neighboring vectors whose directions match best.
1. redescribe equivalence classes of vectors into more abstract streamline curves.
　in a second level of abstraction  streamlines are aggregated and classified into groups with similar flow behavior

	 c 	 d 
figure 1: example steps in sal vector field analysis.  a  input vector field.  b  filtered neighborhood graph.  c  equivalence classes  make a choice at each fork edge  redescribed as streamline curves.  d  higher-level aggregation and classification of curves whose flows converge.
 fig. 1 d    using the exact same operators but with different metrics. as this example illustrates  sal provides a vocabulary for expressing the knowledge required - distance metrics  similarity metrics  etc. - for uncovering multi-level structures in spatial data sets. it has been applied to applications ranging from decentralized control design  baileykellogg and zhao  1; 1  to analysis of diffusionreaction morphogenesis  ordo＞nez and zhao  1   .
1	ambiguity-directed sampling
we extend sal for data-scarce  rather than data-rich  applications  by focusing data collection in areas that will yield information most useful in discriminating among possible models. given a set of possible sal models m = {m1 m1 ... mn} for the data  we want to choose a new sample s to help discriminate among posterior probabilities p mi |s . for instance  in the vector-bundling example  fig. 1   models would represent different choices of how to group vectors into streamlines. by bayes rule  we need to evaluate p s|mi  and p mi . the domain knowledge used to enumerate the possible sal structures also places priors p mi  on the identified models. in the vector-bundling example  a possible streamline can be scored based on how well its curvature matches the directions of the vectors it aggregates. additional domain knowledge then characterizes the dependence of potential sample values on different models  thus helping to optimize the next sample location. as we will show later in this section  one useful form of such dependence relates to addressing ambiguity. for example  the best aggregation for ambiguous streamlines can be determined by sampling the
interpolate : field 〜 new objects 〜 surrogate ★ new values
determine values for new objects based on values for nearby objects in field  according to surrogate function.
classify : objects 〜 equiv predicate ★ classes 〜 ambiguities
apply predicate to neighboring objects  partitioning them into equivalence classes and left-over ambiguous objects. predicate is a function taking a pair of neighbors and returning one of {true  false  ambiguous}.
sample : objects 〜 ambiguities 〜 objective fn ★ new objects
determine new objects to be sampled based on optimization of an objective function indicating information gain with respect to the ambiguities.table 1: ambiguity-directed sampling operators.
flow near streamline  branch points. 
　tab. 1 summarizes the incorporation of domain knowledge in new sal operators by our ambiguity-directed sampling framework. the data interpretation and sampling process proceeds as follows  starting from some initial sparse data.  1  derive qualitative sal structures from either the sparse data  or a dense dataset interpolated with a surrogate function.  1  identify ambiguities arising in the structure formation process.  1  target a sample point that will optimize the information gain with respect to these ambiguities.  1  update the data set and repeat  as long as information gain is substantial enough. the following subsections describe the key parts of this approach in more detail.
1	interpolation with a surrogate function
in some cases it is advantageous to generate a dense dataset and find structures in it  rather than to work directly from sparse data. for example  when possible models have a known  common structure  e.g. they can be treated as locally smooth quadratic functions   then interpolating dense data can simplify structure and ambiguity identification. the interpolate operator in tab. 1 generates such dense data  according to a given surrogate representation.
　the choice of surrogate representation is constrained by the local nature of sal computations. for example  global  least-squares type approximations are inappropriate since measurements at all locations are equally considered to uncover trends and patterns in a particular region. we advocate the use of kriging-type interpolators  sacks et al.  1   which are local modeling methods with roots in bayesian statistics. kriging can handle situations with multiple local extrema  for example  in weather data  remote sensing data  etc.  and can easily exploit anisotropies and trends. given k observations  the interpolated model gives exact responses at these k sites and estimates values at other sites by minimizing the mean squared error  mse   assuming a random data process of known functional form.
　formally  for two dimensions   the true function p is assumed to be the realization of a random process such as:
	p x y  = β + z x y 	 1 
where β is typically a uniform random variate  estimated based on the known k values of p  and z is a correlation function with zero mean and known variance. kriging then estimates a model p1 of the same form  based on the k observations:
	p1 xi yi  = e p xi yi  | p x1 y1  ，，， p xk yk  	 1 
and minimizing mean squared error between p1 and p:
	mse = e p1 x y    p x y  1	 1 
　a typical choice for z in p1 is σ1r  where scalar σ1 is the estimated variance  and the symmetric correlation matrix r can encode domain-specific constraints and factors reflecting the current fidelity of data. we use an exponential function for entries in r  with two parameters c1 and c1:
	rij = e c1|xi xj|1 c1|yi yj|1	 1 
intuitively  function estimation at a given point is influenced more by observations nearby than by those farther away.
　the estimator minimizing mean squared error is then obtained by multi-dimensional optimization:
		 1 
this expression can be derived from the conditions that there is no error between the model and the true values at the chosen k sites  and that all variability in the model arises from the design of z  the derivation is beyond the scope of this paper . the multi-dimensional optimization is often performed by gradient descent or pattern search methods. more details are available in  sacks et al.  1   which demonstrates this methodology in the context of the design and analysis of computer experiments.
1	bottom-up detection of ambiguity
the sal equivalence class clustering mechanism  operating on the sparse input data or the dense surrogate model  exploits continuity  grouping neighboring objects that satisfy a domain-specific equivalence predicate  e.g. similar vector direction . at discontinuities  dissimilar neighboring objects are placed in separate classes. however  within a weakly-similar class or across a weakly-different discontinuity  neighboring objects might almost satisfy the predicate. for example  some vectors in fig. 1 b  have two possible forward neighbors; in some cases  a vector might equally well belong to either of two flows. we call such unclear classification choice points ambiguous.
　the bottom-up sal operators introduced in sec. 1 can be used to detect ambiguities if the equivalence class clustering operator classify is extended as in tab. 1. in particular  a domain-specific equivalence predicate indicates when neighbors are equivalent  not equivalent  or ambiguous  allowing classify to delay ambiguous classification decisions.
1	top-down utilization of ambiguity
ambiguity can reflect the desirability of acquiring data at or near a specified point  to clarify the correct classification and to serve as a mathematical criterion of information content. the sample operator specified in tab. 1 addresses this opportunity by generating samples to optimize a given domain-specific objective function  given a set of ambiguous objects. for example  in response to a vector with an ambiguous neighbor  it might suggest nearby locations to sample. in other applications  it might pick the midpoint between a pair of ambiguous points  or even  see the influence-based model decomposition application below  apply sal recursively to qualitatively analyze a set of ambiguous points. in conjunction with a surrogate function  some functional of the mse  eq. 1  can be used to focus sampling  by a suitable statistical design.1 section 1 describes the use of such an objective function.
　when using a surrogate function  the correlation matrix r  eq. 1  can be modified to emphasize the desirability of focusing the fitting effort on ambiguous regions. in particular  indicator covariance terms modulate r when the standard uniformly parameterized model  c1 and c1 in our case  does not adequately capture the observed variability. our approach is reminiscent of incorporating  type c soft data  into variogram estimation  journel  1 :  soft  data have nonnegligible uncertainty and  type c  data are obtained without additional experimentation  in our case  via sal analysis . by using the pcdf of ambiguous objects as an indicator covariance term  we can improve covariance estimates  and also help suggest data locations that will clarify the correct classification. the exact equations are beyond the scope of this article  but we refer the reader to  journel  1  for an account of this  soft kriging  approach.
1	iteration
data are collected for the indicated sample points  by experiment or simulation. when a surrogate function is used  the fitted model is refined with real data at the indicated points  via interpolate. we note that efficient implementations of some data structures  e.g. delaunay triangulation neighborhood graphs  can be incrementally updated with the additional samples  ordo＞nez and zhao  1   . the aggregation process can then be repeated with the extended data set  terminating when the information-theoretic metric used by sample drops below some specified level.
1	applications
this section discusses how the computational framework of two existing applications can be redescribed in terms of ambiguity-directed sampling  and then illustrates the effectiveness of our approach with two new case studies.
1	existing applications
kam  yip  1  interprets the behaviors of hamiltonian dynamical systems by phase-space analysis. geometric points represent states of the system for a given set of parameters. kam works directly with these samples - it does not interpolate a dense representation. kam groups points into orbits describing the system's temporal evolution; it groups orbits into phase portraits describing evolution of all states for a given set of parameters; and it groups phase portraits into bifurcation maps describing variations in portraits due to variations in parameters. at each stage  kam adds samples when it detects an inadequate description. in our vocabulary  the classify predicate clustering orbits into a phase portrait notices when two neighboring orbits cannot physically be adjacent; sample then starts orbit integration from the mid-point of an ambiguous pair of neighboring points. similarly  in a bifurcation map additional phase portraits are generated for parameter values between those of ambiguous neighboring phase portraits.
　sta  ordo＞nez and zhao  1    has been applied to build high-level descriptions of morphogenesis in diffusionreaction systems by tracking aggregates of sample  floaters  that react to changes in the underlying field. in particular  floaters attempt to ensure an adequate sampling of the field  no interpolation is required   especially in high-gradient areas. they do this in a manner similar to the particle system of witkin and heckbert   by repelling each other  splitting  and merging. in our vocabulary  the classify predicate bundling floaters in a region tests whether or not neighboring floaters are near enough relative to an energy metric measuring adequate representation of the region; sample simply splits one ambiguous floater into two adjacent floaters.
1	pocket identification
our first application domain is motivated by research in spatial statistics  journel  1; sacks et al.  1  and multidisciplinary system design  knill et al.  1 . visualize the ndimensional hypercube defined by xi （   1  i = 1，，，n  with the n-sphere of radius 1 centered at the origin  Σxi1 ＋ 1  embedded inside it. notice that the ratio of the volume of the cube  1n  to that of the sphere  πn/1/ n/1 !  grows unboundedly with n. in other words  the volume of a highdimensional cube is concentrated in its corners  a counterintuitive notion at first . carl de boor exploited this property to design a difficult-to-optimize function which assumes a pocket in each corner of the cube  fig. 1   that is just outside the sphere  rice  1 . formally  it can be defined as:

where x is the n-dimensional point  x1 x1 ，，， xn  at which the pocket function p is evaluated  i is the identity n-vector  and k ， k is the l1 norm.
　it is easily seen that p has 1n local minima; if n is large  say  1  which means it will take more than half a million points to just represent the corners of the n-cube!   naive global optimization algorithms will require an unreasonable number of function evaluations. however  in real-world domains  significant structure exists and can often be exploited. a good example is the stage algorithm  boyan and moore  1   which intelligently selects starting points for local search algorithms. our goals here are very different from global optimization: we wish to obtain a qualitative indication of the existence  number  and locations of pockets  using low-fidelity models and/or as few data points as possible. the

figure 1: a 1d pocket function.
results can then be used to seed higher-fidelity calculations. this is also fundamentally different from dace  sacks et al.  1   polynomial response surface approximations  knill et al.  1   and other approaches in geo-statistics where the goal is accuracy of functional prediction at untested data points. here  accuracy of estimation is traded for the ability to mine pockets.
　in a dense field of data  it is straightforward to identify pockets by applying the vector field bundling implementation discussed in the introduction  see fig. 1  to the gradient field. in the data-scarce setting  we follow the ambiguity-directed sampling framework  incorporating the domain-specific knowledge summarized in tab. 1. given a surrogate model  vector bundling identifies vectors which can participate in multiple good streamlines. the surrogate model incorporates these ambiguities with an indicator covariance term counting the number of possible good neighbors. this  ambiguity distribution  provides a novel mechanism to include qualitative information - streamlines that agree will generally contribute less to data mining  just as samples that are far apart are weighted less in the original r matrix. thus  this framework can be viewed as a natural generalization of the assumptions of sample clustering that underlie kriging.
　the sample objective function described in tab. 1 minimizes the expected posterior entropy on the unsampled design space  which by a reduction argument  can be shown to be maximizing the prior entropy over the entire design space  sacks et al.  1 . in turn  this means that the amount of information obtained from an experiment is maximized. for our purposes  the objective function thus provides a basis to choose sample points that will improve our modeling of p.
　we applied the ambiguity-driven mechanism to determining pockets in both 1d and 1d. we used a variation of the pocket function with a pseudorandom perturbation that shifts the pockets away from the corners in a somewhat unpredictable way. this twist precludes many forms of analyses  such as symbolic parsing  by imposing a highly nonlinear global map of pocket locations. in the traditional pocket function  the dips can be viewed as being influenced by little spheres at the corners  with known radii and centers. the new pocket design uses an additional parameter to impose non-symmetric perturbations which randomize both the radii and centers. as a result  local modeling must be carried out at each corner to determine the exact location of the pocket. more detail about this function can be found in  rice  1 
surrogate model
use kriging interpolator with indicator covariance term  modeling number of similar-enough neighbors from predicate below  to estimate p at unknown points.
vector equivalence predicate
return true if vector directions are similar enough  false if they aren't  and ambiguous if a vector has multiple neighbors with similar-enough directions.
sample objective function
minimize the entropy e  logd   where d is the conditional density of p over the design space not covered by the current data values.table 1: domain knowledge for ambiguity-directed sampling in pocket identification.
pp. 1 .
　the initial experimental configuration used a face-centered design  1 points in the 1d case . the surrogate model then generated a 1n-point grid. the ambiguity-directed mechanism selected new design points  using the vector field bundling approach discussed above. standard parameter settings were applied: required similarity of 1 for dot product of adjacency direction and vector field direction  and factor of 1 distance penalizing the grouping of far-apart vectors. fig. 1 shows a design involving only 1 total data points that is able to mine the four pockets. as previously discussed  our sampling decisions result in highly sub-optimal designs according to traditional metrics of variance in predicted values and d-optimality  but are sufficient to determine pockets. in particular  the ambiguity-driven framework completely skips one of the quadrants in selecting new points. this indicates that neighborhood calculations involving the other three quadrants are enough to uncover the pocket in the fourth quadrant. since the kriging interpolator uses local modeling and since pockets in 1d effectively occupy the quadrants  obtaining measurements at ambiguous locations serves to capture the relatively narrow regime of each dip  which in turn helps to distinguish the pocket in the neighboring quadrant. this effect is hard to achieve without qualitative feedback. for higher dimensions  including 1d   the pockets move further away from the center of the design space  necessitating the sampling of points in all corners.
　fig. 1 shows the distributions of number of design points required for ambiguity-directed and kriging-based pocket identification over 1 perturbed variations of the 1d pocket function. ambiguity-directed sampling required 1 to 1 additional samples  with the latter figure in the pathological case where the random perturbations cause a nearly quintic dip  rendering the initial adjacency calculations misleading. in comparison  conventional incremental kriging techniques  of the form described in section 1 without qualitative analysis  required 1 to 1 additional data points. while pockets in the bigger dips are discovered quickly  the quintic and shallow dips require more function evaluations. tests with pockets in 1d yielded even more significant results: up to 1 additional points for regular kriging  but at most 1 for ambiguitydirected sampling. with the use of block kriging  reductions in both values could be enjoyed  but these figures illustrate

1
 1 1  1
 1
	 1	 1	1.1

figure 1: mining pockets from only 1 sample points  1d .  top  the chosen sample locations: 1 initial face-centered samples  marked as blue circles  plus 1 ambiguity-directed samples  marked as red diamonds . note that no additional sample is required in the lower-left quadrant.  middle  computed variogram for resulting surrogate model: color represents estimated p and isocontours join points of equal estimated mse.  bottom  sal structures in surrogate model data  confirming the existence of four pockets.

figure 1: pocket-finding results  1d  show that ambiguitydirected sampling always requires fewer total samples  1  than conventional kriging  1 .
the effectiveness of our technique.
　the extension to more than 1 dimensions is straightforward and is not detailed here for ease of presentation. it essentially entails using the appropriate covariance matrix and sal data structures  e.g. 1-adjacency in 1d  1-adjacency in 1d  ... . while we believe our ambiguity-directed framework will fare well compared to traditional kriging  a more careful study will be needed to characterize the scalability of our approach.
1	influence-based model decomposition
influence-based model decomposition  bailey-kellogg and zhao  1; 1  is an approach to designing spatiallydistributed data interpretation and decentralized control applications  such as thermal regulation for semiconductor wafer processing and noise control in photocopy machines. a decentralized influence graph  built by sampling the effects of controls on a field  either physically or by solving a partial differential equation   represents influences of controls on distributed physical fields. given the expense of obtaining influence graph values  it is desirable to minimize the number of samples required. this section demonstrates that ambiguity-directed sampling can greatly reduce the number of samples required. note that we do not interpolate a dense representation  following the explicit kriging methodology  since it sometimes does not result in explainable designs  by overlooking  nice  properties such as balance  symmetry  collapsibility  and comparability  easterling  1 .
　influence-based model decomposition uses influence graphs for control placement and parameter design algorithms that exploit physical knowledge of locality  linear superposability  and continuity for distributed systems with large numbers of coupled variables  often modeled by partial differential equations . by leveraging the physical knowledge encapsulated in influence graphs  these control design algorithms are more efficient than standard techniques  and produce designs explainable in terms of problem structures. influence-based model decomposition decomposes a problem domain so as to allow relatively independent design of controls for the resulting regions. fig. 1 overviews the approach:
1. represent in an influence graph the effects of a few sample probe controls on the field - in this example  the heat flows induced in a piece of material by point heat sources.

figure 1: influence-based model decomposition: sample an influence graph  and cluster probes and partition field based on similar control effects. ambiguity-directed sampling techniques close the loop by suggesting new probe locations.
1. cluster the probes based on similarities in their effects  as represented in the influence graph. for example  the geometric constraint imposed by the narrow channel in the dumbbell-shaped piece of material results in similar field responses to the two probes in the left half of the dumbbell and similar responses to the two probes in the right half of the dumbbell. note that influence graphs encapsulate not only geometry but also material properties  which can greatly impact heat flows and thus the proper decomposition.
1. cluster the field nodes based on the probe clustering  applying a predicate testing if neighboring field nodes are well-represented by the same probe nodes. in the example  the field nodes in the left half of the dumbbell are best represented by the probe nodes also in the left half  which belong to the same probe equivalence class   and are thus decomposed from the nodes in the right half. controls are placed in the regions and optimized by a separate process not discussed here.
　the quality of decompositions from a small number of randomly-placed probes is competitive with that of a spectral partition of the complete influence graph  computed following an approach developed for image segmentation  shi and malik  1    but with orders of magnitude less computation and in a decentralized model  bailey-kellogg and zhao  1 . we now extend this approach to show that replacing random sampling with ambiguity-directed sampling achieves even better results. ambiguity-directed sampling effectively closes the loop between the field decomposition and influence graph sampling  dashed arrow in fig. 1 . tab. 1 describes the the domain-specific knowledge used in ambiguitydirected sampling for model-based decomposition.
　we applied ambiguity-directed sampling to the three problems presented by  bailey-kellogg and zhao  1 : a plusshaped piece of material  a p-shaped piece of material  and an anisotropic bar  illustrating different geometries  topologies  the p-shaped material has a hole   and material properties. results were collected for 1 runs each by random probing  and using each possible node in the discretization for the initial probe in ambiguity-directed probing. results are relative to a baseline spectral partitioning of the complete influence graph  computed essentially using probes at every one of the hundreds of nodes in a discretization .
　given a decomposition  a quality metric compares the amount of influence that stays within a region to the amount that leaves it: to be more specific  define the decomposition
field node equivalence predicate
return true if nodes have similar-enough effect to one probe  false if they don't  and ambiguous if the magnitude of the effect is not large enough or if two competing probes yield similar effects.
sample objective function
perform secondary aggregation and classification to find regions of ambiguities. for each ambiguous field node  measure how similar its flows are to other ambiguous field nodes in its region; choose the node with the best similarity to the most ambiguous nodes.table 1: domain knowledge for ambiguity-directed sampling in influence-based model decomposition.

figure 1: comparison of influence-based model decomposition quality using random and ambiguity-directed probes  for three different problems:  left  plus;  middle  p;  right  bar. results are relative to spectral partitioning.
quality q  1 ＋ q ＋ 1  for a partition p of a set of nodes s as follows  i is the influence :

　fig. 1 summarizes the results. the ambiguity-directed method generally does much better than random for a given number of probes  both in mean and standard deviation of quality  and it generally can do as well with 1 probes as random sampling can with 1. one interesting case is the taper in the plus-shaped piece of material. this is due to over-sampling: the samples are clustered in the middle of the plus  yielding a jagged decomposition that results in a worse quality score. in fact  with default parameters  the ambiguitybased metric declines to add samples beyond about 1  indicating that the field was adequately sampled. in order to achieve the desired number of samples  parameters were set to force sampling for only small information gain.
1	discussion
the idea of selective sampling to satisfy particular design criteria arises in many contexts  such as gaussian quadrature  spline smoothing in geometric design  remote sensing data acquisition  crystallography  gopalakrishnan et al.  1  and engineering design optimization. in data mining  sampling has been viewed as a methodology to avoid costly disk accesses  this thread of research  however  doesn't address the issue of where to sample   kivinen and mannila  1 . all these approaches  including ours  rely on capturing properties of a desirable design in terms of a novel objective function.
the distinguishing feature of our work is that it uses spatial information gleaned from a higher level of abstraction to focus data collection at the field/simulation code layer. while flavors of the consistent labeling problem in mobile vision have this feature  they are more attuned to transferring information across two successive abstraction levels. the applications presented here are novel in that they span and connect arbitrary levels of abstraction  thus suggesting new ways to integrate qualitative and quantitative simulation  berleant and kuipers  1 .
　the effectiveness of our approach relies on the trustworthiness of the ambiguity detection mechanism and the ability to act decisively on new information. in both our applications  this was easily achieved by relying on fairly specific qualitative features whose causes are well understood. however  in other applications  e.g. phase portrait exploration for sensitivity analysis of highly non-normal matrices   it is difficult to distinguish between qualitative changes in problem characteristic and numerical error such as roundoff. in such cases  a more detailed modeling of qualitative behavior should be exploited for ambiguity-directed sampling to be successful. in terms of the pocket study  this might require a domainspecific enumeration of the various ways in which pockets  and ambiguities in detecting them  can arise  and a probabilistic model of the elements of a sal hierarchy using  say  superpositions of bayesian expectation-maximization terms.
　sal provides a natural framework for exploiting continuity to uncover structures in spatial data; ambiguity-directed sampling focuses sal's efforts on clarifying those discontinuities that yield multiple  qualitatively-different interpretations. this effort is leading us to explore a completely probabilistic sal framework. such a framework should also be able to incorporate information from multiple  perhaps conflicting  sal hierarchies. this is an emerging frontier in several applications  such as bioinformatics   where diverse experimental methodologies can cause contradictory results at the highest levels of abstraction. our work provides some encouraging results addressing such grand-challenge problems.
acknowledgments
thanks to layne t. watson  virginia tech  and feng zhao  xerox parc  for helpful discussions. this work is supported in part by the following grants to bruce randall donald: national science foundation grants nsf iis-1  nsf eia-1  nsf eia-1  nsf cda-1 
nsf eia-1  nsf cise/cda-1  nsf iri1  and nsf iri-1  u. s. department of justice contract 1-dt-cx-k1  and an equipment grant from microsoft research; and nsf grant eia-1 to naren ramakrishnan.
references
 bailey-kellogg and zhao  1  c. bailey-kellogg and f. zhao. influence-based model decomposition. in proc. aaai  1.
 bailey-kellogg and zhao  1  c. bailey-kellogg and f. zhao. influence-based model decomposition. artificial intelligence  1. accepted  to appear.
 bailey-kellogg et al.  1  c. bailey-kellogg  f. zhao  and k. yip. spatial aggregation: language and applications. in proc. aaai  1.
 berleant and kuipers  1  d. berleant and b. kuipers. qualitative and quantitative simulation: bridging the gap. artificial intelligence  1 :1  1.
 boyan and moore  1  j.a. boyan and a.w. moore.
learning evaluation functions to improve optimization by local search. j. machine learning research  1-1  1.
 easterling  1  r.g. easterling. comment on 'design and analysis of computer experiments'. statistical science  1 :1  1.
 gopalakrishnan et al.  1  v. gopalakrishnan  b.g. buchanan  and j.m. rosenberg. intelligent aids for parallel experiment planning and macromolecular crystallization. in proc. ismb  volume 1  pages 1  1.
 journel  1  a. journel. constrainted interpolation and qualitative information - the soft kriging approach. mathematical geology  1 :1  november 1.
 kivinen and mannila  1  j. kivinen and h. mannila. the use of sampling in knowledge discovery. in proc. 1th acm symposium on principles of database systems  pages 1  1.
 knill et al.  1  d.l. knill  a.a. giunta  c.a. baker  b. grossman  w.h. mason  r.t. haftka  and l.t. watson. response surface models combining linear and euler aerodynamics for supersonic transport design. j. of aircraft  1 :1  1.
 ordo＞nez and zhao  1    i. ordo＞nez and f. zhao.  sta: spatio-temporal aggregation with applications to analysis of diffusion-reaction phenomena. in proc. aaai  1.
 rice  1  j.r. rice. learning  teaching  optimization and approximation. in e.n. houstis  j.r. rice  and r. vichnevetsky  editors  expert systems for scientific computing  pages 1. north-holland  amsterdam  1.
 sacks et al.  1  j. sacks  w.j. welch  t.j. mitchell  and h.p. wynn. design and analysis of computer experiments. statistical science  1 :1  1.
 shi and malik  1  j. shi and j. malik. normalized cuts and image segmentation. in proc. cvpr  1.
 witkin and heckbert  1  a. witkin and p. heckbert. using particles to sample and control implicit surfaces. in proc. siggraph  1.
 yip and zhao  1  k.m. yip and f. zhao. spatial aggregation: theory and applications. j. artificial intelligence research  1  1.
 yip  1  k.m. yip. kam: a system for intelligently guiding numerical experimentation by computer. mit press  1.

a spatial odyssey of the interval algebra: 1. directed intervals
jochen renz
institut fu：r informationssysteme
technische universita：t wien
a-1 vienna  austria

abstract
allen's well-known interval algebra has been developed for temporal representation and reasoning  but there are also interesting spatial applications where intervals can be used. a prototypical example are traffic scenarios where cars and their regions of influence can be represented as intervals on a road as the underlying line. there are several differences of temporal and spatial intervals which have to be considered when developing a spatial interval algebra. in this paper we analyze the first important difference: as opposed to temporal intervals  spatial intervals can have an intrinsic direction with respect to the underlying line. we develop an algebra for qualitative spatial representation and reasoning about directed intervals  identify tractable subsets  and show that path-consistency is sufficient for deciding consistency for a particular subset which contains all base relations.
1	introduction
qualitative spatial representation and reasoning has become more and more important in recent years. the best-known approach in this field is the region connection calculus
      randell et al.  1  which describes topological relationships between -dimensional spatial regions of arbitrary shape. for some applications  however  it is sufficient to use spatial regions with more restricted properties. the block algebra  balbiani et al.  1   for instance  considers only spatial regions which are -dimensional blocks whose sides are parallel to the defining axes. the most restricted spatial regions are  one-dimensional  intervals. a prototypical spatial application of intervals are traffic scenarios. vehicles usually move only along given ways  also sea-/airways . therefore  when looking at vehicles on one particular way  vehicles and their regions of influence  such as safety margin  braking distance  or reaction distance  could be represented as intervals on a line which represents the possibly winded way. similar

　　this research was carried out while the author was visiting the department of computer and information science at the university of linko：ping  sweden. supported by the wallenberg foundation as part of the witas project. thanks to patrick doherty  guy even  alfonso gerevini  and erik sandewall for their comments.
to the well-known interval algebra  allen  1  developed for temporal intervals  it seems useful to develop a spatial interval algebra for spatial intervals.
　there are several differences between spatial and temporal intervals which have to be considered when extending the interval algebra towards dealing with spatial applications.  1  spatial intervals can have different directions  either the same or the opposite direction as the underlying line.  1  ways usually have more than one lane where vehicles can move  i.e.  it should be possible to represent that intervals are on different lanes and that one interval is  e.g.  left of  right of  or beside another interval.  1  it is interesting to represent intervals on way networks instead of considering just isolated ways.  1  intervals such as those corresponding to regions of influence often depend on the speed of vehicles  i.e.  it should be possible to represent dynamic information. this is also necessary for predicting future positions of vehicles which is an important task in traffic control. as for temporal intervals it is also importantto representqualitativeor metric informationon the length of intervals and on the distance between intervals.
　we start this spatial odyssey of the interval algebra by analyzing the first important difference between spatial and temporal intervals  namely  direction of intervals. we define the directed intervals algebra which consists of 1 jointly exhaustive and pairwise disjoint base relations  identify tractable subsets  and show that path-consistency decides consistency for a particular subset which contains all base relations.
1	directed intervals
the interval algebra     describes the possible relationships between convex intervals on a directed line. the default application of the interval algebra is temporal  so the directed line is usually considered to be the timeline. the 1 base relations  before   after   meets   met-by   overlaps   overlapped-by   equals   during   includes   starts   started-by   finishes   and finished-by   describe a combination of topological relations  disconnected  externally connected  partial overlap  equal  non-tangential proper part  tangential proper part  and the converse of the latter two  and order relations      . the topological distinctions are exactly those which are made by . therefore  is often considered as the spatial counterpart of the interval algebra. or  from another point of view  what distinguishes the interval algebra from and what makes it its key

	y	x	y	x
figure 1: four structurally different instantiations of the relation   behind   with directed intervals
feature is the given direction of the  one-dimensional  line. this given direction naturally imposes a direction also on the intervals: an interval can have the same or the opposite direction as the underlying line. however  because of its original temporal interpretation  no event can end before it starts   direction of intervals has never been considered in ai. actually  directed intervals have been studied in the large field of interval arithmetics  but work in this field is completely different from the qualitative and constraint-based approaches studied in ai. when using the interval algebra for spatial applications  direction of intervals has to be taken into account. this leads to the obvious question: can the large body of work and the large number of results obtained on the interval algebra such as algorithms and complexity results also be applied to a spatial interpretationof the interval algebra  or is it necessary to completely start from scratch again 
　before answering this question  consider the example of figure 1 which illustrates the differences of having directed intervals from having only intervals of the same direction. since all four combinations of the directions of the two intervals are possible  there are four structurally different instantiations of every relation instead of just one. therefore  it is possible that inconsistent instances of the interval algebra become consistent when allowing directed intervals.
1	the directed intervals algebra
a straightforward way for dealing with directed intervals would be to add additional constraints on the direction of intervals to constraints over the interval algebra and treat the two types of constraints separately while propagating information from one type to the other  similar to what has been done in  gerevini and renz  1 .  we say that an interval has positive direction if it has the same direction as the underlying line and negative direction otherwise. so possible direction constraints could be unary constraints like   has positive/negative direction  or binary constraints like   and have the same/opposite direction . this approach  however  is not possible since the interval algebra loses its property of being a relation algebra when permitting directed intervals. this can be easily seen when considering the  behind  relation of figure 1. the converse of   behind   is   is behind or in front of    whose converse is   is behind or in front of    i.e.  applying the converse operation     twice leads to a different relation than the original relation. this is a contradiction to one of the requirements of relation algebras      ladkin and maddux  1 . this contradiction does not occur when we refine the  behind  relation into two disjoint sub-relations  behind   and  behind   where the subscript indicates that bothintervals have the same     or opposite     direction. the converse of both relations is  in-front-of   and  behind    respectively. applying the
directed intervalssym-pictorialbase relationbolexamplebehind-x-
-y-in-front-ofbehind-x-
-y-in-front-of-x-
-y-meets-from-behind-x-
-y-meets-in-the-frontmeets-from-behind-x-
-y-meets-in-the-front-x-
-y-overlaps-from-behind-x-
-y-overlaps-in-the-frontoverlaps-from-behind-x-
-y-overlaps-in-the-front-x-
-y-contained-in-x-extends-y-contained-in-x-extends-y-contained-in-the-back-of-x-extends-the-front-of-y-contained-in-the-back-of-x-extends-the-back-of-y-contained-in-the-front-of-x-extends-the-back-of-y-contained-in-the-front-of-x-extends-the-front-of-y-equals-x-
-y-equals-x-
-y-table 1: the 1 base relations of the directedintervals algebra
converse operation again leads to the original relations.
　since a relation algebra must be closed under composition  intersection  and converse  we have to make the same distinction also for all other relations. this leads us to the definition of the directed intervals algebra  dia . it consists of the 1 base relations given in table 1  which result from refining each relation into two sub-relations specifying either same or opposite direction of the involved intervals  and of all possible unions of the base relations. this gives a total number of relations. converse relations are given in the same table entry. if a converse relation is not explicitly given  the corresponding relation is its own converse. we denote the set of 1 base relations as . then .
complex relations which are the union of more than one base relation are written as . the union of all base relations  the universal relation  is denoted .
　a base relation consist of two parts  the interval part which is a spatial interpretation of the interval algebra and the direction part which gives the mutual direction of both intervals  either or . if a complex relation consist of base relations with the same direction part   we
can combine the interval parts and write
instead of	. we write	 resp.	  in

table 1:	base relations	  their reverses	  and their spatial interpretations
order to refer to the union of the interval parts of every subrelation of a complex relation where the direction part is
	 resp.	.  in this way  every	relation	can be
written as . denotes the set of possible interval parts of relations.
　it is important to note that the spatial interpretation of the interval algebra was chosen in a way that the interval part of a relation only depends on the direction of and not on the direction of . therefore  if the direction of is reversed  written as   then only the direction part changes  i.e. 
　　　　	. this would not be the case in a straightforward spatial interpretation of the original temporal relations. for instance 	relations like  	started-by	  or   finished-by   depend on the direction of	. instead  we interpret these relations spatially as  	extends-the-front/back-of   and  	contained-in-the-front/back-of	 . this interpretation is independent of the direction of	. when all intervals have the same direction  both interpretations are equivalent. in order to transform the spatial and the temporal interval relations  independent of the direction of the intervals  into each other  we introduce two mutually inverse functions and	  i.e.  and	. the mapping is given in table 1.
　all relations of the directed intervals algebra are invariant with respect to the direction of the underlying line  i.e.  when reversing the direction of the line  all relations remain the same. this is obviously not the case for the interval algebra  e.g.  if is before and one reverses the direction of the timeline  then is after . in order to transform relations into the corresponding relations and vice versa  we introduce a unary reverse operator     on relations such that specifies the relation which results from when reversing the direction of the underlying line. for all relations we have that . for relations  the reverse relation is given in table 1. the reverse of a complex relation is the union of the reverses of the involved base relations. the reverse of the composition     of two relations is equivalent to the composition of the reverses of the two involved relations  i.e.  . applying the reverse operator twice results in the original relation  i.e.  . using the reverse operator we can also specify what happens with a relation if only the direction of is changed. then the topological relation of the intervals stays the same  but the order changes  i.e.   front  becomes  behind / back  and vice versa. the mutual direction also changes. this can be expressed in the following way: .
　we now have all requirements for computing the composition     of relations using composition of relations  denoted here by   as specified by allen .
theorem 1 let	be	base relations.
1. if  then1. if  thenproof. assume that	and	holds. if
　　and have positive direction  it is clear that the interval part of the composition of the relations is the same as the composition of the relations  with respect to the different interpretations.  the result of the composition is the same if have negativedirection  since relations are invariant with respect to the direction of the underlying line. only depends on the direction of and only depends on the direction of . therefore  reversing the direction of
 i.e.    does not change the result of the interval part of the composition  only the resulting direction part. this proves the first rule.
　assume that and have positive direction while has negative direction. if we reverse the direction of   which changes the relations to and to    then we can apply the first composition rule. this results in
 
the second composition rule. as in the first case  this rule does not change when we reverse the direction of  i.e.    or the direction of all three intervals. this proves the second rule.	
the composition of complex relations is as usual the union of the composition of the contained base relations. it follows from the closedness of the interval algebra that is closed under composition  intersection  converse  and reverse.
1	reasoning over directed intervals
the main reasoning problem in spatial and temporal reasoning is the consistency problem cspsat    where is a set of relations over a relation algebra  renz and nebel  1 .
instance: a set of variables over a domain and a finite set of binary constraints   and . 
question: is there a consistent instantiation of all variables in with values from which satisfies all constraints 
the consistency problem of the directed intervals algebra  cspsat dia   is clearly np-hard since the consistency problem of the interval algebra is already np-hard. on the other hand it is not clear whether the consistency problem is tractable if only the base relations are used.
　additional to the relations  we also give the possibility of explicitly specifying the direction of intervals. we maintain them in a set which contains unary direction constraints of the form where is a variable over a directed interval and gives the direction of   either positive   negative   or indefinite . unary direction constraints and constraints interact in two ways.
proposition 1 given two intervals   the constraint with   and the unary direction constraints and . these constraints interact in the following way:
1. if all     are equivalent  then  a  and if and  b  and if .
1. if	and	are both definite  then  a 	if and  b 	if	.
if all information is propagatedfrom to and from to we write the resulting sets as and . if the empty constraint occurs during this propagation  then is inconsistent.
　there are several ways of deciding consistency of a given set of constraints over a set of relations . the most common way is to use backtracking over a tractable subset of which contains all base relations and enforce path-consistency as forward-checking  this is done by applying for each triple of constraints     the operation ; if the empty relation is not contained  the resulting set is pathconsistent   ladkin and reinefeld  1 . before we can use this method for deciding cspsat dia   we must prove that the consistency problemis tractable for the base relations and preferably that path-consistency decides consistency for these relations. in order to prove this  we need a different method for deciding consistency and we have to show that this method is polynomial for the set of base relations.
　for the interval algebra most tractability proofs were carried out using the endpoint encoding of the relations  e.g.  nebel and bu：rckert  1   which describes the qualitative relations between the four endpoints of the two involved intervals. for instance  the  before  relation can be encoded as plus the default relations and which hold for all non-directed intervals   denote the start points and the end points of the intervals
　　.  it is also possible to specify an endpoint encoding of the relations. since spatial intervals can have different directions  the default relations do not hold anymore. furthermore  we have to take into consideration that relations are invariant with respect to the reverse operation. therefore  it is the most compact way to use the  betweenness  predicate for specifying an endpoint encoding of relations. means that is between
and   no matter which direction the intervals have. using this predicate  the relation   for instance  can be encoded as	. since the
betweenness problem is np-hard  garey and johnson  1   this encoding does not seem to be helpful for proving any tractability results. we will therefore refrain from specifying the endpoint formulas of the base relations.
　another possibility of deciding the consistency problem is to transform a set of constraints into an equivalent set of constraints and decide consistency of . in order to make such a transformation  the direction of every interval must be known. then it is possible to reverse the direction of certain intervals such that all intervals have the same direction and transform the updated constraints into constraints. we call this the normal form of a set of constraints and a set of definite unary direction constraints for each interval involved in . the normal form  written as   is obtained as follows.
proposition 1 given a set of constraints and a set of definite unary direction constraints for each interval involved in . the normal form is obtained by applying the following procedure.
1. for each constraint	do
1. if	has negativedirection  add	to
1. if	has positive direction  add	to
lemma 1 given a set of constraints and a set of definite unary direction constraints for each interval involved in . can be computed in time .
proof. can be computed in time   since all constraints of are definite and information has to be propagated only from every pair of intervals to the corresponding constraint in using rule 1 of proposition 1. is transformed to in time   since each of the constraints is transformed separately in constant time. 
lemma 1 given a set of constraints and a set of definite unary direction constraints for each interval involved in . is consistent if and only if is consistent.
proof. suppose that is consistent and that is an instantiation of . the direction of each interval of is as specified in and the relation between each pair of intervals is a base relation which is a sub-relation of . we can now reverse the direction of all intervals of with negative directions  resulting in . since all relations only depend on the direction of   the relations between the intervals of are now if the direction of was negative in and if the direction of was positive in . transforming these relations into relations results for every pair of intervals in sub-relations of . thus  is a consistent instantiation of . the oppo-
site direction can be proved similarly. suppose that is consistent and that is an instantiation of it where all intervals are considered to have positive direction. let be the set of constraints between all intervals of using base relations. reversing the direction of all intervals which must have negative direction according to results in and adopting the constraints of results in . since applying the reverse operator twice gives the original relation  each constraint of is a sub-constraint of a constraint of . thus  is a consistent instantiation of . 
　using the normal form  we can now decide consistency of a set of constraints by computing or guessing a set containing the direction of all intervals  computing   and deciding consistency of using the methods developed for the interval algebra. since there are different direction combinations of directed intervals  it is in general
   -hard to find a suitable set for which is consistent or to show that there is no such set. if  however  we can show that for a given set of relations all possible candidate sets can be identified in polynomial time and if contains only relations of a tractable subset of the interval algebra  then cspsat    is tractable. using this method  we identify several tractable subsets of the directed intervals algebra in the following section.
1	tractable subsets of
the first set we analyze is the set of	base relations	.
lemma 1 cspsat 	  is tractable.
proof. consistency of a set of constraints over can be decided in polynomial time by using the following steps.
1. transform into a graph where is the set of variables involved in and contains an  undirected  edge if where .
1. split into disjoint subsets such that for each pair of variables there is a path from to in and for each pair of variables       there is no path from to in .
1. generate a set of direction constraints by selecting one variable for each and adding     to .
1. compute	and decide its consistency.
it is clear that each of the four steps can be computed in polynomial time. for each pair of variables of different sets there are only constraints involving the universal relation  i.e.  and which specify subsets of containing all constraints involving only variables of or   respectively  are completely independent of each other. there is a path from each variable of to every other variable of and each path consists of constraints where each constraint involves only relations with the same direction part. therefore  it is sufficient to have the direction of only one variable given in orderto computethe direction of all variables .
if the path contains an odd number of constraints involving relations of the type   the direction of is opposite to the direction of . otherwise they have the same direction. thus  contains definite unary direction constraints for all variables of . if there are conflicting paths  then is inconsistent. since relations are invariant with respect to changing the direction of the underlying line  it does not matter for consistency purposes if we select the direction of as positive or negative. contains only relations of a tractable subset of . it follows from lemma 1 that its consistency is equivalent to the consistency of . 
in the above proof it is not important that all non-universal relations are base relations  only that all non-universal relations consist of base relations with the same direction part. therefore  we can easily extend the above result.
theorem 1 let	be a tractable subset of the interval
algebra which is closed under the reverse operator. then is a
tractable subset of the directed intervals algebra.
proof. we can apply the same proof as given for lemma 1. but only if all relations contained in the normal form are contained in a tractable subset of the interval algebra. this is clearly the case if is closed under the reverse operator which is used in the transformation into the normal form. 
ord-horn  also denoted   is the only maximal tractable subset of the interval algebra which contains all base relations  and for which path-consistency decides consistency   nebel and bu：rckert  1 . using a machine-assisted comparison of the ord-horn relations we found that they are closed under the reverse operator. this is not true for some of the maximal tractable subclasses identified in  drakengren and jonsson  1  which do not contain all base relations.
　all tractability results we have given so far rely on given mutual directions of intervals. for some applications this is a realistic assumption  but what happens if this is not given in all cases  if some constraints involve relations with different direction parts such as   assume that we have given a set of constraints over arbitrary relations.
table 1: transformation of constraints over a triple of variables  dependingon theirdirections   into relations of the normal form.
one way of obtaining a possible candidate set of definite unary directions for each interval in is to look at each triple of variables of separately and check all different combinationsof directions of   can be either or  . if enforcingpath-consistencyto the normal form gives the empty relation for a particular choice of
       then this choice makes inconsistent. if we combine all such inconsistent triples	  then is inconsistent if is satisfiable  
is a placeholder for	if	and for	if
   analogously for and .  a possible candidate set can be obtained by computing a model of the complement of this formula  namely 	. this formula is an instance of 1sat and  thus  -hard to compute. eventually  since we did not propagate information between different triples  we have to check all possible models of .
　because of its hardness  this way of generating a candidate set does not seem to be helpful. however  we can show that it leads to a tractability proof for a restricted but interesting set of relations  namely  those relations which correspond to the set of 1 base relations of the
interval algebra  i.e. 	.
theorem 1 let be a set of constraints over the variables which contains a constraint with if and only if . consistency of can be decided in polynomial time.
proof. for each triple of variables of with we check for all possible directions
if the normal form of the triple is consistent or not. depending on whether   either or is given in .
equivalently  either	or	is given in	. since
       . obviously  it is not possible that and are both in . therefore  we compare in the normal form either  1     1    or  1  with
in order to check consistency of the triple. in all three cases  the result is invariant with respect to the direction of one of the three variables and depends only on the direction of the other two variables. we can verify this using table 1. in the first case  the only thing which changes in table 1 when varying the direction of  which is in the table  are the subscripts of and : changes to   changes to
and vice versa. since all relations used in	are of the form
         and are always equivalent  i.e.  the result of comparing with is invariant with respect to the direction of . in the second case  the result is invariant with respect to the direction of  which is in the table.  when varying in the table  changes to   changes to   and changes to and vice versa.
since is always equal to   the change of does not change the result. the change of is more difficult. using a case analysis of the 1 different cases  we were able to
prove that	and that
if	is of type	. as an example consider the relation	whose converse is	. and	. in the third
case  the result is invariant with respect to the direction of which can be proved equivalently to the first case.
　because consistency of every triple depends on the direction of only two of the three variables  the resulting formula
　 see above  is an instance of 1sat and  thus  solvable in polynomial time. for any model of   the resulting set of unary direction constraints leads to a consistent normal form
       . this is because contains only base relations and because all triples are consistent  as it has been checked when was generated.  therefore  enforcing pathconsistency does not change any relation of . 
　instead of transforming every set of constraints to the normal form in order to decide consistency  it would be nice to know if and for which sets of relations pathconsistency is sufficient for deciding consistency when applied directly to . we show this for   the set of relations which results from ord-horn  see theorem 1 .
theorem 1 path-consistency decides cspsat 	 .
proof. consistencyof a set of constraints over can be decided in polynomial time by deciding consistency of its normal form as it is obtained by applying the steps given in the proof of lemma 1. assume that is path-consistent  i.e.  for everytriple of variables with
　we have that . if is also path-consistent  then is consistent. in order to show that is path-consistent  we have to show that implies for all triples   .  since all relations of consist of base relations with the same directionpart  we can extend the composition rules given in theorem 1 to all relations. according to these rules  can be written as either	 
 	  or depending on the directions of
     . the corresponding restrictions of can be derived using table 1. if   the restrictions of are equivalent to the results of applying to the interval parts of the above given restrictions of . if   we have to reverse and the restrictions of . since
and for all relations   the restrictions of are also equivalent to the results of applying to the interval parts of the restrictions of  e.g. the first restriction is the same as
line 1 in table 1.  thus 	implies	. 
　this result enables us to decide consistency of arbitrary sets of constraints by backtracking over the relations and by using path-consistency as a forward-checking method and as a decision procedure for sub-instances of which contain only relations of .
1	discussion & future work
we extended the interval algebra for dealing with directed intervals which occur when interpreting intervals as spatial instead of temporal entities. reasoning over the directed intervals algebra is more difficult than over the interval algebra   but it is possible to transform a set of constraints into an equivalent set of constraints if the mutual directions of all intervals are known. this enabled us to transfer some tractability results from to . if the mutual directions are not known  a tractable subset of can be identified if two potentially exponential nested problems can be shown to be tractable for the subset:  a  compute a set of mutual directions and  b  decide consistency of the resulting set of constraints. we proved this for a small but interesting subset of   but the problem is mostly open. consequently  no maximal tractable subset of has been identified so far.
　　　can also be used instead of as a basis for defining a block algebra  balbiani et al.  1 . then it is possible to reason about -dimensional blocks with intrinsic directions. remotely related to directed intervals are line segments with arbitrary directions which were analyzed by moratz .
　the spatial odyssey of the interval algebra is to be continued as follows:  1  extend to deal with intervals on parallel lines and on networks of lines   1  add qualitative and metric information on the length of intervals and on the distance between intervals  and finally  1  extend the algebra to deal with dynamic instead of just static information  e.g.  intervals move on lines with a certain velocity and sometimes switch to accessible lines. these are the desired properties of a calculus for representing and reasoning about traffic scenarios  a prototypical application of spatial intervals. hopefully  contact with applications will be made before 1...
references
 allen  1  j. f. allen. maintaining knowledge about temporal intervals. comm. acm  1 :1  1.
 balbiani et al.  1  p. balbiani  j.-f. condotta  and l. farinas del cerro. a tractable subclass of the block algebra: constraint propagation and preconvex relations. in proc. epia'1  1.
 drakengren and jonsson  1  t. drakengren and p. jonsson. a complete classification of tractability in allen's algebra relative to subsets of basic relations. aij  1 :1  1.
 garey and johnson  1  m. r. garey and d. s. johnson. computers and intractability. freeman  san francisco  ca  1.
 gerevini and renz  1  a. gerevini and j. renz. combining topological and qualitative size constraints for spatial reasoning. in proc. cp'1  1.
 ladkin and maddux  1  p. b. ladkin and r. maddux. on binary constraint problems. j.acm  1 :1  1.
 ladkin and reinefeld  1  p. b. ladkin and a. reinefeld. fast algebraic methods for interval constraint problems. annals of mathematics and artificial intelligence  1-1  1.
 moratz et al.  1  r. moratz  j. renz  and d. wolter. qualitative spatial reasoning about line segments. in proc. ecai'1  1.
 nebel and bu：rckert  1  b. nebel and h.-j. bu：rckert. reasoning about temporal relations: a maximal tractable subclass of allen's interval algebra. j.acm  1 :1  1.
 randell et al.  1  d.a. randell  z. cui  and a.g. cohn. a spatial logic based on regions and connection. in proc. kr'1  1.
 renz and nebel  1  j. renz and b. nebel. on the complexity of qualitative spatial reasoning: a maximal tractable fragment of the region connection calculus. aij  1-1 :1  1.
 
 
from images to bodies: modelling and exploiting spatial occlusion and motion parallax 
 
david randell  mark witkowski and murray shanahan 
dept. of electrical and electronic engineering 
imperial college of science  technology and medicine 
exhibition road 
london sw1bt 
united kingdom 
{d.randell  m.witkowski  m.shanahan} ic.ac.uk 
 
 	  
abstract 
this paper describes the region occlusion calculus  roc-1   that can be used to model spatial occlusion and the effects of motion parallax of arbitrary shaped objects.  roc-1 assumes the region based ontology of rcc-1 and extends galton's lines of sight calculus by allowing concave shaped objects into the modelled domain. this extension is used to describe the effects of mutually occluding bodies. the inclusion of van benthem's axiomatisation of comparative nearness facilitates reasoning about relative distances between occluding bodies. further  an envisionment table is developed to model sequences of occlusion events enabling reasoning about objects and their images formed in a changing visual field.  
1  introduction 
spatial occlusion  or interposition  arises when one object obscures the view of another. spatial occlusion is one of several visual cues we exploit to build up our awareness of three-dimensional form and distance. another is motion parallax  whereby a change in viewpoint causes relative displacements of objects at different distances in the visual field  braddick and atkinson  1 . occlusion events help us determine where an object's boundary lies  or infer why an object cannot be seen  and what we need to do in order to render it visible. 
   for example  consider two objects a and b in an agent's visual field. suppose the agent moves to its left  while keeping these objects in sight. if object a passes across b  or  when moving toward a  b becomes completely obscured  the agent can infer that a is in front of b. similarly  if  when moving to the right  no relative change arises  the agent may infer that a and b are far away  or close by and possibly moving in the same direction as itself. conversely  if a  when visible  always appears to be subtended by b  the agent may infer that a and b are physically connected. in each case  occlusion events and motion parallax are being used to derive an objective model of the world from a naturally restricted viewpoint  figure 1 . 
 
  
figure 1: spatial occlusion at work. assuming a fixed viewpoint  in the two sequences shown on the left  the smaller ball passes in front of the larger one  top sequence  and behind it  bottom sequence . on the right  occlusion events arise with a change in viewpoint. 
 
   while visual occlusion remains a topic of some interest in the machine vision literature  e.g.  plantinga and dyer  1; geiger  et al.  1   an opportunity arises to investigate occlusion within the qualitative spatial reasoning  qsr  domain. for example  galton's  lines of sight calculus outlines a theory of occlusion for modelling convex bodies using a discrete set of 1 occlusion relations. it is natural to take a topological approach to modelling occlusion  since occlusion events are very general  and apply to all objects irrespective of their size  shape and function. petrov and kuzmin  provide an axiomatisation of spatial occlusion founded on a point-based ontology. 
   randell  et al.  develop a mereo-topological theory  rcc-1  used to describe spatial relationships between regions based on the primitive relation of connection. cui  et al.  use rcc-1 to develop a qualitative simulation program to model physical processes by specifying direct topological transitions between these relations over time. their work is one example of using qualitative spatial representations to model continuous change  cohn  1 . roc-1 extends rcc-1 to reason about relative distances between bodies from occlusion events  and transitions between occlusion events to model the effects of motion parallax from both object motion and changing viewpoints. 
1  the formal theory 
our universe of discourse includes bodies  regions and points  all forming pairwise disjoint sets. in terms of interpretation  bodies denote physical objects  while regions split into two further disjoint sets that denote either three-dimensional volumes  typically the spaces occupied by bodies  or twodimensional regions  typically projected images of bodies as seen from some viewpoint . 
   for the purposes of this paper  a set of sorts and a sorted logic are assumed. within the sorted logic  possible values of variables in formulae are derived implicitly from the specified sort of the argument position in which it appears  allowing ad hoc polymorphic functions and predicates to be handled. 
   the notation and conventions used throughout this paper is as follows: type a τ1 ..  τn : τn+1 means function symbol a is well sorted when its argument sorts are τ1 ..  τn with τn+1 as the result sort  and type a τ1 ..  τn  means predicate a is well sorted when defined on argument sorts τ1 ..  τn. axioms  definitions and theorems are respectively indicated in the text  as follows:  a1 .. an    d1 .. dn   and  t1 .. tn . where axiom/definitional schema are used  the numbering in the parentheses reflects the number of object-level axioms and definitions generated  e.g.  a1-a1  would indicate that six  axioms are defined. 
1  rcc-1 
the mereo-topological theory rcc-1  randell  et al.  1  is embedded into roc-1. as with rcc-1  the same primitive dyadic relation c/1 is used: 'c x y ' is read as  x is connected with y  and is interpreted to mean that the topological closures of regions x and y share a point in common. all the relations defined in rcc-1 are used  and all carry their usual readings: dc/1  disconnected   p/1  part   eq/1  equal   o/1  overlaps   dr/1  discrete  po/1  partial overlap   ec/1  external connection   pp/1  proper part   tpp/1   tangential proper part   ntpp/1   non-tangential proper part .  pi/1  ppi/1  tppi/1 and ntpi/1 are the inverse relations for p/1  pp/1  tpp/1 and ntpp/1  respectively. of these relations  eight are provably jointly exhaustive and pairwise disjoint  jepd  and can be singled out for reasoning about state-state topological changes  cui  et al.  1 .  for brevity this set of relations is referred to as jepd rcc-1. 
   axioms for c/1 and definitions for the dyadic relations of rcc-1 are as follows: 
 
 a1    x c x x  
 a1    x y  c x y ★ c y x   
 
 d1   dc x y  《def.  c x y  
 d1   p x y  《def.  z c z x ★ c z y   
 d1   eq x y  《def. p x y  & p y x  
 d1   o x y  《def.  z p z x  & p z y   
 d1   dr x y  《def.  o x y  
 d1   po x y  《def. o x y  &  p x y  &  p y x  
 d1   ec x y 《def. c x y  &  o x y  
 d1   pp x y 《def. p x y  &  p y x  
 d1   tpp x y  《def. pp x y  &  
 z ec z x  & ec z y   
 d1   ntpp x y  《def. pp x y  &  
  z ec z x   & ec z y   
 d1  pi x y  《def. p y x  
 d1   ppi x y  《def. pp y x  
 d1   tppi x y  《def. tpp y x  
 d1   ntppi x y  《def. ntpp y x  
 
type Φ region region ; where Φ （ {c dc p eq dr po ec pp tpp ntpp pi ppi  tppi ntppi} 
 
   not reproduced here  but assumed  is an axiom in rcc-1 that guarantees every region has a nontangential proper part  a1   and a set of axioms  a1-a1  introducing boolean functions for the sum  complement  product  difference of regions  and the universal spatial region  and an axiom that introduces the sort null enabling partial functions to be handled - see  randell  et al.  1 . 
1  mapping functions and axioms 
roc-1 uses the set of dyadic relations from rcc-1 to model the spatial relationship between bodies  volumes  and images. the distinction between bodies and regions is maintained by introducing two functions: 'region x ' read as  the region occupied by x  and 'image x v ' read as  the image of x with respect to viewpoint v . the function: region/1  maps a body to the volume of space its occupies  and image/1 maps a body and a viewpoint to its image; i.e. the region defined by the set of projected half-lines originating at the viewpoint and intersecting the body  so forming part of the surface of a sphere of infinite radius centred on the viewpoint. a set of axioms incorporating these functions are defined by the following axiom schema1: 
 
 a1-a1   x y  Φ region x  region y   ★  
 v  Φ image x v  image y v     
 
type region body :region1 type image body point :region 
type Φ region region  where Φ （ {c o p pp ntpp eq} 
 
   not all of the defined rcc-1 relations are shown.  for example  given dc region a  region b   all image relationships between the a and b are possible depending on the shape of the objects and the viewpoint assumed.  this shows that these axioms function as a set of spatial constraints between bodies  a given viewpoint  and their corresponding images. this point is re-visited in section 1 below  where a change in viewpoint  or a change in the relative positions of bodies with respect to a viewpoint  is discussed. 
1  occlusion defined 
a second primitive relation: 'totallyoccludes x y v '  read as  x totally occludes y with respect to viewpoint v   is now added  and is axiomatised to be irreflexive and transitive  and is  by implication  asymmetric : 
 
 a1    x v  totallyoccludes x x v  
 a1    x y z v   totallyoccludes x y v  & 
	  	totallyoccludes y z v   ★  
totallyoccludes x z v   
 
type totallyoccludes body body point  
 
   the intended geometric meaning of total occlusion is as follows. let line p1 p1 p1  mean that points p1  p1 and p1 fall on a straight line with p1 strictly between p1 and p1. then  x totally occludes y from v iff for every point p in y  there exists a point q in x such that line v q p   and there are no points p＞ in y  and q＞ in x  such that line v p＞ q＞ . given the transitivity of total occlusion  an object x can totally occlude an object y even if x itself is totally occluded by another object. 
   several axioms are now introduced to embed rcc-1 into this theory: 
 
 a1    x y z v   totallyoccludes x y v  & 
	  	p region z  region y    ★  
totallyoccludes x z v   
 
i.e. if x totally occludes y  x totally occludes any part of y.  
 
 a1    x y v  totallyoccludes x y v  ★ 
 z p region z  region y    ★ 
 totallyoccludes z x v   
 
i.e. if x totally occludes y no part of y totally occludes x. 
 
 a1    x y v  totallyoccludes x y v  ★  z  p region z  region x   & 
 p region u  region y    ★ 
 totallyoccludes u z v    
 
i.e. if x totally occludes y no part of y totally occludes part of 
x.   
   this latter axiom excludes cases where the occluding body has parts that wrap 'behind' the occluding object.  that is to say  while some nested bodies satisfy this relation  not all do  as in the case where  for example  a body is totally enveloped by another. this particular model is an example of mutual occlusion  which is defined below in definition  d1 . 
 
 a1    x v y z p region y  region x   & 
 p region z  region x   & totallyoccludes y z v   
 
i.e. every x has a part that totally occludes another part of x.  
this axiom guarantees that bodies have 'depth'. 
 
 a1    x y v  totallyoccludes x y v  ★   	p image y v  image x v      
 
i.e. if x totally occludes y  the image of x subtends the image of y. note that  a1  is not a biconditional because the p/1 relation does not take account of relative distance  a topic to be considered shortly. 
   by separating out volumes and images  two non-identical bodies having identical images  as in the case where one body exactly occludes another  can be modelled without inconsistency.  spatial identity in terms of co-location still applies  but is restricted to the dimensionality of the regions being modelled. 
   next  the relation of occlusion is weakened to include  for example  partial occlusion: 'occludes x y v ' is read as  x occludes y from viewpoint v : 
 
 d1   occludes x y v  《def.  
 z u p region z  region x   &  
p region u  region y   &  
totallyoccludes z u v   
 
type occludes body body point  
 
i.e. x occludes y if a part of x totally occludes a part of y.   
 
   total occlusion between two objects implies occlusion  which in turn implies region overlap between their corresponding images: 
 
 t1   x y v totallyoccludes x y v  ★ occludes x y v   
 t1   x y v  occludes x y v  ★  o image x v  image y v    
 
   occludes/1 is non-symmetrical. by contrast  the o/1 relation in rcc-1 is symmetrical  which renders it unsuitable for modelling occlusion relationships. hence the need to augment rcc-1 with an additional primitive relation. 
   other more specific occlusion relations may now be defined: partial  and mutual occlusion. an example of mutual occlusion is two interlinked rings. these relations will then be finessed further by combining them with the set of rcc-1 relations: 
 
 d1   partiallyoccludes x y v  《def. occludes x y v  & 
 totallyoccludes x y v  & 
 occludes y x v  
 
type partiallyoccludes body body point  
 
i.e. x occludes  but does not totally occlude  y  but y does not occlude x. 
 
 d1   mutuallyoccludes x y v  《def. 
occludes x y v  & occludes y x v  
 
type mutuallyoccludes body body point  
 
i.e. x and y occlude each other. 
 
   for completeness  not listed here  inverse relations for occludes/1  totallyoccludes/1 and partiallyoccludes/1 are defined  d1-d1 ; leaving the null case: nonoccludes/1  where no occlusion arises:  
 
  d1  nonoccludes x y v  《def.  
 occludes x y v  &  occludes y x v  
 
type nonoccludes body body point  
 
   the six relations: nonoccludes/1  mutuallyoccludes/1; and totallyoccludes/1  partiallyoccludes/1  and their inverses are pairwise disjoint. 
   finally  these new occlusion relations must be mapped to their rcc analogues: 
 
 a1   x y v nonoccludes x y v  ★  
dr image x v  image y v    
 a1   x y v partiallyoccludes x y v  ★  
	 	 po image x v  image y v   ‥ 
	 	pp image x v  image y v     
 a1   x y v mutuallyoccludes x y v  ★  
 po image x v  image y v   ‥ 
p image x v  image y v   ‥ 
pi image x v  image y v     
1  finessing the occlusion relations 
although a variety of occlusion relations have now been defined  they are still very general  as no spatial relation stronger than p/1 from rcc-1 is used. total occlusion  for example  covers three cases:  i  where the image of the occluded body is a tangential proper part of that of the occluding body   ii  where it is a nontangential proper part  or  iii  the images are identical because one body exactly occludes the other. by refining the existing set of occlusion relations in this manner  a total set of 1 jepd relations become definable.  these are generated using the following definitional schemas: 
 
  d1-d1  ΦΨ x y v  《def.  
　　Φ x y v  & Ψ image x v  image y v    d1-d1  ΧΨ  1 x y v  《def.  
Χ y x v  & Ψ image y v  image x v   
 
type Φ body body point  
 
where if: 
Φ = nonoccludes  then Ψ （ {dc ec} 
Φ = totallyoccludes  then Ψ （ {eq tppi ntppi} Φ = partiallyoccludes  then Ψ （ {po tpp ntpp} Φ = mutuallyoccludes  then Ψ （ {po eq tpp ntpp} and where if:  
Χ = totallyoccludes  then Ψ （ {eq tppi ntppi} 
Χ = partiallyoccludes  then Ψ （{po tpp ntpp} 
Χ = mutuallyoccludes  then Ψ （ {tpp ntpp} 
 
e.g.  totallyoccludeseq x y v  《def.  
totallyoccludes x y v  & 
eq image x v  image y v   
 
totallyoccludeseq  1  x y v  《def.  totallyoccludeseq y x v  & 
eq image y v  image x v   
 
type Φ body body point  where: Φ  is an element from the  set of all 1 occlusion relations. 
 
   it is this part of the region occlusion calculus that is now  referred to as roc-1  and the set of 1 jepd relations as jepdroc-1. 
1  theory comparisons 
it is now possible to map out the relationship between rcc-1  
galton's  lines of sight calculus  los-1   and roc1. consider the jepd rcc-1 overlap relations first  i.e. {po  tpp  ntpp  eq  tppi  ntppi}. these relations are indifferent to relative distance with respect to a viewpoint  and each conflates a pair of galton's relations. for example  given only that x partially overlaps y  it is impossible to say whether x is in front of or behind y. in both los-1 and roc-1  these two cases are distinguished. 
   this leaves two rcc-1 relations {dc  ec}. these map respectively to the los-1 relations c/1  clears  and jc  just clears   and to the two roc-1 relations: nonoccludesdc/1 and nonoccludesec/1. the six remaining relations of roc1 are precisely the cases where non-convex bodies  ruled out in los-1  are allowed into the modelled domain. these correspondences are illustrated in table 1.  
   in table 1 mutually occluding objects are shown thus    indicating that the lighter coloured 'u'-shaped object both occludes and is occluded by the darker. in the special case of mutuallyoccludeseq/1 part of the darker body lies behind the lighter body  and is exactly subtended by it  while a  visible  part of this  extends through a slot in the lighter body and occludes it. 
   the formal relationship between los-1 and roc-1 is also illustrated by the following theorem: 
 
 t1   x y v   mutuallyoccludes x y v    
 nonoccludes x y v  ‥ 
totallyoccludes x y v  ‥  
partiallyoccludes x y v  ‥ 
totallyoccludes -1 x y v  ‥  
partiallyoccludes -1 x y v    
 
where the five disjuncts are provably pairwise disjoint  and where each disjunct in turn respectively splits into 1+1+1 = 1 of  the jepd roc-1 'base' relations. 
 
	nonoccludesdc	c	dc
	nonoccludesec 	jc	ec
partiallyoccludespo -1	phphi    po partiallyoccludespo
                           mutuallyoccludespo 
	partiallyoccludestpp totallyoccludestppi 	jfjhi    tpp
 
mutuallyoccludestpp 
partiallyoccludesntpp -1 fhi    ntpp totallyoccludesntppi
                           mutuallyoccludesntpp 
	totallyoccludestppi -1	jh  
	partiallyoccludestpp -1	jfi   tppi
	-1	 
mutuallyoccludestpp
	totallyoccludesntppi 	h
-1 fi    ntppi
partiallyoccludesntpp
	mutuallyoccludesntpp -1	 
	totallyoccludeseq 	eh  
	totallyoccludeseq -1	ehi   eq
 
mutuallyoccludeseq
  
table 1: the comparison between the jepd relations of roc-1  los-1 and rcc-1. in each case the dark and light objects in the model respectively maps to the x  y variables of each Φ x y v  relation of roc-1  and to each corresponding Φ' x y  relation of los-1 and rcc-1.  
1  comparative distance and occlusion 
while the notion of relative distance between bodies appears in this theory  it only forms part of the interpretation resulting from the model used  and is implicit. made explicit  a robot  for example  can exploit this information to reason about partial orderings of radial distances between itself and bodies based on their observed or inferred occlusion properties. a reworked subset of comparative distance axioms originally proposed by van benthem  is embedded into the theory.  the primitive relation: 'n x y z ' used here  is read as  point x is nearer to body y than x is to body z   while 'e x y z ' is read as  body y is as near to point x as is body z : 
 
 a1   x y z u   n x y z  & n x z u   ★ n x y u   
 a1   x y  n x y y  
 a1   x y z u  n x y z  ★  n x y u  ‥ n x u z    
 
 d1  e x y z  《def.  n x y z  &  n x z y  
 
type Φ point body body   where: Φ （ {n e} 
 
   comparative distance is related to occlusion  and is embedded into roc-1  with the following axioms: 
 
 a1   x y v  totallyoccludes x y v  ★ n v x y   
 
i.e. if x totally occludes y with respect to some viewpoint v  then x is nearer to v than y is to v. 
 
 a1   x y v  n v x y  ★   
 z p region z  region y   ★ n v x z    
 
i.e. if v is nearer to x than y  then v is nearer to x than any part of y. 
   note that the named viewpoint is not necessarily identified with an agent  and intentionally so.  for example  if the agent  holds and aligns two objects  one in each hand  where the one totally occludes the other  it does not follow the agent is  closer to the occluding object  than the one occluded. it is also because of the guiding projective geometry assumed here  and which interprets the image/1 function  that a viewpoint is identified with a point  and not an extended region in space. 
1  relative orientation 
if one body lies just to the left of another with respect to a line of sight  and is closer to the observer  movement to the right will typically increase the apparent separation between them.   the relative left-right hand positions of the bodies will reverse as the line of sight intersects both bodies and passes to the left of that point. in order to be able to model and exploit this example of motion parallax  the ternary primitive relation: 'left x y v '  read as  x is to the left of y from viewpoint v   is added and axiomatized. its dual  right/1  is also defined:1  
 
 a1  x v left x x v  
 a1  x y v  left x y v  ★  left y x v    
 a1  x y z v   left x y v  & left y z v   ★ 
left x z v   
 
 d1  right x y v  《def. left y x v  
 
type Φ body body point  where: Φ （ {left right} 
 
   for completeness  the relation: 'nonleftright x y v '  read as  x is neither to the left or right of y relative to viewpoint v   is added: 
 
 d1  nonleftright x y v  《def.  
 left x y v  &  right x y v  
 
type nonleftright body body point  
 
   here it is assumed that the observer's horizon is fixed  and that the field of view is restricted. without these assumptions  the transitivity of left/1  for example  would fail in the intended model. this would be the case if the agent were at the centre of a circular arrangement of objects  stonehenge  for example   entailing each object could be both to the left and the right of itself. 
   the primitive relation left/1 is embedded into the theory using the following axioms: 
 
 a1  x y v  left x y v  ★  
  z  p region z  region x   & left z y v   &  
  u  p region u  region x   & left y u v     
 
 a1  x y v  left x y v  ★ 
 p image x v  image y v    
 
i.e. in the first case  a1  if from v  x is left of y  some part of x projects to the left of y  while no part of x projects to the right of y; while in the second case  a1   from v  if x is left of y then x is not subtended by y. 
   it is now straightforward to see how roc-1 can be further developed. for example  where one object lies to the left of another and is disjoint  to the left and in boundary contact  and so on. all the distinct states depicted in figure 1 can then be modelled. 
1  relative viewpoints  
a change of viewpoint always carries the possibility of a change in the apparent spatial relationships holding between bodies in the domain  figure 1 . if  for example  two bodies are physically separated  and an agent is allowed to freely move around  several apparent spatial relationships may be seen to apply.  however  for two bodies forming a part-whole relation  no change in the viewpoint will coincide with both bodies separating.  these and other configuration possibilities form the basis of the set of global spatial constraints introduced in section 1. there still remains the question of singling out additional dynamic spatial constraints  this time arising from instantaneous transitions between temporally ordered sequences of occlusion events.  
   as with many discrete based qsr theories  the set of jepdroc-1 relations can be worked into an envisionment  where a set of axioms lay out the dynamic possibilities and constraints of spatial relationships deemed to hold between bodies over consecutive moments in time  cohn  1 . for roc-1 this is represented as a table  table 1  where legal/illegal  instantaneous  transitions between spatial relationships are respectively denoted by  y   yes  or  n   no  entries mapping to pairs of named occlusion relations. a path formed by linking together pairs of nodes denotes a possible projected sequence of states from an initial state  at time t  via successor states  at times t+1 ... t+n . 
   the symmetry about the highlighted diagonal indicates the symmetrical relationship between each pair of named nodes.  for example  the relation nonoccludesec/1 has five such legal transitions  as read across the named row or down the named column. this means the relation nonoccludesec/1 from time t to the next instant t+1  now re-worked as an envisionment axiom  assuming a fixed viewpoint and the continued existence of the bodies from time t to t+1   has the following form: 
 
  x y v t  holdsat nonoccludesec x y v  t  ★  holdsat nonoccludesec x y v  t+1  ‥ 
holdsat nonoccludesdc x y v  t+1  ‥ 
holdsat partiallyoccludespo x y v  t+1  ‥ holdsat partiallyoccludespo-1 x y v  t+1  ‥ 
holdsat mutuallyoccludespo x y v  t+1    
 

nonoccludesdc y y n n n n n n n n n n n n n n n n n n nonoccludesec y y y n n n n n y n n y n n n n n n n n partiallyoccludespo n y y y n y y n y y n n n n n n n y n y partiallyoccludestpp n n y y y y n n y y y n n n n n n n n y partiallyoccludesntpp n n n y y y n n n y y n n n n n n n n y totallyoccludeseq n n y y y y y y y y y n n n n n n y y y totallyoccludestppi n n y n n y y y y n n n n n n n n y y y totallyoccludesntppi n n n n n y y y n n n n n n n n n y y y mutuallyoccludespo n y y y n y y n y y n y y n y y n y n y mutuallyoccludestpp n n y y y y n n y y y y n n y y y n n y mutuallyoccludesntpp n n n y y y n n n y y n n n y y y n n y partiallyoccludespo-1 n y n n n n n n y y n y y n y y n y n y partiallyoccludestpp-1 n n n n n n n n y n n y y y y n n y y y partiallyoccludesntpp-1 n n n n n n n n n n n n y y y n n y y y totallyoccludeseq-1 n n n n n n n n y y y y y y y y y y y y totallyoccludestppi-1 n n n n n n n n y y y y n n y y y n n y totallyoccludesntppi-1 n n n n n n n n n y y n n n y y y n n y mutuallyoccludestpp-1 n n y n n y y y y n n y y y y n n y y y mutuallyoccludesntpp-1 n n n n n y y y n n n n y y y n n y y y mutuallyoccludeseq n n y y y y y y y y y y y y y y y y y y table 1: the envisionment table for roc-1 
 
   the envisionment table can be interpreted two ways: either in terms of the viewpoint changing  or where the positions of the bodies change. in the former case  an additional predicate is required: changepos v1 v1    meaning viewpoint v1 changes to viewpoint v1   which relates v1 at time t in the antecedent of the envisionment axiom to v1 at time t+1 in the consequent. these sequences of occlusion events can then be viewed as building the topology of motion parallax into the model. obviously  where orientation information is added the number of relations and nodes increase  as does the overall complexity of the new set of permissible transitions between specified named occlusion relations. 
1  discussion and conclusions 
roc-1 presents an axiomatisation of spatial occlusion. it assumes the region based ontology of rcc-1  randell  et al.  1  and extends the work of galton  by allowing both convex and concave shaped bodies. it is this extension that describes occlusion events of mutually occluding bodies. the inclusion of van benthem's  notion of comparative nearness facilitates reasoning about relative distance between occluding bodies. an envisionment table models sequences of occlusion events to enable reasoning about objects and the images that may be formed in a visual field. 
   several directions for future work are indicated. the axiomatisation of the primitive relation: totallyoccludes/1  currently rules out models where the occluding body has a part that wraps behind the occluded body. in the theory this is a case of mutual occlusion.  however  we can see potential gains by re-working the current axiomatisation  and relaxing this restriction  so that any degree of enclosure of one body by another  from some assumed viewpoint  could be a case of total occlusion.   
   additional work is required to generate the composition table  see cohn  1  for jepd subsets of the defined occlusion relations.  also of note is the question whether there are any decidable and tractable subsystems of roc-1  as has already been shown for rcc-1  bennett  1; renz and nebel  1 . further computational gains may be made by adding information about the relative size of bodies or regions acting as additional constraints when checking for consistency of sets of these relations  c.f. gerevini and renz  1 . 
   roc-1 lays the theoretical foundations for further work in cognitive robotics  in which the images of objects are used to infer the spatial arrangement of objects in a robot's world - ultimately with map building and route planning in mind.  we argue that the modelling of occlusion and motion parallax within a traditional qsr approach offers a uniform framework to achieve this. galton  has already shown these lines of sight relations can partition an idealized plan view of the embedding space into a set of polygonal regions. for each  view  point in that space exactly one of the jepd line of sight relations holds.  where objects of varying shapes and sizes exist  many named sight lines that form tangents to objects naturally intersect at points. these correspond in this theory to a conjunction of atomic formulae drawn from the set of jepd relations used. this gives rise to a set of extrinsic reference points determined completely by the objective spatial arrangement of the objects in the robot's world. with these points  localization becomes possible  while enabling qualitative and metric quantitative information to be combined. spatial constraints and envisionment axioms now lead into map building and route planning. the robot then acquires the means to plan and execute moves  levitt and lawton  1; schlieder  1  while constantly monitoring and relating its own direction of movement to the observed change and sequence of occlusion events in its visual field.   
acknowledgements 
work described here has been supported by epsrc project gr/n1   cognitive robotics ii . we wish to thank paulo santos  brandon bennett and antony galton for fruitful discussions during the development of ideas presented in this paper; and the comments given by the anonymous referees.   
references 
 braddick and atkinson  1  braddick o. j. and atkinson j.  higher functions in vision  in barlow  h. b. and mollon  j. d.  eds.  the senses. cambridge  cambridge 
university press  page 1 
 bennett  1  bennett  b.  spatial reasoning with 
propositional logic  proc. 1th int. conf. on knowledge representation and reasoning  kr-1   pages 1 
 cohn  1  cohn  a.g.  qualitative spatial representation and reasoning techniques  proc. ki-1  advances in artificial intelligence  lnai 1  springer  pages 1 
 cui  et al.  1  cui  z  cohn  a.g and randell d.a.  qualitative simulation based on a logical formalism of space and time  proc. aaai-1  pages 1 
 galton  1  galton  a.p.  lines of sight  aisb workshop on spatial and spatio-temporal reasoning. 
 geiger  et al.  1  geiger  d.  ladendorf  b. and yuille  a.  occlusions and binocular stereo  int. j. of computer vision  vol. 1  pages 1 
 gerevini and renz  1  gerevini  a and renz  j.  combining topological and qualitative size constraints for spatial reasoning  proc. 1th int. conf. on principles and practice of constraint programming  cp-1   pages 1 
  levitt and lawton  1  levitt  t.s. and lawton  d.t.  
qualitative navigation for mobile robots  artificial intelligence  vol. 1  pages 1 
 petrov and kuzmin  1  petrov  a.p. and kuzmin  l.v.  visual space geometry derived from occlusion axioms  
j. of mathematical imaging and vision  vol. 1  pages 1-
1 
 plantinga and dyer  1  plantinga  h. and dyer  c.r.  visibility  occlusion  and the aspect graph  int. j. of computer vision  vol. 1  pages 1 
 randell  et al.  1  randell  d. a.; cui  z and cohn a. g.  a spatial logic based on regions and connection  proc. 
1rd int. conf. on knowledge representation and 
reasoning  kr-1   morgan kaufmann  san mateo  pages 1 
 renz and nebel  1  renz  j. and nebel  b.  efficient methods for qualitative spatial reasoning  proc. 1th 
euro. conf. on artificial intelligence  ecai-1   john wiley & sons  pages 1 
 schlieder  1  schlieder  c.  representing visible locations for qualitative navigation  in piera carrate  n. and singh  m.g.  eds.  qualitative reasoning and 
decision technologies  barcelona: cimne  pages 1-
1 
 shanahan  1  shanahan  m.p.  robotics and the common 
sense informatic situation  proc. 1th euro. conf. on artificial intelligence  ecai-1   pages 1 
 van benthem  1  van benthem  j.  the logic of time  synthese library vol. 1  reidel  london  appendix a. 

knowledge representation
and reasoning
qualitative reasoning for biological systems

qualitative simulation of genetic regulatory networks: method and application
	hidde de jong  michel page 	celine＞	hernandez  and johannes geiselmann
inria rho ne-alpes  1 avenue de l'europe  montbonnot  1 saint ismier cedex  france
esa  universite＞ pierre mende`s france  grenoble	pegm  universite＞ joseph fourier  grenoble  france
hidde.de-jong  ce＞line.hernandez  michel.page  inrialpes.fr  johannes.geiselmann ujf-grenoble.fr

abstract
computer modeling and simulation are indispensable for understanding the functioning of an organism on a molecular level. we present an implemented method for the qualitative simulation of large and complex genetic regulatory networks. the method allows a broad range of regulatory interactions between genes to be represented and has been applied to the analysis of a real network of biological interest  the network controlling the inititation of sporulation in the bacterium b.subtilis.
1	introduction
it is now commonly accepted in biology that most interesting properties of an organism emerge from the interactions among its genes  proteins  metabolites  and other molecules. this implies that  in order to understand the functioning of an organism  the networks of interactions involved in gene regulation  metabolism  signal transduction  and other cellular and intercellular processes need to be elucidated.
　a geneticregulatorynetwork consists of a set of genes and their mutual regulatory interactions. the interactions arise from the fact that genes code for proteins that may control the expression of other genes  for instance by activating or inhibiting dna transcription  lewin  1 . the study of genetic regulatory networks has received a major impetus from the recent development of experimental techniques permitting the spatiotemporal expression levels of genes to be rapidly measured in a massively parallel way  brown and botstein  1 . however  in addition to experimental tools  computer tools for the modeling and simulation of gene regulation processes will be indispensable. as most genetic regulatory systems of interest involve many genes connected through interlocking positive and negative feedback loops  an intuitive understanding of their dynamics is hard to obtain.
　currently  only a few regulatory networks are wellunderstood on the molecular level  and quantitative information about the interactions is seldom available. this has stimulated an interest in modeling and simulation techniques developed within qualitative reasoning  qr   heidtke and schulze-kremer  1; trelease et al.  1 . a major problem with these approaches  based on well-known methods like qsim  kuipers  1  and qpt  forbus  1   is their lack of upscalability. following approaches in mathematical biology  de jong and page  have proposed a qualitative simulation method capable of handling large and complex networks.
　the aim of this paper is to generalize the latter method and to demonstrate its applicability to real networks of biological interest. the generalization of the method allows a broader range of regulatory interactions between genes to be expressed. this enables more complex systems to be analyzed  such as the network of interactions controlling the inititation of sporulation in the bacterium bacillussubtilis. we have simulated the sporulation network using a model constructed from published reports of experiments. the simulations reveal that an additional interaction  proposed in the literature before but not yet experimentally identified  may be involved.
　in the next section  we will discuss the class of equations being used to model genetic regulatory networks. the third section describes the qualitative simulation algorithm  focusing on the representation of the qualitative state of a regulatory system and the determination of state transitions by the simulation algorithm. the subsequent sections present the results of the analysis of the sporulation network as well as a discussion of the method in the context of related work.
1	modeling genetic regulatory networks
1	approximations of regulatory interactions
in order to model a genetic regulatory network  we first have to describe the regulatory interactions in an empirically valid and mathematically rigorous way. consider a dna-binding protein encoded by gene   activating the expression of a target gene . the rate of transcription of as a function of the concentration of the regulatory protein follows a sigmoid curve  yagil and yagil  1 . below a threshold concentration the gene is hardly expressed at all  whereas above this threshold its expression rapidly saturates.

figure 1: genetic regulatory network underlying the inititation of sporulation in b.subtilis. for every gene  the coding region and the promoters are shown. promoters are distinguished by the specific factor directing dna transcription. the regulatory action of a protein tending to activate  inhibit  expression is indicated by a '+'  '-' . as a notational convention  names of genes　sigmoid curves are also found in the case of more complex regulatory mechanisms. consider the proteins j and k that form a dimer repressing the transcription of gene  fig. 1 b  . analysis of a kinetic model of this regulatory mechanism reveals that the rate of expression of depends in a sigmoidal fashion on the total concentrations and of j and k  respectively. that is  both j and k need to be available above are printed in italic and names of proteins start with a capital. their threshold concentrations for	to be repressed.

	genei	 a 	genei	 b 
figure 1:  a  activation of a target gene by a regulatory protein j.  b  inhibition of by a protein complex j k.
　in the case of steep sigmoids  the combined effect of regulatory proteins on gene expression can be approximated by means of boolean functions  kauffman  1; thomas and d'ari  1 . here we will rewrite a boolean function in terms of step functions and arithmetic sums and multiplications  following the procedure of plahte etal. . for the activator protein in fig. 1 a   we thus obtain a regulation function   where is a step function
 fig. 1  and a rate parameter. similarly  the effect of a regulatory protein repressing gene can be described by
　　　　　　　　　  with	. the dimer repressor example in fig. 1 b  leads to the function	.
　although the above discussion has focused on the representation of interactions regulating the synthesis of proteins  it also applies to the degradation of proteins. sigmoid relations are observed in the latter case as well  so that the logical approximations are valid. in order to formally distinguish protein degradation rates from protein synthesis rates  we will denote the former by instead of .
1	state equations
the dynamics of genetic regulatory networks can be modeled by a system of differential equations suggested by mestl etal.   extending earlier proposals by glass and kauffman  and thomas etal. .
 1 
where is a vector of cellular protein concentrations. the stateequations  1  define the rate of change of the concentration as the difference of the rate of synthesis and the rate of degradation of the protein. exogenous variables can be defined by setting .
	the function	is defined as
 1 
where is a regulation function and a possibly empty set of indices of regulation functions. the function is defined analogously to  1   except that for reasons that will become clear below  we demand that is strictly positive. notice that for the above definitions of and   the state equations  1  are piecewise-linear. equations of this form  and their logical abstractions  have been well-studied in mathematical biology  e.g.   lewis and glass  1; mestl et al.  1; thomas and d'ari  1  .
state equation for spo1e:	state equation for abrb:
threshold inequalities:	threshold inequalities:
equilibrium inequalities:	 a 	equilibrium inequalities:	 b 
figure 1: state equations  threshold inequalities  and equilibrium inequalities for the genes  a  spo1e and  b  abrb in fig. 1.
the subscripts in the equations refer to the sporulation genes kina 	   spo1e 	   spo1a 	   siga     sigh     and abrb　eqs.  1  and  1  generalize upon the formalism employed in  de jong and page  1  in two respects. first  the regulation functions may be the mathematical equivalent of any boolean function  whereas in the earlier paper it was restricted to logical functions composed of boolean products. second  the regulation of protein degradation can now  	 .
be modeled  whereas before the degradation rate was set to
         . these extensions allow the structure of complex regulatory networks to be formalized in a convenient way.
　in fig. 1 the state equations corresponding to two of the genes in the sporulation network of fig. 1 are shown. the differential equation in  a  states that spo1eis transcribed at a rate from a -promoter when its repressor abrb is below its threshold concentration  i.e.   .
in addition  for transcription to commence  the sigma factor encoded by siganeeds to be available at a concentration above the threshold  i.e.   . spo1e degrades at a rate proportional to its own concentration.

	 a 	 b 
figure 1:  a  two-dimensional phase space divided into regulatory domains by the threshold planes. the shaded regulatory domain defined by	and
　　　　　has a target equilibrium in an adjacent regulatory domain.  b  the same phase space with switching zones around the threshold planes.
1	threshold and equilibrium inequalities
in general  a protein encoded by a gene will be involved in different interactions at different threshold concentrations. although exact numerical values will not usually be available  we can order the threshold concentrations of gene   which gives the thresholdinequalities
 1 
the parameter denotes a maximum concentration for the protein denoted by .
　for the sporulation gene abrb two thresholds are defined  and . abrb has two threshold concentrations: and . the first threshold corresponds to the repression of spo1e  sigh  and other early sporulation genes by abrb. the second threshold corresponds to the autoregulation of abrb during vegetative growth  when abrb levels are at their highest. this motivates the ordering of the abrb thresholds in
fig. 1 b :	.
　the	-dimensional threshold hyperplanes	  divide the phase space box into regions  called regulatorydomains fig. 1 . within each regulatory domain  the step function expressions in  1  can be evaluated  which reduces	and	to sums of rate constants. that is 
simplifies to some	  and to some	. the sets and collect the different synthesis and degradation rates of the protein in different domains of the phase space.
　it can be easily shown that all trajectories in a regulatory domain tend towards a single  stable steady state
　　  the target equilibrium  lying at the intersection of the hyperplanes  glass and kauffman  1;
mestl et al.  1; thomas and d'ari  1 . the target equilibrium level of the protein concentration gives an indication of the strength of gene expression in the regulatory domain.
　as in the case of threshold parameters  exact numerical values for the rate constants will not usually be available. however  it is possible order the possible target equilibrium levels of in different regions of the phase space with respect to the threshold concentrations. the resulting equilibrium inequalities define the strength of gene expression in a regulatory domain in a qualitative way  on the scale of ordered threshold concentrations. more precisely  for every  
	  we specify some	 	  such that
 1 
with special cases	and
.
inspection of the state equations of abrb shows that and	. the equilibrium level

　　the equilibrium inequalities have also been called nullclineinequalities de jong and page  1   because the equilibrium levels correspond to nullcline hyperplanes in the phase space.
       is placed above the highest abrb threshold  since otherwise the concentration of abrb would never be able to reach or maintain a level at which negative autoregulation takes place. this leads to the equilibrium inequalities
.
　the threshold and equilibrium inequalities determine the sign of in a regulatory domain  and hence the local dynamics of the system. in fact  given a regulatory domain defined in dimension by   it can be shown that  if   then everywhere inside the regulatory domain. similarly  if   then . on the other hand  if   then the sign of
in the regulatory domain is not unique  written as . in particular  on one side of the hyperplane   on the other side of the plane  and inside the plane.
1	discontinuities in state equations
the question must be raised what happens on the threshold hyperplanes separating the regulatory domains  where the step functions  and hence the state equations  are not defined. several solutions are possible for this non-trivial problem. in this paper we follow the approach of plahte etal.  and replace the discontinuous step functions by continuous ramp functions  fig. 1 . the solution of the pldes with step functions is then defined to be the solution of the des with ramp functions considered in the limit .

figure 1:	step function	and ramp function .	approaches	as	.
　the use of ramp functions divides the phase space into regulatory domains separated by switching zones  regions in which one or more have a value in the -interval around a threshold . an ex-
ample phase space with switching zones is shown in fig. 1. inside the regulatory domains  the des with ramp functions are equivalent to the pldes with step functions. outside the regulatory domains  in the switching zones  the des with ramp functions may be nonlinear functions of the concentration variables. the switching zones separating the regulatory domains vanish and approach  intersections of  the threshold hyperplanes as .
1	qualitative simulation of genetic regulatory networks
the goal of qualitativesimulation is to exploit the qualitative constraints on parameter values in order to predict the qualitative dynamics of a regulatory network. more specifically  we would like to know which regulatory domains can be reached by some solution trajectory starting in the initial regulatory domain  for parameter values consistent with the specified threshold and equilbrium inequalities. a sequence of regulatory domains thus generated gives an indication of the evolution of the functional state of the system  as transitions between regulatory domains reflect changes in the synthesis and degradation rates of proteins.
1	qualitative values  states  and behaviors
the analysis of pldes of the form  1  motivates the introduction of the qualitativevalueof a state variable and its derivative  as well as definitions of the qualitativestate and qualitativebehaviorof the system. let be the solution of pldes with step functions describing a regulatory network on the time-interval for given parameter values and initial conditions .
def. 1  qualitative value  suppose that at some it holds that lies in a regulatory domain  such that
             for every	 	and	 . the qualitative value	of	is given by the inequalities
               while the qualitative value	of is given by one of the inequalities	 	 
  depending on the sign of	in the regulatory domain.
the definition can be straightforwardly generalized to the case of regulatory domains bounded by or . notice that the qualitative value is not defined in the switching zones around the threshold hyperplanes.
def. 1  qualitative state  the qualitative state	for at	is given by the vectors	and	of qualitative values.
　a qualitative state associated with a regulatory domain can be interpreted as representing a functional state of the regulatory system. each protein concentration has a value lying between two consecutive thresholds  and is either tending towards one of the threshold values  or evolving towards a value between the thresholds.
def. 1  qualitative behavior  the qualitative
behavior of is given by the sequence of qualitative states on .
　a qualitative behavior defines a succession of qualitative states of the regulatory system. it is not difficult to show that every solution of  1  can be abstracted into a unique qualitative behavior.
1	qualitative simulation algorithm
in terms of the above definitions  the qualitative simulation procedure can be formulated as follows  kuipers  1 . given initial qualitative values   describing the initial protein concentrations   the simulation algorithm computes the initial qualitative state   and then determines all possible transitionsfrom to successor qualitative states. the generation of successor states is repeated in a recursive manner until all qualitative states reachable from the initial qualitative state have been found.
　the possible transitions from a qualitative state are determined by the rule below.
andand        figure 1: continuity constraints for the qualitative values and	of two qualitative states	and	de-
fined on adjacent regulatory domains. valid for
       the constraints can be easily generalized to the case of qualitative values and .
def. 1  state transition  let and be two qualitative states associated with adjacent regulatory domains. a transition from to is possible  if for every     such
that   the qualitative values and satisfy the continuity constraints in fig. 1.
　fig. 1 a  illustrates the application of the rule. intuitively formulated  the rule says that a transition from one qualitative state to another is possible  if a trajectory may cross the switching zone separating the regulatory domains of the qualitative states.
　a simulation algorithm based on def. 1 is described in  de jong and page  1 . the qualitative states and transitions generated by the algorithm form a statetransitiongraph. the graph may contain cycles and states without successors  which are together referred to as attractors. since the number of possible qualitative states is finite  every path in the state transition graph will reach an attractor at some point. each path running from the initial qualitative state to an attractor forms a possible qualitative behavior of the regulatory system.

	 a 	 b 
figure 1:  a  phase space with derivative vectors in the regulatory domains     and the state transitions     permitted by def. 1.  b  solution trajectory escaping through switching zones.
1	properties of simulation algorithm
given a model and initial qualitative values   what can be said about the correctness of the behaviors produced by qualitativesimulation  we define a set of possible solutions of  1  on   such that for every the numerical values of     and satisfy  1 - 1   and .
the ultimate aim of qualitative simulation is to determine the set of qualitative behaviors  such that  1  for every
there is a	  such that	 soundness   and  1 
for every	there is a	  such that
 completeness .
　unfortunately  the simulation algorithm based on def. 1 is not sound. a trajectory may enter a switching zone from a regulatory domain  and then escape through other switching zones to enter another  possibly non-adjacent regulatory domain  fig. 1 b  . as a consequence  the simulation algorithm may overlook qualitative state transitions. we found that the practical consequences of such omissions are limited  since qualitative states not directly reachable by a transition are often indirectly reachable by a sequence of transitions.
　completeness of the simulation algorithm has neither been proven nor disproven  but seems difficult to guarantee given the behavioral complexity that can be attained by models of the form  1   lewis and glass  1 .
1	genetic network analyzer
the simulation method has been implemented in java 1  in a program called gna geneticnetworkanalyzer . the program reads and parses input files specifying the model of the system  state equations  threshold and equilibrium inequalities  and the initial state. from this information it produces a state transition graph. extensions of the simulation algorithm allow all qualitative states and their transitions to be generated  as well as the completion and simulation of models with unspecified threshold and equilibrium inequalities.
　gna is accessible through a graphical user-interface  which allows the network of interactions between genes to be displayed  as well as the state transition graph resulting from the simulation. in addition  the user can analyze the attractors with their basins of attraction  and focus on qualitative behaviors to study the temporal evolution of protein concentrations in more detail  fig. 1 .
1	application: sporulation in b.subtilis
the method and its implementation have been used to study the regulatory network underlying the initiation of the sporulation process in the gram-positive soil bacterium bacillus subtilis grossman  1; hoch  1 . under conditions of nutrient deprivation  b.subtilis can decide not to divide and form a dormant  environmentally-resistant spore instead. the decision to either divide or sporulate is made by a complex regulatory network integrating various environmental  cellcyle  and metabolic signals.
　a schematic representation of the core of this network  displaying key genes and their regulatory interactions  is shown in fig. 1. the central component of the network is a phosphorylation pathway  a phosphorelay  which transfers phosphates from the kina kinase to the spo1a regulator. above a certain threshold  the phosphorylated form of spo1a  spo1a p  activates various genes that commit the bacterium to sporulation. an example is the spoiia operon  which encodes the transcription factor   essential for the development of the forespore. the flux of phosphate across the phosphorelay 

gna is available from the authors upon request.

1 states
attractors: sporulation and division1 states attractors: division1 states attractors: division1 states attractors: division	 a 	 b 
figure 1:  a  gna output for the equilibrium inequalities . the left window shows the state transition graph with a qualitative behavior running from the initial qualitative state v1 to the attractor state v1. in the right window the temporal evolution of the qualitative value of hpr and kina for this behavior can be followed.  b  summary of simulation results for an initial state expected to induce sporulation  while varying the equilibrium inequalities for spo1e. the simulations take between 1 and 1 seconds to complete on a sun ultra 1 workstation.and hence the concentration of spo1a p  is controlled by various external signals influencing the activity of pathway components. in addition  the flux of phosphate is regulated by spo1a p itself  through a number of direct and indirect feedback loops involving abrb  sini  sinr  and other genes.
　using the extensive literature on b.subtilis sporulation  and the subtilist database at the institut pasteur  we have formulated state equations and appropriate parameter inequalities for every gene in the network. in total  the mathematical model consists of 1 state equations  1 threshold inequalities  and 1 equilibrium inequalities. in the rare cases that the literature did not unambiguously determine the parameter inequalities  we have systematically explored the alternatives and selected those that permit the observed behavior of the bacterium to be reproduced.
　the behavior of b.subtilis has been simulated from a variety of initial states  reflecting different physiological conditions. for example  fig. 1 a  shows the simulation results for an initial state reflecting a perturbation of the vegetative growth conditions  when the protein kinase kina autophosphorylates in response to an external signal indicating a state of nutritional deprivation. under these conditions  a state transition graph with two attractors is produced  corresponding to states in which the bacterium continues to divide  v1  or initiates spore formation  v1 and v1 . both states may be reached  depending on the exact values of the parameters satisfying the threshold and equilibrium inequalities.
　in order to obtain result consistent with experimental data  we found that the target equilibrium concentrations of spo1e have to be placed below the lowest threshold concentrations
   . that is  we need to assume that spo1e expression levels are quite weak. when other equilibrium inequalities are chosen  the simulations predict that sporulation cannot be initiated under appropriate conditions  contrary to what is observed  fig. 1 b  . in fact  spo1e mediates the negative autoregulation of spo1a p  and thus prevents a critical concentration of spo1a p to accumulate.
　the above choice of parameter constraints is troublesome  because it implies that spo1e cannot exert any influence on the decision to sporulate  since its concentration will not reach the threshold levels above which it can block the phosphate flux through the phosphorelay. the simulation results thus suggest that the network in fig. 1  based on interactions reported in the literature  may be incomplete. as a remedy  we could postulate that an unknown signal decreases the activity of spo1e at the onset of sporulation. molecular studies of the interaction of spo1e with components of the phosporelay suggest the existence of such a cellular factor which remains as of yet unidentified  ohlsen et al.  1 .
1	discussion
we have presented an implemented method for the qualitative simulation of genetic regulatory systems that can handle large and complex networks of genes and interactions. the method is a generalization of the method in  de jong and page  1   in that it allows a larger class of regulatory relationships between genes to be modeled. in the first place  there are no restrictions on the logical functions that can be represented by  1 . this permits complex regulatory interactions to be included in the models  as illustrated by the state equation for abrb in fig. 1 b . in the second place  the regulation of protein degradation can be taken into account. although this feature of the method has not been used in the sporulation example  it turned out to be crucial in modeling the network controlling the induction of the lytic cycle following phage infection of e.coli results not shown here .
　the applicability of the method to actual regulatory networks has been demonstrated by an analysis of the large and complex network underlying the initiation of sporulation in b.subtilis. the analysis has resulted in a suggestion to complete the model compiled from the sporulation literature  which shows the potential of the method as a tool to focus further exprimentation. to our knowledge  qualitative simulation of genetic regulatory networks of the size and complexity considered in this paper has not been undertaken thus far.
　upscaling of the simulation method is achieved by modeling genetic regulatory systems by a class of piecewise-linear differential equations imposing strong constraints on the local dynamics of the system. besides in molecular genetics  pldes of this form have been used in other biological domains  for instance in population biology. in order to effectively apply the constraints  the representation of the qualitative state of the system and the simulation algorithm are adapted to the mathematical structure of the equations.
　adaptation to a specific class of models is the principal respect in which the method presented in this paper differs from well-known qr methods like qpt and qsim  forbus  1; kuipers  1 . a major difference with qsim is that the qualitative state of a regulatory system is described on a higher level of abstraction. in particular  the behavior inside a regulatory domain is abstracted into a single qualitative state  making use of the fact that inside a regulatory domain either
　　　    or . in qsim one would have to distinguish an exponentially growing number of qualitative states inside and on the boundary of a regulatory domain.
　approximating step functions by infinitely steep ramp functions allows a precise definition of the behavior of the system in the threshold planes  and hence of the possible successors of a qualitativestate. the state transition rule in def. 1 is simple and intuitively clear  but does not preserve soundness of the algorithm. even though the practical limitations of this may be limited  the development of state transition rules guaranteeing soundness is an important topic for further research.
　the b.subtilis example suggest an approach to validate hypothesized models of genetic regulatory networks. given temporal gene expression patterns observed under certain physiological conditions in wild-type or mutant strains of the bacterium  one can develop algorithms to systematically search the space of freely adjustable parameter inequalities to find constraints for which the model is able to account for the observations. extensions of this type would allow the simulation method presented in this paper to evolve into a more general modeling tool.
acknowledgments the authors would like to thank i. vatcheva and three anonymous referees for comments on a previous version of this paper.
references
 brown and botstein  1  p.a. brown and d. botstein. exploring the new world of the genome with dna microarrays. nat. genet.  1 suppl :1  1.
 de jong and page  1  h. de jong and m. page. qualitative simulation of large and complex genetic regulatory systems. in w. horn  editor  proc. 1th europ. conf. artif. intell.  ecai 1   pages 1. ios press  1.
 forbus  1  k.d. forbus. qualitative process theory. artif. intell.  1-1  1.
 glass and kauffman  1  l. glass and s.a. kauffman. the logical analysis of continuous non-linear biochemical control networks. j. theor. biol.  1-1  1.
 grossman  1  a.d. grossman. genetic networks controlling the initiation of sporulation and the development of genetic competence in bacillus subtilis. annu. rev. genet.  1-1  1.
 heidtke and schulze-kremer  1  k.r. heidtke and s. schulze-kremer. design and implementation of a qualitative simulation model of phage infection. bioinformatics  1 :1  1.
 hoch  1  j.a. hoch. spo1genes  the phosphorelay  and the initiation of sporulation. in a.l. sonenshein etal.  editor  bacillussubtilisand other gram-positive bacteria  pages 1. ams  1.
 kauffman  1  s.a. kauffman. the origins of order. oxford university press  1.
 kuipers  1  b. kuipers. qualitative reasoning. mit press  1.
 lewin  1  b. lewin. genes vii. oxford university press  1.
 lewis and glass  1  j.e. lewis and l. glass. steady states  limit cycles  and chaos in models of complex biological networks. int. j. bifurcation chaos  1 :1  1.
 mestl et al.  1  t. mestl  e. plahte  and s.w. omholt. a mathematical framework for describing and analysing gene regulatory networks. j. theor. biol.  1-1  1.
 ohlsen et al.  1  k.l. ohlsen  j.k. grimsley  and j.a. hoch. deactivation of the sporulation transcription factor spo1a by the spo1e protein phosphatase. proc. natl acad. sci. usa  1-1  1.
 plahte et al.  1  e. plahte  t. mestl  and s.w. omholt. global analysis of steady points for systems of differential equations with sigmoid interactions. dyn. stab. syst.  1 :1  1.
 plahte et al.  1  e. plahte  t. mestl  and s.w. omholt. a methodological basis for description and analysis of systems with complex switch-like interactions. j. math. biol.  1-1  1.
 thomas and d'ari  1  r. thomas and r. d'ari. biological feedback. crc press  1.
 trelease et al.  1  r.b. trelease  r.a. henderson  and j.b. park. a qualitative process system for modeling nf- b and ap-1 gene regulation in immune cell biology research. artif. intell. med.  1-1  1.
 yagil and yagil  1  g. yagil and e. yagil. on the relation between effector concentration and the rate of induced enzyme synthesis. biophys. j.  1-1  1.
discrimination of semi-quantitative models by experiment selection: method and application in population biology
ivayla vatcheva1 olivier bernard1	hidde de jong1 jean-luc gouze＞1 nicolaas j.i. mars1
1department of computer science	1inria sophia antipolis	1inria rho ne-alpes
	university of twente	1 route des lucioles  bp 1  avenue de l'europe
p.o. box 1  1 ae enschede	1 sophia antipolis	1 montbonnot saint martin
	the netherlands	france	franceabstract
modeling an experimental system often results in a number of alternative models that are justified equally well by the experimental data. in order to discriminate between these models  additional experiments are needed. we present a method for the discrimination of models in the form of semiquantitative differential equations. the method is a generalization of previous work in model discrimination. it is based on an entropy criterion for the selection of the most informative experiment which can handle cases where the models predict multiple qualitative behaviors. the applicability of the method is demonstrated on a real-life example  the discrimination of a set of competing models of the growth of phytoplankton in a bioreactor.
1	introduction
obtaining an adequate model of an experimental system is a laborious and error-prone task. in many cases one arrives at a number of rival models that are justified equally well by the experimental data. in order to discriminate between these models  additional experiments are needed. since in real-life applications the number of experiments to perform may be quite large  and the costs of each of them considerable  it is important that the experiments be selected carefully. in fact  experiments need to be chosen such that the set of possible models is maximally reduced at minimal costs.
　for experimental systems described by differential equations  several approaches for modeldiscriminationhave been proposedin the literature  e.g.  espie and macchietto  1  . with few exceptions  e.g.  struss  1; vatcheva et al.  1    these methods apply to completely specified quantitative models. that is  they cannot be used when precise numerical values for the parameters are not available and the precise form of functional relations is unknown.
　this has motivated the development of a method for model discrimination that is able to handle incompletely specified models in the form of semi-quantitativedifferentialequations  sqdes . the method is based on an entropy criterion for the selection of the most informative discriminatory experiment. this experiment is determined from the behavioral predictions obtainedfrom the competingmodels throughsimulation under various experimental conditions.
　in earlier work  we have developed a method for the discrimination of semi-quantitative models  vatcheva et al.  1 . however  the previously proposed approach is restricted to the case that all models predict the same qualitative behavior  a situation rarely occuring in the case of more complex models. the method described in this paper is a generalization of the approach above in that it allows one to deal with situations in which multiple qualitative behaviors are predicted.
　the applicability of the method is demonstrated on a real problem in population biology  the selection of experiments to discriminate between competing models of the growth of phytoplankton in a bioreactor. the choice of good discriminatory experiments is critical in this application  since the experiments may take several weeks to complete. semiquantitative models are appropriate  because the available data is incomplete and imprecise  as for most biological systems. we have compared the optimal experiment as determined by our method with an experiment that has been actually carried out. furthermore  taking into account the results of the latter experiment  the best next experiment to perform has been suggested.
　the paper is organized as follows. the next section deals with the basic concepts of semi-quantitative modeling and simulation. sec. 1 gives an outline of the method for model discrimination  focusing on the criterion for selecting the most informative experiment. the results of the application of the method to the modeling of phytoplankton growth in a bioreactor are presented in sec. 1. the final section discusses achievements and limitations of our work in the context of related work on model discrimination and gives some perspectives on further research.
1	semi-quantitative modeling and simulation
semi-quantitativedifferentialequations sqdes are abstractions of ordinary differential equations  odes  that allow incompletely or imprecisely specified dynamical systems to be modeled  berleant and kuipers  1 . in an sqde  bounding envelopes are defined for unknown functions  as well as numerical intervals to bound the values of parameters and initial conditions.
　fig. 1 shows an example of a second-order sqde describing the growth of the microalgae dunaliellatertiolectaunder monod xb = μ s x   dx sb = d sin   s    ρ s x s
μ s  = μmax s + ks
1
ρ s  = μ s  y
d （  1 1   sin （  1  y （  1 1   μmax （  1 1   ks （  1 1 
figure 1: an example of an sqde describing the growth of phytoplankton in a bioreactor  the monodmodel. the physical meaning of the variables and parameters is given in the caption of fig. 1.
nutrient limitation in a bioreactor  monod  1 . the state variables are the biomass x and the concentration of the limiting nutrient s. the intervals bounding the model parameters μmax  ks  and y have been estimated from preliminary experiments.
　for the simulation of sqdes we employ the techniques q1+q1  berleant and kuipers  1   which refine the qualitative behavior tree produced by the qsim algorithm  kuipers  1 . the results of semi-quantitative simulation consist of one or more qualitative behaviors supplemented by ranges for the values of the variables at qualitatively significant time-points. the behaviors resulting from the simulation of the sqde in fig. 1 are shown in fig. 1. in order to narrow down the interval predictions  we use the comparative analysis technique sqca  vatcheva and de jong  1 . sqca refines the simulation results by comparing a behavior predicted for one experiment with behaviors predicted for related experiments.
1	method for model discrimination
the predictions obtained through semi-quantitative simulation can be exploited to maximally discriminate a set of competing models against minimal costs. the method for achieving this is based on a generalization of the entropy criterion for the most informative experiment developed in  vatcheva et al.  1 .
1	model discrimination and behavior predictions
consider a set m of competing models of an experimental system. each mi （ m has a probability p mi  to be the correct model of the system  and we assume that mi（m p mi  = 1  fedorov  1 . the apriori model probabilities are estimated from data obtained in preliminary experiments or assumed equal when no such data exist. each time an experiment is executed  and new data becomes available  the model probabilities are being updated. if the data does not justify the predictions of some mi  its aposteriori probability becomes zero.
	t1	t1	t1	t1
b1
t1	t1	t1	t1	t1	t1	t1	t1	t1	t1
b1
s   〜1 x smaxb1 1  1  1 -b1 1  1  1  1  1 b1 1  1  1 -smin  〜1 xmaxb1--b1--b1 1  1  1 figure 1: behaviors resulting from the simulation of the sqde in fig. 1  for the initial conditions x1 （  1   s1 （  1 1 . behavior b1 predicts that the system reaches its equilibrium  x  s   asymptotically. in b1  the nutrient concentration s reaches a maximum  before the system approaches its equilibrium. in b1  x reaches first a maximum  followed by a minimum of s and the equilibrium. the table summarizes the interval predictions for each of the three behaviors.
　for the discrimination of the models in m  experiments from a set e of experiments need to be selected. the experiment that discriminates best between the models is estimated from the model predictions. for each experiment e （ e  the models in m are perturbedaccording to e  and then simulated to predict the behavior of the system under the experimental conditions. the prediction of mi for some e is a set of behaviors bie. the set of all qualitatively distinct behaviors resulting from the simulation of the models in m for e is denoted by be.
　for discrimination purposes  only certain characteristics of the predicted behaviors b （ be are taken into account. this gives rise to a set of behavioralfeaturesfb for b. the set of behavioral feature consists of minima  maxima and equilibria of the system variables. the behavioral features defined for b1 in fig. 1  for instance  are the maximum of x and the minimum of s  xmax and smin   and the steady state levels of these variables  s  and x  . here we will assume that the value of a behavioral feature is an interval.
　intuitively  the experiment that can be expected to optimally discriminate between the models is the experiment for which the predicted values of the behavioral features overlap least. this intuition will be formalized below by defining the mostinformativeexperiment. a set of competing models can then be discriminated by repeatedly determining the most informative experiment  performing this experiment  and updating the model probabilities in the light of the outcomes.
1	criterion for most informative experiment
a standard measure in information theory is the information increment of an experiment  fedorov  1 . for every experiment e （ e we define
 h be y e  =   xi（ p mi lnp mi  +
	m	m
	xi（ p mi | be y e lnp mi | be y e  	 1 
	m	m
where are the apriori and apos-
teriori probabilities of mi. be is the behavior of the system observed in e  and y e is the vector of observations for the behavioral features fbe. the observations are assumed to be intervals yje =  yje   εj/1 yje + εj/1   where yje is the midpoint  and εj is the estimated size of the confidence interval for the jth behavioral feature. for clarity of presentation  we will assume for the moment  that each behavior b is characterized by a single feature.
　 h reaches its maximum when all posterior probabilities but one are zero. that is  when the observations obtained in e confirm the predictions of a single model. on the other hand  a minimal value is attained  when all posterior probabilities are equal.
　since the aposteriori probabilities of the models depend on the outcome of the experiment which is not yet known   h cannot be computed directly. instead  we can compute its expected value  or the expectedinformationincrement of e:
		 1 
where be is the set of predicted behaviors  y =  y ε/1 y+ ε/1   d the domain of the behavioral feature  and ge y b  its probability distribution:
ge y b  = xi（ p mi p b|mi gie y b .
　　　　　　　　　　　　m m p b|mi  is the probability of behavior b provided mi is the correct model of the system  and gie y b  is the modelspecific probability distribution of the behavioral feature  defined by
 
with vie the interval prediction of mi for the behavioral feature in experiment e  and | ， | denoting interval length. gie y b  expresses the probability that the value of the behavioral feature is y   if mi is the correct model of the system and b is the system behavior. if the interval y does not overlap with the model prediction vie  the probability of the feature having value y is zero. otherwise  the probability is weighted according to the size of the overlap between y and
vie.
　by substituting the expression for  h in  1  and using the bayes' rule
 
we arrive at the following expression for the expected information increment of an experiment e:
 j e  = xi（ p mi  x（ e p b|mi 
	m	m	b bi
	e	gie y b p b|mi 
　　gi y b ln	ge y b 	dy.	 1  y（d
　the criterion ranks the experiments in e according to their expected informativeness. the optimal discriminatory experiment will be the mostinformativeexperiment  that is  the experiments for which  j e  is maximal. intuitively  experiments which give rise to predictions as different as possible will be favored. fig. 1 a - b  shows the predictions of the four models given in figs. 1 and 1 for two different experiments  see next section . in both cases  each of the models predicts two possible qualitative behaviors for the biomass x. the expected information increment for the first experiment  however  is higher than the expected information increment for the second   j = 1 versus  j = 1   as the predicted intervals overlap less.
　the expression for  j can be simplified in a number of cases. for instance  if all models predict for a given experiment e the same qualitative behavior   1  can be reduced to

which is the criterion previously derived by  vatcheva et al.  1 .
　on the other hand  if for a given e  each model predicts a different set of qualitative behaviors  we obtain:
 j e  =   xi（ p mi lnp mi  
	m	m
which is the maximum value  j e  can take.
　the criterion  1  is easily generalizable to the case when each behavior b is characterized by more than one feature. in this case we have to substitute the probability distributions by joint probability distributions  and the integral by a multiple integral of the k behavioral features. for computational simplicity  we assume in this article that the behavioral features are independent.
contois
xb = μ s x   dx sb = d sin   s    ρ s x s
μ s  = μmax
s + kxx
1　　droop xb = μ q x   dx
qb = ρ s    μ q q sb = d sin   s    ρ s x k
	μ q  = μ． 1  	q  
q   caperon-meyer xb = μ q x   dx
   qb = ρ s    μ q q sb = d sin   s    ρ s x q   k μ q  = μmax	q q   k + k1ρ s  = μ s 	q y	s s
	ρ s  = ρmax	ρ s  = ρmax
	s + ks	s + ks
y （  1  1   kx （  1  1   μ． （  1 1   kq （  1 1   μmax （  1  1   ρmax （  1  1   k1 （  1  1 
figure 1: models for the growth of phytoplankton in a bioreactor. x is the amount of biomass per unit volume  s μmol/l  the nutrient concentration  q μmol/l  the internal quota. the monod and contois models assume constant growth yield y l/μmol . μmax day 1  is the maximum growth rate of cells  and μ． day 1  a theoretical maximum growth rate obtained for infinite quota. ks kx  and k1 μmol/l  are half-saturation constants  kq μmol/l  is the minimum cell quota  ρmax μmol/l/day  is the maximum uptake rate of nutrients. for all models sin （  1  μmol/l is the input nutrient concentration  and d day 1  the dilution rate to be controlled in the experiments. the initial conditions are x1 （  1 1   s1 （  1 1   q1 （  1 1  which are the equilibrium values reached for the initial dilution rate d1 = 1.　the algorithms for the simulation of sqdes  outlined in the previous section have been proven sound. that is  all possible predictions are derived from a given sqde model. if the results obtained in the experiment are correct  this guarantees that a model will never be falsely rejected. however  these algorithms do not exclude spurious predictions. as a consequence an experimentalresult may corroboratea model while it should be ruled out. spurious predictions  therefore  may prolong the discrimination process.
1	computation of behavior probabilities
in order to compute  j e   the conditional behavior probabilities p bj|mi  must be estimated. we have adopted the following approach. let vl be a parameter or initial condition in model mi  and let range vl bj  be the interval value of vl for which behavior bj is obtained. define
.
r bj|mi  estimates the fraction of the interval volume of the model parameters which gives rise to bj. the conditional normalizing the r bj|mi s:
 	i .
i
　consider  for instance  the three behaviors given in fig. 1. b1  b1 and b1 have been obtained for different subintervals of the interval ranges for ks and y:
behksyb1 1 1  1 1 b1 1 1  1 1 b1 1 1  1 1 　by using these values  the procedure outlined above gives r b1  = 1  r b1  = 1  r b1  = 1. consequently  the behavior probabilities can be computed: p b1  = 1  p b1  = 1  p b1  = 1.
1	application: phytoplankton growth
understanding the regulation of phytoplankton growth is essential for predicting how life in the ocean may respond to climate changes. as these processes are difficult to study in the open sea  the growth conditions are recreated in the laboratory in a bioreactor.
　a variety of models can be used to describe the growth of phytoplankton in a bioreactor. which of these applies best in a given situation cannot be determined on apriori grounds. therefore  experiments need to be performed to discriminate between the alternative models. unfortunately  these experiments may take weeks to complete and are thus quite costly to perform.
　we have applied the method of the previous section in the context of the microalgae d.tertiolecta  carried out by population biologists in a marine laboratory. four alternative models to describe the system have been considered  which are shown in figs. 1 and 1. the models make different assumptions about the nutrient consumption  the influence of the biomass on the growth rate of the population  and the relation between growth and uptake rates. the models are labeled after their originators: m  monod  1   c  contois  1   d  droop  1   and cm  caperon and meyer  1 .
　because of coarse and noisy data  and evolution of the system in the time frame of the experiment  precise numerical estimations for the values of the parameters cannot be obtained. this motivates the use of semi-quantitative models. the interval values for the parameters required in the sqdes have been estimated by the biologists  based on the outcome of preliminary experiments  see fig. 1 .
　　　　　　　　　　　　　　　　b b1	cm 1
c	d m
x 
d
	1	1
	 a 	 b 
figure 1:  a  interval predictions of the four competing models  figs. 1 and 1  for the behavioral features x   behavior b1   and x  xmin  behavior b1  in the predicted optimal discriminatory experiment  d = 1 .  b  interval predictions for x   behavior b1   and x  xmin  behavior b1  in the experiment that has been carried out  d = 1 . the values for x  xmin measured in the experiment are also shown by a small rectangle.  c  predictions for the features x   behavior b1   and x  xmax  behavior b1  in the next optimal experiment  d = 1 . m  c  d  and cm stand for the monod  contois  droop  and caperon-meyer models 　in orderto discriminate between the competingmodels  the value of the dilution rate d can be varied by the experimenter. starting from an equilibrium  the dilution rate is changed and respectively.
d = 1d = 1d = 1d = 1d = 1 j11111d = 1d = 1d = 1d = 1d = 1 j11111table 1: values for the expected information increment  j for each of the dilution rate experiments.
d = 1d = 1d = 1p b1 p b1 p b1 p b1 p b1 p b1 m111111c111111d111111cm111111table 1: conditional probabilities of the behaviors b1 and b1 predicted by the four models for the experiments d = 1  d = 1  and d = 1  see fig. 1 . p b1  and p b1  have been estimated using the approach in sec. 1.
the transient behavior of the system towards a new equilibrium is observed. we have considered ten experiments  corresponding to equispaced values in the range  1.1 : d = 1  d = 1  .... taking into account 1% measurement uncertainty  the values of d become intervals.
　the only variable that can be reliably measured in the course of the experiment is the biomass x. this determines the behavioral features that we have considered: the minimum and the maximum value of x  xmin and xmax   and the equilibrium value of x  x  . in order to obtain the predicted values of the behavioral features required for the determination of the most informative experiment  the models have been simulated using the techniques in sec. 1. for each experiment  all models predict multiple qualitative behaviors as a consequence of the large intervals for the parameter values. in total  four different behaviors for x are predicted. none of these behaviors is spurious  as we have been able to establish by comparing the predictions with the qualitative analysis of  bernard and gouze＞  1 .
　starting from the assumption that the models are equiprobable in the beginning  we have calculated the expected information increment  1  for each of the experiments  table 1 . the optimal discriminatory experiment is predicted to be d = 1. for this experiment  each of the four models predicts two behaviors  b1 and b1  that differ with respect to the observable variable x. in b1 the equilibrium of the system is reached asymptotically  whereas in b1  x reaches a minimum before the equilibriumis attained. fig. 1 a  shows the interval predictions of the behavioral features for all four models  and table 1 lists the corresponding conditional behavior probabilities. notice that in b1 only one behavioral feature applies  x    whereas in b1 predictions for x  and xmin need to be taken into account.
　the experiment d = 1 has not been performed  but data for the suboptimal experiment d = 1 was available from an earlier study. the predictions of the behavioral features for this experiment are shown in fig. 1 b  and the behavior probabilities are given in table 1.
　in the experiment d = 1  x was found to reach its equilibrium after passing through a minimum. this rules out b1. the measured values of the behavioral features  shown in fig. 1 b   are xmin =  1 1   and x  =  1 1 . using these results  the aposterioriprobabilities of the models have been computed via bayes' rule: p m  = 1  p c  = 1  p d  = 1  and p cm  = 1. in addition  the measurements have allowed the parameter values to be refined by means of the constraint propagation algorithm in q1  berleant and kuipers  1 .
　the new model probabilities show that experiment d = 1 has not been very discriminating. given the new model probabilities and parameter values  what would be the optimal experiment to perform next  the method advises that d = 1 be tried  as it has the highest expected information increment   j = 1 . the predicted values for the behavioral features  again for two behaviors  are shown in fig. 1 c . table 1 gives the corresponding behavior probabilities. the experiment d = 1 has not been performed yet  we recall that each experiment takes weeks to complete . notice  however  that d = 1 is likely to rule out at least two of the four models due to the lack of overlap between m   c and d   cm.
1	discussion
we have proposed a method for the discrimination of semiquantitative models of an experimental system. the method is based on an entropy criterion for the selection of the most informative experiment. the value of  j for a particular experiment is calculated from the model predictions obtained through semi-quantitative simulation. the method generalizes upon a previous method  vatcheva et al.  1   in that it can handle cases where the models predict multiple qualitative behaviors. this occurs in the case of the phytoplankton growth models  which predict the biomass to asymptotically approach its equilibrium value or to pass through a maximum or a minimum first.
　the applicability of the method has been demonstrated by having it predict the most informative experiment to discriminate between four models of the growth of d.tertiolecta in a bioreactor. this has been achieved in the presence of several complicating factors  in particular the nonlinearity of the models  the crudeestimations of the parametervalues  and the difficulty to observe the behavior of the system. the discrimination of bioreactor models has been attempted before  espie and macchietto  1; cooney and mcdonald  1   but unlike the method discussed in this paper  these approaches require precise numerical data to be available  a requirement that usually cannot be fulfilled in practice.
　within ai  methods for model discrimination have been developed in the field of model-based diagnosis  e.g.   struss  1; de kleer  1  . basically  these methods determine which inputs need to be applied to a faulty device  and which measurements need to be made  in order to optimally discriminate between a number of diagnoses. in comparison with the approach in this paper  these methods have been adapted to qualitative models. by considering only qualitative distinctions  however  one may fail to discriminate between alternative behaviors. although two models may predict the same qualitative behavior  their  semi- quantitative predictions may be different  as clearly shown in fig. 1.
　for the discrimination of the phytoplankton growth models  only one type of experiment was available  a change in the dilution rate. it should be emphasized  though  that the method is not restricted to parameter changes and may even involve structural changes of the models. a limitation of the method  however  is that the set of experiments needs to be specified in advance. in the case of the dilution rate experiments  for instance  ten possible values from a continuous range have been selected. there is obviously no guarantee that the optimal value is included in the list of prespecified experiments. a subject for further research would therefore be to handle continuousranges of experiments and moregenerally  to move away from the selectionof experiments to the designof experiments.
references
 berleant and kuipers  1  d. berleant and b. kuipers. qualitative and quantitative simulation: bridging the gap. artif. intell.  1-1  1.
 bernard and gouze＞  1  o. bernard and j.-l. gouze＞. transient behavior of biological loop models with application to the droop model. math. biosci.  1-1  1.
 caperon and meyer  1  j. caperon and j. meyer. nitrogen-limited growth of marine phytoplankton.i. changes in population characteristics with steady-state growth rate. deep-sea res.  1-1  1.
 contois  1  d.e. contois. kinetics of bacterial growth: relationship between population density and specific growth rate of continuous cultures. j. gen. microbiol.  1-1  1.
 cooney and mcdonald  1  m.j. cooney and k.a. mcdonald. optimal dynamic experiments for bioreactor model discrimination. appl. microbiol. biot.  1-1  1.
 de kleer  1  j. de kleer. using crude probability estimates to guide diagnosis. artif. intell.  1-1  1.
 droop  1  m.r. droop. vitamin b1 and marine ecology. iv. the kinetics of uptake growth and inhibition in monochrysislutheri. j. mar. biol. assoc.  1 :1  1.
 espie and macchietto  1  d. espie and s. macchietto. the optimal design of dynamic experiments. aiche  1 :1  1.
 fedorov  1  v.v. fedorov. theory of optimal experiments. academic press  1.
 kuipers  1  b. kuipers. qualitative reasoning. mit press  1.
 monod  1  j. monod. recherches sur la croissance des cultures bacteriennes＞ . herman and cie  1.
 struss  1  p. struss. testing for discrimination of diagnoses. in working notes dx-1  1.
 vatcheva and de jong  1  i. vatcheva and h. de jong. semi-quantitative comparative analysis. in proceedings ijcai'1  pages 1  1.
 vatcheva et al.  1  i. vatcheva  h. de jong  and n.j.i. mars. selection of perturbationexperimentsfor model discrimination. in proceedings ecai-1  pages 1  1.

knowledge representation
and reasoning
temporal reasoning

a complete classification of complexity in allen's algebra in the presence of a non-trivial basic relation
	andrei krokhin	and peter jeavons
oxford university computing laboratory
wolfson building  parks road  ox1qd oxford  uk andrei.krokhin  peter.jeavons  comlab.ox.ac.uk
peter jonsson
department of computer and information science
linko：ping university  s-1 linko：ping  sweden email: peter.jonsson ida.liu.se

abstract
we study fragments of allen's algebra that contain a basic relation distinct from the equality relation. we prove that such a fragment is either npcomplete or else contained in some already known tractable subalgebra. we obtain this result by giving a new uniform description of known maximal tractable subalgebras and then systematically using an algebraic technique for description of maximal subalgebras with a given property. this approach avoids the need for extensive computerassisted search.
1	introduction
allen's interval algebra  allen  1  is one of the best established formalisms for temporal reasoning. such reasoning is an important task in many areas of computer science and artificial intelligence  see  e.g.   golumbic and shamir  1; no：kel  1  . allen's algebra allows us to specify qualitative information about time intervals. this algebra has also become the kernel of some other formalisms  meiri  1; angelsmark and jonsson  1 . the interval algebra and some of its extensions are closely related with a number of interval-based temporal logics used for real-time system specification  bellini et al.  1 .
　the basic satisfiability problem in allen's algebra is npcomplete  vilain et al.  1    so it is unlikely that efficient algorithms exists for reasoning in the full algebra. the computational difficulty has motivated the search for effective heuristics  e.g.   ladkin and reinefeld  1   and the study of the complexityof fragmentsof the algebra  e.g.   golumbic and shamir  1; nebel and bu：rckert  1 .

　　this research is partially supported by the uk epsrc grants gr/r1 and gr/m1.
　　this research is partially supported by the swedish research council for engineering sciences  tfr  under grants 1 and 1.
　in this paper we follow the second approach  and we also assume throughout that p np. research in this direction has focused on identifying maximal tractable fragments  i.e.  fragments which cannot be extended without losing tractability. so far  eighteen maximal tractable fragments of the algebra have been identified  drakengren and jonsson  1a; 1b; nebel and bu：rckert  1 . it is still unknown whether any others exist.
　a complete classification of complexity within a certain large part of allen's algebra was obtained in  drakengren and jonsson  1 . this result  as well as most similar results  was achieved by computer-assisted exhaustive search. however  it was noted in  drakengrenand jonsson  1 that  for further progress  theoretical studies of the structure of allen's algebra are required. there are some theoretical investigations of the structure of allen's algebra   see  e.g.   hirsch  1; ladkin and maddux  1  . however they generally allow more operations on relations than originally used in  allen  1   which makes them inappropriate for classifying complexity within the interval algebra. in this paper we systematically use an algebraic method that is similar to the approach taken in  ligozat  1 .
　the first novel element in our approach is a new uniform description for all of the maximal tractable subalgebras of allen's algebra which have already been identified  table 1 . then we exhibit several new small np-complete fragments of the algebra  including the first examples of np-complete singletons  proposition 1 . finally  we fully exploit the algebraic properties of allen's algebra by importing a technique from general algebra. this technique has been used in many other contexts to obtain a description of maximal subalgebras of a given algebra with a given property  e.g.   szendrei  1  . here  for the first time  we systematically apply this technique to allen's algebra to obtain a complete classification of the complexity of those fragments of the algebra containing a non-trivial basic relation  theorem 1 . our method also provides  as a by-product a new elementary proof of maximality of ten tractable subalgebras. in previous papers this proof of maximality has required extensive computer-aided case anal-
basic relationexampleendpoints	precedes	pxxx yyy	preceded by	p	meets	mxxxx yyyy	met by	m	overlaps	oxxxx yyyy 	overl. by	o	during	dxxx yyyyyyy 	includes	d	starts	sxxx yyyyyyy 	started by	s	finishes	fxxx yyyyyyy 	finished by	fequalsxxxx yyyy table 1: the thirteen basic relations. the endpoint relations and that are valid for all relations have been omitted.
ysis  drakengren and jonsson  1a; 1b .
　the paper is organized as follows: in section 1 we give the basic definitions of allen's algebra and present the known maximal tractable subalgebras in the new form. section 1 contains the new classification result  and section 1 contains a brief discussion of future extensions.
1	allen's algebra
allen's interval algebra  allen  1  is based on the notion of relations between intervals. an interval is represented as a tuple of real numbers with   denoting the left and right endpoints of the interval  respectively. the possible relations between intervals are the possible unions of 1 basic interval relations  which are shown in table 1. relations between intervals will also be written as collections of basic relations so  for instance  the union of the basic relations p  m and f is written p m f . allen's algebra consists of the possible relations between intervals together with the operations converse   intersection and composition which are defined as follows:
it follows that the converse of	is equal to
           . the intersection of two relations can be expressed as the usual set-theoretic intersection. using the definition of composition  it can be derived that
all algebraic calculations given in section 1 can be done by hand; however  a simple table showing the results of the composition operation on all pairs of basic relations  which can be found  e.g.  in  ladkin and maddux  1   makes such calculations considerably easier.
　the problem of satisfiability   -sat  of a set of interval variables with relations between them is that of deciding whether there exists an assignment of intervals on the real line for the interval variables  such that all of the relations between the intervals are satisfied. this is defined as follows.
definition 1 let be a set of interval relations. an instance of -sat    is a set    of variables and a set of constraints of the form where and .
the question is whether is satisfiable  i.e.  whether there exists a function    from to the set of all intervals such that holds for every constraint in .
　if there exists a polynomial-time algorithm solving all instances of -sat    then we say that is tractable. on the other hand  if -sat    is np-complete then we say that is np-complete. since the problem -sat    is npcomplete  vilain et al.  1   there arises the question of description of tractable subsets of allen's algebra.
　subsets of that are closed under the operations of intersection  converse and composition are said to be subalgebras.
for a given subset of   the smallest subalgebracontaining is called the subalgebra generated by and is denoted by
   . it is easy to see that is obtained from by adding all relations that can be obtained from the relations in by using the three operations of .
　it is known  nebel and bu：rckert  1   and easy to prove  that  for every   the problem -sat    is polynomially equivalent to -sat   . therefore  to classify the complexity of all subsets of it is only necessary to consider subalgebras of . obviously  adding relations to a subalgebra can only increase the complexity of the corresponding satisfiability problem. thus  since is finite  the problem of describing tractability in can be reduced to the problem of describing the maximal tractable subalgebras in   i.e.  subalgebras that cannot be extended without losing tractability.
　the known maximal tractable subalgebras  drakengren and jonsson  1a; 1b; nebel and bu：rckert  1  are presented in table 1. in our proofs as well as in table 1 we use the symbol   which should be interpreted as follows. a condition involving means the conjunction of two conditions: one corresponding to and one corresponding to . for example  condition o d means that both o d and o d hold.
the main advantage of using the symbol is conciseness: in any subalgebra of   the and the conditions are satisfied  or not satisfied  simultaneously  and  therefore only one of them needs to be verified.
　in previous papers  the subalgebras from table 1 were defined in other ways. however  in all cases except for   it is very straightforward to verify that our definitions are equivalent to the original ones. the subalgebra was originally defined as the 'ord-horn algebra'  nebel and bu：rckert  1   but has also been characterized as the algebra of 'pre-convex' relations  see  e.g.   ligozat  1  . using the latter description it is not hard to show that our definition of is equivalent. finally  we note that there is one more known maximal tractable subalgebra that is not included in table 1. that subalgebra is defined as and the only basic relation it contains is .
p pmod f p p pmods p d pmod f d d pmods d o pmod f o o pmods o pmod f s pmods f pmod f s pmods f pmodf s pmod s f pmodf s pmod s f
	pmod	s	  and	pmod	f	  and
	ff	ss
os o f d   and ds d f o   and pm	pm o
table 1: maximal tractable subalgebras of allen's algebra.1	main result
throughout the paper  denotes a subalgebra of . we denote by the set of basic relations in . our result is an extension of the following proposition which was proved in  drakengren and jonsson  1 .
proposition 1
1  let be a subalgebra of with . then is tractable if and only if it is contained in one of the following
1 algebras:	p 	d 	o 	p 	d 	o  and	. otherwise	is
np-complete.
1  let be a subalgebra of such that m or p . if is tractable then p  or p  or .
otherwise	is np-complete.
　we improve this result by showing that any so far unknown tractable subalgebra can contain only a trivial basic relation. throughout the paper  by a trivial basic relation we mean the equality relation .
theorem 1 suppose is a subalgebra of containing a non-trivial basic relation. then is tractable if and only if it is contained in one of the 1 algebras listed in table 1. otherwise is np-complete.
　we can assume  without loss of generality  that contains the relation   since it is easy to show that and have the same complexity  up to polynomial-time equivalence . this implies that the size of is odd since is closed under converse. by combining proposition 1  and the previous observation  we may consider only the case
           . by proposition 1   it suffices to consider the cases where	is one of the following sets:	s s	  f f	 	d d	  or	o o	.
　the rest of this section is organized as follows. in subsection 1 we describe some np-complete sets of relations which we use in our proofs later. in subsection 1  we give a complete proof of theorem 1 for the cases where
　　s s and f f . finally  in subsection 1 we give a sketch of a proof of theorem 1 for the cases where d d and o o .
finally  we observe that whenis one of the algebras  	  or	 	   then	is either	s sor f f . with the help of computer-aided exhaustive search  it was proved in  drakengren and jonsson  1a; 1b  that these ten algebras are maximal tractable subalgebras of allen's algebra. since we do not use computer-aided results in subsection 1  our results provide a new elementary proof of maximality for these algebras.
　throughout this section we shall assume that a non-trivial relation is also non-empty; this will cause no confusion.
1	np-completeness results
this subsection contains the np-completeness proofs that we need in the sequel. our reductions are based on the np-complete problem betweenness  garey and johnson  1 :
instance: a finite set and a collection of ordered triples of distinct elements from .
question: is there a total ordering on such that for each   we have either or  
　let us fix relations	.	then we define to be the following problem instance over the variables	:
further  define 
  and
.lemma 1 let	p	do	s	 	  and let	 	 	be as above. ifis satisfiable while	andare not  thenis np-complete.proof. polynomial-time reduction from betweenness. let be an arbitrary instance of the problem and con-
struct an instance	of	-sat   as follows: 1  for each pair of distinct elements straint to ; and  add the con- 1  for each triple  introduce two fresh vari-	ables	and add	to	.it is a routine verification to show that is satisfiable if and only if has a solution.
the restriction p d o s is imposed to ensure that we get a natural total ordering in the reduction. many other relations such as dsf have this property but there are also relations without this property  e.g.  and mm .
　suppose and is an instance of -sat   . let variables be involved in . further  let be the relation defined as follows. a basic relation is included in if and and only if the instance obtained from by adding the constraint is satisfiable. in this case  we say that is derived from . it can be easily checked that the problems -sat    and -sat    are polynomially equivalent. looking now at the definition of the three operations of the interval algebra  it is easy to see that  generally  one can derive more relations from a given than one can generate using these operations. on the other hand  derivation is essentially harder to manage while the operations of allen's algebra give us the advantage of employing algebraic techniques. therefore  in this paper  we use derivation only once in the proof of the following statement.
proposition 1 the following subsets of	are np-complete:
1  d oo   d pp   and o dd . 1  with ods pmods f .
proof. for part  1   apply lemma 1 with	o  	d   oo	  or with	p  	d	 	pp	  or with	d  	o  	dd	  respectively.
　to prove part  1   let	be the union of all basic relations except for	and s	  and consider the instance over the variables	. in the cases when	ods	or	pmods	f	  it can be shown that	is satisfiable for every basic relation but not satisfiable for any other choice of	. hence  we can derive	from   and we can also derive the relations
　　　　　　　　　s	and that. it is easy to checkpmodspmodsfwhich implies thats . furthermore  we haves	s	ss	  andis the disequality relation so the relation ss	can be obtained from the relation	.
if s   and   then these relations satisfy the conditions of lemma 1 so ss is npcomplete. since all of these relations can be derived from the single relation   it follows that is np-complete.
1	the s f case
we will show that if	s s	  then either
is np-complete or is contained in one of the subalgebras p  d  o      or in one of   . by using the obvious symmetry between the relations s and f   it immediately follows that if f f   then either is np-complete or contained in one of p  d  o     
or   . for the remainder of this subsection  it is assumed that s s .
	given a relation	  we write	to denote the relation
　　　. throughout the proofs we use the obvious fact that if then  for any   we have and
             . if	is a basic relation then	denotes the least relation	such that	  i.e. the intersection of all	with this property. it is easy to show that the relation	exists in	for every basic relation   except for two very simple cases: when every	satisfies exactly one of conditions	ss	and	ff	. in both cases  we have	.
lemma 1 if	contains the relation od   then every relation in	satisfies condition 1  of	.
proof. arbitrarily choose . since od   it follows that d o od and o d . furthermore  s od pmod so p and m . since pm pm p   we have p pm and m pm   which implies that p od and m od . now it follows that od p and od m. thus  if pm   then od and satisfies condition 1  of .
lemma 1 if	contains a non-trivial relation	with
　pp mm ff then is included in one of     or p  or else is np-complete.
proof. case 1.	ff
since ff	s	od	  it follows that any
satisfies condition 1  of	  by lemma 1. now it follows that  for any	  we have o	d	and also f	f
　suppose that   i.e. some fails to satisfy condition 1  or condition 1  of . then  using the facts from the previous paragraph  it can be shown that we can choose so that s ff pmodss ff or ods pmodss ff . in both cases  composing the relations with s from the left we get
	pmodss	s	pmodss
therefore pmodss   and pmods can be obtained as
	pmodss	pmodss	od
then	is np-complete  by proposition 1 .
	case 1.	ff	.
composing and its inverse we get ff   so we may assume that ff . if some relation fails to satisfy condition 1  of   then is either one of f and f   which is impossible under our assumptions  or ff
going back to case 1. suppose now that each satisfies condition 1  of .
we have s ods . if od belongs to then s od ff   which implies .
otherwise we have	od	s	for every
　　　. assume a relation does not satisfy condition 1  of   that is pmod and s . since pmods s   we have pmods
and pm . this implies p   a contradiction. therefore the relations in must satisfy both conditions of
	  that is	.
	case 1.	mm	ff	and	mm	.
without loss of generality we may assume that m . then the relation s is equal either to mm
or to m . in the former case it follows that the relation m mm mm s belongs to as well.
a contradiction.
	case 1.	pp	.
we may assume  without loss of generality  that p	.
then p s pp mm ods . furthermore  p s pp mm   and we have p
     s pp m . if p then p   a contradiction. otherwise we have pp . then p p holds for every .
　we have s pp pp mod f . for every non-empty modf   we have s pp p .
therefore no such	belongs to	. we conclude that  for any
	  if	mod	f	then p	  which means that
p.
we assume further that  for every non-trivial   we have oo dd ss . in the next four lemmas we consider the case when contains a non-trivial relation with ss . in view of the assumption just made we may consider that is either d or o. it is not hard to verify that these four lemmas indeed cover all possible cases for d and
o.
lemma 1 suppose d ss . if d oo dd d or dd d then d.
proof. case 1. d	oo	dd	d .
by assumption  the relation d satisfies condition d	d
   pp mm dff . let be calculated as d s d . then we have d pp mm d . by minimality of d we get d d pp mm d . calculating again  we get d d pp m d . since pp   and since we have m d d s only if p d  we may assume that d is either pd   or p d   or p m d . the first case is impossible because of pd p d s d . let p d d p m d . then p m o df d s . suppose
contains a non-empty subrelation of p m o f . then d s is a non-emptysubrelation of p m
implying that p   a contradiction. therefore  for every   we have p m o f d   which means that d.
	case 1. dd	d	oo	dd	.
suppose a relation satisfies both ss and dd . then it is not hard to verify that either s d or s d contains exactly one of d
and d which contradicts the minimality of d. thus  for every such that ss   we have dd .
this implies that	d.
lemma 1 suppose o ss . if o oo dd o or oo o then o.
proof. similar to the previous lemma.
lemma 1 if d	o	and d	oo	dd	ss then	d.odproof. as in the proof of lemma 1  we can obtain o d pp m o df . if po d d then d sdpp	  a contradiction. therefore we have odd	p	m	o	df   and p	m	o	df	d	s	.
if some non-empty subrelation of p m f belongs to then s is a non-empty subrelation of p m . then p which contradicts our assumptions. therefore  for every   we have p m o f d   which means that
d.
lemma 1 if d o and d oo dd ss od then or is np-complete.
proof. as in the previous lemmas  it can be shown that od d pp mm od . it follows that d and pmod s d . furthermore  oo dd ff s pmod and od
 pmod oo dd ff . by lemma 1  we know that every satisfies condition 1  of . suppose some does not satisfy condition 1  of . then can be chosen so that s f pmodss ff
or	os	pmodss	ff	.	if	f
then ff oo dd ff which contradicts our assumptions. further  if od then oo dd ff f   a contradiction. therefore we may assume that ods f pmodss f . now it can be checked that o d satisfies ods f pmods f . then is npcomplete by proposition 1 . one can proceed similarly if condition 1  of fails in .
it remains to consider the case when each non-trivial satisfies	ss	.
lemma 1 if	ss	for any non-trivial	then for some	.
	proof. case 1. p	ss	s	.
we have ps	p and s	p. let	s	p	.
then it is easy to check that pmod	s	f	  and that s	. it follows that	pmod	s	f	  since otherwise	is non-empty and	ss	. no nonempty subrelation of pmod	f	can belong to	. therefore  for any	 	pmod	f	implies s	.
hence	.
	case 1. d	ss	s	.
the proof is similar to case 1; the only change is that
d	s	  and we deduce that	p	m	o	ds	f   and 
hence 	.
	case 1. o	ss	s	.
in view of cases 1 and 1 we may assume that os	o
	p	mm	od	s	ff	. let	s	o. it is easy to
check that
	od	s	f	p	m	oo	d	s	f
since oo   we obtain p m od s f . it can straightforwardly be verified that if od s f   then   but pp ss pp   which contradicts the assumption made. therefore 
 od s f   and pmod s f . hence  for any   pmod f implies s   and we have .
	case 1. m	ss	s	.
similarly to case 1  we infer that	.
case 1. s is contained in each of p  d  o  and m.
we have f	ss	. then it follows that if s	f then . otherwise  s	f and we have	.
1	the d o case
suppose d d . we give a sketch of a proof that either is contained in d or in d  and therefore is tractable   or else it contains oo or pp  and then it is np-complete by proposition 1  .
　let . first it can be shown that either is contained in d or in d  or else contains oo or pp . then one can check  similarly to lemma 1 
that if there exists a non-trivial	such that	dd
then satisfies ss or ff   and in this case  once again  is contained in d or in d  respectively  or else contains oo or pp . finally  we assume that each non-trivial satisfies dd   and then the required result can be proved by analysing the minimal relations p  m  o  s  and f  as in the proof of lemma 1.
　the case o o is very similar to the previous one. the only essential difference is that the relation pp cannot belong to ; otherwise we get o o pmo and p pp pmo   a contradiction.
1	conclusion
we have proved that any so far unknown tractable fragment of allen's algebra can contain no other basic relation than
   . we believe that  by extending the algebraic techniques used to establish our result  it will be possible to show that there are in fact no other forms of tractability in allen's algebra. in other words  we conjecture that all maximal tractable subalgebras are already identified. however  the proof of this conjecture requires a more elaborate algebraic argument than the proofs in this paper  since in the general case we have no particular relation to start with. algebraic techniques similar to the one used in this paper can also be applied to the study of the complexity in other temporal and spatial formalisms.
references
 allen  1  james f. allen. maintaining knowledge about temporal intervals. communications of the acm  1 :1 1.
 angelsmark and jonsson  1  ola angelsmark and peter jonsson. some observations on durations  scheduling and allen's algebra. in proceedings of the 1th conference on constraint programming  cp'1   volume 1 of lecture notes in computer science  pages 1.springerverlag  1.
 bellini et al.  1  p. bellini  r. mattolini  and p. nesi. temporal logics for real-time system specification. acm computing surveys  1 :1  1.
 drakengren and jonsson  1a  thomas drakengren and peter jonsson. eight maximal tractable subclasses of allen's algebra with metric time. journal of artificial intelligence research  1-1  1.
 drakengren and jonsson  1b  thomas drakengren and peter jonsson. twenty-one large tractable subclasses of allen's algebra. artificial intelligence  1-1 :1  1.
 drakengren and jonsson  1  thomas drakengren and peter jonsson. a complete classification of tractability in allen's algebra relative to subsets of basic relations. artificial intelligence  1 :1  1.
 garey and johnson  1  michael garey and david johnson. computers and intractability: a guide to the theory of np-completeness. freeman  new york  1.
 golumbic and shamir  1  martin c. golumbic and ron shamir. complexity and algorithms for reasoning about time: a graph-theoretic approach. journal of the acm  1 :1  1.
 hirsch  1  robin hirsch. relation algebras of intervals. artificial intelligence  1 :1  1.
 ladkin and maddux  1  peter b. ladkin and roger d. maddux. on binary constraint problems. journal of the acm  1 :1  1.
 ladkin and reinefeld  1  peter b. ladkin and alexander reinefeld. effective solution of qualitative interval constraint problems. artificial intelligence  1 :1- 1  1.
 ligozat  1  ge＞rard ligozat.  corner  relations in allen's algebra. constraints  1-1 :1  1.
 meiri  1  itay meiri. combining qualitative and quantitative constraints in temporal reasoning. artificial intelligence  1-1 :1 1.
 nebel and bu：rckert  1  bernhard nebel and hansju：rgen bu：rckert. reasoning about temporal relations: a maximal tractable subclass of allen's interval algebra. journal of the acm  1 :1  1.
 no：kel  1  klaus no：kel. temporally distributed symptoms in technical diagnosis  volume 1 of lecture notes in artificial intelligence. springer-verlag  1.
 szendrei  1  agnes＞ szendrei. maximal non-affine reducts of simple affine algebras. algebra universalis  1 :1  1.
 vilain et al.  1  marc b. vilain  henry a. kautz  and peter g. van beek. constraint propagation algorithms for temporal reasoning: a revised report. in daniel s. weld and johan de kleer  editors  readings in qualitative reasoning about physical systems  pages 1. morgan kaufmann  san mateo  ca  1.
interval-based temporal reasoning with general tboxes
carsten lutz
lufg theoretical computer science
rwth aachen  germany lutz cs.rwth-aachen.deabstract
until now  interval-based temporal description logics  dls  did-if at all-only admit tboxes of a very restricted form  namely acyclic macro definitions. in this paper  we present a temporal dl that overcomes this deficieny and combines intervalbased temporal reasoningwith general tboxes. we argue that this combination is very interesting for many application domains. an automata-based decision procedure is devised and a tight exptimecomplexity bound is obtained. since the presented logic can be viewed as being equipped with a concrete domain  our results can be seen from a different perspective: we show that there exist interesting concrete domains for which reasoning with general tboxes is decidable.
1	motivation
description logics  dls  are a family of formalisms wellsuited for the representation of and reasoning about conceptual knowledge. whereas most description logics represent only static aspects of the application domain  recent research resulted in the exploration of various description logics that allow to  additionally  represent temporal information  see  e.g.   artale and franconi 1  for an overview. one approach for temporal reasoning with dls is to use so-called concrete domains. concrete domains have been proposed as an extension of description logics that allows reasoning about  concrete qualities  of entities of the application domain such as sizes  weights or temperatures  baader and hanschke 1 . as was first described in  lutz et al. 1   if a  temporal  concrete domain is employed  then description logics with concrete domains are a very useful tool for temporal reasoning. ontologically  temporal reasoning with concrete domains is usually interval-based but may also be point-based or even both.
　in this paper  we define a temporal description logic based on concrete domains which uses points as its basic temporal entity  but which may also be used as a full-fledged intervalbased temporal dl. more precisely  the presented logic extends the basic description logic with a concrete domain that is based on the rationals and predicates and . the well-known allen relations can be defined in terms of their endpoints  allen 1  thus allowing for  qualitative  interval-based temporal reasoning. since it is an important feature of dls that reasoning should be decidable  we prove decidability of the standard reasoning tasks by using an automata-theoretic approach which also yields a tight exptime complexity bound.
　most dls allow for some kind of tbox formalism that is used to represent terminological knowledge as well as background knowledge about the application domain. however  there exist various flavours of tboxes with vast differences in expressivity. to the best of our knowledge  all intervalbased dls and all dls with concrete domains defined in the literature admit only a very restricted form of tbox  i.e.  sets of acyclic macro definitions. compared to existing description logics that are interval-based or include concrete domains  the distinguishing feature of our logic is that it is equipped with a very general form of tboxes that allows arbitrary equations over concepts. thus  the presented work overcomes a major limitation of both families of description logics.
　our results can be viewed from the perspective of intervalbased temporal reasoning and from the perspective of concrete domains. for the temporal perspective  we claim that the combinationof general tboxes and interval-basedtemporal reasoning is important for many application areas. in this paper  we present process engineering as an example. from the concrete domain perspective  our results can be viewed as follows: in  lutz 1   it is shown that  even for very simple concrete domains  reasoning with general tboxes is undecidable. it was an open question whether there exist interesting concrete domains for which reasoning with general tboxes is decidable. in this paper  we answer this question to the affirmative. this paper is accompanied by a technical report containing full proofs  lutz 1 .
1	syntax and semantics
in this section  we introduce syntax and semantics of the description logic .
definition 1. let     and be mutually disjoint and countably infinite sets of concept names  roles  and concrete features. furthermore  let be a countably infinite subset of . the elements of are called abstract features. a path is a composition of abstract features and one concrete feature . the set of concepts is the smallest set such that
1. every concept name is a concept
1. if and are concepts  is a role  is a concrete feature  are paths  and   then the following expressions are also concepts:      
	 	 	  and	.
a tbox axiom is an expression of the form with and are concepts. a finite set of tbox axioms is a tbox.
throughoutthis paper  we will denote atomic concepts by the letter    possibly complex  concepts by the letters   roles by the letter   abstract features by the letter   concrete features by the letter   paths by the letter   and elements of the set by the letter . we will sometimes call the
tbox formalism introduced above general tboxes to distinguish it from other  weaker formalisms such as the ones in  nebel 1 . as most description logics  is equipped with a tarski-style semantics.
definition 1. an interpretation is a pair   where is a set called the domain and is the interpretation function mapping each concept name to a subset of
     each role name to a subset of   each abstract feature to a partial function from to   and each concrete feature to a partial function from to the rationals . for paths   we set
. the interpretation func-
tion is extended to arbitrary concepts as follows:
is undefined
an interpretation	is a model of a tbox	iff it satisfies
c d for all axioms in . is a model of a concept iff .
if for some     and   then we call a concrete successor of in . we write for
and for   where is a concept name. moreover  we write with for .
definition 1  inference problems . let and be concepts and be a tbox. subsumes w.r.t.  written
　　　　  iff	for all models	of	.	is satisfiable w.r.t.	iff there exists a model of both	and	.
it is well-known that  un satisfiability and subsumption can be mutually reduced to each other: iff is unsatisfiable w.r.t. and is satisfiable w.r.t. iff . we now discuss the relationship between and description logics with concrete domains.
definition 1  concrete domain . a concrete domain	is a pair	  where	is a set called the domain  and
is a set of predicate names. each predicate name is associated with an arity and an -ary predicate .
the concrete domain is usually integrated into the logic by a concept constructor	  with semantics
for
and
it is obvious that can be viewed as being equipped with the concrete domain   where and
are binary predicates with the usual semantics. for most dls with concrete domains  it is required that the set of predicates is closed under negation and contains a name for
   . this property ensures that every concept can be converted into an equivalent one in the so-called negation normal form  nnf . the nnf of concepts  in turn  is used as a starting point for devising satisfiability algorithms. it is not hard to see that is not admissible in this sense. however  as we will see in section 1  the conversion of -concepts into equivalent ones in nnf is nevertheless possible.
1	temporal reasoning with
although does only provide the relations     and     on time points  it is not hardto see that the remainingrelations can be defined by writing  e.g.  for
　　　　. however we claim that cannotonly be used for point-based temporal reasoning but also as a full-fledged interval-based temporal description logic. as observed by allen   there are 1 possible relationships between two intervals such as  for example  the relation: two intervals and are related by iff the right endpoint of
is identical to the left endpoint of -see  allen 1  for an exact definition of the other relations. as we shall see  these 1 allen relations  as well as the additional relations from the allen algebra  see  allen 1   can be defined in . in the following  we present a framework for mixed intervaland point-based reasoning in and apply this framework in the application area of process engineering
　the representation framework consists of several conventions and abbreviations. we assume that each entity of the application domain is either temporal or atemporal. if it is temporal  its temporal extension may be either a time point or an interval. left endpoints of intervals are represented by the concrete feature   right endpoints of intervals are represented by the concrete feature   and time-points not related to intervals are represented by the concrete feature . all this can be expressed by the following tbox :
here  is an abbreviation for . let us now define the allen relations as abbreviations. for exam-
ple 	is an abbreviation for where	  i.e. 	and	are words
over the alphabet	. note that
similar abbreviations are introduced for the other allen relations. we use to denote the empty word. for example  is an abbreviation for
intuitively  refers to the interval associated with the abstract object at which the concept is  evaluated .
　since we have intervals and points available  we should also be able to talk about the relationship of points and intervals. more precisely  there exist 1 possible relations between a point and an interval  vilain 1   one example being	that holds between a point	and an interval	if is identical to the left endpoint of . hence  we can define as an abbreviationfor	and similar abbreviations for	 	 	  and	.
　this finishes the definition of the framework. we claim that the combination of interval-based reasoning and general tboxes is important for many application areas such as reasoning about action and plans  artale and franconi 1 . the examples presented here are from the area of process engineering that was first considered by sattler in a dl context  sattler 1 . however  sattler's approach does not take into account temporal aspects of the applicationdomain. we show how this can be done using thus refining sattler's proposal.
　assume that our goal is to represent information about an automated chemical production process that is carried out by some complextechnical device. the device operates each day for some time depending on the number of orders. it needs a complex startup and shutdown process before resp. after operation. moreover  some weekly maintenance is needed to keep the device functional. let us first represent the underlying temporal structure that consists of weeks and days.
the axiom states that each week consists of seven days  where the 'th day is accessible from the corresponding week via the abstract feature . the temporal relationship between the days are as expected: monday starts the week  sunday finishes it  and each day temporally meets the succeeding one. note that this implies that days 1 to 1 are during the corresponding week although this is not explicitly stated. moreover  each week has a successor week that it temporally meets. we now describethe startup  operation shutdown  and maintenance phases.
here       and are abstract features and     is used for better readability  i.e.  paths are written as  . the tbox implies that phases are related to the corresponding day as follows: startup via
or	  shutdown via	or	  and operation via . until now  we did not say anything about the temporal relationship of maintenance and operation. this may be inadequate  if  for example  maintenance and operation are mutually exclusive. we can take this into account by using additional axioms
   
where	is replaced by	 	 	-	 
           -     or yielding 1 axioms.
　until now  we have modelled the very basic properties of our production process. let us define some more advanced concepts to illustrate reasoning with . for example  we could define a busy week as
i.e.  each day  the startup process starts at the beginning of the day and the shutdown finishes at the end of the day. say now that it is risky to do maintenance during startup and shutdown phases and define
expressing that  in a risky week  the maintenance phase is not strictly separated from the startup and shutdown phases. a reasoner could be used to detect that
           i.e.  every busy week is a risky week: in a busy week  the week is partitioned into startup  shutdown  and operation phases. since maintenance may not ovlp with operation phases  see       it must ovlp with startup and/or shutdown phases which means that it is a risky week. we can further refine this model by using mixed point-based and interval-based reasoning  see  lutz 1  for examples.
1	the decision procedure
in this section  we prove satisfiability of -concepts w.r.t. tboxes to be decidable and obtain a tight exptime complexity bound. this is done using an automata-theoretic approach: first  we abstract models to so-called hintikka-trees such that there exists a model for a concept and a tbox
iff there exists a hintikka-tree for and . then  we build  for each -concept and tbox   a looping automaton that accepts exactly the hintikka-trees for . in particular  this implies that accepts the empty language iff is unsatisfiable w.r.t. .
definition 1. let be a set and . a -ary -tree is a mapping that labels each node
	with	. intuitively  the node
is the -th child of	. we use	to denote the empty word
 corresponding to the root of the tree .
	a looping automaton	for	-ary
　-trees is defined by a set of states  an alphabet   a subset of initial states  and a transition relation . a run of on an -tree is a mapping	with
and	for each

figure 1: a constraint graph containing no -cycle that is unsatisfiable over .
           a looping automaton accepts all those	-trees for which a run exists  i.e.  the language	of	-trees accepted by	is
	there is a run of	on
in  vardi and wolper 1   it is proved that the emptiness problem for looping automata is decidable in polynomial time.
　a hintikka-tree for and corresponds to a canonical model for and . apart from describing the abstract domain of the corresponding canonical model together with the interpretation of concepts and roles  each hintikkatree induces a directed graph whose edges are labelled with predicates from . these constraint graphs describe the  concrete part  of  i.e.  concrete successors of domain objects and their relationships .
definition 1. a constraint graph is a pair   where is a countableset of nodesand a set of edges. we generally assume that constraint graphs are equality closed  i.e.  that implies .
a constraint graph is called satisfiable over   where is a set equipped with a total ordering   iff there exists a total mapping from to such that for all . in this case  is called a solution for .
a -cycle in is a finite non-empty sequence of nodes such that  1  for all with   we have
                 where and denotes addition modulo and  1  for some .
the following theorem will be crucial for proving that  for every hintikka-tree  there exists a corresponding canonical model. more precisely  it will be used to ensure that the constraint graph induced by a hintikka-tree  which describes the concrete part of the corresponding model  is satisfiable.
theorem 1. a constraint graph	is satisfiable over	with iff	does not contain a	-cycle.
note that theorem 1 does not hold if satisfiability over is considered due to the absence of density: if there exist two nodes and such that the length of -paths  which are defined in the obvious way  between and is unbounded  a constraint graph is unsatisfiable over even if it contains no -cycle  see figure 1.
　the decidability procedure works on -concepts and tboxes that are in a certain syntactic form.
definition 1  path normal form . a -concept is in path normal form  pnf  if  for all subconcepts of   we have either  1 	and	   1 	and
         or  1  and for some and . a tbox is in path normal form iff all concepts appearing in are in path normal form.
lemma 1. satisfiability of -concepts w.r.t. tboxes can be reduced to satisfiability of -concepts in pnf w.r.t. -tboxes in pnf.
proof let	be a	concept.	for every path
　　　　in   we assume that	are concrete features. we inductively define a mapping from paths in to concepts as follows:
for any	-concept	  a corresponding concept	in
pnf is obtained by replacing all subconcepts of with	and with . we extend the mapping to tboxes in the obvious way. it is easy to check that the translation is polynomial and that a concept is satisfiable w.r.t. a tbox iff is satisfiable w.r.t. .	 
hence  it suffices to prove that satisfiability of concepts in pnf w.r.t. tboxes in pnf is decidable. moreover  we can assume all concepts and tboxes to be in nnf.
definition 1  nnf . a concept is in negation normal form  nnf  if negationoccurs onlyin front of conceptnames. every concept can be transformed into an equivalent one in nnf by eliminating double negation and using de morgan's law  the duality between and   and the following equivalences:
where denotes the exchange of predicates  i.e.  is and is . with   we denote the equivalent of in nnf. a tbox is in nnf iff all concepts in are in nnf.
note that transformation to nnf preserves pnf. we often refer to tboxes in their concept form :
we now define hintikka-trees for concepts and tboxes  in pnf and nnf  and show that there exists hintikka-tree for and iff there exists a model for and .
　let be a concept and a tbox. with   we denote the set of subconcepts of and . we assume that existential concepts in with are linearly ordered  and that yields the -th existential concept in . furthermore  we assume the abstract features used in to be linearly ordered and use to denote the -th abstract feature in .
the set of concrete features used in	is denoted with
. hintikka-pairs are used as labels of the nodes in
hintikka-trees.
definition 1  hintikka-set  hintikka-pair . let be a concept and be a tbox. a set is a hintikkaset for iff it satisfies the following conditions:
 h1   h1  if  then  h1  if  then  h1 for all concept names  h1  if　  then with or .for all conceptswe say that	is enforced by a hintikkaset	iff either	for some concept	or for some and	. a hintikka-pair	for	consists of a hintikka-set	for	and a set	of tuples
with	such that
 h1  if	  then	.
with	  we denote the set of all hintikka-pairs for
	. a path	 of length 1 or 1  is enforced by
iff either is a node in or	for some path and .
intuitively  each node of a  yet to be defined  hintikkatree corresponds to a domain object of the corresponding canonical model . the first component of the hintikkapair labelling is the set of concepts from satisfied by . the second component states restrictions on the relationship between concrete successors of . if  for example    then we must have .
note that the restrictions in are independent from concepts . as will become clear when hintikkatrees are defined  the restrictions in are used to ensure that the constraint graph induced by the hintikka-tree   which describes the concrete part of the model   does not contain a -cycle  i.e.  that it is satisfiable. this induced constraint graph can be thought of as the union of smaller constraint graphs  each one being described by a hintikka-pair labelling a node in . these pair-graphs are defined next.
definition 1  pair-graph . let be a concept  a tbox  and a hintikka-pair for . the pair-graph of is a constraint graph defined as follows:
1. is the set of paths enforced by
1. .
an edge extension of is a set such that for all   we have either
or for some . if is an edge extension of   then the graph is a completion of	.
note that  due to path normal form and the definitions of hintikka-pairs and pair-graphs  we have for every edge extension of a pair-graph .
　we briefly comment on the connection of completions and the -component of hintikka-pairs. let and be nodes in a hintikka-tree and let and be the corresponding domain objects in the corresponding canonical model . edges in hintikka-trees represent role-relationships  i.e.  if is successor of in   then there exists an such that
　　　　　. assume is successor of and the edge between and represents relationship via the abstract feature   i.e.  we have . the second component of the hintikka-pair labelling fixes the relationships between all concrete successors of that   talks about . for example  if and   where is the first component of the hintikka-pair labelling   then   talks about  the concrete -successor and the concrete -successor of . hence  either contains
or for some . this is formalized by demanding that the pair-graph of the hintikka-pair labelling togetherwith all the edges from the -components of the successors of are a completion of . moreover  this completion has to be satisfiable  which is necessary to ensure that the constraint graph induced by does not contain a -cycle. an appropriate way of thinking about the components is as follows: at   a completion of is  guessed . the additional edges are then  recorded  in the -components of the successor-nodes of .
definition 1  hintikka-tree . let be a concept  be a tbox  the number of existential subconcepts in   and be the numberof abstract features in . a -tuple of hintikka-pairs with
andis called matching iff h1  if	and	  then h1  ifand  then h1  if	and	  then. h1  if	is enforced by	 	  and
	  then	.
 h1  the constraint graph is a satisfiable completion of   where is defined as
a -ary -tree is a hintikka-tree for iff is a hintikka-pair for for each node in   and satisfies the following conditions:
 h1 	  where	 
 h1  for all	  the tuple
	with	is matching.
for a hintikka-tree	and node	with
               we use	to denote	and	to denote	. moreover  if	  we use	to denote the constraint graph	as defined in  h1 .
whereas most properties of hintikka-trees deal with concepts  roles  and abstract features and are hardly surprising   h1  ensures that constraint graphs induced by hintikkatrees contain no -cycle. by  guessing  a completion as explained above  possible -cycles are anticipated and can be detected locally  i.e.  it suffices to check that the completions are satisfiable as demanded by  h1 . indeed  it is crucial that the cycle detection is done by a local condition since we need to define an automaton which accepts exactly hintikka-trees and automata work locally. it is worth noting that the localization of cycle detection as expressed by  h1  crucially depends on path normal form.
　the followingtwo lemmas show that hintikka-treesare appropriate abstractions of models.
lemma 1. a concept is satisfiable w.r.t. a tbox iff there exists a hintikka-tree for .
to prove decidability  it remains to define a looping automa-
ton for each concept and tbox such that accepts exactly the hintikka-trees for .
definition 1. let be a concept  be a tbox  the number of existential subconcepts in   and be the number of abstract features in . the looping automaton is defined as follows:
	 	  and
iff
and is matching.
note that every state is an accepting state  and  hence  every run is accepting. the following lemma is easily obtained.
lemma 1.	is hintikka-tree for	iff	.
since the size of is linear in the size of and   it is straightforward to verify that the size of is exponential in the size of and . this  together with lemmas 1  1  and 1  and the polynomial decidability of the emptiness problem of looping automata  vardi and wolper 1   implies the upper bound given in the following theorem which states the main result of this paper. the lower bound is an immediate consequence of the fact that with general tboxes is exptime-hard  schild 1 .
theorem 1. satisfiability and subsumption of concepts w.r.t. tboxes are exptime-complete.
1	conclusion
there are several perspectives for future work of which we highlight three rather interesting ones: firstly  the presented decision procedureis only valid if a dense strict linear order is assumed as the underlying temporal structure. for example  the concept is satisfiable w.r.t. the tbox
over the temporal structures and  with the natural orderings  but not over . to see this  note that induces a constraint graph as in figure 1. hence  it would be interesting to investigate if and how the presented algorithm can be modified for reasoning with the temporal structure .
　secondly  does only allow for qualitative temporal reasoning. it would be interesting to extendthe logic to mixed qualitative and quantitative reasoning by additionally admitting unary predicates and for each .
　thirdly  we plan to extend to make it suitable for reasoning about entity relationship  er  diagrams. as demonstrated in  e.g.   calvanese et al. 1; artale and franconi 1   description logics are well-suited for this task. by using an appropriate extension of   one should be able to capture a new kind of temporal reasoning with er diagrams  namely reasoning over er diagrams with  temporal  integrity constraints. for example  a temporal integrity constraint could state that employees birthdays should be before their employment date. an appropriate extension of for this task could be by  unqualified  number restrictions  inverse roles  and a generalized version of the concrete domain constructor . an extension of the presented automata-theoretic decision procedure to this more complex logic seems possible.
acknowledgements my thanks go to franz baader  ulrike sattler  and stephan tobies for fruitful discussions. the author was supported by the dfg project ba1-1  combinations of modal and description logics .
references
 allen  1  j. allen. maintaining knowledge about temporal intervals. communications of the acm  1   1.
 artale and franconi  1  a. artale and e. franconi. a temporal description logic for reasoning about actions and plans. journal of artificial intelligence research  jair    1   1.
 artale and franconi  1  a. artale and e. franconi. temporal er modeling with description logics. in proc. of er'1  paris  france  1. springer-verlag.
 artale and franconi  1  a. artale and e. franconi. temporal description logics. in handbook of timeand temporal reasoning in artificial intelligence. mit press  to appear.
 baader and hanschke  1  f. baader and p. hanschke. a scheme for integrating concrete domains into concept languages. in proc. of ijcai-1  pages 1  sydney  australia  1. morgan kaufmann publ. inc.
 calvanese et al.  1  d. calvanese  m  lenzerini  and d. nardi. description logics for conceptual data modeling. in logics for databases and information systems  pages 1. kluwer academic publisher  1.
 lutz et al.  1  c. lutz  v. haarslev  and r. mo：ller. a concept language with role-forming predicate restrictions. technical report fbi-hh-m-1  university of hamburg  computer science department  hamburg  1.
 lutz  1  c. lutz interval-based temporal reasoning with general tboxes. ltcs-report ltcs-1  lufg theoretical computer science  rwth aachen  germany  1. see http://www-lti.informatik.rwthaachen.de/forschung/reports.html.
 lutz  1  c. lutz. nexptime-complete description logics with concrete domains. in proc. of ijcar 1  lncs  siena  italy  1. springer-verlag.
 nebel  1  b. nebel. terminological reasoning is inherently intractable. artificial intelligence  1-1  1.
 sattler  1  u. sattler. terminological knowledge representation systems in a process engineering application. phd thesis  lufg theoretical computer science  rwth-aachen  1.
 schild  1  k. d. schild. a correspondence theory for terminological logics. in proc. of ijcai-1  pages 1  sidney  australia  1. morgan kaufmann publ. inc.
 vardi and wolper  1  m. y. vardi and p. wolper. automatatheoretic techniques for modal logic of programs. journal of computer and system sciences  1-1  1.
 vilain  1  m. vilain. a system for reasoning about time. in proceedings of the second aaai  pages 1  pittsburgh  pennsylvania  1.

knowledge representation
and reasoning
belief revision

on the semantics of knowledge update

chitta baral
dept of computer science & engineering
arizona state university
tempe  az 1  usa
e-mail: chitta asu.edu
yan zhang
school of computing & it
university of western sydney
penrith south dc  nsw 1  australia
e-mail: yan cit.uws.edu.au

abstract
we consider the problem of how an agent's knowledge can be updated. we propose a formal method of knowledge update on the basis of the semantics of modal logic s1. in our method  an update is specified according to the minimal change on both the agent's actual world and knowledge. we then investigate the semantics of knowledge update and characterize several specific forms of knowledge update which have important applications in reasoning about change of agents' knowledge. we also discuss the persistence property of knowledge and ignorance associated with knowledge update.
1	introduction and motivation
the well-studied issues of belief updates and belief revision  katsuno and mendelzon  1  are concerned with the update and revision aspects of an agent's belief with respect to new beliefs. the notion of belief update has been used  and often serves as a guideline  kartha and lifschitz  1   in reasoning about the effect of  world altering  actions on the state of the world. thus if represents the agent's belief about the world and the agent does an action that is supposed to make true in the resulting world  then the agent's belief about the resulting world can be described by   where is the update operator of choice.
　now let us consider reasoning about sensing actions  scherl and levesque  1; son and baral  1   which in their pure form  when executed  do not change the world  but change the agent's knowledge about the world. let be a sensing action whose effect is that after it is executed the agent knows whether is true or not. this can be expressed as   where is the modal operator . the current theory of belief updates does not tell us how to do updates with respect to such gain in knowledge due to a sensing action.  note that we can not just have and use the the notion of belief update  as is a tautology . one of our goals in this paper is to define a notion of knowledge update  analogous to belief update  where the original theory     and the new theory     are in a language that can express knowledge. such a notion would not only serve as a guideline to reason about pure and mixed sensing actions in presence of constraints  but also allow us to reason about actions corresponding to forgetting  and ignorance.
　the structure of the rest of the paper is as follows. in section 1 we start with describing the particular modal logic that we plan to use in expressing knowledge  and describe the notion of k-models analogous to 'models' in classical logic. we then define closeness between k-models and use it to define a particular notion of knowledge update. in section 1 we present alternative characterizations of four particular knowledge update cases - gaining knowledge  ignorance  sensing  and forgetting  and show their equivalence to our original notion of knowledge update. some of these alternative characterizations are based on the formulation of reasoning about sensing actions  and thus our equivalence results serve as justification of the intuitiveness of our definition of knowledge update. in section 1 we explore sufficiency conditions that guarantee persistence of knowledge  or ignorance  during a knowledge update. in section 1  we conclude and discuss future directions.
1	closeness between k-models and knowledge update
in this section  we describe formal definitions for knowledge update. our formalization will be based on the semantics of the propositional modal logic s1 with a single agent. in general  under kripke semantics  a kripke structure is a triple
　　　　  where is a set of possible worlds  is an equivalence relation on   and is a truth assignment function that assigns a propositional valuation to each world in . given a kripke structure   a kripke interpretation is a pair   where is referred to the actual world of . the entailment relation between kripke interpretations and formulas is defined to provide semantics for formulas of s1  fagin and et al  1 .
　in the case of single agent  however  we may restrict ourselves to those s1 structures in which the relation is universal  i.e. each world is accessible from every world  and worlds are identified with the set of atoms true at the worlds  meyer and van der hoek  1 . to simplify a comparison between two worlds  e.g. definition 1   we may view an atom iff . therefore  in our context a kripke structure is uniquely characterized by and we may simplify a kripke interpretation as a pair which we call a -model  where indicates the actual world of the agent and presents all possible worlds that the agent may access. note that is in for any -model .
　in our following description  we use to denote primitive propositional atoms; to denote propositional formulas without including modalities  we also call them objective formulas ; and and to denote formulas that may contain modal operator . for convenience  we also use to represent a finite set of formulas and call a  knowledge  set.
definition 1  s1 semantics  let be the set of all primitive propositions in the language. the entailment relation under normal s1 semantics is defined as follows:
1. iff	is primitive  i.e.	  and	;
1. iff	and	;
1. iff it is not the case that	;
1. iff	for all	.
　given a formula   is called a -model of if . we use to denote the set of all models of . for objective formulas   denotes the set of models of   and denotes that is a model of . for a formula   we say that entails   denoted as   if for every -model of   .
　now the basic problem of knowledge update that we would like to investigate is formally described as follows: given a model   that is usually viewed as a knowledge state of an agent  and a formula - the agent's new knowledge that may contain modal operator   how do we update to another -model such that
and is minimally different from with respect to some criterion. to approach this problem  we first need to provide a definition of closeness between two -models with respect to a given -model.
definition 1   -model closeness  let   and be three k-models. we say
is as close to	as	  denoted as	  if:
1. ; or
1. and one of the following conditions holds:
 i  if   then  a  there exist some and such that and and
and	  or  b  for any	if	and   then	;
 ii  if   then condition  a  above is satisfied  or  c  for any if and   then
;
 iii  if	and	  then conditions  b  and
 c  above are satisfied.
we denote	if	and	.
　in the above definition  condition 1 simply says that the symmetric differences between and is not bigger than that between and   while in condition 1   i    ii  and  iii  express that different preferences are applied to compare knowledge between and with respect to .
for convenience  given a	-model	  if we denote
     for all   then  a  is equivalent to and	;  b  is equivalent to	; and  c  is equivalent to	. also  b  and  c  together present a difference on both knowledge decrease and increase between and in terms of . it is also easy to see that is a partial ordering.
　note that during the comparison between two -models  we give preference to the change of the actual world over the change of the knowledge about the world.  the closeness criterion between actual worlds that we use is the commonly used criterion  winslett  1  based on symmetric difference . for instance  if the actual world of a -model is closer to the actual world of than the actual world of another -model   we will think is closer to and the comparison of knowledge between and is ignored. only when both and have the same actual world  we will compare the knowledge of and in terms of .
this seems to be intuitive to us. in fact  the comparison between actual worlds determines the actual distance between two -models to the given -model . if and have the same actual distance to   the knowledge distance is then taken into account.
　basically  condition  i   or  ii  resp.  in definition 1 defines a knowledge preference based on knowledge decrease
 or increase resp . that is  if	only looses knowledge from
    or only gains some knowledge to   resp.   then is preferred over those -models that have both knowledge decrease and increase from   i.e.  a   and also preferred over those -models that only loose more knowledge from  or add more knowledge to   resp.  than does  i.e.  b  or  c  respectively. condition iii   on the other hand  deals with the mixed situation that has both knowledge decrease and increase from . in this case  a combined difference on knowledge decrease and increase is applied to determine the knowledge distance  i.e.  b  and  c . conditions i    ii  and  iii  can be illustrated by the following figures respectively.

figure 1:	under the condition	and
:  a  or  b  holds.
definition 1   -model update  let be a kmodel and a formula. a k-model is called a possible resulting -model after updating with if and only if the following conditions hold:
1. ;
1. there does not exist another k-model such that and .
we denote the set of all possible resulting -models after updating with as .

:  a  or  c  holds.

figure 1: under the condition   and :  b  and  c  hold.
example 1 let	and
. we denote
	 	 	 
	 	 	 
	 	.
clearly 	is	a	-model of
.	consider the update of	with	.	let
               . now we show that is a possible resulting -model after updating with .
　since	  we first consider any possible -model such that
	. clearly  the only possible	would be	itself.
let	  where	is a subset of	.
however  since	  there does not exist any	such that
. therefore  from definition 1  only condition
1 can be used to find a possible	such that	.
so we assume . on the other hand  from and   it is easy to see that and
　　　　　　　　　　　　　　　　　1. then we have and . ignoringthe detailed verifications 
we can show that there does not exist such satisfying	. 
　based on the -model update  updating a formula  knowledge set  in terms of another formula is then achieved by updating every -model of with .
definition 1  knowledge update  let and be two formulas. the update of with   denoted as   is defined by	.
1	characterizing knowledge update
in this section  we first investigate basic properties of models  then provide alternative characterizations for different types of knowledge update and then show their equivalence with respect to our original characterization.
proposition 1 let and be two -models. then the following properties hold:
1. iff	;
1. iff	;
1. iff	;
1. let	  then	;
1. let	and	  then
.
	given a set of	-models	and a	-model	  let	be an
orderingon as we defined in definition1. by we mean the set of all elements in that are minimal with respect to ordering . the following theorem shows that our knowledge update can be characterized by a minimal change criterion based on an ordering on -models.
theorem 1 let	and	be two formulas. then
.
proof: to prove the result  we onlyneed to show that for each
 -model of  	. let . since   .
on the other hand  according to definition 1  for any
	 	we have	.	that
is 	.	so
	.	similarly 	we	can	show
. 
　while the above theorem presents a general minimal change property of our knowledge update  we now give alternative characterizations of the update of	with	when has some specific forms. these specific forms present some of the important features of knowledge update  and their alternative characterization is handy when the use of the notion of knowledge update becomes an overkill. for example  the alternative characterization of sensing update below is a much simpler characterization that is used in reasoning about sensing actions  scherl and levesque  1; son and baral  1 .	we now introduce a notation that will be useful in our following discussions.	let	be a set of worlds and	. by	  we denote the set and	iff	.
proposition 1 given	and	where	is objective and
　　　. then	is a	-model of	if and only if there exists a	-model	of	such that	and
　the above proposition reveals an important property about knowledge update: to know some fact  the agent only needs to restrict the current possible worlds in each of her -models  if this fact itself is already entailed by her current knowledge set. we call this kind of knowledge update gaining knowledge update.
example 1 let	. suppose	 
	 	and	. then	has one	-model
　　　　　　　　　. updating with   according to our -model update definition  we have a unique resulting model . indeed  this result is also obtained from proposition 1. 
　as a contrary case to the gaining knowledge update  we now characterize an agent ignoring a fact from her knowledge set which we call ignorance update  i.e. updating with . from definition 1  it is easy to see that
　　　　　　　. however  it should be noted that updating	can not be used to achieve	.	consider a	-model	.	updating with	we have a possible resulting	-model
　　　　　　　　　　　  while updating	with	will lead to a possible result	.
note that both and entail   but according to definition 1.
proposition 1 given	and	where	is objective. is a	-model of	if and only if there
exists a	-model	of	such that
 i  if	  then	and	  where
;
 ii  otherwise 	and	.
example 1 suppose
and the agent wants to ignore	. let	 
	 	 	 	 	 
	 	.	clearly 	has three	-models:
	 	  and
. from proposition 1 
has the following twelve -models:   where and . 
　nowwe consider the case when is of the form where is objective. updating with this type of is particularly useful in reasoning about sensing actions  scherl and levesque  1; son and baral  1  where represents the effect of a sensing action after whose execution  the agent will know either or its negation. we refer to such an update as a sensing update. the following proposition characterizes the update of with a formula of the form . it is interesting to note that the sufficient and
necessary condition for a -model of similar to the one presented in proposition 1.isproposition 1 given	andwhere	is ob-jective.	is a	-model ofifand only if there exists a	-model
	and	or	andof	such thatproof: let	and. fromthe definition of	  it is easy to see that	.consider any	where	and	.
according to the condition 1 of definition 1 	.
so	can not be a	-model of	. in other words  each
-model of	must have a form of	.
then from theorem 1  to prove the result  it is sufficient to
prove for	-model	where
or   . this can be shown in the same way as in the proof of proposition 1. 
example 1 suppose represents the current knowledge of an agent. note that implies that the agent does not have any knowledge about . consider the update of with which can be thought of as the agent trying to reason - in the planning or plan verification stage - about a sensing action1 that will give her the
knowledge about	. let
and	. it is easy to see that
and	are two	-models of	. then ac-
cording to the above proposition  it is obtained that
	and	are the two	-models of
. 
　as another important type of knowledge update  we consider the update of with . this update can be thought of as the result of an agent forgetting her knowledge about the fact . we will refer to such an update as a forgetting update. the following proposition shows that in order to forget from   for each -model of the current knowledge set  the agent only needs to expand the set of possible worlds of this model with exactly one specific world.
proposition 1 given	and	where	isobjective.	is a-model of	if and only ifthere exists a	-model	of	such that i  if	  then
;and  where ii  if	  then where	;and  iii  otherwise 	and.proof: letand	be a	-model of	.	then it is easy to see that for two-models	and	such thatand	  and	and	 . socan not be a -model of . in other words  a -model of must have a form .
　from theorem 1  to prove the result  we only need to show that for any -model such that and
	 	.
	let	  where	if
and if . we first prove that for any model such that and does not have a form of   .
	suppose	. clearly	.
case 1. consider a	-model	where
               . note that as well. however  from proposition 1  we have . so according to definition 1  i.e. condition  b  .
case 1. suppose  proper set inclusion . without loss of generality  we assume that where . this follows that and . so it is the case that and .
on the other hand  we have	  from definition 1  i.e. condition  a    we have	.
case 1. now suppose	and	. without
loss of generality  we can assume that
　　  where and . again  this results to the situation that and .  from the above discussion  it implies that .
following the same way as above  we can prove that under
the condition that	and where	  for any	-model	such that and	does not have a form of
.
now we show that for any -model that is of the form and is any worldsuch that
if	or	if	 	and
　　　　　. suppose	. since	  then according to definition 1  condition  a  or  b  should be satisfied. as	  condition  a  can not be satisfied.	so condition  b  must be satisfied.	that is  for any	such that	and	 	.
however  this implies that	  and also	. from proposition 1  results 1 and 1   it follows that   that is  . obviously  this is not true.
similarly  we can show that . that means  both and are in . this completes our proof. 
example 1 suppose represents the current knowledge of an agent. after executing a forgetting action the agent now would like to update her knowledge with . let
. it is easy to see thatand	are the two	-models of.	then us-ing proposition 1  we conclude that    and	are the four	-models of.	note that	cannot be a	-model of
proposition 1. according to1	persistence of knowledge and ignorance
like most systems that do dynamic modeling  the knowledge update discussed previously is non-monotonic in the sense that while adding new knowledge into a knowledge set  some previous knowledge in the set might be lost. however  it is important to investigate classes of formulas that are persistent with respect to an update  as this may partially simplify the underlying inference problem  engelfreit  1; zhang  1 . given and   a formula is said tobe persistent with respect to the update of with   if implies
         . if is of the form   we call this persistence as knowledge persistence  while if is of the form   we call it ignorance persistence. the question that we address now is that under what conditions  a formula is persistent with respect to the update of with .
　as the update of with is achieved based on the update of every -model of with   our task reduces to the studyof persistence with respect to a -model update. this is defined in the following definition.
definition 1  persistence with respect to	-model update 
let and be two formulas and be a -model. is persistent with respect to the update of with if for any
	 	implies	.
　clearly a formula is persistent with respect to the update of with if and only if for each -model of   is persistent with respect to the update of with . to characterize the persistence property with respect to -model updates  we first define a preference ordering on -models in terms of a formula.
definition 1  formula closeness  let be a formula and and be two -models. we say is as close to as
     denoted as   if one of the followingconditions holds:
1. ;
1. if	  then for any	 
.
we denote	if	and	.
　intuitively  the above definition specifies a partial ordering to measure the closeness between two -models to a formula. in particular  if is a -model of   then is closer to than all other -models  i.e. condition 1 . if neither nor is a -model of   then the comparison between and with respect to is defined based on the -model preference ordering for each -model of  i.e. condition
1 . note that if both	and	are	-models of	  we have and	  and both of them are equally close to	.
example 1 let	 	 	  and	. clearly 	has one	-model
	. consider two	-models
and . now let us compare which one of them is closer to . since neither nor is a -model of
  we can use condition 1 in definition 1 to compare	and
. according to definition 1  it is easy to see that
	as	. therefore  we conclude
	. furthermore  we also have	. 
proposition 1 let be a formula. for any two -models and   if   then implies .
proof: suppose	. then	. from
definition 1  we know that for any other	-model	 
         . so . but we have . this implies that both and are equally close to . hence 
. 
	given	a	formula	and	a	sequence	of	-models
  if the relation
holds  then it means that	is closer to	than	  where . now under this condition  if there is another formula which satisfies the property that	implies
whenever   we say that formula is persistent with respect to formula . in other words  when -models move closer to   's truth value is preserved in these -models. the following definition formalizes this idea.
definition 1   -persistence  let   be two formulas. we say that is -persistent if for any two -models and
	 	and	implies	.
now we have the followingimportant relationshipbetween -persistence and -model update persistence.
theorem 1 let	and	be two formulas and	be a	-
model. is persistent with respect to the update of with if is -persistent.
proof: let be a -model in . then we have . so for any -model   we have
　　　　　. so . now suppose is -persistent. it follows that implies . as is an arbitrary -model in   we can conclude that is persistent with respect to the update of with . 
　from theorem 1  we have that -persistence is a sufficient condition to guarantee a formula persistence with respect to a -model update. as will be shown next  we can provide a unique characterization for -persistence. we first define the notion of ordering preservation as follows.
definition 1  ordering preservation  given two formulas and . we say that ordering preserves ordering
if for any two	-models	and	 	implies
.
　the intuitionbehind ordering preservation is clear. that is  if preserves   then for any two -models and   whenever is closer to than   will be closer to than as well. finally  we have the following important
result to characterize	-persistence.
theorem 1 given two formulas and   is -persistent if and only if preserves .
proof:  	  suppose	is	-persistent. that is  for any two
-models	and	 	and	implies
　　　　.	so under the constraint that	is	-persistent  whenever	  we have	. that means  preserves	.
　    suppose preserves . from definition 1  we have that for any two -models and  
implies	. now suppose	. so we have
　　　　　. from proposition 1  we have that implies . from this it follows that is -persistent.

1	conclusion and future directions
in this paper we developed the notion of knowledge update - an analogous notion to belief update  that is useful in characterizing the knowledge change of an agent in presence of new knowledge. our notion is particularly relevant in reasoning about actions and plan verifications when there are sensing actions. we presented simpler alternative characterization of knowledge update for particular cases  and showed its equivalence to the original characterization. finally we discussed when particular knowledge  or ignorance  persists with respect to a knowledge update.
　we believe our work here to be a starting point on knowledge update  and as evident from the research in belief update and revision in the past decade. a lot remains to be done in knowledge update. for example  issues such as iterative knowledge update  abductive knowledge update  minimal knowledge in knowledge update  etc. remain to be explored. similarly  in regards to reasoning about actions  additional specific cases of knowledge update need to be identified and simpler alternative characterization for them will be needed to be developed. finally  we are currently working on establishing complexity results for knowledge update.
acknowledgement
the authors thank norman foo and abhaya nayak for useful discussions on this topic. the research of the firat author was supported by nsf under grants iri 1 and 1  and part of this research was performed while he was visiting the university of western sydney. the research of the second author was supported in part by australian research council under grant a1.
references
 engelfreit  1  j. engelfreit. monotonicity and persistence in preferential logics. journal of artificial intelligence research  1-1  1.
 fagin and et al  1  r. fagin and et al. reasoning about knowledge. mit press  1.
 kartha and lifschitz  1  g. kartha and v. lifschitz. actions with indirect effects: preliminary report. in proceedings of kr-1  pages 1  1.
 katsuno and mendelzon  1  h. katsuno and a. mendelzon. on the difference between updatinga knowledge base and revising it. in proceedings of of kr-1  pages 1- 1  1.
 meyer and van der hoek  1  j-j. meyer and w. van der hoek. epistemis logics for computer science and artificial intelligence. in cambridgetracts in theoretical computer science  1. cambridge university press  1.
 scherl and levesque  1  r. scherl and h. levesque. the frame problem and knowledge producing actions. in proceedings of aaai-1  pages 1  1.
 son and baral  1  t. son and c. baral. formalizing sensing actions: a transition function based approach. artificial intelligence  1-1  1.
 winslett  1  m. winslett. reasoning about action using a possible models approach. in proceedings of aaai-1  pages 1  1.
 zhang  1  y. zhang. monotonicity in rule based update. in proceedings of the 1 international conference on logic programming  iclp'1   pages 1. mit press  1.
resource-bounded inference from inconsistent belief bases
pierre marquis and nadege porquet` cril / universite＞ d'artois rue de l'universite＞ - s.p. 1
   f-1 lens cedex  france marquis  porquet  cril.univ-artois.frabstract
a family of resource-bounded paraconsistent inference relations is introduced. these relations are based on entailment  an inference relation logically weaker than classical entailment and parametrized by a set of variables. their properties are investigated  especially from the computational complexity point of view. among the strong features of our framework is the fact that tractability is ensured each time is bounded and that binary connectives behave in a classical manner. moreover  our family is large enough to include both inference  the standard inference relations based on the selection of consistent subbases and some additional forms of paraconsistent reasoning as specific cases.
1	introduction
few would dispute the fact that classical entailment is not suited to common-sense inference. one of its main drawbacks is its inability to deal with inconsistency. indeed  any formula is a logical consequence of a contradiction. such a trivialization of inference is often referred to as ex falso quodlibet sequitur and classical entailment is said to be explosive  i.e.  not paraconsistent .
　many approaches have been proposed so far to address this issue. some of them  like belief revision  aim at preventing inconsistencies from being generated; other approaches aim at removing inconsistencies once they appeared. complementary to them  many approaches deal with inconsistencies. among them are various paraconsistent logics  argumentative logics  belief merging and the so-called coherence-based approach1 to inconsistency handling. unfortunately  existing paraconsistent inference relations are typically intractable  see  cayrol et al.  1   nebel  1  .
　in this paper  we show that trivialization in presence of inconsistency and intractability can be handled within a uniform framework. a family of paraconsistent resourcebounded inference relations is presented. our starting point is the notion of inference  schaerf and cadoli  1  and the coherence-based approach to inconsistency handling  pinkas and loui  1   benferhat et al.  1 . in logic  every propositional symbol is interpreted in a weak way  allowing it to be both true and false at the same time  except for the variables of a given set that are interpreted classically. tractability is ensured by limiting the size of . interestingly  entailment proves sufficient to handle some inconsistencies  those that are not reachable when classical inference is limited to the variables of . in order to handle the remaining inconsistencies while avoiding trivialization  we introduce a family of inference relations which are to entailment what the inference relations of the standard coherence-basedapproachare to classical entailment. specifically  consistency is restored by removing variables  from   instead of removing some explicit beliefs from the belief base. the computational complexity of these inference relations is identified  and some other properties are investigated. among the strong features of our framework is the fact that tractability is ensured each time a computational bound is set. moreover  binary connectives behave in a classical manner while it is not the case for many paraconsistent logics. finally  our framework is general enough to encompass both logic  schaerf and cadoli  1   the standard coherence-based approach to inconsistency handling  pinkas and loui  1   benferhat et al.  1  and some systems for paraconsistent reasoning given in  priest  1   besnard and schaub  1  as specific cases.
1	formal preliminaries
in the following  we consider a propositional language inductively defined from a finite set of propositional symbols  the boolean constants and and the connectives     . for everyformula from   denotes the symbols of occurring in . as usual  every finite set of formulas is considered as the conjunctive formula whose conjuncts are the elements of the set.
　every formula from is interpreted in a classical way  unless stated otherwise. in order to avoid any ambiguity  we sometimes refer to -interpretations  -models  -consistent and -inconsistent formulas instead of the usual notions of  respectively  interpretations  models  consistent formulas and inconsistent formulas.
for the sake of simplicity  we assume that every formula
of	is in negation normal form  nnf   i.e.  only variables are in the scope of any occurrence of in . for instance  is in nnf while the  classically equivalent  formula is not. nevertheless  this assumption could be given up easily  cf. section 1 .
　we assume the reader familiar with the complexity classes p      and of the polynomial hierarchy
 see  papadimitriou  1  for details .
1	inference
our family of inference relations is based on logic  schaerf and cadoli  1   a multivalued logic which can be viewed as a generalization of levesque's logic of limited inference  levesque  1 . in logic  every propositional variable is interpreted in a weak way  allowing it to be both true and false at the same time  except for the variables of a given set .
definition 1 let	be a subset of	.
an -interpretationover is a mapping from the set of all literals over to s.t. for every literal of   we never have
   and we always have whenever the variable of belongs to . is always interpreted as while is always interpreted as .
an -interpretation is an -modelof a nnf formula iff the formula obtained by replacing in every
occurrence of	by	 resp.	  when  resp.	   then every occurrence of	by	 resp.   when	 resp.	  evaluates classically to	.
let be a belief base  i.e.  a conjunctively-interpreted finite set of formulas from  . let be a formula from . is an consequence of   noted   iff every -model of is an -  model of .
	a formula is said to be	consistent when it has an
	-model. a	-interpretation  resp.	-model  is just an
	-interpretation  resp.	-model  and a formula is
-consistent when it has a -model. every formula from which does not contain any occurrence of
is -consistent  the -interpretation s.t. for every literal of is a -model of it .
as shown in  schaerf and cadoli  1   for every
     the corresponding inference relation is an approximation of by below: if then .
　interestingly  the limited power of	inference enables some inconsistencies to be handled:
example 1 let
	.	is   - inconsistent.
let	. we have but	.
	let	. we have	. here 
	is explosive: any formula is an	consequence of	.
　tractability of entailment is ensured by limiting the size of . to be more specific  the time complexity of
inference is in	when	is a cnf formula1and is a clause  schaerf and cadoli  1 . accordingly  the complexity of inference depends essentially on the number of variables in .
1	resource-bounded paraconsistent inference
1	dealing with inconsistency
adhering to entailment is a way to prevent some inconsistencies from being harmful  namely those which are derivable only if classical reasoning over some variables outside is performed. in the previous example with   while we have both and   we do not have
.
　however  restricting to an arbitrary subset of the variables occurring in is not sufficient to avoid trivialization in every situation  see the previous example . in order to deal with the remaining inconsistencies  we suggest to focus on some subsets of   those for which the corresponding inference relations are not explosive. the approach is similar to the standard coherence-based approach to inconsistency handling  except that the inference relation is weakened by removing variables from instead of removing explicit beliefs from . formally  we first need a notion playing a role similar to the notion of consistent subbase in the standard coherence-based approach.
definition 1 let	be a belief base. let	and
	s.t.	. a consistent subset	of	w.r.t.
and	is a subset of	containing	and s.t.	. denotes the set of all consistent subsets of	w.r.t. and	.
　in this definition  is a given set of variables which must be interpreted classically. in our framework  plays a role similar to the one played by integrity constraints. requiring the existence of such a set  possibly empty  imposes that is -consistent  which is not a strong assumption .
　in the following  we adhere to a skeptical approach  every preferred consistent subset is considered :
definition 1 let	be a belief base and	be a subset of	s.t.	. let	be a selection policy s.t. is a subset of	and let	be a formula from	.	is a consequence of	w.r.t.	and
	  noted	  iff	 	.
　now  we can define selection policies for consistent subsets that are similar to the ones defined for consistent subbases in the standard coherence-based approach  pinkas and loui  1   benferhat et al.  1   benferhat et al.  1 .
definition 1 let be a belief base. let be a stratification1 of a given subset of where is s.t.
.
the set of all preferred consistent subsets of w.r.t. and for the possibilistic policy is the singleton   where is either the smallest index     s.t. is an inconsistent subset of w.r.t. and or if is consistent w.r.t. .

result can be easily generalized to the case where is any nnf formula and is a cnf formula; in this situation  inference can still be decided in time linear in the size of the belief base. 1
	a stratification of	is a totally ordered partition of it.
the set of all preferred consistent subsets of w.r.t. and for the linear order policy is the singleton   where     is defined by if is a consistent subset of
	w.r.t.	and	 	otherwise.
the set of all preferred consistent subsets of w.r.t. and for the inclusion-preference policy
	is	is a consistent subset of	w.r.t.	and
	and	s.t.	 
.
the set of all preferred consistent subsets of w.r.t. and for the lexicographic policy is is a consistent subset of w.r.t. and and
	s.t.	 
.
　these definitions assume that a stratification of the given set of variables is available. such a stratification is the formal counterpart of the preferential information used to discriminate among consistent subsets.
　here is an example showing the variety of conclusions that can be drawn in our framework  if both a formula and its negation are consequences of   the truth of given must be considered as doubtful :
example 1 let
	. with	or
	     	and	are consequences
of  but their negations are not  whatever among the four policies considered above. moreover:
	with	:
- w.r.t.  or    none of     and are consequences of .
- w.r.t.   is a consequence of   but and are not.
- w.r.t.   and are consequences of   but their negations are not.
	with	:
- w.r.t.   is a consequence of   but and are not.
- w.r.t.	 	or	 	and	are consequences of
  but their negations are not.
　clearly enough  in the case where   all our inference relations coincides with . accordingly  inference can be recovered as a specific case in our framework in this situation.
　the next proposition states that one of our basic objectives  avoiding trivialization  is always reached.
proposition 1 none of the relations with among       is explosive.
1	instantiating our framework
there are many ways to define a stratified set from a stratified belief base  sbb  1. first  can be defined as the set of all variables from the certain beliefs  i.e.  pieces of knowledge  of when available  provided that such a set is small enough and satisfies . otherwise  is defined as .
　defining is known as a difficult problem in the general case  schaerf and cadoli  1 . interestingly  when is a sbb  it is possible to exploit the given preferential information so as to define . a very natural way consists in considering in priority the variables of the most plausible beliefs until a preset bound on the size of is reached. formally  we consider the following set of variables :
definition 1 let be a sbb. let be a positive integer. let be a subset of s.t.
and . the inference level1 of given and   noted is the smallest     s.t.   where the sequence is inductively defined by and  
  if   otherwise. we define as .
　finally  once both and a flat set are available  a stratification of can be easily derived from the given stratification of . basically  once has been taken into account  it is reasonable to give more importance to the variables of that belong to the more plausible beliefs. loosely speaking  we want to reason as much as classically as possible from the most plausible beliefs in order to take into account their  possibly conflicting  classical consequences: inconsistencies must not be ignored when supported by plausible beliefs.
definition 1 let	be a sbb 	a subset of	and	a subset of	. the stratification of	induced by given	is	s.t. for every	  and	.
　there are many other ways to derive a stratification of . for instance  in the situation where we are ready to give more credit to facts than to rules  we can take advantage of the notion of degree of definiteness1 of a variable  as defined in  besnard and schaub  1   to achieve this goal from a cnf belief base .
1	logical properties and cautiousness
some logical properties. as in the standard coherence-based approach  it is easy to give a preferential models semantics to each of our relations . indeed  holds iff every preferred -model of w.r.t. is a model of   where the preferred -models of w.r.t. are exactly the -models of with .
thus  when   the preferred -models of w.r.t. are the -models of it encodingworlds that are  as normal
as possible   i.e.  as close as possible to	-interpretations .
　another strong point of our approach is that our inference relations exhibit a limited form of syntax-sensitivity. indeed  the binary connectives and of our language behave in a classical manner: the set of models of  resp.   is the intersection  resp. union  of the set of models of and the set of models of . as a consequence  given a
stratification	  everyformulafrom	 and
itself!  can be turned into a cnf formula without modifying the corresponding inference relations . the possibility of normalizing the belief base under cnf is particularly important for readibility reasons and the practical computing of the inference relations . moreover  this valuable property is usually not satisfied by paraconsistent logics  e.g.  the one given in  da costa  1   the one given in  lin  1   or the standard coherence-based approach . contrastingly  like any paraconsistent inference relation  our relations do not satisfy left logical equivalence. for the same reason  they do not satisfy right weakening  since they satisfy reflexivity . finally  unlike inference  schaerf and cadoli  1    they are not monotonic1  neither in nor in   in the general case.
example 1 let	 
	 	  and
	.	we have	but	and
　　　　　. similar counter-examples can be easily found for the other policies.
　this is not very surprising: while monotonicity is a highly desirable property when is consistent  this is not the case otherwise. it is natural that the epistemic status of beliefs changes when more evidence is got through the incorporation of additional explicit beliefs or through an improvement of inferential capabilities. specifically  increasing the computational effort can lead to discover inconsistencies that were hidden before.
cautiousness. in the spectrum of inference relations in our framework  one extreme bound corresponds to the case   while at the other extreme bound  we have . none of these two extreme cases can be considered as a  reasonable  inference relation for common-sense reasoning. on the one hand  the latter case corresponds to an ideal agent  with unlimited inferential capabilities: coincides with . accordingly  if is -consistent  all our inference relations coincide with  admittedly  this contrasts with many paraconsistent inference relations . on the other hand  the former case corresponds to an agent with very limited reasoning capabilities. indeed  if   all our inference relations correspond to levesque's -entailment  levesque  1 . this inference relation is tractable and avoids trivialization  as long as does not contain any occurrence of
      . however  with this inference relation  disjunctivesyllogism can never be applied: it is not possible to conclude
from . this suggests that cautiousness must be taken into account as an important criterion in the choice of a relation.

1
　　nevertheless  we have a  stratumwise  monotonicity property: for   for every   if
	  then	. this property is helpful
to check whether	in an incremental fashion.
　in the case where is a consistent subset  all our inference relations coincide with   so they are just as cautious as
     and the size of can be viewed as a degree of cautiousness. in the remaining case  our inference relations are more cautious relations than but this is what is expected since trivialization must be avoided. in this situation  our inference relations do not coincide in the general case  and results similar to the correspondingones in the standard coherence-based approach can be obtained; especially  the less cautious inference relations correspond to the and policies.
proposition 1
       is more cautious than and   but the converse does not always hold.
       is more cautious than but the converse does not always hold.
	cannot be compared with	and
w.r.t. cautiousness in the general case.
1	a general framework
our framework is quite general. as evoked previously  it includes inference as a specific case. as a consequence  it also includes the inference relation of the three-valued logic 1  priest  1  with  as designated truth values  defined by:
definition 1
an	-interpretation	over	is a mapping from to		.
the semantics of a formula in an -interpretation is defined inductively by:
- and	;
- ;
- ; -	. is an	-model of	iff	.
　　　　holds iff every	-model of	is an  	- model of	.
　indeed  it is not very difficult to prove that coincides with . more interestingly  let be the preference ordering over -interpretations defined by iff
		 ; then  the
relation defined by iff every minimal model of w.r.t. is an   - model of   can be recovered as a specific case of our relation:
proposition 1 let	and	be	two	formulas	from
	. we have	iff	.
　our work is also closely related to the approach to paraconsistent reasoning given in  besnard and schaub  1 . in this work  every formula of is associated to a default theory where:
	is a formula in the language	where
	;	is ob-
tained by replacing in every occurrence of a positive literal by the positive literal and every occurrence of a negative literal by the positive literal .
is a set of default rules

among the inference relations that can be defined from is skeptical signed inference; a formula of is a skeptical signed consequence of   noted
	iff	belongs to every extension of	.
　interestingly  the relation can be recovered as a specific case of our relation.
proposition 1 let	and	be	two	formulas	from
	. we have	iff	.
　finally  our approach encompasses as well the skeptical inference relations of the standard coherence-based approach1 as defined in  pinkas and loui  1   benferhat et al.  1   benferhat et al.  1 .
proposition 1 let	be a sbb.
we associate to	in polynomial time the belief base and the stratification
where all the	are new variables  not occurring in
 . for every selection policy	among	 	 	 
and every formula	not containing a new variable  we have iff	.
　focusing on variables instead of beliefs gives less syntaxsensitivity and some flexibility that is hardly achieved by the standard coherence-based approach. for instance  let us assume that gathers beliefs stemming from two different sources of information. the first source states while the second source states . the two sources are equally reliable; however  the first source is more reliable to what concerns while the second source is more reliable to what concerns . with   the standard coherencebased approach gives	  but the expected conclusion is just the negation of ! indexing variables with sources  we can express in our framework that from source 1. is more reliable than from source 1. and the opposite holds when source 1. is considered. indeed  with
 
　　　　　　　　  and	  we have	and we do not have	.
1	dealing with intractability
despite the generality of our framework  our inference relations are just as hard as the correspondingones in the standard coherence-basedapproach when the size of is not bounded.
clause / literal-complete-complete-complete-completetable 1: complexity of inference.
definition 1 for every selection policy   clause is the following decision problem:
input: a belief base   a subset of   a subset of s.t. and a cnf formula .
	query: does	hold  
literal is the restriction of this problem to the case where is a conjunction of literals.
proposition 1 the complexity of clause and literal is reported in table 1.
　interestingly  limiting the size of is sufficient to ensure tractability. the next proposition shows that our second objective is reached in this case:
proposition 1 the complexity of clause from a belief base given a stratification of a bounded set of propositional variables is in p.
the main reason is that when   there is only subsets of   which is a constant when is a constant.
accordingly  the naive algorithm consisting in generating by filtering out the preferred consistent ele-
ments of	  and testing for every of the resulting elements
whether or not holds runs in time   hence in for a constant   for every and a cnf query .
　of course  this is a  trick  in some sense but we can hardly do better: unless p = np  there is no tractable inference relation that is general enough to include as a specific case. in addition  we argue that focusing on as a tractable approximation of in order to design a paraconsistent inference relation is not so bad  especially when compared with other possible choices. indeed  let be a tractable approximation by below of upon which we want to build a tractable paraconsistent inference relation. requiring that such a tractable paraconsistent relation coincides with whenever is not explosive and is obtained by weakening through the removal of some beliefs of otherwise limits the spectrum of interesting relations . for instance  considering the restriction of where only unitrefutation derivable consequences of are considered would lead to a complex semantics with respect to which and
do not behave classically  crawford and etherington  1 ; considering the restriction of where is the subset of horn clauses of would not lead to a tractable relation  cayrol et al.  1   nebel  1 .
1	other related work
a close look at the ai literature shows that both trivialization and intractability can be handled by considering infraclassical inferencerelations  i.e.  approximationsby below of .
what is quite surprising is the fact that the two issues have usually not been addressed together.
　on the one hand  the proof theory of any paraconsistent logic limits the set of admissible proofs to a strict subset  i.e.  an approximation by below  of the classical proofs in the objective of avoiding trivialization. in the standard coherencebased approach  the focus is laid on some preferredconsistent subbases of   which can be considered as approximations of by below. preferred subbases can also be defined by splitting the belief base into several micro-theories characterized by their sets of variables  and selected in a dynamic way by looking at the variables occurring in the queries  chopra and parikh  1 . unfortunately  for all these approaches  the corresponding inference relations are typically at least as hard as classical inference in the general case.
　on the other hand  many approximation techniques have been developed so far to deal with the intractability issue  e.g.   levesque  1   crawford and kuipers  1   schaerf and cadoli  1   selman and kautz  1   dalal  1   crawford and etherington  1  ; in such approaches  the inference relation or the knowledge base itself are approximated into computationally easier one s . however  the trivialization issue is typically not addressed. for instance  the inference relation defined in  selman and kautz  1  is always explosive when is inconsistent  while the ones given in  schaerf and cadoli  1   crawford and etherington  1  are explosive in the general case when is inconsistent.
1	conclusion
the main contribution of this paper is a family of resourcebounded paraconsistent inference relations. in contrast to many approaches to inconsistency handling  the computational issue is very central in our framework. especially  the computational resources can be tuned to give rise to realistic  computationallyviable  inference relations. nevertheless  our framework is general enough to encompass logic  the standard coherence-based approach to inconsistency handling  and some other systems for paraconsistent reasoning as specific cases.
　several additional families of inference relations could be defined in our framework. analyzing their computational and logical properties is a topic for further research.
acknowledgements
many thanks to the reviewers for their helpful comments  and to the re＞gion nord / pas-de-calais and the iut de lens for their support. nade`ge porquet is granted by the r＞egion nord / pas-de-calais.
references
 benferhat et al.  1  s. benferhat  c. cayrol  d. dubois  j. lang and h. prade. inconsistency management and prioritized syntax-based entailment. proc. ijcai'1  1  1.
 benferhat et al.  1  s. benferhat  d. dubois  and h. prade. how to infer from inconsistent beliefs without revising  proc. ijcai'1  1  1.
 benferhat  1  s. benferhat. computing specificity in default reasoning. in handbook of drums  vol. 1  kluwer academic  1  1.
 besnard and schaub  1  ph. besnard and t. schaub. signed systems for paraconsistent reasoning. journal of automated reasoning  1-1  1.
 cadoli and schaerf  1  m. cadoli and m. schaerf. on the complexity of entailment in propositional multivalued logics. annals of mathematics and artificial intelligence  1-1  1.
 cayrol et al.  1  c. cayrol  m.-c. lagasquie-schiexand th. schiex. nonmonotonic reasoning: from complexity to algorithms. annals of mathematics and artificial intelligence  1-1 :1 1.
 chopra and parikh  1  s. chopra and r. parikh. an inconsistency tolerant model for belief representation and belief revision. proc. ijcai'1  1  1.
 crawford and etherington  1  j.m. crawford and d.w. etherington. a non-deterministic semantics for tractable inference. proc. aaai'1  1.
 crawford and kuipers  1  j.m. crawford and b. kuipers. towards a theory of access-limited logic for knowledge representation. proc. kr'1  1  1.
 da costa  1  n. da costa. on the theory of inconsistent formal systems. notre dame journal of formal logic  1-1  1.
 dalal  1  m. dalal. anytime families of tractable propositional reasoners. proc. ai&math'1  1  1.
 epstein  1  r.l. epstein. the semantic foundation of logic. vol. i: propositional logics. kluwer academic publisher  1.
 levesque  1  h.j. levesque. a logic of implicit and explicit belief. proc. aaai'1  1  1.
 lin  1  f. lin. reasoning in the presence of inconsistency. proc. aaai'1  1  1.
 nebel  1  b. nebel. how hard is it to revise a belief base  in handbook of drums  vol. 1  kluwer academic  1  1.
 papadimitriou  1  ch. h. papadimitriou. computational complexity  addison-wesley  1.
 pinkas and loui  1  g. pinkas and r.p. loui. reasoning from inconsistency: a taxonomyof principles for resolving conflict. proc. kr'1  1  1.
 priest  1  g. priest. reasoning about truth. artificial intelligence 1-1  1.
 priest  1  g. priest. minimally inconsistent lp. studia logica 1-1  1.
 schaerf and cadoli  1  m. schaerf and m. cadoli. tractable reasoning via approximation. artificial intelligence 1 :1  1.
 selman and kautz  1  b. selman and h.a. kautz. knowledge compilation and theory approximation. journal of the association for computing machinery
1 :1  1.
weakening conflicting information for iterated revision and knowledge
integrationsalem benferhat and souhila kaci
i.r.i.t.-c.n.r.s. universite paul sabatier＞
1 route de narbonne
1 toulouse cedex 1  france
{benferhat kaci} irit.fr
daniel le berre and mary-anne williams
business & technology research laboratory
the university of newcastle
newcastle  nsw 1  australia
{daniel maryanne} cafe.newcastle.edu.au

abstract
the ability to handle exceptions  to perform iterated belief revision and to integrate information from multiple sources are essential skills for an intelligent agent. these important skills are related in the sense that they all rely on resolving inconsistent information. we develop a novel and useful strategy for conflict resolution  and compare and contrast it with existing strategies. ideally the process of conflict resolution should conform with the principle of minimal change and should result in the minimal loss of information. our approach to minimizing the loss of information is to weaken information involved in conflicts rather than completely removing it. we implemented and tested the relative performance of our new strategy in three different ways. we show that it retains more information than the existing maxi-adjustment strategy at no extra computational cost. surprisingly  we are able to demonstrate that it provides a computationally effective compilation of the lexicographical strategy  a strategy which is known to have desirable theoretical properties.
1	introduction
information modeling and management is a fundamental activity of intelligent systems. intelligent systems require robust and sophisticated information management capabilities such as exception handling  iterated revision and the integration of information. in this paper we develop a novel and useful strategy for conflict resolution which can be applied to exception handling  iterated revision  and information integration. throughout we assume that the available information is given as ordered knowledge bases  i.e. a ranking of information as logical sentences. solving conflicts in our context means computing a consistent knowledge base. one well known system that can deal with conflicts in knowledge bases is the so-called adjustment procedure  williams  1 . in essence  adjustment propagates as many highly ranked formulas as possible  and ignores information at and below the highest rank where an inconsistency is found. the main advantage of this system is its computational efficiency. for example  it only needs at most log1n calls to a sat solver to build the consistent knowledge base where n is the number of ranks in the knowledge base. the obvious disadvantage of adjustment  however  is that it can remove more formulae than is necessary to restore the consistency of the knowledge base if the independence of information is not made explicit. in order to overcome this shortcoming another strategy called maxi-adjustment was introduced  williams  1  and implemented  williams and sims  1 . maxi-adjustment has proved to be a useful strategy for real world applications e.g. software engineering  williams  1   information filtering  lau et al.  1  and intelligent payment systems  wong and lau  1 . the main idea of maxi-adjustment is to solve conflicts at each rank of priority in the knowledge base. this is done  incremently  starting from the information with highest rank. when inconsistency is encountered in the knowledge base  then all formulas in the rank responsible for the conflicts are removed. the other formulas are kept  and the process continues to the next rank.
clearly maxi-adjustment keeps more information than adjustment  since it does not stop at the first rank where inconsistency is met. even though maxi-adjustment propagates more information than adjustment  one can still argue that maxi-adjustment removes too much information because it adopts a sceptical approach to the way it removes the conflict sets at each rank.
the purpose of this paper is to describe a significant improvement to maxi-adjustment. we call this system disjunctive maxi-adjustment  and denote it by dma. the idea is similar to maxi-adjustment  except that information is weakened instead of being removed when conflicts are detected. so instead of removing all formulas involved in conflicts  as it is done in maxi-adjustment  dma takes their disjunctions pairwise. if the result is consistent  then we move to the next rank. if the result is still inconsistent  then we replace the formulas in conflicts by all possible disjunctions involving 1 formulas in the conflict sets and again if the result is consistent we move to the next layer  and if it is inconsistent we consider disjunctions of size 1  etc. the only case where all formulas responsible for conflicts are removed is when the disjunction of all these formulas is inconsistent with the higher priority information.
this paper focuses on the dma strategy from the theoretical and experimental perspectives. in particular 
  we show that dma is equivalent to the well known lexicographical strategy  benferhat et al.  1; lehmann  1 . more precisely  we show that for an inconsistent base k if δdma k  is the classical base obtained using dma  and δlex k  is the set of all lexicographically maximal consistent subbases of k  then:
 ψ  δdma k  ` ψ iff  a （ δlex k  a ` ψ.
in other words  we obtain the surprising and computationally useful result that dma provides a  compilation  of lexicographical systems.
  it is well known that computing conflicts is a hard task  and we are able to show that dma works even if the conflicts are not explicitly computed. for this  we propose an alternative  but equivalent  approach to dma called whole-dma where disjunctions are built on the whole stratum when we meet inconsistency instead of only on the conflicts.
  we also propose another equivalent alternative to dma called iterative-dma where instead of considering disjunctions of size  1  etc  on the initial set of conflicts  we only compute disjunctions of size 1 but on new sets of conflicts.
  lastly  we compare these different implementations of dma experimently  and contrast their applicability.
1	ordered information in spohn's ocf framework
we consider a finite propositional language denoted by l. let   be the set of interpretations. ` denotes the classical consequence relation  greek letters φ ψ ... represent formulas. we use spohn's ordinal conditional function  spohn  1  framework  which is also known as the kappa function framework.
at the semantic level  the basic notion of spohn's ordinal conditional function framework is a distribution called an ocf  denoted by κ  which is a mapping from   to n  such that  ω κ ω  = 1. n is the set of natural numbers. κ ω  can be viewed as the degree of impossibility of ω.
by convention  κ ω  = 1 means that nothing prevents ω from being the real world  and κ ω  = +± means that ω is certainly not the real world1. the lower κ ω  is  the more expected it is  i.e. if κ ω    κ ω1  then ω is said to be more plausible than ω1.
in practice  ocf distributions over all possible worlds are not available  however a ranked knowledge base provides a compact representation of an ocf distribution  williams  1 . since we will be working with ranked knowledge bases throughout  we define a knowledge base to be ranked. in particular  a knowledge base is a set of weighted formulas of the form k = { φi ki  : i = 1 ... n} where φi is a classical formula and ki is a positive number representing the level of priority of φi. the higher ki  the more important the formula φi.
given k  we can generate a unique ocf distribution  denoted by κk  such that all the interpretations satisfying all the formulae in k will have the lowest value  namely 1  and the other interpretations will be ranked with respect to the highest formulae that they falsify. namely: definition 1  ω （   

then  given κk associated with a knowledge base k  the models of k are the interpretations ω s.t. κk ω  = 1.
1	adjustment and maxi-adjustment
1	stratified vs ranked knowledge base
we have seen that ranked information is represented by means of knowledge bases of the form k = { φi ki  : i = 1 ... n}. we sometimes also represent this base k in a stratified form as follows: k = {s1 ... sn} where si  i = 1 ... n  contains classical formulas of k having the same rank and which are more reliable than formulas of sj for j   i. so the lower the stratum  the higher the rank.
in this representation  subbases are also stratified. that is  if a is a subbase of k = {s1 ... sn}  then a = {a1 ... an} such that aj   sj  j = 1 ... n.
 aj may be empty .
conversely  we can represent a stratified base k = {s1 ... sn} using a weighted knowledge base by associating formulas of each strata si to the same rank ki. these ranks should be such that k1 ... kn.
let us now introduce the notion of conflicts and kernel which will prove useful in the subsequent discussion:
definition 1 let k = {s1 ... sn} be a stratified base. a conflict in k  denoted by c  is a subbase of k such that:
　  c `〕  inconsistency  
　   φ φ （ c  c   {φ} 1〕  minimality . definition 1 let c be the set of all possible conflicts in k. we define the kernel of k  denoted by kernel k   as the set of formulas of k which are involved in at least one conflict in c i.e.  kernel k  is the union of all conflicts in k.
formulas in k which are not involved in any conflict in k are called free formulas.
1	the problem
our aim in this paper is to address the problem of identifying conflicts for the purposes of drawing plausible inferences from inconsistent knowledge bases  iterated revision and information integration. our technique for resolving conflicts can be used:  i  to build a transmutation for iterated belief revision  williams  1  where the new information can be incorporated into any rank  and  ii  for theory extraction  williams and sims  1  which provides a natural and puissant mechanism for merging conflicting information. without loss of generality we focus on a particular case of revision where some new information   is added to some ranked knowledge base k. namely  given a knowledge base k  and a new formula   we compute δ k “ {   +± }   the classical  not stratified  consistent subbase of k “ {   +± }. then  ψ is said to be a plausible consequence of k “ {   +± } iff δ k “ {   +± }  ` ψ. in the rest of this paper we simply write k  instead of k “ {   +± }. in a stratified form we write {s1 s1 ... sn} where s1 = { }. we briefly recall two important methods to compute δ k “ {   +± } : adjustment and maxi-adjustment. we will illustrate them using a simple example. we point the reader to  williams  1; 1  for more details.
1	adjustment
from a syntactical point of view  the idea of adjustment is to start with formulas having the highest rank in k  and to add as many prioritized formulas as possible while maintaining consistency. we stop at the highest rank  or the lowest stratum  where we meet inconsistency called the inconsistency rank of k   denoted by inc k  .
note that a more efficient binary search based algorithm which only needs log1n consistency checks has been developed and implemented1  williams and sims  1 . the selected base will be denoted by δa k  . note that the process of selecting the consistent base using the adjustment for new pieces of information placed in the highest rank is identical to that used in possibilistic logic  dubois et al.  1 . one can easily see that this is not a completely satisfactory way to deal with the inconsistency since formulas with rank lower than inc k   are ignored even if they are consistent with the selected base.
a formula ψ is said to be an adjustment consequence of k   denoted by k  `a ψ  if δa k   ` ψ. one important property of adjustment is that it is semantically well defined. more precisely  we have the following soundness and completeness result: k  `a ψ iff  ω （ pref κk   ω |= ψ  where pref κk   is the set of interpretations which satisfy   and have minimal rank in the ocf distribution κk  given by definition 1.
example 1 let k = {s1 s1 s1} be such that s1 = {  a ‥  b ‥ c 1    d ‥ c 1    e ‥ c 1 }  and 1 =  a 1    1   g 1    1  . let   =  c.
first  we have δa k c  = { c}.
there is no conflict in δa k c  “ s1 then δa k c  ○ { c  a ‥  b ‥ c  d ‥ c  e ‥ c}.
now  s1 contradicts δa k c  due to the conflicts {d  d ‥ c  c} and {e  e ‥ c  c}. then  we do not add the stratum
s1 and the computation of δa k c  is achieved  and we get δa k c  = { c  a ‥  b ‥ c  d ‥ c  e ‥ c}.
note that δa k c  1` h  even if h is not involved in any conflict in k c.
1	maxi-adjustment
maxi-adjustment  williams  1  was developed to address the problem of discarding too much information for applications like software engineering  williams  1  and information filtering  lau et al.  1 .
the idea in maxi-adjustment also involves selecting one consistent subbase from k denoted by δma k  . the difference is that it does not stop at the first rank where it meets inconsistency. moreover  conflicts are solved rank by rank. we start from the first rank and take the formulas of s1 which do not belong to any conflict in { } “ s1. let the set of these formulas. then  we move to the next rank and add all formulas which are not involved in any conflict in  and so on. it is clear that maxi-adjustment keeps more formulas than the adjustment.
example 1 using maxi-adjustment  first  we have δma k c  = { c}.
there is no conflict in δma k c  “ s1 then δma k c  ○ { c  a ‥  b ‥ c  d ‥ c  e ‥ c}.
now  s1 contradicts δma k c  due to the conflicts {d  d ‥ c  c} and {e  e ‥ c  c}. then  we do not add the clauses from s1 involved in these conflicts: δma k c  ○ δma k c  “ {f  f ‥  g ‥ c}.
now  s1 contradicts δma k c  due to the conflicts {a b  a ‥  b ‥ c  c} and {f g  f ‥  g ‥ c  c}.
since all the clauses  except h  from the stratum s1 are involved in one conflict  we only add h to δma k c . finally  we get: δma k c  = { c  a ‥  b ‥ c  d ‥ c  e ‥ c f  f ‥  g ‥ c h}.
note that δma k c  ` h.
1	disjunctive maxi-adjustment
although maxi-adjustment retains more information than adjustment  it can still be argued that it is too cavalier in the way it solves the conflicts.
in this section  we propose a new strategy which is a significant improvement of maxi-adjustment. the computation of the consistent base is essentially the same as in maxi-adjustment  the only difference is when we meet an inconsistency at some rank  instead of removing all the formulas involved in the conflicts at this rank we weaken them  by replacing them by their pairwise disjunctions. if the result is consistent then we move to the next rank  else we replace these formulas by their possible disjunctions of size 1. if the result is consistent then we move to the next rank  else we add the disjunctions of size 1 of these formulas  and so on. we summarize this process in algorithm 1:
notation: dk c  is the set of all possible disjunctions of size k between formulas of c. if k  |c| then dk c  =  .
example 1  using dma  first  we have kb = { c}.
there is no conflict in kb “ s1 then
kb ○ { c  a ‥  b ‥ c  d ‥ c  e ‥ c}.
now  s1 contradicts kb due to the conflicts {d  d ‥ c  c} and {e  e ‥ c  c}. we do not add the clauses from s1 involved in these conflicts: kb ○ kb “ {f  f ‥  g ‥ c}.
now we create all the possible disjunctions of size 1 with
c = {d e}: d1 c  = {d ‥ e}. since kb “ d1 c  is inconsistent  and we cannot create larger disjunctions  we do not add anything from s1 to kb.
algorithm 1: dma k    
data: a stratified knowledge base k = {s1 ... sn} ; a new sure formula:   ; result: a consistent subbase δdma k   begin
if kb “ si is consistent then kb ○ kb “ si
let c be the subset of si in kernel of kb“si;
kb ○ kb “ {φ : φ （ si and φ 1（ c}; k ○ 1 ;
while k ＋ |c| and kb“dk c  is inconsistent
k ○ k + 1;
if k ＋ |c| then kb ○ kb “ dk c  ;
end
please note at this rank  we do not add more information than maxi-adjustment.
now  s1 contradicts kb due to the conflicts {a b  a ‥  b ‥ c  c} and {f g  f ‥  g ‥ c  c}. h is not involved in any conflict. then  kb ○ kb “ {h}.
we now create all the possible pairwise disjunctions with
c = {a b g}: d1 c  = {a ‥ b a ‥ g b ‥ g}.	since
kb “d1 c  is inconsistent  we create d1 c  = {a‥b‥g}. since kb “ d1 c  is consistent  we add d1 c  to kb and the algorithm stops.
then δdma k   = { c  a‥ b‥c  d‥c  e‥c f  f ‥
 g ‥ c h a ‥ b ‥ g}
which is equivalent to { c  a‥ b  d  e f  g h a‥b}.
dma keeps more information from the last stratum than maxi-adjustment does.
definition 1 a formula ψ is said to be a dma consequence of k and    denoted by k  `dma ψ  if it is inferred from δdma k  . namely  k  `dma ψ iff δdma k   ` ψ.
1	two other implementations of dma
in the previous section we have shown a way to compute δdma k   using the computation of the kernel. in this section  we propose two alternative ways to compute δdma k  . the first approach  called whole-dma k     does not compute the kernel. the main motivation for this alternative is that computing the kernel is in general hard. for the second approach  called iterative-dma k     when inconsistency is  again  met after weakening the kernel  then rather than weakening the original kernel by considering its disjunctions of size 1  we only weaken the newly computed kernel obtained by considering disjunctions of size 1. the motivation of this approach is to reduce the size of added  disjunctions  formulas.
1 whole disjunctive maxi-adjustment we propose a slightly modified version of the dma algorithm. the idea is that when kb “si is inconsistent  instead of considering all possible disjunctions of size j of elements of si which are in kernel kb“si   we consider all possible disjunctions of size j of si without computing a kernel. this is justified by the following proposition:
proposition 1 let kb “ s be inconsistent. let c be the subset of s in kernel kb “ s   and f = s   c be the set of remaining formulas. let dj c   resp. dj s   be the set of
all possible disjunctions of size j from c  resp. s . then 
kb “ dj c  “ f 《 kb “ dj s .
proof  sketch 
let us assume that dj 1 s  “ kb is inconsistent  and show that dj s  “ kb 《 dj c  “ kb “ f.
it is clear that dj c    dj s . hence it is enough to show that kb “ dj s  ` f.
let a be a conflict of kb “ dj 1 s   and {ψ1 ... ψn} be a subset of a in dj 1 s . let   （ f.
then {  ‥ ψ1 ...   ‥ ψn}   dj s   with ψi 1=   since   1（ a
 because   is free .
now since kb “ a is inconsistent then kb `  ψ1 ‥ ，，， ‥  ψn.
applying successive resolutions between {  ‥ ψ1 ...   ‥ ψn} and  ψ1 ‥ ，，， ‥  ψn leads to entail  .
hence there is no need to consider disjunctions containing free formulas since they will be subsumed.
with the help of this proposition  the  else  block in the
dma algorithm is replaced by else
k ○ 1
while kb “ dk si  is inconsistent and k ＋| si | do k ○ k + 1
if k ＋|si| then kb ○ kb “ dk si 
to obtain the whole dma algorithm.
example 1  using whole dma  first  we have kb = { c}.
s1 is consistent with kb. then  kb ○ kb “ s1.
now  s1 contradicts kb. we compute all possible pairwise disjunctions with s1. d1 s1  = {d ‥ e d ‥ f d ‥  f ‥  g ‥ c e ‥ f e ‥  f ‥  g ‥ c}.
since  kb “ s1 is inconsistent  we compute all possible disjunctions of size 1 between formulas of s1. we get d1 s1  = {d‥e‥f d‥e‥ f ‥ g‥c} which is consistent with kb. then  kb ○ kb “ d1 s1 .
now  s1 is inconsistent with kb. we compute all possible pairwise disjunctions with s1. d1 s1  = {a ‥ b a ‥ g a ‥ h b‥g b‥h g‥h} which is still inconsistent with kb. we have d1 s1  = {a ‥ b ‥ g a ‥ b ‥ h b ‥ g ‥ h a ‥ g ‥ h} which is consistent with kb  then kb ○ kb “ d1 s1 . hence  δwdma k c  = { c  a‥ b‥c  d‥c  e‥ c d‥ e‥f d‥e‥ f‥ g‥c a‥b‥g a‥b‥h b‥g‥h a‥g‥h} which is equivalent to { c  a‥ b  d  e f  g a‥b h}.
then  it is equivalent to δdma k c .
1	iterative disjunctive maxi-adjustment
the idea of this alternative implementation of dma is as follows: let si be inconsistent with kb. let c and f be the kernel and the remaining formulas of si. now assume that kb “f “d1 c  is still inconsistent. then rather than weakening c again by considering disjunctions of size 1  we only weaken those formulas in d1 c  which are still responsible for conflicts. namely  we split d1 c  into c1 and f1 which respectively represent the kernel and remaining formulas of d1 c . then instead of taking kb “ f “ d1 c  as in dma  we take kb “ f “ f1 “ d1 c1 . the algorithm becomes:
algorithm 1: idma k    
data: a stratified knowledge base k = {s1 ... sn} ; a new sure formula:   result: a consistent subbase δidma k   begin
kb ○ { }  i ○ 1; while i ＋ n do
if kb “ si is consistent then
kb ○ kb “ si ; i ○ i + 1 ;
let c   si be in kernel kb “ si  ; si ○ {φ : φ （ si and φ 1（ c} ; if | c |= 1 then i ○ i + 1 else si ○ si “ d1 c  ;
proposition 1 let kb “ f “ di c  be inconsistent. then 
　　　kb “ f “ di c  《 kb “ f “ f1 “ d1 c1   where f1 and c1 are kernels from di c .
the proof is a corollary of prop. 1 and the following lemma:
lemma 1 let a be a set of formulas. let b = di a  and c = di+1 a  be the set of all possible disjunctions of a of size i and i + 1 respectively. then  c = d1 b .
this lemma means that taking all disjunctions of size i  then reconsidering all disjunctions of size 1 again on the result is the same as considering all disjunctions of size i + 1.
example 1  using iterative dma  first  we have kb = { c}.
there is no conflict in kb “ s1. then  kb ○ kb “ s1. s1 is inconsistent with kb due to the conflicts { c  d‥c d} and { c  e ‥ c e}. we add {f  f ‥  g ‥ c} to kb. the disjunction d‥e is still inconsistent with kb  then we move to s1.
s1 contradicts kb due to the conflicts {a b  a‥ b‥c  c} and {f g  f ‥ g ‥c  c}. h is not involved in any conflict.
then  kb ○ kb “ {h}.
we now create all the possible pairwise disjunctions with c = {a b g}: d1 c  = {a ‥ b a ‥ g b ‥ g}.
kb“d1 c  is inconsistent due to the conflict { c  a‥ b‥ c f  f ‥ g ‥c a‥g b‥g}. a‥b in d1 c  is not involved in the conflict  then kb ○ kb “ {a ‥ b}.
g . 1    = a g . 1    is consistent. however  there is no need to add a ‥ b ‥ g to kb since a ‥ b already belongs to kb. hence  δidma k c  = { c  a ‥  b ‥ c  d ‥ c  e ‥ c f  f ‥  g ‥ c a ‥ b h} which is equivalent to { c  a‥ b  d  e f  g a‥b h}.
hence  it is equivalent to δdma k c .
1	dma: compilation of lexicographical inferences
the aim of this section is to show that dma is a compilation of the lexicographical system  hence it satisfies the agm postulates  alchourron＞ et al.  1 . first let us recall the lexicographical inference.
1	lexicographical inference
the lexicographical system  benferhat et al.  1; lehmann  1  is a coherence-based approach where an inconsistent knowledge base is replaced by a set of maximally preferred consistent subbases. the preference relation between subbases is defined as follows:
definition 1 let	a	=	{a1 ... an}	and	b	=
{b1 ... bn} be two consistent subbases of k.
a is said to be lexicographically preferred to b  denoted by a  lex b  iff
 k s.t. |ak | |bk | and  j   k |aj |=|bj |.
let δlex k   denotes the set of all lexicographically preferred subbases of k   those which are maximal w.r.t.  lex. then  the lexicographical inference is defined by:
definition 1 a formula ψ is said to be a lexicographical consequence of k   denoted by k  `lex ψ  if it is a classical consequence of all the elements of δlex k    namely  a （ δlex k   a ` ψ.
example 1  continued 
we have δlex k c  = {a b} where a = { c  a ‥  b ‥ c  d ‥ c  e ‥ c f  f ‥  g ‥ c a h} and b = { c  a ‥  b ‥ c  d ‥ c  e ‥ c f  f ‥  g ‥ c b h}.
for example  we have
k c `lex a ‥ b since a ` a ‥ b and b ` a ‥ b.
1	basic steps of the compilation
the aim of this section is to show that dma is equivalent to the lexicographical system. dma offers a clear advantage over the lexicographical system because it obviates the need to explicitly compute δlex k   which may be exponential in size. formally  we will show the following equivalence:
	k  `lex ψ   k  `dma ψ	 1 
note that δdma k   is a classical consistent base.
example 1  continued 
let us first show that applying the lexicographical system on k c gives the same results as applying dma on k c. indeed  k c `lex ψ iff a ` ψ and b ` ψ iff a ‥ b ` ψ iff { c  a ‥  b  d  e f  g a ‥ b h} ` ψ
 after removing subsumed formulas in a ‥ b  iff δdma k c  ` ψ iff k c `dma ψ.
to show  1  we follow the following steps:
step 1: we construct a new base k1 from k s.t.
		 1 
namely  applying lexicographical system on k  is equivalent to applying adjustment to.
step 1: in the second step we show that
		 1 
namely  applying adjustment tois equivalent to applying
dma to k .
step 1: constructing k1
in order to show the proof of  1   we need to rewrite the lexicographical system at the semantic level  which is immediate: definition 1 let k = {s1 ... sn}. let ω and ω1 be two interpretations  and aω  aω1 be the subbases composed of all formulas of k satisfied by ω and ω1 respectively. then  ω is said to be lexicographically preferred to ω1 w.r.t. k  denoted by ω  lex k ω1  iff aω  lex aω1  using definition 1 .
proposition 1 let δlex k  be the set of lexicographical preferred consistent subbases of k. let aω be the set of formulas in k satisfied by ω. then  i. if ω is minimal w.r.t.  lex k then aω （ δlex k  ii.  a （ δlex k   ω |= a s.t. ω is minimal w.r.t.  lex k.
using prop. 1  at the semantic level   1  is equivalent to:
         κk 1  ω    κk 1  ω1  iff ω  lex k  ω1  1  where κk 1 is the ocf associated to  obtained from
definition 1.
let us now show how to construct k1 from k such that it satisfies  1 . for this  we use two intuitive ideas.
the first idea is that adjustment is insensitive to the number of equally reliable formulas falsified while lexicographical system is not  i.e. cardinality of conflict sets . assume that we have a base k = { φ i   ψ i } which contains two formulas with a same rank. then  the rank  using def. 1  associated with an interpretation ω falsifying one formula has a same rank as an interpretation falsifying two formulas. however  if we use the lexicographical system  an interpretation falsifying one formula is preferred to an interpretation falsifying two formulas. now one can check that if we construct a knowledge base k1 = { φ i   ψ i   φ‥ψ 1i } from k by adding the disjunction φ ‥ ψ with a higher rank  then equation  1  is satisfied. so the first idea is to add disjunctions with the rank equal to the sum of ranks of formulas composing the disjunctions.
the second idea is related to the notion of compensation. to illustrate this idea  let us now consider k = {s1 s1} such that s1 = {φ1} and s1 = {φ1 φ1 φ1}. the intuition behind this example is to show that ranks associated with the formulas should satisfy some constraints in order to recover the lexicographical inference. indeed  let us for instance associate the rank 1 with φ1  and the rank 1 with φ1 φ1 φ1. let ω and ω1 be two interpretations such that aω = {{φ1} {}} and aω1 = {{} {φ1 φ1 φ1}}. aω means that ω satisfies all formulas of s1 but falsifies all formulas of s1. aω1 means that ω1 satisfies all the formulas of s1 but falsifies all the formulas of s1. following the suggestion of the first idea  let us add all possible disjunctions. we obtain:
k1 = {{ φ1‥φ1‥φ1‥φ1 }; { φ1‥φ1‥φ1 ; φ1‥φ1‥ φ1 ;  φ1 ‥ φ1 ‥ φ1 }; { φ1 ‥ φ1 ; φ1 ‥ φ1 ; φ1 ‥ φ1 ;  φ1‥φ1‥φ1 }; { φ1 ; φ1‥φ1 ; φ1‥φ1 ;  φ1 ‥ φ1 };{ φ1 ; φ1 ; φ1 }}.
we can easily check that κk1 ω  = 1 and κk1 ω1  = 1 while aω  lex k aω1. this is due to the fact that the disjunction φ1 ‥ φ1 ‥ φ1 has a rank higher than φ1. hence  there is a compensation effect. so  in order to recover the lexicographical order  φ1 must have a rank strictly greater than the rank of φ1 ‥φ1 ‥φ1. a way to do this is to significantly differentiate the different ranks associated with strata. for this  we associate to each formula  φij ki  （ si the rank nki where n is very large. n should be s.t.  i  nki   Σj inkj. such an n always exists. it means that the rank given to a stratum must be greater than the sum of all the ranks of the less reliable strata.
following these two ideas  k1 is formally constructed as follows:
let k = {s1 ... sn}  and   a new sure information: 1. we define a new base b:
b = { φij nki  : i = 1 n and φij （ si}.
1. k1 = { dj b  aj } where dj b  is the set of all possible disjunctions of size j between formulas of b  and aj is the sum of ranks of formulas in dj b .
then we have:
proposition 1 .
step 1: adjustment on dma on k 
the following proposition shows that the base k1 constructed in step 1 allows us to recover the lexicographical system.
proposition 1 let k = {s1 ... sn} be a stratified base  and   be a sure formula. let k1 be a base constructed in
step 1. then 
.
due to the lack of space  we skip the proof of prop. 1 and illustrate its main ideas by an example. the idea is to simplify the computation of until recovering δdma k  .
example 1 let k = {s1 s1} where s1 = { a ‥  b ‥ c} and s1 = {a b g}. let   =  c.
first it can be checked that
δdma k c  = { c  a ‥  b ‥ c a ‥ b g}. let n be a large number. using step 1  we have:
b = {  a ‥  b ‥ c n1   a n   b n   g n }.
the base k1 obtained from step 1  after removing tautologies : k1 = {  a ‥  b ‥ c ‥ g n1 + n    a ‥  b ‥ c n1   a ‥ b ‥ g 1n    a ‥ b 1n   a ‥ g 1n   b ‥ g 1n   a n   b n   g n }.
since we apply adjustment on  the first idea is to ignore formulas in k 1 c under the inconsistency level  see section
1 . we can check that. then  
is the classical base  obtained by ignoring the ranks  associated with {  c +±    a‥ b‥c‥g n1+n    a‥ b‥ c n1   a‥b‥g 1n   a‥b 1n   a‥g 1n   b‥g 1n }. the second idea is that subsumed disjunctions are not added. in this example  since  a‥ b‥c and a‥b a‥g b‥g will belong to δa k 1 c  then there is no need to keep the disjunctions  a ‥  b ‥ c ‥ g and a ‥ b ‥ g.
lastly  the other disjunctions can be refined. since c = { c  a ‥  b ‥ c a b} is inconsistent  then all disjunctions constructed from g and this conflict c are reduced to g. therefore  we have which is equivalent to δdma k c .
1	experimental results
we now present some experimental results which illustrate the different behaviour of each strategy. we used a propositional logic implementation of the strategies1. we chose 1 inconsistent bases at random from the dimacs challenge  aim-1-no  containing 1 variables each and 1 clauses for the first 1  1 clauses for the others. then we stratified the bases with 1 clauses per strata  keeping the clauses in their original order. it appeared that each time the conflicts were discovered and weaken in the second strata  no more appeared in the remaining strata. the following table gives the number of clauses in the second strata after applying a given strategy. wdma  resp. idma  stands for whole-dma  resp. iterative dma .
#clausest1t1t1t1t1t1t1t1adj.1111ma1111dma1111wdma1111idma1111there are no differences between dma and idma because on these examples consistency was either restored using d1 c   t1 t1 t1 t1 t1  or all the clauses involved in a conflict have to were removed. whole-dma clearly hides the information contained in the knowledge base by generating a large number of clauses but timewise its fast. let us now take a look at the time spent computing each strategy.
time  s t1t1t1t1t1t1t1t1adj.11111111ma1.1.11111dma1.1.11111wdma11111111idma1.1.11111these results can be interpreted as follows: computing the set of clauses involved in conflicts  kernel  is costly  so all methods relying on this information will require small kb's to revise. this can be achieved for instance using modular kb's  a common practice in knowledge engineering. interestingly  since the three dma approaches we introduced are logically equivalent  we can propose one way to efficiently compute the dma policy: whole dma  only based on satisfiability testing. this method can be used for instance if the knowledge base is hidden to the final user  and that only the queries are important. on the other hand  if the knowledge base itself is important for the user  such that the revised base must be as  close  as possible to the original one  an idma approach should be used  only necessary information will be weakened   but a computational cost must be paid. dma is a tradeoff between these two policies.
1	conclusion
we introduced a new family of computationally effective strategies for conflict resolution which can be used for exception handling  iterated belief revision and merging information from multiple sources. the most important feature of our strategy is that it relies on weakening conflicting information rather than removing conflicts completely  and hence is retains at least as much  and in most cases more  information than all other known strategies. furthermore  it achieves this higher retention of information at no extra computational cost. we compared and contrasted three implementations of our new strategy with existing ones from a theoretical standpoint and by measuring their relative performance. we were also able to show the surprising result that the dma policy provides a compilation of the lexicographical system which is known to have desirable theoretical properties. dma offers the clear advantage of obviating the need to explicitly compute the set of all preferred subbases which can be hard. another pleasing result is that the dma strategy can be implemented as whole-dma where the need to explicitly compute the culprits responsible for the conflicts is not required.
references
 alchourron＞ et al.  1  c. e. alchourron  p. g＞ ardenfors and d.： makinson on the logic of theory change: partial meet contraction and revision functions. journal of symbolic logic  1  1.
 benferhat et al.  1  s. benferhat  c. cayrol  d. dubois  j. lang  h. prade. inconsistency management and prioritized syntax-based entailment. proc. ijcai  france  1.
 dubois et al.  1  d. dubois  j. lang  h. prade. possibilistic logic. handbook of logic in ai and log. prog.   1   1.
 lau et al.  1  r. lau  a.h.m. ter hofstede  p.d. bruza and k.f. wong. belief revision and possibilistic logic for adaptive information filtering agents. proc. ieee  ictai   canada  1.
 lehmann  1  d. lehmann. another perspective on default reasoning. annals of mathematics and artif. intel.  1  1.
 spohn  1  w. spohn. ordinal conditional functions: a dynamic theory of epistemic states. in: causation in decision  belief change and statistic  1   reidel  dordrecht  1.
 williams  1  m. a. williams. transmutations of knowledge systems. proc. kr'1  1.
 williams  1  m. a. williams. a practical approach to belief revision: reason-based change. proc. kr'1  1.
 williams  1  m. a. williams. applications of belief revision. in transactions and change in logic databases  lnai 1  1  springer verlag.
 williams and sims  1  m. a. williams and a. sims saten: an object-oriented web-based revision and extraction engine. proc. of the int. workshop on nonmonotonic reasoning  nmr' 1 . online computer science abstract
http://arxiv.org/abs/cs.ai/1/
 wong and lau  1  o. wong and r. lau possibilistic reasoning for intelligent payment agents. proc. of the 1nd workshop on ai in electronic commerce  aiec' 1   1.

knowledge representation
and reasoning
action and causality

updates  actions  and planning
	andreas herzig	jer＞ ome lang 	pierre marquis	thomas polacsek
	irit/ups	irit/ups	cril/universite＞ d'artois	irit/ups
f-1 toulouse cedex	f-1 toulouse cedex	f-1 lens cedex	f-1 toulouse cedex
	france	france	france	france
	herzig irit.fr	lang irit.fr	marquis cril.univ-artois.fr	polacsek irit.fr

abstract
a general framework for update-based planning is presented. we first give a new family of dependence-based update operators that are wellsuited to the representation of simple actions and we identify the complexity of query entailment from an updated belief base. then we introduce conditional  nondeterministic and concurrent updates so as to encode the corresponding types of action effects. plan verification and existence are expressed in this update-based framework.
1	introduction
three subareas of ai seem significantly connected and still  in the literature  work in each of these areas has been so far disconnected  up to a few exceptions. these areas are  a  belief update   b  reasoning about action and change  and  c  planning in nondeterministic environments.
　belief update  a  mainly focuses on determining how a belief state should evolve after adding a new piece of information reflecting an explicit evolution of the world; how beliefs persist is studied in depth  this often - yet not always - relies on an assumption of minimal change   as well as causality and dependence between pieces of information; in particular  disjunctive information  reflecting uncertainty about the world after update  is taken into account  and many theoretical results exist  characterization of operators thanks to representation theorems  and computational complexity results .
　reasoning about action  b  focuses on the nature of pieces of information to take account for  preconditions for executability  direct changes  indirect changes or ramifications  static laws of the domain  and on the nature of the actions themselves  determinism  conditional effects  normal and/or exceptional effects  concurrent execution...  using sophisticated languages  generally more expressive than those of belief update but slightly less worked from a theoretical point of view  especially to what concerns representation theorems and complexity results - yet some results exist .
finally  while  a  and  b  care on determining the conse-
quences on the agent's current belief state of  respectively  a new piece of information and the execution of an action  most of the previous works about planning  c  are centered on plan generation; algorithmic developments have been pushed forward especially when actions are very simple - like in strips - but less so when they are more complex1.
　our purposeis to show howexisting works on belief update can be extended so as to represent complex actions representations and planning in nondeterministic environments.
　after some formal preliminaries  section 1   we introduce in section 1 a new family of update operators based on literal dependence. such operators generalize existing dependencebased operators but grasp in a better way the incorporation of the effects of an action in a belief base. interestingly  this generalization has no influence on computational properties since the complexity of query entailment from an updated belief base is just as hard as classical entailment in the general case  unlike other operators like winslett's pma - unless the polynomial hierarchy collapses . then we show in section 1 how the effects of more sophisticated  ontic  actions can be represented in our framework by generalizing our family of update operators to deal with conditionals  nondeterminism  distinct from disjunction!  and concurrency; this establishes a closer parallel between updates and effects of complex actions such as considered in action languages. in section 1  we show how to use our  extended  update operators so as to compute the effect of a  possibly conditional  plan in a nondeterministic domain. we successively consider the case where the environment is fully observable and unobservable. complexityresults for plan verificationand plan existence are given for both cases. finally  connections to related work are discussed in section 1  before the concluding section.
1	formal preliminaries
we consider a propositional language built up from a finite set of propositional variables and the boolean constants and . propositional formulas are denoted by etc. and interpretations over  or worlds  are denoted by etc. we represent them as tuples or conjunctions of literals over . in the following  we will identify every propositional formula with its set of models
.
	let	be the set of literals of the language  i.e. 
	.	we recall that a formula	is in nega-
tive normal form iff it makes use of the connectives	   	  only and the scope of negation connectives appearing in	includes propositional symbols only. for instance  is equivalent to the nnf formula	1.	denotes the formula obtained by uniformly replacing every occurrence of propositional variable	by the formula	.	denotes the formula obtained by replacing in a uniform way every positive  resp. negative  occurrence of literal	in the nnf of
by  resp.  . we denote by the set of literals appearing in the nnf of . in the previous example  . for any   we note
	  supposing	identified with	.
lastly  if	is a world and	a consistent set of literals from
	  then	is the world that gives the same truth
value as to all variables except the variables of literals of and	. for instance  if and then	.
　a belief update operator maps the propositional belief base  a formula  representing the initial beliefs of a given agent and an input formula reflecting some explicit evolution of the world  katsuno and mendelzon  1   to a new set of beliefs held by the agent after this evolution has taken place. katsuno and mendelzon  katsuno and mendelzon  1  proposed a general semantics for update. the most prominent feature of km updates  distinguishing updates from revision  is that update must be performed modelwise  i.e. 	.
　finally  we assume the reader familiar with some basic notions of computationalcomplexity see  papadimitriou 1  otherwise .
1	a new family of dependence-based updates
many proposals for update operators have been made. recently  several authors showed that there are good reasons for building a belief operator from a dependence relation  doherty et al.  1   herzig and rifi  1 . the dependencebased update of a belief base by an input formula consists in first forgetting in all information  relevant  to  leaving unchanged the truth value of variables not relevant to the update   and then expanding the result with . what remains to be defined is the notion of  being relevant to .
1	formula-variable dependence
a formula-variable dependence function is modelled a mapping from to . many choices for are possible  see  herzig and rifi  1  for details . whatever the choice  the dependence-based update of a world by a formula w.r.t.   denoted by   is the set of all worlds such that   and for every propositional variable from such that   and assign the same truth value to .
　interestingly  the complexity of query entailment from an updated belief base is  only  conp-complete when dependence-based updates are considered  liberatore  1  while entailment when most usual other operators - like winslett's pma - are used is at the second level of the polynomial hierarchy.
1	formula-literal dependence
forgetting everything about the variables involved in the update often leads to forgetting too much. consider a robot being told to go to a room with two doors 1 and 1  and to ensure that at least one of them is red  possibly by painting one of the doors   which can be represented as an update by red1 red1. suppose now that both doors are initially red: red1 red1. then  using any dependence function s.t. red1  red1 red1 red1   we get
　　　 red1 red1  = red1 red1. thus  we forgot that both doors were already red. clearly  this is not what is expected: intuitively  we should not forget that doors 1 and 1 are red  because updating by red1 red1 has no negative influence on red1 nor on red1. hence  only negative occurrences of red1 and red1 should be forgotten before expanding by the input formula  not positive ones.
　this may not be a problem in some contexts  but clearly  when reasoning about actions  these update operators make us forget too much and therefore do not handle the frame problem correctly. in order to cope with this limitation  we introduce a new family of dependence-based update operators. such operators generalize existing ones because they are based on formula-literal dependence  lang et al.  1   a more general notion than formula-variable dependence.
　 from now on  a formula-literal dependence function is a mapping	. many full-sense can be considered. like formula-variable dependence functions  can be basically defined as or
  where the latter is defined as follows:
definition 1  fl independence   lang et al.  1  let
be a formula from and or be a literal from . is literal-independent of iff
  and	is literal-independent of	iff	.
finally  depends on iff it is not literal-independent of . we denote by the set of literals depends on.
thus 	is literal-dependent on     and ; but not on   because it is equivalent to
	  and	does not appear in the nnf of the
latter.
　more sophisticated dependence functions can also be designed by considering explicit dependence  binary  relations on . one of them is  and similarly with  . we can also take into account persistent literals  i.e.  literals remaining true whatever happens  such as alive  contrarily to alive . for example 	where is a given set of persistent literals is a possible dependence function. clearly  persistent literals cannot be taken into account by formula-variable dependence. the other way round  every formula-variable dependence function can be simulated by a formula-literal dependence function.
1	update based on formula-literal dependence
definition 1 the update of the world by the formula w.r.t is the set of all worlds s.t. and there exists s.t. .
the next result gives us a way to compute
in practice. we first need to define
literal forgetting  lang et al.  1 .
definition 1  literal forgetting  for a formula	and
 is the formula inductively defined by 1 ; 1 
 1 
proposition 1;
.example 1 and letus take. we have; hence.
　interestingly  for all the update operators based on literal dependence listed above  we have the following complexity result  this slightly extends theorem 1 from  liberatore  1   :
proposition 1 given three formulas   and   is conp-complete.
	if	  then	and
are strongly separable  in the sense that 
if	  then
               holds. this is an important property from a computational point of view when cnf queries are considered since:
proposition 1 if and are two strongly separable formulas  then they are separable in the sense of levesque
 levesque  1   i.e.  for every clause	  we have iff	or	.
　accordingly  one global entailment test can be replaced by two local  simpler  entailment tests  and exponential savings can be achieved this way in practice. now  clausal entailment from a formulareduces to clausal entailment from a classical formula:
proposition 1 let	be a non-tautological clause.	then iff	.
　alltogether  the two previous propositions give us some tractable restrictions for clausal query entailment from an updated belief base. let us say that a formula is tractable when it belongs to a class of formulas for which clausal entailment is known as tractable  e.g.  the horn cnf class . when satisfies   is a tractable formula for which literal- in dependence can be tested in polynomial time and is a tractable formula  then determining whether can be achieved in polynomial time for every cnf formula . this is the case for instance when
               is a horn cnf formula and is a set of binary clauses1.
1	updates and actions
the aim of this section is to show that belief update is not far from being able to determine the effects of an arbitraryaction. in order to do so  we introduce the notion of extended input.
definition 1  extended inputs  extended inputs are defined inductively as follows:
	every formula	of	is an extended input;
if and are extended inputs then is an extended input  nondeterminism ;
if and are extended inputs then is an extended input  concurrency ;
if	and	are extended inputs and	is a formula of then if then else is an extended input  conditional .
definition 1  updates by extended inputs  let be an arbitrary update operator  a world and an extended input. we define the update of by inductively as follows: if   then is given by the definition of ;
;
	where	is defined inductively by:
otherwise
as before 	.
           is a nondeterministic update  namely  is nondeterministically updated by or by . this has been first introduced by brewka and hertzberg  brewka and hertzberg  1   who have observed that is in general different from . in the case where and are simple propositional formulas  updating by intuitively means that a nondeterministic action has been performed  whose possible effect is either or   and that  quotingfrom  brewka and hertzberg  1   the planner has no way to know whether one of the postcondition is already satisfied and nondeterministically choose to update by or by . a first example is the toss action  representable by the extended input head head. another example
 adapted from  brewka and hertzberg  1  : recall the red doors example  section 1 . if the robot is told to paint door 1 or door 1 in red but has no way to know whether one of the doors is already red  because it has no sensors   then the action should be modelled by red1 red1. on the other hand  the action consisting of an updateby red1 red1 corresponds to a context where the robot can check whether one of the doors is initially red  and leaves the world unchanged if this is the case . the introduction of these two kinds of
nondeterministic actions needs both standard disjunction and
. note that we have	.
is a conditional update: the ini-
tial model is updated by	or by	depending on whether satisfies the branching condition	or not. in the case where and	are simple propositional formulas  we have	.
conditionalupdates are neededfor action with conditionaleffects  such as the well-used turkeyshootingaction  which correspondsto an updateby if loaded then   alive loaded  else . conditional actions are not well handled with pure unconditional updates.
　　　　　　is a concurrent update. it basically works by gathering conjunctively all possible combinations of alternative results  and then performing the corresponding updates separately; in particular 
.
this can be easily generalized to the case where more than two extended inputs must be considered as concurrent update formulas. concurrent updates aim at representing compactly independent effects of actions. consider the action of loading the gun which has two independent effects  namely:  1  the gun is loaded and  1  the turkey goes hiding if it is not deaf. it corresponds to the concurrent update by  loaded if deaf then hidden else  . concurrency can as well be used for concurrent  or parallel  actions1.
　consider parallel actions having some inconsistent effects. as the effect of the update by their combination is an empty set of models  this has no impact on the final result. this means that our notion of action compatibility is very loose.
for instance  if and if we have two actions and corresponding to the extended inputs
and	then
. more precisely 
two nondeterministic actions	and
　　　　　　are compatible  i.e.  concurrently executable  iff there is an effect of and an effect of such that is consistent1.
　the previous definition is independent of the choice of a particular update operator. in the case of a dependence-based operator  the complexity results established in section 1 enable us to say that the prediction problem  i.e.  the problem of determining if the execution of an action in a given belief state leads to satisfying a given formula  is conp-complete1.
　note that usual ramification methods  such as the use of integrity constraints or causal rules  see  doherty et al.  1  for their handling in an update framework  can be added on top of our framework. we do not discuss them since it is not

1
   note that our notion of concurrency differs from concurrency in dynamic logic which is defined by . like in language
　 giunchiglia and lifschitz  1   concurrency means that the effects of the concurrent actions must be gathered before being used as inputs for belief update. consider for instance the two actions  open the door  and  open the window   corresponding to the updates
door and window  applied concurrently to the initial state door window . for any dependence-based update operator for which door and window are independent we get door door window   window door window   and thus door window   while with our definition we get the intended result door window door window door window . 1
   other notions of compatibility can be represented in our framework. they are not developed further due to space limitations. 1
   if the update operator was instead -complete  as many operators are  then extended update would be -complete too.
a contribution of our paper.
1	update-based planning
1	definitions
in the following  we consider standard ontic actions  which are meant to act on the world  and do not have any other effects on the knowledge state of the agent than the realization of its ontic effects. in our framework  a standard ontic action is described by an extended input  denoted by as well  thus  we identify the action and its effects  which will not lead to ambiguities   and determining its effects on a world simply consists in updating by .
　what is missing now in order to define a plan is the notion of update sequences.
definition 1  plans  given a finite set of standard ontic actions    branching  plans are defined inductively by:
 empty plan  is a plan;
for any extended input   is a plan; if and are plans  then is a plan;
if and are plans and is a propositional formula  then if then else is a plan.
　thus  plans are sequential and conditional extended updates  these extended updates being themselves conditional  nondeterministic and concurrent 1. at the plan level  nondeterministic choice and parallel composition are not allowed.
definition 1  planning problems  a planning problem is a triple	where:
is a propositional formula representing the knowledge
on the initial state;
is a set of standard ontic actions;
is a propositional formula representing the goal.
　note that these definitions do not mention anything regarding observability. they can therefore be used as generic definitions. what changes when various assumptions about observability are made is the type of plans allowed. when full observability is assumed  any branching is allowed  while with unobservability  branching is never allowed. with partial observability we may branch only on conditions whose truth value can be determined by the agent.
definition 1  trajectories  a trajectory	is a nonempty sequence	of models  	 .	if
　　　　　  then . if is a plan and an initial knowledge base  is a possible trajectory for       w.r.t. iff  1  and  1  one of the following conditions holds:
1. and	;
1. and	and	;
1. and can be decomposed in two subtrajectories     s.t. is a possible trajectory for      and is a possible trajectory for  ;
1. if then else and
	either	and	is a possible trajectory for
	or	and	is a possible trajectory for
.
definition 1  succeeding plans  a plan is a succeeding plan for a planning problem iff every possible trajectory for is s.t. .
1	fully observable environments
under full observability  any plan is allowed. let us consider the complexity of plan verification and existence.
proposition 1  complexity of plan verification/existence  suppose that the update operator used is one of the dependence-based operator given in the previous sections.
1. determining whether is a succeeding plan for is conp-complete;
1. determining whether there exists a succeeding plan for is exptime-complete  and determining whether there exists a succeeding plan of polynomially bounded length for is pspace-complete.
point 1. is a key result. it is a strong generalization of theorem 1 from  liberatore  1 . point 1. is easier to establish: the upper bounds are a byproduct of point 1.  and the lower bounds are derived from littman's results on probabilistic planning  littman  1 . these results  especially point 1.  show that update-based planning in fully observable environments is not harder than strips-based nondeterministic planning  which  somewhat  means that the facilities brought with the use of update are  for free .
1	unobservable environments
definition 1  unbranching plans  an unbranching plan is a  possibly empty  sequence of standard ontic actions  or equivalently a plan in which no if ... then ... else appears at the outer level1.
　trajectories and succeedingplans are definedthe same way as above. the following property  reformulating succeeding plans in terms of iterated updates  is almost straightforward:
proposition 1 let be an unbranching plan  and a planning problem. then is a succeeding plan for iff	.
proposition 1  complexity of plan verification/existence  suppose the update operator used is one of the dependencebased operator given in the previous sections.
1. determining whether an unbranching plan is a succeeding plan for is conp-complete.
1. determining whether there exists a succeeding unbranching plan for	is
expspace-complete  and determining whether there exists a succeeding unbranchingplan for of polynomially bounded length is -complete.
　the expspace-hardness result is a direct consequence of a result from  haslum and jonsson  1 . the latter point   -completeness  is similar to results in  baral et al.  1  and in  da costa et al.  1   which again suggests that dependence-based updates can be added  for free .
1	related work
 brewka and hertzberg  1  were the first to use belief update operators for representing actions with conditional and nondeterministic effects. in sections 1 and 1  we truly follow in their steps. they use winslett's pma update operator  which has been shown in many places to possess severe drawbacks  both from the points of view of expressivity and complexity . by making use of further advances in belief update  especially in what concerns dependence-based update  we were able to go much further than they did towards the practical computation of plans  and as well by allowing for concurrent actions  conditional plans and epistemic actions . see also the transition-based update-like operators of cordier and siegel  cordier and siegel  1 .
　 fargier et al.  1  use dependence-based update for one-stage decision making processes  that can be viewed as degenerated planning problems . their use of a formulavariable dependence is  as we show in section 1  not the best choice for reasoning about action  and they do not use nondeterministic updates  nor sequences of updates.
　 del val and shoham  1  point out some correspondances between the katsuno-mendelzon postulates and generic properties of action  by rewriting km postulates in the situation calculus and showing that each km postulate can be expressed as a specific property of actions   which enable them to give some intuitive reasons to accept or reject some of the km postulates. rather than practically proposing action languages based on belief update operators as we did  their work remains at the meta-level and made us think that such an update-based framework for planning was feasible.
　 shapiro et al.  1  represent iterated updates as well as epistemic actions in the situation calculus. they distinguish between ontic actions  leading to updates  and epistemic  leading to revisions . they represent the set of possible worlds  or situations  explicitly  while  using strips-like representation and a propositional formalism  we represent it in a compact way  which makes our framework very different from theirs.
　a line of work which is parallel to ours is the planning as satisfiability frameworks with actions languages  especially  giunchiglia  1  based on the language and  mccain and turner  1  based on the causal theory developed by the authors. the main differences between these languages and update is in the handling of disjunction: the literal completions used in  giunchiglia  1  and  mccain and turner  1  require that dynamic laws expressing the effects of actions have a single literal in their head. the exact links between these expressive languages and belief update is an interesting topic for further research.  geffner  1  also makes use of causal theories for actions and planning  and uses plausibility functions to distiguish various levels of exceptionality in the effects of actions.
1	conclusion
in this paper  we have presented a new update-based framework for planning. our contributionis manyfold. we have introduceda new family of dependence-basedupdate operators  based on formula-literal dependence  which includes existing dependence-based operators as specific cases  suits better the needs of reasoning about action  and whose computational complexityis not being abovethat of classical entailment. we have also shown how belief update has to be augmented with conditionals  nondeterminism and concurrency in order to be fully adapted to the representation of actions. third  we have shown how to define  conditional or unconditional  plans in our framework. finally  we have shown that the complexity of plan verification and existence in our framework is not higher than with a strips-like representation.
　this work calls for several perspectives. one of them concerns the introduction of partial observability in our framework. the simplest way to do it consists in considering that a fixed set of formulas is directly observable; allowed branching conditions are in this case formulas whose truth value can be determined from the truth values of these formulas. a more expressive solution consists in allowing for epistemic actions in the framework. unlike ontic actions  epistemic actions leave the world unchanged and therefore do not imply an update  but instead  a knowledge expansion. in order to distinguish between facts and knowledge  an epistemic modality can be introduced. plans can be defined in a similar way as before except for branching conditions  for which we allow only for epistemically interpretable branching conditions as in  herzig et al.  1 .
acknowledgements
the third authorhas beenpartly supportedby the iut de lens and the re＞gion nord/pas-de-calais.
references
 baral et al.  1  c. r. baral  v. kreinovich  and r. trejo. computational complexity of planning and approximate planning in presence of incompleteness. in proc. of ijcai'1  pages 1  1.
 brewka and hertzberg  1  g. brewka and j. hertzberg. how to do things with worlds: on formalizing actions and plans. j. of logic and computation  1 :1  1.
 cordier and siegel  1  m.o. cordier and p. siegel. prioritized transitions for updates. in proc. of ecsqaru'1  pages 1. lnai 1  springer-verlag  1.
 da costa et al.  1  c. da costa  f. garcia  j. lang  and r. martin-clouaire. possibilistic planning: representation and complexity. in proc. of ecp'1  pages 1 1.
 del val and shoham  1  a. del val and y. shoham. deriving properties of belief update from theories of action. j. of logic  language  and information  1-1  1.  doherty et al.  1  p. doherty  w.  ukasziewicz  and e. madalin＞ska-bugaj. the pma and relativizing change for action update. in proc. of kr'1  pages 1 1.
 fargier et al.  1  h. fargier  j. lang  and p. marquis. propositional logic and one-stage decision making. in proc. of kr'1  pages 1  1.
 ferraris and giunchiglia  1  p. ferraris and e. giunchiglia. planning as satisfiability in nondeterministic domains. in proc. of aaai'1  pages 1  1.
 geffner  1  h. geffner. a qualitative model for temporal reasoning with incomplete information. in proc. of aaai'1  pages 1  1.
 giunchiglia and lifschitz  1  e. giunchiglia and v. lifschitz. an action language based on causal explanation: preliminary report. in proc. of aaai'1  pp. 1  1.
 giunchiglia  1  e. giunchiglia. planning as satisfiability with expressive action languages: concurrency  constraints  and nondeterminism. in proc. of kr'1  pages 1  1.
 haslum and jonsson  1  p. haslum and p. jonsson. some results on the complexity of planning with incomplete information. in proc. of ecp'1  1  1.
 herzig and rifi  1  a. herzig and o. rifi. propositional belief update and minimal change. artificial intelligence  1-1  1.
 herzig et al.  1  a. herzig  j. lang  d. longin  and t. polacsek. a logic for planning under partial observability. in proc. of aaai'1  pages 1  1.
 katsuno and mendelzon  1  h. katsuno and a. mendelzon. propositional knowledge base revision and minimal change. artificial intelligence  1-1  1.
 lang et al.  1  j. lang  p. liberatore  and p. marquis. propositional independence  part i: formula-variable dependence and forgetting. technical report  irit  1.
 levesque  1  h. levesque. a completeness result for reasoning with incomplete first-order knowledge bases. in proc. of kr'1  pages 1  1.
 liberatore  1  p. liberatore. the complexity of belief update. artificial intelligence  1-1  1.
 littman  1  m. l. littman. probabilistic propositional planning: representations and complexity. in proc. of aaai'1  pages 1  1.
 mccain and turner  1  n. mccain and h. turner. satisfiability planning with causal theories. in proc. of kr'1  pages 1  1.
 papadimitriou  1  ch. papadimitriou. computational complexity. addison-wesley  1.
 rintanen  1  j. rintanen. constructing conditionalplans by a theorem prover. journal of artificial intelligence research  1-1  1.
 shapiro et al.  1  s. shapiro  m. pagnucco  y. lespe＞rance  and h. j. levesque. iterated belief change in the situation calculus. in proc. of kr'1  pages 1  1.
causality and minimal change demystified
maurice pagnucco
computational reasoning group
department of computing
division of ics
macquarie university
nsw  1  australiapavlos peppas
dept. of business administration
university of patras
patras 1  greece ppeppas otenet.grmorri ics.mq.edu.auabstract
the principle of minimal change is prevalent in various guises throughout the development of areas such as reasoning about action  belief change and nonmonotonic reasoning. recent literature has witnessed the proposal of several theories of action that adopt an explicit representation of causality. it is claimed that an explicit notion of causality is able to deal with the frame problem in a manner not possible with traditional approaches based on minimal change.
however  such claims remain untested by all but representative examples. it is our purpose here to objectively test these claims in an abstract sense; to determine whether an explicit representation of causality is capable of providing something that the principle of minimal change is unable to capture. working towards this end  we provide a precise characterisation of the limit of applicability of minimal change.
1	introduction
the problem of reasoning about action and change has been one of the major preoccupations for artificial intelligence researchers since the inception of the field. one of the early tenets applied when reasoning about such phenomena was that as little as possible should change in the world when performing an action; what we might call the principle of minimal change.1 this principle is manifest in many guises: preferential-style systems  shoham  1   persistence approaches  krautz  1   circumscription  mccarthy  1   etc. over the years  aspects of this principle have been called into question leading to a variety of suggested fixes: fixed versus variable predicates in circumscription  occluded fluents  sandewall  1   frame fluents  lifschitz  1   to name but a few. moreover  in the more recent literature explicit representations of causality have found favour  lin  1; mccain and turner  1; 1; thielscher  1 . however  what is not clear-beyond some simple representative examples-is the purchase afforded by explicitly representing causality over the more traditional minimal change approaches. it is this imbalance that this paper seeks to redress in a clear and objective manner. in fact  the results we present here have further reaching consequences  giving a rather lucid characterisation of the extent of applicability of minimal change. by this we mean that  given a framework for reasoning about action and change  it will be clear whether such a framework can be modelled by minimal change once certain properties of the framework can be established.
　we achieve our aims through a correspondence between two formal systems which we call dynamic systems and preferential models respectively. intuitively  the dynamic system is an abstract modelling of the dynamic domain under consideration  the behaviour of which we wish to reason about . essentially  this abstract model captures the domain at hand by a result function which returns the states  of the domain  that could possibly result from the application of an action with direct effects  i.e.  postconditions  at the initial state . a preferential model on the other hand is a formal structure that encodes the principal of minimal change in an abstract and quite general manner. with the aid of preferential models we are able to provide a precise characterisation of the class of dynamic systems that are amenable to theories of action based on minimal change; we call such dynamic systems minimisable. having a precise characterisation of minimisable dynamic systems we can then examine whether theories of action adopting an explicit representation of causality  which we shall call causal theories of action are capable of forms of reasoning that cannot be captured by the principle of minimal change; more precisely  we can examine whether causal theories of action are applicable outside the scope of minimisable dynamic systems. according to the results reported herein  the logic of action proposed by thielscher  is indeed applicable to non-minimisable dynamic systems  whereas  perhaps surprisingly mccain and turner's causal theory of action  mccain and turner  1  has a range of applicability that is subsumed by the class of minimisable dynamic systems.
　in the following section we introduce both dynamic systems and preferential models. moreover  we state clearly the notion of minimal change that we shall adopt here. in section 1 we examine the formal properties that the result function of a dynamic system must obey in order for it to be minimisable. section 1 presents an analysis of some of the theories of action found in the literature. we end with a discussion and conclusions  including pointers to future work.
1	dynamic systems and preferential systems
as mentioned above  the main results in this paper will be achieved by demonstrating a correspondence between two formal systems. the first  called a dynamic system  is meant to serve as a general abstraction of domains  such as the blocks world  or the domain described by the yale shooting problem  etc.   for which theories of action are designed to reason about. our main interest shall be in the properties of the dynamic system's result function. in particular  we shall formulate necessary and sufficient conditions under which the system's result function can be characterised in terms of an appropriately defined minimisation policy. minimisation policies are in turn encoded by our second formal system called a preferential model. dynamic systems and preferential models are formally defined below.
1	dynamic systems
throughout this article we shall be working with a finitary propositional language the details of which shall be left open.1 we shall often refer to as the object language. we shall call the propositional variables of fluents. the set of all fluents will be denoted by . a literal is either a fluent or the negation of a fluent. we shall denote the set of all literals by . a state of  also referred to as an object state  is a maximally consistent set of literals. the set of all states of is denoted by . for a set of sentences of   by
we denote the set of all states of	that satisfy	  i.e.	=
　　　　　　　　. finally  for a sentence of we shall use as an abbreviation for .
definition 1 a dynamic system is a triple = where  is a nonempty subset of the elements of which we shall call valid states.
　is a nonempty set of sentences of . the intended meaning of the sentences in is that they correspond to the postconditions  or direct effects  of actions. for simplicity  we shall identify actions with their postconditions and refer to the sentences in as actions.
  is called the result function.
intuitively  the result function returns the set of object states considered to be possible resultant states after applying the action with postcondition at the object state . if for a certain     it happens that =   this is taken to mean that is not applicable at .
1	extensions of the object language
despite the many different ways in which the principle of minimal change has been encoded  mccarthy  1; winslett  1; katsuno and mendelzon  1; doherty  1; sandewall  1   a feature that is common to all these approaches is the existence of an ordering on states used to determine which inferences are drawn about the effects of actions. in some of these approaches  winslett  1; katsuno and mendelzon  1; sandewall  1  the ordering is defined over the set of object states. for example  according to the possible models approach  pma   the ordering associated with an  initial  state is defined as
follows: for any	 	if and only if diff
　diff .1 there are however many theories of action for which the ordering is defined  not over the set of object states  but rather over an extended set of meta-states. consider  for example  a theory of action based on circumscription  mccarthy  1 . circumscription's minimisation policy induces an ordering that is defined over a set of meta-states
　　  generated from the set of object states with the addition of the abnormality predicate . more precisely  if the object language has fluents  and actions  there will be object states in ; with the addition of the abnormality predicate   each objects state  splits  into meta-states  all of which agree with on the truth value of the  object  fluents  and differ only on the value of the abnormality predicate for each pair of  object  fluent and action. thus there will be a total of meta-states over which the ordering is defined.
　as we prove later in this paper  moving the minimisation policy from object states to meta-states results in significant gains in the range of applicability of minimal change approaches. given the major role of meta-states in our study  in the rest of this section we introduce some further notation and formally define the concepts related to meta-states.
　a propositional language is called an extension of if and only if firstly  is finitary and secondly  the propositional variables of are included in . if is an extension of we shall refer to the additional propositional variables of
　 i.e.  those that do not appear in   as control variables or control fluents.1 we shall say that is a -extension of   for a natural number   if and only if is an extension of and it contains precisely control fluents. clearly  any -extension of is identical with .
　for an extension of   any maximally consistent set of literals of is called a meta-state. for a set of sentences of   we define the restriction of to   denoted   to be the set . finally  we define the restriction to of a collection of sets of sentences of   denoted   to be the set consisting of the restriction to of the elements of ; in symbols 	.
1	preferential models
having formally defined meta-states it remains to introduce a general model that encodes the concept of minimisation over meta-states.
definition 1 a preferential structure for	is a triple	= where:
	is an extension of	.
　is a nonempty collection of maximally consistent sets of literals of ; we shall call the elements of valid meta-states.
　is a function mapping each object state to a  partial  preorder over  the set of all maximally consistent sets of literals of  ; we shall denote the preorder assigned to   by .1
　as mentioned earlier  a preferential structure = is meant to be the basis for encoding formally  and in a quite abstract manner  the concept of minimal change. more precisely  let be any object state. the preorder associated with represents the comparative similarity of meta-states to . using  and the principle of minimal change   one can then determine the states that can possibly result from the application of an action to by means of the condition  m  given below:
 m 	=	.
in the above condition  denotes the set of meta-states consistent with the sentence and is the set of such meta-states that are minimal   most preferred   with respect to .
　the intuition behind condition  m  should be clear. essentially  we select those meta-states consistent with formula
　 representing the postcondition of an action  that are minimal under the ordering   filter out the valid ones and then restrict these to the language of the dynamic system under consideration.
　we shall say that a preferential structure = is a preferential model for the dynamic system = if its result function can be reproduced from by means of condition  m ; more precisely  if and only if for all and
       condition  m  is satisfied. if a dynamic system has a preferential model   we shall say that is minimisable; moreover  if there are precisely control fluents in   we shall say that is -minimisable or that it has a preferential model with degree . clearly  if a dynamic system has a preferential model with degree for some it also has a preferential model with degree for any .
1	minimisable dynamic systems
our aim in this section is to provide a characterisation of the class of minimisable dynamic systems  in terms of conditions imposed on the result function .
　let = be a dynamic system  a state in   and any two actions in . consider the conditions  p1  -  p1  below:
 p1 	if	and	  then
 p1 	if	and	  then
 p1 	if	 	 	and	  then these conditions can be interpreted quite simply. condition  p1  says that the postconditions of an action  i.e.    should be true at all possible resultant states. condition  p1  is an irrelevance of syntax condition stating that actions having logically equivalent postconditions should predict the same resultant states. condition  p1  states that if a state is chosen as a possible outcome of an action   then should also be chosen as a possible outcome of any action which is stronger than and consistent with .  nb: implies  . this last condition is similar to the choice theoretic condition know as     in the literature  sen  1 . these three  simply stated conditions suffice to exactly characterise the class of minimisable dynamic systems.
theorem 1 a dynamic system is minimisable if and only if it satisfies the conditions  p1  -  p1 .
　theorem 1 is the central result of this article. what is perhaps surprising about this theorem is that it manages to provide a characterisation of minimality defined over metastates via conditions on the result function  which operates on object states.
　the proof of theorem 1 is omitted due to space limitations. the most interesting part of the proof is a construction that  given a dynamic system with fluents satisfying  p1  -  p1   generates a preferential model for with degree . an immediate corollary of this is that  if a dynamic system is at all minimisable  then it is minimisable with degree . we shall call the smallest number for which a minimisable dynamic system has a preferential model with degree   the minimality rank of . as already mentioned  the corollary below follows directly from the proof of theorem 1.
corollary 1 the minimality rank of a minimisable dynamic system with	fluents is no greater than	.
　having provided a general characterisation of minimisable dynamic systems by means of  p1  -  p1   we shall now turn to special cases. more precisely  we shall impose certain constraints on preferential models and examine their implications on minimisable dynamic systems via condition  m .
　the first such constraint is totality on the preorders of a preferential model. more precisely  we shall say that a preferential model = is linear if and only if all preorders in are total  sometimes referred to as connected . we shall say that a dynamic system = is strictly minimisable if and only if has a linear preferential model. in the presence of  p1  -  p1   condition  p1  below characterises precisely the class of strictly minimisable dynamic systems. the term in  p1   where is a subset of   is used as an abbreviation for .
 p1  for any nonempty such that for all   there exists a nonempty subset of
　　　　  such that	=	  for all such that	and	.
condition  p1  essentially says that  under certain conditions  a collection of states contains a subset of  best  elements. consequently  whenever an action is such that all states are contained in   and moreover  among the -states there are some of the  best  elements of   then any state resulting from is among those  best  -states  i.e.  =  . notice that  p1  collapses to the  much more familiar  condition  p1   below  whenever is defined for all pairs of states   and sentences . in the principal case however where is defined over a proper subset of  
 p1   is strictly weaker than  p1 .
 p1  	if	 	 	  and
	  then	=	.
　this is essentially the  u1  postulate of katsuno and mendelzon . it is also found as property  1  in ga：rdenfors  1  p. 1 .
theorem 1 a dynamic system is strictly minimisable if and only if it satisfies the conditions  p1  -  p1 .
　a direct consequence of the  only-if part of the  above proof is the following corollary.
corollary 1 the minimality rank of a strictly minimisable dynamic system is no greater than 1.
　corollary 1 shows that by imposing totality on the preorders of the preferential model  we get very close to zerominimisable dynamic systems. very close indeed  but not quite there. in this paper we do not provide a complete characterisation of zero-minimisable dynamic systems as it is not central to our aims here; we do however provide some preliminary results in this direction. more precisely  consider the conditions  p1  -  p1  below  we implicitly assume that in each case :
 p1 if.  then p1 if  then. p1 
　condition  p1  says that if all non-valid -states together with those -states  chosen  by the result function are compatible with another action's postconditions      then any -states chosen when performing should also be chosen when performing . notice that this condition implies condition  p1  above.  p1  states that the result function must return at least one possible resultant state if all states satisfying the postconditions of the action are valid.  p1  says that if a state is chosen as a possible next state when either or is performed  then should also be chosen when the action  with postcondition  is performed.
theorem 1 every zero-minimisable dynamic system satisfies the conditions  p1  -  p1    p1  -  p1 .
　the converse of theorem 1 is not true in general. however  for a restricted class of dynamic systems  which we call dense  the conditions  p1  -  p1  and  p1  -  p1  suffice to characterise zero-minimisability. more precisely  we shall say that a dynamic system is dense if and only if every sentence of the object language corresponds to an action  i.e. 
 .
theorem 1 if a dense dynamic system satisfies  p1  -  p1    p1  -  p1   then it is zero-minimisable.
　we conclude this section with a brief comment on previous frameworks that encode the concept of minimal change. perhaps the framework most closely related to our own comes from the area of theory change and it is the one developed by katsuno and mendelzon  for modelling belief update. we shall leave a detailed comparison between the conditions presented herein and the postulates proposed by katsuno and mendelzon  known as the km postulates  for future work. here we simply note that the main difference between the two is that the km postulates are designed to apply only to dense  zero-minimisable  dynamic systems.
1	causality and minimal change
recall that one of the main motivations for this work was the desire to formally evaluate claims about the strength of causal theories of action over ones based on the notion of minimal change. in this section we use the foregoing results to analyse two of the most prominent causal theories of action  the first developed by mccain and turner   and the second by thielscher .
1	mccain and turner
mccain and turner  have developed a theory of action that represents causality explicitly. in their framework they introduce a causal connective where can be read as   causes   and referred to as a causal rule. here and are propositional sentences that do not contain  i.e. 
cannot be nested . they then introduce the notion of causal closure for a set of sentences with respect to a set of causal rules as the smallest set closed under classical deduction that includes and applies causal rules in the direction of the  arrow   i.e.  no contrapositive-the interested reader is referred to the citation above for the full details . the notation refers to the corresponding  causal  consequence relation: if and only if . the result function can be defined using the following fixed-point equation to be found in mccain and turner .
 mt 	iff	=
we can now establish the following results.
theorem 1 for any set of causal rules   the result function defined by means of  mt   satisfies the conditions  p1  -  p1 .
　from theorem 1 it follows that the theory of action developed by mccain an turner is applicable only to minimisable dynamic systems. this is a most curious result for it shows that the conclusions drawn with the aid of causality  as encoded by mccain and turner  can be reproduced by an appropriately defined minimisation policy. it is also especially interesting in light of recent results reported by peppas et al.  who show that for mccain and turner's approach it is in general not possible to construct an ordering over object states such that the minimal object states satisfy-

ing the postconditions of an action are those predicted by the mccain and turner fixed-point equation. this tells us that this approach is not zero-minimisable. we have  however  just shown that this approach is minimisable in a more general sense  i.e.  if one considers meta-states  so this system has a minimality rank greater than zero.
1	thielscher
we shall not describe thielscher's  approach in depth here. however  the underlying principle is to consider trajectories of state-effect pairs each of which is the result of applying a causal law at a previous state-effect pair  starting with the initial state and action postcondition . the resultant states are those at the end of a trajectory where causal laws no longer apply. we note that thielscher's system does not satisfy postulates  p1  -  p1  and hence is not minimisable. in particular  thielscher's result function  which we denote by violates condition  p1 ; the postconditions of the action do not necessarily have to hold after applying the action.
what would be of some interest however  is to generate from a new result function that chooses among the resultant states selected by the ones that satisfy the postconditions of the occurring action; in symbols  = . one can then examine whether the new function satisfies the conditions  p1  and  p1 . we leave this for future investigation.
1	discussion
let us take a step back and examine what we have accomplished thus far. the main result reported herein is a characterisation of the class of dynamic systems amenable to minimisation  in terms of conditions on the result function. existing causal theories of actions can then be assessed against these conditions to determine the added value  if any  of explicit representations of causality. this is clearly a significant step towards  demystifying  the  comparative  strengths and weaknesses of the notions of causality and minimal change in reasoning about action. admittedly though in this paper we have not given the whole story  especially as far as  demystifying  causality is concerned . what is missing is a generic model of the use of the concept of causality in reasoning about action  in the same way that preferential structures are such a model for the concept of minimality   based on which a general  formal comparison between causality and minimality can be made. until such a generic model is available  the best that can be done is evaluations of specific causal theories of action  such as the ones by mccain and turner  or thielscher   against the conditions  p1  -  p1 . there is of course still a lot of value in such assessments; showing for example that all existing causal approaches satisfy these conditions would be strong evidence in support of the claim that minimality subsumes causality  at least as far as the range of applicability is concerned . showing  on the other hand  that some causal approach violates one of the conditions  p1  -  p1   would prove that causality is essential in reasoning about action since it covers domains that are  unreachable  by minimal change approaches. similar  although weaker  conclusions can be drawn from the satisfaction or violation of  p1  and  p1  -  p1 . we have already witnessed that thielscher's  approach does not satisfy condition  p1 . thus some causal approaches do indeed go beyond what is possible with minimal change. more work needs to be done here to properly classify causal approaches both in regard to each other  with respect to the different causal notions they employ  and also with regard to the principle of minimal change.
　it should be noted  that when assessing a theory of action  apart from its range of applicability  a second criterion that is equally important is the conciseness of its representations  a solution to the frame problem ought to be both correct and concise . in this article  conciseness has been left out of the picture altogether. what we have mainly done herein was to axiomatically characterise certain classes of dynamic systems whose result function can be reproduced in terms of preorders on states. no consideration was given as to whether can be represented concisely or not. as far as our results are concerned  describing could be as  expensive  as listing the frame axioms corresponding to   which of course defeats the whole purpose of using minimality. a much more useful result  for practical purposes  would be one that characterises the class of what we might call concisely minimisable dynamic systems; that is  dynamic systems whose result function can be reproduced by preorders   which in turn can be represented concisely.1
　similar considerations apply to causality. characterising the class of dynamic systems for which causality  in one form or another  can duplicate the result function  although an interesting theoretical result  would not fully address the issue of the applicability of causality in reasoning about action. further work would be required to identify the domains that are amenable to concise causal descriptions.
　notice  also  that in cases where the range of applicability does not differentiate between causal and minimality-based approaches  conciseness considerations may well favour one over the other. more precisely  if the class of domains at focus is within the range of applicability of both causal and minimal change approaches  the determining factor in choosing between the two could be the  information cost  associated with the usage of each approach.
　notice that  while the concept of minimality has typically been used in the literature to deal with the frame and ramification problems  the nature of condition  m  is such that it requires minimality to deal with the qualification problem as well. indeed  via condition  m   the preorders of a preferential model are used  not only to determine the set of states that result from the application of an action at an initial state  frame and ramification problems   but also to determine whether is at all applicable at  qualification problem . the latter is decided based on whether is the empty set or not  cf. mccain and turner  who claim this is a derived qualification . one could argue that such an additional burden is perhaps too much from minimality to carry  or that it is even counter-intuitive   and maybe  if liberated from it  minimality could be used in many more situations. more formally  consider the condition  m   below:  m  	if	 	then	=
.
clearly  m   is weaker than  m . in fact  it is exactly the weakening of  m  that is needed to disengage minimality from the qualification problem. it would be a worthwhile exercise to reproduce the results of this article  having replaced  m  with  m  . the class of minimisable systems could be larger  and moreover  we conjecture that under  m    strictly minimisable systems are a proper subclass of zerominimisable dynamic systems.
　we conclude this section with a remark on minimality ranks. preliminary considerations suggest that there is a close relation between the minimality rank of a dynamic system on the one hand  and its ontological properties on the other. for example  domains where actions have no ramifications  tend to have lower minimality ranks than domains where ramifications do appear. if this connection can be generalised and formally proved  the minimality rank of a domain could serve as a precise measure of its complexity. more work however needs to be done in this direction.
1	conclusions
we have developed a formal framework within which we were able to formulate necessary and sufficient conditions under which a dynamic system is minimisable; that is  its result function can be reproduced by an appropriately defined minimisation policy. what is particularly pleasing about these conditions is that they are few in number and relatively easy and intuitive to understand. our original motivation in this study was to answer the question as to whether recently proposed theories of action involving explicit representations of causality are capable of forms of reasoning not possible via minimal change. we have seen that this is not the case with some approaches  mccain and turner   but in others  thielscher   the causal reasoning cannot be characterised by the principle of minimal change. perhaps of wider significance is the fact that the results reported here clearly indicate the range of applicability of the principle of minimal change; one simply needs to verify three properties  viz.  p1  -  p1  .
　this work opens up many interesting avenues for future work  various of which were mentioned in the discussion. one of the more pressing is to consider a variety of theories of action  particularly causal ones  and verify which properties they satisfy. this task is well under way and reserved for a much lengthier work. also of much interest would be to further categorise levels of minimisability and what distinguishes them together with the various theories of action at those levels of minimisability.
acknowledgments
the authors would like to thank three anonymous referees for their inciteful and generous comments. they would also like to thank norman foo  abhaya nayak  mikhail prokopenko  members of the cognitive robotics group at the university of toronto  and attendees of the seminar in applications of logic at the city university of new york for enlightening discussions.
references
 doherty  1  p. doherty. reasoning about action and change using occlusion. in proceedings of the eleventh european conference on artificial intelligence  pp. 1- 1  1.
 ga：rdenfors  1  p. ga：rdenfors. knowledge in flux. the mit press  cambridge  ma  1.
 katsuno and mendelzon  1  h. katsuno and a. o. mendelzon. on the difference between updating a knowledge base and revising it. in p. ga：rdenfors  ed.  belief revision  pp. 1  1.
 krautz  1  a. krautz. the logic of persistence. in proceedings of the fifth national conference on artificial intelligence  pages 1  1.
 lifschitz  1  v. lifschitz. frames in the space of situations  artificial intelligence  1-1  1.
 lin  1  f. lin. embracing causality in specifying the indeterminate effects of actions. in proceedings of the fourteenth international joint conference on artificial intelligence  pp. 1  1.
 mccain and turner  1  n. mccain and h. turner. a causal theory of ramifications and qualifications. in proceedings of the fourteenth international joint conference on artificial intelligence  pages 1. montreal  1.
 mccain and turner  1  n. mccain and h. turner. causal theories of action and change. in proceedings of the fourteenth national conference on artificial intelligence  pages 1  1.
 mccarthy  1  j. mccarthy. circumscription-a form of nonmonotonic reasoning. artificial intelligence  1- 1  1.
 peppas et al.  1  p. peppas  m. pagnucco  m. prokopenko  n. foo and a. nayak. preferential semantics for causal system. in proceedings of the sixteenth international joint conference on artificial intelligence  pp. 1  1.
 sandewall  1  e. sandewall. filter preferential entailment for the logic of action in almost continuous worlds. in proceedings of the eleventh international joint conference on artificial intelligence  1.
 sandewall  1  e. sandewall. features and fluents. the representation of knowledge about dynamical systems. vol. 1. oxford university press  1.
 sandewall  1  e. sandewall. assessments of ramification methods that use static domain constraints. in proceedings of the fifth international conference on knowledge representation and reasoning. 1.
 sen  1  a. sen  social choice theory: a re-examination  econometrica  1-1  1.
 shoham  1  y. shoham. reasoning about change. mit press  cambridge  massachusetts  1.
 thielscher  1  m. thielscher. ramification and causality. artificial intelligence  1-1  1.
 winslett  1  m. winslett. reasoning about actions using a possible models approach. in proceedings of the seventh national artificial intelligence conference  1.
epdl: a logic for causal reasoning
dongmo zhang and norman foo
knowledge systems group
school of computer science and engineering
the university of new south wales  australia dongmo  norman  cse.unsw.edu.auabstract
this paper presents an extended system of propositional dynamic logic by allowing a proposition as a modality for representing and specifying direct and indirect effects of actions in a unified logical structure. a set of causal logics based on the framework are proposed to model causal propagations through logical relevancy and iterated effects of causation. it is shown that these logics capture the basic properties of causal reasoning.
keywords: causality  reasoning about actions  common-sense reasoning
1	introduction
causality plays a basic role in human reasoning. the philosophical study of causality dates back to the origin of the discipline. ai research in causality finds its impetus in considerations from commonsense reasoning  especially the ramification problem. it has been found that propagations of effects cannot be appropriatelyspecified by domain constraints  a set of pure logical expressions    lin 1;mccain and tuner 1;thielscher 1  . not only the causation between actions and fluents but also the causation between fluents has to be explicitly representedand specified. it is obviousthat these two kinds of causation bear some essential similarities and so might be formalized with a similar logical structure. this is of importance not just in ontology. from the methodological point of view  we might take advantage of such a similarity to propose a provably correct theory of causality. we have both a good understanding of  and formalisms for the direct effects of actions. however  causal propagation is often confused with logical relevancy and persistence of fluents.
　in this paper  we introduce a formalism for representing and specifying both the direct and indirect effects of actions in a unified logical structure. our framework is based on dynamic logic. we do so not only because the dynamic logic is one of the typical formalisms for reasoning about action  c.f. the preface of  harel 1   but also because the modal expression of causal relation is one of the main approaches to
causation in philosophy   brand 1   and in ai   mccain and turner 1;giordano et al. 1  .
　in dynamic logic  causation between an action and its effects is expressed by the modal formula   where is a  primitive or compound  action and is a property. for instance  is read as  the action causes a turkey to be dead . in many cases  the effects of an action can be further propagated through causal relations between fluents. for instance  if a turkey is shot down  its death will cause the turkey to be unable to walk: causes
         . however  this cannot be formally expressed in dynamic logic. obviously it cannot simply written as either or	. the first ex-
pression is obviously unsuitable because of the directionality of causation. the second one is not allowed in the syntax of dynamic logic. therefore  our idea for expressing indirect effects of actions in dynamic logic is to extend the traditional dynamiclogic languageby allowing a propositionas a modality  say for proposition . if we allow a proposition as a modality1  we will have a way to unify the treatment of the causation between action and propositionand between propositions  likewise the treatment of direct and indirect effects of actions.
　in this paper  we first present such an extended system of the propositional dynamic logic   . the system serves as a formal platform for causal reasoning on which a set of causal logics are constructed to show how to build a causal logic satisfying our intuitions of causal reasoning with different domains. an inference mechanism on action descriptions is introducedto facilitate reasoning with action laws and causal laws. finally  an application of our approach to the ramification problem is briefly presented and the computational complexity of some causal logics is discussed.
1	extended propositional dynamic logic
in this section  we extend the by introducing propositional modalities  relative axioms and the intended semantics.
the extended system is called	.
1	language of
the alphabet of the language of the extended propositional dynamic logic consists of a countable set of fluent symbols and a countable set of primitive action symbols.
　propositions      formulas     and actions     are defined by the following bnf rules:
	where	and	.
　the intended meaning for is    always  causes if is executable . the dual operator defined as usual  reads as   is executable and possibly  or may  cause s   .
for instance  says that  spinning a gun may cause it to be unloaded  means   is executable .
　the formula where is a proposition  represents propositional causation  reading as   causes  . for example  - says that  a short-circuit causes the circuit to be damaged .
　notice the difference between and . is an action  called test action  which can be compounded with other actions but can not be compounded with actions.
1	semantics
the semantics for is the standard semantics for plus the interpretation of propositional causation.
	as usual  a model for	is a structure
	  where	.
the meanings of   and are the same as the ones for a model. for any proposition     similar to   is a binary relation on .
	the satisfaction relation	can be defined as usual.
for instance:
	iff for any	 	where
.
　as in any modal logic 	is valid in	  written as if	for all	means that	is valid in all models.
　the conditions of standard models for program connectives are as usual. for instance  the condition for test action is:  	iff	 .
　the conditions of standard models for propositional causation are as follows:
: if  then: if  thenthe conditionis one half of the test action seman-tics. it implies	. so	. note that	. thus	. this re-
flects the essential difference between the causal relation and the material implication. the difference between them is obvious. semantically  to verify   we only need to check the truth-values of and in the current world whereas to verify   not only the current world but also other related worlds have to be considered. in other words  a causal relation is action-relevantin the way that it is sensitive to both the history and the future evolution of the system under consideration.
　the condition shows that propositionalcausations we consider are syntax-independent. although it is not necessarily true in practical reasoning  it is certainly a way to make a causal theory simple.
1	deductive system
the axiomatic system for consists of the following axiom schemes and inference rules:
 1 . axiom schemes:
all tautologies of propositional calculus.
all axioms for compound programs 1.
	:	 extended
	:	 causal weakening 
 1 . inference rules:
	: from	and	  infer	 modus
ponens 
	: from	infer	 extended necessitation 
	: from	infer	.  cause
equivalence 
	where	and
.
　　　and are the extensions of the axiom and necessitation of   respectively. they convey the message that propositional causation will be treated as the standard dynamic modality. the specifies propagation of causations under logical implication. the inference rule says that tautologies can always be caused.
　the axiom and the inference rule are new  specifying propositional causation. corresponds to the semantic condition   which reflects the standard way to find and judge a causal relation. the rule   in conditional logic   nute 1    exactly corresponds to the semantic condition .
　note that if we add an extra axiom     into the deductive system  will collapse to  precisely  plus a duplicate of each test action  denoted by
　　　 . so the consistency of this axiomatic system is obvious.
　following the standard technique of using canonical models and filtration  we can prove that the deductive system for is sound and complete.
theorem 1	if and only if	.
also preserves the finite model property  so it is
decidable.
1	reasoning with action descriptions
in reasoning about a dynamic system  we generally specify the system by a set of domain-dependent axioms to describe preconditions  qualifications and effects of actions  causal relations and other domain constraints. such a set is called an action description or domain description  which can be any finite set of formulas.
example 1 consider the circuit of the following figure.
let	 	 	and	 
　　　　. then the circuit can be specified by the following action description:
	 	 light
	 	 
　the first two formulas describe the direct effects of action and . the middle two specify the causal relations among the fluents in the circuit.     means the action is always executable.
　we can easily see that a formula in an action description is different from an ordinaryformula. for instance  the sentence
    in an action description means that  whenever switch 1 is on  toggling it will cause it to be off . in situation calculus language  it can be expressed as	. a similar expression in dynamic logic is	  where is a special action  meaning  any action . with the help of a sentence in an action description can be easily differentiated from an ordinary formula. in  goranko and passy 1   it was shown that can be an modality. therefore  if we want to introduce formally as  prendinger and schurz 1  and  castilho et al. 1  did  we must extend our system further by adding all the axioms of and an extra axiom      stating that is a universal action.1 instead of doing this  however  we prefer a simpler way to deal with action description by treating it as a set of extra axioms  domain axioms .
definition 1 let	be an action description. a formula	is
 -provable  written as   if it belongs to the smallest set of formulas which contains all the theorems of all the elements of and is closed under and 1
example 1 consider the action description in example 1.
we can easily prove the following consequences:
1.
1.
the first expression shows that the causal laws  
　　　　  and     imply the domain constraint    . the second inference relation reflects the propagation of effects of action through the causal law	.
　note that the action description is not complete. all the frame axioms were left out. for such an incompleteaction description  a solution to the frame problem is necessary. otherwise  we can not even derive the following very intuitive consequence:
　we will not include a solution to the frame problem in this paper. a solution to the frame problem based on has been presented in a separated paper 1 there are also several other pdl-based solutions to the problem in the literature which can be easily embedded in	  prendinger and
schurz 1;castilho et al. 1  .
　a standard model is called a -model if for any . is -valid  written by if it is valid in every -model. then we have
theorem 1  soundness and completeness of	-provability  if and only if	.
1	logics for causal propagation
       provides a unified syntax and inference mechanism for reasoning about both direct effects and indirect effects of actions. it is a minor extension of the classical propositional dynamic logic.
　we did not endow with unlimited ability for causal reasoning. for instance  in example 1  it is obvious that one of the switches being off will cause the light to be off. however  we cannot prove in that or
　　　　　　　. the reason for our decision is that causality is a relation sensitive to context  time  domains and so on. it seems impossible to have a universal causal logic to satisfy all kinds of applications. instead  we introduce some optional inference laws for causality in order to form a flexible platform for causal reasoning.
let us consider the following plausible laws of causality:
 chaining 
 pseudo-causation 
　these laws are not hard to understand even though they are not necessarily widely acceptable. for instance  the first one says that if causes   then it still causes with any extra fact. the second one says that if individually and cause   having one will cause .
　chn says that if and are the causes of   then these causes can be chained into a nested causation. for instance  in the circuit of example 1  we could say that the closing of switch 1 causes that the closing of switch 1 causes the light to be on.
psc says that everything can cause a true statement.
　we can easily find examples in everyday reasoning to verify or refute any of these laws . for instance  on one hand by we can say that if causes   then
and cause as well; on the other hand  implies the following intuitive inference rule in the presence of
:
	if	  then
　this law  named as in conditional logic  reflects the regularity of causal propagation through logical relevancy.
	it can be proved in	that	implies the following
law
                            collapsing  we call it the collapsing of cause-effect chain. sometimes it is a good simplification for causal reasoning. however  it could be harmful if the cause-effect chain matters.
　we can also find some pros and cons for the other laws. in fact  we do not intend to persuade people to believe in these laws but try to introduce a hierarchy of causal logics to meet different requirements of causal reasoning. we refer to these logics as:
　note that all the laws are independent. so we can combine them in any fashion. we suggest that in practice such laws should be recombined  or new axioms be introduced to form particular systems of causal reasoning for particular applications. 1
　the following properties are the corresponding conditions on standard models for the associated laws:
	where	is the identity relation.
note that all conditions	and together imply:
1. 1.
　therefore the conditions for models force the accessibility relations of propositional causation to be a boolean lattice on the subsets of the identity relation. fortunately  the gap between causation and implication  or test action  still exists. so does not collapse to .
proposition 1. therefore.proof.	letbe a language ofin whichand. letbe a standard model of  where     letand	.then	is an
.model. we have  but	w	w
accessibility relation for	.	accessibility relation for	.
　this proposition differentiates our approach to causality from the counterfactual-based approaches where selfcausation is generally accepted.
　the following theorem gives the soundness and completeness of each logic  where     represents the syntactical  semantic  entailment in each logic.
theorem 1	if and only if	 	 .
　decidability of each logic can be also proved in a similar way with . -provabilityis readily extended into each logic.

1
	these logics form a chain between	and	:
.
　the ordering shows the degree of closeness between causation and implication in each logic.
1	reasoning with causal laws
two kinds of causal relations have been explicitly distinguished in the philosophy of causality: causal laws and causal instances. roughly speaking  a causal relation is a causal law if it exists everywhere whereas a causal instance only exists in some particular worlds. interestingly  most of our intuition about causal reasoning comes from reasoning with causal laws. for instance  transitivity is generally accepted as a property of causal reasoning. however  if you are not an reasoner  it does not hold for causal instances. more precisely  is not a tautology in or lower. the reason is: even if can cause to be true in any accessible worlds and can cause to be true in any accessible world  does not necessarily cause to be true in all its accessible worlds because does not necessarily cause in 's accessible worlds. however  if we express transitivity in the following form:
	 if	  then	  
it will be valid in any causal logic. this means that if both and are causal laws  will be a derived causal law.
　we call a formula a causal law if it is in an action description. a general question is: given a set of causal laws  what other causal laws can be derived   or formally  whether  
1	properties of causal reasoning
firstly  let's show some basic properties of causal reasoning with causal laws. the following theorem is a summary of
properties of	and its extensions.theorem 1let	be any action description. then1. ifand  then right weakening .1. ifand  then transitivity .1. ifand  then conjunction 1. if lence .and  then left logical equiva-1. ifand  then chain effects .1. ifand  then left strengthening .1. if  then disjunctive antecedents .1. ifand  then reasoning by cases .1. if tivity .and  then cumulative transi-1. if  then conjunctive antecedents .　note that the majority of acceptable properties of causal reasoning have been exhibited by . some properties  for instance  conjunctive antecedents  are not desirable for all situations. this implies that not all the causal logics are applicable to any domain. nevertheless  the list of properties gives us criteria for selecting the best suitable causal logic for reasoning about particular systems. we could also use them to compare the logics we presented in the paper and other causal theories1. for instance   mccain and tuner 1 's causal theory is different from all the causal logics we consider because it does not satisfy reasoning by cases  see  giordano et al. 1   but satisfies cumulative transitivity. those universal causal theories  geffner 1;turner 1;giordano et al. 1  basically using to express causation between and   satisfy almost all the properties except chain effects or pseudo-causation1. on the other hand  if we take mccain and turner's causal explanation   mccain and turner 1   as semantics for causal relation  causal reasoning in and its extensions is sound in the following sense:
　let    . it can be proved that if is causally explained according to   then it is causally explained according to for any . in other words  is a conservative expansion of with respect to the causal explanation semantics.
　some properties are not expected for causal reasoning. besides the self-causation  proposition 1   contraposition is generally not acceptable for causal reasoning  mackie
1  .	for instance 	does not imply
　　　. the following example shows that even in   contraposition is not valid.
example 1 let . based on the proof of proposition 1 we have .
　nonmonotonicity sometimes is an attractive property of reasoning. unfortunately  or fortunately   and its extensions are monotonic in both the following senses:
	a . if	  then	.
	b . if	  then	for	.
　we do not think that nonmonotonicity is essential to causal reasoning. we can use causal logic in the meta-level to build monotonic or nonmonotomic systems for satisfying various applications. nevertheless  it is still interesting to investigate nonmonotonic extensions of . for instance  the following axiom from conditional logic could be a promising
substitute for	1
.
1	application to the ramification problem
before we present some further properties of   let us consider an application of the logic.  foo and zhang 1  presented a unified solution to the ramification problem. the main idea is the following. suppose that we are given the initial state of a dynamic system  which is a maximal consistent set of fluent literals. a possible next state of the system after performed an action should consist of three parts: the direct effects   the unchanged
part	and the indirect effects	.	s1	s
　according to the causation-based idea to the ramification problem  the indirect effects should be caused by the direct effects and the unchanged part .1 this idea can be formalized in our terminology as   where consists of all the causal laws about the system1. therefore  a state is a possible next state of the initial state if and only if
	  where	and	.
　this provides a unified approach to the ramification problem. we can choose different causal logics  not just the ones we presented here  to satisfy different applications against criteria of  for instance  inferential strength  expressive power  computational complexity and so on. this motivated us to provide a set of causal logics rather than a single one.
　the results in the following section show that such a choice could be important at least for the solution to the ramification problem.
1	characteristic theorem and complexity of reasoning with causal laws
a common misunderstandingin ai seems to be that an action theory in dynamic logic is prohibitively expensive in computation because the complexity of theorem proving of propositional dynamic logic is exponential. however  dynamic logic is highly expressive. it can express any complicated program. therefore decidability may be the best result we can expect. the extended propositional dynamic logic for action and causation preserves decidability. with the same expressive power  however  the other formalisms of action may need second-order logic.
　in this subsection  we argue that if we focus on causal reasoning with causal laws  the computational complexity in some causal logics can be lowered. first  we give a characterization theorem for and some of its extensions. this theorem shows that causal reasoning in these logics can be transformed into the classical propositional logic.
theorem 1 let	be a set of causal laws and
. then
1. iff
1. iff
1. iff there exist	 	 	such that and	for each	 	 .
　the results in the theorem are a little bit complicated but still quite intuitive. can be read as the domain constraints implied by the causal laws . take as an example. theorem 1 shows that given a set of causal laws   a causal law is derivable if logically implies some causes whose effects logically imply under the domainconstraints. it is easy to see that the computational complexity of causal reasoning with causal laws in these three logics is in   so it is significantly lower than the complexity of
.
　the characterization theorems for and are still open problem. the one for could be fairly complicated. relatively  the one for might be simpler because we can transform a causal law into a normal form.
example 1 consider the circuit in example 1. the causal relation in this circuit can be described by the following causal laws:
　suppose that the initial state is     . since   the only possible next state is     after performed an action
if we use as the meta-logic for reasoning. note that is enoughfor this examplebecause we need not con-
sider the disjunctive or nested causal laws here.
　 foo and zhang 1  provided a more general approach to deal with more complicated domains  which can also be applied in this framework.
1	conclusion
we have presented an extended system of propositional dynamic logic for reasoning about causality. the extendedpropositionaldynamiclogic providesa unified formalism for reasoning about  direct and indirect  effect of actions. the extension is so slight that only two axioms and two inference rules were added. it has been shown that the resultant system captures the basic properties of causal reasoning. such simplicity may reflect the essential similarity between direct and indirect effect of actions.
         was then extended with a set of extra axioms on causal propagation. a series of causal logics were designed and the properties of these logics in causal reasoning with causal instances and causal laws are discussed  respectively. however  presenting these logics is not the main purpose of the work. we aimed to show in this paper how to construct causal logics for satisfying different applications in different domains. we believe that the representation of causation in dynamic logic affords the flexibility to accommodate different varieties of causal reasoning while retaining a core that captures their uncontroversial aspects.
　there is a lot of work left for the future. besides the nonmonotonic extensions  characterization theorems for and   and complexityanalysis of causal reasoning  a comprehensive comparison needs to be done with other formalisms of causality in ai and philosophy.
references
 brand 1  m. brand ed.  the nature of causation  university of illinois press  1.
 castilho et al. 1  m.a. castilho  o. gasquet  and a. herzig  formalizing action and change in modal logic i: the frame problem  j. of logic and computations  1 :1  1.
 de giacomo and lenzerini 1  g. de giacomo and m. lenzerini  pdl-based framework for reasoning about actions  in m. gori and g. soda  eds.   topics in artificial intelligence  lnai 1-1  1.
 foo and zhang 1  n. foo and d. zhang  dealing with the ramification problem in extended propersitional dynamic logic  to appear in f. wolter  h. wansing  m. de rijke  and m. zakharyaschev eds  advances in modal logic  volume 1  csli publications  1.
 geffner 1  h. geffner  default reasoning: causal and conditional theories  the mit press  1.
 gelfond and lifschitz 1 	m. gelfond and v. lifschitz  action language  electronic transactions on ai   1   1.
 giordano et al. 1  l. giordano  a. martelli and c. schwind  ramification and causality in a modal action logic  j. logic computat. 1   1  1.
 giunchiglia et al. 1  e. giunchiglia  g. n. kartha and v. lifschitz  representing action: indeterminacy and ramifications. artif. intell. 1   1 1.
 goldblatt 1  r. goldblatt  logics of time and computation  stanford univ. press  stanford ca  1.
 goranko and passy 1  v. goranko and s. passy  using the universal modality: gains and questions  j. of logic and computation  1 :1  1.
 harel 1  d. harel  first-order dynamic logic  lncs 1  springer-verlag  1.
 lin 1  f. lin  embracing causality in specifying the indirect effects of actions  ijcai-1  1  1.
 kraus et al. 1  s. kraus  d. lehmann and m. magidor  nonmonotonicreasoning  preferential models and cumulative logics  artif. intell.   1 1  1  1.
 mackie 1  j. l. mackie  the cement of the universe: a study of causation  oxford  1.
 mccain and turner 1  n. mccain and h. turner  a causal theory of ramifications and qualifications ijcai1  1  1.
 mccain and turner 1 	n. mccain and h. turner 
causal theories of action and change. aaai-1  1  1.
 nute 1  d. nute  conditional logic  in: d. gabbay and f. guenthneredts  handbookof philosophicallogic  vol. ii  kluwer academic publishers  1  1.
 prendinger and schurz 1  h. prendinger and g. schurz  reasoning about action and change: a dynamic logic approach  j. of logic  language  and information  1  1.
 ryan and schobbens 1  m. ryan and p. -y. schobbens  counterfactuals and updates as inverse modalities  j. of logic  language and information  1-1  1.
 schwind 1  c. schwind  causality in action theories  linkoping： electronic articles in computer and information science  vol. 1  1 :nr 1. http://www.ep.liu.se/ea/cis/1/.
 shoham 1  y. shoham reasoning about change: time and causation from the standpoint of artificial intelligence  the mit press  1.
 thielscher 1  m. thielscher  ramification and causality  artif. intell.  1 1 :1 1.
 turner 1  h. turner  a logic of universal causation  artif. intell.  1 1-1  1.

knowledge representation
and reasoning
action

a circumscriptive formalization of the qualification problem
g. neelakantan kartha 1 blackfield drive
coppell  tx 1 gnkartha yahoo.com

abstract
the qualification problem refers to the difficulty that arises in formalizing actions  because it is difficult or impossible to specify in advance all the preconditions that should hold before an action can be executed. we study the qualification problem in the setting of the situation calculus and give a simple formalization using nested abnormality theories  a formalism based on circumscription. the formalization that we present allows us to combine a solution to the frame problem with a solution to the qualification problem.
1	introduction
two problems that are important in logical formalizations of commonsense reasoning are the frame problem and the qualification problem  mccarthy  1; 1 . while the frame problem has received a lot of attention in the literature in recent years  lin and shoham  1; lifschitz  1; kartha and lifschitz  1; sandewall  1; mccain and turner  1; thielscher  1; giunchiglia et al.  1; shanahan  1; mcilraith  1   the qualification problem has not been studied as carefully see however the discussion in the section on related work . in this paper  we clearly define what we mean by the qualification problem and introduce a formalization of the problem using nested abnormality theories  lifschitz  1 . this formalization has the advantage that solutions to the frame problem can be incorporated smoothly with the solution to the qualification problem. we also point out problematic aspects of the current approaches to the qualification problem.
　the qualification problem  mccarthy  1  refers to the difficulty that one has in formalizing an action  because it is difficult or impossible to enumerate all the preconditions that need to be satisfied before the action can be performed. one of the more well known examples that illustrate this problem is the potato in the tailpipe example due to mccarthy. the example is as follows: the action of starting a car results in the engine running. however  there are many implicit preconditions to the action of starting the car  for instance that there is no potato in the tailpipe  the battery is not dead etc. the problem is how to formalize such examples without getting bogged down in the representation of all such preconditions.
　let us examine the example in a little more detail. the action of starting the car  without any qualifications  can be formalized in the situation calculus  mccarthy and hayes  1  as follows:
enginerunning
	enginerunning	startcar	 1 
　to take into account various preconditions of the action startcar  we could modify  1  by adding preconditions to its antecedent as in 
	enginerunning	potatointailpipe
	enginerunning	startcar	 1 
　however  note that we will be able to include in the language of our axiomatization only a subset of all the preconditions that can affect the outcome of an action  because we cannot foresee all of them. for instance  a factor that could affect the outcome of the action startcar is the presence  or absence  of a mysterious ray. we would include facts about this ray in the language of our axiomatization only when we are given that this is a relevant fact.
　let us then start by fixing the language. even then  it would be difficult to foresee all the circumstances  in the language  that can affect an action. thus the qualification problem does not vanish when the language is fixed  but takes on a slightly different flavor. now the key question becomes how one can axiomatize the domain of interest in such a way that on learning a new precondition for an action  one is able to incorporate this additional knowledge in a modular fashion in the axiomatization. note that adding preconditions to the antecedent  as we have done in axiom  1  is not very modular.
　thus  it seems that there are really two related  but distinct aspects to the qualification problem.
1. the problem of how to smoothly incorporate various additions to the language of an axiomatization. the addition into the axiomatization of the facts pertaining to the mysterious ray and its effect on starting the car is an instance of this. note that here the language of the axiomatization changes as a result of incorporatingthe change. this aspect of the qualification problem is related to
mccarthy's notion of elaboration tolerance1  mccarthy 
1 . this aspect is also related to the problem of how to move between axiomatizations  in different languages  that axiomatize a domain in different levels of detail.
1. the problem of how to represent the qualifications to anaction  when the language of the axiomatization is fixed. the chief issues here are
 a  whether we can easily incorporate the fact that another circumstance  in the language  qualifies an action without drastically modifying the original axiomatization. for instance  we would like to achieve this by adding a new sentence into the language. again  this is because of our desire for an elaboration tolerant solution.
 b  to see whether the conclusions that are sanctionedformally agree with our intuitions.
　note that the two aspects share a desire for elaboration tolerance  so that a solution for one aspect is likely to contribute to a solution for the other. the current paper deals only with the second of the two aspects of the qualification problem.
　the rest of the paper is organized as follows. in the next section  we explain our formalization and give an example of the kind of results that can be proved using our formalization. we then survey related work  summarize the key contributions of this work and point out directions for future work.
1	formalization
since our aim is to convey the key ideas  we will explain our formalization via an example  but will indicate at the appropriate places how one would generalize. the example we use is a slightly extended version of the potato in the tailpipe example from  thielscher  1 . the example consists of extending the basic potato in the tailpipe scenario with another action  putpotato. the result of this action is to put the potato in the tailpipe  unless this action is qualified. this action is qualified if the potato is so heavy that it cannot be lifted.
　following mccarthy  mccarthy  1   here are some axioms that capture the scenario:
enginerunningstartcarenginerunningstartcar 1 potatointailpipeputpotatopotatointailpipeputpotato 1 potatointailpipestartcar 1 heavypotatoputpotato 1 enginerunning 1 
the extent it is convenient to modify a set of facts expressed in the formalism to take into account new phenomena or changed circumstances .
　here is an abnormality predicate that will be minimized. the advantage of this representation is that as we think of new qualifications  they can be easily incorporated into the theory by means of axioms such as  1  and  1 .
　however  it should be noted that we are rarely interested in solving the qualification problem in isolation. we also need to ensure that the truth values of the fluents do not change without a reason  so that we are able to obtain the intuitively expected results. to see this  let be the theory consisting of axioms  1    1    1    1  and  1 . let be the theory obtained by minimizing in . we would like to entail the formula putpotato and consequently heavypotato . although  intuitively we would then conclude that the fluent heavypotato remains false after executing an action in  since there are no actions that change the truth value of that fluent   is not strong enough to entail formulae such as heavypotato startcar  
       heavypotato	putpotato	etc.	this shows that we need to combine a solution to the frame problem with a solution to the qualification problem to enable us to draw the intuitively expected conclusions.
　the formalism that we use that helps us in tackling the frame problem and the qualification problem in sequence  without the solutions interferingwith each otheris the formalism of nested abnormality theories  nats   lifschitz  1 . the main feature of nats is that the effect of a circumscription can be restricted to a part of the theory called a block.
formally  a block is of the form   where are function and/or predicate constants and are axioms  possibly containing an abnormality predicate . the block corresponds to the circumscription of in the theory consisting of   with
allowed to vary. note that each of
itself may be a block.
　formally  the semantics of nats is defined by a map that translates blocks into sentences of the underlying language. we define as follows:
where
circ
here circ stands for the circumscription of	in
	with	allowed to vary  see  lif-
schitz  1a  for the formal definitions . for a sentence that does not include   is is equivalent to .
　since we need to consider the frame problem  we will need to axiomatize the commonsense law of inertia  according to which the truth values of fluents persist unless they are abnormal. using ideas from  kartha and lifschitz  1   we formalize this as
 1 
and denote it by li. here is a new predicate  explicitly defined outside the block solving the frame problem as
the role of this predicate for solving the frame problem can be explained roughly as follows-by  factoring away  the result function  we are able to focus attention only on the situation and the situation that we obtain after action is performed in . for more details and intuitions  the reader is referred to  kartha and lifschitz  1 .
　in our formalization  we will formalize the effect of performing an action using the predicate. thus  for instance  the axioms  1  and  1  will become
	enginerunning	startcar
	enginerunning startcar	 1 
	potatointailpipe	putpotato
	potatointailpipe putpotato	 1 
　let denote the conjunction of axioms that formalize the effect of performing an action using the predicate . then each formula of is of the syntactic form
or
where denotes a finite conjunction of formulas of the form or . in our example  thus stands for the conjunction of  1  and  1 .
　similarly  let stand for the conjunction of axioms that formalize the qualifications. syntactically  these axioms are of the form
where is a finite disjunction of conjunction of formulas of the form and . in our example  is the conjunction of  1  and  1 .
　let stand for the conjunction of axioms that formalize the observations.these axioms are a finite conjuncion of formulas of the form and . in our case  is  1 .
　we also introduce the uniqueness of names and domain closure axioms for fluents and actions. for instance  in our example  the uniqueness of names axiom for fluents consists of the following axiom.
	enginerunning	heavypotato
enginerunning potatointailpipe heavypotato potatointailpipe
the domain closure axiom for fluents is
enginerunning
potatointailpipe heavypotato
let una and dca denote respectively the uniqueness of names axioms and the domain closure axioms for fluents and actions.
　we need two technical devices for our axiomatization. firstly  we need to include an existence of situation axiom   baker  1    denoted es  that states that corresponding to every set of fluents  there is a situation in which exactly these fluents hold. for our example  the existence of situation axiom consists of the conjunction of
enginerunning
potatointailpipe
heavypotato
and seven other similar formulae asserting the existence of a situation corresponding to other possible truth values of the three fluents.
　secondly  we need to include a formula which states that there is a situation distinct from where exactly those fluents that hold in hold. formally  this formula is
　this formula is denoted esi  for existence of situation for the initial situation .
we are now ready to give our formalization.
es
esi
li
　the inner block solves the frame problem. note that the abnormality predicate that is being circumscribed in this block is the ternary one that occurs in li  not the binary one occurring in . also  and do not contain any occurrence of and   and hence do not play any role in the circumscription of the inner block 1. the outer block minimizes the binary and solves the qualification problem. the existence of situations axioms ensure that the binary abnormalities are minimized over all situations. the strategy of first solving the frame problem and then the qualification problem can also be found in  ginsberg and smith  1; lin and reiter  1 .
　let be the nat that encodesour example. by entailment relation for   we will mean the circumscriptive entailment relation for nats  for details  see  lifschitz  1  . as a representative of the kind of results we can prove  we state the following theorem.
theorem1 the following set of formulae are entailed by	.
1. heavypotato
	potatointailpipe	.
1. heavypotato
heavypotato
1. enginerunning	putpotato	.
1. enginerunning	startcar	where
	putpotato	.
　the first formula states that the fluents heavypotato and potatointailpipe do not hold in the initial situation. this is intuitively expected  since we are given no information as to whether these fluents hold in the initial situation and we would like to assume that these do not hold if we can do so in order to minimize the abnormalities.
　the second formula shows that the value of the fluent heavypotato does not change as a result of performing an action. again  this is expected from a solution to the frame problem  as there are no actions that can change the truth value of the fluent heavypotato. the third formula follows from the solution of the frame problem for the fluent enginerunning and  1 .
　the fourth formula shows that if you put potato in the tailpipe and then start the car  the engine does not start. this is because as a result of performing the first action putpotato  the second action startcar becomes qualified.
　without the es and esi axioms  we would not be able to obtain the desired conclusions from our formalization. to see this  let us consider a formalization    of our example that differs from only in that does not include es and esi. define a structure with the universe of situations  defined as   the universe of fluents  is defined as
 potatointailpipe enginerunning heavypotato   the universe of actions is defined as startcar putpotato   the interpretation of the predicate defined as
　potatointailpipe	and the interpretation of	  defined as	  where	ranges over the actions. it is easy to check that	is a model of	.
hence  we see that	potatointailpipe	.
	we noted that	entails the formula
       heavypotato	.	clearly 	this	formula will	not	be	entailed	if	includes	the	formula
　　　heavypotato . this shows that adding an observation does not reduce the set of models of   but changes them. hence the solution that we present here does not have the restricted monotonicity property  lifschitz  1b  with respect to observation sentences. in general  we do not expect this property to be obeyed by solutions to the qualification problem.
1	related work
mccarthy  mccarthy  1  has suggested using circumscription to solve the qualification problem. the use of in our formalization is based on his abnormality predicate  but several ideas in our formalization such as the use of the predicate and the use of the existence of situations axiom for solving the qualification problem are new. also  we consider how to combine a solution to the frame problem with a solution to the qualification problem.
　in his brief paper on the qualification problem  elkan  elkan  1  suggests using a context mechanism for formalizing the qualification problem  but does not present any such formalization. he criticizes a particular formalization of
the qualification problembased on default logic on the ground that all the potential qualifications to an action need to made explicit in that formalization. while the criticism is on the mark for the particular formalization  it seems that a formalization in default logic that uses an abnormality predicate will not suffer from this criticism.
　lifschitz  lifschitz  1  formalizes the qualification problem by circumscribing a predicate precond. his formalism is less expressivethan ours  since it does not permit one to express that an action is qualified if two fluents are simultaneously true. also  his formalism is embedded within a causal language known to have difficulties with domain constraints1 baker  1 .
　thielscher  thielscher  1  claims that global minimization of abnormalities fails to solve the qualification problem. the examplethat he uses is the one that we have formalizedin the previous section as a nat. the question that thielscher raises is as to what would happen if we perform the action putpotato followed by the action startcar. he claims that if we globally minimize abnormalities  there would be a model where the action putpotato does not have its intended effect  because that action is qualified  and hence in that model the engine would start after performing the action startcar.
　it should be noted that thielscher gives only an informal argument for the claim  not a formal proof. his claim seems to be based on the fact that he is considering a propositional language and does not consider abnormalities to have situational arguments. as we have shown  the unintended model does not arise in our formalization.
thielscher represents qualifications in the form disqualified after
which indicates that after performing the sequence of actions
           action becomes disqualified. to specify such axioms  it is necessary to precompute what the effect of different sequences of actions should be  which is cumbersome. in contrast  our formalization allows us to specify qualifications directly in terms of fluents. however  thielscher considers the issue of  miraculous qualifications   an issue that we do not consider.
　mcilraith  mcilraith  1  addresses the frame  ramification and qualification problem within the context of the situation calculus. in contrast to the approach presented in this paper  mcilraith compiles a set of effect axioms and ramification constraints into a set of successor state axioms. it should be noted that such a compilation is possible only when the effect axioms and ramification constraints follow a syntactic restriction.  theories with such a syntactic restriction are called solitary stratified theories in  mcilraith  1  . whether the approach presented in this paper coincides with that in  mcilraith  1  when restricted to the case of solitary stratified theories remains open.
1	conclusions and future work
we have presented a simple formalization of the qualification problem based on nats and and have shown how to combine a solution to the frame problem with this formalization. we have also argued that this formalization offers several advantages in comparison with existing approaches.
　theorem 1 pertains only to the example presented in this paper. this raises the question as to how we can evaluate the general applicability of the formalization presented here. one way is by proving theorems about what the formalization does and does not entail. for instance  we can prove that the inner block  that solves the frame problem generates conclusions identical to the ones sanctioned by a -like  gelfond and lifschitz  1 language. evaluatingthat the outer block handles qualifications correctly is harder. one approach is to test that the formalization yields intutively expected results on a test suite. a more principled approach is to define a language formalizing intuitions about qualifications and then prove soundness and completeness theorems along the lines of  kartha  1 . both these approaches are currently being pursued.
　one direction for future work is to investigate how the ideas presented in this paper can be generalized to handle domain constraints. in the presence of domain constraints  the situation is more complicated for the following reasons.
the existence of situations axiom needs to be changed  since now  all possible combination of fluents are not consistent.
as pointed out by  ginsberg and smith  1  and  lin and reiter  1   some domain constraints can act as constraints on the performability of an action  these are called qualificationconstraints whereas some other syntactically indistinguishable domain constraints give rise to indirect effects  these are called ramification constraints .
　the first of the above complications can be handled  for instance  by including a block that formalizes a nonmonotonic version of the existence of situations axiom  as in  kartha and lifschitz  1 .
　the second difficulty is more serious. if all the constraints are ramification constraints  then solutions to the frame problem shown here can be easily extended  along the lines of  giunchiglia et al.  1  . if we have both ramification and qualification constraints  we need to separate the set of constraints into these two sets and consider them separately.
acknowledgment
i wish to thank the anonymous referees for several suggestions for improving the clarity of this paper.
references
 baker  1  andrew baker. nonmonotonic reasoning in the framework of situation calculus. artificial intelligence  1-1  1.
 elkan  1  charles elkan. on solving the qualification problem. in working notes of the symposiumon extending theories of actions  1.
 gelfond and lifschitz  1  michael gelfond and vladimir lifschitz. representing action and change by logic programs. journal of logic programming  1-1  1.
 ginsberg and smith  1  matthew l. ginsberg and david e. smith. reasoning about action i: a possible worlds approach. artificial intelligence  1 :1  1.
 giunchiglia et al.  1  enrico giunchiglia  g. neelakantan kartha  and vladimir lifschitz. representing action: indeterminacy and ramifications. artificial intelligence  1-1  1.
 kartha and lifschitz  1  g. neelakantan kartha and vladimir lifschitz. actions with indirect effects. in jon doyle  erik sandewall  and piero torasso  editors  proc. of the fourth int'l conf. on principles of knowledge representation and reasoning  pages 1  1.
 kartha and lifschitz  1  g. neelakantan kartha and vladimir lifschitz. a simple formalization of actions using circumscription. in proc. of ijcai-1  pages 1- 1  1.
 kartha  1  g. neelakantan kartha. soundness and completeness theorems for three formalizations of action. in proc. of ijcai-1  pages 1  1.
 lifschitz  1  vladimir lifschitz. formal theories of action  preliminaryreport . in proc. of ijcai-1  pages 1- 1  1.
 lifschitz  1  vladimir lifschitz. towards a metatheory of action. in james allen  richard fikes  and erik sandewall  editors  proc. of the second int'l conf. on principles of knowledge representation and reasoning  pages 1- 1  1.
 lifschitz  1a  vladimir lifschitz. circumscription. in d.m. gabbay  c.j. hogger  and j.a. robinson  editors  the handbook of logic in ai and logic programming  volume 1  pages 1. oxford university press  1.
 lifschitz  1b  vladimir lifschitz. restricted monotonicity. in proc. of the eleventh national conference of artificial intelligence  pages 1  1.
 lifschitz  1  vladimir lifschitz. nested abnormality theories. artificial intelligence  1-1  1.
 lin and reiter  1  fangzhen lin and raymond reiter. state constraints revisited. journal of logic and computation  special issue on actions and processes  1 :1- 1  1.
 lin and shoham  1  fangzhen lin and yoav shoham. provably correct theories of action. in proc. of the ninth national conference of artificial intelligence  pages 1- 1  1.
 mccain and turner  1  norman mccain and hudson turner. a causal theory of ramifications and qualifications. in proc. of ijcai-1  pages 1  1.
 mccarthy and hayes  1  john mccarthy and patrick hayes. some philosophical problems from the standpoint of artificial intelligence. in b. meltzer and d. michie  editors  machine intelligence  volume 1  pages 1. edinburgh university press  edinburgh  1.
 mccarthy  1  john mccarthy. epistemological problems of artificial intelligence. in proc. of ijcai-1  pages 1  1.
 mccarthy  1  john mccarthy. circumscription-a form of non-monotonic reasoning. artificial intelligence  1- 1 :1  1.
 mccarthy  1  john mccarthy. applications of circumscription to formalizing common sense knowledge. artificial intelligence  1 :1  1.
 mccarthy  1  john mccarthy. elaboration tolerance. in fourth symposium on logical formalizations of commonsense reasoning  1.
 mcilraith  1  sheila mcilraith. integrating actions and state constraints: a closed form solution to the ramification problem  sometimes . artificial intelligence  1- 1  1.
 sandewall  1  erik sandewall. features and fluents. the representation of knowledge about dynamical systems. oxford university press  1.
 shanahan  1  murray shanahan. solving the frame problem. mit press  cambridge  ma  1.
 thielscher  1  michael thielscher. causality and the qualification problem. in l. c. aiello  j. doyle  and s. c. shapiro  editors  proceedings of the international conference on principles of knowledge representation and reasoning  kr   cambridge  ma  november 1. morgan kaufmann.
 thielscher  1  michael thielscher. ramification and causality. artificial intelligence  1-1 :1  1.

computing strongest necessary and weakest sufficient conditions of first-order formulas

patrick doherty
dept. of computer science linko：ping university
s-1 linko：ping  sweden email: patdo ida.liu.se witold  ukaszewicz
             andrzej sza as dept. of computer science  linko：ping university and college of economics and computer science
twp  olsztyn  poland

abstract
a technique is proposed for computing the weakest sufficient  wsc  and strongest necessary  snc  conditions for formulas in an expressive fragment of first-order logic using quantifier elimination techniques. the efficacy of the approach is demonstrated by using the techniques to compute snc's and wsc's for use in agent communication applications  theory approximation and generation of abductive hypotheses. additionally  we generalize recent results involving the generation of successor state axioms in the propositional situation calculus via snc's to the first-order case. subsumption results for existing approaches to this problem and a re-interpretation of the concept of forgetting as a process of quantifier elimination are also provided.
1	introduction
in  lin  1  lin proposes the notion of weakest sufficient and strongest necessary conditions for a proposition under a propositional theory   where the techniques for generating the resulting formulas may be parameterized to contain only a restricted subset of the propositional variables in . in addition  he investigates a number of methods for automatically generating snc's and wsc's  several based on the generation of prime implicates and a preferred method for computing snc's and wsc's based on the notion of forgetting  lin and reiter  1 . snc's and wsc's have many potential uses and applications ranging from generation of abductive hypotheses to approximation of theories. in fact  special cases of snc's and wsc's  strongest postconditions and weakest preconditions  have had widespread usage as a basis for programming language semantics  dijkstra  1 .
　weakest sufficient and strongest necessary conditions for propositional formulas are related to prime implicants and implicates  respectively. classically  a prime implicant of a

　　supported in part by the witas project grant under the wallenberg foundation  sweden.
　　supported in part by the witas project grant under the wallenberg foundation  sweden and kbn grant 1 t1c 1.
1
　　this paper received the best paper award at the kr'1 conference  breckenridge  colorado  1.
email: witlu andsz ida.liu.se
formula is a minimal satisfiable term logically implying and a prime implicate is a minimal satisfiable clause which is logically implied by . the relation between prime implicants/implicates and wsc's/snc's is used by lin in his investigation of methods for automatically generating wsc's and snc's. generating prime implicants and implicates for propositional formulas is intractable and in the general case  the same applies for wsc's and snc's.
　lin's results apply to the propositional case and his algorithms for automatically generating snc's and wsc's are empirically tested. for the propositional case  we propose a different method for computing snc's and wsc's that is based on second-order quantifier elimination techniques and has the following advantages:
the general method applies to the full propositional language and snc's and wsc's are generated directly for arbitrary formulas rather than propositional atoms.
when applying second-order quantifier elimination  one substantially simplifies the propositional case considered by lin and often gets a more efficient computation method.
a non-trivial fragment of the propositional language is isolated where snc's and wsc's for formulas in this fragment can be generated efficiently and are guaranteed to be so.
the quantifier elimination algorithms which provide the basis for automatically generating snc's and wsc's for arbitrary propositional formulas are implemented.
　one of the most interesting and potentially fruitful open problems regarding snc's and wsc's is generalization to the first-order case and developing associated methods for automatically computing snc's and wsc's. in  lin  1   lin
states 
   there are several directions for future work. one of them is to extend the results here to the first-order case. this can be a difficult task. for instance  a result in  lin and reiter  1  shows that forgetting in the first-order case cannot in general be expressible in first-order logic. as a consequence  we expect that strongest necessary conditions of a proposition under a first-order theory cannot in general be expressible in first-order logic either. it seems that the best hope for dealing with the first -order case is to first reduce it to the propositional case  and then try to learn a first-order description from a set of propositional ones.
　with lin  we agree that the task is difficult. we also agree that in the general case snc's and wsc's for a proposition or first-order formula under a first-order theory is not always expressible in first-order logic. in fact  the techniques we propose provide a great deal of insight into why this is the case and in addition define a non-trivial fragment of first-order logic where snc's and wsc's are guaranteed to be expressible in first-order logic.
　rather than using indirect techniques to reduce a first-order case to the propositionalcase and then try to learn a first-order description from a set of propositional ones as lin suggests  we propose a more direct method and provide techniques for the automatic generation of snc's and wsc's for a first-order fragment. given a theory and a formula in this fragment  we simply append appropriate quantifiers over relational variables in  or   and use second-order quantifier elimination techniques to reduce the second-order formula to a logically equivalent first-order formula representing the snc or the wsc for under . complexity results are provided for this fragment  but the method works for full first-order logic. in this case  depending on the nature of and   the technique may return a logically equivalent first-order or fixpoint formula  or terminate with failure  not always because there is not a reduction  but simply because the elimination algorithm can not find a reduction.
　we compute these conditions using extensions of results described in the work of  doherty et al.  1; 1; nonnengart and szalas  1 . for a survey of these and other quantifier elimination techniques  see also  nonnengart et al.  1 .
1	weakest sufficient and strongest necessary conditions
in the following  we will be dealing with the predicate calculus with equality  i.e. we assume that the equality    is always a logical symbol. the following definitions describe the necessary and sufficient conditions of a formula relativized to a subset of relation symbols under a theory .
definition 1 by a necessary condition of a formula on the set of relation symbols under theory we shall understand any formula containing only symbols in such that
　　　　　. it is a strongest necessary condition  denoted by snc	if  additionally  for any necessary condition of	on	under	  we have that	. 
definition 1 by a sufficient condition of a formula on the set of relation symbols under theory we shall understand any formula containing only symbols in such that
　　　　　. it is a weakest sufficient condition  denoted by wsc	if  additionally  for any sufficient condition of	on	under	  we have that	. 
　the set in definitions 1 and 1 is referred to as the target language.
　to provide some intuition as to how these definitions can be used  consider the theory
and the formula	. clearly	.
quite often  it is useful to hypothesize a preferred explanation for under a theory where   is minimal in the sense of not being overly specific and where the explanation is constrained to a particular subset of symbols in the vocabulary. clearly  the weakest sufficient condition for the formula on under provides the basis for a minimal preferred explanation of where . in the case of   the weakest sufficient condition would be   and in the case of	the wsc would be
　　　　　　　　. generating abductive hypotheses is just one application of wsc's. there are many other applications which require generation of wsc's or snc's  several of which are described in section 1.
1	paper structure
in section 1  we begin with preliminaries and state the theorem which provides the basis for second-order quantifier elimination techniques. in section 1  we define snc's and wsc's for propositional formulas under propositional theories as second-order formulas with quantification over propositional symbols  show how the elimination techniques are applied and provide complexity results for the technique. in section 1  we generalize to the first-order case using primarily the same techniques  but with quantification over relational symbols. in section 1  we demonstrate both the use of snc's and wsc's in addition to the reduction techniques by providing examples from a number of potentially interesting application areas such as agent communication languages  theory approximation  generation of abductive hypotheses and generation of successor state axioms in the situation calculus. in section 1  we relate the proposed techniques to the notion of forgetting which serves as a basis for lin's techniques and in so doing  prove subsumption results. in section 1  we conclude with a discussion about these results.
1	preliminaries
the following theorem 1 has been proved in  nonnengart and szalas  1 . it allows us to eliminate second-order quantifiers from formulas which are in the form appearing on the left-hand side of the equivalences  1    1 . such formulas are called semi-horn formulas w.r.t. the relational variable - see also  doherty et al.  1 . observe  that in the context of databases one remains in the tractable framework  since fixpoint formulas over finite domains are computable in polynomialtime  and space  - see e.g.  abiteboul et al.  1; ebbinghaus and flum  1 .  of course  the approach is applicable to areas other than databases  too .
notation 1 let be any expressions and any subexpression of . by we shall mean the expression obtained from by substituting each occurrence of by .
　　　　is the least fixpoint operator and	is defined as	.
let be a formula with free variables . then by we shall mean the application of to arguments
. 
theorem 1 assume that all occurrences of the predicate variable in the formula bind only variables and that formula is positive w.r.t. .
	if	is negative w.r.t.	then
 1 
	if	is positive w.r.t.	then
 1 

example 1 consider the following second-order formula:
　　　　　　　　　　　　　　　　　　　　　　　 1  according to theorem 1 1   formula  1  is equivalent to:
 1 

　observe that  whenever formula in theorem 1 does not contain   the resulting formula is easily reducible to a firstorder formula  as in this case both and are equivalent to . in fact  this case is equivalent to the lemma of ackermann  see e.g.  doherty et al.  1  . semi-horn formulas of the form  1  and  1   where does not contain   are called non-recursive semi-horn formulas.
1	the propositional case
in this section  we define snc's and wsc's for propositional formulas under propositional theories as second-order formulas with quantification over propositional symbols  show how the elimination techniques are applied and provide complexity results for the technique. we start with the following lemma.
lemma 1 for any formula   any set of propositionalsymbols and theory :
1. the strongest necessary condition snc	is defined by	 
1. the weakest sufficient condition wsc	is defined by	 
where consists of all propositions appearing in and but not in .
proof the proof of the lemma for both the strongest necessary and weakest sufficient conditions are similar  but we provide both for clarity.
	by definition  any necessary condition	for	satisfies
             i.e. by the deduction theorem for propositional calculus  also   i.e. . thus also . since is required not to contain symbols from   we have
 1 
on the other hand  the minimal	satisfying  1  is given by equivalence
this proves lemma 1.1.
	by definition  any sufficient condition	for	satisfies
             i.e. by the deduction theorem for propositional calculus  also	  i.e.	. thus also	which is equivalent to
.
	since	is required not to contain symbols from   we have
 1 
maximizing is the same as minimizing . on the other hand  the maximal satisfying  1  is given by equivalence
which is equivalent to
which is equivalent to
this proves lemma 1.1. 
　the quantifiers over propositions can be automatically eliminated using the dls algorithm  see  doherty et al.  1  . for instance  all eliminations in example 1 can be done using the algorithm. theorem 1 reduces in the propositional case to proposition 1. it is worth emphasizing here that propositional fixpoint formulas are equivalent to propositional formulas. 1
proposition 1 assume that the propositional formula is positive w.r.t. proposition .
	if the propositional formula	is negative w.r.t.	then
 1 
	if	is positive w.r.t.	then
 1 

　observe that in the case when an input formula is a conjunction of propositional semi-horn formulas of the form in the lhs of  1  or a conjunction of formulas of the form in the lhs of  1   the length of the resulting formula is  in the worst case    where is the size of the input formula. otherwise the result might be of exponential length  as in the case of the algorithm given in  lin  1 .
example 1 consider the following examples of  lin 
1 .
1. . now  according to lemma 1 
	snc	is defined by formula
　　　　　　  which  according to proposition 1  is logically equivalent to .
	condition snc	is defined by formula
  which  according to propo-
sition 1  is logically equivalent to  observe that is equivalent to the semi-horn formula  .
1. .	we	have	that
	snc	is	defined	by	the	formula
                           which  according to proposition 1  is logically equivalent to .
	condition snc	is defined by the formula
  which is logically equiva-
	lent to	.
1. . the formula snc is equivalent to	  which  according to proposition 1  is logically equivalent to .
observe that we work with formulas more directly than proposed in lin's approach  where a new proposition has to be introduced together with an additional condition that the new proposition is equivalent to the formula in question.

　in summary  propositional snc's or wsc's can be generated for any propositional formula and theory. in the case that the conjunction of both is in semi-horn form  this can be done more efficiently. these results subsume those of lin  lin  1  in the sense that the full propositional language is covered and we work directly with propositional formulas rather than propositional atoms.
1	the first-order case
in this section  we generalize the results in section 1 to the first-order case using primarily the same techniques  but with quantification over relational symbols. the following lemma can be proved similarly to lemma 1. the deduction theorem for first-order logic is applicable  since the theories are assumed to be closed.
lemma 1 for any formula   any set of relation symbols and a closed1 theory :
1. the strongest necessary condition snc	is defined by	 
1. the weakest sufficient condition wsc	is defined by	 
where	consists of all relation symbols appearing in	and but not in	. 
　observe that a second-order quantifier over the relational variable can be eliminated from any semi-horn formula w.r.t. . in such a case the resulting formula is a fixpoint formula. if the formula is non-recursive  then the resulting formula is a first-order formula. the input formula can also be a conjunction of semi-horn formulas of the form  1  or a conjunction of semi-horn formulas of the form  1 . on the other hand  one should be aware that in other cases the reduction is not guaranteed. thus the elimination of second-order quantifiers is guaranteed for any formula of the form   where is a conjunction of semi-horn formulas w.r.t. all relational variables in .1 observe also  that in the case when an input formula is a conjunction of semi-horn formulas of the form  1  or a conjunction of formulas of the form
 1   the length of the resulting formula is  in the worst case    where is the size of the input formula.
example 1 consider the following examples
1.	.
	consider	the	strongest	necessary	condition
	snc	.	according	to
lemma 1  it is equivalent to
 1 
by lemma 1  formula  1  is equivalent to
.
1.
　　　　　　　. consider the strongest necessary condition snc	. according to
lemma 1  it is equivalent to
 1 
in this case  formula  1  is not in the form required in lemma 1  but the dls algorithm eliminates the second-order quantifiers and results in the equivalent formula   which is the required strongest necessary condition. consider now snc
	.	it is equiva-
lent to
 1  i.e.	after eliminating second-order quantifiers 	to
.

　in summary  for the non-recursive semi-horn fragment of first-order logic  the snc or wsc for a formula and theory is guaranteed to be reducible to compact first-order formulas. for the recursive case  the snc's and wsc's are guaranteed to be reducible to fixpoint formulas. in the context of databases  this case is still tractable. the techniques may still be used for the full first-order case  but neither reduction nor complexity results are guaranteed  although the algorithm will always terminate.
1	applications
in this section  we demonstrate the use of the techniques by applying them to a number of potentially useful application areas.
1	communicating agents
agents communicating  e.g. via the internet  have to use the same language to understand each other. this is similar or related to computing interpolants.
　assume an agent wants to ask a query to agent . suppose the query can be asked using terms such that the terms from are unknown for agent . let be a theory describing some relationships between and . it is then natural for agent to first compute the strongest necessary condition snc with the target language restricted to and then to replace the original query by the computed condition. the new query might not be as good as the previous one  but is the best that can be asked. the following example illustrates the idea.
example 1 assume an agent wants to select from a database all persons such that holds.
assume further  that both agents know the terms	and
　　　. unfortunately  the database agent does not know the term .1 suppose  further that lives in a world in which the condition	holds. it is then
natural for	to consider
to be the best query that can be asked. according to lemma 1 this condition is equivalent to:
which  by a simple application of theorem 1  is equivalent to	. 
1	theory approximation
the concept of approximatingmore complex theories by simpler theories has been studied in  kautz and selman  1; cadoli  1   mainly in the context of approximating arbitrary propositional theories by propositional horn clauses. the concept of approximate theories is also discussed in  mccarthy  1 . now  observe that strongest necessary and weakest sufficient conditions provide us with approximations of theories expressed in a richer language by theories expressed in a simpler language.
　the approach by lin in  lin  1  allows one only to approximate simple concepts on the propositional level. the generalization we introduce allows us to approximate any finite propositional and first-order theory which is semi-horn w.r.t. the eliminated propositions or relational symbols.
　in the following example  considered in  kautz and selman  1  approximating general clauses by horn clauses results in the exponential blow up of the number of clauses.
we shall show  that the use of the notion of strongest necessary condition can substantially reduce the complexity of reasoning.
example 1 in  kautz and selman  1  the following clauses  denoted by   are considered:
 1 
 1 
 1 
 1 
and reasoning with this theory was found to be quite complicated. on the other hand  one would like to check  for instance  whether a computer scientist who reads dennett and kosslyn is also a cognitive scientist. reasoning by cases  suggested in  kautz and selman  1   shows that this is the case. one can  however  substantially reduce the theory and make the reasoning more efficient. in the first step one can notice that notions and are not in the considered claim  thus might appear redundant in the reasoning process. on the other hand  these notions appear in disjunctions in clauses  1  and  1 . we then consider
snc
 1 
where denotes all symbols in the language  other than and . after some simple calculations one obtains the following formula equivalent to  1 :
 1 
which easily reduces to
 1  thus the strongest necessary condition for the formula
implies	and  consequently  the formula also implies
.
　assume that one wants to calculate the weakest sufficient condition of being a computer scientist in terms of
. we then consider
wsc	 1 
after eliminating quantifiers over from the second-order formulation of the wsc  one obtains the following formula equivalent to  1 :
thus the weakest condition that  together with theory	  guar-
antees that a person is a computer scientist is that the person reads mccarthy and is not a cognitive scientist. 
1	abduction
the weakest sufficient condition correspondsto a weakest abduction  as noticed in  lin  1 . example 1 consider theory
assume one wants to check whether an object can move.
there are three interesting cases:
1. to assume that the target language is	and consider
wsc
which is equivalent to
1. to assume that the target language is	and consider
wsc
which is equivalent to
1. to	assume	that	the	target	language	is
and consider
wsc
which is equivalent to
after eliminating second-order quantifiers we obtain the following results:
1. wsc
1. wsc
1. wsc
.
the first two conditions are rather obvious. the third one might seem a bit strange  but observe that is an axiom of theory . thus  in the third
case  we have that
wsc

1	generating successor state axioms
example 1 consider the problem of generating successor state axioms in a robot domain. this problem  in the propositional framework  is considered in  lin  1 . on the other hand  a first-order formulationis much more natural and compact. we thus apply first-order logic rather than the propositional calculus and introduce the following relations  instead of propositions as considered in  lin  1 :
         - the robot is performing the action of moving the object from location to location
         - initially  the object	is in the location
         - after the action	  the object	is
in location
         - initially  the robot is at location
         - after the action	  the robot is at
location
         - initially  the robot is holding the object
         - after the action   the robot is holding the object .
assume that the background theory contains the following axioms  abbreviated by :
the goal is to find the weakest sufficient condition on the initial situation ensuring that the formula holds. thus we consider
wsc
the approach we propose is based on the observation that
wsc
after some simple calculations which can be performed automatically using the dls algorithm we ascertain that wsc	is equivalent to:
which  in the presence of axioms of theory	reduces to:
 1 
and  since holds in the theory only for equal to   formula  1  reduces to:
thus  the weakest condition on the initial state  making sure that after the execution of an action the package is in location   expresses the requirement that the robot is in location   holds the package and that it executes the action of moving the package from location to location . 
1	forgetting and quantifier elimination
forgetting is considered in  lin and reiter  1; lin  1  as an important technique for database progression and computing wsc's and snc's.1 given a theory and a relation symbol   forgetting about in results in a theory with a vocabulary not containing   but entailing the same set of sentences that are irrelevant to . observe that forgetting is simply a second-order quantification  as shown in  lin and reiter  1 . namely 
it is no surprise then that forgetting is not always reducible to first-order logic. on the other hand  due to theorem 1  second-order quantifiers can often be eliminated resulting in a fixpoint or first-order formula.
consider the following example.
example 1 let	consist of the following two axioms:
forgetting about results in the second-order formula which  according to theorem 1  is equivalent to:

1	conclusions
using lin's work as a starting point  we have provided new definitions for weakest sufficient and strongest necessary conditions in terms of 1nd-order formulas and provided the basis for algorithms which are guaranteed to automatically generate wsc's and snc's for both the propositional case and a nontrivial fragment of the first-order case. for the general propositional case  propositional snc's and wsc's are always generated using the techniques and for the semi-horn fragment are always generated efficiently. for the first-order case restricted to the non-recursive semi-horn fragment  reduction of wsc's and snc's to first-order formulas is always guaranteed and can be done efficiently. for the first-order case restricted to the recursive semi-horn fragment  reduction to fixpoint formulas is always guaranteed and can be done efficiently. for the general first-order case  the techniques can also be applied  but reductions are not always guaranteed  even though the algorithm will always terminate.
　this work generalizes that of lin which only deals with the propositional case and it provides more direct methods for generating snc's and wsc's via syntactic manipulation. we have also demonstrated the potential use of this idea and these techniques by applying them to a number of interesting applications. finally  we have re-interpreted the notion of forgetting in terms of quantifier elimination  shown how our techniques can be applied to a first-order version of forgetting and applied the technique to generation of successor-state axioms in restricted first-order situation calculus based action theories.
　as a final observation  the quantifier elimination algorithm considered here has been implemented as an extension to the original dls algorithm described in  doherty et al.  1   for both the propositional and 1st-order cases.
references
 abiteboul et al.  1  s. abiteboul  r. hull  and v. vianu. foundations of databases. addison-wesley pub. co.  1.
 brown  1  f.m. brown. boolean reasoning. kluwer academic publishers  dordrecht  1.
 cadoli  1  m. cadoli. tractable reasoning in artificial intelligence  volume 1 of lnai. springer-verlag  berlin heidelberg  1.
 dijkstra  1  e. w dijkstra. a discipline of programming. prentice-hall  1.
 doherty et al.  1  p. doherty  w. lukaszewicz  and a. szalas. a reduction result for circumscribed semihorn formulas. fundamenta informaticae  1-1 :1- 1  1.
 doherty et al.  1  p. doherty  w. lukaszewicz  and a. szalas. computing circumscription revisited. journal of automated reasoning  1 :1  1.
 doherty et al.  1  p. doherty  w. lukaszewicz  and a. szalas. general domain circumscription and its effective reductions. fundamenta informaticae  1 :1  1.
 ebbinghaus and flum  1  h-d. ebbinghaus and j. flum. finite model theory. springer-verlag  heidelberg  1.
 kautz and selman  1  h. kautz and b. selman. knowledge compilation and theory approximation. journal of the acm  1 :1  1.
 lin and reiter  1  f. lin and r. reiter. forget it! in r. greiner and d. subramanian  editors  working notes of aaai fall symposium on relevance  pages 1  menlo park  ca.  1. aaai.
 lin and reiter  1  f. lin and r. reiter. how to progress a database. artificial intelligence  1-1 :1  1.
 lin  1  f. lin. on strongest necessary and weakest sufficient conditions. in a.g. cohn  f. giunchiglia  and b. selman  editors  kr1  pages 1  1.
 mccarthy  1  j. mccarthy. approximateobjects and approximate theories. in a.g. cohn  f. giunchiglia  and b. selman  editors  kr1  pages 1  1.
 nonnengart and szalas  1  a. nonnengart and a. szalas. a fixpoint approach to second-order quantifier elimination with applications to correspondence theory. in e. orlowska  editor  logic at work: essays dedicated to the memory of helena rasiowa  volume 1 of studies in fuzziness and soft computing  pages 1. springer physica-verlag  1.
 nonnengart et al.  1  a. nonnengart  h.j. ohlbach  and a. szalas. elimination of predicate quantifiers. in h.j. ohlbach and u. reyle  editors  logic  language and reasoning. essays in honor of dov gabbay  part i  pages 1. kluwer  1.

knowledge representation
and reasoning
description logics

identification constraints and functional dependencies in description logics
diego calvanese  giuseppe de giacomo  maurizio lenzerini
dipartimento di informatica e sistemistica
universita` di roma  la sapienza 
via salaria 1  1 roma  italy
{calvanese degiacomo lenzerini} dis.uniroma1.it

abstract
dlr is an expressive description logic  dl  with n-ary relations  particularly suited for modeling database schemas. although dlr has constituted one of the crucial steps for applying dl technology to data management  there is one important aspect of database schemas that dls  including dlr  do not capture yet  namely the notion of identification constraints and functional dependencies. in this paper we introduce a dl which extends dlr and fully captures the semantics of such constraints  and we address the problem of reasoning in such a logic. we show that  verifying knowledge base satisfiability and logical implication in the presence of identification constraints and nonunary functional dependencies can be done in exptime  thus with the same worst-case computational complexity as for plain dlr. we also show that adding just unary functional dependencies to dlr leads to undecidability.
1	introduction
in the last years  description logics  dls  have been successfully applied to data management  borgida  1; kirk et al.  1; calvanese et al.  1b; 1 . one of the basic ideas behind applying dls to data management is that database schemas can be expressed as dl knowledge bases  so that dl reasoning techniques can be used in several ways to reason about the schema. in  calvanese et al.  1a; 1b   a very expressive dl with n-ary relations  called dlr  is introduced  and it is shown how database schemas can be captured by this logic. also  suitable mechanisms for expressing queries over dlr schemas have been added  and techniques for reasoning over queries have been designed. notably  the investigation on dlr has led to the design of new dl systems effectively implementing powerful reasoning techniques  horrocks et al.  1 .
　although the above mentioned work has been the crucial step for applying dl technology to data management  there is one important aspect of database schemas that dls  including dlr  do not capture yet  namely the notion of identification constraints and functional dependencies. identification constraints  also called keys  are used to state that a certain set of properties uniquely identifies the instances of a concept. a functional dependency on a relation is used to impose that a combination of a given set of attributes functionally determines another attribute of the relation. it is easy to see that functional dependencies can be used to model keys of a relation  i.e.  attributes that are sufficient to identify tuples of the relation. both types of constraints are commonly used in database design and data management.
　the question addressed in this paper is as follows: can we add identification constraints and non-unary functional dependencies to dlr and still have exptime associated reasoning techniques  somewhat surprisingly  we answer positively to the question  by illustrating an approach that allows us to incorporate both types of constraints in dlr. in particular  we adapt the dlr reasoning algorithm in such a way that reasoning on a dlr schema with both types of constraints and with aboxes  can be done with the same worstcase computational complexity as for the case of plain dlr. also  the proposed technique can be incorporated into present dl systems  such as the one described in  horrocks et al.  1 . we also show that adding to dlr unary functional dependencies leads to undecidability of reasoning. observe  however  that the presence of such functional dependencies is typically considered as an indication of bad schema design in databases.
　both identification constraints and functional dependencies have been extensively investigated in the database literature  see  abiteboul et al.  1   chapters 1  1 . however  database models lack the kinds of constraints expressible in expressive dls  and therefore none of the results developed in the context of databases can be used to solve our problem.
　in the last years  there have been some attempts to add identification constraints to dls. in  calvanese et al.  1   these constraints are modeled by means of special primitive concepts in an expressive dl  and it is shown that this mechanism allows some inference on keys to be carried on. the limitation of this approach is that several interesting semantic properties of keys are not represented in the knowledge base. in  borgida and weddell  1; khizder et al.  1   a general mechanism is proposed for modeling path functional dependencies  and a sound and complete inference system for reasoning on such constraints is presented. path functional dependencies are sufficiently expressive to model both identification constraints and functional dependencies. however  the dls considered are limited in expressiveness. in particular  union  negation  number restrictions  general inclusion axioms  inverse roles and n-ary relations are not part of the considered language  and therefore useful properties of database schemas cannot be represented. the proposal presented in this paper fully captures the semantics of both identification constraints and functional dependencies in an expressive dl with all the above features.
　the paper is organized as follows. in section 1  we recall the dl dlr. in section 1  we illustrate the mechanism for specifying identification constraints and functional dependencies in dlr knowledge bases. in section 1  we discuss the modeling power of the resulting logic  called dlrifd. in section 1  we describe how we can extend the dlr reasoning technique in order to take the new types of constraints into account  and in section 1 we show that minor extensions of dlrifd leads to undecidability of reasoning. finally  section 1 concludes the paper.
1	description logic dlr
we focus on the description logic dlr  which is able to capture a great variety of data models with many forms of constraints  calvanese et al.  1a; 1 . the basic elements of dlr are concepts  unary relations   and n-ary relations.
　we assume to deal with a finite set of atomic relations  having arity between 1 and nmax  and atomic concepts  denoted by p and a  respectively. we use r to denote arbitrary relations and c to denote arbitrary concepts  respectively built according to the following syntax:
	r	::=	 n | p |  i/n:c  |  r | r1 u r1
	c	::=	 1 | a |  c | c1 u c1 |  ＋ k  i r 
where n denotes the arity of the relations p  r  r1  and r1  i denotes a component of a relation  i.e.  an integer between 1 and n  and k denotes a non-negative integer. observe that we consider only concepts and relations that are well-typed  which means that:  i  only relations of the same arity n are combined to form expressions of type r1ur1  which inherit the arity n ;  ii  i ＋ n whenever i denotes a component of a relation of arity n.
we introduce the following abbreviations: 〕 for    ;
for   ＋ k 1 i r ;   i r for  − 1 i r ;   i r for    i  r. moreover  we abbreviate  i/n:c  with  i:c  when n is clear from the context.
　a dlr tbox is constituted by a finite set of inclusion assertions  where each assertion has one of the forms:
	c1 v c1	r1 v r1
with r1 and r1 of the same arity.
　the semantics of dlr is specified as follows. an interpretation i is constituted by an interpretation domain  i  and an interpretation function ，i that assigns to each concept c a subset ci of  i and to each relation r of arity n a subset ri of   i n such that the conditions in figure 1 are satisfied. in the figure  t i  denotes the i-th component of tuple t. observe that  the     constructor on relations is

figure 1: semantic rules for dlr  p  r  r1  and r1 have arity n  and  σ denotes the cardinality of the set σ 
used to express difference of relations  and not the complement  calvanese et al.  1a . an interpretation i satisfies an assertion c1 v c1  resp.   resp. 
r1i   ri .
　introduce a generalized form of dlr abox. we consider an alphabet of new symbols  called skolem constants  sk-constants . intuitively  an sk-constant denotes an individual in an interpretation  in such a way that different skconstants may denote the same individual.
　a generalized dlr abox  or simply abox in the following  is constituted by a finite set of assertions  called abox assertions  of the following types:
	c x 	r x1 ... xn 	x 1= y	x = y
where r is a relation of arity n  and x  y  x1 ... xn are skconstants.
　the notion of interpretation is extended so as to assign to each sk-constant x an individual xi （  i. an interpretation i satisfies
  c x  if xi （ ci;
;
  x 1= y if xi 1= yi;   x = y if xi = yi.
　if t is a dlr tbox  and a is a dlr abox of the above form  then k = t “ a is called a dlr knowledge base. an interpretation is a model of k if it satisfies every assertion in k. a knowledge base k is satisfiable if it has a model. an assertion α  either an inclusion  or an abox assertion  is logically implied by k if all models of k satisfy α.
　logical implication and knowledge base satisfiability are mutually reducible to each other. for one direction  k is unsatisfiable iff k |=  1 v 〕. for the converse direction  it is possible to show that k |= c1 v c1 iff k“{ 1 v   p u 1:c1 u c1  } is unsatisfiable  where p is a new binary relation. similarly  k |= r1 v r1 iff k “ { 1 v   p u  1:  c1 u  c1   } is unsatisfiable  where again p is a new binary relation. finally  k |= c α  iff k “ { c α } is unsatisfiable.
　it follows from the results in  calvanese et al.  1a   that checking a dlr knowledge base for satisfiability is exptime-complete.
1	identification and functional dependency assertions
we extend dlr with identification constraints and functional dependencies. the resulting dl  called dlrifd  allows one to express these constraints through new kinds of assertions in the tbox.
an identification assertion on a concept has the form:
 id c  i1 r1 ...  ih rh 
where c is a concept  each rj is a relation  and each ij denotes one component of rj. intuitively  such an assertion states that two instances of c cannot agree on the participation to r1 ... rh via components i1 ... ih  respectively.
　a functional dependency assertion on a relation has the form:
                fd r i1 ... ih ★ j  where r is a relation  h − 1  and i1 ... ih j denote components of r. the assertion imposes that two tuples of r that agree on the components i1 ... ih  agree also on the component j.
　we assign semantics to these assertions by defining when an interpretation satisfies them. specifically:
  an interpretation i satisfies the assertion  id c  i1 r1 ...  ih rh  if  for all a b （ ci and for all t1 s1 （ r1i ... th sh （ rhi  we have that:
 implies a = b
	and for i 1= ij	  
  an interpretation i satisfies the assertion  fd r i1 ... ih ★ j  if  for all t s （ ri  we have that:
	t i1  = s i1   ...  t ih  = s ih 	implies	t j  = s j 
　a dlrifd knowledge base is a set k = t “ a “ f of assertions  where t “ a is a dlr knowledge base and f is a set of identification and functional dependency assertions.
　note that unary functional dependencies  i.e.  functional dependencies with h = 1  are ruled out in dlrifd. we will come to this in section 1. note also that the right hand side of a functional dependency contains a single element. however  this is not a limitation  because any functional dependency with more than one element in the right hand side can always be split into several dependencies of the above form. also  to verify whether a functional dependency with more than one element in the right hand side is logically implied by a dlrifd knowledge base  it suffices to verify whether each of the functional dependencies in which it can be split  is logically implied by the knowledge base.
1	modeling in dlrifd
dlrifd captures database schemas expressed in several data models . for example  entity-relationship schemas can be represented already in dlr  by modeling each entity as a concept  and each relationship as a relation  calvanese et al.  1b; 1 . attributes of entities are modeled by means of binary relations  and single-valued or mandatory attributes are expressible through the use of number restrictions. attributes of relationships can be modeled in several ways  for instance through special  n + 1 -ary relations  where n is the arity of the relationship. also  integrity constraints such as is-a  cardinality  existence  and typing constraints are expressible by means of inclusion assertions. finally  unary keys  keys constituted by a single attribute  can be modeled through number restrictions. non-unary keys cannot be represented in dlr  while they are obviously expressible in dlrifd.
example 1 suppose that person and university are concepts  enrolledin is a binary relation between person and university  and studentcode is an  optional  attribute  modeled as a binary relation  of person associating to each student  a person who is enrolled in a university  a code that is unique in the context of the university in which she is enrolled. such a situation can be represented by the following dlrifd tbox:
	enrolledin	v	 1: person  u  1: university 
studentcode v  1: person  u  1: string  person v  ＋ 1studentcode 
 id person studentcode  enrolledin 
note that  the notion of student is modeled by the concept person u  enrolledin and  in the conceptual modeling terminology  this concept is a weak entity  i.e.  part of its identifier is external through the relationship enrolledin.
　we additionally want to model the notion of exam in our application. an exam is a relationship involving a student  a course  a professor  and a grade. in an exam  the combination of student and course functionally determines both the professor  and the grade. this can be represented by adding the following assertions to the tbox:
exam v  1: person u  enrolledin   u
 1: course  u  1: person  u  1: grade 

	 fd exam 1 ★ 1 	 fd exam 1 ★ 1 
　observe that generally  in conceptual data models  if an attribute  or a relationship  l is part of a key for an entity e  then in the database schema it must be the case that e has a single and mandatory participation in l  i.e.  each instance of e has exactly one associated value for l  abiteboul et al.  1 . this is not required in dlrifd  but can be asserted when needed   where one can define an attribute as part of a key of an entity  even if the attribute is multi-valued or optional.
　we have mentioned that unary functional dependencies are not allowed in dlrifd. however  this limitation does not prevent one from defining unary keys for relations. indeed  the fact that component i is a key for the relation r can already be expressed in dlr by means of the assertion:  1 v  ＋ 1 i r 
the above observation also implies that functional dependencies in the context of binary relations  which are by definition unary  are expressible in dlrifd. indeed  such functional dependencies correspond to key constraints  which are expressible as specified above. for example  the functional dependency 1 ★ 1 in the context of the binary relation r can be expressed by specifying that component 1 is a key for r. thus  the only functional dependencies that are not admitted in dlrifd are unary functional dependencies in the context of non-binary relations. this is because they lead to undecidability of reasoning  as shown in section 1. note also  that the presence of such functional dependencies is considered as an indication of bad design in the framework of the relational data model  see  abiteboul et al.  1   chapter 1 . in fact  a unary functional dependency in the context of an n-ary relation  with n   1  represents a hidden relationship between the arguments of the relation  which may cause several modeling problems.
　the possibility of defining identification constraints substantially enriches the modeling power of dls. in particular  it is possible to show that  even if only binary relations are allowed in a dl  then the use of identification constraints permits simulating the presence of n-ary relations in such a logic. for example  a relation with arity 1 can be modeled by means of a concept and 1 binary relations. number restrictions are used to state that every instance of the concept participates in exactly one instance of the binary relation  and a suitable identification assertion states that the combination of the three binary relations form a key for the concept. obviously  dlrifd further increases the modeling power by allowing the explicit use of n-ary relations  and the possibility of imposing functional dependencies in the context of relations.
1	reasoning on dlrifd
first of all we observe that  when reasoning in dlrifd  identification assertions of the form  id c  i r   where r is a binary relation  are equivalent to dlr assertions   v  ＋ 1 j  r u  i:c     where j =1 if i=1  and j =1 if i=1.
hence  in the following  without loss of generality  we will not consider such identification assertions.
　next we show that we can reduce logical implication in dlrifd to knowledge base satisfiability. as already observed in section 1  logical implication of inclusion and abox assertions can be reduced to knowledge base satisfiability. we show that the same can be done also for identification and functional dependency assertions. given an identification assertion κ =  id c  i1 r1 ...  ih rh 
we define the abox aκ constituted by the following assertions:
  c x   c y   and x 1= y  where x and y are new skconstants;
  rj tj  and rj sj   with j （ {1 ... h}  where tj of	with tj ij  = x  j  j  =   and j    = j    for = j.
similarly  for a functional dependency assertion
κ =  fd r i1 ... ih ★ j 
we define aκ constituted by the following assertions:
  r t   r s   and t j  1= s j   where t and s are tuples of new sk-constants with t ij  = s ij   for j （ {1 ... h}.
　from the semantics of identification and functional dependency assertions it is immediate to see that aκ provides a concrete counterexample to κ. hence it follows that  given a dlrifd knowledge base k  k |= κ if and only if k “ aκ is unsatisfiable.
theorem 1 logical implication in dlrifd can be reduced to knowledge base satisfiability.
　we now present a reasoning procedure for knowledge base satisfiability in dlrifd.
　dlrifd tboxes  which in fact are dlr tboxes since they do not include identification and functional dependency assertions  have the tree-model property  calvanese et al.  1a   which is true for most dls. in particular  if a dlr tbox admits a model  it also admits a model which has the structure of a tree  where nodes are either objects or  reified  tuples  and edges connect tuples to their components. observe that in such models identification and functional dependency assertions  which in dlrifd are non-unary  are trivially satisfied  since there cannot be two tuples agreeing on more than one component. as an immediate consequence we have that  given a dlrifd tbox t and a set of identification and functional dependency assertions f  t “ f is satisfiable iff t is so. this implies that  in absence of an abox  logical implication of inclusion assertions can be verified without considering identification and functional dependency assertions at all.
　when we add an abox  then we may still restrict the attention to models that have the structure of a tree  except for a cluster of objects representing the sk-constants in the abox  see again  calvanese et al.  1a 1 . we call such models clustered tree models. on such models  identification and functional dependency assertions are always satisfied  except possibly for the cluster of sk-constants. hence we can concentrate on verifying such assertions on the objects and tuples appearing in the abox only.
　given a dlrifd knowledge base k  we define a saturation of k as an abox as constructed as follows:
  for each sk-constant x occurring in k  and for each identification assertion  id c  i1 r1 ...  ih rh  in k  as contains either c x  or  c x ;
  for each tuple t of sk-constants occurring in an assertion r t  of k 
- for	each	identification	assertion
 id c  i1 r1 ...  ih rh  in k  for each rj  j （ {1 ... h}  having the arity of r  as contains either rj t  or  rj t  
- for each functional dependency assertion  fd r1 i1 ... ih ★ j  in k  such that r1 has the arity of r  as contains either r1 t  or  r1 t ;
  for each pair of sk-constants x and y occurring in k  as contains either x = y or x 1= y.
note that the size of a saturation is polynomial in the size of k. note also that there are many  actually  an exponential number  of different saturations of k  one for each possible set of choices in the items above.
　on a saturation one can immediately verify whether an identification or functional dependency assertion of k is violated. indeed  for all sk-constants and tuples of sk-constants appearing in the saturation  membership or non-membership in the relevant relations and concepts appearing in the assertions that could be violated is explicitly asserted  after substituting each sk-constant with a representative of its equivalence class according to the equalities . hence  it suffices to verify whether the semantic condition of the assertion is violated  considering relations and concepts appearing in the assertion as primitives. once such a check on the identification and functional dependencies is done  it remains to verify whether as is consistent with the other assertions of k.
theorem 1 a dlrifd knowledge base k = t “ a “ f is satisfiable if and only if there exists a saturation as of k that does not violate the identification and functional dependency assertions in k and such that the dlr knowledge base t “ a “ as is satisfiable.
　proof sketch .     assume that there exists a saturation as that does not violate the identification and functional dependency assertions in k and such that t “a“as is satisfiable. then there exists a clustered tree model of t “a“as  and such interpretation is also a model of k.
　    assume that k is satisfiable. then from a model i of k one can directly construct a saturation for which i satisfies all assertions.	
　the above result provides us with an upper bound for reasoning in dlrifd  matching the lower bound holding already for dlr.
theorem 1 satisfiability and logical implication in dlrifd are exptime-complete.
　proof  sketch . by theorem 1  logical implication in dlrifd reduces to knowledge base satisfiability in dlrifd. by theorem 1  satisfiability of a dlrifd knowledge base k = t “ a “ f reduces to solving a  possibly exponential  number of tests  where each test involves one saturation as and consists in directly verifying all identification and functional dependency assertions in as  a polynomial step  and checking the satisfiability of the dlr knowledge base t “ a “ as  an exponential step . 
1	unary functional dependencies in dlrifd
one might wonder whether the method described in the previous works also for unary functional dependencies. actually  this is not the case since unary functional dependencies are not trivially satisfied in tree structured interpretations. for example  it may happen that two tuples of a relation r agree on say component 1 and not component 1  and therefore violate the functional dependency  fd r 1 ★ 1 .

figure 1: grid structure enforced by kt
　indeed  we show that if we allow for unary functional dependencies  then reasoning in dlrifd becomes undecidable. to do so we exhibit a reduction from the unconstrained quadrant tiling problem  van emde boas  1   which consists in deciding whether the first quadrant of the integer grid can be tiled using a finite set of square tile types in such a way that adjacent tiles respect adjacency conditions. tiling problems are well suited to show undecidability of variants of description and dynamic logics  van emde boas  1; baader and sattler  1 . the crux of the undecidability proof consists in enforcing that the tiles lie on an integer grid. once the grid structure is enforced  it is typically easy to impose the adjacency conditions on the tiles. in our case we exploit unary functional dependencies to construct the grid.
　formally  a tiling system is a triple t =  d h v  where d is a finite set of elements representing tile types and h and v are two binary relations over d. the unconstrained quadrant tiling problem consists in verifying the existence of a tiling consistent with t  i.e.  a mapping τ from in 〜 in to d such that  τ i j  τ i + 1 j   （ h and  τ i j  τ i j + 1   （ v  for i j （ in. such a problem is undecidable  more precisely Π1-complete  berger  1; van emde boas  1 .
	from a tiling system t	=  d h v  we construct a
dlrifd knowledge base kt as follows. the basic idea is to enforce the grid structure shown in figure 1  where squares represent tuples of arity 1 and circles represent objects. each object depicted using a bold circle and labeled i j represents the node  i j  of the grid. numbers labeling arrows represent components of tuples. a pair of letters x y inside a tuple represents the fact that the tuple is an instance of the relations x and y . in particular we use four relations of arity 1: a b c  and d. the gray box in the figure represents how a tile would be placed in such a grid.
　we enforce the grid structure by means of the following assertions in kt:
for i （ {1 ... 1}  fd a 1 ★ 1   fd b 1 ★ 1   fd c 1 ★ 1   fd d 1 ★ 1   fd a 1 ★ 1   fd b 1 ★ 1   fd c 1 ★ 1   fd d 1 ★ 1 
　we enforce the adjacency conditions on the tiles of the first quadrant by using one concept for each tile type in d and introducing in kt the following assertions: for each di （ d
di v   b u d   1:f di dj （h dj   u
  a u c   1:f di dj （h dj   u   a u d   1:f di dj （v dj   u
  b u c   1:f di dj （v dj  
　finally  to represent the origin of the tiling we use the concept c1 =    1 ufdi（d di then the tiling problem associated to t admits a solution if and only if kt 1|= c1 v 〕. indeed  from a tiling consistent with t one obtains immediately a model of kt with an object satisfying c1. conversely  from a model i of kt with  we can construct a tiling consistent with t. the first set of assertions impose that a portion of i has exactly the structure depicted in figure 1  observe that not the whole model necessarily has a grid structure  but only a portion corresponding to the first quadrant . the second set of assertions impose on such a portion only that instances of concepts representing tile types respect the adjacency conditions. as a consequence of such a reduction we obtain the following result.
theorem 1 knowledge base satisfiability  and thus logical implication  in dlrifd extended with unary functional dependencies is undecidable.
　the reduction above can be easily modified to show that  if we allow for nominals  tobies  1   then even n-ary functional dependencies lead to undecidability. for example  we may use one nominal o and relations of arity 1 instead of 1. then we can force the fifth component of all tuples to be the object o by means of the assertion  1 v  1:o  and we can enforce the grid structure as above  by adding component 1 to the antecedent of the functional dependencies above  thus getting binary functional dependencies . observe that  since all tuples agree on component 1  such binary functional dependencies are actually mimicking the unary dependencies in the previous reduction.
1	conclusions
dlrifd extends dlr by fully capturing identification constraints on concepts and functional dependencies on relations. we have shown that reasoning in the presence of such constraints remains exptime decidable. we have also shown that adding to dlr just unary functional dependencies on non-binary relations  usually considered an indication of bad design in data modeling  leads to undecidability.
　the approach presented in this paper can be extended in several ways. for example  our technique can be directly applied to reasoning in dlrreg extended with identification and functional dependency constraints. moreover  we are working on the following extensions:  i  using chaining in specifying identification constraints  in the spirit of  borgida and weddell  1 ;  ii  introducing a notion of functional dependency between properties of concepts;  iii  query containment and query answering using views in the presence of identification constraints and functional dependencies.
references
 abiteboul et al.  1  s. abiteboul  r. hull  and v. vianu. foundations of databases. addison wesley publ. co.  reading  massachussetts  1.
 baader and sattler  1  f. baader and u. sattler. expressive number restrictions in description logics. j. of log. and comp.  1 :1  1.
 berger  1  r. berger. the undecidability of the dominoe problem. mem. amer. math. soc.  1-1  1.
 borgida and weddell  1  a. borgida and g. e. weddell. adding uniqueness constraints to description logics  preliminary report . in proc. of dood'1  pages 1  1.
 borgida  1  a. borgida. description logics in data management. ieee trans. on knowledge and data engineering  1 :1  1.
 calvanese et al.  1  d. calvanese  g. de giacomo  and m. lenzerini. structured objects: modeling and reasoning. in proc. of dood'1  volume 1 of lncs  pages 1. springer-verlag  1.
 calvanese et al.  1a  d. calvanese  g. de giacomo  and m. lenzerini. on the decidability of query containment under constraints. in proc. of pods'1  pages 1  1.
 calvanese et al.  1b  d. calvanese  g. de giacomo  m. lenzerini  d. nardi  and r. rosati. description logic framework for information integration. in proc. of kr'1  pages 1  1.
 calvanese et al.  1  d. calvanese  m. lenzerini  and
d. nardi. unifying class-based representation formalisms.
j. of artificial intelligence research  1-1  1.
 horrocks et al.  1  i. horrocks  u. sattler  and s. tobies. practical reasoning for expressive description logics. in proc. of lpar'1  number 1 in lnai  pages 1. springer-verlag  1.
 khizder et al.  1  v. l. khizder  d. toman  and g. e. weddell. on decidability and complexity of description logics with uniqueness constraints. in proc. of icdt 1  1.
 kirk et al.  1  t. kirk  a. y. levy  yehoshua sagiv  and divesh srivastava. the information manifold. in proceedings of the aaai 1 spring symp. on information gathering from heterogeneous  distributed enviroments  pages 1  1.
 tobies  1  s. tobies. the complexity of reasoning with cardinality restrictions and nominals in expressive description logics. j. of artificial intelligence research  1- 1  1.
 van emde boas  1  p. van emde boas. the convenience of tilings. in complexity  logic  and recursion theory  volume 1 of lecture notes in pure and applied mathematics  pages 1. marcel dekker inc.  1.
high performance reasoning with very large knowledge bases: a practical case study
volker haarslev and ralf moller：
university of hamburg  computer science department
vogt-kolln-str. 1  1 hamburg  germany：
http://kogs-www.informatik.uni-hamburg.de/ haarslev|moeller/abstract
we present an empirical analysis of optimization techniques devised to speed up the so-called tbox classification supported by description logic systems which have to deal with very large knowledge bases  e.g. containing more than 1 concept introduction axioms . these techniques are integrated into the race architecture which implements a tbox and abox reasoner for the description logic alcnhr+. the described techniques consist of adaptions of previously known as well as new optimization techniques for efficiently coping with these kinds of very large knowledge bases. the empirical results presented in this paper are based on experiences with an ontology for the unified medical language system and demonstrate a considerable runtime improvement. they also indicate that appropriate description logic systems based on sound and complete algorithms can be particularly useful for very large knowledge bases.
1	introduction
in application projects it is often necessary to deal with knowledge bases containing a very large number of axioms. furthermore  many applications require only a special kind of axioms  so-called concept introduction axioms. usually it has been argued that only systems based on incomplete calculi can deal with knowledge bases containing more than 1 axioms of this kind. in this contribution we present an empirical analysis of optimization techniques devised to improve the performance of description logic systems applied to this kind of knowledge bases. the analysis is based on the race1 architecture  haarslev and moller  1a：   which supports inference services for the description logic alcnhr+
 haarslev and moller  1b：	 .1
syntaxsemanticsconceptsa
 cccr	.ddc
 r.c
  n s
 −m s
 ＋a i    dc iii  a is a concept name 
i
i
{
ccaai（（”“  diii | | bb （（   ii :  :  aa  bb   （（ rrii  b （bc（ic}i}
{rolesrri	 	 　as example knowledge bases we consider reconstructions of important parts of the umls  unified medical language system   mccray and nelson  1  by using description logic representation techniques. the reconstruction is described in  hahn et al.  1; schulz and hahn  1  and   i 〜 i
 ，  denotes the cardinality of a set  s （ s  n m （ n  n   1.
tbox axiomssyntaxsatisfied ifrrc （tdsri =  ri + ri   si
ci	diabox assertionssyntaxsatisfied ifa:c
 a b :rai （ ci  ai bi 	ri		 	（
figure 1: syntax and semantics of alcnhr+.
introduces a specific encoding scheme that uses several concept introduction axioms which represent subset as well as composition aspects of conceptual descriptions  words  mentioned in the umls metathesaurus.
　the paper is structured as follows. we first introduce the syntax and semantics of alcnhr+ and characterize the form of axioms occurring in the umls knowledge bases. afterwards we describe the following five complementary optimization techniques:  1  topological sorting for achieving a quasi definition order;  1  a method to cluster siblings in huge taxonomies;  1  a technique for efficiently addressing so-called domain and range restrictions;  1  exploitation of implicit disjointness declarations;  1  subset/superset caching for increasing runtime performance and reducing memory requirements. the effectiveness of these techniques is assessed using the empirical results obtained from processing the umls knowledge bases.
　we briefly introduce the description logic  dl  alcnhr+  haarslev and moller  1b：    see the tables in figure 1  using a standard tarski-style semantics. alcnhr+ extends the basic description logic alc by role hierarchies  transitively closed roles  denoted by the set t in figure 1   and number restrictions. note that the combination of transitive roles and role hierarchies implies the expressivity of so-called general inclusion axioms  gcis . the concept name  is used as an abbreviation for c.
　if r s are role names  then r  s is called a role inclusion axiom. a role hierarchy r is defined by a finite set of role inclusion axioms. the concept language of alcnhr+ syntactically restricts the combinability of number restrictions and transitive roles due to a known undecidability result in case of an unrestricted syntax  horrocks et al.  1 . only simple roles may occur in number restrictions. roles are called simple  denotes by the set s in figure 1  if they are neither transitive nor have a transitive role as descendant.
　if c and d are concept terms  then c  generalized concept inclusion or gci  is a terminological axiom. a finite set w.r.t. to a given role hierarchytr of terminological axioms is called ar.1 a terminological axiom ofterminology or tbox the form a  c is called a concept introduction axiom and c is called the primitive  concept  definition of a if a is a concept name which occurs only once on the left hand side of the
axioms contained in a tbox t . a pair of gcis of the form
.
 abbreviated as a = c  is called a concept definition axiom and c is called the  concept  definition of a if a is a concept name which occurs only once on the left hand side of all axioms contained in a tbox t .
　an abox a is a finite set of assertional axioms as defined in figure 1. the abox consistency problem is to decide whether a given abox a is consistent w.r.t. a tbox t and a role hierarchy r. an abox a is consistent iff there exists a model i that satisfies all axioms in t and all assertions in a. subsumption between concepts can be reduced to concept satisfiability since c subsumes d iff the concept d is unsatisfiable. satisfiability of concepts can be reduced to abox consistency as follows: a concept c is satisfiable iff the abox {a:c} is consistent.
　the dl reconstruction of important parts of the umls introduces a specific scheme where a set of concept introduction axioms is used to represent subset as well as composition aspects of conceptual descriptions  words  mentioned in the umls metathesaurus. for instance  for the notion of a 'heart'  the following concept introduction axioms for heart structures  suffix 's'   heart parts  suffix 'p'  and heart entities  no suffix  are declared  see  schulz and hahn  1  for details :
ana heart  anahearthollow viscus	umls body part organ or organcomponent ana hearthollowviscus s	
ana cardiovascular system p
anaheart p   ana heart	ana heart s	
 −1 anatomical part of ana heart
　note the implicit disjointness declared between ana heart p and ana heart. the following role axiom is generated as well.
anatomical part of ana heart  anatomical part of ana hollow viscus 1the reference to r is omitted in the following if we use t .
　it is beyond the scope of this paper to discuss the pros and cons of specific modeling techniques used in the umls reconstruction. in the next section  optimization techniques for efficiently dealing with these kinds of knowledge bases are presented.
1	optimization techniques
modern dl systems such as race offer at least two standard inference services for concept names occurring in tboxes: classification and coherence checking. classification is the process of computing the most-specific subsumption relationships between all concept names mentioned in a tbox t . the result is often referred to as the taxonomy of t and gives for each concept name two sets of concept names listing its  parents   direct subsumers  and  children   direct subsumees . coherence checking determines all concept names which are unsatisfiable.
　expressive dls such as alcnhr+ allow gcis which can considerably slow down consistency tests. a true gci is a gci of the form c  d where c is not a name and c  d is not part of a pair representing a concept definition axiom. a standard technique  gci absorption   horrocks and tobies  1  which is also part of the race architecture performs a tbox transformation which precedes classification and coherence checking. the axioms in a tbox are transformed in a way that true gcis can be absorbed into  primitive  concept definitions which can be efficiently dealt with by a technique called lazy unfolding  e.g. see  baader et al.  1  . lazy unfolding dynamically expands a concept name by its  primitive  definition during an abox consistency test. the true gcis remaining after the gci absorption are referred to as global axioms.
　our findings indicate that state-of-the-art techniques currently employed for fast classification of tboxes have to be extended in order to cope with very large knowledge bases of the above-mentioned kind. in the following we describe these extensions.
1	topological sorting for quasi definition order
for tbox classification the race architecture employs the techniques introduced in  baader et al.  1 . the parents and children of a certain concept name are computed in socalled 'top search' and 'bottom search' traversal phases  respectively. these phases can be illustrated with the following
example. assume a new concept name ai has to be inserted into an existing taxonomy. the top search phase traverses the
taxonomy from the top node    via the set of children and checks whether ai is subsumed by a concept node. basically  if ai is subsumed by a node aj  then the children of aj are traversed. the top search phase determines the set of parents of ai. when the top search phase for ai has been finished  the bottom search phase for ai analogously traverses the taxonomy from the bottom node via the set of parents of a node. the bottom search phase determines the set of children of ai.
　using the marking and propagation techniques described in  baader et al.  1  the search space for traversals can usually be pruned considerably. it is always advantageous to avoid as many traversals as possible since they require the use of  expensive  subsumption tests. this is even more important for very large tboxes.
　let us assume  a tbox to be classified can be transformed such that no global axioms remain but cyclic  primitive  concept definitions may exist. according to  baader et al.  1  we assume that a concept name a 'directly uses' a concept name b if b occurs in the  primitive  definition of a. the relation 'uses' is the transitive closure of 'directly uses.' if a uses b then b comes before a in the so-called definition order. for acyclic tboxes  i.e. the uses relation is irreflexive  containing concept introduction axioms only  the set of concept names can be processed in definition order  i.e. a concept name is not classified until all the concept names used in its  primitive  definition are classified. in this case the set of children of a concept name to be inserted consists only of the bottom concept. thus  if concept names are classified in definition order  the bottom search phase can safely be omitted for concept names which have only a primitive definition  baader et al.  1 .
　in order to avoid the bottom search phase it is possible to impose a syntactical restriction on tboxes for less expressive dls  i.e. to accept only concept axioms in definition order  i.e. the  primitive  definitions do not include forward references to concept names not yet introduced. however  for an expressive dl such as alcnhr+  which offers cyclic axioms and gcis  in general  the bottom search phase cannot be skipped.
　the umls tboxes contain many forward references occurring in value  e.g.  r.c  and existential restrictions  e.g.  r.c . thus  the definition order of concept names has to be computed in a preprocessing step. as a refinement we devised a slightly less strict notion of definition order which works more efficiently. we assume a relation 'directly refers to' similar to 'directly uses' but with references occurring in the scope of quantifiers not considered. this simplification reduces the overhead caused by computing the 'directly refers to' relation. it is correct since subsumption between concept names with primitive definitions cannot be caused via quantifiers occurring in the concept definitions. again 'refers to' is the transitive closure of 'directly refers to'. the 'refers to' relation induces a partial order relation on sets of concept names. all concept names involved in a cycle become members of one set while the remaining concept names form singleton sets. a topological sorting algorithm is used to serialize the partial order such that a total order between sets of concept names is defined. this serialization is called a quasi definition order.
　during the classification of a tbox with race the sets of concept names are processed in quasi definition order. for each singleton set whose member has a primitive concept definition  the bottom search can be skipped. let a  c be a concept introduction axiom and a is to be inserted into the taxonomy. the 'refers to' relation and the quasi definition order serialization ensure that either all concept names that are potential subsumees of a are inserted after a has been inserted into the subsumption lattice or the bottom search is indeed performed. the quasi definition order is conservative w.r.t. the potential subsumers  note that alcnhr+ does not support inverse roles . moreover  in a basic subsumption test the subsumption lattice under construction is never referred to. thus  strict definition order classification is not necessary. note that in  baader et al.  1  no experiments are discussed that involve the computation of a serialization given a tbox with axioms not already in  strict  definition order. topological sorting is of order n+e where e is the number of given 'refers to' relationships. thus  we have approximately o n logn  steps while the bottom search procedure requires o n1  steps in the worst case.
1	adaptive clustering in tboxes
experiments with the umls tboxes showed that the wellknown techniques described in  baader et al.  1  exhibit considerable performance deficiencies in case of  rather flat  taxonomies where some nodes have a large number of children. therefore  in the race architecture a special clustering technique is employed.
　if the top search phase finds a node  e.g. a  with more than θ children  the θ children are grouped into a bucket  e.g. anew   i.e. a  virtual  concept definition ax-
.
iom anew  is assumed and anew is inserted into the subsumption lattice with {a} being its parents and {a1 ...aθ} being its children. anew is also referred to as a bucket concept. note that bucket concepts  e.g. anew  are considered as virtual in the sense that the external interface of race hides the names of virtual concepts in a transparent way.
　let us assume  a certain concept name b is to be inserted and a node a with its children {a1 ... aθ} is encountered during the top search phase. instead of testing for each child ai  i （ {1..θ}  whether ai subsumes b  our findings suggest that it is more effective to initially test whether the associated virtual concept definition of anew does not subsume b using the pseudo model merging technique  e.g. see  horrocks  1; haarslev et al.  1   which provides a 'cheap' and sound but incomplete non-subsumption test based on pseudo models derived from concept satisfiability tests. since in most cases no subsumption relation can be found between any ai and b  one test possibly replaces θ  expensive  but wasted subsumption tests. on the other hand  if a subsumption relation indeed exists  then clustering introduces some overhead. however  since the umls tboxes mostly contain concept introduction axioms  the pseudo model of  anew being used for model merging is very simple because the pseudo model basically consists only of a set of negated primitive concept names  see  haarslev et al.  1  for further details about pseudo model merging in race . the adaptive clustering is designed in a way that it still works even if a quasi definition order cannot be guaranteed  e.g. due to the presence of defined concepts or gcis . therefore  a bucket becomes obsolete and has to be removed from a concept node if a member of this bucket gets a different concept node as parent.
　for best performance  the number of children to be kept in a bucket should depend on the total number of known children for a concept. however  this can hardly be estimated. therefore  the following adaptive strategy is used. if more and more concept names are  inserted  into the subsumption lattice  the number of buckets increases as well. if a new bucket is to be created for a certain node a and there are already σ buckets clustering the children of a  then two buckets  those buckets with the smallest number of children  of a are merged. for instance  merging of the buckets anew an and bnew bm means that the bucket anew is  re-
.
defined  as anewbm and the bucket bnew is reused for the new bucket to be created  see above .1 whether hierarchical clustering techniques lead to performance improvements is subject to further research.
　the current evaluation of clustering with buckets uses a setting with θ = 1 and σ = 1.
1	dealing with domain and range restrictions
some umls tboxes contain axioms declaring domain and range restrictions for roles. for instance  the domain c of a role r can be determined by the axiom  c and the range d by the axiom . domain restrictions increase the search space during a consistency test since they have to be represented as disjunctions of the form
.
　it is possible to transform a domain restriction of the form c into an equivalent inclusion axiom of the form  r and to absorb the transformed axiom if no inclusion axiom of the form c exists. however  the transformation is not applicable to the umls tboxes since they contain domain restrictions  e.g.  −1 anatomical part of ana heart heart p  as well as inclusion axioms  e.g. ana heart p   ana heart	ana hearts anatomical part of ana heart  violating the above-mentioned precondition. hence  in order to apply the topological sorting optimization  it was necessary to incorporate domain restrictions into an abox consistency test because global axioms may not exist in order to apply the topological sorting technique.
　if gcis representing domain restrictions for roles have been absorbed  they are dealt with by race with a generalized kind of lazy unfolding. whenever a concept of the form  r.d or  −n r is checked for unfolding  it is replaced by c.d or n r if a gci of the formc has been absorbed  r a sub-role of s . this technique is valid sincec can be represented as the global axiom
 c and the unfolding scheme of race guarantees that c is added if an r-successor will be created due to  r.d or  −n r. a role assertion  a b :r is unfolded to { a b :r  a:c}. if lazy unfolding is applied  domain restrictions have to be considered w.r.t. the 'directly refers to' relationship in a special way.
　note that  in principle  race also supports the absorption of gcis of the form  but only if no concept
introduction axiom of the form a and no concept de-
.
finition definition axiom of the form a = c1 for a exists . some knowledge bases can only be handled effectively if the absorption of axioms of the form is supported.
　in contrast to domain restrictions  range restrictions for roles do not introduce new disjunctions. however  it is always advantageous to keep the number of global axioms to be managed as small as possible. therefore  gcis expressing range restrictions for roles are absorbed and concepts of the form  r.c or  −n r are unfolded to.d or
.d if a gci of the form  r a sub-role of s  has been absorbed. a role assertion  a b :r is unfolded to { a b :r  b:d}.
1	exploiting disjointness declarations
as has been discussed in  baader et al.  1   it is important to derive told subsumers1 for each concept name for marking and propagation processes. besides told subsumers  race exploits also the set of  told disjoints1 . in the 'heart' example presented above  ana heart is computed as a told disjoint concept of ana heartp by examining the related concept introduction axioms. if it is known that a concept b is a subsumer of a concept a then a cannot be a subsumee of the told disjoints of b. this kind of information is recorded  and propagated  with appropriate non-subsumer marks  see  baader et al.  1  for details about marking and propagation operations  in order to prune the search space for traversals causing subsumption tests. exploiting disjointness information has not been investigated in  baader et al.  1 .
1	caching policies
race supports different caching policies  see also  haarslev and moller  1a：   for caching in race . two types of caches are provided which can be used together or alternatively. both cache types are accessed via keys constructed from a set of concepts. the first cache  called equal cache  contains entries about the satisfiability status of concept conjunctions already encountered. this cache only returns a hit if the search key exactly matches  i.e. is equal to  the key of a known entry. for instance  the key for a concept conjunction ccn is the  ordered  set {c1 ... cn} of concepts.
　the second cache type consists of a pair of caches. the subset cache contains only entries for satisfiable concept conjunctions while the superset cache stores unsatisfiable concept conjunctions. these caches support queries concerning already encountered supersets and subsets of a given search key  see also  hoffmann and kohler  1; giunchiglia and： tacchella  1  . for instance  if the subset  satisfiability  cache already contains an entry for the key {c1 c1 c1} and is queried with the key {c1 c1}  it returns a hit  i.e. the conjunction c is also satisfiable. analogously  if the superset  unsatisfiability  cache already contains an entry for the key {d1 d1 d1} and is queried with the key {d1 ... d1}  it returns a hit  i.e. the conjunction d is also unsatisfiable. if the equal cache is enabled  it is the first reference  i.e. only if an equal cache lookup fails  the superset or subset caches are consulted.
1	empirical results for umls classifications
the performance of the race system is evaluated with different versions of the umls knowledge bases. umls-1 is a preliminary version whose classification resulted in many unsatisfiable concept names. umls-1 contains approximately 1 concept names and for almost all of them there exists a concept introduction axiom of the form a  c where c not equal to . in addition  in umls-1 1 role names are declared. role names are arranged in a hierarchy.
　umls-1 is a new version in which the reasons for the inconsistencies have been removed. the version of umls-1 we used for our empirical tests uses approximately 1 concept names with associated primitive concept definitions and 1 hierarchical roles.
　originally  the umls knowledge bases have been developed with an incomplete description logic system which does not classify concepts with cyclic definitions  and  in turn  the concepts which use these concepts . due to the treatment of cycles in the original approach  schulz and hahn  1   the cycle-causing concepts are placed in so-called :implies clauses  i.e. these concepts are not considered in concept satisfiability and subsumption tests. for the same reason  the umls reconstruction uses :implies for domain and range restrictions of roles  i.e. domain and range restrictions are also not considered in concept satisfiability and subsumption tests.
　with race  none of these pragmatic distinctions are necessary. however  in order to mimic the original behavior and to test more than one tbox with race  for each of the knowledge base versions  umls-1 and umls-1  three different subversions are generated  indicated with letters a  b and c . version 'a' uses axioms of the style presented above  i.e. the :implies parts are omitted for tbox classification  and coherence checking . in version 'b' the :implies part of the original knowledge base is indeed considered for classification by race. thus  additional axioms of the following form are part of the tbox.
anaheart   has developmental fo.ana fetalheart	
 surrounded by .ana pericardium
　version 'c' is the hardest version. additional axioms express domain and range restrictions for roles. for instance  the following axioms are included in the tbox for anatomical part of ana heart.  −1 anatomical part of ana heart  anaheart p
   anatomical part of ana heart.ana heart
　thus  for the performance evaluation we have tested 1 different knowledge bases. all measurements have been performed on a sun ultrasparc 1 with 1 gbyte main memory and solaris 1. race is implemented in ansi common lisp and for the tests franz allegro common lisp 1.1 has been used. the results are as follows.
　if the generalized unfolding technique for domain and range restrictions is disabled in race  even a small subset  with a size of ゛1%  of the umls tboxes with gcis for domain and range restrictions could not be classified within several hours of runtime. furthermore  the equal cache had to be disabled for all umls tboxes in order to reduce the space requirements during tbox classification.

figure 1: evaluation of the topological sorting and clustering techniques for umls1a-c  see explanation in text .
　without clustering and topological sorting  umls-1a can be classified in approximately 1 hours  1 concept names are recognized as unsatisfiable . with clustering and topological sorting enabled  only 1 hours are necessary to compute the same result for umls-1a. the second version  umls1b  requires 1 hours  with optimization  and 1 hours  without optimization . the reason for the enhanced performance with more constraints is that in this version already 1 concept names are unsatisfiable. with domain and range restrictions we found that even 1 concept names are unsatisfiable. the computation times with race are 1 hours  with optimization  and 1 hours  without optimization . up to 1 mbytes of memory are required to compute the classification results. for umls-1  checking tbox coherence  see above  requires approximately 1 minutes.
　the new second version  umls-1  contains an additional part of the umls ontology and  therefore  is harder to deal with. furthermore  there are no unsatisfiable concept names  i.e. classification is much harder because there are much more nodes in the subsumption lattice. in umls-1  due to the large number of unsatisfiable concepts  the subsumption lattice is rather small because many concept names  disappear  as synonyms of the bottom concept. for umls-1  checking tbox coherence  see above  requires between 1 and 1 minutes  1a: 1 min  1b: 1 min  1c: 1 min .
　the results for classifying the umls-1 tboxes are shown in figure 1. a comparison of setting 1  all optimizations enabled  and setting 1  clustering disabled  reveals that clustering is a very effective optimization technique for the umls1 tboxes. the result for setting 1  topological sorting disabled  and umls-1a/b supports the fact that topological sorting is also very effective. the runtime result for setting 1 and umls-1c is due to removed buckets  see section 1 . this is very likely to happen if topological sorting is disabled and apparently shows a dramatic overhead for umls-1c. if  in setting 1  both clustering and topological sorting are disabled  runtimes increase only to a limited extent. moreover  according to the evaluation results  umls-1b does not require more computational resources than umls-1a  see the discussion about :implies from above . only the incorporation of domain and range restrictions cause runtimes to increase. for the umls-1 tboxes up to 1 mbytes of memory are required. for other benchmark tboxes  e.g. galen  horrocks  1  with approx. 1 concepts  our results indicate an overhead of ゛1% in runtime. this is caused by the presence of many defined concepts and a small average number of concept children in the taxonomy. in summary  the results for the umls tboxes clearly demonstrate that clustering is only effective in conjunction with topological sorting establishing a quasi-definition order.
1	conclusion
in this paper enhanced optimization techniques which are essential for an initiative towards sound and complete high performance knowledge base classification are presented. thus  fast classification of very large terminologies which mostly consist of concept introduction axioms now has become possible with description logic systems based on sound and complete algorithms.
　a final comment concerning the significance of the umls knowledge bases used for the empirical evaluation is appropriate. even though the umls knowledge bases do not make use of defined concepts or arbitrary gcis  a large number of concept introduction axioms and a possibly large role hierarchy can be called the standard case in many practical applications. furthermore  some umls tboxes  version 'c'  demand the absorption of gcis expressing domain and range restrictions for roles. without this new technique even a very small subset of these tboxes cannot be classified within a reasonable amount of time. if description logics are to be successful in large-scale practical applications  being able to deal with very large knowledge bases such as those based on the umls is mandatory.
　even with the above-mentioned optimization techniques  dealing with 1 concept names is by no means a trivial task. large knowledge bases reveal even slightly less than optimal algorithms used for specific subproblems. thus  experiments with very large knowledge bases also provide feedback concerning lurking performance bottlenecks not becoming apparent when dealing with smaller knowledge bases. to the best of our knowledge  race is the only dl system based on sound and complete algorithms that can deal with this kind of very large knowledge bases. we would like to thank stefan schulz for making the umls reconstruction available.
references
 baader et al.  1  f. baader  e. franconi  b. hollunder  b. nebel  and h.j. profitlich. an empirical analysis of optimization techniques for terminological representation systems or: making kris get a move on. applied artificial intelligence. special issue on knowledge base management  1-1  1.
 cohn et al.  1  a.g. cohn  f. giunchiglia  and b. selman  editors. proceedings of seventh international conference on principles of knowledge representation and reasoning  kr'1   breckenridge  colorado  usa  april 1  1  april 1.
 giunchiglia and tacchella  1  e. giunchiglia and a. tacchella. a subset-matching size-bounded cache for satisfiability of modal logics. in proceedings international conference tableaux'1  pages 1. springer-verlag  1.
 haarslev and moller  1a：   v. haarslev and r. moller.： consistency testing: the race experience. in r. dyckhoff  editor  proceedings  automated reasoning with analytic tableaux and related methods  university of st andrews  scotland  1 july  1  pages 1. springerverlag  berlin  april 1.
 haarslev and moller  1b：   v. haarslev and r. moller.： expressive abox reasoning with number restrictions  role hierarchies  and transitively closed roles. in cohn et al.   pages 1.
 haarslev et al.  1  v. haarslev  r. moller  and a.-y.： turhan. exploiting pseudo models for tbox and abox reasoning in expressive description logics. in proceedings of the international joint conference on automated reasoning  ijcar'1  june 1  1  siena  italy  lncs. springer-verlag  berlin  june 1.
 hahn et al.  1  u. hahn  s. schulz  and m. romacker. part-whole reasoning: a case study in medical ontology engineering. ieee intelligent systems  1 :1  september 1.
 hoffmann and kohler  1：   j. hoffmann and j. kohler. a： new method to query and index sets. in proceedings of the sixteenth international joint conference on artificial intelligence ijcai-1  pages 1. morgan-kaufmann publishers  july 1 - august 1  1.
 horrocks and tobies  1  i. horrocks and s. tobies. reasoning with axioms: theory and practice. in cohn et al.   pages 1.
 horrocks et al.  1  i. horrocks  u. sattler  and s. tobies.
practical reasoning for expressive description logics. in proceedings of the 1th international conference on logic for programming and automated reasoning  lpar'1   pages 1. springer-verlag  berlin  september 1.
 horrocks  1  i. horrocks. optimising tableaux decision procedures for description logics. phd thesis  university of manchester  1.
 mccray and nelson  1  a.t. mccray and s.j. nelson. the representation of meaning in the umls. methods of information in medicine  1/1 :1  1.
 schulz and hahn  1  s. schulz and u. hahn. knowledge engineering by large-scale knowledge reuse - experience from the medical domain. in cohn et al.   pages 1.

knowledge representation
and reasoning
complexity analysis

complexity of nested circumscription and abnormality theories

marco cadoli
dipartimento di informatica e sistemistica
universita` di roma  la sapienza 
via salaria 1  i-1 roma  italy
cadoli dis.uniroma1.it
thomas eiter	georg gottlob institut fu：r informationssysteme
technische universita：t wien
favoritenstrasse 1  a-1 wien  austria eiter kr.tuwien.ac.at

abstract
we propose   an extension of circumscription by allowing propositional combinations and nesting of circumscriptive theories. as shown  lifschitz's nested abnormality theories  nats  introduced in aij  1  are naturally embedded into this language. we analyze the complexity of and nats  and in particular the effect of nesting. the latter is found a source of complexity  as both formalisms are proved to be pspace-complete. we identify cases of lower complexity  including a tractable case. our results give insight into the  cost  of using resp. nats as a host language for expressing other formalisms  such as narratives.
1	introduction
circumscription is a very powerful method for knowledge representation and common-sense reasoning  which has been used for a variety of tasks  including temporal reasoning  diagnosis  and reasoning in inheritance networks. the basic semantical notion underlying circumscription is minimization of the extension of selected predicates. this is especially useful when a predicate is meant to represent an abnormality condition  e.g.  a bird which does not fly. circumscription is applied to a formula   either propositional or first-order  and it is used to eliminate some unintended models of .
　since the seminal definition of circumscription in  mccarthy  1   several extensions have been proposed  see  e.g.  lifschitz's survey    all of them retaining the basic idea of minimization. in this paper  we propose   a language which extends propositional circumscription in two important and rather natural ways:
1. on one hand  we allow the propositional combination of circumscriptive theories;
1. on the other hand  we allow nesting of circumscriptions.
　as for the former extension  we claim that it can be useful in several cases. as an example  when different sources of knowledge and coming from two equally trustable agents who perform circumscription are to be integrated  it seems plausible to take their disjunction  i.e. 
gottlob dbai.tuwien.ac.at
	1. in	  all propositional connec-
tives are allowed.
　as for the latter extension  the original idea of nested abnormality theories  nats  has been proposed by lifschitz   and its usefulness for the formalization of narratives has been shown by baral et al. . nesting circumscription is useful for the modularization of a knowledge base. as an example concerning diagnosis of an artifact designed in a modular way  e.g.  a car  a piece of knowledge may model the intended behavior of a subpart  e.g.  the engine. analogously  may model the intended behavior of the electrical part. further unintended models can be eliminated by taking the circumscription of a suitable propositional combination of     and a plain propositional formula encoding observations.
　in this paper  we are mainly concerned with the computational properties of and nats and with their relationship to plain circumscription. in particular  we tackle the following questions:
	can nats be embedded into	 
what is the precise complexity of reasoning under nested circumscription  by reasoning  we mean both model checking and formula inference from an or nat theory.
is there a simple restriction of nats  and thus of   for which some relevant reasoning tasks are feasible in polynomial time 
　we are able to give a satisfactory answer to all these questions. in particular  in section 1  after providing a precise definition of   we prove the main results about its complexity: model checking and inference are shown to be pspacecomplete  the latter even for literals ; moreover  complexity is proven to increase w.r.t. the nesting. it appears that nesting  and not propositional combination  is responsible for the increase in complexity. similar results are proven for nats in section 1  where we also prove that every nat can be easily  and polynomially  translated into a formula of .
　given the high complexity of nested circumscription  we look for significant fragments of the languages in which complexity is lower  and focus on horn nats: it is proven in section 1 that here nesting can be efficiently eliminated if fixed variables are not allowed  and both model checking and inference are polynomial. section 1 compares and nats to other generalizations of circumscription such as prioritized circumscription  lifschitz  1; 1  and theory curbing  eiter et al.  1; eiter and gottlob  1 .
　our results prove that the expressive power that makes and nats useful tools for the modularization of knowledge has indeed a cost  because the complexity of reasoning in such languages is higher than reasoning in a  flat  circumscriptive knowledge base. anyway the pspace upper bound of the complexity of reasoning  and the similarity of their semantics with that of quantified boolean formulas  qbfs   makes fast implementations possible by translating them into a qbf and then using one of the several available solvers  e.g.   rintanen  1 . this approach could be used also for implementing meaningful fragments of nats  such as the one in  baral et al.  1   although this might be inefficient  like using a first-order theorem prover for propositional logic.
　given that qbfs can be polynomially encoded into nats  we can show  in the full paper  that nested circumscription is more succinct than plain  unnested  circumscription  i.e.  by nesting operators  or nats   we can express some circumscriptive theories in polynomial space  while they could be written in exponential space only  if nesting were not allowed. in this sense  we add new results to the comparative linguistics of knowledge representation  gogic et al.  1 .
1	preliminaries
we assume a finite set	of propositional atoms  and let
　　　 for short    if does not matter or is clear from the context  be a standard propositional language over .
for any formula   we denote by the set of its models. capitals     etc stand for ordered sets of atoms  which we also view as lists. if
and	  then	denotes the formula
.
　the extension of a model on atoms is denoted by . we denote by the preference relation on models which minimizes in parallel while is varying and all other atoms are fixed; i.e.    is more or equally preferable to   iff and
	  where	and	and
are taken componentwise. as usual 	means
.
　we denote by the second-order circumscription  lifschitz  1  of the formula where the atoms in are minimized  the atoms in float  and all other atoms are fixed  which is defined as the following formula:
 1 
 here and are lists of fresh atoms  not occurring in   corresponding to and   respectively. the second-order formula  1  is a quantified boolean formula  qbf  with free variables  whose semantics is defined in the standard way. its models  i.e.  assignments to the free variables such that the resulting sentence is valid  are the models of which are -minimal  where a model of is -minimal  if no model of exists such that .
1	language
the language extends the standard propositional language  over a set of atoms   by circumscriptive atoms. formulas of are inductively built as follows:
1.   for every	;
1. if	 	are in	  then	and	are in	;
1. if and are disjoint lists of atoms  then is in  called circumscriptive atom .
further boolean connectives       etc  are defined as usual. the semantics of any formula from is given in terms of models of a naturally associated qbf   which is inductively defined as follows:
1.   if	is an atom	;
1. and	;
1.
.
　note that in 1  the second-order definition of circumscription is used to map the circumscriptive atom to a qbf which generalizes the circumscription formula in  1 . in particular  if is an  ordinary  propositional formula      then coincides with the formula in  1 .
example 1 consider the formula
applying rule 1 to inner circumscriptions we get
　　　　　　　　　　　　　. applying rule 1 again we get	.
as usual  we write if is a model of   and if is a logical consequence of .
eliminating fixed letters
as shown in  de kleer and konolige  1   the fixed letters can be removed from an ordinary circumscription. the same technique can be applied for formulas from as well. more precisely  let be a circumscriptive atom. for each letter   introduce a fresh letter   add   to   and add a conjunct to . let
be the resulting circumscriptive atom. then 
the following holds.
proposition 1 and are equivalent modulo the set of all auxiliary letters .
　using this equivalence  we can eliminate all fixed letters from a formula   by replacing each circumscriptive atom in with   where the fresh atoms are made floating inside and outside . note that the resulting formula
	has size polynomial in the size of	.
1	complexity results
let the	-nesting depth  for short  nesting depth  of
       denoted   be the maximum number of circumscriptive atoms along any path in the formula tree of .
theorem 1 model checking for   i.e.  deciding whether a given interpretation is a model of a given formula
	  is pspace-complete. if	for a constant
       then the problem is complete for   i.e.  polynomial time with many calls to an oracle for   where is the input size.
　proof:  sketch  clearly  any formula can be encoded  by definition  to a qbf  not necessarily in prenex form  in polynomial time. thus deciding  is in pspace. by an inductive argument  we can see that for any
circumscriptive atom	such that
  deciding	is in	. thus for any boolean com-
bination	of atoms and circumscriptive atoms
such that	  deciding
is possible in   i.e.  polynomial time with one round of parallel oracle calls. since   this proves the membership part for .
　pspace-hardness of for general follows directly from the pspace-hardness of model checking for nat theories shown below in theorem 1  and from the fact that nats can be polynomial-time embedded into  corollary 1 . similarly  from the respective results on nats we obtain that for circumscriptive atoms with the problem is -hard  and thus -complete .
	the	-hardness part for the case where
is a boolean combination of formulas
such that is then shown by a reduction from deciding  given instances  ... 
of the model checking problem for circumscriptive atoms  whether the number of yes-instances among them is even. the -completeness of this problem is an instance of wagner's  general result for all -complete problems. moreover  we may use that all instances are on disjoint alphabets and the assertion  wagner  1  that
is a yes-instance only if	is a yes-instance  for
all	. then  we can define
where	is a fresh letter. the interpretation
   is a model of if and only if the number of yes-instances among  ...  is even. clearly  and can be constructed in polynomial time.
theorem 1 deciding  given formulas	whether is pspace-complete. hardness holds even if	.
if the nesting depth of and is bounded by the constant   then the problem is -complete.
　proof: the problem is in pspace  resp.   : an interpretation such that can be guessed and verified in polynomial space  resp.    thus in polynomial time with an oracle for  . hence the problem is in npspace = pspace  resp.   . hardness follows from the polynomial time embedding of nats into  corollary 1  and theorem 1.
corollary 1 deciding satisfiability of a given formula is pspace-complete. if the nesting depth is bounded by a constant   then the problem is -complete.
1	nested abnormality theories  nats 
lifschitz  proposed the hierarchical use of the circumscription principle  applied to building blocks of a more complex theory. this method was used  e.g.  in  baral et al.  1  for defining the formal semantics of narratives.
　we assume that the atoms include a set of distinguished atoms  which intuitively represent abnormality properties . blocks are defined as the smallest set such that if are distinct atoms not in   and each of is either a formula in or a block  then
is a block  lifschitz  1   where are called described by this block. the nesting depth of   denoted
  is 1 if every	is from	  and otherwise.
　a nested abnormality theory  nat   lifschitz  1  is a collection of blocks; its nesting depth 
	  is defined as	.
	the semantics of a nat	is defined by a mapping
to a qbf as follows:	  where for any
block	 
given that	if	is a formula in	.
	thus  a standard circumscription	  where
	  is equivalent to a nat	where
is viewed as the set of abnormality letters	; notice that
.
we note the following useful proposition.
proposition 1 let	be any nat. let
	where	is any subset of the atoms  dis-
joint with	 . then 	and	have the same models.
　indeed  has void minimization of  making each in false   and fixed and floating letters can have any values.
embedding nats into
in the translation   the minimized letters are under an existential quantifier  and thus  projected  from the models of the formula . we can  modulo auxiliary letters  eliminate existential quantifiers as follows:
1. rename every quantifier in such that every quantified variable is different from any other variable.
1. in every circumscriptive subformula of the renamed formula  add to the floating atoms all variables which are quantified in  including subformulas . 1. drop all quantifiers.
let be the resulting formula. note that its size is polynomial  quadratic  in the size of .
example 1 let	and	  where	. then 
where
in step 1  we rename and in to and   respectively  and add in step 1 to the floating letter of . after dropping quantifiers in step 1  we obtain:
let	be the set of atoms from step 1 of the embedding
     . the following result  which can be proven by induction on the nesting depth of   states the correctness of .
proposition 1 for any nat   the formulas and are equivalent modulo .
corollary 1 modulo auxiliary letters  nat is  semantically  a fragment of   and polynomial-time embedded via .
notice that is not possible to add in step 1 of the embedding the quantified variables in to the fixed atoms.
example 1 reconsider the nat	in ex. 1. note that	is the unique model of	. the formula	has  if we disregard	 which are fixed in it   the models and	. they give rise to the two models and	of	  of which	is	-minimal.
　however  if   were fixed in   then both and would be models of   as they are minimal. therefore  proposition 1 would fail.
eliminating fixed letters
every fixed letter can be removed from a nat similarly as from a formula . however  we must take into account that a fixed letter may not be simply declared as a minimized letter in the rewriting  since there is a special set of minimized letter which has restricted uses. we surpass this by introducing two special abnormality letters and

	in	  and by adding the formula

　　　as a new block to   a well as in every other block of . the letter is declared floating in .
1	complexity of nats
ordinary circumscription can express a qbf sentence  where   as follows. let be a fresh variable.
lemma 1  cf.  eiter and gottlob  1   is true if and only if   where .
this circumscription can be easily stated as a nat. set
then lemma 1 implies that iff is true. in fact  denote for any interpretation and set of atoms by the assignment to as given by . then  every model of must be  if we fix the atoms in to   a model of
such that	if and only if	is unsatisfiable.
　starting from this result  we prove pspace-hardness of inference from a nat theory . the basic technique is to introduce further variables as parameters into the formula from lemma 1  which are kept fixed at the inner levels. at a new outermost level to be added  the letter is used for evaluating the formula at a certain level. we must in alternation minimize and maximize the value of .
	consider the case of a qbf	  where	are
free variables in it  considered as  parameters . we nest into the following theory :
this amounts to the following circumscription:
the outer circumscription minimizes and thus maximizes . is  by proposition 1  equivalent to the formula
modulo the atoms	and	. the following can be shown:
lemma 1	if and only if for every assignment
　　  the qbf is true  i.e.  is false .
it follows from the lemma that deciding  given a nat
of nesting depth 1 and	  whether	is	-hard.
　we generalize this pattern to encode the evaluation of a qbf
 1 
where the quantifiers alternate  into inference from a nat theory as follows.
	let	  where	is a fresh atom. define inductively
for all   and let . note that is equivalent to   and that   for all while . we obtain the following.
lemma 1 for every   has some model  and if is odd  then iff is false  i.e.  is true; if is even  then iff is true.
	this statement can be proved by induction on	.
lemma 1 model checking for nats  i.e.  deciding whether a given interpretation is a model of a given nat   is in
pspace.if	for constant	  then it is in	.
　proof: for   the problem amounts to model checking for ordinary circumscription  which is in . for   we can test recursively for each block of whether is a model of each block in   and that no model of exists such that
　. the recursion depth is linear  and the procedure runs in quadratic space. moreover if is bounded by a constant   then the check for and the existence of witnessing that is not model of   can be done in nondeterministic polynomial time with an oracle for . thus  the problem is in pspace  resp.  in  .
　the construction in lemma 1 shows how it is possible to polynomially embed qbf evaluation into inference wrt a nat. in turn  proposition 1 shows that a nat can be polynomially embedded into an formula. the following theorem highlights the consequences of such relations on complexity of inference wrt a nat.
theorem 1 deciding  given a nat theory and a propositional formula   whether is pspace-complete. if for constant   then it is -complete.
　proof: the hardness part follows from lemma 1 above. as for the membership part  a model of such that can be guessed and verified in pspace  resp.  with the help of a -oracle . thus the problem is in co-npspace = pspace  resp.   .
　the next theorem shows that the upper bounds on model checking for nats have matching lower bounds.  this is expected from theorem 1: if model checking were in the polynomial hierarchy  ph   then also inference would be in ph. 
theorem 1 given a nat theory and an interpretation   deciding whether is pspace-complete. if
for a constant	  then the problem is	-complete.
　proof:  sketch  by lemma 1  it remains to show the hardness parts. note that for   this is immediate from results in  eiter and gottlob  1 . we can slightly adapt the above encoding of qbfs into nats. given a qbf as in eq.  1   let the nats   ...  be as there  with the only difference that is replaced by
where if is odd and otherwise. here is a new letter  defined  i.e.  floating  in and fixed elsewhere; resp. is viewed as conjunction of its formulas. let be the resulting nats.
	define	if	is odd and
	if	is even. note that	and	is 
modulo   the only candidate for a model of in which and are true: any such model must satisfy   and if are true  then all other atoms have a unique truth value.
	it holds that	is a model of	iff the qbf	is false
 resp.  true . it follows that model checking is pspacehard and -hard if for constant ; note that
.
1	a polynomial-time fragment
we call a block horn  if each is a horn cnf if   and recursively is horn otherwise. a nat is horn  if each of its blocks is horn. in  cadoli and lenzerini  1   it was shown that deciding
                   where is a propositional horn cnf and is an atom  is -complete. that is  already for horn nats without nesting  i.e.    inference is intractable  and nesting may further add on complexity.
horn nats without fixed letters
the technique of removing fixed letters from a horn nat does not work  since it uses non-horn clauses. thus  horn nats without fixed letters do not immediately inherit complexity  in particular intractability  from general horn nats.
　in fact  for this class we obtain positive results. note that here still minimization of atoms is possible via auxiliary atoms and horn axioms   in .
call any -minimal model of a nat such that and a minimal model of .
theorem 1 let be a horn nat without fixed letters. then   1  has a least  i.e.  a unique minimal  model  and  1  is equivalent to a horn cnf . furthermore  both and are computable in polynomial time.
	proof:  sketch  let  for any horn cnf	and model
  be	the horn cnf resulting from	after substitutingeach   by  true  if and by  false  if and subsequent standard simplifications.letbe a single block	  where
	. define the horn cnf	recursively bywhereis the least model of the horn cnfand
then  by induction on   we can show that  1  is the least model of   and  1  is equivalent to . let  for any formula   block   nat etc denote  
　  etc the representation size of the respective object. obviously  we can compute inductively. for the
horn cnfs	and	  we have	and
　　　　　　　. of the model	  we only need its restriction	to the atoms	occurring in	 all other atoms are irrelevant for computing	 .	we can compute	from	in	time  recall that the least model of a horn cnf	is computable in	time   and from	in	time. overall  it follows that for	  we can compute both	and its least model	in time	  where	is the number of  recursive  blocks in	  thus in polynomial time.
by prop. 1  we can replace a multiple block
   by	  which is horn and without fixed letters  and obtain analogous results.
　using sophisticated data structures  the  relevant parts of  models above can be computed incrementally  where each clause in is fired at most once. overall  and are computable in time. we thus have:
theorem 1  flat normal form  every horn nat without fixed letters can be rewritten to an equivalent horn nat without fixed letters  where is a horn cnf  in time  i.e.  in linear time .
　thus  nesting in horn nats without fixed letters does not increase the expressiveness  and can be efficiently eliminated. we note some easy corollaries of the previous result.
corollary 1 deciding satisfiability of a given horn nat without fixed letters is polynomial.
corollary 1 model checking for a given horn nat without fixed letters and model is polynomial.
for the inference problem  we obtain the following result.
theorem 1 given a horn nat	without fixed letters and
　　　  deciding is -complete. if is a cnf  then the problem is polynomial.
　proof: since model checking is polynomial  the problem is clearly in . the -hardness part follows from
-completeness of validity checking for a given
 ask whether	  where	contains all letters .
　we can reduce to in time  where is a horn cnf. if is a cnf of
clauses	  the latter can be checked in
time  thus in time  check   which is time  for all  .
1	other generalizations of circumscription
as we have discussed above  fixed letters can be removed from and nat theories  respectively. using these techniques  the hardness results of sections 1 and 1 can be sharpened to theories without fixed letters.
prioritized circumscription generalizes circumscription by partitioning the letters into priority levels
  and informally pruning all models of
which are not minimal on   while floats and is fixed  for  cf.  lifschitz 
1  . this can be readily expressed as the nested circumscription
　　　　　　　. this generalization differs from and nats: the complexity does not increase  as inference and model checking remain -complete and -complete  respectively. intuitively  the reason is that prioritization allows only for a restricted change of the role of the same letter in iterations  floating to minimized and minimized to fixed  and forbids reconsidering minimized letters.
curbing is yet another extension of circumscription  eiter et al.  1; eiter and gottlob  1 . rather than the  hierarchical  use of circumscription applied to blocks  curbing aims at softening minimization  and allows for inclusive interpretation of disjunction where ordinary circumscription returns exclusive disjunction. semantically  for a formula is the smallest set which contains all models of and is closed under minimal upper bounds in . a minimal upper bound  mub  of a set of models in is a model such that  1    for every   and  1  there exists no satisfying item 1 such that .
　for example  has two minimal models  if all letters are minimized    : and . the model is not a minimal model of   but is a curb model of . thus  . on the other hand  if
　　　　　　  then	is different from ; the unsupported letter is set to false. note that is different from .
　like for and nats  inference and model checking for are pspace-complete  eiter and gottlob  1 . however  in the first-order case  the formalisms have different expressiveness. let us consider theories in a function-free language under domain closure  dc  and the unique names assumption  una   i.e.  the  finite   datalog  case. in this case  curbing can express some problems which and nats can not. we can  e.g.  write a  fixed  interpreter in this language for curbing varying propositional 1cnf formulas   input as ground facts   such that the curb models of and are in 1 correspondence. notice that curbing such 1cnfs is pspace-complete  and thus  by well-known results in complexity  this is not expressible by a fixed or nat theory  unless ph=pspace .
　further relationships between resp. nats and curbing  as well as other expressive kr formalisms  baral et al.  1; pollett and remmel  1   remain to be explored.
acknowledgments
this work was supported by the austrian science fund  fwf  project n. z1-inf.
references
 baral et al.  1  c. baral  a. gabaldon  and a. provetti. formalizing narratives using nested circumscription. artificial intelligence  1-1 :1  1.
 baral et al.  1  c. baral  v. kreinovich  and r. trejo. computational complexity of planning and approximate planning in the presence of incompleteness. artificial intelligence  1/1 :1  1.
 cadoli and lenzerini  1  m. cadoli and m. lenzerini. the complexity of propositional closed world reasoning and circumscription. jcss  1-1  1.
 de kleer and konolige  1  j. de kleer and k. konolige. eliminating the fixed predicates from a circumscription. artificial intelligence  1-1  1.
 eiter and gottlob  1  t. eiter and g. gottlob. propositional circumscription and extended closed world reasoning are -complete. theoretical computer science  1 :1  1.
 eiter and gottlob  1  t. eiter and g. gottlob. on the complexity of theory curbing. in proc. lpar-1  lncs 1  1. 1.
 eiter et al.  1  t. eiter  g. gottlob  and y. gurevich. curb your theory! a circumscriptive approach for inclusive interpretation of disjunctive information. in proc. ijcai-1  1. morgan kaufman  1.
 gogic et al.  1  g. gogic  h.a. kautz  ch.h. papadimitriou  and b. selman. the comparative linguistics of knowledge representation. in proc. ijcai-1  1.
 lifschitz  1  v. lifschitz. computing circumscription. in proc. ijcai-1  1  1.
 lifschitz  1  v. lifschitz. circumscription. in d.m. gabbay  c.j. hogger  and j.a. robinson  eds  handbook of logic in ai and logic programming  iii  1. 1.
 lifschitz  1  v. lifschitz. nested abnormality theories. artificial intelligence  1 :1  1.
 mccarthy  1  j. mccarthy. circumscription - a form of non-monotonic reasoning. artif. intell.  1-1  1.
 pollett and remmel  1  c. pollett and j. remmel. nonmonotonic reasoning with quantified boolean constraints. in proc. lpnmr'1  1  lncs 1. springer  1.
 rintanen  1  j. rintanen. improvements to the evaluation of quantified boolean formulae. in proc. ijcai '1  1. aaai press  1.
 wagner  1  k. wagner. bounded query classes. siam journal of computing  1 :1  1.
a perspective on knowledge compilationadnan darwiche
computer science department university of california
los angeles  ca 1  usa darwiche cs.ucla.edu
pierre marquis
cril/universite＞ d'artois
f-1  lens cedex  france marquis cril.univ-artois.fr

abstract
we provide a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilationlanguage  and the class of queries and transformations that the language supports in polytime. we argue that such analysis is necessary for placing new compilation approaches within the context of existing ones. we also go beyond classical  flat target compilation languages based on cnf and dnf  and consider a richer  nested class based on directed acyclic graphs  which we show to include a relatively large number of target compilation languages.
1	introduction
knowledge compilation has emerged recently as a key direction of research for dealing with the computational intractability of general propositional reasoning  darwiche  1a; cadoli and donini  1; boufkhad et al.  1; khardon and roth  1; selman and kautz  1; schrag  1; marquis  1; del val  1; dechter and rish  1; reiter and de kleer  1 . according to this direction  a propositional theory is compiled off-line into a target language  which is then used on-line to answer a large number of queries in polytime. the key motivation behind knowledge compilation is to push as much of the computational overhead into the off-line phase  which is amortized over all on-line queries. but knowledge compilation can serve other important purposes as well. for example  target compilation languages and their associated algorithms can be very simple  allowing one to develop on-line reasoning systems for simple software and hardware platforms. moreover  the simplicity of algorithms that operate on compiled languages help in streamlining the effort of algorithmic design into a single task: that of generating the smallest compiled representations possible  as that turns out to be the main computational bottleneck in compilation approaches.
　there are three key aspects of any knowledge compilation approach: the succinctness of the target language into which the propositional theory is compiled; the class of queries that can be answered in polytime based on the compiled representation; and the class of transformations that can be applied to the representation in polytime. the ai literature has thus far focused mostly on target compilation languages which are variations on dnf and cnf formulas  such as horn theories and prime implicates. moreover  it has focused mostly on clausal entailment queries  with very little discussion of tractable transformations on compiled theories.
　the goal of this paper is to provide a broad perspective on knowledge compilation by considering a relatively large number of target compilation languages and analyzing them according to their succinctness and the class of queries/transformations that they admit in polytime.
　instead of focusing on classical  flat target compilation languages based on cnf and dnf  we consider a richer  nested class based on representing propositional sentences using directed acyclic graphs  which we refer to as nnf. we identify a number of target compilation languages that have been presented in the ai  formal verification  and computer science literature and show that they are special cases of nnf. for each such class  we list the extra conditions that need to be imposed on nnf to obtain the specific class  and then identify the set of queries and transformations that the class supports in polytime. we also provide cross-rankings of the different subsets of nnf  according to their succinctness and the polytime operations they support.
　the main contributionof this paper is then a map for deciding the target compilation language that is most suitable for a particular application. specifically  we propose that one starts by identifying the set of queries and transformations needed for their given application  and then choosing the most succinct language that supports these operations in polytime.
　this paper is structured as follows. we start by formally defining the nnf language in section 1  where we list a number of conditions on nnf that give rise to a variety of target compilation languages. we then study the succinctness of these languages in section 1 and provide a cross-ranking that compares them according to this measure. we consider a number of queries and their applications in section 1 and compare the different target compilation languages according to their tractability with respect to these queries. section 1 is then dedicated to a class of transformations  their applications  and their tractability with respect to the different target compilation languages. we finally close in section 1 by some concluding remarks. proofs of theorems are omitted for space limitations but can be found in  darwiche and marquis  1 .
1	the nnf language
we consider more than a dozen languages in this paper  all of which are subsets of the nnf language  which is defined formally as follows  darwiche  1a; 1b .
definition 1 let ps be a finite set of prop. variables. a sentence in nnf is a rooted  directed acyclic graph  dag  where each leaf node is labeled with     or  
       ; and each internal node is labeled with or and can have arbitrarily many children. the size of a sentence in nnf is the number of its dag edges. its height is the maximum number of edges from the root to some leaf in the dag.
　figure 1 depicts a sentence in nnf  which represents the odd parity function  we omit reference to variables when no confusion is anticipated . any propositional sentence can be represented as a sentence in nnf  so the nnf language is complete.
　it is important here to distinguish between a representation language and a target compilation language. the former should be natural enough to enable one to encode knowledge directly  while the latter should be tractable enough to permit some polytime queries and/or transformations. we will consider a number of target compilation languages that do not qualify as representation languages from this perspective  as they are not suitable for humans to construct or interpret. we will also consider a number of representation languages that do not qualify as target compilation languages.1
　for a language to qualify as a target compilation language  we require that it permits a polytime clausal entailment test. therefore  nnf does not qualify as a target compilation language  unless p=np   papadimitriou  1   but many of its subsets do. we define a number of these subsets below  each of which is obtained by imposing further conditions on nnf.
　we will distinguish between two key subsets of nnf: flat and nested subsets. we first consider flat subsets  which result from imposing combinations of the following properties:
flatness: the height of each sentence is at most 1. the sentence in figure 1 is flat  but the one in figure 1 is not.
simple-disjunction: the children of each or-node are leaves that share no variables  the node is a clause .
simple-conjunction: the children of each and-nodeare leaves that share no variables  the node is a term . the sentence in figure 1 satisfies this property.
definition 1 the language f-nnf is the subset of nnf satisfying flatness. the language cnf is the subset of fnnf satisfying simple-disjunction. the language dnf is the subset of f-nnf satisfying simple-conjunction.
cnf does not permit a polytime clausal entailment test and  hence  does not qualify as a target compilation language  unless p=np . but its dnf dual does.
　the following subset of cnf  prime implicates  has been quite influential in computer science:
definition 1 the language pi is the subset of cnf in which each clause entailed by the sentence is subsumed by a clause that appears in the sentence; and no clause in the sentence is subsumed by another.
a dual of pi  prime implicants ip  can also be defined.1
definition 1 the language ip is the subset of dnf in which each term entailing the sentence subsumes some term that appears in the sentence; and no term in the sentence is subsumed by another term.
　we now consider nested subsets of the nnf language  which do not impose any restriction on the height of a sentence. instead  these subsets result from imposing one or more of the following conditions: decomposability  determinism  smoothness  decision  and ordering. we start by defining the first three properties. from here on  if is a node in an nnf  then denotes the set of all variables that label the descendants of node .
decomposability  darwiche  1a; 1b . an nnf satisfies this property if for each conjunction in the nnf  the conjuncts of do not share variables. that is  if are the children of and-node   then for . consider the and-
node marked in figure 1 a . this node has two children  the first contains variables while the second contains variables . this and-node is then decomposable since the two children do not share variables. each other and-node in figure 1 a  is also decomposable and  hence  the nnf in this figure is decomposable.
determinism  darwiche  1 : an nnf satisfies this property if for each disjunction in the nnf  each two disjuncts of are logically contradictory. that is  if are the children of or-node   then for . consider the or-node marked in figure 1 b   which has two children corresponding to sub-sentences and . these two sub-sentences are logically contradictory. the ornode is then deterministic and so are the other or-nodes in figure 1 b . hence  the nnf in this figure is deterministic.
smoothness  darwiche  1 : an nnf satisfies this property if for each disjunction in the nnf  each disjunct of mentions the same variables. that is  if are the children of or-node   then for . consider the marked
or-node in figure 1 c . this node has two children  each of which mentions variables . this or-node is then smooth and so are the other or-nodes in figure 1 c . hence  the nnf in this figure is smooth.1
　the properties of decomposability  determinism and smoothness lead to a number of interesting subsets of nnf.

figure 1: a sentence in nnf. its size is 1 and height is 1.
figure 1: on the left  a sentence in the bdd language. on the right  its corresponding binary decision diagram.
definition 1 the language dnnf is the subset of nnf satisfying decomposability; d-nnf is the subset satisfying determinism; s-nnf is the subset satisfying smoothness; ddnnf is the subset satisfying decomposability and determinism; and sd-dnnf is the subset satisfying decomposability  determinism and smoothness.
　note that dnf is a strict subset of dnnf  darwiche  1a; 1b . the following decision property comes from the literature on binary decision diagrams  bryant  1 .
definition 1  decision  a decision node in an nnf sentence is one which is labeled with     or is an or-node having the form   where is a variable  and are decision nodes. in the latter case  de-
notes the variable	.
definition 1 the languagebdd is the subset of nnf  where the root of each sentence is a decision node.
the nnf sentence in figure 1 belongs to the bdd subset.
　the bdd language corresponds to binary decision diagrams  bdds   as known in the formal verification literature  bryant  1 . binary decision diagrams are depicted using a more compact notation though: the labels and are denoted by and   respectively; and each decision node
or

x α  x β is denoted by αβ. the bdd sentence on the left of figure 1 corresponds to the binary decision diagram on the right of figure 1. obviously enough  every nnf sentence that satisfies the decision property is also deterministic. therefore  bdd is a subset of d-nnf.

figure 1: the set of dag-based languages considered in this paper. an edge means that is a subset of . next to each subset  we list the polytime queries supported by the subset but not by any of its ancestors  see section 1 .
　as we show later  bdd does not qualify as a target compilation language  unless p=np   but the following subset does.
definition 1 fbdd is the intersection of dnnf and bdd.
that is  each sentence in fbdd is decomposable and satisfies the decision property. the fbdd language corresponds to free binary decision diagrams  as known in formal verification  gergov and meinel  1 . an fbdd is usually defined as a bdd that satisfies the read-once property: on each path from the root to a leaf  a variable can appear at most once. read-once is equivalent to the decomposability of bdds.1
　a more influential subset of the bdd language is obtained by imposing the ordering property:
definition 1  ordering  let be a total ordering on the variables ps. the languageobdd is the subset of fbdd satisfying the following property: if and are or-nodes  and if is an ancestor of node   then .
the obdd language corresponds to the well-known ordered binary decision diagrams  bryant  1 .
　the languageswe have discussed so far are depicted in figure 1  where arrows denote set inclusion. the figure contains one additional language that we have not discussed yet:

figure 1: a sentence in language mods.
definition 1 mods is the intersection of sd-dnnf and dnf.
figure 1 depicts a sentence in mods. as we show later  mods is the most tractable nnf subset we shall consider  together with obdd  . this is not surprising since from the syntax of a sentence in mods  one can immediately recover the sentence models.
1	on the succinctness of compiled theories
we have discussed more than a dozen subsets of the nnf language. some of these subsets are well known and have been studied extensivelyin the computerscience literature. others  such as dnnf  darwiche  1b; 1a  and d-dnnf  darwiche  1   are relatively new. the question now is: what subset should one adopt for a particular application  as we argue in this paper  that depends on three key properties of the language: its succinctness  the class of tractable queries it supports  and the class of tractable transformations it admits.
　our goal in this and the following sections is to construct a map on which we place different subsets of the nnf language according to the above criteria. this map will then serve as a guide to system designers in choosing the target compilation languagemost suitable to their application. it also providesan example paradigm for studying and evaluating further target compilation languages. we start with a study of succinctness1in this section  gogic et al.  1 .
definition 1  succinctness  let and be two subsets of nnf. is at least as succinct as   denoted   iff for every sentence   there exists an equivalentsentence where the size of is polynomial in the size of .1
　the relation	is clearly reflexive and transitive  hence  a pre-ordering. one can also define the relation	  where iff	and	.
proposition 1 the results in table 1 hold.
figure 1 summarizes the results of proposition 1 in terms of a directed acyclic graph.
　a classical result in knowledge compilation states that it is not possible to compile any propositional formula into a

figure 1: an edge indicates that is strictly more succinct than   ; indicates that and are equally succinct  and . dotted arrows indicate unknown relationships.
polysize data structure such that: and entail the same set of clauses  and clausal entailment on can be decided in time polynomial in its size  unless np p/poly  selman and kautz  1; cadoli and donini  1 . this last assumption implies the collapse of the polynomial hierarchy at the second level  karp and lipton  1   which is considered very unlikely. we use this classical result from knowledge compilation in some of our proofs of proposition 1  which explains why some of its parts are conditioned on the polynomial hierarchy not collapsing.
　we have excluded the subsets bdd  s-nnf  d-nnf and f-nnf from table 1 since they do not qualify as target compilation languages  see section 1 . we kept nnf and cnf though given their importance. consider figure 1 which depicts table 1 graphically. with the exception of nnf and cnf  all other languages depicted in figure 1 qualify as target compilation languages. moreover  with the exception of language pi  dnnf is the most succinct among all target compilation languages-we know that pi is not more succinct than dnnf  but we do not know whether dnnf is more succinct than pi.
　in between dnnf and mods  there is a succinctness ordering of target compilation languages:
dnnf	d-dnnf	fbdd	obdd	mods
dnnf is obtained by imposing decomposability on nnf; ddnnf by adding determinism; fbdd by adding decision; and obdd by adding ordering. adding each of these properties reduces language succinctness.

nnfdnnf d-dnnf  sd-dnnf  fbddobdddnfcnfpi ipmodstable 1: succinctness of target compilation languages.	means that the result holds unless the polynomial hierarchy collapses.　one important fact to stress here is that adding smoothness to d-dnnf does not affect its succinctness: the sd-dnnf and d-dnnf languages are equally succinct. it is also interesting to compare sd-dnnf  which is more succinct than the influential fbdd and obdd languages  with mods  which is a most tractable language. both sd-dnnf and mods are smooth  deterministic and decomposable. mods  however  is flat and obtains its decomposability from the stronger condition of simple-conjunction. therefore  sd-dnnf can be viewed as the result of relaxing from mods the flatness and simple-conjunctionconditions  while maintaining decomposability  determinism and smoothness. relaxing these conditions moves the language three levels up the succinctness hierarchy  although it compromises only the polytime tests for entailment and equivalence as we show in section 1.
1	querying a compiled theory
in evaluating the suitability of a target compilation language to a particular application  the succinctness of the language must be balanced against the set of queries and transformations that it supports in polytime. we consider in this section a number of queries  each of which returns valuable information about a propositional theory  and then identify target compilation languages which provide polytime algorithms for answering such queries.1
　the queries we consider are tests for consistency  validity  implicates  clausal entailment   implicants  equivalence  and sentential entailment. we also consider countingand enumerating theory models.1
from here on  l denotes a subset of language nnf.
definition 1  co  va  l satisfies co  va  iff there exists a polytime algorithm that maps every formula from l to if is consistent  valid   and to otherwise.
　one of the main applications of compiling a theory is to enhance the efficiency of answering clausal entailment queries:
definition 1  ce  l satisfies ce iff there exists a polytime algorithm that maps every formula from l and every clause from nnf to if holds  and to otherwise.
　a key application for clausal entailment is in testing equivalence. specifically  suppose we have a design expressed as a set of clauses and a specification expressed also as a set of clauses   and we want to test whether the design and specification are equivalent. by compiling each of and to targets and that support a polytime clausal entailment test  we can test the equivalence

nnfdnnfd-nnfs-nnff-nnfd-dnnf sd-dnnf bddfbdd obdddnfcnfpiipmodstable 1: subsets of the nnf language and their corresponding polytime queries. means  satisfies  and means  does not satisfy unless p = np. 
of and in polytime. that is  and are equivalent iff for all and for all .
　a number of the target compilation languages we shall consider  however  support a direct polytime equivalent test:
definition 1  eq  se  l satisfies eq  se  iff there exists a polytime algorithm that maps every pair of formulas   from l to if     holds  and to otherwise.
note that sentential entailment  se  is stronger than clausal entailment and equivalence. therefore  if a language l satisfies se  it also satisfies ce and eq.
for completeness  we consider the following dual to ce:
definition 1  im  l satisfies im iff there exists a polytime algorithm that maps every formula from l and every term from nnf to if holds  and to otherwise.
finally  we consider counting and enumerating models:
definition 1  ct  l satisfies ct iff there exists a polytime algorithm that maps every formula from l to a nonnegative integer that represents the number of models of  in binary notation .
definition 1  me  l satisfies me iff there exists a polynomial and an algorithm that outputs all models of an arbitrary formula from l in time   where is the size of and is the number of its models  over variables occurring in  .
proposition 1 the results in table 1 hold.
　the results of proposition 1 are summarized in figure 1. one can draw a number of conclusions based on the results in this figure. first  nnf  s-nnf  d-nnf  f-nnf  and bdd fall in one equivalence class that does not support any polytime queries and cnf satisfies only va and im; hence  none of them qualifies as a target compilation language in this case. but the remaining languages all support polytime tests for consistency and clausal entailment. therefore  simply imposing either of smoothness  s-nnf   determinism  d-nnf   flatness  f-nnf   or decision  bdd  on the nnf language does not lead to tractability with respect to any of the queries we consider-neither of these properties seem to be significant in isolation. decomposability  dnnf   however  is an exception and leads immediately to polytime tests for both consistency and clausal entailment  and to a polytime algorithm for model enumeration.
	recall the succinctness ordering dnnf	d-dnnf
fbdd obdd mods from figure 1. by adding decomposability dnnf   we obtain polytime tests for consistency and clausal entailment  in addition to a polytime model enumeration algorithm. by adding determinism to decomposability  d-dnnf   we obtain polytime tests for validity  implicant and model counting  which are quite significant. it is not clear  however  whether the combination of decomposability and determinism leads to a polytime test for equivalence. moreover  adding the decision property on top of decomposabilityand determinism fbdd does not appearto increase tractability with respect to the given queries1  although it does lead to reducing language succinctness as shown in figure 1. on the other hand  adding the ordering property on top of decomposability  determinism and decision  obdd    leads to polytime tests for sentential entailment and equivalence.
	as for the succinctness ordering nnf	dnnf	dnf
　ip mods from figure 1  note that dnnf is obtained by imposing decomposability on nnf  while dnf is obtained by imposing flatness and simple-conjunction  which is stronger than decomposability . what is interesting is that dnf is less succinct than dnnf  yet does not support any more polytime queries; see figure 1. however  the addition of smoothness on top of flatness and simple-conjunction mods  leads to five additionalpolytime queries  includingequivalenceand entailment tests.1
　we close this section by noting that determinism appears to be necessary  but not sufficient  for polytime model counting: only deterministic languages  d-dnnf  sd-dnnf  fbdd  obdd and mods  support polytime counting. moreover  polytime counting implies a polytime test of validity  but the opposite is not true.
1	transforming a compiled theory
a query is an operation that returns information about a theory without changing it. a transformation  on the other hand  is an operation that returns a modified theory  which is then operated on using queries. many applications require a combination of transformations and queries.
definition 1     l satisfies     iff there exists a polytime algorithm that maps every finite set of formulas from l to a formula of l that represents
	 	 .
definition 1     l satisfies c iff there exists a polytime algorithm that maps every formula from l to a formula of l that represents .
　if a language satisfies one of the above properties  we will say that it is closed under the correspondingoperator. closure under logical connectives is important for two key reasons. first  it has implications on how compilers are constructed for a given target language. for example  if a clause can be easily compiled into some language   then closure under conjunction implies that compiling a cnf sentence into l is easy. second  it has implications on the class of polytime queries supported by the target language: if a language satisfies co and is closed under negation and conjunction  then it must satisfy se  to test whether   all we have to do  by the refutation theorem  is test whether is inconsistent . similarly  if a language satisfies va and is closed under negation and disjunction  it must satisfy se by the deduction theorem.
　it is important to stress here that some languages are closed under a logical operator  only if the number of operands is bounded by a constant. we will refer to this as boundedclosure.
definition 1     l satisfies     iff there exists a polytime algorithm that maps every pair of formulas and from l to a formula of l that represents
 	 .
we now turn to another important transformation:
definition 1  conditioning   darwiche  1a  let be a propositional formula  and let be a consistent term. the conditioning of on   noted   is the formula obtained by replacing each variable of by true  resp. false  if  resp.   is a positive  resp. negative  literal of .
definition 1  cd  l satisfies cd iff there exists a polytime algorithm that maps every formula from l and every consistent term to a formula from l that represents .
　conditioning has a number of applications  and corresponds to restriction in the literature on boolean functions. the main application of conditioning is due to a theorem  which says that is consistent iff is consistent  darwiche  1b; 1a . therefore  if a language satisfies co and cd  then it must also satisfy ce. conditioning also plays a key role in building compilers that enforce decomposability. if two sentences and are both decomposable  belong to dnnf   their conjunction is not necessarily decomposable since the sentences may share variables. conditioning can be used to ensure decomposability in this case since is equivalent to   where is a term covering all variables shared by and
   . note that	must be decomposable since and do not mention variables in . the previous proposition is indeed a generalization to multiple variables of the well-known shannon expansion in the literature on boolean functions. it is also the basis for compiling cnf into dnnf  darwiche  1a; 1b .
　another critical transformation we shall consider is that of forgetting  lin and reiter  1 :
definition 1  forgetting  let be a propositional formula  and let be a subset of variables from ps. the forgetting of from   denoted   is a formula that does not mention any variable from x and for every formula that does not mention any variable from x  we have precisely when .
therefore  to forget variables from x is to remove any reference to x from   while maintaining all information that captures about the complement of x. note that is unique up to logical equivalence.
definition 1  fo  sfo  l satisfies fo iff there exists a polytime algorithm that maps every formula from l and every subset of variables from ps to a formula from l equivalent to . if the property holds for singleton x  we say that l satisfies .
　forgetting is an important transformation as it allows us to focus/project a theory on a set of variables. for example  if we know that some variables will never appear in entailment queries  we can forget these variables from the compiled theory while maintaining its ability to answer such queries correctly. another application of forgetting is in counting/enumerating the instantiations of some variables y  which are consistent with a theory . this query can be answered by counting/enumeratingthe models of   where x is the complement of y. forgetting also has applications to planning  diagnosis and belief revision. for instance  in the satplan framework  compiling away fluents or actions amounts to forgetting variables. in model-based diagnosis  compiling away every variable except the abnormality ones does not remove any piece of information required to compute the conflicts and the diagnoses of a system  darwiche  1b . forgetting has also been used to design update operators with valuable properties  herzig and rifi  1 .
proposition 1 the results in table 1 hold.
　one can draw a number of observations regarding table 1. first  all languages we consider satisfy cd and  hence  lend themselves to efficient application of the conditioning transformation. as for forgetting multiple variables  only dnnf  dnf  pi and mods permit that in polytime. it is important to stress here that neither fbdd nor obdd permit polytime forgettingof multiple variables. this is noticeable since some of the recent applications of obdd to planning depends crucially on the operation of forgetting and it may be more suitable to use a language that satisfies fo in this case. note  however  that obdd allows the forgetting of a single variable in polytime  but fbdd does not allow even that. ddnnf is similar to fbdd as it satisfies neither fo nor sfo.

nnfdnnfd-nnf    s-nnff-nnfd-dnnf sd-dnnf bddfbddobdddnfcnfpiipmodstable 1: subsets of the nnf language and their polytime transformations. means  satisfies   means  does not satisfy   while means  does not satisfy unless p=np. 
　it is also interesting to observe that none of the target compilation languages is closed under conjunction. a number of them  however  are closed under bounded-conjunction  including obdd   dnf  ip and mods.
　as for disjunction  the only target compilation languages that are closed under disjunction are dnnf and dnf. the obdd and pi languages  however  are closed under bounded disjunction. again  the d-dnnf and fbdd languages are closed under neither.
　the only target compilation languages that are closed under negation are fbdd and obdd   while it is not known whether d-dnnf or sd-dnnf are closed under this operation. note that d-dnnf and fbdd support the same set of polytime queries  equivalence checking is unknown for both  so they are indistinguishable from that viewpoint. moreover  the only difference between the two languages in table 1 is the closure of fbdd under negation  which does not seem to be that significant in light of no closure under either conjunction or disjunction. note  however  that d-dnnf is more succinct than fbdd as given in figure 1.
　finally  obdd is the only target compilation language that is closed under negation  bounded conjunction  and bounded disjunction. this closure actually plays an important role in compiling propositional theories into obdd and is the basis of state-of-the-art compilers for this purpose  bryant  1 .
1	conclusion
the main contribution of this paper is a methodology for analyzing propositional compilation approaches according to two key dimensions: the succinctness of the target compilation language  and the class of queries and transformations it supports in polytime. the second main contribution of the paper is a comprehensive analysis  according to the proposed methodology  of more than a dozen languages for which we have produced a knowledge compilation map  which crossranks these languages according to their succinctness  and the polytime queries and transformations they support. this map allows system designers to make informed decisions on which target compilation language to use: after the class of queries/transformations have been decided based on the application of interest  the designer chooses the most succinct target compilation language that supports such operations in polytime. another key contribution of this paper is the uniform treatment we have applied to diverse target compilation languages  showing how they all are subsets of the nnf language. specifically  we have identified a number of simple  yet meaningful  properties  including decomposability  determinism  decision and flatness  and showed how combinations of these properties give rise to different target compilation languages. the studied subsets include some well known languages such as pi  which has been influential in ai; obdd   which has been influential in formal verification; and cnf and dnf  which have been quite influential in computer science. the subsets also include some relatively new languages such as dnnf and d-dnnf  which appear to represent interesting  new balances between language succinctness and query/transformation tractability.
acknowledgements
we wish to thank ingo wegener for his help with some of the issues discussed in the paper. this work has been done while the second author was a visiting researcher with the computer science department at ucla. the first author has been partly supported by nsf grant iis-1 and muri grant n1-1. the second author has been partly supported by iut de lens and the region nord/pas-de-calais.＞
references
 blum et al.  1  manuel blum  ashok k. chandra  and mark n. wegman. equivalence of free boolean graphs can be decided probabilistically in polynomial time. information processing letters  1 :1  1.
 boufkhad et al.  1  y. boufkhad  e. gregoire  p. mar-＞ quis  b. mazure  and l. sa： s. tractable cover compilations. in proc. of international joint conference on artificial intelligence  ijcai'1   pages 1  1.
 bryant  1  r. e. bryant. graph-based algorithms for boolean function manipulation. ieee transactions on computers  c-1-1  1.
 cadoli and donini  1  m. cadoli and f.m. donini. a survey on knowledge compilation. ai communications  1-1  1  printed in 1 .
 cadoli et al.  1  m. cadoli  f.m. donini  p. liberatore  and m. schaerf. comparing space efficiency of propositional knowledge representation formalisms. in proc. of international conference on knowledge representation and reasoning  kr'1   pages 1  1.
 darwiche and marquis  1  adnan darwiche and pierre marquis. a perspective on knowledge compilation. technical report d-1  cognitive systems laboratory  ucla  ca 1  1.
 darwiche  1a  adnan darwiche. compiling knowledge into decomposable negation normal form. in proc. of international joint conference on artificial intelligence  ijcai'1   pages 1  1.
 darwiche  1b  adnan darwiche. decomposable negation normalform. technical report d-1 cognitivesystems laboratory  ucla  ca 1  1. to appear in journal of the acm.
 darwiche  1  adnan darwiche. on the tractable counting of theory models and its application to belief revision and truth maintenance. technical report d-1  cognitive systems laboratory  ucla  ca 1  1. to appear in journal of applied non-classical logics.
 dechter and rish  1  r. dechter and i. rish. directional resolution: the davis-putnam procedure  revisited. in proc. of international conference on knowledge representation and reasoning  kr'1   pages 1  1.
 del val  1  a. del val. tractable databases: how to make propositional unit resolution complete through compilation. in proc. of international conference on knowledge representation and reasoning  kr'1   pages 1- 1  1.
 gergov and meinel  1  j. gergov and ch. meinel. efficient analysis and manipulation of obdd's can be extended to fbdd's. ieee transactions on computers  1 :1 1.
 gogic et al.  1  g. gogic  h. kautz  ch. papadimitriou  and b. selman. the comparative linguistics of knowledge representation. in proc. of international joint conference on artificial intelligence  ijcai'1   pages 1  1.
 herzig and rifi  1  a. herzig and o. rifi. propositional belief base update and minimal change. artificial intelligence  1 :1  1.
 karp and lipton  1  r.m. karp and r.j. lipton. some connections between non-uniform and uniform complexity classes. in proc. of acm symposium on theory of computing  stoc'1   pages 1  1.
 khardon and roth  1  r. khardon and d. roth. learning to reason. journal of the acm  1 :1  1.
 lin and reiter  1  f. lin and r. reiter. forget it! in proc. of the aaai fall symposium on relevance  pages 1  new orleans  1.
 marquis  1  pierre marquis. knowledgecompilationusing theory prime implicates. in proc. of internationaljoint conference on artifical intelligence  ijcai'1   pages 1  1.
 papadimitriou  1  ch. papadimitriou. computational complexity. addison-wesley  1.
 reiter and de kleer  1  ray reiter and johan de kleer. foundations of assumption-based truth maintenance systems: preliminary report. in proc. of national conference on artificial intelligence  aaai'1   pages 1 1.
 schrag  1  r. schrag. compilation for critically constrained knowledge bases. in proc. of national conference on artificial intelligence  aaai'1   pages 1  1.
 selman and kautz  1  bart selman and henry kautz. knowledge compilation and theory approximation. journal of the acm  1 :1  1.
phase transitions of pp-complete satisfiability problems
delbert d. bailey  v＞ ctor dalmau  phokion g. kolaitis
computer science department
university of california  santa cruz
　　　　　santa cruz  ca 1  u.s.a dbailey dalmau kolaitis  cse.ucsc.eduabstract
the complexity class pp consists of all decision problems solvable by polynomial-time probabilistic turing machines. it is well known that pp is a highly intractable complexity class and that pp-complete problems are in all likelihood harder than np-complete problems. we investigate the existence of phase transitions for a family of ppcomplete boolean satisfiability problems under the fixed clauses-to-variables ratio model. a typical member of this family is the decision problem #1sat   : given a 1cnf-formula  is it satisfied by at least the square-root of the total number of possible truth assignments  we provide evidence to the effect that there is a critical ratio at which the asymptotic probability of #1sat    undergoes a phase transition from
1 to 1. we obtain upper and lower bounds for
by showing that	. we also carry out a set of experiments on random instances of #1sat    using a natural modification of the davis-putnam-logemann-loveland  dpll  procedure. our experimental results suggest that . moreover  the average number of recursive calls of this modified dpll procedure reaches a peak around as well.
1	introduction and summary of results
during the past several years  there has been an intensive investigation of random boolean satisfiability in probability spaces parametrized by a fixed clauses-to-variables ratio. more precisely  if is an integer  is a positive integer and is a positive rational such that is an integer  then denotes the space of random cnf-formulas with variables and clauses that are generated uniformly and independently by selecting variables without replacement from the variables and then negating each variable with probability . much of the work in this area is aimed at establishing or at least providing evidence for the conjecture  first articulated by  chva＞tal and

　　research of the authors was partially supported by nsf grants no. ccr-1  ccr-1  and iis-1
reed  1   that a phase transition occurs in the probability of a random formula in being satisfiable  as . specifically  this conjecture asserts that  for every   there is a positive real number such that if
	  then	  whereas if	  then
so far  this conjecture has been established only for
　by showing that  chva＞tal and reed  1; fernandez de la vega  1; goerdt  1 . for   upper and lower bounds for have been obtained analytically and experiments have been carried out that provide evidence for the existence of and estimate its actual value. for   in particular  it has been proved that  achlioptas and sorkin  1;
janson et al.  1  and extensive experiments have suggested that  selman et al.  1 . moreover  the experiments reveal that the median running time of the davisputnam-logemann-loveland  dpll  procedure for satisfiability attains a peak around . thus  the critical ratio at which the probability of satisfiability undergoes a phase transition coincides with the ratio at which this procedurerequires maximum computational effort to decide whether a random formula is satisfiable.
　boolean satisfiability is the prototypical np-complete problem. since many reasoning and planning problems in artificial intelligence turn out to be complete for complexity classes beyondnp  in recent years researchers have embarked on an investigation of phase transitions for such problems. for instance  it is known that strips planning is complete for the class pspace of all polynomial-space solvable problems  bylander  1 . a probabilistic analysis of strips planning and an experimental comparison of different algorithms for this problem have been carried out in  bylander  1 . in addition to strips planning  researchers have also investigated phase transitions for the prototypical pspacecomplete problem qsat  which is the problem of evaluating a given quantified boolean formula  cadoli et al.  1; gent and walsh  1 . actually  this investigation has mainly focused on the restriction of qsat to random quantified boolean formulas with two alternations  universalexistential  of quantifiers  a restriction which forms a complete problem for the class at the second level of the
polynomial hierarchy ph. the lowest level of ph is np  while higher levels of this hierarchy consist of all decision problems  or of the complements of all decision problems  computable by nondeterministic polynomial-time turing machines using oracles from lower levels  see  papadimitriou  1  for additional information on ph and its levels . another pspace-complete problem closely related to qsat is stochastic boolean satisfiability ssat  which is the problem of evaluating an expression consisting of existential and ranodmized quantifiers applied to a boolean formula. experimental results on phase transitions for ssat have been reported in  littman  1  and  littman et al.  1 .
　between np and pspace lie several other important complexity classes that contain problems of significance in artificial intelligence. two such classes  closely related to each other and of interest to us here  are #p and pp. the class #p  introduced and first studied by  valiant  1a; 1b   consists of all functions that count the number of accepting paths of nondeterministic polynomial-time turing machines. the prototypical #p-complete problem is #sat  i.e.  the problem of counting the number of truth assignments that satisfy a cnf-formula. it is well known that numerous -complete problems arise naturally in logic  algebra  and graph theory  valiant  1a; 1b . moreover  #p-complete problems are encountered in artificial intelligence; these include the problem of computing dempster's rule for combining evidence  orponen  1  and the problem of computing probabilities in bayesian belief networks  roth  1 . recently  researchers have initiated an experimental investigation of extensions of the dpll procedure for solving #sat. specifically  a procedure for solving #sat  called counting davis-putnam  cdp   was presented and experiments on random1cnf formulas from the space were carried out in  birnbaum and lozinskii  1 . the main experimental finding was that the median running time of cdp reaches its peak when . a different dpll extension for solving #sat  called decomposing davis-putnam  ddp   was presented in  bayardo and pehoushek  1 ; this procedure is based on recursively identifying connected components in the constraint graph associated with a cnfformula. additional experiments on random 1cnf-formulas from were conducted and it was found out that the median running time of ddp reaches its peak when .
　in the case of the np-complete problems sat    the peak in the median running time of the dpll procedure occurs at the critical ratio at which the probability of satisfiability appears to undergo a phase transition. since #sat is a counting problem  returning numbers as answers  and not a decision problem  returning  yes  or  no  as answers   it is not meaningful to associate with it a probability of getting a  yes  answer; therefore  it does not seem possible to correlate the peak in the median running times of algorithms for #sat with a structural phase transition of #sat. nonetheless  there exist decision problems that in a certain sense embody the intrinsic computational complexity of #p-complete problems. these are the problems that are complete for the class of all decisionproblems solvableusing a polynomial-time probabilistic turing machine  i.e.  a polynomial-time nondeterministic turing machine that accepts a string if and only if at least half of the computations of on input are accepting. the class was first studied by  simon  1  and  gill  1   where several problems were shown to be
   -complete under polynomial-time reductions. in particular  the following decision problem  also called #sat  is
   -complete: given a cnf-formula and a positive integer   does have at least satisfying truth assignments  this problem constitutes the decision version of the counting problem #sat  which justifies the innocuous overload of notation. another canonical -complete problem  which is actually a special case of #sat  is majority sat: given a cnf-formula  is it satisfied by at least half of the possible truth assignments to its variables  in addition  several evaluation and testing problems in probabilistic planning under various domain representations have recently been shown to be pp-complete  littman et al.  1 .
　it is known that the class contains both np and conp  and is contained in pspace  see  papadimitriou  1  . moreover  as pointed out by  angluin  1   there is a tight connection between and . specifically    which means that the class of decision problems computable in polynomial time using oracles coincides with the class of decision problems computable in polynomial time using oracles. this is precisely the sense in which -complete problems embody the same intrinsic computational complexity as -complete problems. moreover  pp-complete problems  and #p-complete problems  are considered to be substantially harder than np-complete problems  since in a technical sense they dominate all problems in the polynomial hierarchy ph. indeed  the main result in  toda  1  asserts that . in particular  toda's result implies that no -completeproblemlies in   unless collapses at one of its levels  which is considered to be a highly improbable state of affairs in complexity theory.
　in  littman  1   initial experiments were carried out to study the median running time of an extension of the dpll procedure on instances of the pp-complete decision problem #sat in which was a random 1cnf-formula drawn from and   for some nonnegative integer . these experiments were also reported in  littman et al.  1   which additionally contains a discussion on possible phase transitions for the decision problem#sat and preliminary results concerning coarse upper and lower bounds for the critical ratios at which phase transitions may occur  in these two papers #sat is called majsat . as noted earlier  the main emphasis of both  littman  1  and  littman et al.  1 is not on #sat or on -complete problems  but on stochastic boolean satisfiability ssat  which is a pspacecomplete problem containing #sat as a special case.
　in this paper  we embark on a systematic investigation of phase transitions for a large family of -complete satisfiability problems. specifically  for every integer and every integer   let #ksat    be the following decision problem: given a cnf-formula with variables  does have at least satisfying truth assignments  in particular  for and for every   we have the decision problem #ksat   : given a cnf-formula  is it satisfied by at least the square-root of the total number of possible truth assignments  clearly  each problem in this family is a restriction of the decision problem #sat. note that  while an instance of #sat is a pair   an instance of #ksat    is just a cnf-formula ; this makes it possible to study the behavior of random #ksat    in the same framework as the one used for random sat. one may also consider the behavior of random majority sat 
     . in section 1  however  we observe that the asymptotic behavior of random majority sat is trivial and that  in particular  it does not undergo any phase transition. in contrast  the state of affairs for random #ksat    will turn out to be by far more interesting.
　we first show that  for every and every   the problem #ksat    is indeed -complete. we conjecture that each of these problems undergoes a phase transition at some critical ratio of clauses to variables: as
         for ratios   almost all formulas in are  yes  instances of #ksat     whereas for ratios
         almost all formulas in are  no  instances of #ksat   . as a first step towards this conjecture  we establish analytically upper and lower bounds for . a standard application of markov's inequality easily yields that  is an upper bound for  this was also implicit in  littman et al.  1  . using an elementary argument and the fact that the probability of satisfiability of random 1cnf-formulas undergoes a phase transition at   we show that is a coarse lower bound for . in particular  these results imply that the critical ratio of #1sat    obeys the following bounds:
. after this  we analyze a randomized algorithm  called
extended unit clause  euc   for #1sat    and show that it almost surely returns a  yes  answer when ; therefore  . although euc is a simple heuristic  its analysis is rather complex. this analysis is carried out by adopting and extending the powerful methodology of differential equations  first used by  achlioptas  1  to derive improvedlower bounds for the critical ratio of random 1cnf-formulas.
　finally  we complement these analytical results with a set of experiments for #1sat    by implementing a modification of the counting davis-putnam procedure  cdp  and running it on formulas drawn from . our experimental results suggest that the probability of #1sat    undergoes a phase transition when . thus  the upper bound for obtained using markov's inequality turns out to be remarkably close to the value of suggested by the experiments. moreover  the average number of recursive calls of the modified cdp procedure reaches a peak around the same critical ratio .
1	pp-completeness of #ksat 	 
in valiant  valiant  1a   the counting problem #sat was shown to be -complete via parsimonious reductions  i.e.  every problem in can be reduced to #sat via a polynomial-time reduction that preserves the number of solutions. moreover the same holds true for the countingversions of many other np-complete problems  including # sat  the restriction of #sat to cnf-formulas. we now use this fact to identify a large family of -complete problems.
proposition 1: for every integer and every integer   the decision problem #ksat    is -complete. in particular  #1sat    is -complete.
proof: for concreteness  in what follows we show that #1sat    is -complete. let be the following problem: given a 1cnf-formula and a positive integer   does have at least satisfying truth assignments  since #1sat is a -complete problem under parsimonious reductions  there is a polynomial-timetransformation such that  given a cnf-formula with variables   it produces a 1cnf-formula whose variables include and has the same number of satisfying truth assignments as . consequently  is a  yes  instance of majority sat  i.e.  it has at least satisfying truth assignments  if and only if is a  yes  instance of . consequently  is -complete.
　we now show that there is a polynomial-time reduction of to #1sat   . given a 1cnf-formula with variables and a positive integer   we can construct in polynomial time a 1cnf-formula with variables
that is tautologically equivalent to the
cnf-formula	. it is clear that
         where and denote the numbers of truth assignments that satisfy and respectively. consequently  has at least satisfying truth assignments if and only if has at least satisying truth assignments. 
1	upper and lower bounds for #ksat 	 
let be the random variable on such that is the number of truth assignments on
that satisfy	  where	is a random	cnf-formula in
       . thus 	is a  yes  instance of #ksat 	  if and only if	. we now have all the notation in place to formulate the following conjecture for the family of problems #ksat 	   where	and	.
conjecture 1: for every integer and every integer   there is a positive real number such that:
	if	  then	.
	if	  then	.
　we have not been able to settle this conjecture  which appears to be as difficult as the conjecture concerning phase transitions of random sat  . in what follows  however  we establish certain analytical results that yield upper and lower bounds for the value of ; in particular  these results demonstrate that the asymptotic behavior of random #ksat    is non-trivial.
1	upper bounds for #ksat 	 
let be a random variable taking nonnegative values and having finite expectation . markov's inequality is a basic result in probability theory which asserts that if is a positive real number  then  the special case of this inequality with has been used in the past to obtain a coarse upper bound for the critical ratio in random sat. we now use the full power of markov's inequality to obtain an upper bound for . as usual  denotes the logarithm of in base .
proposition 1: let and be two integers. for every positive rational number  
it follows that if	exists  then	. in particular 	
proof:	for every truth assignment	on the variables
           let	be the random variable on	such that	  if	satisfies	  and	  otherwise.	each	is a bernoulli random variable with mean
           . since   the linearity of expectation implies that . by markov's inequality  we have that
it follows that if	is such that	  then
. the result then is obtained
by taking logarithms in base in both sides of the above inequality and solving for . 
　several remarks are in order now. first  note if is kept fixed while is allowed to vary  then the smallest upper bound is obtained when . moreover  the quantity is the coarse upper bound for the critical ratio for random sat obtained using markov's inequality. in particular  for random sat this bound is   which is twice the bound for given by proposition 1.
　let majority sat be the restriction of majority sat to cnf-formulas  . obviously  a formula in is a  yes  instance of majority sat if and only if . markov's inequality implies that
　　　　　　　　　　　　　　　  from which it follows that	  for every . thus  for every   the asymptotic behavior of random majority sat is trivial; in particular  majority sat does not undergo any phase transition.
1	lower bounds for #ksat 	 
we say that a partial truth assignment covers a clause if it satisfies at least one of the literals of . we also say that covers a cnf-formula with variables if covers every clause of . perhaps the simplest sufficient condition for to have at least satisfying truth assignments is to ensure that there is a partial assignment over variables covering . the next proposition shows that if is small enough  then this sufficient condition is almost surely true for formulas in
	  as	.
proposition 1: let	and	be two integers. if
　　　　　  then  as	  almost all formulas in are covered by a partial truth assignment on variables. consequently  if	  then
it follows that if	exists  then	. in particular 
proof: in  chva＞tal and reed  1; fernandez de la vega  1; goerdt  1    it was shown that if   then 1cnfformulas in are satisfiable with asymptotic probability
. fix a ratio	and consider a random formula	in
       . by removing literals at random from every clause of   we obtain a random 1cnf-formula which is almost surely satisfiable. let be a satisfying truth assignment of and let be the partial truth assignment obtained from by taking for each clause a literal satisfied by . since
           we have that is a truth assignment on variables that covers ; hence  covers as well. 
　the preceding propositions 1 and 1 imply that  unlike majority sat  for every and every   the asymptotic behavior of #ksat    is non-trivial.
1	an improved lower bound for #1sat 	 
in what follows  we focus on #1sat   . so far  we have established that if exists  then . the main result is an improved lower bound for . theorem 1: for every positive real number  
it follows that if	exists  then	.
　the remainder of this section is devoted to a discussion of the methodology used and an outline of the proof of this result. we adopt an algorithmic approach  which originated in  chao and franco  1  and has turned out to be very fruitful in establishing lower bound for the critical ratio of random sat  see  achlioptas  1  for an overview . we consider a particular randomized algorithm  called extended unit clause  euc   that takes as a input a 1cnf-formula on variables and attempts to construct a small partial assignment covering all clauses of . algorirthm euc succeeds if the number of variables assigned by is   and fails otherwise. our goal is to show that algorithm euc succeeds almost surely on formulas from for each
	. consequently 	.
euc algorithm:
for	to	do
if there are any 1-clauses 	 forced step  pick a 1-clause uniformly at random and satisfy it.
otherwise   free step  pick an unassigned variable uniformly at random and remove all literals involving that variable in all remaining clauses.
return true if the number of assigned variables is	; otherwise  return false.
　to analyze the average performance of algorithm euc we use the differential equations methodology  dem   initially introduced in  achlioptas  1  and described more extensively in  achlioptas  1 . due to space limitations  we give only a high-level description of how dem can be applied to the analysis of algorithm euc and also outline the steps in the derivation of the improved lower bound .
　since dem was developed to analyze satisfiability testing algorithms  it should not be surprising that certain modifications are needed so that it can be applied to counting algorithms  such as euc. the main component of euc not handled directly by dem is the free step  since in a satisfiability context it is always a better strategy to assign a value to the selected variable  instead of removing all the literals involving that variable. we will describe where and how we extend dem to handle free steps.
　let be the random set of variables remaining at iteration and let denote the set of random
 -clauses remaining at iteration . to trace the value of   we rely on the assumption that  at every iteration of the execution of the algorithm being considered  a property called uniform randomness is maintained. this property asserts that in every iteration   conditional on and   is drawn from
　　　　. in  achlioptas  1   a protocol  called card game  is presented; this protocol restricts the possible ways in which a variable can be selected and assigned. it is shown that any algorithmobeyingthat protocol. satisfies the uniform randomness property. unfortunately  due to the presence of the free steps  algorithm euc does not satisfy the card protocol  unlike the majority of algorithms for satisfiability analyzed so far. it is tedious  but straightforward  to show that the natural generalization of the card game in which we allow the elimination of the literals involving the selected variable guarantees the uniform randomness property.
　we analyze the algorithm euc by studying the evolution of the random variables that count the number of clauses in
　　  . we also need a random variable that counts the number of variables assigned up to iteration . we trace the evolution of and by using a result in  wormald  1   which states that if a set of random variables evolving jointly
with such that  1  in each iteration   the random variable with high probability w.h.p.  is
very close to its expectation and  1  evolves smoothly with   then the entire evolution of will remain close to its mean path  that is  the path that would follow if was  in each iteration  the value of its expectation. furthermore  this mean path can be expressed as the set of solutions of a system of differential equations obtained by considering the scaled version of the space-state of the process obtained by dividing every parameter by .
　as discussed in  achlioptas  1   wormald's theorem guarantees that the value of the random variables considered
differs in from its mean path. this possible deviation produces some difficulties in our analysis; indeed  at each iteration we need to know the value of with far more precision  because depending on the exact value taken by algorithm euc performs different operations. to settle this technical difficulty  achlioptas derived an elegant solution  called the lazy-server lemma. intutively  this lemma states that the aforementioned difficulty can be overcome if  instead of handling unit clauses deterministically as soon as they ap-
pear  at iteration we take care of unit clauses with probability and perform a free step with probability   where has to be chosen appropriately.
　an additional technical difficulty remains. as discussed in  achlioptas  1   condition  1  of wormald's theorem does not hold when the iteration is getting close to .
this second problem is fixed by determining an iteration at which our algorithm will stop the iterative process and it will deal with the remaining formula in a deterministic fashion. we now modify algorithm euc by incorporating the features described above and obtain the following algorithm:
euc with lazy-server policy:
for	to	do
set	with probability
1. if
a  if there are	-clauses 
pick a 1-clause uniformly at random and satisfy it.
b  otherwise
pick an unset variable uniformly at random and assign it uniformly at random
1. otherwise
pick an unset variable uniformly at random and remove all literals with that underlying variable in all remaining clauses.
find a minimal covering for the remaining clauses. return true if the number of assigned variables is	; otherwise  return false.
　next  we compute the equation that determine the expected value for the evolution of   and   conditional on the history of the random variables considered up to iteration . let be a random variable representing this history  i.e.    where
	 .	since	distributes as
  the expected number of clauses in
containing a given literal is equal to . using this  we obtain the following system of equations describing the evolution of   and .


with initial conditions	 	 	.
　it is time to fix the value of . according to the lazy-server lemma any value such that

would suffice. this inequality is solved by setting

with	.
　to obtain the set of differential equations associated to the process  as discussed in  achlioptas  1   we consider the scaled version of the process  obtained by dividing every parameter by . we obtain the following differential equations.



with	 and initial conditions	 	and	. we solve the system numerically using the utility dsolve of mapple  it is easy to get	analytically  for	  and	.
now we are almost done. first  we can see that for every w.h.p. by testing numerically that if	and appealing to
the lazy-server lemma. moreover  we get and  transforming this result back to the randomized space-state  we can infer that .
similarly  the number of remaining clauses at that iteration is
　　　　　　　. it is easy to verify that the ratio of clauses to variables at iteration is smaller than   and we can apply again the argument used to obtain the first naive lower bound of to guarantee that there exists a covering partial assignment with size . thus  by adding the previous quantities  we get   which means that the algorithm succeeds.
1	experimental results for #1sat 	 
preliminary experiments were run for random 1cnfformulas with 1  1  1 and 1 variables on a sun ultra 1 workstation. for each space we generated 1 random 1cnf-formulas with sizes ranging from 1 to 1 clauses in length. each clause was generated by randomly selecting 1 variables without replacementand then negatingeach of them with probability of .
　our goal was to test the formulas for being  yes  instances of #1sat     i.e.  for having at least as many satisfying assignments as the square-root of the total number of truth assignments. for this  we implemented a threshold dpll algorithm by modifying the basic counting davis-putnam algorithm in  birnbaum and lozinskii  1  to include tracking of lower and upper bounds on the count and early termination if the threshold is violated by the upper bound or satisfied by the lower bound.
　the results are depicted in figures 1 and 1. in both figures the horizontal axis is the ratio of the number of clauses to number of variables in the space. the ranges of formula sizes represented in the graphs are 1 to 1  1 to 1  1 to 1  and 1 to 1 for the 1  1  1 and 1 variable spaces respectively.
　the phase transition graphs show for each test point the fraction of 1 newly generated random formulas that had a number of satisfying truth assignments greater than or equal to the square-root of the total number of truth assignments. they strongly suggest that is a critical ratio around which a phase transition occurs. the performance graphs show the average number of recursive calls required to test each formula and they exhibit a peak around the same ratio. in the test runs a range of 1 to 1 clauses was used for each space and the run-times on the sun ultra 1 were approximately1  1 and 1 minutes for the 1  1 and 1 variable cases  and 1 hours for the 1 variable case.
　after a larger set of experiments is carried out  we plan to apply finite-size scaling to furtheranalyzethe phase transition phenomenon exhibited by #1sat   .

figure 1: phase transition graphs

figure 1: performance graphs
　we conclude by pointing out that in section 1.1 of  littman et al.  1  it was suggested that for every
           the critical ratio of #1sat    is given aproximately by the formula . by taking   this formula suggests that the critical ratio of #1sat    should be approximately 1  which is at odds with our experimental finding of 1 as the approximate value of . we stand behind our experimental results; actually  we believe that this discrepancy is not caused by any significant difference in the outcome between the experiments carried out by  littman et al.  1  and ours  but rather is due to the way in which the above formula was extrapolated from the experiments in  littman et al.  1 . specifically  in  littman et al.  1  experiments were carried out by varying and the ratio of clauses to variables  but keeping the number of variables to a fixed value . the above formula was then derived by visual inspection of the resulting surface. we believe that  instead  the value of the critical ratio should be estimated by the crossover points of the curves obtained from experiments for different values of the number of variables. in any case  we see no theoretical argument or experimental evidence that a linear relationship between the critical ratio and should hold.
acknowledgments: we are grateful to dimitris achlioptas for stimulating discussions and pointers to his work on lower bounds for 1sat via differential equations. we also wish to thank peter young for generously sharing with us his expertise on phase transition phenomena  and moshe y. vardi for listening to an informal presentation of the results reported here and offering valuable suggestions on an earlier draft of this paper.
references
 achlioptas and sorkin  1  d. achlioptas and g.b. sorkin. optimal myopic algorithms for random 1-sat. in 1st annual symposium on foundations of computer science  pages 1  1.
 achlioptas  1  d. achlioptas. setting two variables at a time yields a new lower bound for random 1-sat. in 1nd annual acm symposium on theory of computing  pages 1  1.
 achlioptas  1  d. achlioptas. lower bounds for random
 -sat via differential equations. theoretical computer science  1. to appear.
 angluin  1  d. angluin. on counting problems and the polynomial-time hierarchy. theoretical computer science  1-1  1.
 bayardo and pehoushek  1  r.j. bayardo and j.d. pehoushek. counting models using connected components. in 1th nat'l conf. on artificial intelligence  1.
 birnbaum and lozinskii  1  e. birnbaum and e.l. lozinskii. the good old davis-putnam procedure helps counting models. journal of artificial intelligence research  1-1  1.
 bylander  1  t. bylander. the computational complexity of propositional strips planning. artificial intelligence  1-1  1.
 bylander  1  t. bylander. a probabilistic analysis of propositional strips planning. artificial intelligence  1-1  1.
 cadoli et al.  1  m. cadoli  a. giovanardi  and m. schaerf. experimental analysis of the computational cost of evaluating quantified boolean formulas. in proc. 1th conference of the italian association for artificial
	intelligence 	volume 1 of lnai  pages 1.
springer-verlag  1.
 chao and franco  1  m.-t. chao and j. franco. probabilistic analysis of two heuristics for the 1-satisfiability problem. siam j. comput.  1 :1 1.  chva＞tal and reed  1  v. chva＞tal and b. reed. mick gets some  the odds are on his side . in 1th annual symposium on foundations of computer science  pages 1- 1  1.
 fernandez de la vega  1  w. fernandez de la vega. on random 1-sat. manuscript  1.
 gent and walsh  1  i.p. gent and t. walsh. beyond np: the qsat phase transition. in proc. 1th national conference on artificial intelligence  pages 1  1.
 gill  1  j. gill. computational complexity of probabilistic turing machines. siam j. comput.  1 :1  1.
 goerdt  1  a. goerdt. a threshold for unsatisfiability. j. comput. system sci.  1 :1  1.
 janson et al.  1  s. janson  y.c. stamatiou  and m. vamvakari. bounding the unsatisfiability threshold of random 1-sat. random structures and algorithms  1 :1  1.
 littman et al.  1  m.l. littman  j. goldsmith  and m. mundhenk. the computational complexity of probabilistic planning. journal of automated reasoning  1.
 littman et al.  1  m.l. littman  s.m. majercik  and t. pitassi. stochastic boolean satisfiability. journal of automated reasoning  1. to appear.
 littman  1  m. littman. initial experimentsin stochastic satisfiability. in proc. of the 1th national conference on artificial intelligence  pages 1  1.
 orponen  1  p. orponen. dempster's rule of combination is #-p-complete. artificial intelligence  1-1  1.
 papadimitriou  1  c. h. papadimitriou. computational complexity. addison-wesley  1.
 roth  1  d. roth. on the hardness of approximate reasoning. artificial intelligence  1-1 :1  1.
 selman et al.  1  b. selman  d. g. mitchell  and h. j. levesque. generating hard satisfiability problems. artificial intelligence  1-1 :1  1.
 simon  1  j. simon. on some central problems in computational complexity. phd thesis  cornell university  computer science department  1.
 toda  1  s. toda. on the computationalpower of pp and
   . in proceedings 1th ieee symposium on foundations of computer science  focs'1   research triangle park  north carolina  usa   pages 1  1.
 valiant  1a  l. g. valiant. the complexity of computing the permanent. theoretical computer science  1 :1- 1  1.
 valiant  1b  l. g. valiant. the complexity of enumeration and reliability problems. siam journal on computing  1 :1  1.
 wormald  1  n. c. wormald. differential equations for random processes and random graphs. ann. appl. probab.  1 :1 1.

knowledge representation
and reasoning
description logics and conceptual graphs

decision procedures for expressive description logics with intersection  composition  converse of roles and role identity1
fabio massacci
dip. di ingegneria dell'informazione - universita di siena` via roma 1 - 1 siena - italy massacci dii.unisi.it

abstract
in the quest for expressive description logics for real-world applications  a powerful combination of constructs has so far eluded practical decision procedures: intersection and composition of roles.
we propose tableau-based decision procedures for the satisfiability of logics extending alc with the intersectionu  composition   uniont  converse ，  of roles and role identity id ， . we show that
1. the satisfiability of alc u   t   for which a
1-exptime upper bound was given by treeautomata techniques  is pspace-complete;
1. the satisfiability of alc u   t ，  id ，    an open problem so far  is in nexptime.
1	introduction and motivations
description logics  dls  are a popular knowledge representation formalism based on concepts and roles  where concepts model classes of individuals  and roles model relationships between individuals  baader et al.  1  chap. 1 .
　in the last years  the investigation on dls has been driven by the modeling needs of applications ranging from semistructured data to planning  baader et al.  1  part iii   which have stimulated the usage of expressive constructs. for instance  to model semi-structured data one needs to represent arbitrary relations  graphs with labeled edges   and use constructs for stating irreflexivity of relations  their transitive closure  or their intersection. for reasoning about actions  one may want to express the fact that two long sequences of actions  roles  must be completed in parallel.
　these applications have called for decision procedures to make reasoning services up to the modeling duties  see  baader et al.  1  chap. 1  or sec. 1 . yet  a powerful combination of constructs has so far eluded this quest: composition and intersection of roles. there is only an involved automata-based algorithm by danecki  giving a 1-exptime upper bound.
　decision procedures are hard to find because dls with intersection and composition  even without converse and role identity1  haven't the tree model property. with composition and intersection  we can write concepts whose models are directed acyclic graphs. adding role identity  we can force a relation to be well-founded or a model to be a cyclic graph.
　for instance  suppose we want to model the tangled web of corporate ownerships by using the roles owns  app-board  app-ceo  denoting that a corporation owns another company  appoints the company's board of directors  or its ceo . we can represent the corporations having a doubly indirectly controlled subsidiary with the concept
corpu  app-board owns u owns app-ceo .corp
if regulators forbid corporations to be owners of themselves  we can forbid it too  without ad-hoc well-founded constructs:  ownsuid corp .〕
we can also model  self-owned  chinese-box  corporations:   owns has-shares app-board uid corp . 
these models are not representable with the  classical  expressive dls such as alcreg or dlr.
　here  we consider the dl alc u   t ，  id ，   which  in addition to the constructs of alc  provides intersection  composition  union  converse of roles and role identity. alc u   t  is the fragment without inverse and identity.
　these logics are siblings of three extensions of alc: alcreg with composition and transitive/reflexive closure of roles but without intersection  baader et al.  1  chap.1 ; alb with intersection  union and negation  lutz and sattler  1   and possibly converse  hustadt and schmidt  1 ; dlr  calvanese et al.  1  with intersection and role difference but only on atomic roles and with additional limitations. in the correspondence with pdl by schild   alc u   t  corresponds to the the *-free and test-free fragment of ipdl and alc u   t ，  id ，   is the *-free fragment of converse-ipdl  harel  1 .
　in the next sections we recall some preliminaries. then we present our tableau calculus  sec.1   transform it into algorithms  sec.1   sketch the complexity results  sec.1   and discuss related works  sec.1 .
1	preliminaries
let a and p denote atomic concepts and atomic roles respectively. concepts c d and roles r s are formed as follows:

a tbox t is a finite set of inclusions c v d. in the sequel we focus on the satisfiability of concepts wrt empty tboxes. however  we will build non-empty tboxes of inclusions with a special form during the proof search itself.
an interpretation i = h i ，ii consists of a non-empty set
 i  the domain of i - whose members are called elements - and a function ，i  the interpretation function of i  that maps every concept to a subset of  i and every role to a subset of  i 〜  i. we refer to borgida  or baader et al.  1  chap. 1  for details.
　an interpretation i satisfies a concept c if there exists an element d （  i such that d （ ci  i.e. if ci =1	 .
1	a tableau calculus
taming intersection and composition requires the novel combination of a number of intuitions from the literature and some new features. they are highlighted here  for the reader familiar with the literature.
  we use the graph-based representation of rules for tableau-structures originally due to kripke and used by castillo et al.  and horrocks and sattler .
  we borrow the idea by danecki  of pebble games to mark elements linked by both r and s roles so that the labeling an element with both pebbles marks the intersection  triggering the insertion of suitable concepts;
  we internalize pebbles in the calculus by introducing new propositional constants as done by de giacomo and massacci  for eventualities in cpdl.
  we use skolemization for generating  new  nodes and atomic concepts in the proof search  exploiting the results from dl translations into first-order logics  hustadt and schmidt  1 .
  we exploit some semantical properties of intersection by balbiani and vakarelov  to show we are sound.
  we use the idea of on-the fly modification of tboxes proposed by massacci  for the universal modality.
  we borrow the idea of lazy unfolding of axioms proposed by horrocks and tobies  for plain alc.
  we use a new role name to denote equivalence of individuals but only for the proof search.
we use tableau-structures i.e. labeled graphs:
s = hn e c ，  r ， ， i
where n is a finite nonempty set of nodes denoted by x y z possibly with indices; e   n 〜 n is a set of oriented edges; c ，  maps nodes to sets of concepts  and r ， ，  maps edges to sets of roles. we also use an initially empty tbox t . inclusions of the form ax ruax s v c are added on-the-fly.
axiom:s   s “ {if x:a1 （s:c }x. :a1 （s  and a1ua1 v c （ t then x
conjunction: if x:cud（s then s   s “ {x:c x:d}.
disjunction: if x:ctdor（ss   s “ {then non-deterministically eitherx:d}.
s   s “ {x:c}
universal atomic roles: if x: p.c （s  hx yi:p （s then s   s “ {y:c}.
universal role concatenation: if x: r s.c （s then s   s “ {x: r. s.c}.
universal role intersection: if x :  r u s.c （ s then let ax r and ax s be new atomic concepts and
	{s   s “ {ax ruax s xv: cr}.a. x r x: s.ax s} and t	  t “
universal role union:x: r.c x: s.cif}.x :  rts.c （ s then s   s “ {
existential restriction: if x: r yr.c:c（s}. then let y be a new node and s   s “ {hx yi:
role concatenation: if hx yx zi:ri:r  shz y（si:then lets}. z be a new node and s   s “ {h
role intersection:	then s   s “
role union: if hx yi:rx ytsi:r（s} orthen non-deterministicallys   s “ {hx yi:s}. either s   s “ {h
figure 1: the reduction rules for alc u   t .
　we start with a structure with a single node labeled with an input concept c  and apply the tableau rules to build a reduced structures without contradictions that can be used as a model for c. if none can be build  c is unsatisfiable.
　the reduction rules in fig. 1 are applicable to a tableaustructure s and a set of inclusions t for alc u   t .
w.l.o.g. we assume that negation and converse are pushed down to atomic concepts and roles. we also abuse the dl syntax for tableau systems  buchheit et al.  1 : we write
xewhenr:1 c， ，r（ s（ rto indicate that x y . thus  byc （ cs   s “ {1 such that x   andnxh1:x yc= }xin “ {we mean :“ {r （ scx}}  
that we obtain a new structure sx and c1 x  = c
1most rules are close to other tableau approaches= e  =  c1r y   =， ， . similarly for roles.c1 y  if y 1=	 buchheit
et al.  1; castilho et al.  1; de giacomo and massacci  1; horrocks and sattler  1 . a difference is that they only label edges with atomic roles whereas we also use complex roles. so  we have rules for reducing these roles.
　the role of pebbles is played by the new atoms introduced by the universal role intersection rule. the idea is that since
athusx ryis new  it can label a nodemust be an r-successors ofy only by reducingx. if both ax r andr.aax rx s. label node y  then y is linked to x by an rus role. then we can add c to y.
　since we cannot forecast where ax r and ax s will appear  we must potentially check every node. to internalize this check into the calculus  we use the idea of on-the-fly modification of tboxes by  massacci  1 . with the intersection rule we  update  t with the inclusion ax ruax s v c and then we use the axiom rule to add c to nodes on demand.
　notice that we apply the axiom rule only when both ax r and ax s are present in the same node. this technique  borrowed from the lazy unfolding of horrocks and tobies   works because our inclusions are acyclic.
　notice also that the new concept ax r introduced by the reduction of x :  rus.c must depend on the node where the concept is located. following danecki  1  pag. 1   one may think that it suffices to make them dependent only on the subformula  e.g. ar  rus.c   possibly distinguishing between occurrences. this would be unsound. the concept
  r. rus.c u   r r u r s . c 
is satisfiable but introducing the same  new  concepts at different nodes for the reduction of the only occurrence of  rus.c would result in a  clash . by applying the rules of
fig. 1  one can see that the pebbles ar  rus.c and as  rus.c would propagate along the  wrong  path.
　the generation of new nodes and new atomic concept symbols may clearly lead to redundant rule applications or even to a non-terminating process. however  we do not need to generate really  new  nodes or atomic concepts. we can use the equivalent of skolemization  or hilbert's -terms  introduced by ohlbach and later refined by hustadt and schmidt  for the translations of dls into first-order logic.
　the generation of skolem functions is standard and we assume that for  new  concepts we use the function genconcept x r   and for  new  nodes we use gennodeconcept x c  and gennoderole x y r . so  when reducing x: r.c  we call y = gennodeconcept x  r.c  and add y:c and hx yi:r to the structure.
definition 1 a tableau structure s and a set of inclusions t are reduced for a rule r if the application of r maps s and t into themselves. a structure and a set of inclusions are reduced if they are reduced for all rules.
definition 1 a tableau-structure contains a clash if there is a node x and a concept c such that {c  c}   c x .
theorem 1 a concept c of alc u   t  is satisfiable iff there is a non-deterministic application of the rules in fig. 1 to an initially empty tbox and a structure with only one node labeled with c leading to a reduced and clash-free structure.
for every feature  the proof cleverly combines the techniques from the cited works in the literature. the twist is the soundness of the universal role intersection rule. to this extent  we set and similarly for s. in words  the newly introduced concepts are fulfilled by the appropriate individuals r-reachable  or s-reachable  from x. due to lack of space  details are left to the full paper.
　next  we define the tableau rules for the full language. beside converse  the tricky bit is the presence of the intersection between role identity and complex roles: id c ur is a selfloop describing individuals in the class c that are in relation r with themselves  e.g. the self-owned companies . to this extent we employ an atomic role symbol  not occurring in the input concept  for equality of nodes ，「，.
universal converse of roles:then s   s “ {x:c}.	if y: r .c （s  hx yi:r （s
universal role identity:x: dtc}.	if x: id d .c （s then s   s “
{
role converse: if hx yi:r （s then s   s “ {hy xi:r} .
role identity: if hx yi:id c （s then s   s “ {x「y x:c}
role identity distribution: if x 「 y （ s and x : c （ s then s   s “ {y:c}.
role identity symmetry: if x「y （ s then s   s “ {y「x}. role identity transitivity:x「z}. if x「y （ s and y「z （ s for then
s   s “ {
figure 1: the additional reduction rules for ，  and id ， 
algorithm worlds;
input node x1; set of concepts c; set of inclusionsotherwise;	t ; output sat if c is satisfiable x; labeled graphunsats;
variables node begin
n= {x1}; e=  ; c ， = {hx1 ci}; r ， ，  =  ;
if clash s  then returnnode s unsat t  =/ =;none do
while x=choose
r=chooserule x s t  ;  apply r to x  s  t updating s and t  
　　　if clash s  then return unsat ; forall
if worlds x c x  t  ==unsat then return unsat ;
return sat
end;
figure 1: the algorithm for alc u   t 
the reduction rules in figure 1 and the additional rules in
figure 1 are applicable to a tableau-structureinclusions t for the logicmight be eliminated if the rule foralc u   t ，  id ，s  .and a set ofid ， 
　the rules for ，「， collapses nodes. however  this would complicate the algorithms  as we would need a unique way for collapsing nodes.
theorem 1 a concept c of alc u   t ，  id ，   is satisfiable iff there is a non-det. application of the rules in fig. 1 to an initially empty tbox and a structure with only one node labeled with c yielding a reduced and clash-free structure.
1	from calculi to algorithms
the pspace-algorithm forfor modal logicalc u   tk . it is plural becauseis called worlds
after ladner's world each call works on a fragment of a kripke model and not just on a set of concepts true for an individual. it is in fig. 1. to check the satisfiability of a concept c one calls worlds 1 {c}    and applies the rules from fig. 1.
　in the algorithms  we use the symbol  =  for assignment  and  ==  for equality testing. we assume all functionalities for working with labeled direct graphs  an auxiliary function clash s  which detects clashes  and auxiliary procedures for selecting objects: choosenode selects a node to be reduced; chooserule selects an applicable rule for a node.
algorithm cworlds; input node; set of concepts c; output set of concepts d c if c is satisfiable  unsat otherwise; variables node x; s labeled graph; d set of concepts; global variables set of inclusions t ; begin
;
if clash s  then return unsat;
start: while x=choosenode s t  =/=none do r=chooserule x s t    apply r to x  s and t updating s and t  
if clash s  then return unsat;
forallx （ n   {x1} do
 d=cworlds x c x  ; if d==unsat then return unsat; else if c x =/= d; then c x = d; goto start; return c x1  
end;
figure 1: the cworlds algorithm
　these procedures must work in time polynomial in the size of the input and respect the following constraints:
search criteria 1 a rule is not selected if the structure is already reduced for it. a node is not selected if the structure is already reduced for all rules applicable to the node.
search criteria 1 the existential restriction rule is selected only for reduction of concepts labeling the initial node.
if the procedures cannot select anything respecting the above constraints they return the value none.
　the tricky bit is showing that the algorithm is in pspace  sec. 1 . we just highlight the differences from trace-based methods used for alc by ladner   schmidt-schauss and smolka  or tobies .
　the classical tableau-based algorithm for alc would have been identical to ours except for the last for cycle: for all	do
	if world	unsat
then return unsat 
 in words  for alc we explore the one-step r-successors of  by recursively calling the procedure. so  we only examine
one node    at every call.
　the worlds algorithm examines the initial node and reduces all existential concepts labeling  i.e. all one step r-successors of. thus  it builds a fragment of the kripke structure. however  the reduction of existential-concepts labeling successors nodes is deferred to recursive calls. the rules for intersection and composition may transform a  onestep  r-successors into  many-steps  atomic p-successors. still  the number of intermediate p-steps is linearly bounded by the size of the input concept.
　the full algorithm is called cworlds because it returns a set of concepts. it is shown on figure 1. the rules that can be applied to the algorithm are those from fig. 1. it must respect an additional search constraint:
search criteria 1 if x1 「 x1  x1 「 x1 ...xn 1 「 xn are present in the structure then only one node xi （ {x1 ...xn} can be selected for the recursive call of cworlds in the forall cycle. if is among them  none can be selected.
　we must return a set of concepts rather than just sat to cope with converse and role identity. the idea is borrowed from modal logics with symmetric relations  massacci  1  and dls with converse  tobies  1 .
　for example  the concept  r id  id  r.c .   r. c is unsatisfiable. if we used algorithm worlds with the full set of rules it would return sat. the explanation is that we would introduce a node y labeled by  id  r.c .  and would evaluate the existential concept in the next recursive call. in the recursive call we would add  r.c to the initial node  which would correspond to y  but we could not use this information to derive a clash in the initial call.
　the use of skolemization  rather than truly fresh names  is necessary to guarantee that cworlds is both correct and terminating. for instance  if fresh names are used in the recursive calls following a restart  the concept  r. s. s ut.c would make cworlds non-terminating.
1	complexity analysis
we denote by n the size of the input concept c which must be proved  un satisfiable  measured as the number of symbols.
　we first prove that worlds requires only polynomial space in n. the idea is that alc u   t  lost the tree-model property but kept the cactus-model property.
　loosely speaking  our models can be arranged into the shape of a cactus  i.e. a tree in which the edge connecting two nodes of the tree is not a slim branch but a  fat   cactus-like stem. this fat stem is made by other nodes and edges  but its size is polynomially bounded by n. many stems sprout from each stem  as in a real cactus  but their number is still polynomially bounded. since the height of the cactus is also polynomially bounded we are done.
　concepts labeling nodes can be seen as spikes. we must also prove that they aren't exponentially many. for most dls this is obvious: we only introduce subconcepts of the input concept. here  we introduce new atomic concepts.
　the next tree lemmata make these intuitions precise. at first  we say that the cactus height is polynomially bounded:
lemma 1 the call stack of worlds is at most o n  deep. then the size of each cactus stem is bounded:
lemma 1 each invocation of worlds generate a structure with at most o n  nodes.
finally  we say that the the number of spikes is bounded.
lemma 1 the number of new atoms occurring as  sub concepts in the the labels of the nodes of an invocation of worlds is bounded by o n1 .
　to prove lemma 1 we cannot use the standard proof that each interaction reduces the modal depth  nesting of universal and existential roles  of the set of concepts labeling a node because of the axiom rule. we need a new notion of modal depth: we associate different depths to atomic concepts from the input concept and to  invented  atomic concepts.
  if a is from input concept then d a  = 1
  if ax r is an atomic concept introduced by the reduction of one or more x :  rus.c then d ax r  is equal to maxs c{d c |x: rus.c}
  d  r.c  = |r|+d c  where |r| is the size of r  measured as the number of symbols.
now by induction on the number of applied rules we can show that for all nodes x =1 x1 and all concepts c （ c x   there is a concept such d d    d c . since the depth of every concept  and hence the maximal depth of a set of concepts  is bounded by n we are done.
　for the proof of lemma 1  observe that new nodes in the structure s can only be created by two rules: the rules reducing existential and the rules reducing composition of roles. the first rule can only be applied to  thus bounding the nodes so introduced by o n . for the second rule  the number of nodes thus introduced is bounded by o n .
　for lemma 1  observe that two  new  concepts are introduced only when we reduce an universally quantified role intersection. the newly generated concepts depend on the same node and on the roles which are immediate subroles of an intersection of roles occurring in the input concept. hence  by lemma 1  for each invocation we can create at most o n，n  different new concepts. by lemma 1 the number of nested recursive calls of worlds is at most o n . when a leaf call is reached we generated at most o n1 ，n  different concepts.
theorem 1 worlds can be implemented using only polynomial space in n.
for the proof observe that the stack is at most o n  deep by
lemma 1 and for each call of worlds at depth i we have
  o n  + o n  new nodes and at most o n1  edges 
  o n1  concepts for each node and each concepts taking at most o n  space 
  o n  roles labeling each edge and each role taking at most o n  space
  o i ， n1  inclusions  which can be added by reducing a concept x :  rus.c where the number of different xs is bounded by lemma 1  rus occurs in the input concept and c either occurs in the input concept or is one of o n1  new concepts invented at any j ＋ i steps. pspace-hardness follows from alc.
corollary 1 the satisfiability of alc u   t  concepts is pspace-complete.
　the nexptime-upper bound of alc u   t ，  id ，   satisfiability  has a more involved proof. the analogous of lemma 1 and lemma 1 can be proved with a similar argument. the only twist is observing that  by criteria 1  we only reduce the existential concepts in one of the node linked by the equality predicate. thus  we never recursively call cworlds on the nodes  equal  to the root node  as for them the induction would fail . the equivalent of lemma 1 fails and we only have a global bound:
lemma 1 the total number of new atoms occurring as  sub concepts in the the labels of the nodes throughout the execution of worlds is bounded by o nn  = 1o nlogn .
　using lemma 1  we bound the number of concepts labeling a formula to o n ，1o nlogn  = 1o nlogn  and the number of inclusions by doubling the multiplicative constant in the o nlogn  expression at the exponent.
theorem 1 cworlds terminates after 1o n1 logn  time.
for the proof observe that cworlds can be  restarted  at most finitely many times. more precisely   i  the set of concepts returned by cworlds is larger or equal to the initial set upon which cworlds is called  in symbols   and  ii  the total number of concepts is
bounded by 1o nlogn . then the total number of restarts for a structure in a single invocation of cworlds is bounded by o n  ， 1o 1nlogn . the bound on the stack does the rest.
corollary 1 the satisfiability of alc u   t ，  id ，   concepts is in nexptime.
　an intriguing question is why the standard technique for giving pspace-bounds for dls with converse  tobies  1  or symmetric modal logics  massacci  1  fails here. according this technique  we take the  converse-free  algorithm and add some  restarts   this is what cworlds does . for the bound  we observe that  i  restarts add a bounded number of new concepts  indeed subconcepts of the input concept  and  ii  the time wasted by restarts doesn't matter.
　the argument fails here because converse or role identity and intersection plus composition can force cworlds to label a node with exponentially many new atomic concepts.
　to force this exponential generation of concepts  we start by constructing a concept gn whose model is a cyclic graph with edges labeled by two roles r and s. looking only at role r  the graph is a binary tree with height n  with gn labeling its root. each s-edge connects the child node of an r-edge to its parent. the key feature of gn is that we can start at the root  traverse forward a path of r-edges  reach a leaf and then continue to traverse s-edges forward to the root. once there  we can take another r-path  and so on. the model of gn has a path whose length is exponential in n. pictorially  gn can be seen as a daisy with exponentially many petals.
　this construction is impossible in alc  alc with converse  or alc u   t  because there are always acyclic models. in alc we can keep on traversing edges  roles  on one path until we arrive at dead-end. since the depth of the longest path is linear in the size of the concept to be verified we are done. in alc with converse  edges can be oriented forward or backward: we get a two-ways path but still it terminates into a dead-end. with intersection we must check that two paths meet at certain points but the idea is the same.
　still  cworlds only uses polynomial space for visiting gn. the problems start when we add to gn one or more concepts that ask to verify the intersection of two paths. loosely speaking  one path reaches the top of a petal  the other goes back to center  then round another petal and back to the top of the first petal. the problem is that we don't know a priori which petal we must visit among the exponentially many that are available. we can put one of these concepts in each leaf. then  the number of pebbles that accumulate in the root  restart after restart  will be exponential in n.
1	related methods
a number of decision procedures and complexity results for logics extending alc  or multimodal k  can be found in the literature. yet  none of them fully tackle our results.
　with respect to complexity results based on encodings  de giacomo and lenzerini  proved the exptimecompleteness of a dl with composition  converse and intersection restricted to atomic roles with additional limitations. calvanese et al.  proved the exptime-completeness of dlr  where intersection  union and difference of roles are allowed but composition is not permitted. baader and sattler  proved the undecidability of a number of combination of expressive dls with number restrictions and intersection.
　with respect to decision procedures  horrocks et al.  1; 1  tamed expressive dls with converse and rolehierarchies where intersection between atomic roles can be simulated. they do not allow for composition of roles. baader and sattler  have proposed an exptimecalculi for alc with number restrictions with composition  intersection and union of role chains. however  concepts are restricted to plain alc. lutz and sattler  give a decision procedure for alb  alc plus intersection  union  and negation of roles but without composition . tobies  gives a pspace-algorithm for dls with number restrictions  converse and intersection of atomic relations.
　in the realm of modal and dynamic logic  danecki  has shown  using automata-based techniques  that ipdl  pdl with intersection  can be decided in 1-exptime. ipdl strictly extends the logic alc u   t  allowing for the transitive and reflexive closure of roles and for role identity. however  danecki claims that it is possible to use pebbles which depends only on the occurrence of the subformula. for our calculus  this is unsound and danecki's involved construction may need to be checked against our counterexample. for converse-pdl automata theoretic techniques have been proposed by vardi and wolper  and a tableau calculus has been given by de giacomo and massacci .
　hustadt and schmidt  have shown a decision procedures for boolean modal logic with converse. their procedure is based on a clever translation into first-order logic of modal formulae followed by a decision procedure for the guarded fragment. they only prove decidability results. the possibility of a pspace-algorithm for the extension of alc with intersection  converse and union is just claimed.
　decision procedures for first order logic with two variables  see the survey by gradel and otto：   can be used for dls with intersection but without composition  via first-order translations. one could also use the decision procedures by ganzinger and de nivelle  for the guarded fragment. current methods based on translations into decidable fragments of first order logic cannot treat intersection and composition at the same time  as this requires to have at least three variables and does not allows for guards in predicate clauses.
references
 baader and sattler  1  f. baader and u. sattler. expressive number restrictions in description logics. jlc  1-1  1.
 baader et al.  1  f. baader  d. mcguinness  d. nardi  and
p. patel schneider  editors. the description logic handbook:
theory  implementation and applications. cambridge univ. press. 1. to appear.
 balbiani and vakarelov  1  p. balbiani and d. vakarelov. iteration-free pdl with intersection: a complete axiomatization. fundamenta informaticae  1. to appear.
 borgida  1  a. borgida. on the relative expressiveness of description logics and predicate logics. aij  1-1  1.
 buchheit et al.  1  m. buchheit  f. donini  and a. schaerf. decidable reasoning in terminological knowledge representation systems. jair  1-1  1.
 calvanese et al.  1  d. calvanese  g. de giacomo  and m. lenzerini. on the decidability of query containment under constraints. in proc. of pods'1  pp.1  1.
 castilho et al.  1  m. castilho  l. farinas del cerro  o. gas-  quet  and a. herzig. modal tableaux with propagation rules and structural rules. fund. inf.  1-1  1.
 danecki  1  r. danecki. nondeterministic propositional dynamic logic with intersection is decidable. in proc. of the 1th sym. on comp. theory  lncs 1  pp.1. springer  1.
 de giacomo and lenzerini  1  g. de giacomo and m. lenzerini. what's in an aggregate: foundations for description logics with tuples and sets. in proc. of ijcai'1  pp.1  1.
 de giacomo and massacci  1  g. de giacomo and f. massacci. combining deduction and model checking into tableaux and algorithms for converse-pdl. information and computation  1-1  1. short version in proc. of cade-1.
 ganzinger and de nivelle  1  h. ganzinger and h. de nivelle. a superposition decision procedure for the guarded fragment with equality. in proc. of lics'1  pp.1  1.
 gradel and otto  1：   e. gradel and m. otto. on logics with two： variables. tcs  1-1  1.
 harel  1  d. harel. dynamic logic. in handbook of philosophical logic  vol  ii  pp.1. reidel  1.
 horrocks and sattler  1  i. horrocks and u. sattler. a description logic with transitive and inverse roles and role hierarchies. jlc  1-1  1.
 horrocks and tobies  1  i. horrocks and s. tobies. reasoning with axioms: theory and practice. in proc. of kr 1  pp.1- 1. 1.
 horrocks et al.  1  i. horrocks  u. sattler  and s. tobies. practical reasoning for expressive description logics. logic j. of the igpl  1-1  1.
 hustadt and schmidt  1  u. hustadt and r. a. schmidt. using resolution for testing modal satisfiability and building models. jar  1. to appear.
 ladner  1  r. ladner. the computational complexity of provability in systems of modal propositional logic. siam joc  1-1  1.
 lutz and sattler  1  c. lutz and u. sattler. the complexity of reasoning with boolean modal logic. in proc. of aiml 1.
 massacci  1  f. massacci. tableaux methods for formal verification in multi-agent distributed systems. jlc  1-1  1.
 massacci  1  f. massacci. single step tableaux for modal logics: methodology  computations  algorithms. jar  1-1  1.
 schild  1  k. schild. a correspondence theory for terminological logics. in proc. of ijcai'1  pp.1. 1.
 schmidt-schau  and smolka  1  m. schmidt-schau  and g. smolka. attributive concept descriptions with complements. aij  1-1  1.
 tobies  1  s. tobies. pspace reasoning for graded modal logic. jlc  1  to appear.
 vardi and wolper  1  m. vardi and p. wolper. automatatheoretic techniques for modal logics of programs. jcss  1-1  1.

ontology reasoning in the shoq d  description logic

ian horrocks
department of computer science university of manchester  uk horrocks cs.man.ac.uk ulrike sattler
lufg theoretical computer science
rwth aachen  germany sattler informatik.rwth-aachen.de

abstract
ontologies are set to play a key role in the  se-  mantic web  by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. shoq d  is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages  e.g.  oil and daml . we present sound and complete reasoning services for this logic.
1	introduction
the recent explosion of interest in the world wide web has also fuelled interest in ontologies.1 this is due both to the use of ontologies in existing web based applications and to their likely role in the future development of the web 	 van heijst et al.  1; mcguinness  1; uschold and gruninger  1：  . in particular  it has been predicted that ontologies will play a pivotal role in the  semantic web-the world wide web consortium's vision of a  second generation  web in which web resources will be more readily accessible to automated processes  berners-lee  1 .
　a key component of the semantic web will be the annotation of web resources with meta-data that describes their content  with ontologies providing a source of shared and precisely defined terms that can be used in such metadata. this requirement has led to the extension of web markup languages in order to facilitate content description and the development of web based ontologies  e.g.  xml schema  rdf  resource description framework   and rdf schema  decker et al.  1 . rdf schema  rdfs  in particular is recognisable as an ontology/knowledge representation language: it talks about classes and properties  binary relations   range and domain constraints  on properties   and subclass and subproperty  subsumption  relations. however  rdfs is a very primitive language  the above is an almost complete description of its functionality   and more expressive power would clearly be necessary/desirable in order to describe resources in sufficient detail. moreover  such descriptions should be amenable to automated reasoning if they are to be used effectively by automated processes.
　these considerations have led to the development of oil  fensel et al.  1  and daml  hendler and mcguinness  1   two ontology languages that extend rdfs with a much richer set of modelling primitives. both languages have been designed in such a way that they can be mapped onto a very expressive description logic  dl .1 this mapping provides them with a formal semantics  a clear understanding of the characteristics of various reasoning problems  e.g.  subsumption/satisfiability   and the possibility of exploiting existing decision procedures. oil  in particular  was designed so that reasoning services could be provided  via a mapping to the shiq dl  by the fact system  horrocks et al.  1; horrocks  1 .
　unfortunately  these mappings are currently incomplete in two important respects. firstly  any practical ontology language will need to deal with concrete datatypes  numbers  strings  etc.   baader and hanschke  1 . e.g.  ontologies used in e-commerce may want to classify items according to weight  and to reason that an item weighing more than 1 kilogrammes is a kind of item that requires special shipping arrangements. oil already supports the use of integers and strings in class descriptions  and it is anticipated that daml+oil  a new language developed from a merging of the daml and oil efforts  will support  most of  the datatypes defined or definable by xml schema. however  the shiq logic implemented in the fact system does not include any concrete datatypes  so there is no mechanism for reasoning with this part of the language.
　secondly  realistic ontologies typically contain references to named individuals within class descriptions. e.g.   italians  might be described as persons who are citizens of  italy   where italy is a named individual  schaerf  1 . the required functionality can be partially simulated by treating such individuals as pairwise disjoint atomic classes  this is the approach taken in the existing oil  ★ fact mapping   but this can result in incorrect inferences.
in this paper we will present a new dl that overcomes both of the above deficiencies by taking the logic shq and extending it with individuals  o  and concrete datatypes  d  to give shoq d . the starting point for these extensions is shq rather than shiq  i.e.  without inverse roles   because reasoning with inverse roles is known to be difficult and/or highly intractable when combined with either concrete datatypes or named individuals: the concept satisfiability problem is know to be nexptime hard even for the basic dl alc augmented with inverse roles and either concrete datatypes or named individuals  lutz  1; tobies  1 . this hardness result for concrete datatypes is not yet directly applicable to shoq d  as it depends on comparisons of concrete values  binary predicates   but the addition of such comparisons would be a natural future extension to shoq d . moreover  the presence of nominals in any dl leads to the loss of the tree/forest model property  which becomes particularly problematical in the presence of inverse roles  number restrictions  and general axioms. as a result  to the best of our knowledge  there is no  practicable  decision procedure for shiq with nominals or converse-dpdl with nominals  the latter being a close relative of shiq from dynamic logics  streett  1 . finally  since individuals and concrete datatypes are much more widely used in ontologies than inverse roles  corcho and perez  1＞    shoq d  is a very useful addition to our reasoning armoury.
1	preliminaries
in this section  we will describe our choice of concrete datatypes and named individuals  and introduce the syntax and semantics of shoq d .
concrete datatypes concrete datatypes are used to represent literal values such as numbers and strings. a type system typically defines a set of  primitive  datatypes  such as string or integer  and provides a mechanism for deriving new datatypes from existing ones. for example  in the xml schema type system the nonnegativeinteger datatype is derived from the integer datatype by constraining values of nonnegativeinteger to be greater than or equal to zero  biron and malhorta  1 .
　in order to represent concepts such as  persons whose age is at least 1   we can extend our concept language with a set d of concrete datatypes and concepts of the form  r.d and  r.d  where d （ d. to be more precise  we assume that we have a set of datatypes d  and  with each d （ d  a set dd    d is associated  where  d is the domain of all datatypes. we will assume that:
1. the domain of interpretation of all concrete datatypes d  the concrete domain  is disjoint from the domain of interpretation of our concept language  the abstract domain   and
1. there exists a sound and complete decision procedure forthe emptiness of an expression of the form  where di is a  possibly negated  concrete datatype from d  where  d is interpreted as  d   dd .
we will say that a set of datatypes is conforming if it satisfies the above criteria.
　the disjointness of the abstract and concrete domains is motivated by both philosophical and pragmatic considerations. on the one hand  concrete datatypes are considered to be already sufficiently structured by the type system  which may include a derivation mechanism and built-in ordering relations; therefore  we do not need the dl mechanism to form new datatypes as in  baader and hanschke  1 . on the other hand  it allows us to deal with an arbitrary conforming set of datatypes without compromising the compactness of our concept language or the soundness and completeness of our decision procedure.
　this scheme can be trivially extended to include boolean combinations of datatypes and number restrictions qualified with data types  but to simplify the presentation we will only consider  possibly negated  atomic datatypes and exists/value restrictions. the type system can be as complex as that defined for xml schema  or as simple as the one defined in the oil ontology language  fensel et al.  1   where the only primitive datatypes are integer and string  and new types are derived by adding minimum and maximum value constraints. using the oil typesystem we could  for example  define the type  min 1  and use it in the concept person u  age. min 1 .
named individuals allowing named individuals to occur in concepts provides additional expressive power that is useful in many applications; nominals  as such individuals can be called  are a prominent feature of hybrid logics  blackburn and seligman  1   and various extensions of modal and description logics with nominals have already been investigated  see  e.g.   schaerf  1; de giacomo  1; areces et al.  1  . as we have seen  nominals occur naturally in ontologies as names for specific persons  companies  countries etcetera.
　from a semantic point of view  it is important to distinguish between a nominal and an atomic concept/simple class  since the nominal stands for exactly one individual-in contrast to a concept  which is interpreted as some set of individuals. modelling nominals as pairwise disjoint atomic concepts can lead to incorrect inferences  in particular with respect to implicit maximum cardinality constraints. for example  if italy is modelled as an atomic concept  then it would not be possible to infer that persons who are citizens only of italy cannot have dual-nationality  i.e.  cannot be citizens of more than one country .
　finally  nominals can be viewed as a powerful generalisation of dl abox individuals  schaerf  1 : in an abox we can assert that an individual is an instance of a concept or that a pair of individuals is an instance of a role  binary relation   but abox individuals cannot be used inside concepts. for example  if giuseppe and italy are abox individuals  we could assert that the pair  giuseppe  italy  is an instance of the citizen-of role  but we could not describe the concept italian as a person who is a citizen-of italy. using nominals  not only can we express this concept  i.e.  person u  citizen-of.italy   but we can also capture abox assertions with concept inclusion axioms of the form giuseppe v italian  giuseppe is an italian  and giuseppe v  citizen-of.italy  giuseppe is a citizen-of
italy .
construct namesyntaxsemanticsatomic concept c abstract role ra concrete role rda
r
triia i  ii 〜i idinominals i datatypes do do        o = 1 dd    dconjunction disjunction negation exists restriction value restriction atleast restriction atmost restriction datatype exists datatype valuecc utcdd
 d
 
 r.c
 r.c
  ns.c 
 1ns.c 
 t.d
 t.dc
h  	|  
h  	implies
x yx yhx yx y x yx y  1 c i （      ns.cns.c   i （i （ i （utr.cr.ci （t.dt.dcdrddt  issiriit        iiiiiiiiii=  ii=  implies}}========andand””{{cc{{{{diccxxxxxxiiyy ii |  |  |||  “”yy（（  cd  1   （iy.y.ddiy.y.cd{{dy.y.iicdinn}d}}}i}
h h	h	i （	（	}
figure 1: syntax and semantics of shoq d 
shoq d  syntax and semantics
definition 1 let c  r = ra   rd  and i be disjoint sets of concept names  abstract and concrete role names  and individual names.
　for r and s roles  a role axiom is either a role inclusion  which is of the form r v s for r s （ ra or r s （ rd  or a transitivity axiom  which is of the form trans r  for r （ ra. a role box r is a finite set of role axioms.
　a role r is called simple if  for v* the transitive reflexive closure of v on r and for each role s  s v* r implies trans s  1（ r.
　the set of shoq d -concepts is the smallest set such that each concept name a （ c is a concept  for each individual name o （ i  o is a concept  and  for c and d concepts  r an abstract role  t a concrete role  s a simple role  and d （ d a concrete datatype  complex concepts can be built using the operators shown in figure 1.
　the semantics is given by means of an interpretation i =  consisting of a non-empty domain  i  disjoint from the concrete domain  d  and a mapping ，i  which maps atomic and complex concepts  roles  and nominals according to figure 1    denotes set cardinality . an interpretation
i =   i ，i  satisfies a role inclusion axiom r1 v r1 iff   and it satisfies a transitivity axiom trans r  iff ri =  ri +. an interpretation satisfies a role box r iff it satisfies each axiom in r.
　a shoq d -concept c is satisfiable w.r.t. a role box r iff there is an interpretation i with ci =1   that satisfies r. such an interpretation is called a model of c w.r.t. r. a concept c is subsumed by a concept d w.r.t. r iff ci v di for each interpretation i satisfying r. two concepts are said to be equivalent  w.r.t. r  iff they mutually subsume each other  w.r.t. r .
　some remarks are in order: in the following  if r is clear from the context  we use trans r  instead of trans r  （ r. please note that the domain of each role is the abstract domain  and that we distinguish those roles whose range is also the abstract domain  abstract roles   and those whose range is the concrete domain  concrete roles . in the following  we use r for the former and t for the latter form of roles  possibly with index . we have chosen to disallow role inclusion axioms of the form t v r  or r v t  for r an abstract and t a concrete role  since each model of such an axiom would necessarily interpret t  or r  as the empty relation.
　restricting number restrictions to simple roles is required to yield a decidable logic  horrocks et al.  1 .
　next  negation of concepts and datatypes is relativised to both the abstract and the concrete domain.
　as usual  subsumption and satisfiability can be reduced to each other  and shoq d  has the expressive power to internalise general concept inclusion axioms  horrocks et al.  1 . however  in the presence of nominals  we must also add  o.o1 u...u o.o` to the concept internalising the general concept inclusion axioms to make sure that the universal role o indeed reaches all nominals oi occurring in the input concept and terminology.
　finally  we did not choose to make a unique name assumption  i.e.  two nominals might refer to the same individual. however  the inference algorithm presented below can easily
be adapted to the unique name case by a suitable initialisation
.
of the inequality relation =1 .
1	a tableau for shoq d 
for ease of presentation  we assume all concepts to be in negation normal form  nnf . each concept can be transformed into an equivalent one in nnf by pushing negation inwards  making use of demorgan's laws and the following equivalences:
  r.ct.d	《《 1  nr.cr.t.  dc 
  
    n + 1 r.c      1r.c 
   
    r.ct.d n + 1 《《r.c  r.t.   dc
《
《	c 1u  nr.cc  
《we use ゛c to denote the nnf of  c. moreover  for a concept d  we use cl d  to denote the set of all subconcepts of d  the nnf of these subconcepts  and the  possibly negated  datatypes occurring in these  nnfs of  subconcepts.
definition 1 if d is a shoq d -concept in nnf  r a role box  and r are the sets of abstract and concrete roles occurring in d or r  a tableau t for d w.r.t. r is defined to be a quadruple  s l ea ed  such that: s is a set of individuals  l : s ★ 1cl d  maps each individual to a set of concepts which is a subset of cl d   ea : rda ★ 1s〜s maps each abstract role in rda to a set of pairs of individuals  ed : rdd ★ 1s〜 d maps each concrete role in rdd to a set of pairs of individuals and concrete values  and there is some individual s （ s such that d （ l s . for all s t （ s 
  and
st s c  := {t （ s | hs ti （ ea s  and c （ l t } 
it holds that:
 p1  if c （ l s   then  c /（ l s  
 p1  if c1 u c1 （ l s   then c1 （ l s  and c1 （ l s  
 p1  if c1 t c1 （ l s   then c1 （ l s  or c1 （ l s  
 p1  if hs ti （ ea r  and r v* s  then hs ti （ ea s   if hs ti （ ed t  and t v* t1  then hs ti （ ed t1 
 p1  if  r.c （ l s  and hs ti （ ea r   then c （ l t  
 p1  if  r.c （ l s   then there is some t （ s such that hs ti （ ea r  and c （ l t  
 p1  if  s.c （ l s  and hs ti （ ea r  for some r v* s with trans r   then  r.c （ l t  
 p1  if   ns.c  （ l s   then  st s c    n 
 p1  if  1ns.c  （ l s   then  st s c  1 n  and
 p1  if { 1ns.c   ゛ ns.c } ” l s  =1   and hs ti （ ea s   then {c  c} ” l t  =1   
 p1  if o （ l s  ” l t   then s = t 
 p1  if  t.d （ l s  and hs ti （ ed t   then t （ dd 
 p1  if  t.d （ l s   then there is some t （  d such that hs ti （ ed t  and t （ dd.
lemma 1 a shoq d -concept d in nnf is satisfiable w.r.t. a role box r iff d has a tableau w.r.t. r.
proof: we concentrate on  p1  to  p1   which cover the the new logical features  i.e.  nominals and datatypes; the remainder is similar to the proof found in  horrocks et al.  1 . roughly speaking  we construct a model i from a tableau by taking s as its interpretation domain and adding the missing role-successorships for transitive roles. then  by induction on the structure of formulae  we prove that  if c （ l s   then s （ ci.  p1  ensures that nominals are indeed interpreted as singletons  and  p1  and  p1  make sure that concrete datatypes are interpreted correctly.
　for the converse  each model is by definition of the semantics a tableau.	1
1	a tableau algorithm for shoq d 
from lemma 1  an algorithm which constructs a tableau for a shoq d -concept d can be used as a decision procedure for the satisfiability of d with respect to a role box r. such an algorithm will now be described in detail. please note that  due to the absence of inverse roles  subset blocking is sufficient  see also  baader and sattler  1   to ensure termination and correctness.
definition 1 let r be a role box  d a shoq d -concept in nnf  rda the set of abstract roles occurring in d or r  and id the set of nominals occurring in d. a completion forest for d with respect to r is a set of trees f where each node x of the forest is labelled with a set
l x    cl d  “ {● r o  | r （ rad and o （ id} 
and each edge hx yi is labelled with a set of role names l hx yi  containing roles occurring in cl d  or r. additionally  we keep track of inequalities between nodes of the .
forest with a symmetric binary relation =1 between the nodes of f. for each o （ id there is a distinguished node xo in f such that o （ l x . we use ● r o  （ l y  to represent an
r labelled edge from y to xo.
　given a completion forest  a node y is called an rsuccessor of a node x if  for some r1 with r1 v* r  either y is a successor of x and r1 （ l hx yi   or ● r1 o  （ l x  and y = xo. ancestors and roots are defined as usual.
for a role s and a node x in f we define sf x c  by
sf x c  := {y | y is an s-successor of x and c （ l y }.
　a node x is directly blocked if none of its ancestors are blocked  and it has an ancestor x1 that is not distinguished such that l x    l x1 . in this case we will say that x1 blocks x. a node is blocked if is directly blocked or if its predecessor is blocked.
for a node x  l x  is said to contain a clash if
1. for some concept name a （ nc  {a  a}   l x  
1. for some role s   1ns.c  （ l x  and there are n + 1
s-successors y1 ... y. n of x with c （ l yi  for each 1 ＋ i ＋ n and yi =1 yj for each 1 ＋ i   j ＋ n 
1. l x  contains  possibly negated  datatypes d1 ... dn such that is empty  or if
.
1. for some o （ l x   x =1	xo.
　if o1 ... o` are all individuals occurring in d  the algorithm initialises the completion forest f to contain ` + 1 root nodes x1 xo1 ... xo` with l. x1  = {d} and l xoi  =
{oi}. the inequality relation =1 is initialised with the empty relation. f is then expanded by repeatedly applying the rules from figure 1  stopping if a clash occurs in one of its nodes.
　the completion forest is complete when  for some node x  l x  contains a clash  or when none of the rules is applicable. if the expansion rules can be applied in such a way that they yield a complete  clash-free completion forest  then the algorithm returns  d is satisfiable w.r.t. r   and  d is unsatisfiable w.r.t. r  otherwise.
lemma 1 when started with a shoq d  concept d in nnf  the completion algorithm terminates.
proof: let m = |cl d |  k = |rda|  n the maximal number in atleast number restrictions  and ` = |id|. termination is a consequence of the following properties of the expansion rules:  1  each rule but the 1- or the o-rule strictly extends the completion forest  by extending node labels or adding nodes  while removing neither nodes nor elements from node.  1  new nodes are only generated by the  - or the  -rule as successors of a node x for concepts of the form  r.c and   ns.c  in l x . for a node x  each of these concepts can trigger the generation of successors at most once-even though the node s  generated was later removed by either the 1- or the o-rule. if a successor y of x was generated for a concept  s.c （ l x   and y is removed later  then there will always be some s-successor z of x such that c （ l z   and hence the  -rule cannot be applied again to x and  s.c.
u-rule: if c1 u c1 （ l x   x is not blocked  and {c1 c1} 1  l x   then l x  = l x  “ {c1 c1}
t-rule: if c1 t c1 （ l x   x is not blocked  and {c1 c1} ” l x  =    then l x  = l x  “ {c} for some c （ {c1 c1}
 -rule: if  r.c （ l x    or  t.d （ l x   x is not blocked  and x has no r-successor y with c （ l y 
　　　　 resp. no t-successor y with d （ l y    then create a new node y with l hx yi  = {r} and l y  = {c}  resp. with l hx yi  = {t} and l y  = {d}   -rule: if  r.c （ l x   or  t.d （ l x    x is not blocked  and there is an r-successor y of x with c /（ l y  
　　 resp. a t-successor y of x with d 1（ l y    then l y  = l y  “ {c}  resp. l y  = l y  “ {d} 
 +-rule: if  s.c （ l x   x is not blocked  and there is some r with trans r  and r v* s  and an r-successor y of x with  r.c /（ l y  
	then l y  = l y  “ { r.c}	゛
choose-rule: {  ns.c   1ns.c } ” l x  =1	   x is not blocked  and゛	y is an s-successor of x with {c 	c} ” l y  =    then l y  = l y  “ {e} for some e （ {c 	c}
 -rule: if   ns.c.   （ l x   x is not blocked  and there are no n s-successors y1 ... yn of x with c （ l yi  and yi =1 yj for 1 ＋ i   j ＋ n 	. then create n new nodes y1 ... yn with l hx yii  = {s}  l yi  = {c}  and yi =1 yj for 1 ＋ i   j ＋ n.
	1-rule: if  1ns.c  （ l x   x is not blocked  and x has n + 1.	s-successors y1 ... yn with c （ l yi  for
	each 1 ＋ i ＋ n  and there exist i =1	j s. t. not.	yi 1=	yj and  if only one of.	yi yj is distinguished  then it is yi 
then 1. l yi  = l yi  “ l yj  and add y =1	yi for each y with y =1	yj  and if both yi  yj are not distinguished  then 1. l hx yii  = l hx yii  “ l hx yji 
if yi is and yj is not distinguished  then 1. l x  = l x  “ {● s o  | s （ l hx yji } for some o （ l yi 
and 1. remove yj and all edges leading to yj from the completion forest.
o-rule: if o （ l x   x is neither blocked nor distinguished  and not x =1	xo then  for z distinguished with o （ l z   do 1. l z  = l z  “ l x   and
1. if x has a predecessor x1  then. l x1  = l x1  “ {● r o  | r （ l hx1 xi } 
.
1. add y =1	z for each y with y =1	x  and remove x and all edges leading to x from the completion forestfigure 1: the complete tableaux expansion rules for shoq d 　for the  -rule  if y1 ... yn were generated by an appli-. cation of the  -rule for a concept   ns.c   then yi =1 yj is added for each i =1 j. this implies that there will always be n s-successorssince neither the 1.-rule nor the o-rule ever merges two nodes with  and  whenever the 1- or the o-rule removes a successor of x  there will be some s-successor z of x that  inherits  all inequalities from yi1.
hence the out-degree of the forest is bounded by nm.
 1  nodes are labelled with subsets of cl d “{● r o  | r （ rda and o （ id}  so there are at most 1m+k` different node labellings. therefore  if a path p is of length at least 1m+k`  then  from the blocking condition in definition 1  there are two nodes x y on p such that x is directly blocked by y. hence paths are of length at most 1m+k`. 1
lemma 1 if a shoq d  concept d in nnf has a tableau w.r.t. r  then the expansion rules can be applied to d and r such that they yield a complete  clash-free completion forest.
proof: again  we concentrate on the new features nominals and datatypes and refer the reader to  horrocks et al.  1  for the remainder. given a tableau t for d w.r.t. r  we can apply the non-deterministic rules  i.e.  the t-  choose  and 1-rule  in such a way that we obtain a complete and clashfree tableau: inductively with the generation of new nodes  we define a mapping π from nodes of the completion forest to individuals in the tableau and concrete values in such a way that l x    l π x   for π x  （ s and  for each pair of nodes x y and each  abstract or concrete  role r  if y is an r-successor of x  then hπ x  π y i （ ea r  or hπ x  π y i （ ed r . please note that the latter also holds in the case that y is not a successor of x but a distinguished node  i.e.  ● r o  （ l x  and y = xo   and in the case that y is a concrete value  i.e.  π y  1（ s . due to  p1  and  p1   we do not encounter a clash of the form  1   and  p1  makes sure that the o-rule can be applied correctly. 1
lemma 1 if the expansion rules can be applied to a shoq d  concept d in nnf and a role box r such that they yield a complete and clash-free completion forest  then d has a tableau w.r.t. r.
proof: from a complete and clash-free completion forest f  we can obtain a tableau t =  s l1 ea ed  by unravelling as usual. that is  each element of the tableau is a path in the completion forest that starts at one of the root nodes and that  instead of going to a blocked node  goes to the node that is blocking this node  we disregard nodes that have datatypes in their labels . e-successorship for abstract roles is defined according to the labels of edges  i.e.  if r1 （ l hxn xn+1i  in
f with r1 v* r  then hx1 ...xn x1 ...xnxn+1i （ ea r  in t  and following labels ● r o   i.e.  if ● r o  （ l xn  in f  then hx1 ...xn xoi （ ea r  . e-successorship for concrete roles is defined following the edges to those  disregarded  nodes with datatypes in their labels. clash-freeness makes sure that this is possible.
　to satisfy  p1  also in cases where two r-successors y1 y1 of a node x with   nr.c  are blocked by the same node z  we must distinguish between individuals that  instead of going to yi  go to z. this can be easily done as in  horrocks et al.  1   annotating points in the path accordingly. finally  we set l1 x1 ...xn  = l xn .
　it remains to prove that t satisfies each  pi .  p1  to  p1  are similar to those in  horrocks et al.  1 .  p1  is due to completeness  otherwise  the o-rule was applicable   which implies that nominals can be found only in the labels of distinguished nodes  note that the definition of blocking is such that a distinguished node can never block another one .  p1  and  p1  are due to the fact that f has no clash of form  1   and that the  - and  -rule are not applicable. 1 as an immediate consequence of lemmas 1  1  1  and 1  the completion algorithm always terminates  and answers with  d is satisfiable w.r.t. r  iff d is satisfiable w.r.t. r. next  subsumption can be reduced to  un satisfiability. finally  as we mentioned in section 1  shoq d  can internalise general concept inclusion axioms  and we can thus decide these inference problems also w.r.t. terminologies.
theorem 1 the completion algorithm presented in definition 1 is a decision procedure for satisfiability and subsumption of shoq d  concepts w.r.t. terminologies.
1	conclusion
as we have seen  ontologies are set to play a key role in the  semantic web  where they will provide a source of shared and precisely defined terms for use in descriptions of web resources. moreover  such descriptions should be amenable to automated reasoning if they are to be used effectively by automated processes.
　we have presented the dl shoq d   along with a sound and complete decision procedure for concept satisfiability/subsumption. with its support for both nominals and concrete datatypes  shoq d  is well suited to the provision of reasoning support for ontology languages in general  and web based ontology languages in particular. in addition  the shoq d  decision procedure is similar to the shiq decision procedure implemented in the highly successful fact system  and should be amenable to a similar range of performance enhancing optimisations.
　the only feature of languages such as oil and daml  and shiq  that is missing in shoq d  is inverse roles. its exclusion was motivated by the very high complexity of reasoning that results from the unconstrained interaction of inverse roles with nominals and datatypes. future work will include a detailed study of this interaction with a view to providing  restricted  support for inverse roles without triggering the explosion in complexity. an implementation  based on the fact system  is also planned  and will be used to test empirical performance.
references
 areces et al.  1  c. areces  p. blackburn  and m. marx. the computational complexity of hybrid temporal logics. logic journal of the igpl  1. to appear.
 baader and hanschke  1  f. baader and p. hanschke. a scheme for integrating concrete domains into concept languages. in proc. of ijcai-1  pages 1  1.
 baader and sattler  1  f. baader and u. sattler. tableau algorithms for description logics. in proc. tableaux 1  vol. 1 of lnai  pages 1  1.
 berners-lee  1  t. berners-lee. weaving the web. orion business books  1.
 biron and malhorta  1  xml schema part 1: datatypes. w1c candidate recommendation  oct 1. http:// www.w1.org/tr/xmlschema-1/.
 blackburn and seligman  1  p. blackburn and j. seligman. what are hybrid languages  in advances in modal logic  vol. 1  pages 1. csli publications  1.
 corcho and perez  1＞   o. corcho and a. gomez p＞ erez.＞ evaluating knowledge representation and reasoning capabilities of ontology specification languages. in proc. of ecai-1 workshop on applications of ontologies and problem-solving methods  1.
 de giacomo  1  g. de giacomo. decidability of classbased knowledge representation formalisms. phd thesis  universita degli studi di roma  la sapienza   1.`
 decker et al.  1  s. decker et al. the semantic web - on the respective roles of xml and rdf. ieee internet computing  1.
 fensel et al.  1  d. fensel et al. oil in a nutshell. in proc. of ekaw-1  lnai  1.
 hendler and mcguinness  1  j. hendler and d. l. mcguinness. the darpa agent markup language. ieee
intelligent systems  1.
 horrocks et al.  1  i. horrocks  u. sattler  and s. tobies. practical reasoning for expressive description logics. in proc. of lpar'1  vol. 1 of lnai  1.
 horrocks  1  i. horrocks. benchmark analysis with fact. in proc. tableaux 1  vol. 1 of lnai 
1.
 lutz  1  c. lutz. nexptime-complete description logics with concrete domains. in proceedings of the esslli1 student session  1.
 mcguinness  1  d. l. mcguinness. ontological issues for knowledge-enhanced search. in proc. of fois-1. ios-press  1.
 schaerf  1  a. schaerf. reasoning with individuals in concept languages. data and knowledge engineering  1 :1  1.
 streett  1  r. s. streett. propositional dynamic logic of looping and converse is elementarily decidable. information and computation  1-1  1.
 tobies  1  s. tobies. the complexity of reasoning with cardinality restrictions and nominals in expressive description logics. jair  1-1  1.
 uschold and gruninger  1：   m. uschold and m. gruninger.： ontologies: principles  methods and applications. knowledge eng. review  1   1.
 van heijst et al.  1  g. van heijst  a. th. schreiber  and b. j. wielinga. using explicit ontologies in kbs development. int. j. of human-computer studies  1/1   1.

the	family: extensions of simple conceptual graphs

jean-franc ois baget lirmm
1  rue ada
1 montpellier  cedex 1
france
http://www.lirmm.fr/ baget/ marie-laure mugnier
lirmm
1  rue ada
1 montpellier  cedex 1
france
http://www.lirmm.fr/ mugnier/

abstract
we introduce the family of graph-based knowledge representation and reasoning models  basically extensions of the simple conceptual graphs model. objects of these models are colored simple graphs and are used to represent facts  rules and constraints. reasonings are based on graphtheoretic mechanisms  mainly graph homomorphism. models of this family are defined by the kind of objects composing a knowledge base. in this paper  we focus on the formal definitions of these models  including their operational semantics and relationships with fol  and we study their decidability properties and computational complexity.
1	introduction
conceptual graphs  cgs  have been proposed in  sowa  1  as a knowledge representation and reasoning model  mathematically founded on logics and graph theory. though mainly studied as a graphical interface for logics or as a diagrammatic system of logics  their graph-theoretic foundations have been less investigated. most works in this area are limited to simple conceptual graphs  simple graphs or sgs   sowa  1; chein and mugnier  1   corresponding to the positive  conjunctive and existential fragment of fol. this model has three fundamental characteristics:
1. objects are bipartite labelled graphs  nodes represent entities and relations between these entities ;
1. reasonings are based on graph-theoretic operations  mainly a graph homomorphism called projection;
1. it is logically founded  reasonings being sound and complete w.r.t. a fol semantics called .
　main extensions of the sg model  keeping projectionbased operations and sound and complete semantics  are inference rules  gosh and wuwongse  1; salvat  1  and nested graphs  chein et al.  1 ; for general cgs   kerdiles  1  introduces an original deduction system  combining analytic tableaux with projection.
　we present here a family of extensions of the simple graphs model. the common ground for these extensions is that objects are colored simple graphs representing facts  rules or constraints  and operations are based upon projection; the deduction problem asks  given a knowledge base and a simple graph  which may represent a query  a goal  ...   whether can be deduced from . according to the kinds of objects considered in   different reasoning models are obtained  composing the family. in this paper  we focus on the formal definitions of these models  including their operational semantics and relationships with fol  and we study their decidability properties and computational complexity.
　in section 1 basic definitions and results about simple graphs are recalled. section 1 presents an overview of the family. in particular  we explain why we consider sgs graphical features as essential for knowledge modeling and point out that these properties are preserved in the family. in next sections we study the different members of the family.
1	basic notions: the	model
basic ontological knowledge is encoded in a structure called a support. factual knowledge is encoded into simple graphs  sgs   which are bipartite labelled multigraphs  there can be several edges between two nodes . elementary reasonings are computed by a graph homomorphism called projection.
definition 1  support  a	support	is	a	1-tuple
　　　　　　. and are two partially ordered finite sets  respectively of concept types and relation types. relation types may be of any arity greater or equal to 1. is the set of individual markers and is a mapping from to . we denote by the generic marker  where . the partial order on considers elements of as pairwise non comparable  and as its greatest element.
definition 1  simple graph  a simple graph  defined on a support   is a bipartite multigraph   where
　　　　　　. and are the sets of concept nodes and relation nodes  is the set of edges. edges incident on a relation node are numbered from 1 to the degree of the node. we denote by the neighbor of a relation node in
　. each node has a label given by the mapping . a concept node is labelled by a couple  type c   marker c    where type c  is an element of   called its type  and marker c  is an element of   called its marker. if marker c  = m is an individual marker  then type c = . a relation node is labelled by type r   an element of   called its type  and the degree of must be equal to the arity of type r .

figure 1: simple graphs.
　in the drawing of a sg  concept nodes are represented by rectangles and relation nodes by ovals. generic markers are omitted. and since in our examples we use binary relations only  numbers on edges are replaced by directed edges: a relation node is incident to exactly one incoming and one outgoing edge. fig. 1 shows two  connected  simple graphs assumed to be defined over the same support.
　simple graphs correspond to the existential conjunctive and positive fragment of fol  by the semantics  . types are mapped to predicates and individual markers to constants. a set of formulas is assigned to any support   translating partial orders on types. given any sg   a formula is built as follows. a term is assigned to each concept node: a distinct variable for each generic node  and the constant corresponding to its marker otherwise. an atom
　　 resp.   is associated to each concept node  resp. relation node of arity    where is the type of the node  and  resp.   is the term assigned to this node  resp. assigned to  . let be the conjunction of these atoms. is the existential closure of . e.g. the formula assigned to the dashed subgraph of in fig. 1 is
	works-with	.
definition 1  projection  let and be two sgs defined on a support   a projection from into is a mapping
from to and from to that preserves edges and may decrease node labels:
1. ; and if	  then
;
1.  if is a concept node  is the product of the orders on and  
	i.e.	and
.
　we note   subsumes   if there exists a projection from into . typically  represents a query  a fact  and projections from to define answers to . in fig. 1  suppose researcher person  then there is one projection from into . the image of by this projection is the dashed subgraph of . projection is sound and complete w.r.t. the semantics   up to a normality condition for completeness; the normal form of a sg is the sg nf obtained by merging concept nodes having the same individual marker. this sg always exists  and is computable in linear time with a naive algorithm . then: let and be two s. nf	 chein and mugnier 
1    gosh and wuwongse  1 .
　for the sake of brevity  we consider in what follows that sgs  and more complex constructs built upon sgs  are given in normal form  and put into normal form if needed after a modification. and  since a sg needs not be a connected graph  we confuse a set of sgs with the sg obtained by performing the disjoint union of its elements. in following definition for instance  the sg represents a set of sgs.
definition 1   deduction  let and be two sgs. can be deduced from if .
　the deduction problem is np-complete  chein and mugnier  1 . note that the subsumption relation induced by projection over sgs is a quasi-order. two graphs are said to be equivalent if they project to each other. a sg is said to be redundant if it is equivalent to one of its strict subgraphs. redundancy checking is an np-complete problem. each equivalence class admits a unique  up to isomorphism  non redundant graph  chein and mugnier  1 .
1	overview of the	family
from a modeling viewpoint  the simple graphs model has two essential properties. the objects  simple graphs  are easily understandable by an end-user  a knowledge engineer  or even an expert . and reasonings are easily understandable too  for two reasons: projection is a graph matching operation  thus easily interpretable and visualisable; and the same language is used at interface and operational levels. it follows that reasonings can be explained in a natural manner to the user  step by step  and directly on his own modelization  see for instance the knowledge engineering application described in  bos et al.  1  or experiments in document retrieval done by  genest  1   where in both cases the representation language is at the expert level . the family keeps these essential properties.
　let us now informally present the most general model of the family. throughout this section  we will use examples inspired from a modelization of a knowledge acquisition case study  called sysiphus-i: it describes a resource allocation problem  where the aim is to assign offices to persons of a research group while fulfilling constraints  baget et al.  1 .
　simple graphs are the basic constructs  upon which more complex constructs  rules and constraints  are defined  and operations are based on projection. a rule expresses a knowledge of form  if holds then can be added . it is encoded into a simple graph provided with two colors  highlighting the hypothesis and the conclusion of the rule. in drawings  we represent the hypothesis by white nodes  and the conclusion by gray ones. rules are used in the following way: if the hypothesis of a rule can be projected into a graph  then the rule is applicable to the graph  and its conclusion can be added to the graph according to the projection. the rule of fig. 1 can be understood as  for all persons and   if works with   then works with  . it can be applied to of fig. 1  adding a relation node  work-with  with predecessor  researcher: k  and successor  researcher .
　a constraint can be a positive constraint or a negative constraint  expressing a knowledge of form  if holds  so must    or  if holds  must not . it is also a bicolored simple graph: the first color defines the condition part  and the

figure 1: colored sgs
second color the mandatory  or forbidden  part. a graph satisfies a positive constraint if each projection from the condition part of into can be extended as a projection of the whole . and satisfies a negative constraint if no projection of into can be extended as a projection of the whole . fig. 1 represents the negative constraint  two persons working together should not share an office . the graph of fig 1 does not satisfy this constraint because  there is a researcher who works with researcher k   projection of the condition part of    and they share office #1   extension of the projection to a projection of the whole  .
when constraints are involved  we distinguish between two duced from  we note  . if we impose some of the sets   or to be empty  one obtains specific reasoning models. note that in the absence of constraints      inference and evolution rules have the same behavior  thus and can be confused. the family is then composed of the six following models.
themodel for=themodel for=themodel for=themodel for=themodel for=the	model for	=kinds of rules: inference rules of form  if	thenpsfragaddreplacements  and　since a fact has the same semantics as a rule with an empty hypothesis  the set is only used in models names when both rule sets and are empty. the hierarchy of these models is represented in fig. 1. it highlights the decidability and complexity of the associated deduction problems.

evolution rules of form  if then add   except if it brings inconsistency . now  a knowledge base contains four sets representing different kinds of knowledge: a set of simple graphs encoding factual knowledge  a set of inference rules  a set of evolution rules and a set of constraints.
　let us outline the deduction problem: the problem takes in input a knowledge base  kb  and a goal expressed as a sg  and asks whether there is a path of consistent worlds evolving from the initial one to a world satisfying the goal. factual knowledge describes an initial world; inference rules represent implicit knowledge about worlds; evolution rules represent possible transitions from one world to other worlds; constraints define consistency of worlds; a successor of a consistent world is obtained by an evolution rule application; solving the problem consists in finding a path of consistent worlds evolving from the initial one to a world satisfying the goal.
　in the particular case of the sysiphus-i modelization  and describe initial information  office locations  persons and group organization   represents allocation constraints  consists of one evolution rule:  wheneverthere are a person and an office  try to assign this office to this person . the goal represents a situation where each person of the group has an office. a solution to the problem is a world obtained from the initial one by a sequence of office assignments  where each person has an office  while satisfying allocation constraints.
let us now specify definitions and notations concerning the family. rules and constraints are defined as colored sgs.
definition 1  colored sgs  a colored simple graph is a pair where	is a sg and	is a mapping from
into . the number associated to a node is called the color of the node. we denote by the subgraph of induced by -colored nodes. the subgraph must form a sg  i.e. the neighbors of a relation node of the hypothesis must also belong to the hypothesis .
　a kb is denoted by . given a kb and a goal   the deduction problem asks whether can be de-

	figure 1: reasoning models hierarchy for the	family
1	sgs and rules: the	model
a simple graph rule  sg rule  embeds knowledge of form  if then  . the following definition is equivalent to the more traditional one  two sgs related with coreference links  used in  gosh and wuwongse  1; salvat  1 .
definition 1  sg rules  a simple graph rule is a colored sg. is called its hypothesis  and its conclusion.
　deduction depends on the notion of a rule application: it is a graph transformation based upon projection.
definition 1  application of a sg rule  let be a sg  and be a sg rule. is applicable to if there exists a projection  say   from  the hypothesis of   into . in that case  the result of the application of on according to is the graph obtained by making the disjoint union of
and of a copy of  the conclusion of    then  for every edge   where and   adding an edge with the same number between and the copy of . is said to be an immediate -derivation from .
　a derivation is naturally defined as a  possibly empty  sequence of rule applications:
definition 1  derivation  let	be a set of sg rules  and be a sg. we call	-derivation from	to a sg	a sequence
of sgs such that  for   is an immediate -derivation from   where .
　to deduce a sg   we must be able to derive a sg into which can be projected  hence the following definition:
definition 1  deduction in	  letbe a kband let	be a sg.	can be deduced from notation       if there exists an	-derivation from that	.to a sg	suchthe semantics is extended to translate sg rules: let and be two sgs  s.t. and is the
sg obtained from by adding the neighbors of the relation nodes of which are concept nodes of . then
  where
   are the variables of and are the variables of that do not appear in . the following soundness and completeness result is obtained:
	 salvat  1 . the	deduc-
tion problem is semi-decidable  coulondre and salvat  1 .
1	sgs and constraints: the	model
let us now introduce constraints  which are used to validate knowledge. in presence of constraints  deduction is defined only on a consistent knowledge base.
definition 1  constraints  a positive  resp. negative  constraint is a colored sg. is called the trigger of the constraint  is called its obligation  resp. interdiction . a sg -violates a positive  resp. negative  constraint if is a projection of the trigger of into the non redundant form of  resp. into   that cannot be extended  resp. that can be extended  to a projection of as a whole. violates if it -violates for some projection . otherwise  satisfies .
　notice there may be two equivalent sgs  such that one satisfies a positive constraint and the other does not. e.g. given a constraint   take satisfying   and the equivalent  redundant  graph obtained by making the disjoint union of and the trigger of . violates . we have thus chosen to define constraint satisfaction w.r.t. the non redundant form of a sg. this problem does not occur with negative constraints. two constraints and are said equivalent if  for every graph   violates iff violates . any negative constraint is equivalent to the negative constraint obtained from by coloring all its nodes by 1. furthermore  negative constraints are indeed a particular case of positive ones: consider the positive constraint obtained from a negative one by coloring all nodes of by 1  then adding a concept node typed notthere  colored by 1  where notthere is incomparable with all other types and does not appear anywhere excepted in . then a graph violates if and only if it violates . positive constraints strictly include negative constraints  in the sense that the associated consistency problems are not in the same complexity class  see theorem 1 .
　let us relate our definitions to other definitions of constraints found in the cg literature. the constraints of  mineau and missaoui  1  correspond to a particular case of our constraints where the trigger and the obligation are not connected. in turn  our constraints are a particular case of the minimal descriptive constraints of  dibie et al.  1   where the disjunctive part is restricted to one graph.
definition 1  consistency/deduction in   a kb is consistent if satisfies all constraints of . a sg can be deduced from if is consistent and can be
deduced from	.
　note that a that violates a constraint of may still be deduced from . it does not matter since is a partial representation of knowledge deducible from . how can we translate the notion of consistency into fol  for negative constraints  the correspondence is immediate  and relies on projection soundness and completeness.
theorem 1 a sg	violates a negative constraint
       iff   where is the sg underlying .
　consistency relative to positive constraints can be explained with fol  translating  projection  into a notion of logical  substitution  between the formulas associated to graphs  see for instance the s-substitution of  chein and mugnier  1  . another bridge can be built using rules. indeed  a graph satisfies a positive constraint if and only if  considering as a rule  all applications of on produce a graph equivalent to . or  more specifically:
property 1 a sg -violates a positive constraint iff  considering as a rule  the application of on according to produces a graph not equivalent to .
　using soundness and completeness of the deduction  and property 1  one obtains the following relation with fol:
theorem 1 a sg violates a positive constraint iff there is a sg such that	and not
	  where	is the translation of
considered as a rule.
　the problem  does a given graph satisfy a given constraint   is co-np-complete if this constraint is a negative constraint  we must check the absence of projection   but becomes -complete for a positive one   is co-npnp .
theorem 1  complexity in	  consistency in	is
   -complete  but is co-np-complete if all constraints are negative .
sketch of proof: -consistency belongs to since it corresponds to the language	  where encodes an instance and iff is a projection from into   is a projection
from	into	s.t.	. now  let us consider the
   -complete problem : given a boolean formula   and a partition of its variables  is it true that for any truth assignment for the variables in there exists a truth assignment for the variables in s.t. is true  we first show that the special case where is an instance of 1-sat remains
   -complete  let us call this problem 1-sat  . for that we use the polynomial transformation from any boolean formula to a set of clauses described in   papadimitriou  1   example 1   followed by a polynomial transformation from a clause to clauses with 1 literals. the  new  variables are put in the set . we then reduce 1-sat to -consistency. is mapped to a constraint : briefly said  there is one concept node  t:*  for each variable and one ternary relation node with type for each clause . is composed of all concept nodes coming from variables in . is composed of two individual concept nodes  t:1  and  t:1  corresponding to the truth values and  for each clause there is one relation node of type for each triple of truth values giving the value true to . note that  since does not contain any relation node  any truth assignment for the variables of is a projection from to   and reciprocally.
corollary 1 deduction in	is	-complete.
note that the theorem 1 can be used to show that consistency of minimal descriptive constraints in  dibie et al.  1  is also -complete.
1	rules and constraints:
the two kinds of rules  inference rules	and evolution rules
   define two alternative models. in   is seen as the initial world  root of a potentially infinite tree of possible worlds  and describes the possible evolutions from one world to others. the deduction problem asks whether there is a path
of consistent worlds from	to a world satisfying.definition 1  deduction in	  letbe akb  and let	be a sg.	can be deduced fromif there isan	-derivation	such that  for 	is consistent and	can be deduced from.　in   provided with is a finite description of a potentially infinite world  that has to be consistent. applying a rule to can create inconsistency  but a further application of a rule may restore consistency. let us formalize this notion of consistency restoration. suppose there is a -violation of a positive constraint in ; this violation is said to be -restorable if there exists an -derivation from into such that the projection of the trigger of into can be extended to a projection of as a whole. the violation of a negative constraint can never be restored. note that the -restoration can create new violations  that must themselves be proven -restorable.
definition 1  consistency/deduction in   a kb is consistent if  for any sg that can be -derived from   for every constraint   for every -violation of in   is -restorable. a sg can be deduced from if is consistent and can be deduced from .
　consider for instance a kb containing the sg in fig. 1  expressing the existence of the number 1  a constraint and a rule  both represented by the colored sg . the constraint asserts that for every integer   there must be an integer   successor of . if the rule is an evolution rule  is seen as an inconsistent initial world  there is no successor of 1 in   and nothing will be deduced from this kb. if the rule is an inference rule  its application immediately repairs the constraint violation  while creating a new integer  that has no successor  thus a new violation. finally  every constraint violation could eventually be repaired by a rule application  and the kb should be proven consistent.
　let us point out that the model is obtained from or when is empty  and is obtained from  resp.   when  resp.   is empty.

integer: zero	a simple graph g	a colored sg k
figure 1: consistency in
theorem 1  complexity in	  deduction in
is semi-decidable. consistency and deduction in are truly undecidable.
proof: includes thus -deduction is not decidable. when is deducible from   a breadth-first search of the tree of all derivations from   each graph being checked for consistency  ensures that is found in finite time. for
　　  we show that checking consistency is truly undecidable. let be a kb where contains a positive constraint and a negative constraint   both with an empty trigger. for proving consistency  one has to prove that   and the algorithm does not stop in this case  from semidecidability of deduction in  . the same holds for the complementary problem  proving inconsistency  taking instead of   hence the undecidability.
1	combining inference and evolution
the model combines both derivation schemes of the and models. now  describes an initial world  inference rules of complete the description of any world  constraints of evaluate the consistency of a world  evolution rules of try to make evolve a consistent world into a new  consistent one. the deduction problem is: can evolve into a consistent world satisfying the goal 
definition 1  deduction in   a sg is an immediate -evolution from a sg if there exists an -derivation from into and an immediate -derivation from into . an -evolution from a sg to a sg is a sequence of sgs such that  for   is an immediate -evolution from . given a kb
                 a sg	can be deduced from	if there is an	-evolution	where  for	  is consistent  and	can be deduced from	.
　when   one obtains the model  there is only one world . when   one obtains  information contained in a world is complete . as a generalization of
　　  deduction in is truly undecidable. let us now consider a decidable fragment of  which in particular was sufficient for the sysiphus-i modelization . first note a rule has only to be applied once according to a given projection: further applications with this projection obviously produce redundant information. such applications are said to be useless. a sg is said to be closed w.r.t. a rule if all applications of on are useless. given a set of rules   we note  when it exists   the closure of w.r.t.   the smallest graph derived from that is closed w.r.t. every rule in . when it exists is unique. is called a finite expansion set  f.e.s.  if  for every sg   its closure exists  and thus can be computed in finite time . if is a finite expansion set  the deduction problem in the model becomes decidable  but it is not a necessary condition for decidability .
property 1  finite expansion sets  let	be a
kb where is a finite expansion set. then is consistent iff is consistent  and a sg can be deduced from iff can be deduced from .
the following decidable case is based on property 1.
property 1  decidable case  deduction in is semidecidable if is a f.e.s. and is decidable if is a f.e.s.
proof: suppose is a f.e.s. when can be deduced  one obtains an answer in finite time; we proceed as for  see proof of theorem 1  but consistency checks are done on the graph closure instead of the graph itself. now  if is a f.e.s.  exists  thus the derivation tree is finite  and consistency checks may only cut some parts of this tree.
　a rule is said to be range restricted  by analogy with the so-called rules in datalog  where all variables of the head must appear in the conclusion  if no generic concept node belongs to its conclusion. then:
property 1 a set of range restricted rules is a f.e.s.
proof: since all graphs are put into normal form  an individual marker appears at most once in a graph. then the closure of a sg can be obtained with a derivation of length   where is the size of and is the greatest arity of a relation type appearing in a rule conclusion.
1	conclusion
we propose a family of models that can be seen as the basis of a generic modeling framework. main features of this framework are the following: a clear distinction between different kinds of knowledge  that fit well with intuitive categories  a uniform graph-based language that keeps essential properties of the sg model  namely readability of objects as well as reasonings. we guess this later point is particularly important for the usability of any knowledge based system. in our framework  all kinds of knowledge are graphs easily interpreted  and reasonings can be graphically represented in a natural manner using the graphs themselves  thus explained to the user on its own modelization.
　technical contributions  w.r.t. previous works on conceptual graphs  can be summarized as follows:
the representation of different kinds of knowledge as colored sgs: facts  inference rules  evolution rules and constraints.
the integration of constraints into a reasoning model; more or less similar notions of a constraint had already been introduced in  dibie et al.  1; mineau and missaoui  1  but were only used to check consistency of a simple graph  as in the model . the complexity of consistency checking was not known.
a systematic study of the obtained family of models with a complexity classification of associated consistency/deduction problems.
　we established links between consistency checking and fol deduction. the operational semantics of models including constraints  namely   and   is easy to understand but there is an underlying non monotonic mechanism whose logical interpretation should require non standard logics. the definition of a logical semantics for these models is an open problem.
acknowledgments
we are indebted to genevie`ve simonet for her careful reading and some error corrections on an earlier version of this paper  and to michel chein  as well as the anonymous reviewers  for their helpful comments.
references
 baget et al.  1  j.-f. baget  d. genest  and m.-l. mugnier. knowledge acquisition with a pure graph-based knowledge representation model - application to the sisyphus-i case study. in proc. of kaw'1  1.
 bos et al.  1  c. bos  b. botella  and p. vanheeghe. modeling and simulating human behaviors with conceptual graphs. in proc. of iccs'1  lnai 1  pages 1- 1. springer  1.
 chein and mugnier  1  m. chein and m.-l. mugnier. conceptual graphs: fundamental notions. revue d'intelligence artificielle  1 :1  1.
 chein et al.  1  m. chein  m.-l. mugnier  and g. simonet. nested graphs: a graph-based knowledge representation model with fol semantics. in proc. of kr'1  pages 1. morgan kaufmann  1.
 coulondre and salvat  1  s. coulondre and e. salvat. piece resolution: towards larger perspectives. in proc. of iccs'1  lnai 1  pages 1. springer  1.
 dibie et al.  1  j. dibie  o. haemmerle＞  and s. loiseau. a semantic validation of conceptual graphs. in proc. of iccs'1  lnai 1  pages 1. springer  1.
 genest  1  d. genest. extension du modele` des graphes conceptuels pour la recherche d'informations. phd thesis  universite＞ montpellier ii  dec. 1.
 gosh and wuwongse  1  b. c. gosh and v. wuwongse. a direct proof procedure for definite conceptual graphs programs. in proc. of iccs'1  lnai 1  pages 1. springer  1.
 kerdiles  1  g. kerdiles. projection: a unification procedure for tableaux in conceptual graphs. in proc. of tableaux'1  lnai 1  pages 1  1.
 mineau and missaoui  1  g. w. mineau and r. missaoui. the representation of semantic constraints in conceptual graphs systems. in proc. of iccs'1  lnai 1  pages 1. springer  1.
 papadimitriou  1  c. h. papadimitriou. computational complexity. addison-wesley  1.
 salvat  1  e. salvat. theorem proving using graph operations in the conceptual graphs formalism. in proc. of ecai'1  1.
 sowa  1  j. f. sowa. conceptual structures: information processing in mind and machine. addison-wesley  1.

knowledge representation
and reasoning
description logics and formal concept analysis

matching under side conditions in description logics 1
franz baader and sebastian brandt
theoretical computer science  rwth aachen email: {baader sbrandt} cs.rwth-aachen.de
ralf kusters：
institute for computer science and applied mathematics  university of kiel email: kuesters ti.informatik.uni-kiel.de

abstract
whereas matching in description logics is now relatively well-investigated  there are only very few formal results on matching under additional side conditions  though these side conditions were already present in the original paper by borgida and mcguinness introducing matching in dls. the present paper closes this gap for the dl aln and its sublanguages: matching under subsumption conditions remains polynomial  while strict subsumption conditions increase the complexity to np.
1	introduction
the traditional inference problems  like subsumption  in description logics  dls  are now well-investigated  which means that there exist complexity results and algorithms for a great variety of dls of differing expressive power  donini et al.  1  as well as optimized implementations of the algorithms for expressive dls  horrocks  1 . in contrast  matching concepts against patterns is a relatively new inference problem in dls  which has originally been introduced in  borgida and mcguinness  1; mcguinness  1  to help filter out the unimportant aspects of large concepts appearing in knowledge bases of the classic system  brachmann et al.  1 . more recently  matching  as well as the more general problem of unification  has been proposed as a tool for detecting redundancies in knowledge bases  baader and narendran  1  and to support the integration of knowledge bases by prompting possible interschema assertions  borgida and kusters  1：	 .
　all three applications have in common that one wants to search a large knowledge base for concepts having a certain  not completely specified  form. this  form  can be expressed with the help of so-called concept patterns  i.e.  concept descriptions containing variables. for example  the pattern d := x u  child. y u female  looks for concepts that restrict the child role to fillers that are female  such as the concept c :=  −1 child  u  child. female u rich . in fact  applying the substitution σ := {x 1★  −1 child   y 1★ rich} to the pattern d yields a concept equivalent to c  i.e.  σ is a solution  matcher  of the matching problem c 《  d.
　this type of matching problems has been investigated in detail for sublanguages of the dls aln and ale in  baader et al.  1  and  baader and kusters  1：    respectively. in particular  it was shown that  for sublanguages of aln  solvable matching problems always have a least matcher  w.r.t. subsumption   which can be computed in polynomial time. for sublanguages of ale  deciding solvability of matching problems modulo equivalence is already np-complete.
　in  borgida and mcguinness  1; mcguinness  1   the expressivity of matching problems was further enhanced by allowing for additional side conditions on the variables  through the as-construct : a  strict  subsumption condition is of the form x v  e  x    e  where x is a variable and e a pattern  and it restricts the matchers to substitutions satisfying σ x  v σ e   σ x    σ e  . using a subsumption condition  the matching problem of the above example can be written more intuitively as x u  child.z 《   −1 child  u  child. female u rich  under the subsumption condition z v  female. one result of this paper is that also more complex sets of subsumption conditions do not extend the expressive power of matching problems  see below . however  they are often more convenient to state. in contrast  strict subsumption conditions cannot always be simulated by pure matching problems. they can  e.g.  be used to avoid trivial matches. for example  the pattern d1 := x u  child.y matches every concept since  child.  《    where the top concept   stands for the set of all individuals . the additional strict subsumption condition y      ensures that we can only match concepts with a real restriction on child.
　the first  rather restricted  formal results on matching under side conditions were given in  baader et al.  1 : it was shown that matching under strict subsumption conditions in the small dl fl1 is already np-hard  and that matching under so-called acyclic subsumption conditions can be reduced to matching without side conditions. however   baader et al.  1  does not give a complexity upper bound for matching under strict subsumption conditions and the reduction for acyclic subsumption conditions given there is exponential.
　this paper investigates in detail matching under side conditions in sublanguages of aln. we will show that matching under subsumption conditions can be reduced in polynomial time to matching without side conditions. in particular  this implies that solvable matching problems under subsumption conditions in sublanguages of aln always have a least matcher  which can be computed in polynomial time. for acyclic strict subsumption conditions  matching is shown to be np-complete in the sublanguages fl〕 and fl  of aln.
1	description logics
concept descriptions are inductively defined with the help of a set of concept constructors  starting with a set nc of concept names and a set nr of role names. in this paper  we consider concept descriptions built from the constructors shown in table 1. in the description logic fl1  concept descriptions are formed using the constructors top-concept      conjunction  c u d   and value restriction   r.c . the description logic fl〕 additionally provides us with the bottom concept  〕   and fl  also allows for primitive negation   p . finally  aln extends fl  with number restrictions  − n r  and  ＋ n r   see table 1 .
　as usual  the semantics of concept descriptions is defined in terms of an interpretation. the domain
 i of i is a non-empty set and the interpretation function ，i maps each concept name p （ nc to a set pi    i and each role name r （ nr to a binary relation ri    i 〜 i. the extension of ，i to arbitrary concept descriptions is defined inductively  as shown in the second column of table 1.
　one of the most important traditional inference services provided by dl systems is computing the subsumption hierarchy. the concept description c is subsumed by the description d  c v d  iff ci   di holds for all interpretations i; c and d are equivalent  c 《 d  iff they subsume each other; c is strictly subsumed by d  c   d  iff c v d and c 1《 d. for all dls listed in table 1  subsumption can be decided in polynomial time using a structural subsumption algorithm  borgida and patel-schneider  1 .
matching in description logics
in order to define concept patterns  we additionally need a set nx of concept variables  which we assume to be disjoint from nc “ nr. informally  an aln-concept pattern is an aln-concept description over the concept names nc “ nx and the role names nr  with the only exception that primitive negation must not be applied to variables. more formally  concept patterns  denoted d d1  are defined using the following syntax rules:
d d1  ★ x | c | d u d1 |  r.d 
where x （ nx  r （ nr  and c is an aln-concept description. for example  if x y are concept variables  r a role name  and a b concept names  then d := aux u r. b u y   is an aln-concept pattern  but  x is not. the notion of a pattern  and also the notions  substitution  and  matching problem  introduced below  can be restricted to sublanguages of aln in the obvious way.
　a substitution σ is a mapping from nx into the set of all aln-concept descriptions. this mapping is extended to concept patterns in the usual way by replacing the occurrences of the variables x in the pattern by the corresponding concept description σ x . for example  if we apply the substitution σ := {x 1★ a u b  y 1★ a} to the pattern d from above  we obtain the description σ d  = a u a u b u  r. b u a . the result of applying a substitution to an aln-concept pattern is always an aln-concept description. note that this would no longer be the case if negation were allowed in front of concept variables.
　subsumption can be extended to substitutions as follows: the substitution σ is subsumed by the substitution τ  σ v τ  iff σ x  v τ x  for all variables x （ nx.
definition 1 let c be an aln-concept description and d an aln-concept pattern. then  c 《  d is an alnmatching problem. the substitution σ is a solution  matcher  of c 《  d iff c 《 σ d .
in the following  we will abbreviate a matching problem of the form c 《  c u d as c v  d. this notation is justified by the fact that σ solves c 《  c u d iff c v σ d .
　a matching problem can either be viewed as a decision problem  where one asks whether the problem is solvable  or as a computation problem  where one asks for actual matchers of this problem  if any . although the computation problem is usually the more interesting one  the decision problem can serve as a starting point for the complexity analysis. in general  matching problems may have several  even an infinite number of  solutions  and thus the question arises which matcher to compute. following  borgida and mcguinness  1; baader et al.  1  we will here concentrate on the problem of computing a least matcher  w.r.t. the ordering v on substitutions .
　instead of a single matching problem  we may also consider finite systems {c1 《  d1 ... cm 《  dm} of such problems  which must be solved simultaneously. as shown in  baader et al.  1   solving such a system can  however  be reduced to solving the single matching problem
 r1.c1 u ，，， u  rm.cm 《   r1.d1 u ，，， u  rm.dm
where the ri are pairwise distinct role names.
　the following theorem summarizes the results obtained in  baader and narendran  1; baader et al.  1  for matching in  sublanguages of  aln:
theorem 1 let l （ {fl1 fl〕 fl  aln}. then there exists a polynomial time matching algorithm that computes the least matcher of a given system of l-matching problems  if this system has a solution  and returns  fail  otherwise.
obviously  this implies that the existence of a solution can be decided in polynomial time. in the sequel  let matchl denote an algorithm according to theorem 1.
matching under side conditions
in this paper  we focus on more general matching problems  those that allow for additional side conditions.
definition 1 a subsumption condition is of the form x v  e where x is a concept variable and e is a pattern; a strict subsumption condition is of the form x   e where x and e are as above. a side condition is either a subsumption condition or a strict subsumption condition. the substitution σ satisfies the side condition x ρ e for ρ （ {v  } iff σ x  ρ σ e .
syntaxsemantics1  ixxxxc u dci ” dixxxx r.c{x （  i |  y:  x y  （ ri ★ y （ ci}xxxx〕 xxx p  p （ nc i   pixx − n r   n （ n{x （  i | #{y |  x y  （ ri} − n}x 	n r   n n	x	 i	# y	 x y 	ri	nx	fl	fl〕	fl 	aln
	＋	（	{	（	|	{ |	（	} ＋	}
table 1: syntax and semantics of concept descriptions.　a matching problem under side conditions is a tuple m := hc 《  d si  where c 《  d is a matching problem and s is a finite set of side conditions. if the set s contains only subsumption conditions  then m is called matching problem under subsumption conditions. the substitution σ is a solution  matcher  of m iff it is a matcher of c 《  d that satisfies every side condition in s.
in the next section  we will restrict the attention to matching problems under subsumption conditions. section 1 then treats matching problems under acyclic side conditions.
　in order to define matching problems under acyclic side conditions  we say that a variable x directly depends on a variable y in s iff s contains a side condition x ρ e such that y occurs in e. if there are n − 1 variables x1 ... xn such that xi directly depends on xi+1 in s  1 ＋ i ＋ n   1   then we say that x1 depends on xn in s. the set of side conditions s is cyclic iff there is a variable x that depends on itself in s; otherwise  s is acyclic.
1	matching under subsumption conditions
let l be one of the dls fl〕 fl  aln. we present a polynomial time algorithm that  given an l-matching problems under subsumption conditions  returns a least matcher  w.r.t. the ordering v on substitutions  if the problem is solvable  and  fail  otherwise. in principle  the algorithm iterates the application of matchl until a fixpoint is reached. however  the matcher computed in one step is used to modify the matching problem to be solved in the next step. given an l-matching problem under subsumption conditions m := hc 《  d si and a substitution σ  we define
mσ := {c 《  d} “ {σ x  v  e | x v  e （ s}.
recall that σ x  v  e abbreviates the matching problem σ x  《  σ x  u e. thus mσ is a system of l-matching problems without side conditions  to which matchl can be applied.
algorithm 1 let m := hc 《  d si be an l-matching problem under subsumption conditions. then  the algorithm
match works as follows:
1. σ x  := 〕 for all variables x;
1. if matchl mσ  returns  fail   then return  fail ; else if σ 《 matchl mσ   then return σ; else σ := matchl mσ ; continue with 1.
let σ1 denote the substitution defined in step 1 of the algorithm  and σt  t − 1  the matcher computed in the t-th iteration of step 1. note that σt is undefined if matchl returns  fail  in the t-th iteration or if the algorithm has stopped before the t-th iteration.
　to show that the algorithm is correct  we must show soundness  completeness  and termination  i.e.  i  if the algorithm terminates and returns a substitution  then this substitution in fact solves the problem; ii  if the algorithm terminates and returns  fail   then there indeed is no solution; and iii  the algorithm halts on every input. the following lemma proves soundness and completeness of the algorithm. the first two items establish a loop invariant.
lemma 1 let m := hc 《  d si be an l-matching problem under subsumption conditions.
1. if σt is defined and τ is a solution of m  then σt v τ.
1. if σt σt+1 are defined  then σt v σt+1.
1. if match returns the substitution σ  then σ solves m  soundness .
1. if match returns  fail   then m has no solution
 completeness .
proof. 1. obviously  the claim is true for σ1. assume that σt v τ  and that σt+1 is defined. to prove σt+1 v τ  it is sufficient to show that τ solves mσt since σt+1 is the least solution of mσt. since τ solves m  we know that it solves c 《  d and that τ x  v τ e  for all x v  e （ s. the induction assumption σt v τ implies σt x  v τ x   and thus σt x  v τ e   which shows that τ solves mσt.
1. obviously  σ1 v σ1. now assume that σt 1 v σt. to-
gether with the fact that σt solves mσt 1  this implies
 
that σt+1 solves the system mσt 1. since σt is the least
 
solution of mσt 1  we can conclude σt v σt+1.  
1. assume that σ = σt. by definition of match σt d . it remains to show that σt solves the side conditions. we know that σt 《 σt+1 and σt+1 solves mσt. thus  σt x  v σt+1 e  《 σt e  for every x v  e （ s.
1. assume that match returns  fail   and that σt is the last substitution computed by the algorithm. now assume that τ solves m. as in the proof of 1. we can show that τ solves mσt. consequently  mσt is solvable  and thus match mσt  returns the least matcher of this system  in contradiction to the assumption that match returns  fail  in this step of the iteration.

　proving termination of algorithm 1 is more involved  and the exact argument depends on the dl l under consideration. due to the space limitations  in this paper we sketch the proof for the dl fl〕  see  baader et al.  1  for complete proofs . the proof depends on the so-called reduced normal form of fl〕-concept descriptions  to be introduced next.
　it is easy to see that any fl〕-concept description can be transformed into an equivalent description that is either   or a  nonempty  conjunction of descriptions of the form  r1.，，， rm.a  where r1 ... rm are m − 1  not necessarily distinct  roles  and a is the bottom concept 〕 or a concept name. we abbreviate  r1.，，， rm.a by  r1 ...rm.a  where r1 ...rm is viewed as a word over the alphabet nr of all role names. if m = 1  then this is the empty word ε  and thus  ε.a is our  abbreviation  for a. in addition  instead of  w1.a u ... u  w`.a we write  l.a where l := {w1 ... w`} is a finite set of words over nr; we define   .a 《  . using these abbreviations  any fl〕-concept description c containing only concept names in the finite set c   nc can be written as
 
where uh for h （ c “ {〕} are finite sets of words over
nr  called role languages . this representation of c will subsequently be called its u-labeled normal form.
　as an example  consider the fl〕-concept description cex :=  r. 〕 u  r.〕  u  r. s.a u  s.a. its fl1-normal form.
　the role languages may contain redundant words  i.e.  words that  when removed  yield equivalent concept descriptions: i  since  w.〕 v  wv.〕 for every  we can require u〕 to be prefix-free  i.e.  w wv （ u〕 implies v = ε; and ii  since  w.〕 v  wv.a  we can require ua ”  u〕 ， nr   =  . a normal form satisfying these conditions is called reduced normal form.
　obviously  any fl〕-concept description can be turned into such a reduced normal form  in polynomial time . in our example  the reduced normal form of cex is  {r}.〕 u  {s}.a  which is obtained from  by removing rr from u〕 = {r rr} and rs from ua = {rs s}. reduced normal forms can be used to characterize equivalence of fl〕concept descriptions:
lemma 1 assume that the fl〕-concept descriptions c d are given in their u- and v -labeled reduced normal forms  respectively. then c 《 d iff uh = vh for all h （ c“{〕}. termination of algorithm 1 for l = fl〕
let the substitutions σt be defined as above. we assume that every σt x  is given in ut x-labeled reduced normal form  and that c  as defined in algorithm 1  is in u-labeled reduced normal form. then  termination follows from the fact that every solvable matching problem under subsumption conditions has a matcher that only uses concept names already contained in the matching problem m  denoted by the set c   nc  and the following three properties of the languages for h （ c “ {〕}. in the formulation of these properties we implicitly assume that the substitution σt is defined whenever we talk about one of the languages.
1. for every variable x and every h （ c “ {〕}  the set uht x contains only suffixes of uh.
1. for every word w  if w （ uht x   uht+1 x  then w 1（  for any t1   t.
1. if σt and σt+1 are defined and σt 1《 σt+1  then there exists an h （ c“{〕}  a variable x  and a word w such that w （ uht x   uht+1 x.
a complete proof of these properties can be found in  baader et al.  1 . it also depends on details of the algorithm match   which we have here introduced only as a black box. l
note that these properties would not hold if we did not use reduced normal forms.
　given these properties  it is now easy to show that the algorithm halts after a polynomial number of steps. in fact  property 1 yields a polynomial upper bound on the size of the role languages uht x. property 1 shows that in every step of the iteration at least one word is removed from one of these languages  and property 1 ensures that words that have been removed cannot reappear.
to sum up  we have shown the following theorem.
theorem 1 let l （ {fl〕 fl  aln}. the algorithm matchv is a polynomial time algorithm that  given an lmatching problem with subsumption conditions  returns al least matcher of this problem if it is solvable  and  fail  otherwise.
　it should be noted that the algorithm matchv does not work for l = fl1 since this language does not allow for thel bottom concept  and thus the initialization step  step 1  is not possible. however  instead of starting with σ x  := 〕  the algorithm can also start from the least matcher of c 《  d. in case the side conditions do not introduce new variables  i.e.  variables not contained in d   this modification works and yields a polynomial time matching algorithm. in contrast  if new variables are introduced  then we can show  baader et al.  1  that the size of the least matcher may grow exponentially in the size of the matching problem  and that there exists an exponential time algorithm for computing such matchers. nevertheless  the size of the substitutions for variables in d can still be bounded polynomially  and if one is only interested in substitutions for these variables  then these can still be computed in polynomial time.
1	matching under acyclic side conditions
matching under acyclic side conditions  i.e.  strict and nonstrict acyclic subsumption conditions  is more complex than matching under subsumption conditions for two reasons.
　first  as already shown in  baader et al.  1   deciding the solvability of an fl1-matching problem under strict  and acyclic  subsumption conditions is np-hard. it is easy to see that the same reduction works for the dls fl〕  fl   and aln. thus  assuming that p =1 np  there cannot exist a polynomial time algorithm computing matchers of matching problems under general side conditions.
　second  as shown by the following example  solvable matching problems under strict subsumption conditions no longer need to have a least matcher  but rather finitely many minimal matchers .
example 1《  x1 uconsider the... u xn under the strict acyclic subsumptionfl〕-matching problem a1 u...u
an conditions {xi+1    xi | 1 ＋ i ＋ n   1} “ {x1     }.
　the pure matching problem enforces that each xi must be replaced by a  possibly empty  conjunction of concept names from {a1 ... an}. thus  the strict subsumption conditions can only be satisfied if x1 is replaced by one of these names  x1 by a conjunction of this name with an additional one  etc. from this it is easy to derive that the matchers of the problem are of the following form: given a permutation p :=  p1 ... pn  of  1 ... n   the substitution σp is defined by σp xi  := ap1 u...uapi  1 ＋ i ＋ n . thus  there are n! non-equivalent matchers  and it is easy to see that each of them is minimal.
　the new contribution of this section is a  nondeterministic  algorithm  match   that computes matchl
ers of l-matching problems under acyclic side conditions for l （ {fl〕 fl }. this non-deterministic algorithm matches the lower complexity bound  np-hard  for the decision problem in the following sense. the length of every computation path of this algorithm is polynomially bounded in the size of the given matching problem. in case the problem is not solvable  every computation returns  fail . otherwise  the successful computation paths yield all minimal matchers.
the algorithm handling acyclic side conditions in the following  let m = hc 《  d si be an l-matching problem  l （ {fl〕 fl }  under acyclic side conditions. for distinct
variables x1 ... x` and patterns e1 ... e` such that ei does not contain the variables xi ... x`.  the case where not all the left-hand side variables are distinct can be treated similarly.  we denote by sv the set of side conditions obtained from s by replacing every ρi by v. applied to input m  the algorithm match  first calls l
match . if this yields  fail   then m is also unsolvable. otherwise  the computed substitution σ solves c 《  d  but may still violate some of the strict subsumption conditions. starting with the violated side condition with the largest index  the algorithm tries to modify σ such that this side condition is satisfied.
　assume that xk    ek is this side condition. since σ solves xk v  ek  we know that σ xk  《 σ ek . thus  we must either make σ xk  more specific or σ ek  more general. since matchv computes the least solution  the first option cannot lead to a solution of the overall system. hence l we must try the second one. the idea  which will be explained in more detail later  is that we consider the reduced normal form of σ ek . we try to make σ ek  more general by  non-deterministically  choosing one word from one of its role languages and by removing this word by appropriately modifying the role languages of the variables occurring in ek. since we want to compute minimal matchers  we make as little changes as possible in order to keep the substitution as specific as possible.
　the new substitution σ1 obtained this way solves xk    ek  and since we only modified variables occurring in ek  the side conditions with larger index are still satisfied. however  the side conditions with smaller index  even the nonstrict ones  as well as the matching problem need no longer be solved by σ1. to overcome this problem  matchv is used to h 《l   d s i 
compute the least substitution that  i  solves1. it can be shown that the second condi-c v and  ii  subsumes σ tion  which can be expressed by a system of matching problems  makes sure that the computed substitution still solves the strict subsumption conditions from index k to `. we can now continue the modification process with this substitution.
algorithm 1 let m = hc 《  d si be an l-matching problem under acyclic side conditions. then  match  works as follows:	l
1. if match returns  fail   then return  fail ;
1. k := `; σ := match;
1. if k = 1  then return σ; if σ xk  ρk σ ek   then continue with 1.
1. guess modification σ1 of σ for xk    ek;
if σ1 ek  《 σ ek   then return  fail ;
m1 := h{c 《  d}“{σ1 xj  v  xj|1 ＋ j ＋ `} svi;
if match returns  fail   then return  fail ; σ := match
1. k := k   1; continue with 1.
how to guess modifications
here we sketch the modifications fortion for fl〕 and fl  can be found infl baader〕.  a formal definiet al.  1 . -
recall that the goal is to make σ ek  more general by  nondeterministically  choosing one word w from one of its role languages and by removing this word by appropriately modifying the role languages of the variables occurring in ek.
　we call this a c-modification if w is picked from a role language corresponding to some atomic concept a. in this case  removing certain words from role languages of the variables in e suffices to obtain a minimal modification.
　in case of a 〕-modification  where w is picked from the role language corresponding to the 〕-concept  the removal of some word v in the role language of a variable implicitly removes every continuation vv1 of v. to correct this effect  every word in {v}，nr is put back whenever some v is removed. in addition  since v is also implicitly removed from role languages corresponding to atomic concepts  it is also transferred to such role languages. this ensures that the computed substitution is as specific as possible. this is vital both for the proof of correctness and to obtain all minimal solutions.
　the following example illustrates in more detail how the modifications work.
example 1 consider the fl〕-matching problem a u  {r s}.〕 《  x1 u r.x1 u r.x1 under the strict subsumption conditions x1    x1  x1    x1.
　executing the above algorithm  we obtain in step 1 as initial solution σ the following substitution:
{x1★  {r s}.〕 u  {ε}.a x1★  {ε}.〕 x1★  {ε}.〕}.
　the iteration begins in step 1 by checking the second side condition  which is violated. choosing a 〕-modification in
step 1  we must choose a word from the role languagecorresponding to 〕 in σ x1  = σ x1 . in this case  we can{ε} only pick ε. to keep the change minimal  we do not simply remove it  but rather replace it by {r s} in the role language corresponding to 〕 in σ x1 . in addition  we transfera. this yields σ1 x1ε  =to the role language corresponding to
 {in this case  the substitutionr s}.〕 u  {ε}.a. the other variables remain unchanged.σ1 itself solves the matching
problem m1 considered in step 1  and thus match returns σ1.
　in the second iteration  we find in step 1 that the first side condition x1    x1 no longer holds. in step 1  we again
ification replaceschoose arole language〕-modification  and choose the word{r sr}bycorresponding torr rs and adds〕r into the role languageσ x1 . the mod-r from the
corresponding to {new value ofε r}.a. again  this substitution solvesσ isaσ1. this yields.	σ1 x1  :=m {1rr rs s  and thus the}.〕 u
　in the next iteration we have k = 1  ending the iteration in step 1. the algorithm finally returns the substitu-
 {tion {.a xx1★  1★  {rr rs s{ε}.〕}}..〕 u  {ε r}.a x1 ★  1 {r s}.〕 u ε}
　note that  in the first iteration step  it was not possible to apply a c-modification since the role language corresponding to a was empty. in the second step  we could have applied asponding toc-modification  removinga in σ x1 . then  however  the systemε from the role language corre-m1 obtained this way would not have been solvable. in fact  it is
easy to see that the two matching problemsx1 u  r.x1 u  r.x1 and  {r s}.〕 v  x1aoccurring inu {r s}.〕 《m 1
cannot be solved simultaneously.
in soundness and completeness for baader et al.  1 . it is easy to see that the length offl〕 and fl  is proved each computation branch of the nondeterminstic algorithm match  is polynomially bounded. because matching under strict acyclic subsumption conditions inknown to be np-hard  we obtain the following theorem.l	fl〕 and fl  is
theorem 1 let l （ {fl〕 fl }. deciding the solvability of l-matching problems under acyclic side conditions is an np-complete problem.
　the algorithm described above not only decides solvability of matching problems under acyclic side conditions. it also computes matchers in case the problem is solvable. to be more precise  all minimal matchers  w.r.t. subsumption of substitutions  are computed.
1	future work
in contrast to the algorithm for matching under subsumption conditions  the algorithm match  presently is restricted to  i  l （ {fl〕 fl } and  ii  acyclic side conditions. natu-l rally  the next step is to overcome the first restriction by extending the result for acyclic side conditions to aln. we strongly conjecture that an algorithm similar to match  can l
be devised for this purpose.
　in  baader et al.  1   fl1-matching problems with cyclic subsumption conditions are transformed into equivalent ones with acyclic subsumption conditions. unfortunately  this approach cannot be extended to fl〕 in a straightforward way. instead  in order to overcome the second restriction  one might try to adapt the general idea of the algorithm match  to cyclic side conditions. hence  one would first compute the solution to the analogous non-strict match-l ing problem and then proceed by iteratively modifying the solution to satisfy the strict side conditions. proving termination of this approach appears to be the main problem.
　apart from aln and its sublanguages  it also seems promising to consider matching problems under side conditions in ale  which extendsvflproposed here is defined on top  by existential restrictions.
since the algorithm match
of an existing algorithm for pure matching problems  withl certain properties   one might similarly take advantage of the matching algorithm proposed in  baader and kusters  1：   for ale-matching problems.
references
 baader et al.  1  f. baader  s. brandt  and r. kusters. match-： ing under side conditions in description logics. ltcsreport 1  lufg theoretical computer science  rwth aachen  germany  1. see http://www-lti.informatik.rwthaachen.de/forschung/reports.html.
 baader and kusters  1：   f. baader and r. kusters.： matching in description logics with existential restrictions. in proc. of kr1  pp. 1  morgan kaufmann publishers  1.
 baader et al.  1  f. baader  r. kusters  a. borgida  and d.： mcguinness. matching in description logics. journal of logic and computation  1 :1  1.
 baader and narendran  1  f. baader and p. narendran. unification of concept terms in description logics. in proc. of ecai1  pp. 1  john wiley & sons ltd.  1.
 borgida and kusters  1：   a. borgida and r. kusters. what's not： in a name: some properties of a purely structural approach to integrating large dl knowledge bases. in proc. of dl1  number 1 in proc. of ceur-ws  1.
 borgida and mcguinness  1  a. borgida and d. l. mcguinness. asking queries about frames. in proc. of kr'1  pp. 1  morgan kaufmann publishers  1.
 borgida and patel-schneider  1  a. borgida and p. patelschneider. a semantics and complete algorithm for subsumption in the classic description logic. journal of artificial intelligence research  1-1  1.
 brachmann et al.  1  r. j. brachman  d. l. mcguinness  p. f. patel-schneider  l. a. resnick  and a. borgida. living with
classic: when and how to use a kl-one-like language. principles of semantic networks  pp. 1. morgan kaufmann publishers  1.
 donini et al.  1  f. m. donini  m. lenzerini  d. nardi  and a. schaerf. reasoning in description logics. principles of knowledge representation  studies in logic  language and information  pp. 1. csli publications  1.
 horrocks  1  i. horrocks. using an expressive description logic: fact or fiction  in proc. of kr'1  pp. 1  morgan kaufmann publishers  1.
 mcguinness  1  d. l. mcguinness. explaining reasoning in description logics. phd thesis  department of computer science  rutgers university  october  1.

computing least common subsumers in

ralf kusters：
institut fu：r informatik und praktische mathematik
christian-albrechts-universita：t zu kiel
germany kuesters ti.informatik.uni-kiel.de
ralf molitor
swiss life
it research and development group
switzerland
ralf.molitor swisslife.ch

abstract
computing the least common subsumer  lcs  has proved to be useful in a variety of different applications. previous work on the lcs has concentrated on description logics that either allow for number restrictions or for existential restrictions. many applications  however  require to combine these constructors. in this work  we present an lcs algorithm for the description logic   which allows for both constructors  thereby correcting previous algorithms proposed in the literature.
1	introduction
computing the least common subsumer  lcs  in description logics  dls  is an inference task first introduced by cohen  borgida  and hirsh  cohen et al.  1  for sublanguages of classic. since then it has found several applications: as a key operation in inductive learning algorithms  cohen and hirsh  1   as a means to measure the similarity of concepts for information retrieval  mo：ller et al.  1   and as an operation to support the bottom-up construction of dl-knowledge bases  baader and ku：sters  1; baader et al.  1 . roughly speaking  the lcs of a set of concepts is the most specific concept description  among a possibly infinite number of concept descriptions  that subsumes all of the input descriptions  and as such allows to extract the commonalities from given concept descriptions  a task essential for all the mentioned applications.
　the first lcs algorithms proposed in the literature were applicable to sublanguages of classic  more precisely  dls that in particular allow for number restrictions  cohen et al.  1; cohen and hirsh  1 . more recently  motivated by the bottom-up construction of knowledge bases in a chemical engineering application  sattler  1; von wedel and marquardt  1   the lcs has been investigated for the dl
 baader et al.  1   which allows for existential restrictions instead of number restrictions. although first empirical results are encouraging  baader and molitor  1   they also show that this application asks for a more expressive dl  one

　　this work was carried out while the author was still at the lufg theoretical computer science  rwth aachen.
that allows to combine number restrictions and existential restrictions. such a logic can  for example  be used to describe a reactor with cooling jacket and exactly two inlet valves by
reactor	is-coupled-to.cooling-jacket	 =1 has-inlet  has-inlet.valve.
　in this work  we propose an algorithm for computing the lcs of -concept descriptions. the dl allows for conjunction  a restricted form of negation  value restrictions  existential restrictions  and number restrictions. similar to previous approaches  cohen et al.  1; baader and ku：sters  1; baader et al.  1   our lcs algorithm builds on a structural characterization of subsumption.
　typically  such a characterization works in two steps. first  concept descriptions are turned into a structural normal form  which makes all facts implicitly representedin the description explicit. second  the subsumer and the subsumee  given in structural normal form  are compared syntactically. a sound and complete characterization then ensures that the structural normal form indeed contains all implied facts.
　now  given that the structural normal form of concept descriptions can be computed effectively  the lcs of concept descriptions can be obtained by first computing their structural normal forms and then extracting the  common facts  present in these normal forms.
　for   however  computing the structural normal form already requires to  inductively compute the lcs  see our running example in section 1 . consequently  the proof of correctness of the lcs algorithm needs to be interleaved with the proof of soundness and completeness of the characterization of subsumption  making the proofs quite involved. in  mantay  1   this approach is in fact pursued. however  in an attempt to avoid these interleaving proofs  mantay made unproved assumptions concerning the existence and other properties of the lcs. moreover  the lcs algorithm presented there is incorrect in that the computed concept description not necessarily subsumes the input descriptions.
　in this work  we devise a more relaxed notion of structural normal form  which does not involve the lcs computation and therefore allows to decouple the characterization of subsumption from the lcs computation. instead of a single
     -concept description  our normal form consists of a set of -concept descriptions  where some of the implicit
facts are not made explicit.
construct namesyntaxsemanticstop-conceptbottom-conceptconcept nameprimitive negation conjunctionexistential restrictions forvalue restrictions fornumber restrictions
for	 	table 1: syntax and semantics of	-concept descriptions.the outline of our paper is as follows: in section 1  we formally introduce the language and the lcs operation. section 1 contains a running example and a discussion of the main difficulties that occur when computing the lcs. in subsequent sections  this example is used to illustrate the notions introduced. section 1 then covers the characterization of subsumption and section 1 the lcs algorithm. finally  in section 1  we briefly discuss the results obtained. due to space limitations  we cannot give all technical details. these details as well as complete proofs can be found in  ku：sters and molitor  1 .
1	preliminaries
concept descriptions are inductively defined with the help of a set of constructors  starting with a set of concept names and a set of role names. in this work  we consider the dl   i.e.  concept descriptions built from the constructors shown in table 1  subsequently called -concept descriptions.
　the semantics of a concept description is defined in terms of an interpretation . the domain of is a non-empty set of individuals and the interpretation function maps each concept name to a set and each role name to a binary relation
   . the extension of to arbitrary concept descriptions is inductively defined  as shown in the third column of table 1.
　one of the most important traditional inference services provided by dl systems is computing the subsumption hierarchy. the concept description is subsumed by the description     iff holds for all interpretations . the concept descriptions and are equivalent     iff they subsume each other.
　in this paper  we are interested in the computation of least common subsumers.
definition 1 given	-concept descriptions	 
　　　  the -concept description is the least common subsumer  lcs  of   for short  iff  i  for all   and  ii  is the least concept description with this property  i.e.  if satisfies for all   then .
depending on the dl under consideration  the lcs of two or more descriptions need not always exist  but if it exists  then it is unique up to equivalence. the main contribution of this paper is to show that in the lcs always exists and that it can be computed effectively.
　for the sake of simplicity  we consider -concept descriptions over a set of concept names and assume to be the singleton . however  all definitions and results can easily be generalized to arbitrary sets of role names. furthermore  w.l.o.g.  we assume all -concept descriptions to be in the following normal form: each conjunction in an
-concept description contains
1. at most one number restriction of the form
	 this is w.l.o.g. due to	if
 ;
1. at most one number restriction of the form
	 this is w.l.o.g. due to	if
 ;
1. at most one value restriction of the form
	 this is w.l.o.g. due to	.
1	running example
in order to highlight the main problems to be solved in the structural characterization of subsumption and the computation of the lcs in   we will use the following concept descriptions:
and
the key point in the characterization of subsumption and the lcs computation is to describe the  non-trivial  concept descriptions  say   subsuming a given concept description  say   where  non-trivial  means that does not occur as a conjunct on the top-level of . subsequently  these concept descriptions are called induced. it suffices to only consider inducedconcept descriptionsthat are minimal w.r.t. subsumption.
　in what follows  we describe the concept descriptions induced by and . it turns out that some of the concept descriptions induced by correspond to the lcs of certain subdescriptions in . as we will see  given the induced concept descriptions of and   it is easy to determine the lcs of and . we start with the concept descriptions induced by .
number restrictions: because of the existential restrictions on the top-level of   e.g. and   we know   i.e.  is induced by . conversely  there is no induced -restriction  i.e.  the most specific -restriction subsuming is the -restriction explicitly present on the top-level of .
existential restrictions: due to the	-restriction
on the top-level of   each instance of has at most two -successors. consequently  some existential restrictions have to be  merged  to a single existential restriction  where  merging  means conjoining the concept descriptions occurring in the existential restrictions. for   there are several ways to merge the five existential restrictions on the top-level of into two existential restrictions. the merging process gives rise to new  derived  concept descriptions  where the only consistent ones are:
and
it is clear that	. the existential restrictions
　　　　　　　　　 	 	  and	subsume both	and	. from this it can be concluded that these restrictions are induced by
.
　as we will see  the induced existential restrictions can be obtained by picking one existential restriction from each of the  consistent  derived concept descriptions and applying the lcs operation to them. in our example  we have  for instance 
　　　　. however  as explained below  our characterization of subsumption avoids to explicitly use the lcs by employing the fact that
	iff	for all	 1 
value restrictions: in view of   it not only follows that every instance of has exactly two successors but that these -successors must satisfy the existential restrictions given in and . in either case  all -successors belong to   and thus  the value restriction is induced by .
　there are two things that should be pointed out here: first  note that there is an induced value restriction only if the number of successors induced by existential restrictions coincides with the number in the -restriction  because only in this case  we have  full  information about all -successors of an instance of . for example  if we consider the concept description	  no value restriction is induced.1
　second  if is the most specific value restriction induced by   then corresponds to the lcs of all concept descriptions occurring in the merged existential restrictions. in

1the structural subsumption algorithm introduced in  mantay 
1   however  computes as a value restriction induced by and thus  is incorrect. for the same reason  the lcs-algorithm
presented in  mantay  1  is incorrect: although the lcs of and is   the algorithm returns .
the example 
　　　　　　　　　　　　　　　　　　　　　　　　. again  using the equivalence  1   we avoid to explicitly compute the lcs in the characterization of subsumption.
　it is easy to see that the only  minimal  concept description induced by	is	.
　now  given the concept descriptions induced by and it is not hard to verify that the lcs of and can be stated as
1	structural characterization of subsumption
in what follows  let   be -concept descriptions in the normal form introduced in section 1. since both
and trivially imply   our characterization of subsumption explicitly checks these equivalences. otherwise  roughly speaking  each conjunct in   i.e.  each  negated  concept name  number restriction  existential restriction  and value restriction occurring on the top-level of   is compared with the corresponding conjuncts in . for the existential restrictions and value restrictions  however  it will be necessary to resort to the concept descriptions derived from by merging existential restrictions   and in the example . if all the comparisons have succeeded  it follows . in the remainder of this section  the structural comparison between and is further explained. finally  theorem 1 establishes the complete characterization of subsumption.
　we first need some notation to access the different parts of the concept descriptions: denotes the set of all  negated  concept names occurring on the top-level of ;
;
;
if there exists a value restriction of the form on the top-level of   then ; otherwise 
;
	there exists	on the top-level of
.
note that is always finite  whereas may be   namely if there exists no positive integer such that
           . although these values need not be explicitly present in number restrictions of   they can be computed in polynomial time in the size of using an oracle for subsumption of -concept descriptions  ku：sters and molitor  1 . in our example  we get  
	 	  and	.
the structural comparison between the different parts of
and	can now be stated as follows  assuming	and
 :
 negated  concept names and number restrictions:  cf. theorem 1  1.-1.  in order for to hold  it is obvious that the following conditions need to be satisfied:
	 	 
       . otherwise  it is easy to construct a counter-model for	.
existential restrictions:	 cf. theorem 1  1.  given
         then for to hold one at first might expect that there need to exist a with .
although this works for	-concept descriptions  it fails for
       as	shows  see section 1 . the reason is that in -restrictions may require to merge existential restrictions  yielding new  implicit  existential restrictions. to deal with this phenomenon  sets of derived concept descriptions are considered with certain existential restrictions merged  see and in our running example .
　the result of merging existential restrictions is described by so-called existential mappings
where	and	.
we require	to obey the following conditions:
1. for all	;
1. and	for all
.
1. for all	.
although the first two conditions are not essential for soundness and completeness of the characterization of subsumption  theyreducethe numberof existential mappingsthat need to be considered.
　given	  yields an concept description obtained from by substituting all existential restrictions on the top-level of with
the set of all existential mappings on satisfying the conditions - is denoted by   where if
	. it is in fact sufficient to consider	mod-
ulo permutations  i.e.  modulo the equivalence iff	there exists a permutation	on
	s.t.	for all	.
in the sequel  for the sake of simplicity  we stay with instead of   though.
	in	our	running	example 	let
	with	 	 
etc. then 	consists of the two mappings
and it is	 	 see section 1 .
　for the characterization of subsumption  we will use the following notation:
　now  in case	 	implies that for each the following holds: for each	  there exists	such that	.
this is what is stated in theorem 1  1. note that  using the equivalence  1   and provided that the lcs of -concept descriptions always exists  this condition could be rewritten as: there exists a set
with	. however  since the existence of the lcs is not guaranteed a priori  the lcs cannot be used in the structural characterization of subsumption  unless the characterization and the lcs computation are interleaved.
if	  then for all	it must hold  and	.
in our running example 
illustrates the case where	and illustrates	.
value restrictions:  cf. theorem 1  1.  value restrictions can only be induced for two reasons. first  if   then   and thus  for all concept descriptions .
　second  the merging of existential restrictions may induce value restrictions. in contrast to induced existential restrictions  however  one further needs to take care of restrictions induced by  incompatible  existential restrictions:
if	  we define	. in our example 
	 	  and	.
　now  only if   value restrictions can be induced  since only then we  know  all the -successors of instances of . in our example    which accounts for . conversely  since	.
　to formally state the comparison between value restrictions of and   we use the following notation:
one can show that	implies
provided that	.	in case
         however  it suffices if the value restriction of satisfies	for all	.
again  using the equivalence  1   and provided that the lcs of
     -concept descriptions always exists  this condition can be restated as	. for reasons already mentioned  we have not employed this variant.
　we are now ready to state the structural characterization of subsumption in	.
theorem 1  kusters and molitor  1：	  let	be two
     -concept descriptions with	. then iff     or the following holds:
1. ;
1. ;
1. ;
1. for all	it holds that
 a   	  and	; or
 b  and for each   there exists such that	; and
let	be two	-concept descriptions.
if	  then c-lcs	  and if	  then c-lcs.otherwise  letc-lcs
c-lcsand  and define fordefinec-lcsc-lcsc-lcswhereis omitted ifor  andc-lcsifor.	figure 1: the recursive computation of the lcs in	.1. if	  then
 a  ; or
 b  and	; or
 c  and for all	.
as known from the literature   hemaspaandra  1   testing subsumption of -concept descriptions is pspacecomplete. however  our characterization is not intended to serve as a basis for a polynomial-space subsumption algorithm. it rather yields the formal basis for the lcs algorithm presented in the next section.
1	computing the lcs in
the recursive algorithm computing the lcs of two concept descriptions and is depicted in figure 1. for a set of -concept descriptions  c-lcs
is computed by iteratively applying the binary c-lcsoperation  i.e.  c-lcs c-lcs c-lcs c-lcs	for .
　since the role depth of and is finite  the recursive computation of c-lcs always terminates. the following theorem states correctness of the lcs algorithm depicted in figure 1. as an immediate consequence  we obtain that the lcs of two -concept descriptions always exists. the theorem is proved by induction on the role depth of c-lcs and makes heavy use of the structural characterization of subsumption given in theorem 1.
theorem 1  kusters and molitor  1：   let be concept descriptions. then  c-lcs .
　in the remainder of this section  we illustrate the definition of c-lcs in case that and  since the special cases   are trivial   using our running example introduced in section 1. the conjuncts occurring on the top-level of c-lcs can  as before  be divided into three parts  namely  1   negated  concept names and number restrictions   1  the existential restrictions  and  1  the value restriction. these conjuncts are defined in such a way that
 a  the conditions 1.-1. in theorem 1 for	c-lcs and	c-lcs	are satisfied  and
 b  c-lcs is the least concept description  w.r.t.   satisfying  a .
for the conditions 1.-1.  this is quite obvious  since  for
  we obtain
c-lcs	  c-lcs
  and
c-lcs
.
in our running example  c-lcs   c-lcs   and c-lcs
.
　for existential and value restrictions things are more complicated. let us first consider the definition of c-lcs   i.e.  the existential restrictions obtained from the sets and . roughly speaking  if
	  then	contains all  minimal  concept de-
scriptions occurring in an existential restriction induced by
for	. each such concept description is obtained as the recursively computed lcs of a set of concept descriptions consisting of one concept description from
 conjoined with   for each . in our running example  each pair of concept descriptions occurring in the existential restrictions on the top-level of and   respectively  yields such an lcs  and thus
 see section 1 .
	if	and	  we set
         since then   and is the unique minimal existential restriction induced by . this case is illustrated by in our running example  where
	 	  and
.
　finally  if   i.e.  there exists no existential restriction subsuming   then obviously  no existential restriction can occur on the top-level of a common subsumer of and . therefore  we set .
	given	and	  the lcs of each pair
and gives rise to an existential restriction on the top-level of the lcs of and . in our example  we obtain c-lcs	  giving rise to the existential restrictions and
 where the latter one can be omitted .
it remains to comment on c-lcs . intuitively  is the most specific value restriction subsuming for
           .  thus  c-lcs is the most specific value restriction subsuming both and .  if has no induced value restriction  coincides with . this is the case for   where	.
if   the fact that is made explicit by defining . finally  if   then the induced value restriction is again made explicit by recursively computing the lcs of the merged existential restrictions  each conjoined with  . in our running example  this case is illustrated by : there    but since
  we obtain
which is the recursively computed lcs of  see section 1 .
1	conclusion and future work
we have presented an algorithm for computing the lcs in
     . its proof of correctness is based on the structural characterization of subsumption introduced in section 1. in this characterization  we avoided to explicitly use the lcs in order to be able to decouple the characterization of subsumption and the lcs computation. interleaving these two tasks caused previous work on the lcs in to be incorrect.
　due to the interaction between number restrictions and existential restrictions the characterization of subsumption and the lcs computation became much more involved than for the sublanguages  baader et al.  1  and  cohen and hirsh  1; baader and ku：sters  1 . as an immediate consequence of results shown in  baader et al.  1  for   we obtain an exponential lower bound for the size of the lcs of two -concept descriptions. it is  however  not known whether this bound is tight. the lcs algorithm presented in section 1 takes double exponential time  ku：sters and molitor  1 . nevertheless  since a prototype implementation of the exponential-time lcs algorithm for behaves quite well in the chemical process engineering application  baader and molitor  1   the hope is that the lcs algorithm for proposed here will also work in realistic application situations. for evaluation  continuing the work on   a prototype of the algorithm for will be implemented using the dl-system fact  horrocks  1  for deciding subsumption in   which then is to be applied to the chemical engineering knowledge base.
references
 baader and ku：sters  1  f. baader and r. ku：sters. computing the least common subsumer and the most specific concept in the presence of cyclic -concept descriptions. in proc. of ki'1  lnai 1. 1.
 baader and molitor  1  f. baader and r. molitor. building and structuring description logic knowledge bases using least common subsumers and concept analysis. in proc. of iccs1  lnai 1. 1.
 baader et al.  1  f. baader  r. ku：sters  and r. molitor. computing least common subsumers in description logics with existential restrictions. in proc. of ijcai'1. morgan kaufmann  1.
 cohen and hirsh  1  w.w. cohen and h. hirsh. learning the classic description logic: theoretical and experimental results. in proc. of kr'1. morgan kaufmann  1.
 cohen et al.  1  w.w. cohen  a. borgida  and h. hirsh. computing least common subsumers in description logics. in proc. of aaai'1. mit press  1.
 hemaspaandra  1  e. hemaspaandra. the complexityof pure man's logic. in essays dedicated to johan van benthem on the occasion of his 1th birthday  1.
 horrocks  1  i. horrocks. using an expressive description logic: fact or fiction  in proc. of kr'1. morgan kaufmann  1.
 ku：sters and molitor  1  computing least common subsumers in . ltcs-report 1  rwth aachen  germany  1. see http://www-lti.informatik.rwthaachen.de/forschung/reports.html.
 mantay  1  t. mantay. computing least common subsumers in expressive description logics. in proc. of ai'1  lnai 1. 1.
 mo：ller et al.  1  r. mo：ller  v. haaslev  and b. neumann. semantics-based information retrieval. in proc. of it&knows. 1.
 sattler  1  u. sattler. terminological knowledge representation systems in a process engineering application. phd thesis  rwth aachen  1.
 von wedel and marquardt  1  l. von wedel and w. marquardt. rome: a repository to support the integration of models over the lifecycle of model-based engineering processes. in proc. of escape-1  1.
fca-merge: bottom-up merging of ontologies
gerd stummealexander maedcheinstitute for applied computer science and
formal description methods  aifb 
university of karlsruhe
   d-1 karlsruhe  germany www.aifb.uni-karlsruhe.de/wbs/gst   fzi research center for information technologies haid-und-neu-strasse 1
d-1 karlsruhe  germany www.fzi.de/wimabstract
ontologies have been established for knowledge sharing and are widely used as a means for conceptually structuring domains of interest. with the growing usage of ontologies  the problem of overlapping knowledge in a common domain becomes critical. we propose the new method fca-merge for merging ontologies following a bottom-up approach which offers a structural description of the merging process. the method is guided by application-specific instances of the given source ontologies  that are to be merged. we apply techniques from natural language processing and formal concept analysis to derive a lattice of concepts as a structural result of fca-merge. the generated result is then explored and transformed into the merged ontology with human interaction.
1	introduction
ontologies have been established for knowledge sharing and are widely used as a means for conceptually structuring domains of interest. with the growing usage of ontologies  the problem of overlapping knowledge in a common domain occurs more often and becomes critical. domain-specific ontologies are modeled by multiple authors in multiple settings. these ontologies lay the foundationfor buildingnew domainspecific ontologies in similar domains by assembling and extending multiple ontologies from repositories.
　the process of ontology merging takes as input two  or more  source ontologies and returns a merged ontology based on the given source ontologies. manual ontology merging using conventional editing tools without support is difficult  labor intensive and error prone. therefore  several systems and frameworks for supporting the knowledge engineer in the ontology merging task have recently been proposed  hovy  1; chalupsky  1; noy and musen  1; mcguinness et al  1 . the approaches rely on syntactic and semantic matching heuristics which are derived from the behaviorof ontologyengineers when confrontedwith the task of merging ontologies  i.e. human behaviour is simulated. although some of them locally use different kinds of logics for comparisons  these approaches do not offer a structural description of the global merging process.
　we propose the new method fca-merge for merging ontologies following a bottom-up approach which offers a global structural description of the merging process. for the source ontologies  it extracts instances from a given set of domain-specific text documents by applying natural language processing techniques. based on the extracted instances we apply mathematically founded techniques taken from formal concept analysis  wille  1; ganter and wille  1  to derive a lattice of concepts as a structural result of fca- merge. the produced result is explored and transformed to the merged ontology by the ontology engineer. the extraction of instances from text documents circumvents the problem that in most applications there are no objects which are simultaneously instances of the source ontologies  and which could be used as a basis for identifying similar concepts.
　the remainder of the paper is as follows. we briefly introduce some basic definitions concentrating on a formal definition of what an ontology is and recall the basics of formal concept analysis in section 1. before we present our generic method for ontology merging in section 1  we give an overview over existing and related work in section 1. section 1 provides a detailed description of fca-merge. section 1 summarizes the paper and concludes with an outlook on future work.
1	ontologies and formal concept analysis
in this section  we briefly introduce some basic definitions. we thereby concentrate on a formal definition of what an ontology is and recall the basics of formal concept analysis.
1	ontologies
there is no common formal definition of what an ontology is. however  most approaches share a few core items: concepts  a hierarchical is-a-relation  and further relations. for sake of generality  we do not discuss more specific features like constraints  functions  or axioms here. we formalize the core in the following way.
definition:	a	 core 	ontology	is	a	tuple
        where is a set whose elements are called concepts   is a partial order on  i.e.  a binary relation  which is reflexive  transitive  and antisymmetric   is a set whose elements are called relation names  or relations for short   and is a function which assigns to each relation name its arity.
as said above  the definition considers the core elements of most languages for ontology representation only. it is possible to map the definition to most types of ontology representation languages. our implementation  for instance  is based on frame logic  kifer et al  1 . frame logic has a wellfounded semantics  but we do not refer to it in this paper.
1	formal concept analysis
we recall the basics of formal concept analysis  fca  as far as they are needed for this paper. a more extensive overview is given in  ganter and wille  1 . to allow a mathematical description of concepts as being composed of extensions and intensions  fca starts with a formal context defined as a triple
               where	is a set of objects 	is a set of attributes  and	is a binary relation between	and	 i.e.  .	is read  object	has attribute	 .
definition: for   we define and  for   we define
.
　a formal concept of a formal context is defined as a pair with     and .
the sets and are called the extent and the intent of the formal concept . the subconcept-superconcept relation is formalized by
　　　　　　　　the set of all formal concepts of a context together with the partial order is always a complete lattice 1 called the concept lattice of and denoted by  .
　a possible confusion might arise from the double use of the word 'concept' in fca and in ontologies. this comes from the fact that fca and ontologies are two models for the concept of 'concept' which arose independently. in order to distinguish both notions  we will always refer to the fca concepts as 'formal concepts'. the concepts in ontologies are referred to just as 'concepts' or as 'ontology concepts'. there is no direct counter-part of formal concepts in ontologies. ontology concepts are best compared to fca attributes  as both can be considered as unary predicates on the set of objects.
1	related work
a first approach for supporting the merging of ontologies is described in  hovy  1 . there  several heuristics are described for identifying corresponding concepts in different ontologies  e.g. comparing the names and the natural language definitions of two concepts  and checking the closeness of two concepts in the concept hierarchy.
　the ontomorph system  chalupsky  1  offers two kinds of mechanisms for translating and merging ontologies: syntactic rewriting supports the translation between two different knowledge representation languages  semantic rewriting offers means for inference-based transformations. it explicitly allows to violate the preservation of semantics in trade-off for a more flexible transformation mechanism.
　in  mcguinness et al  1  the chimaera system is described. it provides support for merging of ontological terms from different sources  for checkingthe coverageand correctness of ontologies and for maintaining ontologies over time. chimaera offers a broad collection of functions  but the underlying assumptions about structural properties of the ontologies at hand are not made explicit.
　prompt  noy and musen  1  is an algorithm for ontology merging and alignment embedded in prote＞g＞e 1. it starts with the identification of matching class names. based on this initial step an iterative approach is carried out for performing automatic updates  finding resulting conflicts  and making suggestions to remove these conflicts.
　the tools described above offer extensive merging functionalities  most of them based on syntactic and semantic matching heuristics  which are derived from the behaviour of ontology engineers when confronted with the task of merging ontologies. ontomorph and chimarea use a description logics based approach that influences the merging process locally  e.g. checking subsumption relationships between terms. none of these approaches offers a structural description of the global merging process. fca-merge can be regarded as complementary to existing work  offering a structural description of the overall merging process with an underlying mathematical framework.
　there is also much related work in the database community  especially in the area of federated database systems. the work closest to our approach is described in  schmitt and saake  1 . they apply formal concept analysis to a re-
lated problem  namelydatabase schema integration. as in our approach  a knowledge engineer has to interpret the results in order to make modeling decisions. our technique differs in two points: there is no need of knowledge acquisition from a domain expert in the preprocessing phase; and it additionally suggests new concepts and relations for the target ontology.
1	bottom-up ontology merging
as said above  we propose a bottom-up approach for ontologymerging. our mechanismis based on application-specific instances of the two given ontologies and that are to be merged. the overall process of merging two ontologies is depicted in figure 1 and consists of three steps  namely  i  instance extraction and computing of two formal contexts and    ii  the fca-merge core algorithm that derives a common context and computes a concept lattice  and  iii  the generation of the final merged ontology based on the concept lattice.
　our method takes as input data the two ontologies and a set of natural language documents. the documents have to be relevant to both ontologies  so that the documents are described by the concepts contained in the ontology. the documents may be taken from the target application which requires the final merged ontology. from the documents in   we extract instances. the mechanism for instance extraction is further described in subsection 1. this automatic knowledge acquisition step returns  for each ontology a formal con-

figure 1: ontology merging method
text indicating which ontology concepts appear in which documents.
　the extraction of the instances from documents is necessary because there are usually no instances which are already classified by both ontologies. however  if this situation is given  one can skip the first step and use the classification of the instances directly as input for the two formal contexts.
　the second step of our ontology merging approach comprises the fca-merge core algorithm. the core algorithm merges the two contexts and computes a concept lattice from the merged context using fca techniques. more precisely  it computes a pruned concept lattice which has the same degree of detail as the two source ontologies. the techniques applied for generating the pruned concept lattice are described in subsection 1 in more detail.
　instance extraction and the fca-merge core algorithm are fully automatic. the final step of deriving the merged ontology from the concept lattice requires human interaction. based on the pruned concept lattice and the sets of relation names and   the ontology engineer creates the concepts and relations of the target ontology. we offer graphical means of the ontology engineering environment ontoedit for supporting this process.
　for obtaining good results  a few assumptions have to be met by the input data: firstly  the documents have to be relevant to each of the source ontologies. a document from which no instance is extracted for each source ontology can be neglected for our task. secondly  the documents have to cover all concepts from the source ontologies. concepts which are not covered have to be treated manually after our merging procedure  or the set of documents has to be expanded . and last but not least  the documents must separate the concepts well enough. if two concepts which are considered as different always appear in the same documents  fca-merge will map them to the same conceptin the target ontology  unless this decision is overruled by the knowledge engineer . when this situation appears too often  the knowledge engineer might want to add more documents which further separate the concepts.
1	the fca-merge method
in this section  we discuss the three steps of fca-merge in more detail. we illustrate fca-merge with a small example taken from the tourism domain  where we have built several specific ontology-based information systems. our general experiments are based on tourism ontologies that have been modeled in an ontology engineering seminar. different ontologies have been modeled for a given text corpus on the web  which is provided by a www provider for tourist information.1 the corpus describes actual objects  like locations  accommodations  furnishings of accommodations  administrative information and cultural events. for the scenario described here  we have selected two ontologies: the first ontology contains 1 concepts and 1 relations  and the second ontology contains 1 concepts and 1 relations. the underlying text corpus consists of 1 natural language documents taken from the www provider described above. for demonstration purposes  we restrict ourselves first to two very small subsets and of the two ontologies described above; and to 1 out of the 1 documents. these examples will be translated in english. in subsection 1  we provide some examples from the merging of the larger ontologies.
1	linguistic analysis and context generation
the aim of this first step is to generate  for each ontology
　　　　　  a formal context . the set of documents is taken as object set      and the set of concepts is taken as attribute set    . while these sets come for free  the difficult step is generating the binary relation . the relation shall hold whenever document contains an instance of .
　the computation uses linguistic techniques as described in the sequel. we conceive an information extraction-based approach for ontology-based extraction  which has been implemented on top of smes  saarbru：cken message extraction system   a shallow text processor for german  cf.  neumann et al  1  . the architecture of smes comprises a tokenizer based on regular expressions  a lexical analysis component including a word and a domain lexicon  and a chunk parser. the tokenizer scans the text in order to identify boundaries of words and complex expressions like  $1  or  mecklenburg-vorpommern  1 and to expand abbreviations.
　the lexicon contains more than 1 stem entries and more than 1 subcategorization frames describing information used for lexical analysis and chunk parsing. furthermore  the domain-specific part of the lexicon contains lexical entries that express natural language representations of concepts and relations. lexical entries may refer to several concepts or relations  and one concept or relation may be referred to by several lexical entries.
　lexical analysis uses the lexicon to perform  1  morphological analysis  i.e. the identification of the canonical common stem of a set of related word forms and the analysis of compounds   1  recognition of named entities   1  part-ofspeech tagging  and  1  retrieval of domain-specific information. while steps  1    1   and  1  can be viewed as standard for information extraction approaches  step  1  is of specific interest for our instance extraction mechanism. this step associates single words or complex expressions with a concept from the ontology if a corresponding entry in the domainspecific part of the lexiconexists. for instance  the expression  hotel schwarzer adler  is associated with the concept hotel. if the concept hotel is in ontology and document

figure 1: the contexts	and	as result of the first step
contains the expression  hotel schwarzer adler   then the
relation    hotel 	holds.
　finally  the transitivity of the  -relation is compiled into the formal context  i.e. and  implies . this means that if    hotel  holds and hotel  accommodation  then the document also describes an instance of the concept accommodation:
   accommodation 	.
　figure 1 depicts the contexts and that have been generated from the documents for the small example ontologies. e.g.  document doc1 contains instances of the concepts event  concert  and root of ontology   and musical and root of ontology . all other documents
contain some information on hotels  as they contain instances of the concept hotel both in and in .
1	generating the pruned concept lattice
the second step takes as input the two formal contexts and which were generated in the last step  and returns a pruned concept lattice  see below   which will be used as input in the next step.
　first we merge the two formal contexts into a new formal context   from which we will derive the pruned concept lattice. before merging the two formal contexts  we have to disambiguate the attribute sets  since and may contain the same concepts: let	  for . the indexation of the concepts allows the possibility that the same concept exists in both ontologies  but is treated differently. for instance  a campground may be considered as an accommodation in the first ontology  but not in the second one. then the merged formal context is obtained by with     and	.
　we will not compute the whole concept lattice of   as it would provide too many too specific concepts. we restrict the computation to those formal concepts which are above at least one formal concept generated by an  ontology  concept of the source ontologies. this assures that we remain within the range of specificity of the source ontologies. more precisely  the pruned concept lattice is given by 
		 with
as defined in section 1 .
for our example  the pruned concept lattice is shown in

figure 1: the pruned concept lattice
figure 1. it consists of six formal concepts. two formal concepts of the total concept lattice are pruned since they are too specific compared to the two source ontologies. in the diagram  each formal concept is represented by a node. the empty nodes are the pruned concepts and are usually hidden from the user. a concept is a subconcept of another one if and only if it can be reached by a descending path. the intent of a formal concept consists of all attributes  i.e.  in our application  the ontology concepts  which are attached to the formal concept or to one of its superconcepts. as we are not interested in the document names  the extents of the contexts are not visualized in this diagram.
　the computation of the pruned concept lattice is done with the algorithm titanic  stumme et al  1 . it is slightly modified to allow the pruning. compared to other algorithms for computing concept lattices  titanic has - for our purpose - the advantage that it computes the formal concepts via their key sets  or minimal generators . a key set is a minimal description of a formal concept: is a key set for the formal concept if and only if
and for all with . in other words: generates the formal concept .
　in our application  key sets serve two purposes. firstly  they indicate if the generated formal concept gives rise to a new concept in the target ontology or not. a concept is new if and only if it has no key sets of cardinality one. secondly  the key sets of cardinality two or more can be used as generic names for new concepts and they indicate the arity of new relations.
1 generating the new ontology from the concept lattice
while the previous steps  instance extraction  context derivation  context merging  and titanic  are fully automatic  the derivation of the merged ontology from the concept lattice requires human interaction  since it heavily relies on background knowledge of the domain expert.
　the result from the last step is a pruned concept lattice. from it we have to derive the target ontology. each of the formal concepts of the pruned concept lattice is a candidate for a concept  a relation  or a new subsumption in the target ontology. there is a number of queries which may be used to focus on the most relevant parts of the pruned concept lattice. we discuss these queries after the description of the general strategy - which follows now. of course  most of the technical details are hidden from the user.
　as the documents are not needed for the generation of the target ontology  we restrict our attention to the intents of the formal concepts  which are sets of  ontology  concepts of the source ontologies. for each formal concept of the pruned concept lattice  we analyze the related key sets. for each formal concept  the following cases can be distinguished:
1. it has exactly one key set of cardinality 1.
1. it has two or more key sets of cardinality 1.
1. it has no key sets of cardinality 1 or 1.
1. it has the empty set as key set.1
the generation of the target ontology starts with all concepts being in one of the two first situations. the first case is the easiest: the formal concept is generated by exactly one ontology concept from one of the source ontologies. it can be included in the target ontology without interaction of the knowledgeengineer. in our example  these are the two formal concepts labeled by vacation1 and by event1.
　in the second case  two or more concepts of the source ontologies generate the same formal concept. this indicates that the concepts should be merged into one concept in the target ontology. the user is asked which of the names to retain. in the example  this is the case for two formal concepts: the key sets concert1 and musical1 generate the same formal concept  and are thus suggested to be merged; and the key sets hotel1   hotel1   and accommodation1 also generate the same formal concept.1 the latter case is interesting  since it includes two concepts of the same ontology. this means that the set of documents does not provide enough details to separate these two concepts. either the knowledge engineer decides to merge the concepts  for instance because he observes that the distinction is of no importance in the target application   or he adds them as separate concepts to the target ontology. if there are too many suggestions to merge concepts which should be distinguished  this is an indication that the set of documents was not large enough. in such a case  the user might want to re-launch fca-merge with a larger set of documents.
　when all formal concepts in the first two cases are dealt with  then all concepts from the source ontologies are included in the target ontology. now  all relations from the two source ontologies are copied into the target ontology. possible conflicts and duplicates have to be resolved by the ontology engineer.
　in the next step  we deal with all formal concepts covered by the third case. they are all generated by at least two concepts from the source ontologies  and are candidates for new ontology concepts or relations in the target ontology. the decision whether to add a concept or a relation to the target ontology  or to discard the suggestion  is a modeling decision  and is left to the user. the key sets provide suggestions either for the name of the new concept  or for the concepts which should be linked with the new relation. only those key sets with minimal cardinality are considered  as they provide the shortest names for new concepts and minimal arities for new relations  resp.
　for instance  the formal concept in the middle of figure 1 has hotel1  event1   hotel1  event1   and accommodation1  event1 as key sets. the user can now decide if to create a new concept with the default name hotelevent  which is unlikely in this situation   or to create a new relation with arity  hotel  event   e.g.  the relation organizesevent.
key sets of cardinality 1 serve yet another purpose: being a key set implies that neither 
nor  currently hold. thus when the user does not use a key set of cardinality 1 for generating a new concept or relation  she should check if it is reasonable to add one of the two subsumptions to the target ontology. this case does not show up in our small example. an example from the large ontologies is given at the end of the section.
　there is exactly one formal concept in the fourth case  as the empty set is always a key set . this formal concept gives rise to a new largest concept in the target ontology  the root concept. it is up to the knowledge engineer to accept or to reject this concept. many ontology tools require the existence of such a largest concept. in our example  this is the formal concept labeled by root1 and root1.
　finally  the is a order on the concepts of the target ontology can be derived automatically from the pruned concept lattice:
if the concepts and are derived from the formal concepts and   resp.  then  if and only if
　　　　 or if explicitly modeled by the user based on a key set of cardinality 1 .
querying the pruned concept lattice. in orderto supportthe knowledge engineer in the different steps  there is a number of queries for focusing his attention to the significant parts of the pruned concept lattice.
　two queries support the handling of the second case  in which different ontology concepts generate the same formal concept . the first is a list of all pairs
with . it indicates which concepts from the different source ontologies should be merged.
　in our small example  this list contains for instance the pair  concert1  musical1 . in the larger application which is based on the german language   pairs like  zoo1  tierpark1  and  zoo1  tiergarten1  are listed. we decided to merge zoo  engl.: zoo  and tierpark  zoo   but not zoo and tiergarten  zoological garden .
　the second query returns  for ontology with   the list of pairs with . it helps checking which concepts out of a single ontologymight be subject to merge. the user might either concludethat some of these concept pairs can be merged because their differentiation is not necessary in the target application; or he might decide that the set of documents must be extended because it does not differentiate the concepts enough.
　in the small example  the list for contains only the pair  hotel1  accommodation1 . in the larger application  we had additionally pairs like  r：aumliches  gebiet  and  auto  fortbewegungsmittel . for the target application  we merged r：aumliches  spatial thing  and gebiet  region   but not auto  car  and fortbewegungsmittel  means of travel .
　the number of suggestions provided for the third situation can be quite high. there are three queries which present only the most significant formal concepts out of the pruned concepts. these queries can also be combined.
　firstly  one can fix an upper bound for the cardinality of the key sets. the lower the bound is  the fewer new concepts are presented. a typical value is 1  which allows to retain all concepts from the two source ontologies  as they are generated by key sets of cardinality 1   and to discover new binary relations between concepts from the different source ontologies  but no relations of higher arity. if one is interested in having exactly the old concepts and relations in the target ontology  and no suggestions for new concepts and relations  then the upper bound for the key set size is set to 1.
　secondly  one can fix a minimum support. this prunes all formal concepts where the cardinality of the extent is too low  compared to the overall number of documents . the default is no pruning  i.e.  with a minimum support of 1%. it is also possible to fix different minimum supports for different cardinalities of the key sets. the typical case is to set the minimum support to 1% for key sets of cardinality 1  and to a higher percentage for key sets of higher cardinality. this way we retain all concepts from the source ontologies  and generate new concepts and relations only if they have a certain
 statistical  significance.
　thirdly  one can consider only those key sets of cardinality 1 in which the two concepts come from one ontologyeach. this way  only those formal concepts are presented which give rise to concepts or relations linking the two source ontologies. this restriction is useful whenever the quality of each source ontology per se is known to be high  i.e.  when there is no need to extend each of the source ontologies alone.
　in the small example  there are no key sets with cardinality 1 or higher. the three key sets with cardinality 1  as given above  all have a support of  . in the larger application  we fixed 1 as upper bound for the cardinality of the key sets. we obtained key sets like  telefon1
 telephone   ：offentlicheeinrichtung1  public institution    support = 1%    unterkunft1  accommodation   fortbewegungsmittel1  means of travel    1%    schlo 1  castle   bauwerk1  building    1%   and  zimmer1  room   bibliothek1  library    1% . the first gave rise to a new concept telefonzelle  public phone   the second to a new binary relation hatverkehrsanbindung  haspublictransportconnection   the third to a new subsumption schlo  bauwerk  and the fourth was discarded as meaningless.
1	conclusion and future work
fca-merge is a bottom-up technique for merging ontologies based on a set of documents. in this paper  we described the three steps of the technique: the linguistic analysis of the texts which returns two formal contexts; the merging of the two contexts and the computation of the pruned concept lattice; and the semi-automatic ontology creation phase which supports the user in modeling the target ontology. the paper described the underlying assumptions and discussed the methodology.
　future work includes the closer integration of the fca- merge method in the ontology engineering environment
ontoedit. in particular  we will offer views on the pruned concept lattice based on the queries described in subsection 1. it is also planned to further refined our informationextraction based mechanism for extracting instances.
　the evaluation of ontology merging is an open issue  noy and musen  1 . we plan to use fca-merge to generate independentlya set of merged ontologies  based on two given source ontologies . comparing these merged ontologies using the standard information retrieval measures as proposed in  noy and musen  1  will allow us to evaluate the performance of fca-merge.
　on the theoretical side  an interesting open question is the extension of the formalism to features of specific ontology languages  like for instance functions or axioms. the question is     how they can be exploited for the merging process  and     how new functions and axioms describing the interplay between the source ontologies can be generated for the target ontology.
acknowledgements
this research was partially supported by dfg and bmbf.
