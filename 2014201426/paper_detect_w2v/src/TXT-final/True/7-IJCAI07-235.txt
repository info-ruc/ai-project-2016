
dipra  distributed practical reasoning architecture  implements the main principles of practical reasoning via the distributed action selection paradigm. we introduce and motivate the underlying theoretical and computational peculiarities of dipra and we describe its components  also providing as a case study a guards-and-thieves task.
1 introduction
practical reasoning  bratman et al.  1  is a kind of reasoning which is focused on the role of intentions. bdi   belief  desire  intention    rao and georgeff  1  is the most famous agent architecture implementing it  which underestimates however some architectural and cognitive features such as resource-boundedness  knowledge-boundedness and context-sensitiveness  bratman et al.  1 .
모there are four main functions of practical reasoning: means-ends reasoning  opportunity analysis  filtering and deliberation. the peculiarity of practical reasoning is that these operations are managed in a plan-centered way: the adopted plan  filled in with the intention  drives means-ends reasoning  plans are means for the end  the intention   provides constraints for analyzing and filtering opportune options  only options which are relevant with the current intention are evaluated  and sets a priority level for its beliefs  only relevant beliefs will influence further practical reasoning . the rationale behind our work is that a rational agent architecture performing practical reasoning can be implemented as a modular and parallel system  in which each belief  goal  action and plan is a module operating asynchronously  with different activity levels  and having relations with other modules  such as: belief 붹 supports goal 붺 . a special module  the reasoner  maintains a consistent representation of the modules' activation level and their relations by using a fuzzy cognitive map  fcm   kosko  1 . it weighs the alternative goals  exploiting a mixture of means-ends reasoning  opportunity analysis and filtering  and deliberates. there is a continuous interplay between the reasoner and the other modules: after selecting a  new  intention the reasoner assigns to modules an activity level  i.e. the thread's priority  proportional to their values in the fcm  and thus  as we will see  to their contextual relevance   so that more relevant modules influence more the computation. at the same time  the modules act in the environment and provide feedback for the values of the fcm used by the reasoner. for example  a condition can be verified or falsified by an action of the agent  in the example we will provide  detecting if a door is open or close   or a plan can succeed or fail; the results are notified to the reasoner which updates the values of the corresponding nodes in the fcm. as a result  practical reasoning is realized with central deliberation and a decentralized control structure: differently from bdi interpreters  the reasoner simply activates the  modules encapsulating  adopted plans  but after this phase the control flows between the modules in a dynamic way. plans activate actions and subgoals without a new intervention of the reasoner; any further deliberation  choosing subgoals  is performed inside the plan. the activity of the modules  success of action  testing of beliefs and conditions  provides feedback to the reasoner  too.
모in this work we only focus on present-directed intentions  bratman et al.  1 : intentions which are selected to be activated here and now. we illustrate dipra  a modular  parallel and resources-bounded architecture  arguing that it permits to model the four functions of practical reasoning as an interplay of knowledge  goals  contextual factors and opportunities; we also provide a case study.
1 dipra specification and components
the components of dipra are: the reasoner  goals  plans  actions and beliefs. each of these components is implemented as a concurrent module in the multi-thread framework akira  akira  1 ; dipra is also interfaced with an environment  e.g. the physical simulator  irrlicht  1    called the world engine  which evaluates its actions.
모let s be a set of worldstates  p+ a set of atoms  and 뷇 : p+ 뫄 s 뫸  1..1  a function assigning a truth value to each atom in each worldstate. p is a set of atoms and negated atoms where 뷇 p s  == 1 뷇  p s . l is a propositional language over p and the logical connectives 뫇 and 뫈  where: 뷇 p 뫇 q s  := 뷇 p s    뷇 q s  and   is any continuous triangular norm  e.g. min p q  pq ; 뷇 p 뫈 q s  := 뷇 p s   뷇 q s  and  is any continuous triangular conorm  e.g. max p q  x + y   xy   see  saffiotti et al.  1  .
dipra is described by a tuple  붱 붞 붫 붯 bel 붲   where:
- 붱 is the reasoner  a tuple  fcm  body . fcm is a fuzzy cognitive map  kosko  1   a representation of the state and the relations between all the modules; body is the procedural body  whose main task is to assign the status intended to a goal and adopted to a plan.
- 붞 is the set of goals  tuples  type  status  gcond  absrel  conrel . type is the type of goal: achieve or maintain; status is the current status of the goal: intended  instrumental  waiting or not intended; gcond 뫍 l is the  graded  satisfaction condition of the goal; absrel is the absolute relevance; conrel is the contextual relevance.
- 붫 is the set of plans  tuples  status  scond  econd  pcond  actionset  body  goals  results  absrel  pcondrel  conrel . status is the current status of the plan: adopted or not adopted; scond is the set of start conditions sc 뫍 l which are checked at the beginning of plan execution and must be true to start it; econd is the set of enduring conditions ec 뫍 l which are checked continuously during plan execution; if an enduring condition becomes false  the plan is stopped; pcond is the set of beliefs 붹 뫍 l which are expected to be true after the plan  but not all of them have to be intended ; actionset is the set of actions 뷋 or  sub goal 붺 activated by the plan. actions and goals are chained inside actionset by logical connectives 뷂; body is the behavior which is executed; it normally consists in activating actions and  sub goals in the actionset; goals is the set of goals 붺 that make the plan satisfied; they are the subset of pcond which are intended  the reasons for activating the plan ; results is the set of the final plan results pr 뫍 l  corresponding to the gcond of the goals at the end of the plan; absrel is the absolute reliability value of the plan  i.e. how reliably it succeeds; pcondrel is the set of the reliability values ar 뫍 l of the plan with respect to its pconds  i.e. how reliably it produces its pconds; conrel is the contextual relevance.
- 붯 is the set of actions  tuples  scond  pcond  body  goals  results  absrel  pcondrel  conrel . scond is the set of start conditions sc 뫍 l which are checked at the beginning of action execution and must be true to start it; pcond is the set of beliefs 붹 뫍 l which are expected to be true after the action  but not all of them have to be intended ; body is the behavior which is executed once the action is executed; goals is the set of goals 붺 that make the action satisfied; they are the subset of pcond which are intended  actually the reasons for activating the action ; results is the set of the final action results ar 뫍 l  corresponding to the gcond of the goals at the end of the action; absrel is the absolute reliability value of the action  i.e. how reliably it succeeds; pcondrel is the set of the reliability values ar 뫍 l of the action with respect to its pconds  i.e. how reliably it produces its pconds; conrel is the contextual relevance.
- bel is the set of epistemic states  i.e. beliefs 붹 뫍 l. all the conditions  gcond  scond  pcond  econd  are kinds of beliefs. bel are tuples  붹  absrel  conrel . 붹 뫍 l is the value of the belief or condition; absrel is the absolute relevance; conrel is the contextual relevance.
- 붲 is a set of parameters used to control the energetic dynamics of the modules:  is the activation of the reasoner; 붽 is the threshold for goal intention; 붾 is the threshold for plan

figure 1: the fcm used for the thief in the house scenario

figure 1: the house scenario  description in the text 
adoption; 붿 is the commitment level of intended goals; 뷁 is the commitment level of adopted plans;   is the total amount of resources available to the whole system.
1 the reasoner
the reasoner maintains a consistent representation of the activity of all the modules and performs deliberation using an additive fuzzy system called fcm  kosko  1  whose nodes and edges represent the modules and their links  and in which activation spreads between nodes. fig. 1 shows a sample fcm in the house scenario  fig. 1  introduced later .
모deliberation consists in intending a goal and adopting  the best  plan for it; it is performed by the fcm by weighting the alternative plans and goals and  at the same time  by evaluating chains of goals and plans  including of course conditions. in traditional practical reasoning there are three different mechanisms for generating alternatives: means-ends analysis  opportunity filtering  filter overriding. the fcm formalism permits to represent the constraints of all these mechanisms in a compact way  and to provide at the same time suitable values for deliberation. the fcm can represent many typical situations in practical reasoning: goals concurrence  via inhibitory links ; beliefs sustaining a plan or a goal; a plan preferred to another one because one of its preconditions is already matched; a goal activating one or more plans which are able to satisfy it  etc.
모in the fcm there are six kinds of  weighted  links:  1  satisfaction a goal links plans and actions whose pcond satisfy its gcond; in this way  activation is spread from intended goals to plans which realize them.  1  predecessor a plan links  sub goals whose gcond realize its scond or econd; in this way  if a plan has a missing pcond or econd it can  subgoal .  1  support a belief links goals  plans or actions which correspond to their gcond  scond or econd; this is a way to represent contextual conditions: goals  plans and actions which are  well attuned  with the context are preferred.  1  feedback a plan or action feedbacks on goals; this special case of support permits to select goals having good paths to action  1  inhibition goals and plans with conflicting gconds or pconds  and plans realizing the same gcond have inhibitory links. in this way it is possible for a goal or plan  especially if intended or adopted  to inhibit competitors.  1  contrast beliefs have inhibitory links with plans and goals having conflicting gconds  pconds  sconds and econds: this is a kind of  reality check .
the cycle of the reasoner
the reasoner  and the fcm  runs concurrently with all other modules  having an activation : in a real-time system  even reasoning takes resources. the reasoner has two main tasks:  1  to deliberate  select a goal and a plan  and  1  to set the activation of the modules. both are realized in this cycle:
1. set the values of the fcm nodes according to the activitylevel of the corresponding modules1  and their links;
1. run the fcm and obtain the values of the nodes; as explained later  this value represents the contextual relevance  conrel  of the corresponding modules;
1. the most active goal is selected  if over a threshold 붽 ; if not already achieved  its status becomes intended  intended goals replace old intended ones . otherwise  another goal has to be selected. a recurrent connection  with weight 붿  is set for the intended goal  which thus gains activation1.
1. the most active plan for the intended goal is selected  ifover a threshold 붾 ; its status becomes: adopted. the plan is filled in with the intended goal: the goal of the plan becomes the gcond. if there is an already adopted plan  it is stopped only if its pcond conflict with the conditions of the new adopted one. a recurrent connection  with weight 뷁  is set for the adopted plan.
1. if no plans are possible for the intended goal  its statusbecomes waiting  and maintains the recurrent connection ; a new goal has to be intended  this is unlikely  since the evaluation of a goal also depends on how suitable are its plans ;
1. if no goals or plans are over the thresholds 붽 and 붾  the thresholds lower and the cycle restarts. otherwise sets the activity level of the modules to the value of the nodes in the fcm. thus  even if the reasoner runs concurrently with the other modules  it resets their activity level only if a new plan is adopted  either or not a new goal is intended . resources boundedness is guaranteed by the parameter  : the total amount of activation to be assigned to all the modules can be fixed so that the computation never exceeds that threshold.
contextual relevance and impact. the value of the nodes in the fcm represent the contextual relevance of the corresponding modules. the value of the edges in the fcm represent the impact of the corresponding modules; by default they are set according to the  epistemic component  of the module: the value of a belief  the gcond of the goal  the scond or econd of the plan. for example  if a belief has 붹 = 1 and its sustains a goal  the impact of its edge in the fcm is +1. the impact of the modules varies during the computation; for example  an achievement goal which is close to satisfaction inhibits more and more its competitors.
모not all the modules have to be represented at once in the fcm  and not all the threads have to run . fcm nodes having contextual relevance equal  or close  to zero have no impact and can be deleted  and the threads of the corresponding modules stopped : in this way only relevant knowledge is considered  and the fcm never exceeds a certain size. this feature is very useful in means-ends analysis: at the beginning  only top-level plans are considered in the fcm; plans  and the fcm  are filled in with subplans only as long as the activity proceeds. knowledge augments in a bounded way  too  as long as conditions and beliefs related to active plans  actions and goals are checked.
1 beliefs  conditions and goals
all the declarative components use fuzzy logic  kosko  1 . all the conditions  goal conditions  pre and post conditions of plans and actions  etc.  are special kinds of beliefs. for example  a belief   office is far   can be matched using fuzzy rules with the precondition of a plan   office is close   and generate a graded truth value. also goal conditions share this formalism; in this way they can be matched e.g. against post conditions in order to verify their satisfaction  e.g. the goal  go to office  becomes more and more satisfied when the truth value of  office is close  increases .
모there are policies for both achieve and maintain goals. in achievement goals  such as  reach office    the contextual relevance increases on nearing the goal  when the truth value of the gcond increases . in maintain goals  such as  stay close to office    the contextual relevance lowers on nearing  when the truth value of the gcond increases .
intended vs. instrumental goals. in practical reasoning it is assumed that only one goal is intended  but many goals can be active at once  and activate plans or actions ; they are named instrumental goals as opposed to intended ones; their purpose is to favor the intention  e.g. by creating appropriate contextual conditions. if the intended goal  or another instrumental goal  they depend on is achieved  they are stopped.
1 plans
plans are the main control structures in dipra; they do not depend on the reasoner except for starting. plans are activated for satisfying an intended goal; once the plan is adopted  a subset of their pcond is set as goal. a plan is basically an execution scheme  activating actions and goals from the actionset and subgoaling; this is their behavior:
- if the intended goal is already achieved  the plan returns immediately and no action is executed.
- if any scond or econd is false  the plan  delegates  their satisfaction to other modules by passing them activation via the predecessor links; subgoals activated in this way gain the status of instrumental.
- if all the scond and econd are met  the plan starts executing the actions in the actionset  chaining them according to the connectives in the actionset1. plans can load from the actionset not only actions  but even goals. this mechanism produces the typical subgoaling of practical reasoning:  sub goals activate  sub plans or actions  and so on. even goals activated in this way gain the status of instrumental.
- plans continue subgoaling and executing their body until all the possible actions and subgoals fail. a failed plan returns the control to the calling goal  which remains not satisfied and activates another plan. however  it is likely that unsuccessful plans are stopped before exhausting all the possibilities; in fact  if many conditions of a plan fail  despite commitment it weakens in the fcm and other plans replace it.
plans and subgoaling. there are two subgoaling mechanisms realized by the plans: the first one consists in activating goals which realize their scond and econd  if they are not already realized ; the second one consists in activating goals instead of actions from the actionset. both kinds of goals are instrumental. at the same time  plans spread activation to instrumental goals  which gain priority.
1 actions
an action is the minimal executable operation; typically it consists in an interaction with the world engine; but actions can also check  add  remove or modify a belief  epistemic actions . actions are activated by goals whose gcond correspond to their pcond or by plans via the actionset.
post conditions vs. goals. in practical reasoning  not all the expected results of actions and plans are intended. when the plan is adopted  one of its pcond  corresponding to the intended goal  is selected and becomes the goal; the same happens to actions. depending on the situation  actions and plans can be activated for different reasons: their post conditions are the same  but the goal is different.
1 dynamics in dipra
even if there is only one intention  many modules can be active at once in dipra. not intended and not adopted goals and plans have a certain amount of activation  too  which can be used for fulfilling operations such as building up parts of the fcm  although these operations tend to be slower.
goal-driven pressures. goals represent desired states of the system. in dipra an active goal  drives  the computation toward a certain result  such as  office   in three ways:  1  by actively competing for being intended: in this way they can lead to adopt an explicit plan leading to office.  1  by indirectly introducing a pressure. even not intended goals have an influence which is proportional to their activation. for example  in choosing between two plans  an active but not intended goal can do the difference  e.g. by reinforcing a plan whose pcond are close to its gcond  or by weakening a plan whose pcond are far from its gcond .  1  by updating knowledge related to them  e.g. gcond ; in this way more beliefs which are pertinent to the goal  and in principle can reinforce it or weaken the other ones  are produced.
epistemic dynamics. knowledge is distributed and available to different extent to deliberation  depending on the activation level of the modules corresponding to beliefs. for example  not all the consequences of adopting a plan  e.g. subplans  pconds  can be considered in means-ends analysis  but only those currently available; this is why sometimes long term conflicts are discovered only after a plan is adopted. this is represented by putting in the fcm only beliefs  plans and goals having a non-zero contextual relevance.
모it is important to note that more active beliefs intervene more into the computation: they activate more the goals and plans they support. this aspect models their availability: for example  an highly active belief is ready to be exploited for reasoning and  if it sustains a goal  gives it more activation.
모beliefs are retrieved in an activity-based and bounded way: not all the knowledge is ready to be used  but modules actively search for and produce knowledge  and that activity takes time  with a bias toward knowledge useful in the context of the most active goals. goals  plans and actions assign an updated truth value to their conditions  and to related beliefs  during their execution  for example by reading a sensor or asking memory; more active beliefs  receiving activation from more active goals and plans  will auto-update their truth value more frequently. produced  or updated  beliefs are added to the current state  and to the fcm  and linked to the relevant goals  plans or actions: new knowledge can lead to intention reconsideration or to replanning. more active goals and plans can build longer means-ends chains and have more  up-to-date  knowledge  since they can perform more epistemic actions. the epistemic component of dipra  what the agent knows  is influenced by its current activity  what the agent is doing : in practical reasoning a crucial role of intentions is selecting relevant information.
1 practical reasoning in dipra
in dipra means-ends analysis  opportunity filtering and filter overriding are  weak constraints  of the same mechanism and  at the same time  provide suitable values to deliberation.
means-ends analysis and deliberation. means-ends analysis builds causal chains: what is necessary for achieving a goal. deliberation evaluates utility: what is better for achieving a goal. these two activities are related. means-
ends analysis consists in building causal chains of means  of plans   sub goals  conditions and actions  to achieve ends. normally this process is incremental: even if declarative knowledge about plans and their effects is already available  in order to fulfill new goals a  chain of means  has to be built anew. we have seen that the fcm is more and more filled in with elements of this chain  as long as the analysis proceeds  new nodes are added as the result of epistemic actions of the modules  e.g. a plan verifying its preconditions ; and as long as the agent acts  its actions have consequences which can be added as beliefs . the rationale is that knowledge related to the goals and plans  e.g. about conditions and actions  becomes more and more  relevant  and is thus added to the fcm. normally means-ends analysis is performed only for top-level plans  which are not totally filled in. however  plans whose chains  from top-level plans to terminal actions  are stronger  having reliable subplans and actions and true conditions  are privileged  because top-level plans gain more activation from them. as long as the analysis proceed  with or without adopting a plan  e.g. if the threshold 붾 is not reached  or if there is another adopted plan   new knowledge about the plan is added and it can make it more likely to be selected.
모means-ends analysis  which is mainly qualitative  produces at the same time results which are suitable for deliberation  because the utility of a course of actions depends also on the availability of the conditions and the reliability of the actions. since in the fcm the plan receives activation from all its conditions  while performing means-ends analysis the  best  plans receive also more and more activation. it is also very likely that the most active plan has many pcond and econd already met  at least partially . in a similar way  plans having highly reliable actions are more likely to be very active. in this way  deliberation exploits the results of meansends analysis: the values of the nodes in the fcm  built during means-ends analysis  can be directly used for selection  we provide as a simple heuristic: choose the highest one  but more sophisticated ones are possible .
모all the preference factors normally related to goals and plans in the bdi  e.g. urgency  utility  are encoded into modules activation. preference is mainly based on epistemic factors: goals and plans are activated by knowledge  that can be explicitly represented  e.g.  goal x is very important    implicitly represented into the modules  e.g. a pre condition of a plan  or encoded in the relations between the components  e.g. a link between a goal and a plan means that the plan is able to satisfy the goal . the rationale is that the belief structure of an agent motivates its choices and preferences; the causal structure built by means-ends reasoning is also used for deliberation. there are two main difference with practical reasoning as traditionally implemented  e.g. in bdi :  1  conditions satisfaction and action evaluation are treated as  weak constraints ;  1  there is an active and bounded view of how knowledge to be evaluated is added.
opportunity analysis and filtering. in traditional implementations of practical reasoning the consistency of new plans or goals with old ones is routinely checked; inopportune plans and goals are ruled out. eventually  an intention which is discarded because of its incompatibility can be reconsidered in another mechanism  the filter overriding. in dipra these brittle and costly operations are replaced by  weak constraints  in the fcm: plans or actions which pcond conflict with existing states  or desired ones such as goals  are simply much less likely to be selected and  at the same time  become less and less relevant. this is mainly due to the inhibition and contrast links  but also to the fact that selected goals and plans create areas of high  relevance  around them: conditions and beliefs which potentially activate them are very likely to be added to the fcm.
모in general  some requisites of practical reasoning  such as opportunity analysis  are perhaps too strong; we argue that a cognitive agent  with limited rationality and bounded resources  implements weaker requirements. for example  an intended goal or an adopted plan do not rule out their competitors  but simply gain more contextual relevance and weakens the other alternatives  too. new goals can be intended and new plans adopted if they are able to overwhelm the  weight  of the previous ones. intention reconsideration  changing goal  or replanning  changing plan  only occur when needed. once a goal is intended  it only has to be replaced if a goal which is more important or was previously intended but was not executable becomes achievable. once a plan is adopted  it only has to be replaced if one of its econd become false or if its goal is no more intended. all these situations happen naturally in the fcm.
commitment. the most distinctive point of a practical reasoning agent is that it is committed to its intentions  and to doing what it plans . commitment  however  comes in grades  since agents should also be able to be opportunistic and revise their intentions. commitment is implemented in bdi as a strict rule; in dipra is comes in grades and it is regulated by two parameters  붿 and 붺. commitment to a goal or a plan is also maintained by the structure of the links  since achievement goals which are close to satisfaction impact more and more  and adopted plans are more and more reinforced by their conditions which increase their truth value.
1 a case study: the house scenario
we implemented the house scenario  see fig. 1  using the framework akira  akira  1  and the 1d engine  irrlicht  1 ; the house has five rooms and seven doors which open and close randomly. the agent we model is the thief; it appears in a random position in the house  having the achievement goal to possess the valuable v  that is hidden in the house  and the maintenance goal to avoid the guard  an agent which moves randomly  but when spots the thief moves straight toward it . the guard and the thief have the same size and speed  and a limited range of vision. four implementations of the thief were tested:  1  dipra;  1  a baseline  random system ;  1  the a* algorithm  hart et al.  1   which has full knowledge of the environment  including the location of v  plans the shortest path to it but and replans when something changes in the environment  e.g. a door closes ;  1  a classic bdi  based on  rao and georgeff  1   having the same goals  plans and beliefs of dipra .
모percentage of success  having v without being captured by the guard  was measured in 1 runs: analysis of variance  anova  shows that dipra  1%  performs significantly better than the other strategies  p   1 in all cases :

figure 1: fcm after intention change  description in the text 
baseline  1%   a*  1%  and bdi  1% . resources and knowledge boundedness make dipra much more efficient in real time and dynamic situations.
모some situations occurred during the simulations may help illustrating the behavior of dipra. consider the following case: the thief is in the bathroom and has the goal to find the valuable v  assuming by default that all the doors are open  and to escape the guard. fig. 1 shows the corresponding fcm  including two competing goals  have v and escape guard  note that all the horizontal links are inhibitory . since only the former is contextually relevant  1 vs. -1   only its  means-ends  causal chain is constructed by dipra and included in the fcm  all the other goals  plans and beliefs are supposed to have a value close to zero . given this contextual situation  with many possible goals and plans supported by many beliefs  the thief intends the goal with the highest value  1   search living; this goal is selected both because the living is the closest room and because the thief believes that v is there. now the thief adopts the best plan  1  realizing the intended goal  pass 1. actions  such as moves  are not shown in the fcm. now  if door 1 is found closed  the intention remains the same and a new plan is selected: passing doors 1 and 1  this situation is not shown here . if door 1 is close  too  it is impossible to realize any plan for the given intention. assuming that no subgoal  such as: open door 1 or 1  is possible  it is necessary to have a new intention  e.g.
search the kitchen : the resulting fcm is shown in fig. 1.
모another case of intention reconsideration  different from plans failure  is a conflict between an intention and another goal which becomes contextually active  i.e. an opportunity. for example  while the thief has the intention to search the living  processing the plan to pass doors 1 and 1   it could spot the guard near door 1. at that point  the goal to avoid the guard comes in play  too  and it could be so strong to defeat the current intention  becoming the new intention.
1 conclusions
deliberation is implemented in bdi via a central interpreter  which selects goals and plans  updates knowledge and monitors the environment. dipra instead distributes control among semi-independent  parallel modules  goals  plans and actions  which are assigned an activity level proportional to their contextual relevance; more relevant goals and plans are more likely to be selected. also knowledge is distributed among modules representing beliefs  preconditions or postconditions. the only central component  the reasoner  is only responsible for setting the activity level of the modules once a new intention is selected. in dipra  practical reasoning is en emergent property of the modular architecture.
모an advantage of distributed systems is that many situations  such as the conflicts between goals and means-ends analysis  are resolved on-line by a dynamic  anytime system. many interesting dynamics emerge; for example  two conflicting goals can influence one another even via energy dynamics in a way that varies with time. or  a given plan can start with many resources when a goal is very powerful  be weakened when the goal weakens  and be stopped when a conflicting goal grows and inhibits the former. all these possibilities have not to be pre-planned  i.e. the exact moment when the plan stops is not explicitly set but it depends on the dynamics of the system. moreover  the relations of conflict or cooperation between two goals have not to be always explicitly represented  with inhibition links  but can emerge as set points of the system's dynamics.
모dipra is influenced by its expectations and monitors them. goals represent desired and expected future states; plans and actions have explicit pconds. by activating goals  plans and actions some  beliefs about the future  appear in the fcm and influence the deliberation. as it happens for all the beliefs  modules for testing pconds become active  too.
모in our experiments the model has shown to be effective and scalable: the competition between goals and plans is credible; since only relevant modules are considered  even adding more goals  plans and actions the size of the fcm remains bound. the system is committed to its current goals and plans and it is smooth in shifting from one another. as shown in  kosko  1   machine learning techniques such as hebbian learning can be used for learning the fcm  too.
