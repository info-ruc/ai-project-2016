
this paper proposes a new method to rank the cases classified by a decision tree. the method applies a posteriori without modification of the tree and doesn't use additional training cases. it consists in computing the distance of the cases to the decision boundary induced by the decision tree  and to rank them according to this geometric score. when the data are numeric it is very easy to implement and efficient. the distance-based score is a global assess  contrary to other methods that evaluate the score at the level of the leaf. the distance-based score gives good results even with pruned tree  so if the tree is intelligible this property is preserved with an improved ranking ability. the main reason for the efficacity of the geometric method is that in most cases when the classifier is sufficiently accurate  errors are located near the decision boundary.
1 introduction
decision trees  dt  are a very popular classification tool because they are easy to build and they provide an intelligible model of the data  contrary to other learning methods. the need for intelligibility is very important in artificial intelligence for applications that are not fully automatic  if there is an interaction with the end-user  expert or not. this is the reason why dt algorithm are widely used for classification purpose  see murthy  for examples of real world applications . but for some applications knowing the class of each case is not sufficient to make a decision  one needs to compare the cases to one another in order to select the most promising examples. this is often the case in marketing applications  allocation of resources or of grants  etc.  see  zadrozny and elkan  1  for a description of the charitable donation problem . the traditional idea in this case is to look for the probability of each case to belong to the predicted class  rather than just the class. the cases are then ranked according to the probability-based score. unfortunately  methods that are highly suitable for probability estimate produces generally unintelligible models. this is the reason why some recent works aim at improving decision tree probability estimate. smoothing methods are particularly interesting for that purpose. they consist in replacing the raw conditional probability estimate at the leaf by some corrected ratio that shifts the probability toward the prior probability of the class. the raw conditional probability estimate at the leaf is defined by  where k is the number of training cases of the class label classified by the leaf  and n is the total number of training cases classified by the leaf. it is the same for all the cases that are classified by a leaf. the most general type of correction generally used are the m-estimate pm  see equation  1    which uses the prior probability of the class and a parameter m  and the laplace correction pl which is a particular case of m-correction when all the c classes have the same priors  see  cestnik  1; zadrozny and elkan  1  .

the main interest of smoothing methods is that they don't modify the structure of the tree. but in order to improve the probability estimate  these methods are often applied to unpruned trees  see  provost and domingos  1    so the intelligibility of the model is very much reduced  although it is one of the main interest of decision trees compared to other classifiers  like naive bayes  neural networks for instance . ensemble methods like bagging are also used successfully to rank cases  although the margin is not a priori an estimate of the class membership. nevertheless  ensemble methods loose also the intelligibility of the model.
모the method we propose here aims firstly at preserving the intelligibility of the model  so the objective is to improve the ranking without modifying the tree itself. this method is based on the computation of the distance of the cases from the decision boundary  the boundary of the inverse image of the different classes in the input space   when it is possible to define a metric on the input space. the distance of a case from the decision boundary defines a score that is specific to each case  unlike other methods for which the score is defined at the level of the leaf and so it is shared by all cases classified by the same leaf. in other geometric methods  like support vector machine  svm  it has been proved that the distance to the decision boundary can be used to estimate the posterior probabilities  see platt  for the details in the two-class problem : an additional database is needed in order to calibrate the probabilities. but since in many applications
	database	dataset	 ut-pt 	 ut 

table 1: comparison of the size of pruned  pt  and uncollapsed unpruned  ut  trees: mean and standard deviation of the difference of the number of leaves n over 1 resamples.
we don't need the exact posterior probability  it is generally possible to use directly the score induced by the distance to rank and to select the most interesting cases.
모the paper is organized as follow: section 1 examines from the intelligibility viewpoint the methods applied to decision trees to rank cases or to estimate posterior probabilities. section 1 presents our method for obtaining a distance-based score  and it explains why it is interesting from a theoretical point of view. section 1 presents the experimental results which have been drawn from the numerical databases of the uci repository  in comparison with the results obtained from the smoothing methods applied on the same databases. we make further comments about geometric score and hybrid method in the concluding section.
1 decision tree methods for ranking: the intelligibility viewpoint
the success of decision trees as classification method is for a good part due to the intelligibility of the model produced by the algorithms. pruning methods  breiman et al.  1; bradley and lovell  1; esposito et al.  1  produce shorter trees with at least the same performance than longer trees  since the generalization performance are enhanced. they also produce shorter tree on purpose  seeking for a compromise between accuracy  or other performance criteria  and the size of the tree. table 1 shows that unpruned trees can be very large compared to pruned trees with similar accuracy  the mean absolute difference over the databases is 1% and it is always less than 1% . because of this size problem  it is desirable to improve the probability estimate given by dt  in order to allow a compromise between size and ranking ability.
모with smoothing methods the probability estimate is the same for all the examples classified by a leaf. in order to produce more specific probability estimates  other methods learn directly the probability class membership at the leaf. for instance   smyth et al.  1  use kernel-based density estimator at the leaf  without modification of the tree structure. this method improves significantly the class probability estimates. but the practical use of kernel density estimator is limited to very low dimension  and the setting of parameters is not easy. kohavi  builds naive bayes classifiers at the level of the leaf  using its own induction algorithm. the objective of the tree partition is not to separate the classes but to segment the data so that the conditional independence assumption is better verified. the size of the tree is limited to cover each leaf with enough data. in our experiment the size of the naive bayes trees  nbt  is comparable to the size of the pruned trees  but the segmentation of the space is completely different . with different objectives and structures  the interpretation of dt and nbt cannot compare easily.
모other methods try to correct the probability estimate at each nodes by propagating a case through the different possible path from each node. these methods  like fuzzy trees
 umano et al.  1   fuzzy split  quinlan  1   or more recently  ling and yan  1  deal with a different issue: managing the uncertainty in the input case and in the training database. generally the computation of the probability estimate is very complex and in some cases difficult to understand: a lot of nodes can be involved  although non-convex area of the input space corresponding to one class can be divided arbitrarily into several leaves. so from the point of view of intelligibility these methods are not totally convincing.
모we propose here to keep the structure of the pruned tree but to rank the cases accordingly to their distance from the decision boundary which is defined by the tree.
1 distance ranking methods for decision trees
we consider here axis-parallel dt  adt  operating on numerical data: each test of the tree involves a unique attribute. we note 붞 the decision boundary induced by the tree. 붞 consists of several pieces of hyperplanes which are normal to axes.
모we consider a multi-class problem  with a class of interest c  the positive class . let x be a case  c x  the class label assigned to x by the tree  d = d x 붞  the distance of x from the decision boundary 붞. we use the distance of an example from the decision boundary to define its geometric score.
1 global and local geometric ranking
definition 1 geometric score
모the geometric score g x  of x is the distance of x from the decision boundary if c x  = c and its opposite otherwise.
if c x  is the positive class   1 
	 d x 붞 	otherwise.
theorem 1 global geometric ranking
모the geometric score induce a quasi-order  over the examples classified by the tree.
		 1 
cases are ranked in decreasing order relatively to the geometric score. the most promising cases have the highest geometric score  which means that their predicted class is the positive one and that they are far from the decision boundary.
모with the geometric score  examples are ranked individually  not leaf by leaf.
모the geometric score is specific to each example  so it is also possible to first rank the leaves with a smoothing method  or an equivalent method that ranks the leaves  not the cases  and then to rank the cases inside a leaf.
theorem 1 local geometric ranking
모the geometric score induce a quasi-order l over the examples classified by a leaf.
 1 
.
모with the local geometric score  the leaves are ranked according to the probability estimate  then inside each leaf  or inside each group of leaves with the same output probability estimate   examples are ranked according to their geometric score.
모the distance of a case x to the decision boundary is computed with the algorithm described in  alvarez  1 . it consists in projecting x onto all the leaves f which class label differs from c x . the nearest projection gives the distance.
algorithm 1 distancefrom x dt 
1. d = カ
1. gather the set f of leaves f which class c f  =1	c x ;
1. for each f 뫍 f do: {
1. compute pf x  = projectionontoleaf x f ;
1. compute df x  = d x pf x  ;
1. if  df x    d  then d = df x  }
1. return d = d x 붞 
algorithm 1 projectionontoleaf x f =  ti i뫍i 
1. y = x;
1. for i = 1 to size i  do: {
1. if y doesn't verify the test ti then yu = b } where ti involves attribute u with threshold value b
1. return y
the projection onto a leaf is straightforward in the case of adt since the area classified by a leaf f is a hyper-rectangle defined by its tests. the complexity of the algorithm is in o nn  in the worst case where n is the number of tests of the tree and n the number of different attributes of the tree.
1 theoretical viewpoint
we expect geometric ranking to give interesting results when errors occur near the decision.
모if this property is verified  positive cases  cases which class is the class of interest  that are not recognized have negative geometric score but with a small absolute value. false positive  that is negative cases classified as positive have also small  but positive  geometric score. on the contrary  true positive have higher geometric score and true negative have negative geometric score with high absolute value. so independently from the score estimated at the leaf  the geometric score tends to bring side by side false negative and false positive  and to repel true positive and true negative. this can be seen on a receiver operating characteristic  roc  curve  in the way described in  adams and hand  1  . the ratio of positive examples is plotted against the ratio of all other  negative  examples as the score varies. with methods that give constant probability estimates at the leaf  the points are plotted from one leaf to another. the affine interpolation between consecutive points assumes that examples are selected randomly inside a leaf  or a set of leaves with the same score . if we use a roc curve to visualize the ranking  geometric ranking will be very good at the beginning of the curve  as seen in figure 1.

negative ratio
figure 1: roc curves of different ranking method  wd breastcancer sample . the local geometric ranking curve intersects the basic m-estimate curve at leaf points.
모since dt algorithms are generally designed to maximize accuracy  it is not unreasonable to hypothesize that errors lie near the decision boundary. in a very ideal case  it is even possible to demonstrate that this hypothesis is true.
모a dt builds a partition in the input space. in a two class problem  it is possible to associate to a dt a unique function g : e 1뫸 {1} such that g x  is the predicted class of x.  but several decision trees are associated to the same function . we consider an ideal case of statistical decision  where the joint distribution p of observations would be uniform on the graph of the indicator function f of a set s in e =  1 n. we also suppose that the size of all the maximal hypercubes in s and e s has a lower bound 뷄   1  to prevent pathological situation for s and its boundary . in this case  decision trees built on samples drawn from p can approach f as closely as wanted if the size of the sample can grow indefinitely. for this particular case of function f  errors are located near the decision boundary.
theorem 1 proximity of errors. for a dt which associated function g is close enough to f  errors are near the decision boundary  g of g.
모if we note a the area where f   g = 1  set of errors   a빡 the interior of a  and if we consider   small enough so that     뷄n  we have:

모proof. let x be in a빡  we consider the maximal hypercube centered at x in its connected component. the volume of b is included into re |f   g|  so we have:뫏n dn       뷄n. so the size of b is smaller than 뷄 and d    . the boundary of b   b  encounters the decision boundary of f   g on at least two meeting points of two different hyperplanes  since b is maximal. if f is not constant on b  then both  f and  g cross b  and so necessarily is constant on b  since the size of b is smaller than 뷄  at least one of the meeting point lies on  g  otherwise the size of b would be smaller than the lower bound of the maximal balls .
so once again
모even if real conditions are very far from this ideal case  in first place  generally f doesn't exist   we can test if the hypothesis of proximity of errors is generally verified. table 1 shows the mean of the difference of the mean distance of correctly classified cases  hits  and errors from the decision boundary. we also computed for each sample 뷂  the inverse of the coefficient of variation for the difference of the means defined by  1   where dh and 횯 are the mean and the standard deviation of the distance of correctly classified examples from the decision boundary  and de and 횬 the same magnitude for error examples.
		 1 
table 1 shows the percentage of the samples for which 뷂 뫟 1  which is the 1% confidence coefficient under the normal assumption  the test is unilateral . we can see that errors are closer from the decision boundary for a majority of databases. datasets for which this property is not verified have generally a low mean accuracy  1% for bupa and 1% for sonar . if we consider only the samples for which the accuracy is better than 1%  the proportion shifts to 1% and 1% respectively.
  of the% of samplesdatabasemeans뷂with 뷂 뫟1bupa11111glass11111ionosphere11111iris11111newthyroid11111optdigits11111pendigits11111pima11111sat11111segment.11111sonar11111vehicle11111vowel11111wdbc11111wine11111table 1: comparison of the mean distance of errors and hits to the decision boundary  over the test bases of 1 samples per database. the mean of the difference is estimated for each sample.  bad results are bold 
모a corollary of theorem 1 is that if a tree is not accurate  errors may lie everywhere  not only near the decision boundary.
in this case the geometric score cannot be good. so we expect the geometric score to be better with more accurate trees.
1 experimental results
1 experimental design
we have studied the geometric ranking on the database of the uci repository  blake and merz  1  that have numerical attributes only and no missing values. we are not directly concerned in this study with the problem of the prevalence of the positive class  since our method doesn't build the decision tree: it applies on the grown tree. so we didn't pay any particular attention to the relative frequency of the classes in the datasets. we chose as positive class either the class with the lowest frequency in the database  either a class which grouped together several classes when it was more logical. when the classes were equiprobable and with no particular meaning we chose it randomly. although there is a lot of work on the analysis of multi-class problem  for simplicity we have treated multi-class problem as a two class problem  class of the examples were modified before growing the trees .
모for each database  we divided 1 bootstrap samples into separate training and test sets in the proportion 1 1  respecting the prior of the classes  estimated by their frequency in the total database . even if it is not the best way to build accurate trees for unbalanced dataset or different error costs  here we are not interested in building the most accurate or efficient tree  we just want to study the effect of geometric ranking on pruned trees. for the same reason we grow trees with the default options of j1  weka's  witten and frank  1  implementation of c1  although in many cases different options would build better trees. for unpruned trees we disabled the collapsing function.
모we used laplace correction and m-estimate smoothing methods to correct the raw probability estimate at the leaf for reduced-error pruned tree and normal pruned tree. the value of m was chosen such that m 뫄 p c  = 1 where p c  is the prior probability of the class of interest  as suggested in  zadrozny and elkan  1  .
모we used two different metrics in order to compute the distance from the decision boundary  the min-max  mm  metric and the standard  s  metric. both metrics are defined with the basic information available on the data: an estimate of the range of each attribute i or an estimate of its mean ei and of its standard deviation si. the new coordinate system is defined by  1 .
		or	
the parameters of the metric are estimated on each sample. the choice of the metric has a very limited effect on the geometric score; if we measure the difference between the area under the roc curve  auc    for each database  it is always less than 1 1  1 1  except for the thyroid and vehicle1 databases  less than 1    and the glass database  1   .
1 comparison between distance-based ranking and smoothing methods
the geometric score is only used to rank the examples without changing the tree structure. it is not used to estimate the
red.-errornormal	nodatasetpruningpruning	pruning	nbtreebupa11-11 -11 1 1glass11-11 -11 -11iono.-11-11	-11 -1 1iris1111 11	11letter1111 -11	11thyroid1111 11 -11optdig.1111 11 -11pendig.1111 1111pima11-11 -1111sat1111 1111segment. 1111 1111sonar	1111 11-11vehicle	1111 -1111vowel	1111 1111wdbc	1111 1111wine	1111 11-11table 1: absolute difference of the auc between global geometric ranking with standard metric and smoothing methods at the leaf. the last column shows the difference between global geometric ranking on red.-error pruning tree with nbtree.  all mean values and standard deviations are 뫄1. insignificant values are italic. bad results are bold 
posterior probability of an example  so the appropriate measure of performance in that case is the auc. table 1 shows the difference between global geometric ranking and laplace or m-estimate correction at leaf.
모apart from a few cases  global geometric ranking gives better values than either laplace or m-estimate correction  with a 1% confidence coefficent . the differences are relatively small  from 1 to 1   but since they are absolute values the improvement can be important. we have also shown the difference of the auc between global geometric ranking on reduced-error pruned tree and nbtree.
모table 1 shows the difference between local geometric ranking and smoothing correction at leaf. local geometric ranking is always better  with a 1% confidence coefficent  than smoothing method alone  except in one case which is not significant. but like for global ranking  the improvement can vary a lot  absolute value from 1 to 1 .
모as we said in the theoretical viewpoint section  we expect geometric ranking to outperform smoothing method at the beginning of the roc curve. to measure the relative behavior of roc curves for increasing value of the negative ratio  we have computed auc x   1 뫞 x 뫞 1  the integral function of the roc curve  with a 1 step value  for the global geometric score  g  and the smoothing correction  s . table 1 shows for normal pruned trees theshows the maximum absisse value x such that aucg y  뫟 aucs y  with a confidence coefficient of 1  under the normal assumption  for every y 뫞 x. for all smaller values of the negative ratio  the global geometric ranking outperforms the other method  in term of auc .
모we can see in table 1 that for most bases  the global geometric ranking methods is rather efficient at the beginning of the roc curve  even when on the total range it performs badly  like for the pima database  see table 1 . the
reduced-errornormaldatasetpruningpruningunprunedbupa111111glass111111iono111111iris111111letter111111thyroid111111optd.111111pend.111111pima111111sat111111segment.111111sonar111111vehicle111111vowel111111wdbc111111wine111111table 1: absolute difference of the auc between local geometric ranking with standard metric and the best smoothing method.  all mean values and standard deviations are 뫄1. insignificant values are italic. there is no bad value. 
experiment partially confirms the theoretical viewpoint concerning the fact that geometric score gives interesting results when misclassified examples are near the decision boundary. this is particularly true for the bupa  liver-disorder  and ionosphere databases. table 1 shows that these datasets doesn't verify the hypothesis of proximity of errors on a majority of samples  and actually the global geometric score give bad results for these datasets.
모concerning the improvement of the geometric ranking when the accuracy of the tree is better  the experiment is not conclusive. if we compute table 1 and table 1 for a subset of the samples  the best quartile for tree accuracy  the global geometric ranking is not improved  results are not significant . but local geometric ranking gives always better results than on the total sample  except on the glass and ionosphere database  for which the hypothesis of proximity of errors is not much improved on the subset of the samples .
1 conclusion
we have presented in this article a geometric method to rank cases that are classified by a decision tree. it applies to every axis-parallel tree that classifies examples with numerical attributes. we were not concerned here with the problem of growing the tree  problem with unbalanced datasets or different misclassification costs which lead to pre-processing of the data or new pruning methods . the geometric method doesn't depend on the type of splitting or pruning criteria that is used to build the tree. it only depends on the shape of decision boundary induced by the tree. it consists in ranking the case according to their distance to the decision boundary  taking into account the class of interest and the class that is predicted by the decision tree. theoretical arguments suggest that this method is interesting when the misclassified examples lie near the decision boundary  and this was partially confirmed by the experimentation. the combination of geomet-
	mm metric	standard metric
	dataset	m-estimate	laplace	m-estimate	laplace
bupa1111glass1111iono1111iris뫟1뫟1뫟1뫟1thyroid뫟1뫟1뫟1뫟1optdigits1111pendigits뫟1뫟1뫟1뫟1pima1111sat뫟1뫟1뫟1뫟1segment.뫟1뫟1뫟1뫟1sonar뫟1뫟1뫟1뫟1vehicle뫟1뫟1뫟1뫟1vowel뫟1뫟1뫟1뫟1wdbc뫟1뫟1뫟1뫟1wine뫟1뫟1뫟1뫟1table 1: abscissa below which the global geometric ranking auc is always greater.  bad results are bold .
ric ranking and smoothing methods almost always improve the global ranking  measured with the auc . different kind of experiment should be performed in order to compare geometric ranling  and particularly local geometric ranking  to nbtree or other algorithm: since the structure of the trees are different  the choice of pruning method can be important.
모the main limit of the method is that it is limited to numerical attributes. it could be extended to ordered attributes  but without the definition of a utility function it cannot be used with attributes that have unordered modalities.
모further work is in progress in order to understand more precisely when the geometric ranking should perform well. following the idea from  smyth et al.  1   we think that density estimator could be used on the distance itself rather than on the attribute of the cases  in order to deal with 1dimension estimator  which are very efficient . another interesting point is the definition of a geometric score for real multi-class problem  with no particular class of interest . actually the algorithm that computes the distance to the decision boundary computes already the distance of an example to the different classes  so these distances could be used for that purpose.
