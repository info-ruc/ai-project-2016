 
a key open problem in reinforcement learning is to assure convergence when using a compact hypothesis class to approximate the value function. although the standard temporal-difference learning algorithm has been shown to converge when the hypothesis class is a linear combination of fixed basis functions  it may diverge with a general  nonlinear  hypothesis class. this paper describes the bridge algorithm  a new method for reinforcement learning  and shows that it converges to an approximate global optimum for any agnostically learnable hypothesis class. convergence is demonstrated on a simple example for which temporal-difference learning fails. weak conditions are identified under which the bridge algorithm converges for any hypothesis class. finally  connections are made between the complexity of reinforcement learning and the pac-learnability of the hypothesis class. 
1 	introduction 
reinforcement learning  rl  is a widely used method for learning to make decisions in complex  uncertain environments. typically  an rl agent perceives mid acts in an environment  reeei ving rewards that provide some indication of the quality of its actions. the agent's goal is to maximize the sum of rewards received. rl algorithms work by learning a value/unction that describes the long-term expected sum of rewards from each state; alternatively  they can learn a q-function describing the value of each action in each state. 
these functions can then be used to make decisions. 
¡¡temporal-difference  to  learning  sutton  1  is a commonly used family of reinforcement learning methods. to algorithms operate by adjusting the value function to be locally consistent. when used with function approximators  such as neural networks  that provide a compact parameterized representation of the value function  to methods can solve realworld problems with very large state spaces. because of this  one would like to know if such algorithms can be guaranteed to work-i.e.  to converge and to return optimal solutions. 
¡¡the theoretical study of rl algorithms usually divides the problem into two aspects: exploration policies that can guarantee complete coverage of the environment  and value deter-
mination to find the value function that corresponds to a given policy. this paper concentrates on the second aspect. prior work  jaakkola et al.   1  has established convergence of 
1 	machine learning 
to-learning with probability 1 when the value function is represented as a table where each state has its own entry. for large state spaces  however  compact parametric representations are required; for such representations  we are interested in whether an algorithm will converge to the function that is closest  by some metric  to the true value function  a form of agnostic learning . gordon  proved that to conveiges in this sense for representations called  averagers  on which the to update is a max-norm contraction  see section 1 . tsitsiklis and van roy   1  proved convergence and established error bounds for td  a  with linear combinations of fixed basis functions. 
¡¡with nonlinear representations  such as neural networks  to has been observed to give suboptimal solutions  bertsekas and tsitsiklis  1  or even to diverge. this is a serious problem since most real problems require nonlinearity. baird  introduced residual algorithms  for which convergence can be proved when combined with a gradient descent learning method  such as used with neural networks . unfortunately the error in the resulting approximation can be arbitrarily large and furthermore the method requires two independent visits to each sampled state. 
¡¡this paper describes the bridge algorithm  a new rl method for which we establish convergence and error bounds with any agnostically learnable representation. 
¡¡section 1 provides the necessary definitions and notation. section 1 explains the problem of nonconvergence and provides examples of this with to. section 1 outlines the bridge algorithm  sketches the proof of convergence  and shows how it solves the examples for which to fails. section 1 briefly covers additional results on convergence to local optima for any representation and on the use of pac-learning theory. section 1 mentions some alternative techniques one might consider. the paper is necessarily technically  tense given the space restrictions. the results  however  should be of broad interest to the ai and machine learning communities. 
1 	definitions 
1 mdp 
a markov decision process m =  is a set of states s  a set of actions a  transition probability distributions that define the next state distribution given a current 
state x and action a  reward distributions r  that define the distribution of real-valued reward received upon executing a in x  and a discount factor . since we are interested in the problem of value determination  we assume we 

are given a fixed policy  choice of action at each state . when executing only this fixed policy  the mdp actually becomes a markov chain  and we may therefore also write the transition probabilities as p -|x  and the reward distributions as r - x . we assume that we are able to define the stationary distribution ¦Ð of the resulting markov chain and also that the rewards lie in the range 
observed random rewards  i.e. rk has distribution r - xk . define the true value function v* at a state x to be the expected  discounted reward to go from state x: 

¡¡the problem of value determination is to determine the true value function or a good approximation to it. classical td solutions have made use of the backup operator t  which takes an approximation v and produces a better approximation 
¡¡an operator a is said to be a contraction with factor under some norm ||   || if 
if 	= 1  a is said to be a nonexpansion. if we define the 
v x and the -normto then t is a contraction 
with factor and fixed point v* under both max-norm and -norm {tsitsiklis and van roy  1 . therefore repeated application of t  i.e. the iterative process converges to . we will use tj to represent the operator t applied j times. 
¡¡if the transition probabilities p and reward distributions r are known  then it is possible to compute tv directly from its definition. however  if p and r are not known  then it is not possible to compute the expectation in the definition of t. in this case  by observing a sequence of states and rewards in the markov chain  'we can form an unbiased estimate of tv. specifically  if we observe state x and reward r followed by state x 1   then the observed backed-up value    is an unbiased estimate of tv x . formally we define to be the conditional probability density of observed backedup values from state x: 
where and are  as defined above  random variables with distributions respectively. thus if a random variable y has associated density then e y  = . similarly  we d e f i n e t o be the 
conditional probability density of j-step backed-up values observed from state x. 
1 	function approximation 
as state spaces becomes large or even infinite  it becomes infeasible to tabulate the value function for each state x  and we must resort to function approximation. our approximation scheme consists of a hypothesis class h of representable functions and a learning operator  which maps arbitrary value functions to functions in %. 
¡¡the standard t based approaches that use function approximation essentially compute or approximately compute the iterative process in practice  the 
mapping usually cannot be performed exactly because  even if we have access to the necessary expectation to compute tv x  exactly  it is infeasible to do so for all states x. thus we perform an approximate mapping using samples. we will take the state sample distribution to be the stationary distribution ¦°. in general when we cannot compute tv ar  exactly  we approximate  by generating samples  x  y  with sample distribution 

and passing them to a learning algorithm for h. the joint probability density  from which we generate the samples  simply combines  from which we sample the state x  with the conditional probability density    from which we generate an estimate y for tv x  . 
¡¡in this paper we focus on agnostic learning. in this case  the learning operator seeks the hypothesis h that best matches the target function v  even though typically the target function is not in the hypothesis class. if we measure distance using the -norm  then we can define the learning operator for agnostic learning to be: 

¡¡as already mentioned  in the typical case we do not have access to the exact function v to be learned  but rather we can draw samples  x  y  from a sample distribution p such that the expected value of the conditional distribution is v x . if  in addition  v samples the states according to  or whatever distribution was used to measure distance in the previous definition  then an equivalent definition for agnostic learning is based on minimizing risk; 

where we define the risk of a hypothesis h with respect to a distribution v to be: 
¡¡in practice  is approximately performed by generating enough samples  x  y  from the sample distribution p so as to be able to estimate risk well  and thus to be able to output the hypothesis in % that has minimal risk with respect to this distribution. in the algorithm we present  we assume the ability to compute  exactly for the given hypothesis class h. this is certainly not a trivial assumption. in a later section  we briefly discuss an extension to our algorithm for the case where is a pac-agnostic learning step rather than an exact agnostic learning step. 
¡¡finally  let us define our goal.  is defined to be the best approximation to v* possible using %  

we seek techniques that return a value function v that minimizes a relative or absolute error bound: 

1 nonconvergence of td 
in this section  we examine the non-convergence problem of td when used with non-linear function approximators. we present simple examples which we will reconsider with the bridge algorithm in the next section. 
¡¡as mentioned above  standard td with function approximation is based on the iterative process 
	papavassiuou and russ€ll 	1 

 is a non-expansion under the same norm that makes t a contraction  then the composite operator t is a contraction and this process will converge to some error bound relative to 
¡¡¡¡. for example  tsitsiklis and van roy  consider a 
¡¡¡¡linear hypothesis class  for which  is simply a projection. if one uses a nonlinear hypothesis class h for which iih is not a nonexpansion then this iterative process can either diverge or get stuck in a local minimum arbitrarily far from 
¡¡we now give simple examples demonstrating ways in which to can fail when it is used with a nonlinear hypothesis class. consider an mdp with two states where the probability of going from one state to the other is 1  and the rewards are also deterministic. the stationary distribution is 1 for each state  the discount factor is .1 and the hypothesis class h 

figure 1:  a  suboptimal fixed point and  b  oscillation 
	if we modify the rewards slightly to be 	 r1 r1  	= 
 1  -1  then the true value function v * s no longer in h. the best representation of v* = iih v* = . if we start from  1  as above  we will again reach a suboptimal fixed point around  1  -1 . however  starting from v1 =  1   or even v1 =  1   the result of repeated applications of t as shown in figure 1 b  displays a different type of failure - oscillation between points approaching  1 1  and  1 . as in the previous e x a m p l e   i s small  so the relative error bound is large. 
1 	the bridge algorithm 
we begin with a high level description of the algorithm  details are in the appendix . this is followed by the convergence results and another look at the examples from the previous section. 
¡¡the main algorithm bridge valuedet  determines the value function within some error bound by making repeated calls to bridgestep. we will now describe the first invocation of bridgestep. 
¡¡metaphorically it consists of throwing a bridge across the treacherous terrain that is the hypothesis class h  towards a point on the far side of the optimal solution. if the bridge lands somewhere close to where we aimed it  we will be able to walk along it in a productive direction  achieve a contraction . if the bridge lands far from our target  then we know that there isn't any h-expressible value function near our taiget on which the bridge could have landed  hence an error bound . this is made precise by lemma 1 in the next section. 
¡¡we are given an old approximation v from which we try tocreate a better approximation vnew. webasically have two tools to work with: t and iih. as can be seen in figure 1  and in the example in the previous section   if we combine these two operators in the standard way  vnew = h   t v   we can get stuck in a local minimum. we will instead use them more creatively to guarantee progress or establish an error bound. 

figure 1: stuck in a local minimum 
¡¡we begin by using not t but rather  where j is determined by the main algorithm bridgevaluedet before it calls bridgestep. we can then ask the question  given we know where v and  are  what does that tell us about the location of v*   it turns out that v* is restricted to lie in some hypersphere whose position can be defined in terms of the positions of v and . this is made precise by lemma 1 in the next section. the hypersphere is depicted in figure 1 and as required  v* lies inside it. 

figure 1: the bridge is aimed 
¡¡we now define a new operator b based on  and the identity operator i. 

b simply amplifies the bellman residual by a factor of 
as can be seen in figure 1  b v is the point on the far side 

of the hypersphere from v. this operates- is what we use to throw a bridge. we aim the bridge for bv  which is beyond anywhere where our goal might be  i .the true value function lies somewhere between v and bv. the motivation for using b is in a sense to jump over all local minima between v and v* 
¡¡ideally we would be able to represent b v  just as in the standard approach we would want to represent t v   but this function is most likely not in our class of representable functions. therefore we must apply the operator h to map it into h. the result  w = i l h b v e h  is shown in figure 1. the bridge is supported by v and w and is shown as a line between them. in summary we throw the bridge aiming for b v  but h determines the point w on which it actually lands. 

figure 1: the bridge is established 
¡¡in practice we perform the mapping iih by generating samples from an appropriate distribution and passing them to a learning algorithm for h. in particular to compute h b v   we generate samples  x  y  according to the distribution: 
i 
the key feature of this distribution is that if a random variable 
y has associated density i then e y  = bv x . 
¡¡the final step is to walk along the bridge. the bridge is a 
¡¡line between v and w and our new approximation vnew will be some point on this line  see figure 1 . this point is determined by projecting a point n   1 of the way from v to onto the line  where n is a function of the input parameters.  we could just project    but using n is a refinement that yields a better guaranteed effective contraction factor.  

figure 1: the new approximation 
¡¡thus the new approximation vnew  which is not necessarily inh  is a weighted average of the old approximation v and w %. calculating the weights  p and 1 - p  in this average requires the ability to measure distance and risk. in particular we need to measure the distance between v and w and the risk of v and w with respect to the distribution . these three lengths  and n  determine the relative position of with respect to v and w  see figure 1 . to practice we es-
timate the true risk with the empirical risk  haussler  1   which we calculate using samples drawn from the distribution 
 have just described a single invocation of bridgestep 
that represents the first iteration of the main algorithm. each iteration builds a new bridge based on the previous one  so a generic iteration would begin with a v that was the  of the previous iteration  see figure 1 . in particular  the input v of a generic iteration is not in h  but is rather a linear combination of the initial approximation and all previous w functions. thus the final result is a tall weighted tree whose leaves are in u. if we insist on a final result that is in h  then we can apply a final mapping at the very end. 
just as the standard td algorithm was summarized as 
  the bridge algorithm can be essentially 
summarized as 

figure 1: generic iteration of bridgestep 
1 	convergence of the bridge algorithm 
we will state the main convergence theorem for the bridge algorithm  but space limitations allow us to state only the two most important lemmas used in the proof. we begin with a very useful observation about the geometric relationship between v  tv and v*. 
lemma 1 let a be a contraction with contraction factor under some norm. let v* be the fixed point of a. for any 
point 
	in words  given the positions of v and av  let c 	= 
         . then we know that the position of v* has to be on or inside the hypersphere of radius centered at o  see 
figure 1 . this hypersphere is simply the set of points that are at least a factor of c closer to av than to v. note that the distance from v to the furthest point on the hypersphere is 
	we apply lemma 1 using  for a and 	for 	this de-
fines a hypersphere inside of which the true value function must lie. lemma 1 is used mainly to prove lemma 1  which characterizes the behavior of bridgestep and provides most of the meat of the convergence proof. 
lemma 1 given an approximation v and parameters andj   1  bridgestep  returns a new approximapapavassiliou and russell 1 


tion vnew that satisfies at least one of the following two conditions  where the error bound  is 
defined in the appendix 
		 error bound  
¡¡intuitively  if the bridge lands close to where we aimed it  we will achieve a contraction towards the goal. if the bridge lands far away  we will prove a relative error bound for the result. the key quantity that determines which of these two events happens  is the angle formed between the bridge and the line from v to b v . if w = is close to b v   
then  will be small  the bridge will lie close to the hypersphere  and we will be able to walk along the bridge and make progress. if instead w is far from b v   then  will be large and walking along the bridge will not take us closer to the goal  but we will be able to prove that we are already close enough. 
¡¡figure 1 shows the case where the angle  is small. as described previously  the small hypersphere represents the set of points that are at least a factor of closer to than they are to v. this follows from applying lemma 1 to the operator . now think of bridgestep as an operator that takes v and returns   and ask the question  what set of 
points are at least a factor of    which is an input parameter to bridgestep  closer to than to v  applying lemma 1 to this question defines another  much larger hypersphere which is depicted in figure 1 with center at o for the case = arcsin  - arcsin note that this larger hypersphere completely contains the smaller hypersphere which contains v*. thus v* also lies inside the larger hypersphere and so is at least a factor of closer to v* than v is. this holds for = arcsin - arcsin . i f is smaller than this  the achieved contraction is even better. 

¡¡figure 1 shows the case where the angle is large  is large when it is not possible to find a hypothesis in % close to b v . in fact we choose w =  to be the closest such hypothesis  so the rest of % must lie further away. in particular % must lie completely outside the big hypersphere depicted in figure 1 with center at b v  for otherwise w would not be the closest hypothesis to b v . furthermore we know that v* must lie on or inside the small hypersphere in figure 1. thus there is a separation between v* and h and this separation allows us to prove  for any possible position of v*  an upper bound on the relative error 
¡¡it should be noted that in general we do not know and we cannot measure 1 to determine which of the two conditions of lemma 1 vnew satisfies. we only know that it satisfies at least one of them. 
¡¡by lemma 1  if v already satisfies the relative error bound then so will v n e w   because if vnew achieves a contraction over v  its error decreases. thus each successive approximation is better than the one before  until we achieve the relative error bound from which point every subsequent approximation will also achieve that bound. 
¡¡we now give the main result  which is guaranteed convergence to a relative or absolute error bound. moreover  the maximum number of invocations of bridgestep  and thus the maximum number of hypotheses in the linear combination  can be specified. 
theorem 1 let v   1 and e1   1 be the desired relative and absolute error bounds respectively. let n be an upper bound on the desired number of iterations. then the algorithm 
bridgevaluedet z/  €o n  produces an approximation v  consisting of a linear combination of at most n+l hypotheses 
from h  that satisfies at least one of either the relative error bound v or the absolute error bound eo-' 

¡¡the proof of the theorem follows directly from lemma 1; rewards are bounded  so the true value function is bounded  so the absolute error of the initial approximation can be bounded. if all n iterations achieve a contraction  then the absolute error will be smaller than requested. if at least one of the iterations failed to achieve a contraction  then it achieved a relative error bound and all subsequent iterations  including the last one  will achieve the requested relative error bound. again  since we do not know which of the two conditions of lemma 1 

each iteration satisfies  we do not know whether the final answer v satisfies the relative or the absolute error bound. we know only that it satisfies at least one of them. 
corollary 1 let v  and n be as defined in theorem 1 letv = bridgevaluedet i/     n   a linear combination of hypotheses firm h. then v  the result of map-
ping v back into h satisfies at least one of either the relative 
error bound 1v + 1 or absolute error bound 
1 	the examples revisited 
we now reconsider the examples from section 1. the main algorithm bridgevaluedet takes parameters v     and n  from which it computes the number of lookahead steps j to use to achieve the requested error bounds. also for each iteration  it chooses a parameter an which determines the contraction factor achieved or relative error bound established for that iteration. these two parameters  j and   are passed to bridgestep at each iteration. in this section  we examine the effect of repeated applications of bridgestep  using j = 1 and = .1 for every iteration. 
¡¡for the first example  with initial =  1   the results of repeated applications of bridgestep are shown in figure 1 a . because for this example  = 1  i.e. 
the true value function is in h   the relative error bound is always infinite. therefore  by lemma 1  every step achieves at least a contraction a and so the algorithm converges to the true value function. 

figure 1: examples revisited with bridge 
¡¡for the second example  with v1 =  1   the results of repeated applications of bridgestep are shown in figure 1 b . looking at the first step more closely  =  1 1   b v 1 =  -1 1  and   =  -1 1 . the dotted line between and w1 is the bridge. v i   being a weighted average of and wo  lies on this bridge. similarly  v1 lies on the bridge between vx and 
w1  

figure 1:  a  lemma 1 applied to second example 
 b  linear combination 
¡¡figure 1 a  demonstrates lemma 1 on every fifth application of bridgestep. in particular  note that the effective contraction factor only exceeds a after the desired relative error bound has been achieved. in fact cm this example  the algorithm performs far better than the theory guarantees  figure 1 b  shows the weights of the averages and the structure of the resulting linear combination after 1 steps. 
1 	extensions 
it is possible to extend the algorithm in many ways. in particular relying on an exact  agnostic learning operator  is not practical. here we briefly discuss the use of two other learning operators and we hope in the future to consider others still. 
1 
most significantly we have extended our algorithm to the case where the learning step cannot be done exactly but is instead a pac learning step  see  papavassiliou and russell  1  . we actually use the same and for every iteration  so the learning step has the same complexity for every iteration. this is simple but most likely not optimal. one appealing aspect of considering pac agnostic learning is the potential availability of sample complexity results based on some measure of the complexity of h. unfortunately it is necessary to learn and estimate risk under the stationary distribution of the markov chain. simply running the chain to generate samples will only generate them correctly in the steadystate limit. therefore computing sample complexity results for the risk estimation and agnostic learning steps requires extending the current state of the theory to the case where samples are generated from a markov chain  rather than i.i.d. one would expect the sample complexity to depend on the mixing time of the markov chain and the variance of the sample distribution. the form of these theorems will also determine the extent to which samples can be reused between the different risk estimation steps within an iteration or even across iterations. 
1 	suboptimal learning 
previous algorithms for this problem have been shown to converge for learning operators un that are non-expansions 

the convergence results for bridge hold for learning operators that perform agnostic learning. unfortunately there is a general lack of useful agnostic learning algorithms  the risk minimization step is typically intractable   so it would be beneficial to extend the results to learning systems that are not optimal. 
¡¡it is possible to weaken the conditions on the learning operator and give convergence results for bridge that hold when satisfies the banana-fudge condition 
for some nondecreasing functions k1   1 and k1- the only modification necessary to bridge is to include k1 and k1 in the calculation of the relative error bound errbound 
note that for 	  this condition re-
duces to 

which is in fact the property of agnostic learning that is used to derive the results in this paper. 
¡¡intuitively  the nonexpansion condition for iih requires that two points that are close to each other  me mapped close 
	papavassiliou and russell 	is 

to each otter. the banana-fudge condition requires that two points that are close to each other  are mapped a similar distance away  but they can be mapped in opposite directions and so end up very for from each other. the banana-fudge condition is obviously the weaker one  requiring only similarity in level of success and not similarity in outcome. it disallows the case where one function is learned very well  but another function very close to the first is learned very poorly. we are currently searching for learning algorithms that satisfy the banana-fudge condition  but unfortunately it seems most common practical learning algorithms do not. 
1 	other approaches 
we briefly discuss other known alternatives to bridge as well as mention some of the new directions one mi ght consider. 
1 	alternatives 
if h is convex and  is the agnostic learning operator  then  is a nonexpansion and verges. for nonconvex h  an alternative approach to bridge is to pac-agnostically learn the convex hull of h using 
at each iteration  lee et a/.  1 . the resulting iterated procedure 	converges since is a nonexpansion. unfortunately  this algo-
rithm requires many more agnostic learning steps per iteration than seems practical. 
¡¡a noniterative method that returns the optimal answer is to reduce the value determination problem to a single instance of supervised learning by using the operator   otherwise known as .it does unlimited lookahead  has contraction factor = 1 and so it generates v* after just one iteration. looked at another way  the distribution  has mean . unfortunately there is 
empirical evidence that suggests the sample distribution is very hard to learn and requires very many samples  perhaps because it can have high variance . 
¡¡strictly speaking  it is not necessary to backup values beyond the -horizon which is . even this  however  may yield sample distributions with too much variance for practical use  although it is offset by the need to perform only a single learning step. 
¡¡finally it may be possible  using lemma 1  to establish convergence rates and error bounds for the iterated procedure  where m is less than the -horizon. however  m would probably have to be much larger than j  the number of lookahead steps used by bridge  and so again we would expect bad sample complexity. 
1 	new directions 
there are many ways in which the basic tools used in constructing this algorithm might be used in constructing more powerful methods. specifically the geometric relationship between v  t v   and v* established in lemma 1 is very useful in  1  providing geometric intuition to design new methods and  1  proving performance guarantees for these methods. 
¡¡one can think of many different ways to throw a bridge and many different kinds of bridges to throw. for example  we establish w  the other end of the bridge by learning the point 
. this choice is rather arbitrary  picked 
to simplify the error bound analysis. one might try instead learning a point further or a little closer to v. 
¡¡once we establish w  we throw a one-dimensional  linear bridge from v to w and learn a point close to tjv cm this line  learning is equivalent to projection in linear hypothesis classes . one might try establishing more than two points with which to support the bridge. for example  given v and after establishing w   l     we could try establishing w 1  by learning a point strategically located far from both v and w  1  on the other side of the hypersphere defined by lemma 1. then we could throw a two-dimensional  planar bridge across these three points and project tjv  or a point close by  onto this plane. we can continue in this way  considering methods that establish w l  ....  w n  and use an n-dimensional hyperplane to learn . in the logical limit this method looks like a local version of  lee et al  1  which learns the full convex hull. it is local in that it only closes under weighted averaging those points of % that are closest to some point of the hypersphere defined by lemma 1. our current method which only uses one-dimensional bridges is effectively a light version of these convex-hull methods  in that before learning it closes under linear combinations only two points from h  namely v and w. 
1 	conclusion 
we have developed a method that reduces the value determination problem to the agnostic learning problem. requesting that our algorithm halt in fewer iterations or with better error bounds pushes more of the complexity into the learning step and in the limit effectively forces it to consider infinite lookahead which is . similarly  if we were to extend our algorithm to use more and more supports for the bridge  we suspect it would approximate the performance of the convex hull learning algorithm. thus our method can be thought of as a more versatile and hopefully more efficient alternative to these aggressive methods. 
¡¡the key features that characterize our approach are  1  the complication of learning is abstracted into a learning operator 
   1  we use a new operator b rather than being restricted to the backup operator t   1  we form linear combinations of hypotheses from a class h rather than being limited to just h  and  1  we use lemma 1 to prove convergence and error bound results. these techniques can be applied or modified to develop endless variations on bridge as well as completely new algorithms. 
¡¡a big missing ingredient in justifying one method over another is sample complexity. in particular  we do not know how sample complexity depends on the lookahead j  or  in the case of   how it depends on   and so we cannot properly trade off these parameters to achieve the best performance. 
¡¡our results are stated for the problem of value determination  but they apply to any situation with an operator that is a contraction with respect to a norm defined by a samplable distribution. for the problem of value determination  the operator is t  the one-step backup operator  and it is a contraction under the norm defined by the stationary distribution of the markov chain. as stated previously  this distribution can only be sampled exacdy in the steady-state limit  so improvements in the theory are necessary. finally  a big hurdle to a practical  implementable algorithm is the lack of useful  well-behaved  agnostic or not  learning algorithms. by applying the techniques used in developing bridge  we hope to bridge the gap between the available supervised learning algorithms and those needed by theoretically justified reinforce-

ment learning methods. 
