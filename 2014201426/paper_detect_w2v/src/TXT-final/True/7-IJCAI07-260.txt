
the research areas of plan recognition and natural language parsing share many common features and even algorithms. however  the dialog between these two disciplines has not been effective. specifically  significant recent results in parsing mildly context sensitive grammars have not been leveraged in the state of the art plan recognition systems. this paper will outline the relations between natural language processing nlp  and plan recognition pr   argue that each of them can effectively inform the other  and then focus on key recent research results in nlp and argue for their applicability to pr.
1 introduction
without performing a careful literature search one could easily imagine that the fields of plan recognition pr  and natural language processing nlp  are two separate fields that have little in common. there are few papers in either discipline that directly cite work done in the other. while there are exceptions  carberry  1; blaylock and allen  1; pynadath and wellman  1; vilain  1   even these papers often are only citing nlp in passing and not making use of recent research results.
　interestingly  many researchers do see these two areas as very related  but are still not taking the recent lessons learned in one area and applying them to the other. in an effort to rectify this lack  this paper will outline the commonalities between pr and nlp  argue why the results from each of these research areas should be used to inform the other  and then outline some recent research results that could inform a unified view of these two tasks.
1 commonalities
in this section we will sketch the similarities at the surface and algorithmic levels between pr and nlp before more formally drawing their representations together in the section 1. we will start this process by laying out some terminology so that we can see the common parts of nlp and pr.
　both pr and nlp take as input a set of observations. in pr these are observations of action executions and in nlp these are individual words or utterances. in both cases  the observations are used to create a higher level structure. in nlp these higher level structures may be parse trees  collins  1  or logical forms  bos et al.  1 . in pr they are usually a hierarchical plan structure kautz and allen  1; kaminka et al.  1; geib and goldman  1  or at least a high level root goal horvitz et al.  1 . in either case  both nlp and pr construct a higher level knowledge structure that relates the meanings of each of the individual observations to a meaning for the collection of observations as a whole.
　for the purposes of this discussion it will aid us to abstract away from the specific details of the higher level structure that is built by this process. to simplify this discussion we will talk about these systems as if they were creating an hierarchical data structure that captures the meaning of the collection of observations. we will use the pr terminology and call this structure an explanation and following the nlp terminology call the process of producing a single explanation parsing.
　in order to parse a set of observations into an explanation both pr and nlp must specify the patterns of observations they are willing to accept or the rules that govern how the observations can be combined. in pr this specification is done in the form of a library of plans  while in nlp this is done through a grammar. in section 1 we will argue that there is no significant distinction between pr plan libraries and nlp grammars. therefore  in this paper we will call all such specifications of the rules for acceptable combination of observations grammars.
　with this terminology in place  we can now describe both nlp and pr as taking in as inputs a set of observations and a grammar specifying the acceptable sets of observations. both nlp and pr then parse these observations to produce explanations that organize the observations into a structured representation of the meaning of the collection.
　given this level similarity  it is not surprising that grammars in both nlp and pr can result in multiple explanations for a given set of observations. however  it is of interest that in both disciplines this ambiguity has been resolved using very similar probabilistic methods. in both areas  the state of the art methods are based on weighted model counting. these systems build the set of possible explanations and establish a probability distribution over the set in order to determine the most likely explanation.
the work in nlp often uses probability models derived from an annotated corpus of text clark and curran  1  while the probability models from pr have been based on markov models of the world dynamics  bui et al.  1  or probabilistic models of plan execution  geib and goldman  1 . while space prohibits a full exposition of these very different probability models  it is still telling that a weighted model counting method is the state of the art in both fields.
　beyond these surface and algorithmic similarities there are psycholinguistic reasons for believing that pr and nlp are very closely tied process that should inform one another. for example  consider indirect speech acts like asking someone  do you know what time it is   to correctly understand and respond to this question requires both nlp and pr.
　correctly responding requires not merely parsing the sentence to understand that it is a request about ones ability to provide a piece of information. it also requires recognizing that asking the question of someone else is the first step in a two part plan for finding out a piece of information by asking someone else. pr allows one to conclude that if someone is following this plan they most likely have the goal of knowing the piece of information  the current time in this case  and that providing the desired information will be more helpful than answering the literal question asked.
　given the similarities between the two areas  it seems reasonable that work in one area should inform the other. however important results in each area are not being leveraged in the other community. in the next section we will more formally specify the relation between these two areas to help researchers take advantage of the results in both areas.
1 plans as grammars
our argument that pr and nlp should inform one another would be significantly strengthened if we could show  as we have asserted above  that the plan libraries used by pr systems are equivalent to the grammars used by nlp systems. in the following section we will show the parallels between these two constructs and a mapping between them.
　almost all pr work has been done on traditional hierarchical plans.1 while much of the work in plan recognition has not provided formal specifications for their plan representations they can all generally be seen as special cases of hierarchical task networks  htn  as defined in  ghallab et al.  1 .
　according to ghallab the actions of an htn domain are defined as either operators or methods. an operator corresponds to an action that can be executed in the world. following ghallab we will define them as a triple  n add   list delete list  where n is the name of the operator  add list is a list of predicates that are made true or added to the world by the operator  and delete   list is the set of predicates that are made false or deleted from the world by the operator.
　a method on the other hand represents a higher level action and is represented as a 1-tuple  name t {st1 ...  stn} c  such that name is a unique identifier for the method  t names the higher level action this method decomposes  and {st1 ...  stn} identifies the set of sub-tasks that must be performed for the higher level task to be performed. finally  c represents a set of ordering constraints that have to hold between the subtasks for the method to be effective.
　we will draw a parallel between htns and context free grammars  cfgs . following aho and ullman aho and ullman  1  we define a cfg  g  as a 1-tuple g =  n Σ  p s  where
  n is a finite set of nonterminal symbols 
  Σ is a finite set of terminal symbols disjoint from n 
  p is a set of production rules that have the form n ★ ω where n （ n and ω （  Σ “ n    and
  s is a distinguished s （ n that is the start symbol.
　given these definitions  we would like to map the plans represented as an htn into an equivalent cfg. we first consider the case of a collection of htn plans that are totally ordered. that is  we assume that for every method definition the constraints on the subtasks st1 ...  stn define a total ordering over the subtasks. without loss of generality  we assume that the subtasks' subscripts represent this ordering.
　to encode the htn as a cfg  we first consider the operators. the processing for these is quite simple. we identify the names of each operator as a terminal symbols in our new grammar  and attach the add and delete lists to the nonterminal as features. next we consider mapping the method definitions into productions within the grammar.
　given a totally ordered method definition  we can add the task to be decomposed to the set of non-terminal symbols. then we define a new production rule with this task its left hand side. we then define the right hand side of the rule as the ordered set of subtasks. thus  the method definition  name t {st1 ...  stn} c  is rewritten as the cfg production rule: t ★ st1 ...  stn and t is added to the set of nonterminals.
　for example  consider the very simple htn method  m1 for acquiring shoes:
 m1 	acquire shoes   choose shoes  buy shoes } 
	{	}
where the constraints  1   1  indicates the task goto store  must precede the task choose shoes  and  1   1  indicates that choose shoes  must precede buy shoes . this is very easily captured with the cfg production: acquire shoes  ★
goto store  choose shoes  buy shoes 
this process of converting each method definition into a production rule and adding the task to be decomposed to the set of non-terminals is repeated for every method in the htn to produce the cfg for the plans. now we turn to the question of partial ordering.
　limited cases of partial orderness could be handled in cfgs by expanding the grammar with a production rules for each possible ordering. however  as the nlp community has realized this can result in an unacceptable increase in the size of the grammar  and the related runtime of the parsing algorithm barton  1 .
　so instead  to address this  the nlp community has produced a number of different grammar formalisms that allow the grammar to separately express decomposition and ordering. this includes the work of shieber on id/lp grammars  shieber  1   nederhof on poms-cfgs  nederhof et al.  1   and hoffman hoffman  1  and baldridge baldridge  1  on partial orderness in combinatory catagorial grammars. all of of these are attempts to include partial orderness within the grammar formalism  and parsing mechanism  without the exponential increase in the grammar size and runtime. since each of these formalisms use very different representations  rather than presenting examples  we refer the reader to the cited papers. it suffices to say that these grammar formalisms introduce notational additions to denote partial orderness within the production rules and to explicitly specify the ordering relations that are required in each production. these formalisms can be used to capture htn plan domains that require partial ordering.
　it should be clear from this exposition that the grammar formalisms found in the nlp literature are sufficient to cover the method definitions found in most if not all of the pr literature. however  to the best of our knowledge no one has used any of the relatively recent grammar formalisms and their associated parsing machinery for plan recognition. making use of these grammatical formalisms would also allow the use of their associated formal complexity results as well  something that has often been lacking in the work in pr.
　thus  we propose that nlp and pr could be unified by the use of the same underlying grammatical formalisms for representing the constraints on observations  and using a common parsing mechanism. in the case of probabilistic nlp and pr systems  we believe these systems may need to retaining separate methods for computing their probability distributions  however the parsing of observations into explanations could share a common framework. in the next section we will advocate a specific class of grammars for this task.
1 new grammar formalisms for pr
given that researchers in nlp have been working on the close relationship between grammars  parsers  and language expressiveness it shouldn't be a surprise that results from this work could inform the work in pr. there are some classes of grammars that are too computationally expensive to parse for real world application. for example  the well known complexity results for parsing context sensitive grammars  csgs  have all but ruled them out for nlp work. likewise we expect poor performance for applications of csgs to pr. unfortunately  pr researchers have used these results as a motivation to build their own algorithms for parsing  often without even considering the limitations of the existing parsing algorithms. examples include graph covering kautz and allen  1  and bayes nets bui et al.  1   that trade one np-hard problem for another. what has been largely ignored by the pr community is the nlp work in extending context free grammars and their efficient parsing algorithms.
　recent work in nlp has expanded the language hierarchy with grammars that have a complexity that falls between context free and context sensitive. examples  include il/lp grammars shieber  1   tree adjunction grammars tag  joshi and schabes  1   and combinatory catagorial grammars ccg  steedman  1; hockenmaier  1; clark and curran  1 . these  mildly context sensitive grammars  mcsgs  have a number of properties that make them attractive for nlp including greater expressiveness than cfgs but still having polynomial algorithms for parsing. these properties also make them attractive for adoption by the pr community.
　while these grammars are of scientific interest  we should justify their use  since it is not clear that pr requires grammars that are more expressive than cfgs. such a claim would rest on the empirical need for plans that are not context free. if nothing more than a cfg is needed for pr  then a well known parsing algorithm like cky that has a cubic complexity seems to be the obvious choices for application to pr. however  if there are pr problems that require recognizing plans that are not within the class of cfg plans  this would provide a convincing argument that pr requires a grammar that is not context free. in the following we will provide just such an example. while there are a number of different classes of mcsgs with different expressiveness results  and the exploration of all of them may prove useful for pr research  we will focus on the subclass of mcsgs that includes ccgs and tags called linear index grammars lig .
　steedman steedman  1  has argued convincingly that ccgs and other ligs are able to capture phenomena beyond cfgs that are essential to real world language use. given the parallels we have already demonstrated between nlp and pr  we argue that if this class is necessary for nlp it shouldn't be surprising to us if this class of grammars captured essential phenomena in pr as well. in this light  steedman shows that ccgs provide for crossing dependencies in nlp  a critical extension that context free grammars cannot capture. likewise  if we find that such crossing dependencies are necessary for recognizing plans we would have a strong argument that pr requires a grammar that is in the mcsg family.
　while a full discussion of the handling of crossing dependencies in ccgs is beyond the scope of this paper  it will be helpful to understand their basic structure in order to identify them in pr contexts. crossing dependencies occur when the words that make up a constituent  like a relative clause  are interleaved in the sentence with the elements of a different constituent. steedman steedman  1  has argued that a particularly strong example of the naturalness of these constructs are dutch verbs like proberen 'to try' which allow a number of scrambled word orders that that are outside of the expressiveness of cfgs.
　for example the translation of the phrase  ... because i try to teach jan to sing the song.  has four possible acceptable orderings  1 - 1  and a fifth that is more questionable.
1. ...omdat ik1 jan1 het lied1 probeer1 te leren1 zingen1.
	...because i jan	the song	try	to teach to sing.
1. ...omdat ik1 probeer1 jan1 het lied1 te leren1 zingen1.
1. ...omdat ik1 probeer1 jan1 te leren1 het lied1 te zingen1.
1. ...omdat ik1 jan1 probeer1 te leren1 het lied1 te zingen1.
1.  ...omdat ik1 jan probeer1 het lied te leren zingen
　the subscripts are included to show the correspondence of the noun phrases to the verbs. for example in the first ordering the noun phrases are all introduced first followed by their verbs in the same order as their nouns. this produces the maximally crossed ordering for this sentence.
　the realization of these kinds of crossed dependencies in a pr context is relatively straightforward. its important to keep in mind the mapping that we are using between traditional language grammars and planning grammars will mean that dependencies in pr are not the same as in nlp. in nlp dependencies are features like gender  number or tense that must agree between different words within the sentence. in the pr context  dependencies are equivalent to causal links in traditional nonlinear planning mcallester and rosenblitt  1 . that is  they are states of the world that are produced by one action and consumed by another. therefore  a plan with a crossing dependency would have the causal structure shown in figure 1 where in act1 is found to produce the preconditions for actions act1 and act1 which each produce a precondition for act1. such a structure requires that two different conditions be created and preserved across two different actions for their use. note that while the actions are only partially ordered  there is no linearizion of them that will remove the crossing dependency. that is  act1 and act1 can be reordered but this will not remove the crossing dependency.

figure 1: an abstract plan with a crossed dependency structure
　the argument for the necessity of mcsgs for planning rests on real world examples of plans with this structure. being able to describe what such a plan looks like is not compelling if they never occur in pr problem domains. fortunately  examples of plans with this structure are relatively common. consider recognizing the activities of a bank robber that has both his gun and ski-mask in a duffel bag and his goal is to rob a bank. he must open the bag  put on the mask and pick up the gun and enter the bank. figure 1 shows this plan. this plan has exactly the same crossed dependency structure shown in figure 1.
　note  that we could make this plan much more complex with out effecting the result. actions could be added before opening the bag  after entering the bank  and even between putting on the ski-mask and picking up the gun so long as the critical causal links are not violated. the presence of plans with this structure and our desire to recognize such plans gives us a strong reason to look at the grammars that fall this class as a grammatical formalism for pr.

figure 1: an example plan with crossing dependency structure
1	why mcsgs 
joshi joshi  1  first formally defined the class of mcsgs as those grammars that share four properties that are relevant for nlp:
  the class of languages included covers all context free languages.
  the languages in the class are polynomially parsable.   the languages in the class only capture certain types of dependencies including nested  non-crossing  and crossed dependencies.
  the languages in the class have the constant growth property which requires that if all of the sentences in the language are sorted according to their length then any two consecutive sentences do not differ in their length by more than a constant factor determined by the grammar.
this set of properties are also relevant for defining the class of grammars that would work well for pr. we will argue for each of them in order.
　first  we have just demonstrated the need for grammars that are more than context free for pr. second  clearly polynomial parsing is desirable for pr. in order to use these algorithms in real world applications they will need to be extended to consider multiple possible interleaved goals and to handle partially observable domains geib and goldman  1 . if a single goal can't be parsed in polynomial time what hope do we have for efficient algorithms for the needed extensions  further  pr is needed in a great many applications that will not tolerate algorithms with greater complexity. for example  assistive systems are not useful if their advice comes to late.
　third  plans do have structure that is captured in dependency structures. therefore  it seems natural to try to restrict the grammar for plans to the kinds of dependency structures that are actually used. whether or not the dependency restrictions that are consistent with nlp are the same set for pr is largely an empirical question. we have already seen evidence of crossing dependencies that required us to abandon cfgs in favor of mcsg's. while nested and crossing dependencies in the abstract can cover all the kinds of dependencies needed in planning  different mcsgs place different restrictions on the allowable depth of crossings and the number of nesting. this will have a significant impact on the expressiveness of a particular mcsg and its applicability to pr.
　fourth and finally  the requirement of the constant growth rate may be the hardest to understand. intuitively in the pr domain this means that if there is a plan of length n then there is another plan of length at most n+k where k is a constant for the specific domain. for example this rules out that the length of the next plan is a function of the length of the previous plan or some other external feature. note this says nothing about the goals that are achieved by the plans. the plan of length n and length n+k may achieve very different goals but they are both acceptable plans within the grammar. this speaks to the intuition that given a plan one should be able to add a small fixed number of actions to the plan and get another plan. again this seems to be the kind of property one expects to see in a pr domain and therefore in a plan grammar.
　now  while we believe we have made a strong argument for the use of mcsgs for pr  this is not the final word on the question. while we have presented an argument that we need at least the expressiveness of ligs  it may be the case that still more powerful grammar formalisms are needed. the most promising method for proving such a result would require finding plans with dependency structures that are not in mcsg that our pr systems need to recognize. thus  determining if mcsgs are sufficient for pr is an open research question for the community.
　while there are well known languages that are not in mcsg it is difficult to see their relevance to planning domains. for example the language {a1n}  that is the language where in the length of any sentence of the language is a power of two  is not mcsg as it fails the constant growth requirement. it is possible to imagine contrived examples where this would be relevant for pr  perhaps as part of some kind of athletic training regime  we want to recognize cases where in someone has run around the track a number of times that is a power of two.  however  this certainly seems anomalous and most likely should be dealt with by reasoning that falls outside of the grammar  like a counter and a simple test.
1 conclusions
there are close ties between the process of natural language processing and plan recognition. this relation should allow these two processes to inform each other and allow the transfer of research results from one area to the other. however  much recent work in both fields has gone unnoticed by researchers in the other field. this paper begins the process of sharing these results  describing the isomorphism between the grammatical formalisms from nlp and plan representations for pr  arguing that like nlp  pr will require a grammatical formalism in the mildly context sensitive family  and finally that nlp and pr can form a common underlying task that can usefully be explored together.
acknowledgments
the work described in this paper was conducted within the
eu cognitive systems project paco-plus  fp1-ist1  funded by the european commission.
