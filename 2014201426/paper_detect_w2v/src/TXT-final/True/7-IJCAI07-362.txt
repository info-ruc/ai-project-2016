
this work presents an iterative anytime heuristic search algorithm called anytime window a*  awa*  where node expansion is localized within a sliding window comprising of levels of the search tree/graph. the search starts in depth-first mode and gradually proceeds towards a* by incrementing the window size. an analysis on a uniform tree model provides some very useful properties of this algorithm. a modification of awa* is presented to guarantee bounded optimal solutions at each iteration. experimental results on the 1 knapsack problem and tsp demonstrate the efficacy of the proposed techniques over some existing anytime search methods.
1 introduction
development of optimization methods that can work within time limitations is a major need for modern practitioners facing large-sized problems that are np-hard in nature. this requirement has spawned research in the direction of designing anytime algorithms  dean and boddy  1   i.e.  the algorithms which can work under any time limitations. anytime algorithms are expected to regularly produce solutions of improved quality and when suddenly terminated  return the best solution produced so far as the result.
모among heuristic search techniques  the depth-first branch and bound  dfbb  approach and its variants  sarkar et al.  1; zhang  1  are by default anytime algorithms  as they produce a stream of gradually improving solutions. however  in general  the depth-first techniques suffer from excessive node expansions before they actually reach the optimal solution.the best-first anytime search algorithms can be broadly classified into two categories  namely  weighted heuristic search  pohl  1; hansen et al.  1; zhou and hansen  1; likhachev et al.  1  and beam search  zhou and hansen  1; furcy  1 .
모in weighted heuristic search  the heuristic estimate is multiplied by a weight  w  w 뫟 1   to yield f n  = g n  + w   h n . in the anytime variants of weighted a*  a series of solutions are produced either by iterating with the same chosen weight  hansen et al.  1; zhou and hansen  1  or by gradually decrementing the weight using a chosen 붟 after each iteration  likhachev et al.  1 . another algorithm was proposed to modify the weighted a* technique with selective commitments  kitamura et al.  1 . one of the major disadvantages of using the weighted a* based techniques is that weighing the heuristic introduces non-admissibility. as the basic characteristics of a* change substantially with non-admissible heuristics  pearl  1   it becomes very difficult to establish a relation between node expansions and the weighing factors  w 붟 . therefore  to use a weighted a* technique  lengthy simulations are required to find appropriate parameter settings. also  there are applications where no specific heuristic for the remaining path is available  the search may be guided only by the g n  information  pearl  1  or a lower bound f n -value may be used without distinct g n  and h n  components kumar et al.  1 . in such cases  the weighted a* techniques are not directly applicable.
모on the other hand  in beam search  a chosen number of most promising nodes at each level of the search tree/graph are selected for further expansion  and the remaining nodes are pruned. several anytime variants of the basic beam search technique  zhang  1; zhou and hansen  1; furcy  1  have been applied for different problems with reasonably good results. however  the lack of models  which can providethe user an apriori estimate about the quality-time trade-off  remains a problem for these algorithms as well.
모in this work  we present an alternative anytime heuristic search algorithm anytime window a*  awa*   which localizes the global competition performed by a* within a fixedsize window comprising of levels of the search tree/graph. the nodes which lie outside the active window are not considered for expansion at that point. the window is slid downwards to provide a depth-first component to the best-first search that occurs within a window. the window size is set to 1 to start with  depth-first mode  and is then increased iteratively proceeding towards pure best-first search algorithms like a*. the solution quality is expected to improve for iterations with a larger window size. while in awa*  we use the basic anytime principles of depth guided pruning and iterative relaxation  the algorithm retains the admissibility of the heuristic estimate while it continues to search in best-first mode within the given window. it may be noted that awa* does not need explicit decomposition of the evaluation function  f n   into g and h components. we analyze the characteristics of the presented algorithm using a uniform search tree model. this reveals some very interesting properties of the algorithm in terms of heuristic accuracy versus solution quality and related search complexity.
모we also present a bounded optimal version of the awa* algorithm  called bounded quality anytime window a*  bqawa*   which is guaranteed to produce solutions within a pre-specified sub-optimality bound  at each iteration. in bqawa*  the search backtracks and dynamically adjusts the window size within an iteration  such that the bound restriction is adhered to.
모we demonstrate the efficacy of the presented strategies on two optimization problems  namely  the traveling salesman problem  tsp  and the 1 knapsack problem. comparative results with the existing anytime techniques  like dfbb  ara*  beam-stack search  show considerable improvement in performance.
1 anytime window a* algorithm
the algorithm a* considers each node to be equivalent in terms of information content and performs a global competition among all the partially explored paths to select a new node. in practice  the heuristic errors are usually distance dependent  pearl  1 . therefore  the nodes lying in the same locality are expected to have comparable errors  where as the error may vary substantially for nodes which are distant to each other. thus  if the global competition performed by a* is localized within some boundaries  tighter pruning can be obtained. also  as better nodes at a level are generally very competitive even within small localities  we can expect good quality solutions with localized expansions. this observation forms the basis of awa*  where we localize the node expansions using a sliding window. the awa* algorithm works in two phases  in the inner loop it uses the window a*  algo. 1  routine with a fixed window size and computes the solution under the current restriction. in the outer loop  algo. 1  the window restrictions are gradually relaxed to obtain iteratively improving solutions.
모the window a* routine works with a pre-specified window size. this window size is used to restrict the active queue of the a* algorithm. for a given window size 뷎  the expansions are localized in the following way. when the first node of any level  say l  is expanded  all nodes in the open list which are from level  l   뷎  or less will be suspended for this iteration. the window slides in a depth-first manner when deeper nodes are explored. window a* terminates when the first goal node is expanded  a* like termination  or if the open list does not contain any node having estimated cost less than the cost of the best solution obtained in previous iterations.
모the awa* algorithm calls the window a* routine multiple times with gradual relaxation of the window bounds. at the start  window size is set to 1  depth-first mode . once the window a* routine terminates  awa* checks whether there are any nodes suspended in the previous iteration. if the suspended list is empty  the algorithm is terminated returning the optimal solution. otherwise  the open list is emptied and the suspended list is moved to the open list. the window algorithm 1 procedure window a*

1: curlevel 뫹 1;
1: if openlist is empty then
1:	return bestsol;
1: end if
1: select node n which has the least f n -value node from openlist;
1: insert n into closedlist; 1: if f n  뫟 bestsol then
1:	return bestsol;
1: else if level n  뫞 curlevel   windowsize then 1:	remove n from closedlist; insert n into suspendlist;
1:	goto line 1
1: else if level n    curlevel then
1:	curlevel 뫹 level n 
1: end if 1: if isgoal n  then
1:	bestsol 뫹 f n ; goal 뫹 n;
1:	return bestsol;
1: end if
1: for each successor nodedo calculate;
1:	if n is not in openlist or closedlist or suspendlist then
;
1:	insert
1:	else ifthen
1:	if previously calculated path estimation then
1:	; update;
1:	end if
1:	else ifthen
1:	if previously calculated path estimation then
1:	; update;
1:	insert;
1:	end if
1:	end if
1: end for
1: goto line 1

size is increased in a pre-decided manner  we have used gradual increment by 1   and window a* is called again. if the algorithm is terminated through external interrupts  the best solution obtained thus far is returned.

algorithm 1 anytime window a*  awa* 

1:
1:	뫹 ;	뫹 ;	뫹 1; calculate f s ; level s  뫹 1; insert s to openlist; bestsol 뫹カ
1: bestsol 뫹 window a*  ; {call window a*}
1: if suspendlist is empty then 1:	terminate with bestsol as the optimal solution;
1: else
1:	add openlist to closedlist; move suspendlist to openlist;
windowsize 뫹 windowsize +1;
1:	goto line 1
1: end if

모for an anytime algorithm  there are two desirable properties  namely  interruptibility and progress. the first property ensures the anytime nature  i.e.  the algorithm should provide a solution at pre-mature terminations. the second one states that the solution quality should improve with time  and if enough time is allocated the algorithm should eventually return the optimal solution. for finite search spaces  awa* adheres to both the properties mentioned. it starts in a depthfirst mode  with no back-tracking trying to obtain fast  possibly sub-optimal  solutions  and then iteratively increases the window size to produce better quality solutions. eventually  when the window size becomes such  that all the 'optimalpath' nodes lie within the active window  the optimal solution is returned.
while iterative algorithms provide opportunities to attain different trade-offs through anytime terminations  the search efforts can increase substantially if re-expansions are not controlled. for awa* with a consistent heuristic  a node is expanded at most once within the window a* routine  and it can only be re-expanded in a later iteration if its g-value is lowered. thus awa* requires minimal re-expansion.
모it may be noted  that in the generalized case awa* does not guarantee a solution at each iteration  window a* loop . if this property is a requirement; an easy way to attain this is through back-tracking  i.e.  by moving the suspended list to open list at line 1 of algo. 1   when no solution is found. however  we propose a better method for this in bqawa*  sec. 1   which not only provides a solution but guarantees a bounded quality solution  at each iteration.
1 analyzing awa* using a search tree model
in this section  we analyze the quality-time response of window a* algorithm in terms of a uniform cost tree model  as suggested in  pearl  1  . we model the search space as a uniform m-ary tree t  with a unique start state s. the leaf nodes are grouped into two parts. there is a unique optimal goal node g  situated at a distance n from s  and all other nodes at level n are sub-optimal terminal nodes. window a* terminates when either the optimal goal node or a suboptimal terminal node is reached. the optimal solution path is given as  s ng 1..ng i..ng n 1 g  where ng i denotes the solution path node at level i. the trees t1...ti...tn are 'offcourse' sub-trees of t  one level removed from the solution path. an 'off-course' node is labeled as ni j  where j denotes the level of that node and i denotes the root level of the 'offcourse' sub-tree  ti  to which the node belongs. using this

figure 1: uniform binary tree  model 
model  fig. 1   we try to quantify the expected node expansions and the probability of reaching the optimal goal node  for a specific window restriction. first we present few terminologies 
definition 1. qi j 뷎 : qi j 뷎 denotes the expansion probability of an 'off-course' node ni j when it is in the open list and the window size is 뷎 
	q	= p f n	  뫞 min 붞    붞 = min l + 뷎 n 
such that n 뫍 open list
 1 
definition 1. rg j 뷎 : rg j 뷎 denotes the expansion probability of a 'on-path'node ng j when it is already in the open list and the window size is 뷎 
rg j 뷎 = p f ng j  뫞 min 붞   붞 = min l + 뷎 n 
 1 
모next  we use the search tree model to obtain the expressions for the expected node expansions and convergence probability for a given 뷎. the expected node expansions and convergence probabilities  for a given window size  are presented in the following lemmas 
lemma 1. expected number of node expansions en 뷎 
 window size = 뷎  for a m-ary uniform cost search tree t
 with depth n  is given as 
  and 
 1 
lemma 1. expected probability of reaching the optimal goal node ps 뷎   window size = 뷎  for a m-ary uniform cost search tree t  with depth n  is given as 
		 1 
we try to estimate the expansion probability values
 qi j 뷎 rg j 뷎  from the heuristic information available. we assume that the relative estimation errors  eqn. 1   are independent random variables y  n  within  1 e  bound  1 뫞 e 뫞 1  with an arbitrary distribution function.
	y  n  =  h  n    h n  /h  n 	 1 
now  for a node ni j at depth j and belonging to 'off-course' sub-tree ti  figure. 1   we have
	 1  similarly  for an 'on-path' node ng j  we have
	 1  thus  with the chosen error model  eqn. 1  

모모모모모모모모모모모모모모모모모모모모모모모 1  where y  n   y  g  are identically distributed random variables within  1 e  bounds. combining eqn. 1 with the earlier definitions  we obtain
qi j 뷎 = 1   p y  n  뫞  j + 붸   min 붞  /붸  where  붸 = n + j + 1   i  and  붞 = min l + 뷎 n 
 1 
and 
	rg j 뷎 = 1   p y  n  뫞  n   min 붞  / n   j  	 1 
from eqns. 1 and 1  we observe that apart from the f n  value  the node expansion probability also depends on the min l + 뷎  value. now  the min l  value at a given level is bounded by the minimum and maximum possible f-values for that level. also  for a consistent heuristic the min l  function should be non-decreasing with l. thus  we obtain the following properties for min l .
n   e n   l  뫞 min l  뫞 1l + n
 1 
min l  뫟 min l   1 
with these observations  we represent the min l  function as follows 
min l  = max min l   1  
n 1   e 1   s l   + l e +  1   e s l      1  where  s l  is a function of l  1 뫞 s l  뫞 1.
the min l  function denotes the minimum value of a set of numbers. from statistics  we know that with the increase of the set cardinality the probability of the minimum converging to the lower limit increases exponentially. the implication suggests that with increase in l  the probability of min l  converging to the lower limit increases exponentially. thus  s l  can be approximated using a decreasing function with l  and most probably the decrease will be exponential.
모using the above presented results  we can estimate the expected node expansion and the optimal convergence probabilities for a chosen error distribution and s l  function. we assume a simple linear distribution for y  n  and approximate s l  as an exponential function of l  i.e.  we assume 
1 if  x   1 
	p y  n  뫞 x  =if  1 뫞 x 뫞 e 	 1 
1 otherwise
and 
	 1  substituting the variables in eqns. 1 and 1  we obtain the

figure 1: expected node expansions versus window size  for a uniform binary tree of height 1 for awa*
expected values for node expansions as well as convergence probabilities for different window constraints. in figures 1 and 1  we present the curves for expected node expansions and convergenceprobabilities with different window size  for a uniform binary tree of height 1. the expected values are computed using two error boundaries  e = 1 and e = 1   and two chosen constants  c = 1 and c = 1 . from the curves  we observe that the node expansions and convergence probabilities obtained are almost independent of the choice of c. this can be explained by the exponential nature of chosen s l  function  which rapidly converges to the lower limit. investigating the trends with different heuristic errors  we observe that the convergence probability versus window

figure 1: convergence probability versus window size  for a uniform binary tree of height 1 for awa*

figure 1: average node expansions versus window size  1 knapsack problem
size curves  fig. 1  do not show much variability with different errors. while the convergence probability for a given window size increases with heuristic accuracy  the improvement is marginal. however  the expected node expansions for a given window increases substantially with more error  fig. 1 . this observation can be explained by the fact that with more error  both the min l  and the f n  values are expected to decrease  causing relatively small changes in the qi j 뷎 and rg j 뷎 probabilities. the difference is even less pronounced for rg j 뷎 as the f n  values of the 'optimal-path' nodes are expected to remain close to the min l  values. on the other hand  the expected node expansions increase exponentially with the increase in qi j 뷎 values  eqn. 1 . thus  with increased error more 'off-course' nodes are expanded  causing appreciable variance in the node expansions.
모we conclude this section by presenting the experimental results for average node expansions and convergence probabilities with changing window size  for the 1 knapsack problem using two different heuristics h1  max. density  and h1  fitting based   such that h1 dominates h1  fig. 1 and 1 .

figure 1: average optimal convergence versus window size 
1 knapsack problem
the figures depict that the average node expansions and the convergenceprobabilities with different heuristics show identical trends to that obtained from the analytical model.
1 bqawa* - anytime window a* with provable quality bounds
in awa*  we have an anytime algorithm which attempts to produce a stream of gradually improving solutions. however  no bound on the obtained solution is established. in this section  we present a modified version of the awa*  termed as bounded quality anytime window a*  bqawa*   where each solution produced will be bounded within a prespecified factor    of the optimal solution  the bound is iteratively tightened to produce an iteratively improving stream of solutions.

algorithm 1 procedure bounded quality window a*

1: curlevel 뫹 1; minsus 뫹カ
1: if openlist is empty then
1:	goto line1
1: end if
1: select node n which has the least f n -value node from openlist;
1: insert n into closedlist;
1:  then
1:	goto line 1 {update window size and back-track}
1: else if level n  뫞 curlevel   windowsize then 1:	move n from closedlist to suspendlist; update minsus;
1:	goto line 1
1: else if level n    curlevel then
1:	curlevel 뫹 level n 
1: end if
1: goal evaluation {same as window a*}
1: expansion {same as window a*.} 1: goto line 1
1: if f n    bestsol then 1: move n from closedlist to openlist;
1: end if
1: merge suspendlist with openlist; increment windowsize;
1: if openlist is empty then
1:	return bestsol; 1: end if
1: goto line 1

모to achieve the pre-specified bounds at each iteration  we modify the window a* routine to obtain bounded quality window a* or bqwa*  algo. 1 . in bqwa* routine  at any intermediate point  the minimum f-value among the suspended nodes  min fsus   is noted. if the f-value of the top node in the open list is greater than  for minimization problems   the open list and the suspended list are merged  and the search back-tracks after incrementing the window size. the first solution produced by the algorithm is published with  bound guarantee. the bqawa* algorithm

algorithm 1 bounded quality anytime window a*

1: input :: a search tree/graph  a start node.
1: closedlist 뫹 뷋; suspendlist 뫹 뷋; windowsize 뫹 1; calculate f s ; level s  뫹 1; insert s to openlist; bestsol 뫹カ
1: bestsol 뫹 bounded quality window a*  ;
1: if suspendlist is empty then 1:	terminate with bestsol as the optimal solution;
1: else
1:	add openlist to closedlist; move suspendlist to openlist;
1:	;
1:	goto line 1
1: end if

 algo. 1  works with the same principle of ara* technique  i.e.  the algorithm starts with a high initial bound    and iteratively reduces the bound to get improved solutions. it uses the bqwa* routine  algo. 1  to obtain intermediatesolutions within the chosen sub-optimality bound. after each iteration  the suspended list is moved to the open list  weight bound is reduced and bqwa* is re-run to get improved solutions. the algorithm terminates if the suspended list is empty after an iteration.
1 experimental results
in this section  we present empirical results comparing the performance of awa* with some existing anytime techniques  dfbb  ara* and beam-stack search . we performed experiments with two optimization problems  namely 1 knapsack and euclidean traveling salesman  tsp  problem. since time spent is usually proportionalwith the number of nodes expanded  we present the results in terms of node expansions.

figure 1: average cost versus node expansions - 1 knapsack problem
모for 1 knapsack  we estimated the heuristic by fitting the remaining objects in decreasing order of their cost-densities. we performed experiments on a set of 1 random instances of the 1-object 1 knapsack problem. weight and costs of individual objects were generated randomly  while the constraint was chosen within 1   1 of the sum of weights. in figure 1  we present the cost versus nodes expansion results obtained for dfbb  ara*  beam-stack search and awa*. figure 1 includes the percentage optimal convergence versus node expansion curves. for ara*  the weight limits were initialized to 1  and decreased by 1 per iteration.
모for euclidean tsp  we used the minimum spanning tree  mst  heuristic. we performed the experiments on a randomly generated set of 1-city problem instances. the intermediate tour length and percentage optimality results comparing awa* with dfbb  ara* and beam-stack search are included in figures 1 and 1  respectively.
모from the results obtained for both 1 knapsack and tsp  we observe that the performance of awa*  is superior to all  dfbb  ara* and beam-stack search  both in terms of intermediate solution qualities as well as percentage optimal convergences.
모in table 1  we present the average node expansions for ara* and bqawa* applied to 1 knapsack and tsp  with different sub-optimality bounds. the results shows for all the

figure 1: percentage optimal convergence versus node expansions - 1 knapsack problem

figure 1: average tour length versus node expansions - euclidean tsp

figure 1: percentage optimal convergence versus node expansions - euclidean tsp
bounds  average node expansions required by bqawa* is less than that required by ara*  reiterating the superiority of the window based restrictions.
모from the results presented  we can concludethat the awa* algorithm provides considerably better quality-time trade-off than dfbb  ara* or beam-stack search. intermediate solution qualities are better and faster optimal convergence is achieved.
1 conclusions
in this paper  we proposed an anytime algorithm awa*  which localizes the global competition performed by a* using a sliding window. experiments performed with 1 knapsack and tsp demonstrate the efficacy of awa* over some existing anytime techniques. it may be noted  that awa*  in its current form  is not designed to work under memory restrictions. it would be interesting to explore how the standard table 1: average number of node expansions with different sub-optimality bounds
boundsknapsacktspara*bqawa*ara*bqawa*11111111111111111111111111.1111111memory bounded techniques can be adopted in awa*.
