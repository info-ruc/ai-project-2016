
in this paper we introduce a novel algorithm for the induction of the markov network structure of a domain from the outcome of conditional independence tests on data. such algorithms work by successively restricting the set of possible structures until there is only a single structure consistent with the conditional independence tests executed. existing independence-basedalgorithmshavewellknown shortcomings  such as rigidly ordering the sequence of tests they perform  resulting in potential inefficiencies in the number of tests required  and committing fully to the test outcomes  resulting in lack of robustness in case of unreliable tests. we address both problems through a bayesian particle filtering approach  which uses a population of markov network structures to maintain the posterior probability distribution over them  given the outcomes of the tests performed. instead of a fixed ordering  our approach greedily selects  at each step  the optimally informative from a pool of candidate tests according to information gain. in addition  it maintains multiple candidate structures weighed by posterior probability  which makes it more robust to errors in the test outcomes. the result is an approximate algorithm  due to the use of particle filtering  that is useful in domains where independence tests are uncertain  such as applications where little data is available  or expensive  such as cases of very large data sets and/or distributed data .
1	introduction
in this paper we focus on the task of learning the structure of markov networks  mns   a subclass of graphical models  from data in discrete domains.  other graphical models include bayesian networks  represented by directed graphs.  mns consist of two parts: an undirected graph  the model structure   and a set of parameters. an example markov network is shown in fig. 1. learning such models from data consists of two interdependent problems: learning the structure of the network  and  given the learned structure  learning the parameters. in this work we focus on structure learning of

figure 1: example markov network. the nodes represent variables in the domain v = {1 1 1 1}.
the mn from data  which is frequently the most challenging of the two tasks.
﹛the structure of a mn encodes graphically a set of conditional independencies among the variables in the domain. these independencies are a valuable source of information in a number of fields that rely more on qualitative than quantitative models  e.g.  social sciences . markov networks have also been used in the physics and computer vision communities  geman and geman  1; besag et al.  1   where they have been historically called markov random fields. recently there has been interest in their use for spatial data mining  which has applications in geography  agriculture  climatology  ecology and others  shekhar et al.  1 .
1	motivation and related work
there exist two broad classes of algorithms for learning the structure of graphical models: score-based  heckerman  1  and independence-based or constraint-based  spirtes et al.  1 . score-based approaches conduct a search in the space of legal structures  of size super-exponential in the number of variables in the domain  in an attempt to discover a model structure of maximum score. independence-based algorithms rely on the fact that a graphical model implies that a set of independencies exist in the distribution of the domain  and thereforein the data set provided as input to the algorithm  under assumptions  see below ; they work by conducting a set of statistical conditional independence tests on data  successively restricting the number of possible structures consistent with the results of those tests to a singleton  if possible   and inferring that structure as the only possible one.
﹛in this work we present an algorithm that belongs to the latter category  that presents advantages in domains where independence tests are  a  uncertain or  b  expensive. the first case may occur in applications where data sets are small relative to the number of variables in the domain. case  b  occurs in applications involving very large data sets  number of data points  and/or domains where the data are heterogeneously distributed i.e.  where columns of the data set  attributes  may be located in geographically distinct locations  e.g.  sensor networks  weather modeling and prediction  traffic monitoring etc . in such settings conducting a conditional independence test may be expensive  involving transfers of large amounts of data over a possibly slow network  so it is important to minimize the number of tests done.
﹛interesting work in the area of structure learning of undirected graphical models includes learning decomposable  also called chordal  mns  srebro and karger  1  or more general  non-decomposable  mns  hofmann and tresp  1   which is a score-based approach. an independencebased approach to mn structure learning is the gsimn algorithm  bromberg et al.  1   which uses pearl's inference axioms  pearl  1  to infer the result of certain independence tests without actually performing them. however  gsimn has two disadvantages:  i  potential inefficiencies with regard to the number of tests required to learn the structure due to the relatively rigid  predefined  order in which tests are performed  and  ii  potential instability due to cascading effects of errors in the test results. instead  the present paper takes a bayesian approach that maintains the posterior probability distribution over the space of structures given the tests performed so far. we avoid the inefficiencies of previous approaches by greedily selecting  at each step  the optimally informative tests according to information gain  decrease in entropy of the posterior distribution . as such the approach can be seen as an instance of active learning  tong and koller  1 . in addition  our approach is more robust to errors in the test outcomes by making use of the probability of independenceinstead of making definite decisions  corresponding to probability 1 or 1 . in this way an error in an independence test does not permanently cause the correct structure to be excluded but only lowers its posterior probability.
﹛the rest of the paper is organized as follows: in the next section we present our notation  followed by a description of our approach in detail. following that  we present experimental results and conclude with a summary of our approach. 1 notation and preliminaries
a markov network of a domain v is an undirected model that can be used to represent the set of conditional independencies in the domain. the application domain is a set of random variables of size n = |v|. in this work we use capital letters a b ... to denote domain random variables and bold letters for sets of variables  e.g.  s . the space of all structures  given v  is denoted by x and the space of all conditional independence  ci  tests by y. conditional independence of a and b given s is denoted by  a﹠﹠b | s . the set of conditional independencies implied by the structure of a mn are at least the ones that are implied by vertex separation i.e.   a﹠﹠b | s  if a and b are separated in the mn graph after removing all nodes in s  and all edges adjacent to them . for example  in fig. 1   1﹠﹠1 | {1} . if exactly those independencies hold in the actual probability distribution of the domain  we say that the domain and the graph are faithful to one another. faithfulness excludes certain distributions that are unlikely to happen in practice  and is needed

figure 1:	generative model of domain. left: correct. right: assumed.
for proofs of correctness. it is therefore a common assumption of independence-based algorithms for graphical model discovery. we assume faithfulness in the present work.
﹛as described later in our main algorithm  we maintain populations of structures at each time step t; slightly abusing our notation we denote these populations by xt. we denote a sequence of t tests y1 ... yt by y1:t and a sequence of value assignments to these tests  independence or dependence  corresponding to true and false respectively  by y1:t. 1 generative model over structures and tests
for an input data set d  our approach uses the posterior probability over structures pr x | d  x ﹋ x to learn the structure of the underlyingmodel as explainedin more detail in the next section. for that probability to be calculated in a principled way  we need a generative model that involves variables x y1 y1 ... yt and d that reflects any independence constraints among these variables. our only constraint here is the assumption that the tests are the sufficient statistics for the structure i.e.  there is no information in the data set d beyond the value of the tests as far as structure is concerned. this stems from the fact that the structure x faithfully encodes the independencies in the domain. note that this assumption is not particular to our approach  but is implicit in any independence-based approach. our generative model only formalizes it and makes it explicit.
﹛the generative model that encodes this constraint is shown in fig. 1  left   where x and d are d-separated by the tests y1 ... yt. however  the posterior over structures pr x | d  cannot be computed from this model because  according to the model  
y1:t d pr y1:t = y1:t | d   which requires the computation of pr y1:t = y1:t | d   currently an unsolved problem. we therefore assume the model shown in fig. 1  right   which contains multiple data sets d1 ... dt  abbreviated d1:t . in this model  it can be shown that the tests y1 through yt are independent given data sets d1:t. this allows the model to be solved because now pr  yi | di   where the factors pr yi = yi | di  can be computed by known procedures such as the discrete version of the bayesian test of  margaritis  1 . in practice we do not have more than one data set  and therefore we use the same data set for all tests i.e.  j. thus  the model depicted on fig. 1  right  is used only as an approximation to overcome the lack of an exact solution described above. as we will show in the experiments section  this approximation works well in both artificial and real world data sets.
﹛under this modified model  the posterior probability over structures must now be computed given data sets d1:t  i.e.  pr x | d1:t   which we abbreviate as prt x . in our calculations below we also use the conditional entropy h x |
y1:t d1:t  of x given the set of tests y1:t and the data sets d1:t  similarly abbreviated as ht x |y1:t .
﹛finally  the other parameters of the model are as follows: the prior pr x  is assumed uniform  and each test yi is completely determined given x = x i.e.  pr yi = true | x = x  ﹋ {1}  this is a direct consequence of the faithfulness assumption .
1	approach
to learn the mn structure of a domain from data  we employ a bayesian approach  calculating the posterior probability prt x  of a structure x ﹋ x given data sets d1:t. the problem of learning a structure in this framework can be summarized in the following two steps:  i  finding a sequence of tests y1:t of minimum cost such that h x | y1:t d1:t  = 1  and  ii  finding the  unique  structure x such that pr x =
.  this is always possible due to our assumption of faithfulness  which guarantees the existence of a single structure consistent with the results of all possible tests in the domain. 
﹛in practice  the above procedure presents considerable difficulties because:
  the space of structures x is super-exponential: |x| =
. thus  the exact computationof the entropyht x | y1:t   a sum over all x ﹋ x  is intractable.
  the space of candidatetests y is also at least exponential in size: there are tests  a﹠﹠b | s  with |s| = m  and m ranges from 1 to n 1. moreover  for a given number of tests t  there exist  possible candidate test sequences y1:t to consider.
we address the first issue using a particle filtering approach  discussed in section 1 . at each step t  we maintain a population of candidate mn structures xt for the purpose of representing the posterior probability distribution over structures given the outcomes of tests performed so far. in this way  all required quantities  such as posterior probability prt x  or conditional entropy ht x | y1:t   can be estimated by simple averaging over the particle population xt. the second issue  choosing the next test to perform  is addressed using a greedy approach. at each step of our algorithm  we choose as the next test to perform the member of y that minimizes the expected entropy  penalized by a factor proportional to its cost. since y is exponential in size  the minimization is performed through a heuristic search approach.
the next section explains our algorithm in detail.
1	the pfmn algorithm
﹛our algorithmis called pfmn  particle filter markovnetwork structure learner   and is shown in algorithm 1. at each time step t  the algorithm maintains a set xt containing n structure particles.
﹛initially  each structure in x1 is generated by randomly and uniformly selecting a number m of edges from 1 to  and then randomly and uniformly selecting the m edges by picking the first m pairs in a random permutation of all possible pairs. this ensures that all edge-set sizes have equal probability of being represented in x1.
algorithm 1 particle filter markov network  pfmn  algorithm. x = pfmn.

1: x1 ↘  sample n independent structure particles uniformly distributed among all edge-set sizes  see text .
1: t ↘  1
1: loop
1:	yt+1 ↘  argmax a b  argmaxs scoret a b | s 
1:	y1:t+1 ↘  y1:t ﹍ {yt+1}
1:	pt ↘  pr dt+1 | yt+1 = t  /* perform test on data. */
1:	pf ↘  pr dt+1 | yt+1 = f  /* perform test on data. */
1:	update	using eq.  1 .
1:	t+1 ↘  pf  t	prt+1 	  q 	  
1:if ht+1 x | y1:t+1  = 1 then1:return unique structure x such that prt x  = 1:t ↘  t + 1
at each time t during the main loop of the algorithm  lines
1   the test  that optimizes a score function is selected  the score function is described below . since for each pair of variables  a b  ﹋ v ℅ v the space of possible conditioning sets is exponential  equaling the power set of v   {a b}   this optimization is performed by heuristic search; in particular  first-choice hill-climbing is used. during this procedure  the neighbors of a current point s are all possible additions to s of a non-member all possible removals from s of a member  and all possible replacements of a member of s by a non-member. the score of test yt+1 is defined as follows:
 1 
where the factor w y   denotes the cost of y   which we take to be proportional to the number of variables involved in the test. this factor is used to discourage expensivetests. ht x | y1:t yt+1  is the entropy given the  not yet performed  test yt+1  and is equal to:
	ht x | y1:t yt+1  = ht x | y1:t    igt yt+1 .	 1 
the derivation of eq.  1  makes use of our generative model  fig. 1  and is simple and omitted due to lack of space. the term igt yt+1  denotes the information gain of candidate test yt+1 given all information at time t  i.e.  data sets d1:t  and is equal to:
ig logpr
﹛﹛﹛﹛﹛x﹋x where by we denote the value of tests y1:t in structure x  each test is either true or false . the above expression follows from bayes' rule and our generative model which implies that given a mn structure x  any test y is completely determined using vertex separation  assuming faithfulness .
this implies that pr  where 汛 a b  is the kronecker delta function that equals 1 iff a is equal to b. we say that structure x is consistent with assignment. the quantity pr of
eq.  1  can be calculated as
	pr	 1 
1:
 where by {y1:t} we denote the equivalence class of structures that are consistent with assignment y1:t. using this notation represents the set of all structures that imply the same values for the ci tests y1:t as structure x does. if x and x are in the same class {y1:t}  we say they are equivalent w.r.t. tests y1:t and we denote this by.
﹛eq.  1  has an interesting and intuitive interpretation: at each time t  the posterior probability of a test y being true  false   given some assignment y1:t of tests y1:t  equals the total posterior probability mass of the structures in the equivalence class {y1:t} that are consistent with y = true  y = false   normalized by the posterior probability mass of the class {y1:t}.
﹛to compute the information gain in eq.  1  we also need the posterior prt x  = pr x | d1:t  of a structure x ﹋ x for eq.  1 . as explained in section 1  we use the bayesian test described in  margaritis  1  . this test calculates and compares the likelihoods of two competing multinomial models  with different numbers of parameters   namely pr dt | yt = yt   yt ﹋ {true false}. since according to our generative model pr di | y1:t x  = pr di | yi  and
pr   by bayes' law we have:
t
pr .
i=1
﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛ 1  the constant of proportionality is independent of x and thus can be calculated by a sum over all structures in x. as explained above  this  like all equations in this section that involve summations over the space of structures x  is approximated by a summation over xt  the population of particles from the previous iteration of the algorithm.
﹛this completes the description of test score function of eq.  1 . returning to our description of alg. 1  lines 1 and 1 calculate the data likelihoods of the optimal test yt+1  which are used to update the posterior over structures and obtain prt+1 x  using eq.  1 . with this updated distribution  the new set of particles xt+1 is computed in line 1 using the particle filter algorithm  described in the next section.
﹛the pfmn algorithm terminates when the entropy ht x | y1:t   estimated over the population xt  becomes 1  returning the unique structure particle whose posterior probability equals 1.
1	particle filter for structures
a particle filter  andrieu et al.  1  is a sequential markovchain monte-carlo  mcmc  method that uses a set of samples  called particles  to represent a probability distribution that may change after each observation in a sequence of observations. at each step  an observation is performed and a new set of particles is obtained from the set of particles at the previous step. the new set represent the new  posterior  distribution given the sequence of observations so far. one of the advantages of this sequential approach is the often drastic reduction of the cost of sampling from the new distribution  relative to alternative  non-sequential  sampling approaches. in our case  the domain is x and particles represent structures. observations correspond to the evaluation of a single ci test on data.
﹛the particle filter algorithm  shown in algorithm 1  is used in line 1 of pfmn to transform population xt to xt+1. the change in the probability distribution from prt x  to algorithm	1	particle	filter	algorithm. pf x m f q .

1: prt x  ↘  f x 
1:	x ﹋ x do
1:		/* compute weights. */
1:	x ﹋ x do
1:	e	﹋x	/* normalize weights. */
1: /* resample particles in x using we as the sampling probabilities. */
1:  resample x we 
1: /* move each particle in times using metropolis-hastings and distribution prt x . */ 1: 
1: for all
 m-h x m prt q .
1: return x
algorithm 1 metropolis-hastings algorithm. m   h x m p q .x=1: x 1  ↘  x
1: for i = 1 to m   1 do
1:	‵ u
.
1:	ifthen
1:  i+1 	 i  ↘  x
1:
1: return x

prt+1 x  is reflected in a  weight.  this weight is used as a probability in the resampling step  line 1  to bias selection  with replacement  among the particles in xt. in the final step  all particles are  moved   lines 1  through m pairs of proposal/acceptance steps within the metropolis-hastings  m-h  algorithm  shown in alg. 1  andrieu et al.  1   in our experiments we use m = 1 . this algorithm requires that prt x  and a proposal distribution be provided as parameters. prt x  has already been addressed above in eq.  1 . as in many particle filtering applications  the proposal distribution is a key factor for the success of pfmn  and is discussed in detail in the next section.
1	proposal distribution for structures
as discussed previously  to use the metropolis-hastings algorithm one must provide a proposal distribution. here  we use a mixture proposal q = paqa +  1   pa qb  consisting of a global component qa and a local component qb.  we used pa = 1 in our experiments . the components qa and qb are as follows:
  global proposal qa  random walk :  generates a sample x from x by iteratively inverting each edge of x with probability 汐.  in our experiments  we use 汐 = 1 . thus  if h denotes the hamming distance between structures  and m = n n 1 /1  where n is the number of nodes  same for both structures   we have that.
  local proposal qb  equivalence-class moves :
x  chooses to either  a  remove an edge from set a  x 

figure 1: performance comparison of pfmn vs. gsimn for artificial domains. left: ratio of weighted cost for n = 1 and 而 = 1 1 1 1 1. middle: accuracy for n = 1 and 而 = 1 1 1 1 1. right: number of tests of conducted by pfmn andgsimn compared to.
 described below    b  add an edge from set a+ x   or  c  remove an edge e and add e where . each of  a    b   or  c  is chosen with probability 1. sets a+ x   a  x    and a㊣ x  are defined as:

where e x  is the set of edges of structure x and e‘ x  is its complement. the set a+ x   a  x   is the set of all edges that are missing  existing  in x that if added to  removed from  x  produce a structure that will be equivalent to x i.e.  in class. the set a㊣ x  is the set of edge pairs  such that if e is removed and e added to x the resulting structure will also be equivalent. therefore the result of any move under this proposal is a structure that is guaranteed to be equivalent to x  with hamming distance from x of at most 1.
﹛the local proposal was designed to maximize acceptance of proposed moves  i.e.  line 1 of the m-h algorithm evaluating to true  by proposing moves to independenceequivalent structures  which have provably the same posterior probability. a well-designed proposal i.e.  one that ensures convergenceto the posterior distribution prt x   must also satisfy the requirements of aperiodicity and irreducibility  andrieu et al.  1 . the above proposal q satisfies both requirements: it is aperiodic since it always allows for rejection  and irreducible since its support is the entire set of structures x.
1	experiments
we experimentally compare pfmn with the gsimn algorithm of  bromberg et al.  1  on both artificial and real-world data sets. gsimn is a state-of-the-art exact independence-based algorithm that uses pearl's axioms of ci tests to infer certain tests without actually conducting them. we report:  a  weighted number of tests  and  b  accuracy. the weight of each test is used to account for the execution times of tests with different conditioning set sizes  and is taken to be the number of variables involved in each test- see  bromberg et al.  1  for details. to obtain the quality of the recovered network  we measure accuracy as the fraction of unseen ci tests that are correct i.e.  we compare the result of each test on the resulting structure  using vertex separation  with the true value of the test  calculated using vertex separation in the true model for artificial domains or statistical tests on the data for real-world domains . the purpose of this procedure is to measure how well the output network generalizes to unseen tests.
1	artificial experiments
we first evaluated our algorithmin artificial domains in which the structure of the underlying model  called true network  is known. this allowed  i  a systematic study of its behavior under varying conditions of domain sizes n and amount of dependencies  reflect in the number of edges m in the true network   and   ii  a better evaluation of quality of the output networks because the true model is known. true networks were generated randomly by selecting the first pairs in a random permutation of all possible edges  而 being a connectivity parameter.
﹛fig. 1 shows the results of experiments for which ci tests were conducted directly on the true network through vertexseparation  i.e.  outcomes of tests are always correct. fig. 1  left  shows the ratio of the weighted cost of ci tests conducted by pfmn compared to gsimn for two domain sizes n = 1  above  and n = 1  below   a number of connectivities  而 = 1 1 1 1  and 1   and an increasing number of structure particles n. we can see that weighted cost of pfmn is lower than that of gsimn for small  而 = 1 1  and large  而 = 1 1  connectivities  reaching savings of up to 1% for n = 1 and 而 = 1. the case of 而 = 1 is inconclusive-pfmn does better in some  but not all cases. this can be explainedby the fact that the numberof structures with connectivity 而 grows exponentially as 而 approaches 1  from either side   which clearly makes the search task in pfmn more difficult. fig. 1  middle  shows that even though pfmn is an approximate algorithm  its accuracy for large enough number of particles n is exactly 1 in all but a few cases  e.g.  n = 1 n = 1 而 = 1 . gsimn is an exact algorithm  i.e.  its accuracy is always 1 when independence tests are correct  which is the case for the above experiments. its accuracy is thus omitted from the figures. fig. 1  right  shows that pfmn does much fewer  non-weighted  test than gsimn  and very close to the optimal number of tests which
	accuracy vs dataset size  n=1     = 1而	cost and accuracy for real data sets	table 1:	comparison of pfmn vs.
1haberman	111 11	adult	111 11hayes-roth	111 11balance-scale 111 11nursery111 11monks-1	11.1.1car111 11baloons111 11crx11111hepatitis11111cmc11111tic-tac-toe11111bridges11111alarm11111imports-1	11.1.1	flag	111 11dermatology 111 1	1k	1k	1k	1k	1k	1k	1k	1k	 1 1 1 1 1 1 1 1 1
	         number of data points in dataset  |d| 	dataset
figure 1: performance comparison of pfmn and gsimn on data. left: accuracy of pfmn and gsimn vs. data set size for sampled data. right: ratio of weighted cost of pfmn vs. gsimn and difference between accuracy of pfmn and gsimn on real data sets. the numbers on the x-axis correspond to data set indices in table 1. is  assuming a uniform distribution over structures  since the size of the space of structures is . the figure shows that pfmn is very close to this optimal for a range of domain sizes n.
﹛we also compared the robustness of pfmn and gsimn to uncertainty in the outcome of tests. this uncertainty is expected to occur in small data sets. data sets of different sizes were sampled from true networks of n = 1 and n = 1 variables both of connectivity 而 = 1. fig. 1  left  shows the accuracy of pfmn and gsimn vs. the size of the data set  |d| . accuracy of pfmn clearly outperforms that of gsimn  with the gap between their accuracies growing steadily with the data set size for both n = 1 and n = 1.
1	real-world experiments
we also conducted experiments on a substantial number of data sets obtained from the uci ml archive  d.j. newman and merz  1 . as above  we compare the weighted cost and accuracy of pfmn vs. gsimn. fig. 1  right  shows the ratio of the weighted cost of pfmn vs. gsimn  i.e.  a number smaller than 1 shows improvementof pfmn vs. gsimn  and the difference in accuracies of pfmn and gsimn  i.e.  a positive histogram bar shows an improvement of pfmn vs. gsimn . in these experiments  pfmn used n = 1. the numbers in the x-axis are indices to the data sets shown in table 1. for 1 out of 1 data sets  pfmn required lower weighted cost  reaching ratios as low as 1. moreover  for 1 out 1 data sets pfmn achieves this reduction in cost with little or no reduction in accuracy compared to gsimn  the rest exhibit only a modest reduction  less than 1% .
1	summary and conclusion
in this paper we presented pfmn  an independence-based algorithm for learning the structure of a markov network from data. we presented an analysis of the domain of structures and independencies and a number of interesting relations using an explicit generative model. we also showed experimentally that  compared to existing independence-based algorithms  pfmn is more resistant to errors in the test conducted and executes fewer tests in many cases. this helps in domains where data are scarce and tests uncertain or data are abundant and/or distributed over a potentially slow network.
