
nonparametric bayesian mixture models  in particular dirichlet process  dp  mixture models  have shown great promise for density estimation and data clustering. given the size of today's datasets  computational efficiency becomes an essential ingredient in the applicability of these techniques to real world data. we study and experimentally compare a number of variational bayesian  vb  approximations to the dp mixture model. in particular we consider the standard vb approximation where parameters are assumed to be independent from cluster assignment variables  and a novel collapsed vb approximation where mixture weights are marginalized out. for both vb approximations we consider two different ways to approximate the dp  by truncating the stick-breaking construction  and by using a finite mixture model with a symmetric dirichlet prior.
1 introduction
mixture modeling remains one of the most useful tools in statistics  machine learning and data mining for applications involving density estimation or clustering. one of the most prominent recent developments in this field is the application of nonparametric bayesian techniques to mixture modeling  which allow for the automatic determinationof an appropriate numberof mixturecomponents. current inferencealgorithms for such models are mostly based on gibbs sampling  which suffer from a number of drawbacks. most importantly  gibbs sampling is not efficient enough to scale up to the large scale problemswe face in modern-daydata mining. secondly  sampling requires careful monitoring of the convergence of the markov chain  both to decide on the number of samples to be ignored for burn-in and to decide how many samples are needed to reduce the variance in the estimates. these considerations have lead researchers to develop deterministic alternatives which trade off variance with bias and are easily monitored in terms of their convergence. moreover  they can be orders of magnitude faster than sampling  especially when special data structures such as kd trees are used to cache certain sufficient statistics  moore  1; verbeek et al.  1; kurihara et al.  1 .
¡¡ blei and jordan  1  recently applied the framework of variational bayesian  vb  inferenceto dirichlet process  dp  mixture models and demonstrated significant computational gains. their model was formulated entirely in the truncated stick-breaking representation. the choice of this representation has both advantages and disadvantages. for instance  it is very easy to generalize beyond the dp prior and use much more flexible priors in this representation. on the flip side  the model is formulated in the space of explicit  nonexchangeable cluster labels  instead of partitions . in other words  randomly permuting the labels changes the probability of the data. this then requires samplers to mix over cluster labels to avoid bias  porteous et al.  1 .
¡¡in this paper we propose and study alternative approaches to vb inference in dp mixture models beyond that proposed in  blei and jordan  1 . there are three distinct contributions in this paper: in proposing an improved vb algorithm based on integrating out mixture weights  in comparing the stick-breaking representation against the finite symmetric dirichlet approximation to the dp  and in the maintaining optimal ordering of cluster labels in the stick-breaking vb algorithms. these lead to a total of six different algorithms  includingthe one proposedin  blei and jordan  1 . we experimentallyevaluate these six algorithms and compare against gibbs sampling.
¡¡in section 1 we explore both the truncated stick-breaking approximation and the finite symmetric dirichlet prior as finite dimensional approximationsto the dp. as opposed to the truncated stick-breaking approximation  the finite symmetric dirichlet model is exchangeable over cluster labels. theoretically this has important consequences  for example a gibbs sampler is not required to mix over cluster labels if we are computing averages over quantities invariant to cluster label permutations  as is typically the case .
¡¡in section 1 we explore the idea of integrating out the mixture weights ¦Ð  hence collapsing the model to a lower dimensional space. this idea has been shown to work well for
lda models  teh et al.  1 wherestrong dependenciesexist between model parametersand assignmentvariables. such dependencies exist between mixture weights and assignment variables in our mixture model context as well  thus collapsing could also be important here. this intuition is reflected in the observation that the variational bound on the log evidence is guaranteed to improve.
¡¡in section 1 we derive the vb update equations corresponding to the approximations in section 1. we also consider optimally reordering cluster labels in the stick-breaking vb algorithms. as mentioned  the ordering of the cluster labels is important for models formulated in the stick-breaking representation. in the paper  blei and jordan  1  this issue was ignored. here we also study the effect of cluster reordering on relevant performance measures such as the predictive log evidence.
¡¡the aboveconsiderationslead us to six vb inferencemethods  which we evaluate in section 1. the methods are: 1  the truncated stick-breaking representation with standard vb  tsb   1  the truncated stick-breaking representation with collapsed vb  ctsb   1  the finite symmetric dirichlet representation with standard vb  fsd   1  the finite symmetric dirichlet presentation with collapsed vb  cfsd   and 1  and 1  being tsb and ctsb with optimal reordering  o-tsb and o-ctsb respectively .
1 four approximations to the dp
we describe four approximations to the dp in this section. these four approximations are obtained by a combination of truncated stick-breaking/finite symmetric dirichlet approximations and whether the mixture weights are marginalized out or not. based on these approximations we describe the six vb inference algorithms in the next section.
¡¡the most natural representation of dps is using the chinese restaurant process  which is formulated in the space of partitions. partitions are groupings of the data independent of cluster labels  where each data-point is assigned to exactly 1 group. this space of partitions turns out to be problematic for vb inference  where we wish to use fully factorized variational distributions on the assignment variables  q z  =
. since the assignments z1 = 1 z1 = 1 z1 = 1 represent the same partition  1  1  as z1 = 1 z1 = 1 z1 = 1  there are intricate dependencies between the assignment variables and it does not make sense to use the factorization above. we can circumvent this by using finite dimensional approximations for the dp  which are formulated in the space of cluster labels  not partitions  and which are known to closely approximate the dp prior as the number of explicitly maintained clusters grows  ishwaran and james  1; ishwaran and zarepour  1 . these finite approximations are what will we discuss next.
1 tsb and fsd approximations
in the first approximation we use the stick-breaking representation for the dp  ishwaran and james  1  and truncate it after t terms 
	i = 1 ... t   1	 1 
 1  i = 1 ... t	 1 
	i   t	 1 
where b v;1 ¦Á  is a beta density for variable v with parameters 1 and ¦Á  and one can verify that. incorporating this into a joint probability over data items x = {xn}  n = 1 ... n  cluster assignments z = {zn}  n = 1 ... n  stick-breaking weights v = {vi}  i = 1 ... t and cluster parameters ¦Ç = {¦Çi}  i = 1 ... t we find

where ¦Ð v  are the mixture weights as defined in  1 . in this representation the cluster labels are not interchangeable  i.e. changing labels will change the probability value in  1 . note also that as t ¡ú ¡Þ the approximation becomes exact.
¡¡a second approach to approximate the dp is by assuming a finite  but large  number of clusters  k  and using a symmetric dirichlet prior d on ¦Ð  ishwaran and zarepour  1  
	 1  this results in the joint model 

the essential difference with the stick-breaking representation is that the cluster labels remain interchangeable under this representation  i.e. changing cluster labels does not change the probability  porteous et al.  1 . the limit k ¡ú ¡Þ is somewhat tricky because in the transition k ¡ú ¡Þ we switch to the space of partitions  where states that result from cluster relabelings are mapped to the same partition. for example  both z1 = 1 z1 = 1 z1 = 1 and z1 = 1 z1 = 1 z1 = 1 are mapped to the same partition
 1  1 .
¡¡in figure 1 we show the prior average cluster sizes under the truncated stick-breaking  tsb  representation  left  and under the finite symmetric dirichlet  fsd  prior  middle  for two values of the truncation level and number of clusters respectively. from this figure it is apparent that the cluster labels in the tsb prior are not interchangeable  the probabilities are ordered in decreasing size   while they are interchangeable for the fsd prior. as we increase t and k these priors approximate the dp prior with increasing accuracy.

figure 1: average cluster size for three finite approximations to the dp prior. left: truncated stick-breaking prior  tsb  as given in  1 . middle: finite symmetric dirichlet prior  fsd . right: stick-breaking representation corresponding to the fsd prior. in each figure we show results for two truncation levels: t/k = 1  left bars  and t/k = 1  right bars .¡¡one should note however  that they live in different spaces. the dp itself is most naturally defined in the space of partitions  while both tsb and fsd are defined in the space over cluster labels. however  tsb and fsd also live in different spaces! more precisely  one can transform a sample from the fsd prior into the stick-breaking representation by performing a size-biased permutation of the mixture weights ¦Ð  i.e. after every sample from d ¦Ð  we sample an ordering according to ¦Ð without replacement . as it turns out  for finite k this does not exactly recover the left hand figure in 1  but rather samples from a prior very closely related to it shown in the right pane of figure 1. this prior is given by a stick-breaking construction as in eqn. 1  with stick-lengths sampled from 
	vi ¡« b vi;1 +	 1 
conversely  we can obtain samples from the fsd prior by applying a random  uniformly distributed permutation on the cluster weights obtained from eqn. 1 . although these two stick-breaking constructions are slightly different  for large enough k t they are very similar and we do not expect any difference in terms of performance between the two.
1 marginalizing out the mixture weights
the variational bayesian approximations discussed in the next section assume a factorized form for the posterior distribution. this means that we assume that parameters are independent of assignment variables. this is clearly a very bad assumption because changes in ¦Ð will have a considerable impact on z. ideally  we would integrate out all the parameters  but this is too computationally expensive. there is however a middle ground: we can marginalize out ¦Ð from both methods without computational penalty if we make another approximation which will be discussed in section 1. for both tsb and fsd representations the joint collapsed model over x z ¦Ç is given by 

with different distributions over cluster labels p z  in both cases. for the tsb representation we have 
		 1 
with
		 1 
and n¡Ýi = ni + n i. for fsd we find instead 
		 1 
1 variational bayesian inference
the variational bayesian inference algorithm  attias  1; ghahramani and beal  1  lower bounds the log marginal likelihood by assuming that parameters and hidden variables are independent. the lower bound is given by 

z
where ¦È is either {¦Ç v}  {¦Ç ¦Ð} or {¦Ç} in the various dp approximations discussed in the previous section. approximate inference is then achieved by alternating optimization of this bound over q z  and q ¦È . in the following we will spell out the details of vb inference for the proposed four methods. for the tsb prior we use 
		 1 
where q v  is not used in the tsb model with v marginalized out. for the fsd prior we use 

as well  q ¦Ð  is left out for the collapsed version.
1 bounds on the evidence
given the variational posteriors we can construct bounds on the log marginal likelihood by inserting q into eqn. 1 . after some algebra we find the following general form 
b

   + extra term	 1  where the  extra term  depends on the particular method. for the tsb prior we have 
termtsb
 1 
on the other hand for the fsd prior we find 
termfsd
 1 
for both collapsed versions these expressions are replaced by 
termctsb/cfsd 
1 vb update equations
given these bounds it is now not hard to derive update equations for the various methods. due to space constraints we will refer to the papers  blei and jordan  1; ghahramani and beal  1; penny  1; yu et al.  1  for more details on the update equations for the un-collapsed methods and focus on the novel collapsed update equations.
¡¡below we will provide the general form of the update equations where we do not assume anything about the particular form of the prior p ¦Çi . the equations become particularly simple when we choose this prior in the conjugate exponential family. explicit update equations for q ¦Çi  can be found in the papers  ghahramani and beal  1; blei and jordan  1; penny  1; yu et al.  1 .
for q ¦Çi  we find the same update for both methods 

while for q zn  we find the update
	 1  where the conditional p zn|z n  is different for the fsd and tsb priors. for the tsb prior we use  1   giving the conditional

where ni n = ni   i zn = i   n  in = n i   i zn   i  are the corresponding counts with zn removed. in contrast  for the fsd prior we have 
		 1 
1 gaussian approximation
the expectation required to compute the update  1  seems intractable due to the exponentially large space of all assignments for z. it can in fact be computed in polynomial time using convolutions  however this solution still tended to be too slow to be practical. a much more efficient approximate solution is to observe that both random variables ni and n i are sums over bernoulli variables:
. using the central limit theorem these sums are expected to be closely approximated by gaussian distributions with means and variances given by 

to apply this approximationto the computationof the average in  1   we use the following second order taylor expansion 
		 1 
this approximation has been observed to work extremely well in practice  even for small values of m.
1 optimal cluster label reordering
as discussed in section 1 the stick-breaking prior assumes a certain ordering of the clusters  more precisely  a size-biased ordering . since a permutation of the cluster labels changes the probability of the data  we should choose the optimal permutation resulting in the highest probability for the data. the optimal relabelling of the clusters is given by the one that orders the cluster sizes in decreasing order  this is true since the average prior cluster sizes are also ordered . in our experiments we assess the effect of reordering by introducing algorithms o-tsb and o-ctsb which always maintain this optimal labelling of the clusters. note that optimal ordering was not maintained in  blei and jordan  1 .

figure 1: average log probability per data-point for test data as a function of n.

figure 1: relative average log probability per data-point for test data as a function of n.
1 experiments
in the following experiments we compared the six algorithms discussed in the main text in terms of their log-probability on held out test data. the probability for a test point  xt  is then given by 
ztrain  q ztrain 
where the expectation e p zt|ztrain  q ztrain  is computed using the techniques introduced in section 1. all experiments
were conducted using gaussian mixtures with vague priors on the parameters.
¡¡in the first experiment we generated synthetic data from a mixture of 1 gaussians in 1 dimensions with a separation

figure 1: average log probability per data-point for test data as a function of t  for tsb methods  or k  for fsd methods .

figure 1: relative average log probability per data-point for test data as a function of t  for tsb methods  or k  for fsd methods .
coefficient1 c = 1. we studied the accuracy of each algorithm as a function of the number of data cases and the truncation level of the approximation. in figures 1 and 1 we show the results as we vary n  keeping t and k fixed at 1  while in figures 1 and 1 we plot the results as we vary t and k  keeping n fixed at 1 . we plot both the absolute value of the log probability of test data and the value relative to a gibbs sampler  gs . we 1 iterations for burn-in  and run another 1 iterations for inference. error bars are computed on the relative values in order to subtract variance caused by the different splits  i.e. we measure variance on paired experiments .
tsb	
o-tsb ctsb	
o-ctsb 
fsd	
cfsd	
figure 1: 1 most populated clusters found by the various algorithms in descending order of e ni . algorithms were trained on a random subset of 1 images from mnist and dimensionality reduced to 1 dimensions using pca. log probability of
1 test images are given by  l =  1 ¡À 1  tsb  
l =  1 ¡À 1  o-tsb   l =  1 ¡À 1  ctsb   l =  1 ¡À 1  o-ctsb   l =  1 ¡À 1  fsd   and l =  1 ¡À 1  cfsd . standard error over differences relative to o-ctsb are given by: dl =  1 ¡À 1  tsb   dl =  1 ¡À 1  o-tsb   dl =  1 ¡À 1  ctsb   dl =  1 ¡À 1  fsd   and dl =  1 ¡À 1  cfsd .
results were averaged over 1 independently sampled training/testing datasets  where the number of test instances was always fixed at 1
¡¡in the second experiment we have run the algorithms on subsets of mnist. images of size 1 ¡Á 1 were dimensionality reduced to 1 pca dimensions as a preprocessing step. we trained all algorithms on 1 splits of the data  each split containing 1 data-cases for training and 1 data-cases for testing. truncation levels were set to 1 for all algorithms. unfortunately  the dataset was too large to obtain results with gibbs sampling. all algorithms typically find between 1 and 1 clusters. results are shown in figure 1.
1 discussion
in this paper we explored six different ways to perform variational bayesian inference in dp mixture models. besides an empirical study of these algorithms our contribution has been to introduce a new family of collapsed variational algorithms where the mixture weights are marginalized out. to make these algorithms efficient  we used the central limit theorem to approximate the required averages.
¡¡we can draw three conclusions from our study. firstly  there is very little difference between variational bayesian inference in the reordered stick-breaking representation and the finite mixture model with symmetric dirichlet priors. secondly  label reordering is important for the stick-breaking representation. thirdly  variational approximations are much more efficient computationally than gibbs sampling  with almost no loss in accuracy.
¡¡we are currently working towards models where the parameters ¦Ç are marginalized out as well. we expect this to have a more significant impact on test accuracy than the current setup which only marginalizes over ¦Ð  especially when clusters are overlapping. unfortunately  it seems this will come at the cost of increased computation.
¡¡collapsed variational inference has also been applied to lda models  teh et al.  1   where preliminary results indicate significant performance improvement. we are currently also exploring collapsed variational inference for hierarchical dp models  teh et al.  1 .
