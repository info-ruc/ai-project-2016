
gait optimization is a basic yet challenging problem for both quadrupedal and bipedal robots. although techniques for automating the process exist  most involve local function optimization procedures that suffer from three key drawbacks. local optimization techniques are naturally plagued by local optima  make no use of the expensive gait evaluations once a local step is taken  and do not explicitly model noise in gait evaluation. these drawbacks increase the need for a large number of gait evaluations  making optimization slow  data inefficient  and manually intensive. we present a bayesian approach based on gaussian process regression that addresses all three drawbacks. it uses a global search strategy based on a posterior model inferred from all of the individual noisy evaluations. we demonstrate the technique on a quadruped robot  using it to optimize two different criteria: speed and smoothness. we show in both cases our technique requires dramatically fewer gait evaluations than state-of-the-art local gradient approaches.
1 introduction
legged robot platforms offer many advantages over traditional wheeled robots. in addition to their ability to traverse a wide variety of terrain  walking robots are basically a requirement for performing useful tasks in our human-centric world. despite these advantages  walking is also one of the fundamental challenges for legged robots.
모optimizing a robot's gait is not a simple control problem. an open loop gait consists of a sequence of joint values for an already high degree of freedom system of leg joints. simplified parametric representations of leg trajectories can result in a manageable number of parameters. however  these parameters will likely have complicated interactions making manual tuning of gait parameters time-consuming for simple robots and nearly impossible for the increasing complexity of humanoid platforms.
모even worse  no single gait will be effective in all circumstances. the walking surface is critical and can vary in terms of friction  softness  and height variation  e.g.  compare concrete  linoleum  carpet  and grass . robot platforms themselves also vary due to manufacturing imperfections and general joint and motor wear. lastly  even the criterion for determining an effective gait is likely to be situation specific. although velocity may seem like the obvious choice  relative stability of the robot's sensor hardware can also be important. although tailoring a robot's gait to the environmental  robot  and task specific circumstances would be ideal  constant manual re-tuning is impractical. automatic gait optimization is an attractive alternative to this laborious manual process.
모although walk learning is not a new idea  existing techniques are based on the common approach of local function optimization. hence  existing algorithms also share key drawbacks including local minima and inefficient use of gait evaluations. these drawbacks combine to increase the number of gait evaluations required to find effective parameters. in this work  we propose a different approach based on gaussian process regression. not only does the approach not suffer drawbacks inherent in local methods  but it has the additional advantages of providing confidence estimates on the gait's performance  which is useful for exploration  and allows for a natural inclusion of prior knowledge. we demonstrate the effectiveness of the approach on the sony aibo quadruped robot. we show that under two separate criteria the gaussian process regression not only finds effective gait parameters  but also does so with an order of magnitudefewer evaluations than a local gradient competitor.
모we begin in section 1 by examining some of the recent approaches to gait optimization. we note the common drawbacks in these approaches as motivation for our proposed technique. in section 1 we present relevant background material on gaussian process regression. we then  in section 1 describe how we apply gaussian processes to the problem of gait optimization. in section 1 we show results of our technique applied to two different gait optimization scenarios  demonstrating dramatically more efficient learning than both simple and state-of-the-art gait optimization techniques. finally  we discuss some future extensions of our approach.
1 motivation
the sony aibo  a commercially available quadruped robot  has spurred recent interest in gait optimization. its use in robot soccer within the robocup legged league gives an immediate application where gait velocity and uncertain competition conditions reward well tuned walks. since the aibo is our evaluation platform as well  we will examine a number of recent approaches pioneered on this robot.
모a common foundation for all of these approaches  including ours as well  is the notion of a  walk engine   or parameterized gait generation system. since the number of degrees of freedom in legged robots is large  optimizing the angle of each joint at a finely discretized time scale would involve searching over a thousand parameters. the walk engine reduces the number of parameters by focusing on leg trajectories that are both physically possible and intuitively plausible. these parameters usually define properties of each leg's trajectory such as the  distance the foot is lifted off the ground  and  the period of the walk . the space of parameterized walks obviously has a large impact on the final quality of any automatic gait optimization  but the optimization problem itself is the same regardless of the walk engine. optimization in all cases requires finding a point in parameter space  ranging from eight to fifty parameters  that results in an effective gait. all of the cited work below are based on different walk engines. hence  because of the variation in walk engine  surfaces  and robots themselves  reported walk speeds are largely incomparable.1
모a second common feature of the approaches is the experimental setup. all of the approaches involve evaluating specific parameter settings by having the aibo walk in a structured arena using the parameters to be evaluated. local sensor readings on the aibo-either camera images of visual landmarks or the ir sensor readings of walls-are then used to compute a noisy estimate of the gait's average speed. the procedure may be replicated and each estimate averaged to compute a more accurate evaluation.
1 evolutionary approaches
an evolutionary approach was the first proposed method for gait optimization on the aibo. hornby et al.  1; 1  used a fairly standard evolutionary search on an early prototype aibo. a population of parameters were maintained at each evolutionary generation. a new population was formed through mutation and crossover of individuals from the previous generation  replacing parameters in the population that evaluated poorly. their procedure showed slow gait improvement over 1 generations  requiring approximately twenty-five robot-hours of learning.
모the evolutionary approach was revisited by chernova and veloso  chernova and veloso  1 . they used similar mutation and crossover operations to generate candidate gaits. unlike the work of hornby  their parametric space of walks included a measurement of the possibility that the aibo could physically performthe gait. this allowed them to throw out poor gait parameters without requiring an empirical evaluation. in addition  they used a  radiation  procedure to disperse clusters of similar parameters in the population  forcing further search. they demonstrated that the technique learned competitive walks using only 1 evaluations and a total running time of approximately five hours distributed across four robots for a total of twenty robot-hours.
모as evaluations are noisy  both approaches must deal with the possibility that an inaccurate evaluation will cause poor gait parameters to incorrectly remain in the population. both used targeted reevaluation to reduce this possibility  reevaluating either the parameters that remained for multiple generations or the ones that performed disproportionately well.
1 function optimization
the second family of approaches that has been explored involves adapting techniques for multidimensional function optimization to the gait optimization problem. kim and uther  used powell's method  press et al.  1   which performs line search along a chosen search direction based on the effectiveness of previously chosen search directions. kohl and stone  used a hill climbing approach. although the gradient is not known  a set of random perturbations are evaluated empirically  and these evaluations are used to approximate the gradient. the parameters are then adjusted by a fixed step size in the direction of this estimated gradient. kohl and stone reported the fastest learning result in the literature  requiring only three hours distributed across three robots for a total of nine robot-hours. it is important to note that the reported experiments for both techniques involved initializing the optimization with a known set of reasonable parameters. this differs from evolutionary approaches  which begin with a random initial population.
1 drawbacks
all of the previous approaches share three key drawbacks. first  they can get stuck at local optima. kim and uther reported actual experiences of local optima and kohl and stone noted the importance of starting from good initial parameters  having found considerably poorer performance under different starting conditions. there are techniques to deal with local optima  such as random restarts for local function optimization approaches and radiation for evolutionary approaches. however  both involve a considerable increase in the required number of gait evaluations. furthermore  the approaches forget previously evaluated gaits once they either die out of the population or after the gradient step is taken. not only is this an inefficient use of expensive gait evaluations  but certain parameter settings may be unnecessarily reevaluated. finally  none of the approaches explicitly model the noise in the evaluation process. hence they all involve long evaluations or even repeated evaluations that are averaged to compute a less noisy estimate. in summary  if the goal is to reduce the total number of robot-hours these drawbacks severely hurt the usefulness of the approaches. our approach addresses these disadvantages and consequently requires considerably fewer gait evaluations.
1 background
at the core of our proposed method is a gaussian process regression model that describes our knowledge about the function we are optimizing. before discussing its use for gait optimization we give a brief overview of gaussian processes and gaussian process regression.
1 gaussian processes
a gaussian process  gp   williams  1; rasmussen  1; rasmussen and williams  1  is a collection f of random variables fx1 fx1 ... for which any finite subset of the variables has a joint multivariate gaussian distribution. the variablesare indexedby elementsx of a set x. for any finite length vector of indices x =  x1 x1 ... xn t  we have a corresponding vector fx =  fx1 fx1 ... fxn t of variables that has a multivariate gaussian  or normal  distribution 
	fx 몲 n {뷃 x  k x x } 	 1 
where the elements of 뷃 x  are given by a prior mean function 뷃 xi   and k is the kernel function. the kernel takes two indices xi and xj  and gives the covariance between their corresponding variables fxi and fxj. given vectors of indices xi and xj  k returns the matrix of covariances between all pairs of variables where the first in the pair comes from fxi and the second from fxj. note that each fxi is marginally gaussian  with mean 뷃 xi  and variance k xi xi .
1 gaussian process regression
suppose we have a function f x  that we would like to optimize. further  suppose that we cannot observe f directly  but that we can observe a random variable fx that is indexed by the same domain as f and whose expected value is f  i.e.   x 뫍 x  e fx  = f x . in particular  we assume that our prior belief about the function f conforms to a gaussian process with prior mean 뷃 and kernel k  as described above. suppose that fx is an observation of f x  that has been corrupted by zero-mean  i.i.d. gaussian noise  i.e.    where. hence  f x  is a hidden variable whose posterior distribution we can infer after observing samples of fx at various locations in the domain. the resulting inference is called gaussian process regression.
모let x be the set of observationspoints and fx be the resulting real-valued observations. we want to compute the posterior distribution of some new point x  뫍 x. the distribution will be gaussian with mean and variance  뷃 x |x  = 뷃 x   + k x   x k x x  1  fx   뷃 x   1  1 x |x  = k x   x     k x   x k x x  1k x x  .  1 
note that the inverse applies to the kernel matrix of observed domain points  and so can be computed once and used to evaluate the posterior at many points in the domain.
모since the problem is to find an optimum of the unknown function  the final step is to compute the optimum of the resulting posterior mean xr = argmaxx 뫍x 뷃 x |x . this  in general  cannot be computed in closed form  and requires the application of some technique for function optimization. although not optimal  another technique is to simply return the xi from x with the largest observed value fxi. this has the practical side-effect that the technique will not return a point that has never been evaluated  adding some protection from an incorrect prior.
1 observation selection
given a gaussian process model and a set of observations we have shown how to incorporate the observations to create the function posterior and then find the expected optimum. the remaining challenge  then  is to decide for which points xi to receive observations. this is  in fact  a sequential decision making task  as each observation gives additional information that can be used when selecting future points.
optimal selection. assume we have a fixed horizon  i.e.  a limited number of remaining observations  the optimal action given our model can theoretically be computed. this requires considering all possible points in the domain  all possible real-valued outcomes of this function evaluation  and then taking expectations and maximizations for all possible future observation sequences out to the the horizon. we then make the next observation at the point which maximizes the eventual posterior maximum assuming future optimal decisions. this enumeration for exploration is discussed in our previous work  wang et al.  1   but in domains with significant size  it is impractical. instead  we will focus on myopicsearch strategies.
most probable improvement. consider our proposal to return the domain point with the largest observed value. if we want to maximize the largest observed value  one approach would be to search for points that are likely to be better than our current best observation. let fx+ be the maximum observed value. choose xmpi to maximize the posterior probability p fxmpi   fx+ . we call this the most probable improvement  mpi  criterion.
모computing the posterior probability of improvement at a point x  in the domain is a simple computation since the posterior distribution of fx  is gaussian as given in equations 1 and 1. finding the point xmpi then requires a search through the domain to maximize the probability of improvement. this is equivalent to maximizing 
	뷋 x  	=	 x |x / fx+   뷃 x |x  .	 1 
this criterion for deciding where to evaluate next was suggested by mockus . fortunately  the posterior mean and standard deviation in this ratio are continuous and differentiable functions  so we can apply function optimization techniques in the model to find xmpi.
1 gait optimization
our proposed approach to gait optimization is basically the application of the above described gaussian process optimization procedure. like previous approaches  we assume that we already have some parameterized walk engine with k parameters. in addition  we can empirically take noisy evaluations of a gait's velocity  or other feature of the gait . unlike previous approaches  we model the stochastic velocity function  f : rk 뫸 r  which maps walk parameters to velocity  as a gaussian process. the exact mean and kernel function of the gaussian process are chosen based on prior domain knowledge. we use the most probable improvement selection rule to decide which parameters to empirically evaluate based on the previous observations. after some number of gait evaluations  the parameters that generated the fastest observed walk are returned.
모in place of choosing mutation and crossover rates or initial parameters and step sizes  our approach requires the specification of the mean and kernel of the gaussian process. this provides a very natural way to encode domain knowledge  if it exists  or one can just use  generic priors. we first describe our walk engine and then detail exactly how priors were chosen for our aibo experiments.
1 walk engine
as discussed earlier  a complete joint trajectory for a gait could have thousands of parameters. all gait optimization techniques parameterize this walk space using a walk engine. for this work  we have used carnegie mellon university's tekkotsu1 software to control the aibo  which includes a walk parameterization  as of release 1.1  originating from the carnegie mellon cmpack 1 robot soccer team. this is an early version of the cmwalk engine used in the work by chernova et al. . in consultation with a domain expert  we identified 1 walk parameters along with reasonable bounds on each parameter to define our domain  i.e. 
x   r1.
1 priors
defining the prior for the gaussian process means defining a mean function 뷃 : x 뫸 r and kernel function k : x 뫄x 뫸 r. we also need to specify the variance of the noise that is believed to be added to the observationfx. the priors that we use have a very simple form. for the mean  we use a constant function 뷃 x  = 뷃f  which means we have no a priori belief about any specific parameters' velocities. for the covariance function  it is hypothesized that  in general  parameter vectors that are close in terms of euclidean distance are likely to have similar walk velocities  and therefore a large positive covariance. furthermore  we expect that some parameters may have wide-reaching consequences  and so parameters far apart should still have some small positive correlation. we therefore chose to use the radial basis function kernel 
	 	 1 
where s is a scaling matrix that has along its diagonal the inverse of the range of each dimension  i.e.  the distance from one end to the other of one dimension's range is scaled to equal one. we now need to simply choose the constants  뷃f  and 횭1. setting these to appropriate values depends on the feature of the gait to be optimized. in section 1  we present results both for optimizing the gait's velocity and its smoothness. so we examine each of these cases in turn.
prior for velocity. gait velocity has been studied relatively extensively both on the aibo and with our particular walk engine. in consulting with our domain expert  we easily solicited useful prior information. in particular  we chose 뷃f = 1 and 횭1 = 1  which correspond to the intuition   for some random gait parameters  we expect the observed velocity to be 1 meters per second and within about 1 meters per second 1% of the time.  for observation noise  our domain expert was not familiar with our particular experimental setup. instead  we computed the sample variance of a small number of observations of one particular setting of the walk parameters. this gave us a value of   which seemed to work well in practice.
모as an interesting test of the stability of our method with respect to the model parameters  we also ran the velocity optimization with a prior variance parameter of 횭1 = 1  ten times larger .
prior for smoothness. since smoothness had not been evaluated before  we had no domain expert to consult. instead  we took a small number of samples  about 1  and used sample means and variances to estimate the parameters of the prior. as we will show in the next section  even this simple uninformed method for specifying the gaussian process model can be very effective.
1 implementation details
in order to compute the most probable improvement point as described in section 1  we used the generic constrained hill climber in matlab  fmincon  supplied with the function and gradient of 뷋 x   from equation 1. we used as default starting points the two best parameters found so far  and 1 drawn uniformly random within the bounded domain for a total of 1 starting points. in addition  we forced the first gait evaluation selection by choosing the center point of the domain. since the gaussian process model starts with a uniform belief over the domain  all function points are equally good to the most expected probable rule.
1 results
we have applied our gaussian process approach to two gait optimization tasks on the sony aibo ers-1. we first look at the standard problem of maximizing walk velocity  and we also examine the problem of optimizing a gait's smoothness. the goal of any gait optimization technique is to find a nearoptimal gait in as few evaluations as possible. therefore  we'll want to compare techniques using this criterion.
모since previousgait learninghas not involvedthe same walk engine  experimental setup  or robots  comparing directly with previously reported results can be somewhat problematic. for a more direct comparison  we have implemented the hill climbing method of kohl and stone  as described in section 1 and applied it in identical circumstances as our approach. the kohl and stone algorithm is the most data efficient technique for the aibo from the literature  demonstrating effective walks with only nine robot-hoursof training. we replicated their experimental setup  using 1 random evaluations to approximatethe gradient at each  iteration  and using a step size of 1. the empirical epsilon used in estimating the gradient was 1 of the parameters' range  which seemed to be an adequate change in performance. as with the gaussian process model  we started the hill climber from the point in the center of the space.
1 physical setup
to evaluate each parameter choice  we had the robot walk between two visual landmarks while focusing the head on the center of the landmark. the robot determined its distance from a landmark as it walked toward it based on the landmark's apparent width in the camera's field of view  and that change is used to estimate velocity. to determine a gait's smoothness  we measured the time-averaged distance from the center of the landmark to the center of the robot's field of view  and negated this. unstable walks that result in a large amount of head movement yield negative smoothness values  since it is difficult for the robot to keep the head and the camera aimed at the target. more fluid walks allow the robot to aim the camera more directly at the target  resulting in a smoothness much nearer to zero.
모each observed measurement is the result of three  traversals  from one landmark to the other. the average time for three traversals  including time to turn around  was approximately one minute. this was chosen to be consistent with kohl and stone's hill climbing experiments. we could have easily only used a single traversal and compensated by increasing the observation variance used in the model.
1 gait velocity
a graph showing the result of 1 observations is shown in figure 1 a . we chose this number of observations a priori to allow the hill climber 1  steps  or iterations with 1 test points for each. both our gaussian process technique and the kohl and stone hill climbing technique are shown  as well as the simple baseline of choosing gaits uniformly at random. the solid lines represent the maximum achieved walk speed over the accumulating observations  and the corresponding isolated markers show the maximum velocity achieved over the most recent 1 observations.
모we found that both the gaussian process and hill climbing methods performed appreciably better than random evaluations. the best walk velocity found was 1 m/s  which was found by the gaussian process with the over-estimated prior variance  횭1 = 1  although the walk found by the gaussian process with the  sensibly  initialized variance is nearly as fast. despite having already warned of the difficulties in comparing walk speeds  note that this speed is comparable to other learned gaits. more impressively  though  is the fact this speed was attained after only 1 observations  which took approximately two robot-hours. this is nearly a five-fold improvementin the required number of robot-hours1
모it is interesting that the best walks found by the two gaussian process models are in widely separated parts of the space. the walk found by the sensible setting of 횭1 = 1 has a low period and shorterstride  with a parametervectorfar from the center of the space. on the other hand  the experiment at 횭1 = 1 found a similarly fast walk near the center of the space with longer  slower strides that cover a similar distance.
모although the results of the hill climbing approach were not poor  we were somewhat surprised that the performance was not better. the method tended to take very poor steps once it reached a value of about 1 m/s. there are two natural explanations. one may simply be local optima  which is in line with kohl and stone's noted importance of the initial parameter vector. alternatively  based on the frequency of taking poor gradient steps  the step size may have been too large along certain dimensions. random restarts and a variable step size for each dimension could mitigate these unimpressive results.
1 gait smoothness
a similar graph for gait smoothness is shown in figure 1 b . the gaussian process optimization found the smoothest walk of all three methods and did so within the first 1 observations  or only twenty minutes of robot time. later improvement was only incremental. in a post-mortem analysis  the gait smoothness task is apparently much simpler. the impact of parameters on our measure of smoothness ended up being quite simple as a wide range of walks with a short period  below about 1 ms  and a moderate requested walk speed  between 1 and 1 m/s  resulted in a smooth gait measurement. scores of such walks were typically greater than -1. due to the independence of this pair of parameters from the rest  it is not unlikely that even random search would find a smooth gait as choosing parameters in this range will occur on average every 1 gait evaluations. our random search trial did happen to find one such point  giving it a moderate win over the hill climbing technique. again  we suspect the unimpressive hill climbing result to be due to initial conditions and local maxima.
1 conclusion
we proposed a new approach to gait learning based on gaussian process optimization. the approach overcomes many drawbacks of previous techniques  effectively avoiding local optima  efficiently using all gait evaluations  and explicitly modelling noisy observations. even with all of the caveats associated with comparing gait velocities and training time  we have demonstrated that the approach is not only effective in our high dimensional  noisy  non-convex optimization problem  but also requires drastically fewer evaluations. this was also accomplished with a minimum of parameter tuning  demonstrating effective performance when using prior settings from a domain expert  incorrect settings  and even data derived settings.
모there are three main directions we still feel should be explored from this point. first  our model is quite simple in comparison to gaussian process models in the recent machine learning literature. many interesting innovations involving kernel optimization  hyper-parametermodels  and dimensionality reduction seem well suited to this problem and are worth investigating. it would also be compelling to build

    gp w/mpi
1 m/s
1
 =1 f    gp w/mpi 1 m/s
1 =1
	f	.
 a  뫄  h.clmb
1 m/s    u.rand
1 m/s    gp w/mpi
-1 뫄  h.clmb
-1
 b     u.rand -1figure 1: results for  a  gait velocity and  b  gait smoothness. solid lines represent cumulative maximum  and the small markers indicate the maximum observation from the last 1 observations.models that incorporate knowledge from previous gait optimization runs with similar robots or surfaces. second  it would be interesting to explorehow knowledgeof an imposed budget on the number of samples or compute time could be efficiently incorporated into this approach. third  because of the generality of the method  it could also easily be applied in other application areas. we are currently investigating its use in the problem of  graph-cut stereo matching   kolmogorov  1   which has a number of continuous parameters that must be tuned. initial reports from our colleagues in vision are that gaussian process regression with the most probable improvement rule is also working very well on this problem.
acknowledgments
we would like to sincerely thank our domain expert sonia chernova  and our team of programmers: nelson loyola  eric coulthard  wesley loh  james neufeld  and sam vincent. this research is supported by alberta ingenuity  icore  and nserc. daniel lizotte is supported by the
killam trusts.
