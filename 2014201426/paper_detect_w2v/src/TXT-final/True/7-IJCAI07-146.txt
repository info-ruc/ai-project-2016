
we introduce relational grams  r-grams . they upgrade n-grams for modeling relational sequences of atoms. as n-grams  r-grams are based on smoothed n-th order markov chains. smoothed distributions can be obtained by decreasing the order of the markov chain as well as by relational generalization of the r-gram. to avoid sampling object identifiers in sequences  r-grams are generative models at the level of variablized sequences with local object identity constraints. these sequences define equivalence classes of ground sequences  in which elements are identical up to local identifier renaming. the proposed technique is evaluated in several domains  including mobile phone communication logs  unix shell user modeling  and protein fold prediction based on secondary protein structure.
1	introduction
probabilistic sequence models occupy an important position within the field of machine learning. they are not only theoretically appealing but have also proven to provide effective learning algorithms across many application areas  ranging from natural language processing to bioinformatics and robotics. in traditional sequence models  sequences are strings s = w1 ...wk over a  finite  alphabet ¦². typically  the goal is to estimate the joint distribution p s  over all possible strings. given p s  several tasks can be solved  including sequence classification  sampling  or predicting the next event in a sequence given a history of preceding events.
¡¡recent technological advances and progress in artificial intelligence has led to the generation of structured data sequences. one example is that of the smart phone  where communication events of many phone users have been logged during extended periods of time  raento et al.  1 . other ones are concerned with logging the activities and locations of persons  liao et al.  1   or visits to websites  anderson et al.  1 . finally  in bioinformatics  sequences often contain also structural information  durbin et al.  1 . these developments together with the interest in statistical relational learning  see  de raedt and kersting  1  for an overview  have motivated the development of probabilistic models for relational sequences. these are sequences of tuples or atoms such as that indicated in example 1. the relational markov model  rmm  approachby  anderson et al.  1  extends traditional markov models to this domain  and the logical hidden markov models  lohmms  by  kersting et al.  1  upgradetraditional hmms towards relational sequences and also allow for logical variables and unification. motivated by these models and the success and simplicity of n-gram models  we introduce relational grams  which upgrade the traditional n-grams towards relational sequences. there are two distinguished features of the r-grams. first  as lohmms and rmms they allow one to work with generalized  i.e. abstract atoms in the sequences. for instance  in the bioinformatics example the atom helix right alpha short  can be generalized to helix x alpha y  to describe an alphahelix of any orientation and length. secondly  the use of variables and unification allows one to make abstraction of object identifiers and to share information between events. for example  the  abstract  sub-sequence outcall x fail   outtxt x  describes that a user  after failing to reach a person  writes a text message to the same person  without stating the identity of the person. this is especially important when generalizing the patterns across phones or users  as the objects referred to will typically be different  and the precise identifiers do not matter but the relationships and events they occur in do.
¡¡the paper is structured as follows: in section 1  we introduce relational sequences and define the notion of a generative model that takes into account the nature of identifiers and object identity; in section 1  we then start from n-grams to derive r-grams; in section 1  we report on some experiments  and finally  in section 1  we conclude and touch upon related work.
1	relational sequences
example 1. the following sequences over atoms are examples for relational sequences from different domains:
outcall 1  fail   outcall 1  fail   incall 1  succ   outtxt 1   intxt 1    . . . mkdir rgrams   ls rgrams   emacs rgrams.tex   latex rgrams.tex   . . . strand sa plus short   helix right alpha medium   strand blb plus short   helix right f1to1 short  . . .
the first domain describes incoming and outgoing calls and text messages for a mobile phone user. in the second domain  unix shell commands executed by a user are described in terms of command name and arguments. in the third domain  helices and strands as protein secondary structure elements are defined in terms of their orientation  type  and length.an atom p t1 ... tn  is a relation p of arity n that is followed by n terms ti. we will work with three kinds of terms: constants  in slanted font   identifiers  in italic  and variables  starting with an upper case character . furthermore  the relations are typed  i.e. for each argument position i of the relation p  one can either have constants or identifiers. variables may appear at any position. we shall also distinguish constant-variables from identifier-variables; the former can be instantiated to constants  the latter to identifiers. they will be written in italic or slanted font respectively. for instance  the relation outcall has identifiers at the first position  and constants at the second one. therefore  outcall 1  fail  is type-conform. the types  constants  identifiers  variables and relations together specify the alphabet ¦²  i.e. the set of type-conform atoms. ¦²¡¥   ¦² is the set of atoms that do not contain identifiers. a relational sequence is then a string s = w1 ... wm in ¦² . an expression is ground if it does not contain any variable. example sequences will typically be ground  cf. example 1.
¡¡notice that constants typically serve as attributes describing properties of the relations  and identifiers identify particular objects in the domain. identifiers have no special meaning and they are only used for sharing object identity between events. moreover  there is no fixed vocabulary for identifiers which is known a priori  rather  new identifiers will keep appearing when applying a model to unseen data. therefore  it is desirable to distinguish ground sequences only up to identifier renaming  which motivates the following definition.
definition 1  sequence congruence . two relational sequences p1 ...pm  q1 ...qm are n-congruent if for all i ¡Ê {1 ... m   n} the subsequences pi ...pi+n 1  ri ...ri+n 1 are identical up to identifier renaming. two sequences s1 s1 are identical up to identifier renaming if there exist a one-to-one mapping   of the identifiers in s1 to those in s1 such that s1 equals s1 after replacing the identifiers according to  .
example 1. the sequences r x p a y r y p b x  and r z p a w r w p b u  are 1-congruent  but not 1-congruent .
¡¡m-congruent sequences are identical up to identifier renaming. for n   m  the definition takes into account a limited history: two sequences are congruent if their object identity patterns are locally identical. finally we note that ncongruence defines an equivalence relation  i.e. it is reflexive  symmetric and transitive.
¡¡let us now define a generative model for relational sequences. it should not sample actual identifier values  but rather equivalence classes of congruent sequences. this yields the following definition:
definition 1  generative model . let ¦² be a relational alphabet. let s ¦²  be the set of all ground sequences of length m over ¦²  and let sn ¦²  be the set of equivalence classes induced on s ¦²  by n-congruence. then a generative model of order n over ¦² defines a distribution over sn ¦² .
¡¡such a generative model can be learned from a set of ground sequences and the alphabet ¦². in its simplest form  the learning problem can be stated as maximum likelihood estimation:
given
¡¡  a relational alphabet ¦²
¡¡  a set s of ground sequences over ¦²
¡¡  n ¡Ê n
¡¡  a family ¦« of generative models of order n over ¦²  find a model ¦Ë ¡Ê ¦« that maximizes

where  s  denotes the equivalence class of s with regard to ncongruence. a simple instance of such a family of generative models will be introduced in the next section.
1	r-gram models for relational sequences
markov chains are amongst the simplest yet most successful approaches for sequence modeling. n-grams are based on higher-order markov chains  and employ certain smoothing techniques to avoid overfitting the sample distribution. in this section  we will briefly review n-grams  and then propose a simple extension of n-grams for relational sequences.
1	n-grams: smoothed markov chains
n-grams define a distribution over sequences w1...wm of length m by an order n   1 markov assumption:

 where wi n+1 is a shorthand for wmax 1 i n+1  . in the most basic case  the conditional probabilities are estimated from a set s of training sequences in terms of  gram  counts:

where c w1...wk  is the number of times w1...wk appeared as a subsequence in any s ¡Ê s. note that this is indeed the estimate maximizing the likelihood.
¡¡the gram order n defines the tradeoff between reliability of probability estimates and discriminatory power of the model. rather than selecting a fixed order n  performance can be increased by combining models of different order which are discriminative but can still be estimated reliably  manning and schu¡§tze  1 . the two most popular approaches are back-off and interpolation estimates. in this paper  we will focus on the latter approach  which defines conditional distributions as linear combinations of models of different order:

where the ¦Á1 ... ¦Án are suitable weights with  and the lower-order distributions pk wi | wi k+1 ...wi 1  are estimated according to maximum likelihood  equation 1 . several more advanced smoothing techniques have been proposed  cf.  manning and schu¡§tze  1    but are beyond the scope of this paper.
1	r-grams: smoothed relational markov chains
consider ground relational sequences of the form g1 ...gm ¡Ê s ¦² . the key idea behind r-grams is a
markov assumption
	.	 1 
however  defining conditional probabilities at the level of ground grams does not make sense in the presence of object identifiers. thus  ground grams will be replaced by generalized ones. generality is defined by a notion of subsumption:
definition 1  subsumption . a relational sequence l1 ... lk subsumes another sequence k1 ... kn with substitution ¦È  notation   if and only if k ¡Ü n and  i 1 ¡Ü i ¡Ü k : li¦È = ki. a substitution is a set
{v1/t1 ... vl/tl} where the vi are different variables and the ti are terms such that no identifier or identifier variable occurs twice in {t1 ... tl}.
¡¡the restriction on the allowed substitutions implements the object identity subsumption of  semeraro et al.  1 . the notion of sequence subsumption is due to  lee and de raedt  1 . it can be tested in linear time.
example 1. let y z u v be identifier variables.
r
r
but
¡¡¡¡¡¡¡¡r. with ¦È1 = {x/w y/u} and ¦È1 = {x/w y/u z/v}.
¡¡we can now refine equation 1 to take into account generalized sequences. this will be realized by defining

where. this generalization abstracts from identifier values and at the same time yields smoothed probability estimates  with the degree and characteristic of the smoothing depending on the particular choice of li n+1...li. this is formalized in the following definition.
definition 1  r-gram model . an r-gram model r of order n over an alphabet ¦² is a set of relational grams ln1 ¡Å ... ¡Å lnd ¡û l1...ln 1
where
1.  i : l1...ln 1lni ¡Ê ¦²¡¥ ;
1.  i : lni contains no constant-variables;
1.  i : lni is annotated with the probability values  such that
1. ; i.e. the heads are mutually exclusive.
example 1. the following is an example of an order 1 relational gram in the mobile phone domain  see example 1 .
	1	outtxt x      
	1	outtxt y  
	1	outcalloutcall x  fail 
...
	1	intxt y  	  
it states that after not reaching a person a user is more likely to write a text message to this person than to somebody else.
¡¡we still need to show that an r-gram model r defines a distribution over relational sequences. we first discuss a basic model by analogy to an unsmoothed n-gram  before extending it to a smoothed one in analogy to equation 1.
a basic model
in the basic r-gram model  for any ground sequence g1...gn 1 there is exactly one gram  with
. its body l1...ln 1 is the most specific sequence in ¦²¡¥ subsuming g1...gn 1. according to equation 1  we start by defining a probability pr g | g1...gn 1  for any ground atom g given a sequence g1...gn 1 of ground literals. let g be a ground literal and consider the above gram r subsuming g1...gn 1. if there is an i ¡Ê {1 ... d} such that it is unique and we define

otherwise  pr g | g1...gn 1  = 1. from pr g | g1...gn 1   a probability value pr g1...gm  can be derived according to equation 1. note that this is not a distribution over all ground sequences of length m  as the model does not distinguish between n-congruent sequences. instead  the following holds:
 lemma 1. let r be an order n r-gram over ¦²  and  be relational sequences with s n-congruent to s. then .
	let us therefore define pr  s  	:=	pr s  for any
 s  ¡Ê sn q . furthermore . there-
fore 
theorem 1. an order n r-gram over ¦² is a generative model over ¦².
example 1. consider the r-gram model r with grams p a x  ¡Å p b x  ¡û r x  r x  ¡û p b x  r x  ¡Å r y   ¡û p a x  r
and uniform distributions over head literals.  is an artificial start symbol that only matches at the beginning of the sequence. the ground sequence g1...g1 = r u p a u r v p b v r v  has probability pr g1...g1  = 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 = 1.
smoothing r-grams
in the basic model  there was exactly one gram r ¡Ê r subsuming a ground subsequence g1...gn 1  namely the most specific one. as for n-grams  the problem with this approach is that there is a large number of such grams and the amount of training data needed to reliably estimate all of their frequencies is prohibitive unless n is very small. for n-grams  grams are therefore generalized by shortening their bodies  i.e.  smoothing with k-gramestimates for k   n  equation 1 .
¡¡the basic idea behind smoothingin r-grams is to generalize grams logically  and mix the resulting distributions:

where pr g | g1...gn 1  is the probability defined by r as explained above  r  is the subset of grams in r subsuming g1...gn 1  and ¦Á is a normalization constant  i.e.
. the more general r  the more smooth the probability estimate pr g | g1...gn 1  will be. the actual degree and characteristic of the smoothing is defined by the set of matching r-grams together with their relative weights ¦Ár.
¡¡by analogy with n-grams  additional smoothing can be obtained by also considering relational grams r ¡Ê r with shorter bodies l1...lk 1  k   n. however  there is a subtle problem with this approach: relational grams of order k   n define a probability distribution over sk ¦²  rather than sn ¦²   i.e. the sequences are partitioned into a smaller number of equivalence classes. however  this can be taken care of by a straightforward normalization  which distributes the probability mass assigned to an equivalence class modulo k equally among all subclasses modulo n. example 1. consider the r-gram model r with grams r q given by
¡¡¡¡¡¡¡¡¡¡¡¡r : r x  ¡Å r y   ¡û r x  q :	r x  ¡û uniform distribution over head literals and ¦Ár = ¦Áq = 1. we expect pr r u  | r v  +pr r v  | r v   = 1. however  when directly mixing distributions
pr r u  | r v   = ¦Árpr r y   | r x  +¦Áqpq r x   = 1
pr r v  | r v   = ¦Árpr r x  | r x  +¦Áqpq r x   = 1  as the r-gram q does not distinguish between the sequences r x r x  and r x r y . instead  we mix by

where ¦Ã	=	1 is the number of subclasses modulo
1-congruence of the class  r x  .
¡¡the ultimate level of smoothing can be obtained by a relational gram r of the formwhere the are fully variablized  also for non-identifier arguments . for this gram

where x1 ... xa are the non-identifier arguments of lni and x1 ... xa their instantiations in g. this case corresponds to an out-of-vocabulary event  observing an event that was not part of the training vocabulary  for n-grams.
1	building r-grams from data
to learn an r-gram from a given set s of training sequences  we need to 1  choose the set r of relational grams; 1  estimate their corresponding conditional probabilities; and 1  define the weights ¦Ár for every r ¡Ê r. before specifying our algorithm  we need to define counts in the relational setting:

our algorithm to learn r-grams is specified below: r-grams input: sequences s; alphabet: ¦²; parameters: ¦Ã n 
1 b := {l1 ...lk 1 ¡Ê ¦²¡¥ |c l1 ...lk 1    1 and k ¡Ü n}
1 for each l1 ...lk 1 ¡Ê b
1 do let {lk1 ...lkd} contain all maximally specific literals lki ¡Ê ¦²¡¥ such that c l1 ...lk 1lki     1
1 add	with
1.
1
1
1	return
in line 1 of the algorithm  one computes all r-grams that occur in the data. notice that no identifiers occur in these r-grams  cf. ¦²¡¥  . this can be realized using either a frequent relational sequence miner  such as mineseqlog  lee and de raedt  1   or using an on-the-fly approach. in the latter case  a relational gram with body l1...lk is only built and added to r when and if it is needed to evaluate
pr g | g1...gn 1  with on unseen data. in
line 1  all possible literals are sought that occur also in the data. they are maximally specific  which means that they do not contain constant-variables  cf. condition 1 of definition 1 . line 1 then computes the maximum likelihood estimates and lines 1 the weight ¦Ár. here s r  denotes the set of all ground subsequences g1...gn 1g appearing in the data which are subsumed by r. the likelihood l r  of r defined in line 1 is a measure for how well the distribution defined by r matches the sample distribution. the ¦Ár as in line 1 is then defined in terms of |s r | and the parameter ¦Ã  which controls the tradeoff between smoothness and discrimination. highly discriminative  specific  rules have higher likelihood than more general ones as they are able to fit the sample distribution better  and thus receive more weight if ¦Ã   1.
1	experiments
this section reports on an empirical evaluation of the proposed method in several real-world domains. more specifically  we seek to answer the following questions:
 q1  are r-grams competitive with other state-of-the-art approaches for relational sequence classification 
 q1  is relational abstraction  especially of identifiers  useful 
experiments were carried out on real-world sequence classification problems from three domains. in the unix shell domain  greenberg  1; jacobs and blockeel  1  
domainr-gramslohmmlohmm + fkprotein111domainr-gramsknnc1unix-1.1 ¡À 111unix-1.1 ¡À 111table 1: comparison of classification accuracy of r-grams to logical hidden markov models and fisher kernels in the protein fold domain  and to k-nearest neighbor and c1 in the unix shell domain. for protein fold prediction  a single split into training and test set is used. in the unix shell domain  1 subsets of 1 examples each are randomly sampled  accuracy determined by 1-fold cross-validation and averaged.
the task is to classify users as novice programmers or non-programmers based on logs of 1 shell sessions containing 1 commands  constants  and their arguments  identifiers . to reproduce the setting used in  jacobs and blockeel  1   we sampled 1 subsets of 1 instances each from the data  measured classification accuracy on these using 1-fold cross-validation  and averaged the results. in the protein fold classification domain  the task is to classify proteins as belonging to one of five folds of the scop hierarchy  hubbard et al.  1 . strand names are treated as identifiers  all other ground terms as constants. this problem has been used as a benchmark before  kersting et al.  1; kersting and ga¡§rtner  1   and we reproduce the experimental setting used in this earlier work: the same 1 examples per fold are used for training  and the remaining examples as the test set. in the context phone domain  data about user communication behavior has been gathered using a software running on nokia smartphones that automatically logs communication and context data. in our study  we only use information about incoming and outgoing calls and text messages. phone numbers are identifiers  other ground terms constants. the task in phone i is to discriminate between real sequences of events and  corrupted  ones  which contain the same sequence elements but in random order. for k ¡Ê {1 1 1}  1 subsets of size k were sampled randomly 1-fold cross-validation performedand averagedfor each k. in phone ii  the task is to classify communication logs as belonging to one of three users  based only on their communication patterns but without referring to actual phone numbers in the event sequence.
¡¡in all domains sequence classification is performed by building an r-gram model rc for each class c and labeling unseen sequences s with the class that maximizes pc s p c . we used bigram models in the phone ii domain and trigram models for all other domains  and the smoothing parameter ¦Ã was set to 1 in all experiments. learning the rgram model was done on-the-fly as explained in section 1.
¡¡table 1 compares the classification accuracy of r-grams with accuracy results from the literature in the protein fold and unix shell domains. in the protein fold domain  a handcrafted logical hidden markov model achieves 1% accuracy  kersting et al.  1 . this has been improved to 1% by a fisher kernel approach  in which the gradient of the likelihood function of the logical hidden markov model is used
domainr-gramsn-gramsn-grams w/o idsprotein111unix-1.1 ¡À 111 ¡À 1unix-1.1 ¡À 111 ¡À 1phone i1 ¡À 111 ¡À 1phone ii1 ¡À 111 ¡À 1table 1: accuracy comparison of r-grams to n-grams  and to n-grams w/o ids. for protein/unix domains settings are as before. for the phone i domain  1 subsets of size 1 have been sampled from the data  a 1-fold cross-validation is performed on each set and results are averaged. for phone ii  results are based on one 1-fold cross-validation. results for n-grams are based on one sample only.

figure 1: accuracy for different training set sizes ranging from 1 to 1 examples in the phone i domain. for each size  1 sets are sampled  a 1-fold cross-validation is performed and the result averaged.
as input in a support vector machine  kersting and ga¡§rtner  1 . the unix shell log classification problem was originally tackled using a k-nearest neighbor method based on customized sequence similarity  jacobs and blockeel  1 . in the same paper  the authors present results for a c1 decision tree learner using a bag-of-words representation. in both cases  r-grams yield competitive classification accuracy  which is a positive answer to question  q1 . furthermore  we note that even using a na¡§ ve implementation r-grams are computationally efficient. times for building an r-gram model ranged from 1 to 1 seconds in the presented experiments1.
¡¡in a second set of experiments  the effect of using relational abstraction was examined in more detail. more precisely  r-grams were compared to n-grams which implement non-relational smoothing as outlined in section 1  treating the atoms in ¦² as flat symbols. for these experiments  we tried keeping identifiers in the events  n-grams  or removing them from the data  n-grams w/o ids . accuracy results for the protein fold  unix shell and context phone domains are

1
all experiments were run on standard pc hardware with
1ghz processor and 1gb of main memory.
given in table 1. if identifiers are treated as normal constants  accuracy is reduced in all cases  especially for the identifierrich unix and context phone domains. this is not surprising  as most identifiers appearing in the test data have never been observed in the training data and thus the corresponding event has probability zero. when identifiers are removed from the data  performance is similar as for r-grams in the protein fold and unix shell domain  but worse in the context phone domains. on phone i  r-grams significantly outperform n-grams w/o ids  unpaired sampled t-test  p = 1 . figure 1 shows more detailed results on phone i for different numbers of training examples. it can be observed that relational abstraction is particularly helpful for small numbers of training examples. to summarize  there are domains where it is possible to ignore identifiers in the data  but in other domains relational abstraction is essential for good performance. therefore  question  q1  can be answered affirmatively as well.
1	conclusions and related work
we have presented the first approach to upgrading n-grams to relational sequences involving identifiers. the formalism employs variables  unification and distinguishes constants from identifiers. it also implements smoothing by relationally generalizing the r-grams. the approach was experimentally evaluated and shown to be promising for applications involving the analysis of various types of logs.
¡¡the work that we have presented is related to other work on analyzing relational and logical sequences. this includes most notably  the rmms by  anderson et al.  1  who presented a relational markov model approach  and the lohmms by  kersting et al.  1  who introduced logical hidden markov models. rmms define a probability distribution overground relational sequenceswithout identifiers. furthermore  they do not employ unification  or variable propagation  but realize shrinkage through the use of taxonomies over the constant values appearing in the atoms and decision trees to encode the probability values. rmms only consider first order markov models  i.e. the next state only depends on the previous one  so rmms do not smooth over sequences of variable length. rmms have been applied to challenging applications in web-log analysis. whereas rmms upgrade markov models  lohmms upgradehmms to work with logical sequences. as r-grams  they allow for unification and offer also the ability to work with identifiers. in addition  they allow one to work with structured terms  and functors . however  as rmms  they only consider first order markov models. furthermore  they do not smooth distributions using models of different specificity. finally  mineseqlog  lee and de raedt  1  is a frequent relational sequences miner that employs subsumption to test whether a pattern matches a relational sequence. one might consider employing it to tackle the first step in the r-gram algorithm.
¡¡there are several directions for further work. these include: extending the framework to work with structured terms  considering back-off smoothing instead of interpolation  and  perhaps most importantly  applying the work to challenging artificial intelligence applications.
acknowledgments
the authors would like to thank the anonymous reviewers  kristian kersting and hannu toivonen for valuable comments  and mika raento for making the context phone data available. the research was supported by the europeanunion ist programme  contract no. fp1  application of probabilistic inductive logic programming ii.
