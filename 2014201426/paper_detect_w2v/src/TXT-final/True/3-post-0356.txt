
recent research on point-based approximation algorithms for pomdps demonstrated that good solutions to pomdp problems can be obtained without considering the entire belief simplex. for instance  the point based value iteration  pbvi  algorithm  pineau et al.  1  computes the value function only for a small set of belief states and iteratively adds more points to the set as needed. a key component of the algorithm is the strategy for selecting belief points  such that the space of reachable beliefs is well covered. this paper presents a new method for selecting an initial set of representative belief points  which relies on finding first the basis for the reachable belief simplex. our approach has better worst-case performance than the original pbvi heuristic  and performs well in several standard pomdp tasks.
1 introduction
partially observable markov decision processes  pomdps  provide a general framework for planning under uncertainty  kaelbling et al.  1; sondik  1 . one of the standard algorithms used to provide solutions to pomdps is value iteration  which associates values to probability distributions over states. because exact value iteration is intractable  a lot of recent work has focused on approximate algorithms  which rely on compressing the belief space or considering a finite set of reachable beliefs. a representative algorithm for this class is point-based value iteration  pbvi   pineau et al.  1 . in pbvi  a finite set of reachable belief points is selected heuristically  and values are computed only for these points. the success of pbvi depends crucially on the selection of the belief points. in particular  these points should cover the space of reachable beliefs as evenly as possible. the main pbvi heuristic uses one-step simulated trajectories in order to find reachable beliefs. the belief that is  farthest away  from the points already included is greedily added to the set. we explore an alternative heuristic for choosing belief points. we aim to find core beliefs  i.e. beliefs which form a basis for the space of all reachable beliefs. this approach is aimed at covering more quickly the space of beliefs that are reachable  by looking farther into the future. this improves the worst-case behavior of the pbvi algorithm. we discuss how core beliefs can be found  and we illustrate the behavior of the algorithm on several standard pomdp benchmarks.
1 pomdps and point-based value iteration
a pomdp is described by a tuple hs a o t   r 붺 b1i  where s is a finite set of states  a is a finite set of actions and o is a finite set of observations. the probability of any state at t = 1  is described by the initial belief  b1. the transition function t describes the probability of going from state s to state s given that action a is chosen. the observation function   o s a  describes the probability of observation o given that action a was taken and state s was reached. the reward function  r s a   specifies the expected immediate reward  obtained after executing action a in state s. the discount factor 붺 뫍  1  is used to weigh less delayed rewards. the goal of an agent acting in a pomdp is to find a way of choosing actions  policy  maximizing the future return: e 뫉t 붺tr st at  . in order to achieve this goal  the agent must keep either a complete history of its actions and observations  or a sufficient statistic of the history. the sufficient statistic in a pomdp is the belief state  b  a vector with |s| components  which represents a probability distribution over states:
모모모모모bt s  = p st = s|b1 a1...at 1ot  on each time step  after taking action a and making observation o  the agent updates its belief state using bayes rule:
b
p o|a b 
where the denominator is just a normalizing constant.
모in value-based pomdp methods  the agent computes a value function  which is maintained over belief states. a lot of recent research has been devoted to computing such values approximately. compression-based methods  roy and gordon  1; poupart and boutilier  1  aim to compress the belief space  and maintain a value function only on this compressed version. point-based value iteration  pbvi  pineau et al.  1  maintains values and gradients  붸-vectors  in pomdp terminology  for a selected set of belief points  b. the idea is that much of the belief simplex will not be reachable in general. the set b is expanded over time  in order to cover more of the reachable belief space  and these expansions are interleaved with computing new value estimates.
1 belief selection
in order to extend the belief set b  the standard pbvi heuristic considers  for each b 뫍 b  all possible actions a and samples one observation o for each action. among these beliefs reachable from b  the belief b1 that is farthest away from b is picked and added to b. this heuristic is motivated by an analytical upper bound on the approximation error  which depends on the maximum l1 distance from any reachable belief to b:
1
where  몬 is the set of all reachable beliefs. pbvi does not keep an upper bound on the value function when expanding its belief set. however smith and simmons  smith and simmons  1  have developed an alternative anytime solution method  hsvi  based on maintaining an upper and a lower
bound of the optimal value function to guide the local value updates. they choose the belief points to update using forward heuristic search. vlassis and spaan  vlassis and spaan  1  instead sample a large set of reachable beliefs b during a random walk  but then only update a subset of of b  sufficient to improve values overall.
모we propose an approach for covering the space of reachable beliefs more quickly  by considering longer courses of action  as well as by removing the strictly greedy character of the pbvi algorithm. we will use core beliefs  which form a basis for the belief simplex. the idea of core beliefs is inspired by work on predictive state representations  psrs   littman et al.  1   an alternative way of representing stochastic dynamical systems. in order to reason about the space of reachable beliefs  one can consider the initial belief vector  b1  and all possible sequences of actions and observations following it. these sequences are called tests. james and singh  1  introduced the concept of an infinite matrix  whose rows correspond to all histories and whose columns correspond to all tests. in this paper we use the same infinite matrix but the rows correspond to all reachable beliefs from a given initial belief point. for a row b and a test a1 ...an 1on  the content of the corresponding element in the matrix is the probability that o1 ...on is observed given that action sequence a1 ...an 1 is executed starting from b. this matrix has finite rank  at most equal to the number of states in the pomdp. we define core beliefs as the set of linearly independent rows of this matrix.
모the computation of core beliefs can be done exactly using the pomdp model  as shown in algorithm 1. it is similar to the computation of core tests in  james and singh  1   except that here we compute the elements of the matrix having the pomdp model while they estimate these elements from data without the model. but in large environments  considering all one-step extensions for all tests is prohibitive. we experimented with two heuristics for the extensions:
1. random: pick a random action  but consider all observations.
1. 붼-greedy: consider actions that are likely to be picked by a good policy. we generate first a subset of the core beliefs based on all one-step tests. then  we run pbvi on this subset  for a fixed number of iterations  in order algorithm 1 finding core beliefs
initialize the matrix with b1 and rows corresponding to all beliefs reachable by one-step tests a o. one column is all 1's and the others correspond to one-step tests a o. for all elements  b ao  do
compute p o|b a   using  1  and store it in row b  column a o
end for repeat
compute a set of linearly independent rows and columns
add the linearly independent rows to the set of core be-
liefs
eliminate all linearly dependent rows and columns add all one-step extensions to the matrix and compute the rank of the extended matrix
until the rank is unchanged

to get a first approximation of the value function. we extend the core belief matrix by picking one action  in 붼-greedy fashion. the next observation can be sampled as well.
note that for pbvi  the maximum error value for 붼b that can be achieved is 1  attained for the case in which two beliefs are non-zero over different states. this case cannot occur when all core beliefs are part of b. hence  the worst-case error when using core beliefs is strictly smaller.
1 experimental results
we performed experiments on several standard pomdp domains used in the literature. for the smaller three domains  1 grid  1 grid and cheese  we computed the core beliefs exactly  column  cbvi-all  of table 1 . the proposed heuristics are run using all possible observations in the onestep extensions. we also ran standard pbvi with a similar number of initial beliefs. in all cases we performed 1 iterations of pbvi to obtain a value function. then  we ran 1 trials  with each trial cut at 1 steps. we averaged the results obtained over these trials  and over 1 independent runs.
모all versions of the cbvi algorithm yield improvements over the pbvi heuristic. surprisingly  in the 1 and cheese domains  the best result is achieved by the 붼-greedy heuristic. we conjecture that the reason is that the 붼-greedy heuristic actually focuses the computation on good actions  that are likely to be used in the optimal policy.
domainpbvicbvi-allcbvi-randcbvi-egr1 grid1  11  11  11 11 grid1  11  11  11  1cheese1  11  11  11  1table 1: small domains
domainpbvi-origvspbvicbvi 1 cbvi 1 cbvi 1 cbvi 1 cbvi 1 maze1.1.1.1.1  11  11  11  11  1hallway1.1.1.1.1  11  11  11  11  1hallway1.1.1.1.1  11  11  11  11  1coffee-1--1 11  11 1---table 1: large domains모in the larger domains  we only used the 붼-greedy heuristic with sampled observations  as the other versions are too expensive. the experimental setup is as above. in this case  we find a first set of core beliefs  then run pbvi for 1 iterations. we take the resulting belief set  and add one-step extensions using 붼-greedy actions and sampled observations  as above. this results in a new matrix  which yields a new set of core beliefs. this process is repeated at most 1 times; if no more core beliefs are detected in a new iteration  the process stops. the results are presented in the columns cbvi 1  to cbvi 1  in table 1. note that for the coffee domain  all core beliefs are found after two such repetitions. hence  there is no data for the cbvi 1 -cbvi 1 . the reward for standard pbvi using a similar number of initial beliefs is in the column pbvi in table 1. the results reported in the original pbvi paper  pbvi-orig  pineau et al.  1  and the results by vlassis and spaan  vs  vlassis and spaan  1  are in the columns pbvi-org and vs respectively. as shown in table 1  for maze1 all algorithms obtain very similar results. the coffee domain is a very favorable case for cbvi  because it has a nice structure  and the reachable space of beliefs lies in a very low dimensional part of the entire belief simplex. generating core beliefs can be done in 1 seconds for this domain  because there are only two core beliefs for this problem  although its pomdp representation has 1 states. our experiments show a huge value improvemnet for this particular case over pbvi. for the larger domains  hallway1 and hallway1  cbvi also has a definite advantage  especially in the later iterations  when the initial subset of core beliefs is larger. the time complexity of cbvi is the same as pbvi if generating the core beliefs can be done in a preprocessing phase. in small domains and large domains with small intrinsic dimensionality  e.g. coffee  this can be done in a few seconds. however  generating all core beliefs in large domains in general is very expensive. in our experiments  we reached the maximum number of core beliefs in at most five extensions of the matrix in algorithm 1; however  the computation time  taking into account the process of belief generation is two orders of magnitude longer than for pbvi. although this approach improves the value function and the policy that is obtained  it is significantly more expensive  at the moment  unless the problem at hand has special structure. we are investigating different ways to compute a reasonably large portion of the space of core beliefs at a lower computation cost.
acknowledgments
this research was supported in part by funding from nserc and cfi. we thank joelle pineau for very helpful discussions and insights into point-based algorithms  as well as for constructive feedback. we thank matthijs spaan for providing code for his point-based value iteration algorithm.
