 
we present a problem solving paradigm called generate  test and debug  gtd  that combines associational rules and causal models  producing a system with both the efficiency of rules and the breadth of problem solving power of causal models. the generator uses associational rules to generate plausible hypotheses; the tester uses causal models to test the hypotheses and produce a detailed characterization of the discrepancy in case of failure. the debugger uses the ability to reason about the causal models  along with a body of domain-independent debugging knowledge  to determine how to repair the buggy hypotheses. the gtd paradigm has been implemented and tested in three different domains; we report in detail on its application to our principal domain of geologic interpretation. we also explore in some depth the character of the problems for which gtd is well suited and consider the character of the knowledge required for successful use of the paradigm. 
1. introduction 
problem solving efficiency and broad range of applicability  robustness  are desirable but often incompatible features of al systems. we present a paradigm called generate  test and debug  gtd  that combines the efficiency of associational rule-based systems with the robustness of reasoning from causal models and we characterize the domains for which gtd may be applicable. an implemented program called gordius uses the gtd paradigm to solve planning and interpretation problems in several different domains 
- geologic interpretation  the domain for which gordius was initially developed   blocks world planning and the tower of hanoi problem. 
we have explored the use of gtd primarily for planning and interpretation tasks. both tasks are of the general form  given an initial state and a final  goal  state  find a sequence of events which could achieve the final state.  if the final state is in the future  we regard it as a 
planning problem; if the initial state is in the past  we regard it as interpretation. examples of planning are block stacking and route planning. interpretation problems include geologic interpretation and figuring out what happened to the economy. 
gtd solves problems in three stages. figure 1 shows the data and control flow between the generate  test and debug stages. 
1. generate - the generator uses associational rules to map from effects to cause. the left-hand side of a rule is a pattern of observable effects and the right-hand side is a sequence of events which could produce those effects. the rules are matched against the final state and the resultant sequences are combined to produce an initial hypothesis - a sequence of events that is hypothesized to achieve the final state. 
1. test - the tester simulates the sequence of events to determine the validity of the hypothesis. if the test is successful  i.e.  the results of the simulation matches the final state  then the hypothesis is accepted as a solution. otherwise  
the tester produces a causal explanation for why the hypothesis failed to achieve the final state. it then passes this explanation and the buggy hypothesis on to the debugger. 
1. debug - the debugger uses the causal explanation from the tester to track down the source of the bugs in the hypothesis. it uses both domain-specific causal models and domainindependent debugging knowledge to suggest modifications which could repair the hypothesis. the modified hypothesis is then submitted to the tester for verification. alternatively  the debugger has the option to invoke the generator to produce a new hypothesis. 

figure 1. control flow of gtd paradigm 
as its name implies  gtd is related to the traditional al method of generate and test  newell . the debugger is included in gtd to provide more guidance in the search for a solution. in traditional generate and test  the tester reports only a single bit of information - success or failure. in the case of failure  the generator has no indication of what went wrong with the previous hypothesis and so it can only  try again.  in gtd  the 
	simmons and davis 	1 

debugger uses the test results extensively to focus the search for a valid hypothesis. 
the core idea underlying gtd has a long history in al. 
early work by  sussman  explored  problem solving by debugging almost right plans  as a model of skill acquisition.  rich & waters  describe a similar paradigm for the programmer's apprentice  calling it aid   abstraction  inspection and debugging   and more recently   hammond  presented a similar method which learned classes of bug repairs for the cooking domain. 
our work advances the state of the art in several ways: 
1. our implementation of the gtd paradigm is relatively domain independent. just by replacing the associations rules and causal models our program gordius has solved problems in geologic interpretation  all of the blocks-world examples in  sacerdoti  1  and the three-ring tower of hanoi problem. in addition  the test and debug portions of gordius have been used to help diagnose faults in semiconductor manufacturing  mohammed . 
1. our debugger is fairly robust. we have taken a large step toward developing a general method for debugging sequences of events. the key ideas include reasoning about time and causality using causal models of the domain. we have extended the available methods for reasoning about causal models  since previous domainindependent programs for reasoning about sequences of events  such as  chapman   
 sacerdoti  1    tate    wilkins   were limited to models which were not complex enough for the geologic interpretation problem. for instance  we need to represent and reason about quantified effects  conditional effects and effects that create or destroy objects. 
1. we have analyzed the relationships between the generator and debugger to determine how each contributes to the overall problem solving capabilities of the system. based on this analysis  we have characterized the domains for which gtd is likely to be useful. 
the remainder of this paper describes gtd in more detail and discusses the relative contributions of associational rules and causal models to the gtd paradigm. section 1 presents the use of gtd in geologic interpretation. section 1 describes the gtd paradigm in a domain-independent fashion and section 1 discusses how the associational rules used by the generator and causal models used by the debugger each contribute to the overall performance of the system. finally  the conclusions characterize domains in which gtd might be applicable. 
	1. 	geologic interpretation 
our research has focused primarily on the problem known as geologic interpretation  shelton   in which we are given a diagram representing a vertical cross-
1 	reasoning 
section of the earth and a legend indicating the rock types  figure 1 . the task is to infer a sequence of events which plausibly explains how the region came into existence. figure 1 shows one plausible solution to the cross-section in figure 1. 
we use a simplified model of geology known as  layer cake  geology. deposition  which occurs when silt in water deposits on the sea bed  creates horizontal sedimentary formations that stack up like the layers of a cake. erosion occurs when wind abrades exposed rock formations and is assumed to occur horizontally  slicing through the earth like a knife. intrusion creates igneous formations when molten rock from below intrudes  pushes  into or through upper rock layers. faulting splits the earth and  in our model  moves one side of the fault downwards relative to the other side. uplift and subsidence move the earth uniformly up or down  respectively  and tilt rotates the earth around some origin. 
d granite 
뫳 mafic-igneous 
figure 1. geologic interpretation example 
1 deposit shale 
1 intrude granite into shale 
1 uplift 
1 intrude mafic igneous through granite and shale 
1 fault across shale and granite 
1 erode shale and mafic igneous 
figure 1. one plausible solution sequence to figure 1 
1 generation of the initial hypothesis 
this section describes how gordius interprets the diagram of figure 1  detailing the knowledge used at each stage of the problem solving process. the first step is to generate an initial hypothesis by matching scenarios against the diagram. a scenario is an associational rule that maps from geologic effects  which are observable patterns in the diagram  to a local interpretation  which is a sequence of events that could have produced the geologic effects. 
geologic effects are represented in terms of topological patterns of edges and faces and their associated geometric constraints. for example  figure 1 illustrates the  intrusion  scenario  which represents that an igneous formation intruded into an existing rock formation. the pattern in this rule  figure 1a  matches those parts of the diagram where an igneous rock is between two rocks of the same type and the edges of those rocks are parallel. one match in figure 1 occurs where fign1 is between fs1 and fs1. 

events' 
1 create rock1 
1. intrude ign through rockl 
pattern constraints 
igneous ign  
interpretation constraints: 
same-type r1 r1  
part-of rocki rl  
parallel e1  e1  
part-of rock1  r1  lntersects rock1 ign  
figure 1a  intrusion  scenario pattern 
1b. local interpretation 
the local interpretation of a scenario contains three types of information: 1  the events that occurred to form the observable effects; 1  temporal orderings between events; 1  the objects which are related via a part-whole hierarchy. in the intrusion scenario  the local interpretation  figure 1b  contains an event that creates rock formation rockl  where r1 and r1 are pieces of formation rockl   followed by an intrusive event that creates the igneous formation ign. in addition  the local interpretation includes constraints necessary for testing the hypothesis. for example  the intrusion scenario asserts that the igneous formation is constrained to intersect the rock formation. 
note that scenario patterns do not necessarily imply a unique sequence of events. for example  there are  at least  two plausible explanations for the pattern  sedimentary-over-igneous  where a sedimentary rock is on top of an igneous rock: 1  the igneous rock intruded into the sedimentary rock; 1  the igneous rock intruded into some pre-existing rock  everything was uplifted  the upper layers eroded  exposing the igneous rock  which then subsided  after which the sedimentary rock was deposited on top of the igneous rock. note that in the first interpretation the igneous rock is younger  while in the second it is older. the generator prefers the first interpretation since it is considered simpler  it is shorter  and hence more likely to have occurred. the second interpretation is used only if the first one leads to an inconsistency. 
the local interpretations derived from matching scenarios are combined to yield an initial hypothesis. in this example  four of the fourteen currently defined scenarios were used to produce the initial hypothesis shown in figure 1. the applicable scenarios are  intrusion   used twice    faulting    erosion  and  sedimentary-over-igneous   using the first interpretation . note that the interpretation of figure 1 differs from the solution in figure 1 in two important respects. first  the faulting and intrusion are left unordered since the diagram in figure 1 does not contain enough information to tell which event occurred first. second  uplift does not appear because  for this example  the uplift event was removed from the local interpretation of the  erosion  scenario; we are using an incomplete scenario in order to provide a simple illustration of the test and debug stages of gtd. 
	1 	testing the hypothesis 
the initial hypothesis is tested using a simulation technique called imagining that combines qualitative and quantitative simulations. the imagining technique is described in detail in  simmons  1a ; in  simmons  1b  we argue the need for both qualitative and 

모모모모모figure 1. initial hypothesis sequence generated quantitative simulations in this domain. 
the imaginer is given a hypothesis - a partially ordered sequence of events  e.g.  figure 1  - and simulates one of the totally ordered sequences consistent with it. simulating a total order is necessary because our quantitative simulator can handle only linear sequences of events. simulating just one of the total orders is sufficient because our task is to produce one plausible interpretation. 
the qualitative simulation produces a causal dependency structure describing how the events cause the geologic objects to change over time; the quantitative simulation produces a diagram that is matched against the goal diagram. if the two diagrams match  the hypothesis is considered to be a plausible interpretation. if they do not match  the hypothesis is handed to the debugger for refinement. 
in this example  a bug is found during the qualitative simulation: our geologic models state that deposition occurs below sea-level  while erosion occurs above sea-level. thus  when it comes time to simulate the erosion step in the sequence of figure 1  the imaginer finds a mismatch between the state of the simulation and the preconditions required to carry out erosion. 
	1 	debugging the sequence 
the debugger attempts to repair the hypothesis by modifying the sequence of events. it uses both the causal dependency structure produced by the tester and geologic domain models to suggest plausible repairs. the modified hypotheses are then tested until a plausible hypothesis is found. alternatively  if the debugging efforts seem to be moving away from a solution  the generator may be invoked to provide an alternative hypothesis. 
the debugger traces back through the causal dependency structure to locate the underlying assumptions made during the generation of the hypothesis that eventually led to the buggy situation. it then uses domain-independent debugging knowledge and the causal geologic models to suggest ways to change the assumptions in order to repair the bug. 
recall that  in our current example  the bug is that the surface of the earth is below sea-level at the time of the erosion. two of the many assumptions underlying that bug are 1  some fixed amount of deposition  dlevel-1  is done and 1  deposition is the last event to affect the height of the earth's surface. in an attempt to raise the surface of the earth above sea-level  which would repair 
	simmons and davis 	1 

the bug  the debugger first considers increasing dlevei1  the amount of deposition. however  the debugger quickly infers that this would not help because our deposition model indicates that deposition can only occur under water  hence no amount of deposition can raise the surface above sea-level. 
the debugger next considers inserting processes that can increase the height of the surface. the two possibilities are uplift and tilt. the debugger prefers the uplift process because it infers that inserting a tilt would introduce additional bugs. the debugger thus inserts an uplift event between the deposition and erosion events  adding the constraint that the amount of uplift is enough to raise the surface of the earth above sea-level  see 
figure 1  

figure 1. sequence generated by gtd algorithm 
this modified hypothesis is then submitted to the tester for verification. as mentioned above  the tester chooses an arbitrary total ordering  see figure 1  consistent with the partial ordering in figure 1. this time the simulation completes successfully  producing the diagram in figure 
1. 	gordius thus concludes that the sequence of 
figure 1 is one plausible interpretation of the diagram in 
figure 1. this does not imply  however  that all total orderings consistent with the partial order of figure 1 are plausible solutions. 

figure 1 sucessful simuiation of interpretation example 
1. the gtd algorithm 
the previous section illustrated how gtd works in the particular domain of geologic interpretation. this section briefly examines the characteristics of the gtd paradigm as it applies to planning and interpretation problems in general and supplies additional details about the technique. 
	1 	generate 
the knowledge base for the generator consists of associations rules that map from effects to cause. in this section we describe the general form of the rules and discuss why the associational rules can be used to generate hypotheses efficiently. 
the right-hand side of a rule is a partially ordered sequence of events and associated constraints. in the 
1 	reasoning 
blocks-world  for example  the rule for interchanging two blocks x and y where x is initially on y consists of the event  puton x z   followed by  puton y x   with the constraint that  z#y   that is  put x somewhere and then put y on top of x. 
the left-hand side of a rule is a pattern consisting of parts that are matched against both the final state and the initial state. the part of the pattern matched against the final state represents observable effects that are produced by the sequence of events given in the rule's right hand side. the part matched against the initial state represents preconditions that must hold for the events to occur. for example  in the block interchange rule described above   on y x end   is matched against the final state and the pattern  on x y start   
clear x.start   is matched against the initial state.* 
the inference engine for the generator consists of matching the rule patterns against the initial and final states and combining the sequences of events in a consistent manner. the generator is  in general  quite efficient at producing hypotheses. this arises in part because we use efficient pattern-matching techniques  including discrimination nets. 
the main source of the generator's efficiency  however  comes from having associational rules that are nearly independent  that is  the events suggested by applicable rules can be pieced together to form a valid hypothesis  or at worst one that needs only a small amount of debugging.** thus  an important task in constructing a good rule set is finding the level of granularity where the assumption of independence largely holds. this issue is discussed in more detail in section 1. 
to deal with obvious cases where the assumption of independence fails  the generator possesses a limited form of consistency checking together with the ability to backtrack if an inconsistent hypothesis is detected. the generator can detect inconsistencies in temporal orderings and parameter values for example  it is inconsistent for the sequences  deposition-1 followed by intrusion-1 and  lntrusion-1 followed by deposition1  to appear in the same hypothesis. similarly  the generator can infer that one rule suggesting a tilt of 1뫢 is inconsistent with another rule stating that the tilt should be 1뫢. 
although it is preferable to have the generator do as much consistency checking as possible  testing for global consistency is a relatively expensive procedure that should be done infrequently. thus  only after the generator produces an initial hypothesis is the resulting sequence of events completely tested. 
모the scenarios for geologic interpretation differ slightly from this general description. first  they do not refer to the initial state since it is assumed always to be the same - bedrock that is initially below sea-level. second  for efficiency reasons the scenario patterns are represented as diagrams  however  we are currently re-representing the patterns as sets of predicates in order to use the same generator for all our domains. 
** to be precise  the assumption of independence holds only for rules with different patterns since  as noted  the same pattern can be caused by different sequences of events. 

1 test 
we need to test tor two reasons. first  the hypothesis may be incomplete if there is no rule to account for some effect. this is a common problem in developing associational rule sets and occurs when the knowledge engineer overlooks unusual or infrequent situations. second  combining local interpretations may not yield a globally consistent solution. for example  one of our scenarios matches any single  non-horizontal layer of sedimentary rock and infers that the rock was tilted. if tilting had occurred to that rock  however  all other existing formations would have been tilted as well  which may not be correct. 
there are two characteristics the tester must have for 
gtd to work properly. first  as in the traditional generate and test paradigm  the tester must be correct over the domain of interest relative to the generator. that is  the tester cannot reject valid hypotheses produced by the generator  i.e.  allow false negatives  otherwise the gtd algorithm might miss solutions. we use the qualitative simulator of  simmons  1a  for our tester since we have found simulation to be a relatively accurate  yet simple  testing technique. second  for the debugger to know how to proceed  the tester must return an explanation for why the test failed. the explanation is a causal dependency structure that details how the events in the hypothesis affect the state of the world and explicitly represents when objects persist  that is  the intervals of time during which they do not change. these two properties - causality and persistence - provide a foundation for a general method of tracking down and repairing a wide variety of bugs  see section 1 . 
figure 1 shows part of the dependency structure produced by our simulator for the bug encountered earlier  in which the surface of the earth is below sealevel at a time when we would like erosion to occur  the complete dependency structure produced by gordius has almost 1 nodes . it indicates  for instance  that the deposition of shale caused the height of the surface to increase by dlevel-1 from time to to t1 and that that height persisted between t1 and t1. 
	1 	debug 
the task of the debugger is to modify the hypothesis in order to repair the bugs found by the tester. the debugger uses the dependency structure created by the tester to track down potential sources of a bug - those assumptions made in generating the hypothesis on which the bug causally depends. this set is generated by collecting the leaf nodes of the dependency structure. in the example of figure 1  the potential bug sources 
include 1  nothing changes the height of the surface between time t1 and time t1; 1  time t1 is before time t1; 1  the amount of deposition done is dlevel-1; 1  sealevel is a constant; 1  all of deposition's remaining  implicit  preconditions hold  this covers all conditions which are beyond the scope of the model  hence are not explicitly represented  such as the fact that deposition can occur only if sediment is present in the water . we still have the difficult task of deciding which of the potential bug sources is actually to blame and 

figure 1 partial causal dapandancy structure 
determining what to do in order to repair the bug. this is handled by having domain-independent bug repair strategies that suggest modifications to the hypothesis based on an analysis of the domain models  the dependency structure  and the change to the state of the world needed to repair the bug. the bug repair strategies can suggest inserting or deleting an event  replacing an event with a  similar  event  changing parameter values and changing the temporal ordering between events. the debugger chooses among the possible bug repairs using a best-first search strategy whose distance metric is based on the number of unachieved goals and the length of the sequence of events. that is  it prefers short plans that account for as much of the goal state as possible. 
different bug repair strategies are used for different types of assumptions. the current system includes bug repair strategies for: 1  the assumption that nothing changes a value during an interval  i.e.  the value persists during that interval ; 1  the assumption that a parameter has a particular value; 1  the assumption of a temporal ordering between events; 1  the assumption that all the implicit preconditions of an event hold. currently the system cannot deal with situations in which the causal models are incorrect  the given initial state is incorrect and something assumed to be constant is in fact varying. 
to illustrate the use of repair strategies  consider the assumption  nothing changes height surface  from t1 to t1.  since this is an assumption about persistence  the strategy is to insert a process that can change the value. in this case  we need to change the height of the surface to a value greater than sea-level. in the geologic domain there are several processes that can affect the height of the surface. two of them  subsidence and faulting  are rejected because their effects are to decrease height. uplift is suggested because its effect  raising the height of all geologic objects  is what is 
	simmons and dsvis 	1 
needed. tilting can either increase or decrease height  depending on the direction and origin of the tilt. thus  the debugger does not know whether adding tilt will actually repair the bug by raising the surface above sealevel. however  the debugger still suggests adding tilt as a possible repair because it considers the resultant  uncertain situation to be an improvement over the currently known  but buggy  situation. 
much of the robustness of the debugger derives from its ability to reason from causal domain models  see section 1 . the debugger makes use of the following types of information in determining how to repair bugs: 1  the types of objects which can be changed  created or destroyed by a process; 1  whether a process can affect an object in a particular way  such as changing its height; 1  the magnitude of the effect; 1  the conditions under which the effect occurs; 1  how long the change will last  persistence ; 1  whether constraints on parameter values are satisfied. 
to facilitate this reasoning  we have developed a representation language for processes that is used in the four domains we have explored  geology  semiconductor fabrication  blocks-world and tower of hanoi . processes are represented as  discrete actions   that is  they specify what the state of the world will be like after the process occurs  but say nothing about what happens while the process is occurring. the language represents time explicitly and describes the state of the world after the process occurs as functions of parameter values and the state of the world before the process occurs. in order to represent complex domains like geology  we have extended the range of traditional strips-like action representations by including conditional effects  universally quantified effects and explicit constraints on parameter values  see  simmons  1a  . 
1. the nature of associational rules and causal models 
the utility of gtd is based on the claim that the generator provides an efficient means of synthesizing hypotheses and the debugger provides a robust means of modifying them. in this section  we analyze this claim. 
1 the robustness provided by causal models 
the claim that the debugger is rooust stems in part from the fact that the debugging algorithm described in section 1 uses a fairly general technique to determine how to effect needed changes. however  the robustness of the debugger in a given domain depends largely on the robustness of the causal models it analyzes. our approach will thus be useful in domains where such causal models are known and where they can be represented in a language that the debugger can analyze. a major research problem is whether such a domain-independent representation language exists  but experience with four rather different domains bolsters our belief that our process representation language and debugger can be used in a wide variety of domains. 
a related assumption underlying gtd is that it is easier 
1 	reasoning 
to construct robust causal models than to construct robust associational rules. otherwise  it would be less work simply to develop a robust generate and test system. although there is empirical evidence from our own work and others  e.g.   koton   that robust causal models are indeed often easier to construct than robust associational rules  we are working to characterize why and for which domains this assumption holds. one way to do this is to observe that associational rules are typically derived either from experience or from domain models. in any reasonably complex domain one is unlikely  in practice  to experience enough specific cases to span a large fraction of the domain. thus  to build a robust generator the bulk of the rule set must be derived from domain models. that is  domain models are a pragmatic precursor to the rules - hence it is more work to construct robust associational rules precisely when experience alone cannot span most of the domain. 
reasoning from models is needed in many domains since it is often difficult to ensure that one's rules cover all situations that could arise. for example  in the geology domain there are infrequently occurring classes of events that might easily be overlooked. for example  figure 1 shows the system's rule about intrusion. figure 
1 shows similar rules  covering infrequently occurring cases in which two or more intrusions coincidentally intrude side by side. these rules are needed to infer that r1 and r1 are pieces of the same formation. the large number of possible  although unlikely  interactions among geologic processes makes it likely that a robust generator would need many more special case rules of this type. 

figure 1. additional  intrusion  scenario patterns 
in some domains  a robust generator might be derived from a formal analysis of the domain. such was the case with dendral where group theory was used to prove completeness  brown & masinter . however  in domains such as geologic interpretation it is not clear what types of topological and geometric analyses are needed or even if such an analysis is tractable. 
another possible method for constructing a robust generator would be to simply reindex the causal models  which map from cause to effect  to form associational rules  which map from effects to cause . the problem  as we will discuss below  is that due to its size the resultant rule set would be no more efficient to use than the causal models themselves. thus  constructing a generator in this way would make it robust but inefficient. 
1 the efficiency provided by associational rules 
our experience has shown that the generator is efficient at producing valid or nearly valid hypotheses. since  as was argued above  most of the associational rules can be derived from the causal models  why is using the using the associational rules so much more efficient than reasoning directly from causal models  
we believe that this efficiency stems from the fact that the associational rules we use encode two important abstractions of the causal domain models - the encapsulation of interactions and the encoding of problem solving knowledge. 
the encapsulation of interactions relates to the assumption  discussed in section 1  that the rules should be nearly independent. a good set of associational rules is one in which each rule covers just enough of the domain so that it can be used nearly independently. for example  the rules in figure 1 encapsulate the interaction that the formation consisting of pieces r1 and r1 was split by the intrusions. if the patterns did not include r1  then invalid hypotheses might be produced since the rules would not encode the fact that r1 and r1 are created by the same process. on the other hand  if the patterns constrained r1 and 
r1 to be horizontal  the rules would be too specific and valid hypotheses might be missed. 
associational rules also typically encode problem solving knowledge. such rules derive from an analysis of both the domain models and the problem at hand. for example  the rule that a  fork  is useful in chess combines knowledge about legal chess moves  the domain model  with an analysis of possible moves and countermoves. rules like these are efficient to use because the generator does not have to repeat the problem solving effort each time the rule is applicable - the problem solving knowledge is  compiled into  the rule. such rules  however  are specific to the problem for which they were designed. a fork  for instance  is specific to the standard chess game and would be useless if one were allowed two moves at a time or if the object of the game were to lose all one's pieces. 
we have described what a good associational rule set is like  but in general it is an open problem how to create such a rule set. it seems that much of the knowledge comes from analyzing how the domain models interact and from practical experience with situations that are likely to occur. this raises the possibility of using the results of debugging an hypothesis to learn associational rules in a manner similar to the work in explanation based learning  e.g.   mitchell  et al.  . we are currently exploring using gordius' tester and debugger to automatically construct rules in the semiconductor fabrication domain;  smith  et al.  has explored using causal models to generate geologic rules for doing dipmeter interpretation. 
1. related work 
the core idea in gtd of  debugging almost right plans  was pioneered by  sussman  in the blocks-world domain. however  sussman's debugger was not very robust since it used only an ad hoc set of debugging rules rather than reasoning from domain models.  goldstein  attempted to construct a taxonomy that related bugs to types of errors made during planning. more recently   rich & waters  and  hammond  have explored paradigms similar to gtd in more complex domains - programming and cooking  respectively. both systems employ a library of  cliches  to generate hypotheses and both reason from causal models to debug hypotheses. our work takes a further step in this direction by extending the range of the paradigm - both in terms of its domain-independence and in the complexity of domains which can be handled. gtd has been used in both planning and interpretation tasks to construct a sequence of events that can achieve a final state from an initial state. the major difference between planning and interpretation is the relative amount of information contained in the final states. in interpretation problems the final state is usually more completely specified since it is a state that has already occurred and is thus  presumably  easier to observe. in planning problems  the initial state is usually more completely specified  especially in comparison with domains such as geologic interpretation where the initial state occurred so long ago that it cannot be specified accurately. 
the planning algorithms used by most implementations of domain-independent planners  e.g.   chapman    sacerdoti  1    tate    wilkins   are similar to our debugging algorithm - a subgoal is chosen  a method is found to achieve the subgoal by reasoning from domain models  and the global effect of the action is computed. this is repeated until all the goals are achieved. our debugger extends this line of research by extending the complexity of models that can be reasoned about. for instance  we represent and reason about quantified effects  conditional effects and effects that create or destroy objects. for example  in geology we need to represent that erosion affects all rocks on the earth's surface  quantification  - either reducing their thickness or eroding them completely away  conditional effects and destruction of objects . 
such planners  and our debugger  all suffer from the problem of potentially exponential search. this problem led to research aimed at solving problems at different levels of abstraction. early work by  sacerdoti  1  showed the utility of abstraction  but the abstraction technique used - mainly removing preconditions of actions - was fairly simplistic.  patil  showed that having multiple levels of vocabulary was a powerful problem solving tool. our work demonstrates the utility of having different representations and inference mechanisms specialized for each level of abstraction  the generator and the debugger . 
an interesting point of comparison to gtd is the generate and test system of dendral  buchanan . buchanan has observed  personal communication  that the test results of dendral were fairly uninformative - knowing that a peak was the wrong height did little to identify which bonds were wrong since many bonds contributed to each peak. this case illustrates a crucial assumption for gtd - that the goals are not tightly interdependent. 
a major open problem with gtd is determining when one is sufficiently  far  from a solution to stop debugging and to ask the generator for a new hypothesis. by running gordius over a wide variety of problems  we hope to detect characteristics of the domain and problem solving strategy that will be useful in determining which stage of the problem solver to use. 
1. conclusions 
this paper has presented gtd  a paradigm for combining associational rules and causal models to 
	simmons and davit 	1 

achieve efficient and robust problem solving behavior. we have also discussed some aspects of gordius  our implementation of gtd  as it relates to solving problems in geologic interpretation. the following list summarizes many of the domain characteristics that we believe are necessary in order for gtd to be a useful paradigm. 
1. goals are not totally independent. if they were  we would not need both the generator and debugger  since with totally independent goals the debugging approach  solving goals independently  would always lead to a valid solution with little or no search needed. since  in this case  the generator would lose its advantage of efficiency it would not be worth the extra effort to build both a generator and a debugger. 
1. goals are not totally interdependent. if they were  one would again lose the efficiency advantage of the generator and so would not need both a generator and debugger. with totally interdependent goals the efficiency of the generator becomes as bad as that of the debugger  since the generator approach  using rules independently  would be likely to produce many invalid hypotheses before finding a correct solution. 
1. nearly independent rules can be identified. gtd requires a set of rules whose right-hand sides can be pieced together independently to form a  nearly  valid hypothesis. it might not be possible to find such a set for a given domain  even if the goals are not totally interdependent. 
1. robust causal models must be representable in a language that the debugger can analyze. although we have developed a representation language suitable for modeling complex domains  it is likely that for other domains our language will need extensions or a different representation altogether will be needed. 
1. experience alone cannot cover most of the domain. if experience can cover the domain then a robust set of associational rules would be easier to construct than robust causal models and the debugger would lose its main advantage of broad range of applicability. 
1. the tester must be correct. in particular  it must not allow false negatives. 
1. the tester must give causal explanations for any bugs found. otherwise  the modifications made by the debugger will be random or empirical. however  the tester does not have to use the same models as the debugger if other methods of constructing explanations are available. 
the strategy behind gtd is to construct hypotheses using nearly independent associational rules but to include reasoning from causal models as a means of analyzing unforeseen interactions. this research has demonstrated the increased performance and competence exhibited by the gtd paradigm compared with using associational rules or causal models alone. 
acknowledgements 
the intellectual history of gtd is long and much 
1 	reasoning 
research has been done to lay the groundwork for this work  especially that of sussman's hacker. we also thank bruce buchanan  dan carnese  tom dean  ken forbus  walter hamscher  john mohammed  chuck 
rich  reid smith and peter szolovits for their help with this research and for comments on this paper. 