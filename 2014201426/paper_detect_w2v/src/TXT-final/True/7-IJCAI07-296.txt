
modeling the dynamicsof cellular processes has recently become a important research area of many disciplines. one of the most important reasons to model a cellular process is to enable highthroughput in-silico experiments that attempt to predict or intervene in the process. these experiments can help accelerate the design of therapies through their cheap replication and alteration. while some techniquesexist for reasoning with cellular processes  few take advantage of the flexible and scalable algorithms popularized in ai research. in this domain where scalability is crucial for feasible application  we apply ai planning based search techniques and demonstrate their advantage over existing enumerative methods.
1 introduction
the cell maintains its functions via various interconnections and regulatory controls among genes and proteins. therefore  it is critical  for understanding the living cell  to unravel how such cellular components as genes and proteins interact with each other. mathematical modeling and computational simulation of cellular systems  especially gene regulatory systems  has been crux of computational systems biology  or biomedical science in general. once a model is constructed  it can be used to predict the behavior of a cellular system under unusual conditions  to identify how a disease might develop  and/or how to intervene such development to prohibit cells from reaching undesirable states. such models can aid biologists by quickly ruling out or confirming simple hypotheses before expensive and time-consuming wet lab experiments.
﹛in this paper  we address the problem of planning to intervene in cellular processes  focusing on gene regulatory networks  grns . a grn describes a cellular process by a set of genes and their regulatory influences over each other. grns focus solely on genes  omitting proteins or other molecules  to model the high level behavior of many genes versus the low level behavior of a much smaller system. because practical grn models are typically learned from micro-array data  kim et al.  1   they are situated at the right level of granularity for automated parametrization. micro-array experiments measure the activity level of thousands of genes from living tissue in terms of mrna concentrations  the products of gene transcription used to code proteins . correlations between observed gene activity levels help describe regulatory influences. predictor functions characterize the regulatory influences and provide a dynamic model  e.g.  when g1 and g1 are highly active  g1 becomes inactive . the grn state models the activity levels of genes and the predictor functions describe possible next states. outside interventions  e.g.  using rna interference to suppress a gene's activity level  alter predictor functions to control the dynamics of the grn  providing a natural planning problem. in a plan  each action models either a possible intervention or non-intervention that will change the state of the grn.
﹛there are a number of grn features that affect our choice of ai planning model. first  intervention plans need only focus on horizons long enough to ensure the grn will naturally transition to nominal states. biological knowledge and computational simulation of grns  kim et al.  1  tell us that cellular processes  left to their own  transition to  or through  stable attractor states. these states represent common cellular phenomena such as the cell cycle  division  etc. however  some states are consistent with disease  such as the metastasis of cancer. from abnormal states  planning interventions provides a method to push the evolution of the cell toward nominal attractor states. second  the cell has an inherently hidden state because full observationsare prohibitivelycostly and we have limited accessability  datta et al.  1 . planning with partial observability is important because biologists analyzing cellular processes cannot be expected to understand  nor obtain complete state information. third  cellular processes are commonly viewed as stochastic  rao et al.  1 . genes are typically regulated many different ways  meaning grns must allow for the probabilistic selection of predictor function for each gene. because state transitions are stochastic  provide only partial observations  and are only relevant over a limited planning horizon  we formulate our model of grn intervention as decision theoretic planning in a finite horizon partially observable markov decision process  pomdp .
﹛this work is not the first to address the problem of planning interventions for grns  but it is the first to apply ai planning techniques. existing research  datta et al.  1  enumerates all possible plans and then uses a dynamic programming algorithm to find the optimal finite horizon plan. our planner is based on the ao* algorithm  nilsson  1   which uses upper bounds on plan quality  reward  for pruning. we evaluate our planner on several formulationsof a random grn with different reward functions to see the benefit pruning. we also evaluate on the wnt1a grn  weeraratna et al.  1   which  datta et al.  1  use for intervention planning. wnt1a is a gene that plays a significant role in the development of melanoma and is known to induce the metastasis of melanoma when highly active. on the wnt1a problemsstudied by  datta et al.  1   ourplannerperforms significantly better than the enumeration algorithm of  datta et al.  1 . our study formalizes this interesting planning domain as a finite horizon pomdp and shows the gains of applying relatively standard ai techniques.
﹛we start by describing a simple example of planning interventions in a grn. we then briefly define the components of a finite horizon pomdp and a grn intervention problem. with these definitions  we map the intervention problem to the pomdp. the solution to the pomdp is a conditional plan  for which we define the semantics and provide two solution algorithms - ao* and enumeration  datta et al.  1 . we compare both algorithms on several grns  either randomly generated or learned from actual micro-array experiments. we end with related work  a conclusion  and discussion of future work.
1 intervention planning example
to clarify intervention planning  consider a small two gene network where each gene g is either active  g  or inactive
  g . there are two predictor functions {fg1 fg1} that describe the grn dynamics  and an intervention to suppress g1 described by the predictor function. using the predictor functions  we can define the following two actions:
actionno interventiong1 interventionreward1-1effectsfg1 : g1 ↙  g1   g1 ↙ g1fg1 : g1 ↙  g1   g1 ↙ g1 predictors fg1 :  g1 ↙  g1  g1 ↙ g1observationg1 /  g1g1 /  g1intervening to suppress g1 incurs a reward of -1 by rewriting the predictor function fg1 with. we also assume that we can observe g1 but not g1. the goal of the intervention problem associates a reward of 1 with activating gene g1. while the actions are deterministic  for the sake of example simplicity   we can describe stochastic actions by allowing a probabilistic choice of predictor function for each gene.
﹛assume that we start with the initial belief state where each grn state is equally likely  as depicted in figure 1. the figure shows a horizon three plan to achieve the goal of activating g1. the first action in the plan is to not intervene; we will let the genes affect each other  and then observe g1. not intervening leads to the belief state {1{ g1 g1} 1{g1  g1}}. using the observation of whether g1 or  g1 holds  the plan branches to belief states consistent with the appropriate observation. since we do not know which value of g1 will occur  we plan for both branches. in the first branch  we apply our g1 intervention action to suppress g1. in the following step  we do not intervene and having g1 inactive will activate g1  leading to a state satisfying the goal. in the second branch  we can apply the non intervention action indefinitely because the goal is satisfied and will remain satisfied.

figure 1: example plan.
﹛the quality of the plan is the expected reward of all possible plan branches. the first branch  taken with 1 probability has a reward of 1  because we had to intervene   and the second branch  taken with 1 probability has a reward of 1. thus  the total expected reward is 1.
1 decision theoretic planning
this section describes the basics of the finite horizon pomdp model  formally defines intervention planning  and details the formulation of intervention planning in the pomdp.
pomdp model: unlike most work on pomdps  that find a policy over the belief space  we find a policy  conditional plan  rooted at an initial situation. the finite horizon pomdp problem p is defined as the tuple   where s is a set of states  a is a set of actions  t : s ℅ a﹍ {﹠}℅s﹍{﹠} ↙  1  is a transition function  r : s ℅a﹍ {﹠}℅s ﹍{﹠} ↙ r is the transition reward function  次 is a set of observations  o : s℅a℅次 ↙  1  is the observation function  h is the planning horizon  and bi : s ↙  1  is the initial belief state. we overload the symbol ﹠ to denote both the terminal action and state signifying the end of the plan.
intervention planning problem: given our definition of the pomdp model  in the following we show how an intervention planning problem maps to the pomdp. the intervention problem is defined by the tuple  where g is a set of genes  dom is the set of activity levels for genes  f is a set of predictor functions  x is a set of interventions  w is an initial situation  y is a goal description  o is a set of observations  and h is the horizon. the genes and their activity levels describe states of the pomdp  the predictor functions and interventions describe actions  interventions and the goal description define the reward function  and the initial situation  observations  and horizon map directly to their pomdp counterparts.
states: each gene g ﹋ g has an activity level from the domain dom of values  of which we will only illustrate boolean domains {g  g}  active or inactive. a state s : g ↙ dom of the gene network maps each gene to a value d ﹋ dom. the entire set of grn states defines the pomdp states s.
predictorfunctions and interventions: given a state of the gene network  the predictor functionsf describe states reachable after one step. interventions re-write predictor functions in f for specific genes to ensure the gene network transitions to specific states. thus  each possible action in the pomdp is described by a set of predictor functions. a non intervention simply uses f to describe the action  but an intervention action x ﹋ x replaces predictor functions in f to get a new set fx. each intervention x ﹋ x is a set of predictors {fg1 fg1 ...}  allowing us to define
.
﹛each predictor function fg is defined as the mapping fg : from activity levels of genes in to the activity level of gene g. the interventions described in this work contain a single predictorfunctionfg where  meaning that irrespectiveof the state g has its activity level set deterministically.
﹛since each gene may be predicted by several predictor functions  where each state transition selects one probabilistically  we assign a weight w fg  to each. while we assume the predictor functions are given  we can learn them from micro-array experiments. in the empirical analysis we evaluate grns where the predictor functions and weights are both generated randomly and from real micro-array experiments.
﹛each action af defined by the set of predictor functions f  similarly fx  describes the transition function probability
	p	w f
.
observations: after each step  whether by intervention or non intervention  we can observe the state of the gene network. the set o   g defines which genes are observable  by genetic markers  physiology  etc. . the set of observations 次 = {o|o ﹋ dom|o|} is defined by all joint activity levels of genes in o. in this work  we assume that observations are perfect and the same for each action  meaning that if a state s and observation o agree on the activity level of each gene  then the probability of the observation is one  i.e.  o s a o  = 1   otherwise zero  i.e.  o s a o  = 1 . observations can be noisy  i.e.  1 ≒ o s a o  ≒ 1  in general.
rewards: the goal y is a function describing desirable states. we assume that the goal maps states to real values y : dom|g| ↙ r. the reward function for terminal actions and goal states is defined by the goal r s ﹠ ﹠  = y  s . we assume that the reward associated with actions is -1 for intervention actions  i.e.   and 1 for non intervention  i.e. 
initial situation: the initial situation w is a distribution over grn states w : dom|g| ↙  1 . this mapping to the pomdp initial situation is straight-forward  bi s  = w s .
﹛since the formulation has a distinct initial belief state  we do not explore more traditional pomdp value or policy iteration techniques for computing a finite horizon policy. rather  we use the knowledge of the initial belief state to guide expansion of a conditional plan. by searching forward from the initial belief state  it is possible to focus plan construction on reachable belief states.
1 search
this section describes our approach to solving a finite horizon pomdp. we start by defining the semantics of conditional plans and our search space. we follow with two search algorithms to find plans. the first algorithm is ao*  nilsson  1   and the second datta is based on a competing approach  datta et al.  1  from the grn literature.
 conditional plans: a solution to the problem p is a conditional plan p of horizon h  described by a partial function p : v ↙ a ﹍ {﹠} over the belief state space graph g =  v e . a subset of the vertices b ﹋ v  which are belief states  are mapped to a  best  action a  denoted p b  = a. each edge e ﹋ e directed from b to boa is mapped to an action a ﹋ a and an observation o ﹋ 次  and denoted e b boa  =  a o . if p b  = a and e b boa  =  a o   then  is the action to execute after executing a and receiving observation o. throughout our discussion  we assume that the horizon is a feature of every state to ensure that the graph g is acyclic. belief states where the horizon is equal to h have a single available action ﹠ to signify the end of the plan  leading to a terminal ﹠.
﹛if p b  = a  and there exists an edge then the successor belief state boa is defined
 
 where 汐 is a normalization constant. if for all    because no observation is consistent with the be-
lief state ba  then the belief state is not added to the graph.
﹛the expected reward q a b  of a plan that starts with action a at belief state b is the sum of current and future rewards:

where the expected reward for a belief state is v b  = maxa﹋a q a b . terminal vertices are assigned the expected goal reward
.
ao* algorithm: we solve the finite horizon pomdp problem with ao* search  nilsson  1  in the space of belief states. the ao* algorithm  listed in figure 1  takes the planning problem as input  and iteratively constructs the belief space graph g rooted at bi. the algorithm involves three iterated steps: expand the current plan with the expandplan routine  line 1   collect the ancestors z' of new vertices z  line 1   and compute the current best partial plan  line 1 . the algorithm ends when it expands no new vertices. in the following we briefly describe the sub-routines used by ao*.
﹛the expandplan routine recursively walks the current plan to find unexpanded vertices  lines 1 . upon finding a vertex to expand  it generates all successors of the vertex  lines 1 . generating successors involves assigning the ﹠ action if the vertex it is at the max horizon  line 1  or constructing the vertices reached by all action and observation combinations  lines 1 . notice that each vertex has its value initialized with an upper bound 
 
ao* p 
1: expanded bi  = false
1: repeat
1:	z = expandplan bi  1 
1:	z' = addancestors z 
1: update z' 
1: until |z| = 1
expandplan b  hzn 
1: if expanded b  then
1:	fordo
1:	z' = expandplan bop b   hzn+1 
1:	z = z ﹍ z'
1:	end for
1: else
1:	expanded b  = true
1:	z = z ﹍ {b}
1:	if hzn == h then
1:	e = e ﹍ {e b ﹠  =  ﹠ ﹠ }
1:	else
	for	do
1:
1:	e = e ﹍ {e b boa  =  a o }
1:	expanded boa  = false
1:	v boa  = rmax
1:	end for
1:	end if
1: end if
1: return zfigure 1: ao* search algorithm.
on its expected reward. the upper bound plays a role in pruning vertices from consideration in the search.
﹛after expanding the current plan  expandplan returns the set of expanded vertices z. in order for update  figure 1  to find the best plan  given the new vertices  addancestors adds to z' every ancestor vertex of a vertex in z. the resulting set of vertices consists of every vertex whose value  and best action  can change after update. the update routine iteratively removes vertices from z that have no descendent in z and calls backup until no vertices remain in z. the backup routine computes the value of a vertex and sets its best action. the reason update chooses vertices with no descendent in z is to ensure each vertex has its value updated with the updated values of its children.
﹛ao* can often avoid computing the entire belief space graph g  leading to significant savings in problems with large horizons. by initializing vertices with an upper bound on their value it is possible to ignore vertices that have a consistently lowest upper bound. for example in backup  if there exists an action whose q-value is always greater than the alternative actions  then the best action will never be set to one of the alternatives. further  because the alternative actions are neverconsideredbest  expandsolutionwill neverexpand them. as we will explore in the empirical evaluation  the reward function has a significant effect on the number of vertex expansions. in the worst case  it is possible to expand
addancestors z 
1: z' = z
1: while  b s.t. e b bop b   ﹋ e and bop b  ﹋ z do
1:	z' = z' ﹍b
1: end while
1: return z'
update z  1: while |z|   1 do
1:	remove b ﹋ z s.t.z where
1: backup b 
1: end while
backup b 
1: for all a ﹋ a ﹍ {﹠} do
1:	compute q a b 
1: end for
1: v b  =	max	q a b 
a﹋a﹍{﹠}
1: p b  = argmax q a b 
a﹋a﹍{﹠}figure 1: ao* subroutines.
datta p 
1: expandpland bi  1 
1: update v 
expandpland b  hzn 
1: if hzn == h then
1:	e = e ﹍ {e b ﹠  =  ﹠ ﹠ }
1: else
1:	for a ﹋ a o ﹋ 次 do
1:	v	v ﹍ {bo}
1:
1:	expandpland boa  hzn+1 
1:	end for
1: end if       figure 1: datta enumeration algorithm. the entire graph g  as would datta.
enumeration algorithm: in order to compare our planner to the work of  datta et al.  1   we provide a description of their algorithm  we call datta  in figure 1. unlike the iterative ao*  datta consists of two steps: expand g with expandpland  and then update each vertex v ﹋ v with update. the expandpland routine recursively expands g by either reaching a terminal vertex at the horizon  line 1   or generating and recursing on each child of a vertex  lines 1-
1 . following expandpland  update computes the best action for each vertex in v .

figure 1: number of expanded vertices for random grn  left   wnt1a intervention  center   pirin intervention  right .

figure 1: total planning time s  for random grn  left   wnt1a intervention  center   pirin intervention  right .﹛unlike ao*  datta is unable to prune vertices from expansion  making it insensitive to the reward function. while the result of the two algorithms is identical  the time and space required can be very different. we implemented both algorithms within our planner and demonstrate their effectiveness on the grn intervention planning problems. we also implemented a straight-forward version of the  datta et al.  1  algorithm that does not make use of several efficiency improvements within the planner  such as using adds  bryant  1  for compact action and belief state representation  as well as duplicate belief state detection.
1 empirical evaluation
to test the feasibility of using our planner to solve grn intervention problems we experimentwith a randomgrn and the wnt1a grn  weeraratna et al.  1; datta et al.  1 . the following table summarizes the features of the grns:
|g||dom||f||x||o|random111wnt1a111both networks use seven genes  each with two activity levels   two predictor functions per gene  each with two genes as predictors. the predictor functions were selected randomly in the random grn and learned from micro-array data  measurements of mrna concentrations in a cell that indicate gene activity levels  in the wnt1a grn. in order to learn predictor functions  we use a coefficient of determination  cod  statistic to measure the strength of correlation between predictor and target genes from normalized and discretized data  kim et al.  1 . we use the two predictor functions with highest cod for each gene  setting their weight equal to the cod.
﹛within both grns we study several intervention problems. in the random grn  we vary the goal w to assign different rewards to terminal states  while assigning interventions a negative reward of one. this illustrates the ability of ao* to prune the search space in comparison with enumeration. in the wnt1a grn  we reproduce two intervention problems studied by  datta et al.  1 . the first directly intervenes to suppress wnt1a  which happens to be the goal  and observes the pirin gene. the second attempts to indirectly control wnt1a by pirin  a predictor gene of wnt1a  intervention. both wnt1a problems use the same reward function  assigning interventionsa negativereward of one and the goal  activating wnt1a  a negative reward of three. we use negative reward for the goal to maintain consistency with the  datta et al.  1  model. thus plans avoid activating wnt1a  which is equivalent to pursuing suppression. both wnt1a networks use the initial belief state where each gene is set to an activity level with probability proportional to its observed frequency in the data.
﹛our planner is implemented in c++ and ran on a 1ghz p1 linux machine with 1gb of ram. each experiment was given a twenty minute time limit. for our planner binary and gene regulatory network encodings  several of which we did not have space to describe  please visit: http://verde.eas.asu.edu/grn.
random grn: the leftmost plot in figure 1 depicts the number of expanded vertices  including terminal actions  in ao*  datta  and the maximum possible  max . the results for ao* are indexed by a number indicating the reward associated with the goal  since datta is insensitive to reward. max represents the number of vertices expanded in a search tree  versus a graph   similar to the original implementation of  datta et al.  1 . because we implemented the datta algorithm in our planner  like ao* it finds duplicate belief states. without duplicate detection  datta would expand as many vertices as max. the leftmost plot in figure 1 shows the associated total planning time  which is proportional to the number of expanded vertices . missing bars indicate the instance was cut off by reaching the 1 minute time limit.
﹛we notice several important points in these results. ao* is sensitive to the goal reward function  expanding much fewer vertices than datta in some cases. despite ao* using dynamic programming over its partial solution many times  it never takes more time than datta. as a result  when it is able to prune vertices  ao* scales much better.
wnt1a grn: the center plots in figures 1 and 1 show results in the wnt1a grn intervening wnt1a and observing pirin. here ao* greatly outperforms datta. the difference is partly due to the fact that ao* quickly recognizes that the optimal plan intervenes once at the end of each plan branch where wnt1a is not already deactivated. we also directly implemented the approach of  datta et al.  1   which does no duplicate detection  and it was able to solve this problem to a horizon of ten  using significantly more memory and time. while our implementation of datta is limited by time  the direct implementationis limited by space  exceeding 1 gb memory past horizon ten. the rightmost plots in figures 1 and 1 show results in the wnt1a grn intervening pirin and observing wnt1a. finding plans in this problem requires more search  but ao* can still prune.
discussion: we have shown that ao* is a viable approach to solving grn intervention problems. where the datta algorithm quickly exceeds our time limit as the horizon increases  ao* is sensitive to reward functions and can scale to larger horizons. using both artificial and existing grns of practical interest  we have seen that ao* performs well by pruning vertices based on upper bounds. perhaps the next step is to achieve tighter upper bounds through heuristics. while we did not discuss alternative grn formulations  e.g.  with more genes  activity levels  and observations   we have studied such problems and see similar scalability.
1 related work
planning in cellular processes is a relatively new area of research  with some initial work on problem formulation as well as solution algorithms. there also exists some preliminary works on representing cellular processes without specific emphasis on planning interventions. the closest body of work for planning interventions  datta et al.  1  models the problem as a finite horizon control problem.
﹛some recent works in the ai community have focussed on simply representing cellular processes. one approach  discovers signal transduction pathways with a deterministic classical planner  khan et al.  1 . further along this vein   tran and baral  1  model change in cellular processes as exogenous actions  termed triggers.
1 conclusion & future work
we have presented a formulation of grn intervention as decision theoretic planning. our planner relies on several advances in ai planning to perform efficient reasoning. as a result  we have improved the scalability of planning interventions in grns over previous work.
﹛in the future  we would like to decouple the representation of cellular process dynamics from our intervention actions. we think the right approach is to use exogenous actions or processes. while recent work in deterministic planning  mcdermott  1  has discussed models for controlling exogenousprocesses and triggers tran and baral  1   less thought has been given to such processes in a probabilistic setting  with the exception of  blythe  1  . alternatively  research in simulating cellular processes  ramsey et al.  1  has considered probabilistic exogenous processes  but not under the control of a planner.
acknowledgements: this work was supported in part by nsf grant iis-1  by onr grant n1  the arcs foundation  and an ibm faculty award. we thank ashish choudhary  edward dougherty  william cushing  and subbarao kambhampati for helpful discussions.
