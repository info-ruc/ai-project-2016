 
     this paper presents a formal  foundational approach to learning from examples in machine learning. it is assumed that a learning system is presented with a stream of facts describing a 
     domain of application. the task of the system is to form and modify hypotheses characterising the relations in the domain  based on this information. presumably the set of hypotheses that may be so formed will require continual revision as further information is received. 
     the emphasis in this paper is to characterise those hypotheses that may potentially be formed  rather than to specify the subset of the hypotheses that  for whatever reason  should be held. to this end. formal systems are derived from which the set of potential hypotheses that may be formed is precisely specified. a procedure is also derived for restoring the consistency of a set of hypotheses after conflicting evidence is encountered. in addition  this work is extended to where a learning system may be  told  arbitrary sentences concerning a 
domain the approach is intended to provide a basic framework lor the development of systems that learn from examples  as well as a neutral point from which such systems may be viewed and compared. 
1. introduction 
     learning from examples is an important yet basic subarea of machine learning. for this approach  a learning system receives information concerning a domain of application in the form of facts  or ground atomic formulae. on the basis of this information the learning system induces general statements characterising the domain  and hence hypothesises relations among the known relations in the domain. these hypotheses are phrased independently of any particular individuals. further facts may enable other hypotheses to be formed while falsifying existing hypotheses. thus the consistency of the set of hypothesised statements must continually be maintained as new information is discovered  and the question arises as to how a set of hypotheses may be modified as falsifying instances are encountered. 
     the early work of patrick winston  winston 1  provides a good example of such an approach. in winston's system  descriptions of concepts are formed from a set of carefully chosen examples of the concept and  near misses . a near miss is an example that is quite similar to an instance of the concept. but differs in a small number of significant details. relevant features that the concept  presumably  must have are extracted from the positive examples  while negative information is extracted from the near misses. thus from positive examples the program might infer that an arch must have two supports  while from a negative example it might infer that the supports must not be touching. as examples are received  the definition of a concept passes through successive refinements  presumably converging to some acceptable definition. 
     in this paper a formal  foundational approach to learning from examples is presented. the overall aim is to investigate the underlying formal aspects of such learning. in contrast to previous work  pragmatic concerns dealing with notions of evidence. confirmation and justification of hypotheses are ignored insofar as is possible. the goal of this paper then is to characterise the hypotheses that may potentially be formed. the question of what hypotheses may justifiably be formed is not addressed. thus for example if we have a set of black ravens and know of no non-black ravens  we could hypothesise that ravens are black. however the approach at hand gives no indication as to when such a hypothesis should be formed or what constitutes adequate evidence for such an assertion. so the goal is to determine formal criteria which prescribe the set of potential conjectures  rather than to determine pragmatic criteria whereby an acceptable set of conjectures may be formed. a similar distinction can be made in a deductive system  where an underlying logic specifies what could be derived  but not what should be derived. 
     the remainder of this section expands on these ideas and surveys related work. in the second section  a language for expressing conjectures is introduced  and formal systems are developed for guiding the formation of conjectures. these systems lead immediately to a procedure for restoring the consistency of a set of conjectures extensions to the approach are briefly described in the third and fourth sections  and in the fifth section the approach is compared with representative ai systems for learning from examples. further details  proofs of theorems  etc. may be found in  delgrande 1 . 
1. the approach 
     the domain of application is assumed to be describable as a 
     collection of individuals and relations on these individuals. further  it is assumed that some portion of the domain  described by a finite set of ground atomic formulae  or. informally   facts    is known by the learning system. as time progresses the learning system' will presumably encounter new information  and so the set of known ground atomic formulae will monotonically increase. initially i assume that this  increasing  set of ground formulae is all that is known about the domain. 
     hypotheses are proposed and modified on the basis of this finite  monotonically increasing set of ground instances. the hypotheses are expressed in a language. hl. that is a simple variant of the language of elementary algebra. the criteria for proposing a hypothesis are straightforward: there is a reason to do so  i.e. some notion of evidence is satisfied  and the hypothesis is not known to be false. these criteria though are far too simplistic  and in general the resultant set of hypotheses will be inconsistent. 
	delgrande 	1 
¡¡¡¡these difficulties are circumvented in the following manner. with each term in a sentence of hl we can associate two subsets of the ground instances  consisting of those known to satisfy the term and those known to not. thus to the term  black raven  we can associate the set of individuals known to be black and a raven  and the set known to be either non-black or non-raven. these  pairs of  sets interrelate in various ways: formal systems are developed to precisely characterise relations between terms in hl by means of these sets. from these systems  ground instances whose truth values are unknown can be iteratively located so that determining their truth value leads to a convergence of the hypothesis set to consistency. these  knowable but unknown ground instances  arc composed of constants and predicates symbols that occur in the set of known ground atomic formulae. informally they correspond to unknown but potentially knowable  facts  in the domain. the capability of testing individuals for membership in a relation  where both individuals and relation have been encountered in the set of known ground atomic formulae  proves essential for restoring 
consistency in a set of hypotheses. 
¡¡¡¡the approach is clearly a restriction of the general problem of induction. however  induction  as such  plays a relatively minor role; it is used to suggest an initial  and usually incon-
sistent  set of hypotheses  which then are modified using strictly deductive techniques. the set of hypotheses that may be formed is shown to be perhaps surprisingly general and in fact  with respect to expressiveness  subsumes a number of existent systems for learning from examples. 
in summary  i initially assume that: 
1. the domain is describable as a set of ground atomic formulae. 
1. some finite subset of the ground atomic formulae is known; 
1. the set of known ground atomic formulae is correct and error-free; 
1. the set of known ground atomic formulae grows monotonically with time: 
1. known individuals may be tested for membership in a known relation. 
¡¡¡¡the first assumption is somewhat restrictive  and in section 1 is relaxed so that a learning system may be  told  arbitrary sentences. also in the third section  the second assumption is relaxed to allow relations in the domain whose membership is completely known. 
¡¡¡¡the third assumption is clearly unrealistic for any practical learning system. however  arguably the issue of how to deal with erroneous data is a pragmatic one. and is not relevant to our concerns here. consider  as illustration  where we have some conjecture  say.  ravens are black   and encounter an albino. if we don't want to totally abandon our original hypothesis  then there seems to be two ways we can discharge the exception. first  we could amend the conjecture to something like  normally ravens are black   and perhaps also introduce  normally albino ravens are white . that is. one way or another  the exception is  excused . second we could determine  or simply declare  that the observation is erroneous. however  this procedure of determining that an observation is incorrect  or otherwise excusing it. is a pragmatic concern  and is distinct from our concern of what hypotheses  follow  potentially from a given set of ground atomic formulae. 
1. related work 
¡¡¡¡there has been much work in ai addressing the problem of learning from examples  including  brown 1 .  buntine 1    hayes-roth 1 .  mitchell 1 .  shapiro 1 .  solway and riseman 1 .  vere 1  and  winston 1 .  michalski 1  is a particularly detailed approach to learning from examples. an extensive survey of ai learning systems is given in  dietterich et el 1 . while  smith et al 1  describes a proposed model for learn ing from examples and  dietterich and michalski 1  compares four particular generalisation programs. typically such work is concerned with proposing and refining a description of a concept  and many of the above approaches detail particular rules or strategies for forming a general concept from a set of instances. most of this work also assumes that the only information regarding a domain is in the form of examples  or instances. in contrast  the work at hand deals with characterising the hypotheses formable under a set of  arguably  minimal assumptions and hence is concerned with exploring intrinsic properties and limitations of such approaches. in addition  an extension of learning from examples to include learning by being told is addressed. 
¡¡¡¡the work presented in  morik 1  employs assumptions similar to those used here  in that very little is assumed about the domain of application. however  in this case the author is interested in the problem of acquiring an initial model of a domain  but presupposes an agent and learning algorithms to help in this initial investigation. thus this work addresses the first step of a number of steps in a technique-specific approach to learning. 
¡¡¡¡most formal approaches to learning from examples have been concerned with inducing instances of a given type of formal language. the area of learning theory  gold 1  studies systems that implement functions from evidential states to languages. a survey of such approaches is presented in  angluin and smith 1   while  osherson et el 1  gives recent results in this area. the key difference between such approaches and the present work is that no underlying formal grammar is assumed here  
beyond that for elementary set theory. 
1. introducing conjectural information 
1. initial considerations 
     the domain of application is assumed to be describable as a presumably infinite set of ground atomic formulae  formed from presumably infinite sets of individuals and predicates. at any point in time  the truth values of some subset of the ground atomic formulae are assumed to be known. given a particular predicate then  all that can be known of it is a subset of those individuals  or tuples  which satisfy it and a subset of those individuals which do not. a predicate will be referred to as known if its truth value on a given individual  tuple  is known or can be determined. an individual will be referred to as known  if it is known to be or not be part of the extension of a 
     known predicate. informally  a known individual or predicate is one  encountered  by a learning system. the sets of tuples known to belong to the extension of a predicate and known to not belong to the extension are referred to as the known extension and the known antiextension respectively. so for a known n-place predicate p and known individuals a   . . .  an there are three possibilities: 
1. piax  . . .   a   is known to be true. 
1. -p a1 . . .   an  is known to be true. 
1. neither 	p a1 a   nor 	-p a1 	 an  are known to be true. 
definition: for each known predicate symbol p define sets p+ and p  by: 
	p+ = { a1 . . .  an  | 	p a1 	a   is known to be true . 
p  = { a1 a   | 	-p a1 	an  is known to be true}. 

1 	knowledge acquisition 

delgrande 	1 for a set of known individuals i. the pair of elements in an ele- h ment of h corresponds to a possible known extension/antiextension pair. upper and lower bounds of h are given by: 
then we can locate individuals and predicates that will  via our criteria for evidence  allow the conclusion aqhd to be hypothesised  or else will refute one of the premisses. 
the proof of the theorem is constructive  and leads 

1. restoring the consistency of hypotheses 
¡¡¡¡this section examines how the consistency of a set of conjectures can be enforced and maintained in the face of conflicting ground instances. the maintenance of consistency relies of the relationship between the naive criteria for forming conjectures in hl and conditions for equality of expressions of hla. consider for example the criteria for hypothesised equality: 

and contrast it with strict equality: 

for hypothesised equality we are not guaranteed  for example  transitivity of equality. so if we have p=hq and q=hr we are not guaranteed  for strict equality we of course have transitivity. 
1 	knowledge acquisition 
o1  . . .  an. or else will refute one of a1  . . .  an. in the third alternative for the theorem  a specific sentence a of hla which implies a is identified. the procedure is linear in the length of the proof of a. since the evidence required for the naive criteria for forming conjectures is drawn from finite sets of individuals and relations  consistency can thus be restored in a set of conjectures by repeatedly applying this procedure. moreover  if we begin solely with a set of ground instances we will  by repeated application of our naive criteria for forming conjectures together with this procedure  arrive at a set of consistent conjectures. 
this resolves the three questions posed at the end of section 
1 first  the procedure derived from the above theorem shows how instances can be located for the restoration of consistency. second  the conjectures to which the procedure can be applied are those that are governed by the postulates of hla. thirdly  then  the conjectures to which the procedure may be applied correspond precisely to the sentences of elementary algebra  including composition  the converse  and the image   except that we do not have a universal complement. moreover  this limitation is unavoidable in this approach  or any approach based on the five assumptions listed in the introduction . however  we do retain involution. de morgans laws  laws concerning universal bounds  and the  kleene  postulate. 
1. hla  boolean algebra  and naive set theory 
¡¡¡¡this section describes an extension to the approach wherein entities in the domain whose extensions are completely known are also considered. so far 1 have assumed that for a particular predicate in the domain  such as red. only part of the extension and part of the complement of the extension can be known. the set of all red things  i.e. the extension of red  cannot wholly be known by a learning system. while this seems reasonable for things such as red  bird  and left of. in other cases it is unreasonable. for example. i may know that a particular suck of blocks is composed of the blocks {s1 sk } in this case we could perhaps name this set slack stack1 and so 
stackl = {s1 sk . the crucial point here  of course  is that all the members of stackl are known  in contrast to the membership of red. which cannot be completely known. sets such as stackl  whose extension is known. i will refer to as reducible. sets such as red. whose extension cannot be wholly known. 1 will refer to as irreducible. reducible sets  clearly  correspond exactly with the familiar notion of  set . in this paper though i will be concerned only with sets with a finite extension. 
so there are two questions of interest: 
how can we formally characterise the irreducible sets  
how do the reducible and irreducible sets interrelate  
these questions are addressed by giving a set of axioms to characterise the set of allowable reducible and irreducible sets. for both reducible and irreducible sets  the axioms developed parallel those in the system of zermelo-fraenkel  fraenkel et al 1 . we obtain: 

¡¡¡¡the axioms for irreducible sets are derived from intuitions similar to those motivating the axioms for the reducible sets. for extensionality  for example  we would want to say that two irreducible sets are equal just when their known extension and antiextension coincide. for the power set axiom  for every irreducible set a we know that the power set of a must contain the power set of a+; moreover  this is all that can be known to be in the power set. on the other hand  any set of elements not contained in a+ cannot be in the power set. and hence are in the antiextension.  delgrande 1  contains an informal discussion and development of the other set axioms. 
¡¡¡¡the axiomatisation then provides a set of constraints that bound the irreducible sets and. moreover  specifies how they may interrelate. as well it provides a primitive basis for defining and justifying the hypothetical operations of the previous section. for example  it is easily shown using the axioms of separation and extensionality that: 
	delgrande 	1 
unchanged. the formal results of the last section still apply 
and. since in the intended model the property of being an individual is decidable  the overall system remains decidable. 
1. learning from examples and learning by being told 
in this section 1 consider where we have a knowledge base 
 kb  that consists of an arbitrary  consistent set of sentences that the system has been  told  are true. we now want to form hypotheses from the ground instances  but also taking these other sentences into account. since we require that the kb be able to reason about knowledge and hypothesis  this part of the problem either requires or presupposes a theory of incomplete knowledge. 1 have taken the latter course  and adopted the theory given in  levesque 1 . this work presents a logical language kl that can refer both to application domains and to what a knowledge base might know about such domains. kl extends first-order logic  fol  by adding a sentential operator k  where ka can be read as  a is known to be true . kl is extended here to a language called hkl that is able to deal also with conjectural sentences. hkl extends kl by the addition of a sentential operator h. where ha can be read as  a is conjectured to be true . using hkl we can express sentences such as  john or bill is hypothesised to be a teacher  or  it is known that mary is hypothesised to be a teacher . hkl is specified as follows: 
axiom schemata 
1. 
1. 1. 
1. 1. 1. 1. 1. 1. 
1. 
rules of inference - modus ponens and universal generalisation 
¡¡¡¡the first five axiom schemata and the rules of inference are those of kl. we obtain that knowledge and conjecture are closed under modus ponens  1. 1  1. 1 . and meta-knowledge is complete and accurate  1 . also if something is known  neither 
it nor its negation is conjecture  1 . and generalisation applies analogously to conjecture and knowledge  1. 1 . what this extension from kl to hkl buys us is a means of distinguishing and reasoning with sentences known to be true  from those that are only hypothesised to be true. in  delgrande 1  an extension of the soundness and completeness results of  levesque 1   with respect to a possible worlds semantics  is provided. 
¡¡¡¡hkl seems to have reasonable properties with respect to reasoning deductively with knowledge and hypothesis. there is a problem however with updating a kb  i.e. with  telling  a kb a new sentence . consider where we have a known portion. kbk  of the kb and we want to form a hypothetical component. kbh   based on the known portion. basically we want to  apply  hl to this kb to produce a hypothesised component. thus for example if kbk is 

then applying hl to what is known about the ground instances 
could yield: 

or the equivalent hypothetical kb in hkl 
1 	knowledge acquisition 

¡¡¡¡the basic idea is that kbk  which is expressed in hkl. determines a set of ground instances and a set of sentences that are representable in hl. by applying the procedure of section 1 for restoring consistency to these sets we obtain a set of hypotheses expressed in hl. if kb =  kbk. kbh.  where initially kb/  = 1 then we have the following procedure for forming a hypothetical component. 
1. let g = |glg is a ground atomic formula and kb*h 
1. apply the procedure of section 1 to g to obtain a 
consistent set of conjectures c expressed in hl. 
1. let kbh  be the translation of the sentences of c into sentences of hkl; exclude any of those provable in kbk 
¡¡¡¡there is generally a straightforward translation of sentences from hl to hkl and. given this  the above procedure can be constructed in a straightforward manner. however it proves to be the case that unless kbk is equivalent to a set of ground instances  this procedure may result in inconsistency. the difficulty is that the procedure of section 1 relies on the existence of knowable ground instances whose truth value is unknown. however in first-order logic  and so in hkl. it is possible to attribute a property to an unknown individual  and this attribution may lead to inconsistency here. for example  consider where all that is known is 

hence  for the corresponding relations in the domain  either p=q or r=s. if we apply the procedure for restoring consistency to the known ground instances and sentences that can be expressed in hl. we obtain 

which  of course  when translated into hkl. is inconsistent with the original sentences. 
     while this last result appears somewhat limiting  things in practice may not be too bad. several considerations are relevant. first  the assumptions underlying hl are those that presumably underlie any system that learns from examples. hence the problems addressed here arguably are the problems that must be addressed by any system that learns from examples  or else must be discharged by means of a priori decisions by the system designers. second  while applying the procedure to a general kb may lead to inconsistency  it need not necessarily do so. if it does  it may be possible that pragmatic considerations can be used to resolve or skirt a particular inconsistency. 
1. comparison with learning systems 
     this section compares the present approach with related work on learning from examples. three systems are particularly relevant and serve to place the present work within the field. the early work of john seely brown  brown 1  on automatic theory formation is a direct precursor to mine. patrick winstons dissertation  winston 1  on learning structural descriptions from examples is a well-known early ai learning system and serves as a good representative of approaches to learning from examples. ehud shapiro's work  shapiro 1  is similar to mine in broad outline  except that the author makes substantial assumptions  concerning how the domain of application is described. 
¡¡¡¡the task of browns system is to propose definitions for a set of binary relations based on knowledge of the extensions of the relations. the system begins with a set of binary relations 

r = {r1 rn }and a database containing the complete extension for each r1€ r. hence there is no notion of modifying a definition in light of later knowledge. a procedure is given for proposing definitions of the relations. however the body of a 
definition is restricted to be the disjunction of compositions of relations. this format though is adequate for a variety of domains  including that of kinship relations. the system is heuristic and was intended for direct implementation. thus it dealt with matters such as efficiently searching for possible definitions  proposing definitions in a  simplest first  manner  etc. no analysis is carried out with regard to what may be conjectured  nor is an algorithmic analysis of the system given. in contrast 1 have not addressed implementation issues  but rather have attempted to address general problems of hypothesis formation  and thus issues dealing with characterising a set of conjectures and maintaining the consistency of a set of conjectures. 
¡¡¡¡winston's work was briefly described in the introduction. basically winston is concerned with the pragmatic aspects of a learning system  and concentrates on techniques to speed the learning process. in some sense then his approach is complementary to the one taken here. the system of section 1 subsumes the conjectures that may be formed in winston's system: thus anything that can be formed in his system can also be conjectured in hl.  winston actually gives a semantic net representation for his concepts. this representation however is clearly equivalent to a set of binary relations  and is useful mainly as a notational or implementational device.  however  in hl the set of formable conjectures and the means of restoring consistency are precisely laid out whereas winston does not address these issues. 
¡¡¡¡shapiro's work is. superficially  the most similar to that presented here. shapiro assumes that a domain is described by a stream of ground instances; based on the ground instances  a set of conjectured axioms for the domain is proposed and refined. a general  incremental algorithm for proposing a set of rules which imply the known ground instances is developed. the algorithm has tuneable parameters that determine the complexity of the structure of a hypothesis. the key difference between shapiro's work and the work at hand is that shapiro makes substantial assumptions about the way the domain is described. in particular  the domain is assumed to be describable by a set of rules in the form of restricted horn clauses in addition the user is given some control over the form of the hypotheses. these assumptions allow an elegant algorithm for inducing rules for a wide class of problems to be derived. in the approach at hand  in contrast  the emphasis is on what may potentially be formed  rather than what can efficiently be induced. 
1. conclusion 
¡¡¡¡this work develops a formal  unified  and general  but basic  framework for investigating learning from examples. a primary goal was to keep the approach as general as possible and independent of any particular domain  representation scheme  or 
set of learning techniques. hence  for example  there is no restriction placed on the ordering of the ground instances nor is there any assumption that the input examples have been already aggregated into complex entities such as arches. neither is there any restriction with regard to introducing new   known   predicate names during the learning process. also  no agent is assumed to exist  to help direct or focus the acquisition process. 
¡¡¡¡presumably the issues addressed here are common to  and are relevant to. any system for learning from examples  at least any system that satisfies the five assumptions given in the introduction . hence the framework may be appropriate as both a basis for the development of systems that learn from examples  and perhaps as a neutral point from which such systems may be viewed and compared. however only a set of formal issues have been addressed  and the concern has been with what conjectures may potentially be formed  rather than with which of those conjectures should in fact be held. pragmatic issues concerned with the justification of conjectures  strength of evidence  and degrees of confirmation  to name just a few. are outside the scope of this work. 
¡¡¡¡formal systems are developed for introducing and maintaining the consistency of conjectures. an exact specification of what conjectures may potentially be formed is provided  and it is shown how the consistency of a set of conjectures can be restored in the face of conflicting instances. the system illustrates that a reasonably rich and expressive set of conjectures can be derived using only a minimal set of assumptions. two extensions to the system are described. first the system is augmented to allow relations in the domain whose extension is completely known; to this end an axiomatisation of the  so-called  reducible and irreducible sets is provided. also addressed is learning from examples  but where the system may be told arbitrary sentences in addition to the ground instances. the first extension does not affect the formal results previously obtained; the second is limiting  in that it may give rise to problems with inconsistency that have to be resolved by pragmatic means. 
¡¡¡¡the expressiveness of the system is indicated by the fact that it is as at least as general as a number of existing systems  including  brown 1 .  hayes-roth 1 .  vere 1 . and  winston 1 . results concerning decidability lend credence to the possibility that learning systems based directly on this approach and. in particular  incorporating the procedure for restoring consistency  may be efficiently implementable. in addition  given the generality of the approach  it is possible that the framework could also provide an appropriate starting point for an investigation of other types of learning systems. that is. the approach could conceivably be extended by incorporating further assumptions concerning the domain  underlying representation scheme  or an agent to assist in the learning. 
¡¡¡¡the approach as it stands may have immediate practical applications. as a specific example  database systems often use integrity constraints to partly maintain consistency and reliability. however  given a large number of relations  it is an arduous task to specify all integrity constraints and to ensure that the set is consistent. the approach then seems suited to the task of automatically proposing and verifying such constraints. this possibility is explored in  delgrande 1 . 
acknowledgement 
¡¡¡¡this paper is based on my doctoral dissertation in the department of computer science at the university of toronto. 1 would like to thank my supervisor. john mylopoulos. for his guidance  as well as graeme hirst. david israel  and hector levesque. financial assistance from the province of ontario and the department of computer science. university of toronto  is gratefully acknowledged. 
bibliography 
 l  d. angluin and c.h. smith   a survey of inductive inference: theory and methods . computing surveys  1  september 1 university. 1 
 j.s. brown   steps toward automatic theory formation . proceedings of the third international conference on artificial intelligence  stanford. ca.. 1. pp 1 
 j.p. delgrande.  a foundational approach to conjecture and knowledge . ph.d. thesis. technical report csri-1. department of computer science. university of toronto. 
september 1 
	delgrande 	1 

 j.p. delgrande.  formal bounds on the automatic generation and maintenance of integrity constraints   1th acm symposium on principles of database systems. march 1 
 t.g. dietterich. b. london. k. clarkson and g. dromey. 
 learning and inductive inference . the handbook of artificial intelligence  p.r. cohen and e.a. feigenbaum 
 eds. . william kaufmann inc.. 1 
 t.g. dietterich and r.s. michalski.  a comparative review of selected methods for learning from examples   in machine learning: an artificial intelligence approach. r.s. michalski  j.g. carbonell. and t.m. mitchell  eds.   tioga. 1 
 a.a. fraenkel. y. bar-hillel and a. levy  foundations of set theory 1nd revised ed.. north-holland pub. co.. 1 
 e.m. gold.  language identification in the limit . information and control 1. 1. pp 1 
 f. hayes-roth   the role of partial and best matches in knowledge systems   in pattern-directed inference systems  d.a. waterman and f. hayes-roth  eds. . academic press. 1 
 j.a. kalman.  lattices with involution . transactions of the american mathematical society  vol. 1. 1. pp 1 
 ll  s.c. kleene. introduction to metamathematics. north holland pub. co.. 1 
 h.j. levesque.  a formal treatment of incomplete knowledge bases   ph.d. thesis. department of computer 
science. university of toronto  1 
 r.s. michalski   a theory and methodology of inductive learning   in machine learning: an artificial intelligence 
approach. r.s. michalski. j.g. carbonell. and t.m mitchell  eds. . tioga. 1 
1 	knowledge acquisition 
 t.m. mitchell.  version spaces: a candidate elimination approach to rule learning . proceedings of the fifth international conference on artificial intelligence  cambridge. mass.. 1. pp 1 
 k. morik.  acquiring domain models   in proceedings of the knowledge acquisition for knowledge-based systems workshop. banff. canada. 1 
 d.n. osherson  m. stob and s. weinstein   formal theories of language acquisition: practical and theoretical perspectives . proceedings of the eighth international conference on artificial intelligence  karlsruhe  west germany  1 
 l 1  n. rescher  many-valued logic  mcgraw-hill. 1 
 e.y. shapiro   inductive inference of theories from facts . research report 1. department of computer science  yale university. 1 
 r.g. smith. t.m. mitchell. r.a. chestek and b.g. buchanan.  a model for learning systems . proceedings of the fifth international conference on artificial intelligence. 
cambridge. mass.. 1. pp 1 
 e.m. solway and e.m. riseman   levels of pattern description in learning . proceedings of the fifth international conference on artificial intelligence  cambridge. mass.. 1. pp 1 
 s.a. vere.  inductive learning of relational productions . in pattern-directed inference systems. waterman and hayes-roth  eds. . academic press. 1 
 p.h. winston.  learning structural descriptions from examples  in the psychology of computer vision  p. winston  ed . mcgraw-hill. 1 
