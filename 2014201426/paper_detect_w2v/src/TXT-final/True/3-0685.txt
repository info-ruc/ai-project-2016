
we consider the most realistic reinforcement learning setting in which an agent starts in an unknown environment  the pomdp  and must follow one continuous and uninterrupted chain of experience with no access to  resets  or  offline  simulation. we provide algorithms for general connected pomdps that obtain near optimal average reward. one algorithm we present has a convergence rate which depends exponentially on a certain horizon time of an optimal policy  but has no dependence on the number of  unobservable  states. the main building block of our algorithms is an implementation of an approximate reset strategy  which we show always exists in every pomdp. an interesting aspect of our algorithms is how they use this strategy when balancing exploration and exploitation.
1 introduction
we address the problem of lifelong learning in a partially observable markov decision process  a pomdp . we consider the most general setting where an agent begins in an unknown pomdp and desires to obtain near optimal reward. in this setting  the agent is forced to obey the dynamics of the environment  which  in general  do not permit resets.
모the problem of lifelong learning has been well studied for observable mdps. kearns and singh  1  provide the e1 algorithm  which has finite  polynomial  time guarantees until the agent obtains near optimal reward. unfortunately  such an algorithm is not applicable in the more challenging pomdp setting. in fact  none of the guarantees in the literature for learning in the limit for mdps apply to pomdps  for reasons which are essentially due to the partially observability.
모for pomdps  the problem of balancing exploitation with exploration has received rather little attention in the literature - typically most results in pomdps are on planning  see for example sondik  1 ; lovejoy  1a  1b ; hauskrecht  1 ; cassandra  1  . most of the existing learning algorithms such as parr and russell  1 ; peshkin et al.  1  either assume a goal state or assume a reset button.
in fact  to the best of our knowledge  the literature does not contain even asymptotic results for general pomdps which guarantee that the average reward of an agent will be near optimal in the limit.
모part of the technical difficulty is that there are currently no general results for belief state tracking with an approximate model showing that divergence in the belief state does not eventually occur. crudely  the issue is that if a belief state is being tracked in an approximate manner  then it is important to show that this approximation quality does not continually degrade with time - otherwise the agent will eventually loose track of the belief state in the infinite horizon  this of course is not an issue in an mdp where the current state is observable . boyen and koller  1  address the issue of approximate belief state tracking  but in their setting the model is known perfectly and their goal is to keep a compact representation of the belief state. note that approximate belief state tracking is much simpler if the agent is only acting over a fixed finite horizon  since then one can bound the error accumulation as a function of the horizon.
모we present new algorithms for learning in pomdps which guarantee that the agent will obtain the optimal average reward in the limit. furthermore  we provide a finite time convergence rates for one of our algorithms which has an exponential dependence on a certain horizon time  of an optimal strategy  but has no dependence on the number of states in the pomdp. this result is reminiscent of the trajectory tree algorithm of kearns et al.  1  which has similar dependencies  though there they assumed access to a generative model  which allowed simulation of the pomdp . given the plethora of complexity results in the literature on planning in pomdps  see lusena et al.  1    we feel these dependencies are the best one could hope for in the most general setting.
모central to our algorithms is the implementation of an approximate reset strategy or a homing strategy. the idea of a reset strategy is not new to the literature - homing sequences were used in the learning of deterministic finite automata  see rivest and schapire  1   though there the sequence provided exact resets . here  the agent follows a homing strategy in order to move approximately towards a reset. we show that such a strategy always exists  and our finite convergence rates also depend on a characteristic time it takes to approximately reset. however  note that existence of such a strategy alone does not imply that such a strategy will be useful.
the reason is that the agent must take actions to reset  which might otherwise be better spent exploring or exploiting. it turns out that our algorithms use the homing strategy while both exploring and exploiting. in fact  they use the homing strategies infinitely often  which  unfortunately  detracts from exploiting. however  we are able to show that the ratio of the time these homing strategies are used compared to the time spent exploiting is decreasing sufficiently rapidly  such that near optimal average reward can be obtained.
1 preliminaries
a partially observable markov decision process  pomdp  is defined by a finite set of states s  an initial state s1  a set of actions a  a set of observations o  with an output model q  where q o r|s a  is the probability of observing o and reward r after performing action a at state s  we assume that r 뫍  1    and a set of transitions probabilities p  where p s1|a s  is the transition probability from s to s1 after performing action a. we define r s a  as the expected reward under q 몫|s a  after performing action a in state s.
모a history h is a sequence of actions  rewards and observations of some finite length  i.e. h = { a1 r1 o1  ...  at rt ot }. a strategy or policy in a pomdp is defined as a mapping from histories to actions. we define a belief state b to be a distribution over states. given an initial belief state b1 let pr h|b1  = pr r1 o1.... rt ot|a1 ... at b1  be the probability of observing the sequence of reward-observations  r1 o1 ...rt ot  after performing the actions a1 ...at.
모for each strategy 뷇 we define its t-horizon expected reward from a belief state b as 
. a t-markov strategy
is a strategy that depends only on the last t observations. the optimal t-markov strategy's expected return from initial belief state b is defined as rt  b .
모the only assumption we make is that the pomdp is connected  i.e. for all states s s1  there exists a strategy 뷇 which can reach s1 with positive probability starting from s.  we do not make any ergodicity assumptions  since strategies are by definition non-stationary . note that if the pomdp is disconnected  then the best statement we could hope for is to obtain the optimal average reward for one of its connected components.
모connectivity implies that there exists a strategy 뷇  that maximizes the average reward. more formally  there exists a 뷇  such that: i  for every exists and does not depend on b  which we denote by r   and ii  for all 뷇 and b  r  뫟 limt뫸 suprt뷇 b . hence  for all there exist a 뷉  such that for all b and t 뫟 뷉:

and we refer to 뷉 as the -horizon time of the optimal strategy. essentially  뷉 is the timescale in which the optimal strategy achieves close to its average reward.
모when we say that we restart a t-markov strategy 뷇 from a belief state b we mean specifically that we reset the history  i.e.  h =    and run 뷇 starting from a state s distributed according to b.
1 homing strategies
clearly  having an action which resets the agent to some designated state would be useful  as it would allow us to test and compare the performance of various policies  starting at the same start state. however  in general  such an action is not at our disposal.
모instead our algorithms utilize an approximate reset  which we show always exists. there are a few subtle points when designing such a reset. first  we must select actions to achieve the approximate reset  i.e. the approximate reset is done through the use of a homing strategy. hence  while homing  the agent is neither exploring nor exploiting. second  rather than moving to a fixed state  the homing strategy can only hope to move to toward a fixed  unknown  belief state. third  as we shall see  since the pomdp might be periodic the stopping time must be a random variable1. to implement this randomized stopping time  we introduce fictitious 'stay' actions  in which the agent does not take an action that period. by this  we mean that if the homing strategy decides to take a 'stay' action at some time - which may not be possible if the true pomdp does not permit 'stay' actions - then the agent just ignores this 'stay' action and obtains another action from the homing strategy to execute. hence  after the agent has taken t homing actions  which are either real or 'stay' actions   the agent has taken t   m real actions in the pomdp and m stay actions. we now define an approximate reset strategy.
definition 1 h is an -approximate reset  or homing  strategy if for every two belief states b1 and b1  we have
  where he b  is the expected
belief state reached from b after k homing actions of h  so at most k real actions have been taken  and h b  is a random variable such that he b  = eh b 몲h b h b  .
모the above definition only states that h will approximately reset  but this approximation quality could be poor. we now show how to amplify the accuracy of an approximate homing strategy  and then we show that an approximate homing strategy always exists.
lemma 1 suppose that h is an  approximate reset then h` is an  approximate reset  where h` consecutively implements h for ` times. furthermore  this implies there exists a unique belief state bh such that he bh  = bh.
모proof: the proof is a standard contraction argument  and we use induction. for l = 1  the claim follows by definition.
 assume now that . let  and  so ps |q1 s   
. for an arbitrary state s1  and using the fact that h is a linear operator  we have

where the first term is 1 since for any two distributions ps q1 s    q1 s  = 1   1 = 1  and the vector h s1  is constant in this sum   and we have used the fact that
 by definition of h .	
모we now show that the random walk strategy  including 'stay' actions  is an approximate reset strategy in every pomdp  including periodic ones   though with prior knowledge we might have better approximate reset strategies at our disposal.
lemma 1 for all pomdps  the random walk strategy  using 'stay' actions  constitutes an  approximate reset strategy for some k 뫟 1 and .
모proof: by our connectivity assumption  for all states s and s1  there exists some strategy that reaches s1 from s with positive probability. this implies that under h  the random walk strategy   there is positive probability of moving from one state to another  i.e. the markov chain is irreducible. furthermore  since h performs 'stay' actions  then the markov chain is aperiodic. thus  there exists a unique stationary distribution. we choose k to be the time at which the error in convergence is less than 1  from all starting states. hence  by linearity of expectation  the error is less than 1 from all belief states in k steps.	
1 reinforcement learning with homing
we now provide two algorithms which demonstrate how near-optimal average reward can be obtained  with different rates of convergence. the key to the success of these algorithms is their use of homing sequences in both exploration and in exploitation. for exploration  the idea is that each time we attempt some exploration trajectory we do it after implementing our reset strategy - hence our information is  approximately  grounded with respect to the belief state bh  recall he bh  = bh . the idea of exploration is to find a good t-markov strategy from bh. during exploitation  the goal is to use this t-markov strategy. unfortunately  we have only guaranteed that 뷇 t  performs well starting from bh and only for t steps. hence  after each time we exploit with
  we run our homing sequence to get back close to bh  and then we rerun 뷇 t  . we gradually increase t in the process.
모the problem is that while homing  we are wasting time and neither exploiting nor exploring. furthermore  since we use
input : h /*a  1 kh  approximate reset strategy */ for t = 1 to  do
/*exploration in phase t */;
;
foreach policy 뷇 in 붫t do fordo
run 뷇 for t steps;
   repeatedly run	times; end
let v뷇 be the average return of 뷇 in from these trials;
end
/*exploitation in phase t */;
let = argmax뷇뫍붫t v뷇; current time t 

모모모모+ time in t + 1-th exploration phase   ; for  do
run 뷇 t  for t steps;
모모모repeatedly runtimes; end endalgorithm 1: policy search
the homing sequence between every run of 뷇 t   asymptotically we never stop homing. nonetheless  we able to show that there exists an algorithm which obtains near optimal reward in a pomdp  since the ratio of the time spent exploiting vs. homing decreases sufficiently fast.
theorem 1 there exists an algorithm a  such that in any connected pomdp  a obtains the optimal average reward in the limit  with probability 1.
모we later provide an algorithm with a better convergence rate  see theorem 1 . however  we start with a simpler policy search algorithm which establishes the above theorem.
1 policy search
algorithm 1 takes as input a  1 kh -approximate reset strategy  which could be the random walk strategy with a very crude reset. the algorithm works in phases  interleaving exploration phases with exploitation phases. let us start by describing the exploration phases. let 붫t be the set of all t-markov strategies. an estimate of the value of a policy 뷇 뫍 붫t can be found by first resetting and then running 뷇 for t steps. the exploration phase consists of obtaining an estimate v뷇 of the return of each policy 뷇 뫍 붫t  where each estimate consists of an average of k1 trials  followed by approximate resets .
모these estimates have both bias and variance. the variance is just due to the stochastic nature of the pomdp. the bias is due to the fact that we never can exactly reset to bh. however  if we run times  where t is an error parameter in the t-th phase  which will be fixed latter  then  by lemma 1  all expected belief states we could approach will be  close to bh. the following lemma shows that accurate estimates can be obtained.
lemma 1 in phase and each reset consists of using the homing sequence  times  then for all policies 뷇 뫍 붫t  the estimated t-horizon reward  v뷇  satisfies  with probability greater than 1   1.
모proof: first  let us deal with the bias. any expected belief states b that results from using the homing sequence 
times must satisfy . now it is straightforward to see that if b and bh are belief states such
  then for every strategy 뷇  |rt뷇 b   
 . to see this  let the belief states at time t be bt and bht   which result from following either 뷇 starting from b or bh  respectively. by linearity of expectation  it follows
. this directly implies that |rt뷇 b   
.
모for the variance  the hoeffding bound and our choice of imply that the average return of each policy is t close to its expectation  the expectation is both over the initial state and on the policy trajectory  with probability 1   1/ 1 .
모now during exploitation  the algorithm uses the policy which had the highest return in the exploration phase  and by the previous lemma this is close to the policy with largest return . note that we have only guaranteed a large return for executing 뷇 t  from bh for t steps. however  we would like to exploit for a longer period of time than t. the key is that we again reset  times after each time we run 뷇 t   which resets us close t close to bh. unfortunately  this means we spend steps between each run of 뷇  . hence  our average return could be less than we would like  since is the fraction of time we spend resetting. note that this fraction could be large if we desire that t be very small  thought this would guarantee very accurate resets .
모now  when we do exploit  and reset   we run the exploitation phase long enough  for time  such that our overall average reward is comparable to the average reward in the last exploitation phase.
lemma 1 at any time t after phase t  the average reward from time 1 to time t satisfies:  with probability at least .
모before proving this lemma  let us state a corollary from which theorem 1 follows.
corollary 1 let. then 
 with probability at least .
모importantly  note the loss term goes to 1 as t goes to infinity. furthermore  for a large enough phase t  we know that rt  bh  will approach the optimal average reward r   since r  is independent of the starting b . theorem 1 follows. essentially  although we have to home infinitely often  the ratio of time spent homing to the time spent using our t-step exploitation policies is going as  which goes to 1.
	proof:	first 	let us show that the average reward 
  is no less than t from the average
reward obtained in the t-th exploitation phase. to do this  we set the time of exploitation phase   to be times greater than previous amount of time spent in the mdp time plus the amount of time that will be spent in the next exploration phase  this latter factor accounts for the case in which time t lies in the exploration phase immediately after t .
모now we bound the average reward obtained in the exploitation phase. first. let us show that the -average reward of the policy used   satisfies with probability at least . by lemma 1 for each policy 뷇 뫍 붫t  we have with probability at least . therefore -optimal with probability 1/ 1 . now the observed average return of 뷇 t  in the exploitation period is close to with probability at least   since our observed average return in exploitation is least as good as those used to find  since
모however  the average return during the exploitation phase is not the observed average return of 뷇 t   since we reset after each t exploitation steps for a number of steps that is . the resets in the exploitation period can change the average reward by at most a fraction
	.	
1 a model based algorithm
the previous algorithm was the simplest way to demonstrate theorem 1. however  it is very inefficient  since it is testing all t-markov policies - there are doubly exponential  in t  such polices1. here  we provide a more efficient model based algorithm  which resembles the algorithms given in kearns et al.  1 ; mcallester and singh  1   and is exponential in the horizon time  yet it still has no dependence on the number of states in the pomdp.
모we now state a convergence rate in terms of 뷉  the horizon time of an optimal policy  see section 1  and and in terms of the homing time kh  recall  such a time exists for every pomdp using a random walk policy .
theorem 1 there exists an algorithm a  such that in any connected pomdp and with probability greater than 1   붻  a achieves an average reward that is 1 close to the optimal average reward in a number of steps in the pomdp which is polynomial in |a| |o| kh and log 1/붻  and exponential in 뷉. furthermore the computational runtime of this algorithm is polynomial in |a| |o|  and log 1/붻  and exponential 뷉.
모we provide such an algorithm in the next page. in exploration phase  the algorithm builds an approximate model of the transition probabilities after some history has occurred starting from bh. in the t-th phase  it builds a model with respect to the set of all t-length histories  which we denote by ht. in the exploitation phase  it uses the best t markov strategy with respect to this model. the use of homing strategies is similar to that in the previous algorithm.
모let l = |a||o|  and note that 1lt 뫟 |ht|. in the exploration phase  the algorithm takes actions uniformly at random for t steps and then resets  running the homing strategy
input : h /*an  1 kh  approximate reset strategy */
let l = |a| 몫 |o|; for t = 1 to  do
	t	;
for times do
run random for t steps;
	run	steps.
end
for h 뫍 ht  a 뫍 a and o 뫍 o do
pr   o|h b1 a  = 1;
if  then
pr   o|h b1 a  = pr    hh| ba o1  pr  |ba1  
pr 
end
end
compute 뷇 t  using pr   o|b1 h a ;
current time t 

       + time in t + 1-th exploration phase   ; for times do
run 뷇 t  for t steps;
	run	steps;
   end endalgorithm 1: model based
for  times . this is done times.1 then using the empirical frequencies in these trajectories the algorithm forms estimates pr   o|h bh a   which is just the empirical probability of observing o conditioned on history h followed by taking action a. for histories h which are unlikely  these empirical estimates could be very bad  though  as we shall see  we do not need accurate estimates of pr   o|h bh a  for such histories. let h a o  be a history with h followed by  a o .
lemma 1 in phase  and each reset consists of using the homing sequence 
times  then:  1    and  1  for every h a o  뫍 ht such that   we have
  with probability at
least .
모proof: we first note that  with probability 1   1/t1  for every history h 뫍 ht we have 
 using the hoeffding bound . the error  |pr o|h b1 a    pr   o|h b1 a |  is then
 pr   
pr 
뫞
	pr    	1	l1t
 
where the first inequality holds with probability   and in the last inequality we used the fact that pr . 
모the exploitation policy can be found using dynamic programming with the model. note that the pomdp is equivalent to an mdp where the histories are states. in the exploitation phase  the algorithm uses the best t-markov policy  뷇 t  
  with respect to the approximate model  interleaving it with  homing steps.
lemma 1	t	  satisfies
 with
probability at least .
proof:  sketch  we observe that by ignoring all histories
 which we view as nodes in a tree  such that pr     the return of an optimal strategy in this empirical model is decreased by at most   due to the fact that the true history probability is bounded by   the return from each node is bounded by t and the total number of such nodes is bounded by 1lt. next we prove that the return of the optimal policy in the empirical model loses at most due to the tree approximation on the other nodes  the other histories . using backward induction  we show that the policy 뷇 t  has return not less than in comparison to the true optimal value  starting from  t   k + 1 -length histories. the base case  for the leaves  the t-length histories   holds since the reward  which is encoded through the observations  is within  where the first error is due to the imperfect reset and the second is due to the marginal distribution error that is bounded by  by lemma 1. assume the induction assumption holds for k   1. there are two sources of error  the first is due to the current estimation error  of both the marginal distribution and the immediate reward  which is bounded by and the second is due to errors from the previous levels and is bounded by  by the induction assumption. summing the terms completes the induction step.	
모similarly to subsection 1  we exploit long enough such that the overall average reward is essentially the average reward in the last exploitation period.
lemma 1 at any time t after phase t  the average reward from time 1 to time t satisfies: 
 with probability at least using the above lemma  theorem 1 follows immediately if we set.
모proof:  sketch  we first note that all exploitations and explorations from phases 1 to t and from the next   t + 1 -th  exploration phase can effect the average reward by at most t. by lemma 1  the exploitation policy is near optimal and satisfies 
with probability 1   1/ 1 . as in lemma 1  we observe that the bias of the exploitation policy is t and the variance due to hoeffding's bound and the large exploitation time is bounded by t with probability 1   1/ 1 . the last source for loss is the resets in the exploitation period and its effect can be bounded by. 
모a direct and simple corollary from which theorem 1 follows as well.
corollary 1 let. then 
 with probability at least .
acknowledgements
this work was supported in part by the ist programme of the european community  under the pascal network of excellence  ist-1  and by a grant from the israel science foundation and an ibm faculty award. this publication only reflects the authors' views.
