 
in traditional text classification  a classifier is built using labeled training documents of every class. this paper studies a different problem. given a set p of documents of a particular class  called positive class  and a set u of unlabeled documents that contains documents from class p and also other types of documents  called negative class documents   we want to build a classifier to classify the documents in 
u into documents from p and documents not from p. the key feature of this problem is that there is no labeled negative document  which makes traditional text classification techniques inapplicable. in this paper  we propose an effective technique to solve the problem. it combines the rocchio method and the svm technique for classifier building. experimental results show that the new method outperforms existing methods significantly. 
1 introduction 
text classification is an important problem and has been studied extensively in information retrieval and machine learning. to build a text classifier  the user first collects a set of training examples  which are labeled with pre-defined classes  labeling is often done manually . a classification algorithm is then applied to the training data to build a classifier. this approach to building classifiers is called supervised learning/classification because the training examples/documents all have pre-labeled classes. 
   this paper studies a special form of semi-supervised text classification. this problem can be regarded as a two-class  positive and negative  classification problem  where there are only labeled positive training data  but no labeled negative training data. due to the lack of negative training data  the classifier building is thus semi-supervised. since traditional classification techniques require both labeled positive and negative examples to build a classifier  they are thus not suitable for this problem. although it is possible to manually label some negative examples  it is labor-intensive and very time consuming. in this research  we want to build a classifier using only a set of positive examples and a set of unlabeled examples. collecting unlabeled examples or documents is normally easy and inexpensive in many text or web page domains  especially those involving online sources  nigam et al 1; liu et ai  1 . 
   in  liu et al  1; yu et al  1   two techniques are proposed to solve the problem. one is based on the em algorithm  dempster et al.  1   called s-em  and the other is based on support vector machine  svm   vapnik  1   called pebl . however  both techniques have some major shortcomings. s-em is not accurate because of its weak classifier. pebl is not robust because it performs well in certain situations and fails badly in others. we will discuss these two techniques in detail in section 1 and compare their results with the proposed technique in section 1. 
   as discussed in our earlier work  liu et al.  1   positive class based learning occurs in many applications. with the growing volume of text documents on the web  
internet news feeds  and digital libraries  one often wants to find those documents that are related to one's interest. for instance  one may want to build a repository of machine learning  ml  papers. one can start with an initial set of ml papers  e.g.  an 1cml proceedings . one can then find those ml papers from related online journals or conference series  e.g.  ai journal  aaai  ijcai  etc. 
   the ability to build classifiers without negative training data is particularly useful if one needs to find positive documents from many text collections or sources. given a new collection  the algorithm can be run to find those positive documents. following the above example  given a 
   collection of aaai papers  unlabeled set   one can run the algorithm to identify those ml papers. given a set of sigir papers  one can run the algorithm again to find those ml papers. in general  one cannot use the classifier built using the aaai collection to classify the sigir collection because they are from different domains. in traditional classification  labeling of negative documents is needed for each collection. a user would obviously prefer techniques that can provide accurate classification without manual labeling any negative documents. 
　this paper proposes a more effective and robust technique to solve the problem. it is based on the rocchio method  rocchio  1  and svm. the idea is to first use rocchio to extract some reliable negative documents from the unlabeled set and then apply svm iteratively to build and to select a classifier. experimental results show that the new method outperforms existing methods significantly. 

learning 	1 

1 related work 
many studies on text classification have been conducted in the past. existing techniques including rocchio  naive bayes 
 nb   svm  and many others  e.g.  rocchio  1; lewis & gale  1; vapnik  1; joachims  1; nigam et al  1; yang & liu  1 . these existing techniques  however  all require labeled training data of all classes. they are not designed for the positive class based classification. 
   in  denis  1   a theoretical study of pac learning from positive and unlabeled examples under the statistical query model  kearns  1  is reported.  letouzey et al  1  presents an algorithm for learning using a modified decision tree algorithm based on the statistical query model. it is not for text classification  but for normal data. in  muggleton  1   muggleton studied the problem in a 
baycsian framework where the distribution of functions and examples are assumed known. the result is similar to that in  liu et al  1  in the noiseless case.  liu et al  1  extends the result to the noisy case. the purpose of this paper is to propose a robust practical method to improve existing techniques. 
in  liu et al  1   liu et al. proposed a method 
 called s-em  to solve the problem in the  text domain. it is based on naive bayesian classification  nb  and the em algorithm  dempster et al  1 . the main idea of the method is to first use a spy technique to identify some reliable negative documents from the unlabeled set. it then runs em to build the final classifier. since nb is not a strong classifier for texts  we will show that the proposed technique is much more accurate than s-em. 
   in  yu et al.  1   yu et al. proposed a svm based technique  called pebl  to classify web pages given positive and unlabeled pages. the core idea is the same as that in  liu et al.  1   i.e.   1  identifying a set of reliable negative documents from the unlabeled set  called strong negative documents in pebl   and  1  building a 
   classifier using svm. the difference between the two techniques is that for both steps the two techniques use different methods. in  yu et al.  1   strong negative documents are those documents that do not contain any features of the positive data. after a set of strong negative documents is identified  svm is applied iteratively to build a classifier. pebl is sensitive to the number of positive examples. when the positive data is small  the results are often very poor. the proposed method differs from pebl in that we perform negative data extraction from the unlabeled set using the rocchio method and clustering. although our second step also runs svm iteratively to build a classifier  there is a key difference. our technique selects a good classifier from a set of classifiers built by svm  while pebl does not. the proposed method performs well consistently under a variety of conditions. 
   in  scholkopf et al  1   one-class svm is proposed. this technique uses only positive data to build a svm classifier. however  our experiment results show that its classification performance is poor. 
   another line of related work is the use of a small labeled set of every class and a large unlabeled set for learning  nigam et al.  1; blum & mitchell  1; goldman & zhou  1; basu et al  1; muslea et al  1; bockhorst & craven  1 . it has been shown that the unlabeled data helps classification. this is different from our work  as we do not have any labeled negative training data. 
1 the proposed technique 
this section presents the proposed approach  which also consists of two steps:  1  extracting some reliable negative documents from the unlabeled set   1  applying svm iteratively to build a classifier. below  we present these two steps in turn. in section 1  we show that our technique is much more robust than existing techniques. 
1 	finding reliable negative documents 
this sub-section gives two methods for finding reliable negative documents:  1  rocchio and  1  rocchio with clustering. the second method often results in better classifiers  especially when the positive set is small. the key requirement for this step is that the identified negative documents from the unlabeled set must be reliable or pure  i.e.  with no or very few positive documents  because svm  used in our second step  is very sensitive to noise. 
1.1 method 1: rocchio 
this method treats the entire unlabeled set u as negative documents and then uses the positive set p and u as the training data to build a rocchio classifier. the classifier is then used to classify u. those documents that are classified as negative are considered  reliable  negative data  denoted by rn. the algorithm is shown in figure 1. 
1. assign the unlabeled set u the negative class  and the positive set p the positive class; 
1. let  1. let  
1. for each document d in u do 
1. then 
1.  
figure 1: rocchio extraction using u as negative 
   in rocchio classification  each document d is represented as a vector  salton & mcgill  1   d =  q1 q1 ... qn . each element qi in d represents a word w1 and is calculated as the combination of term frequency  tf  and inverse document frequency  idf   i.e.  qi=tfi* idf. tf is the number of times that word w  occurs in d  while 
here 	is the total number of documents and   is the number of documents where word wt occurs at least once. 
1 	learning 　building a classifier is achieved by constructing positive and negative prototype vectors and  lines 1 and 1 in figure 1 . and parameters adjust the relative impact of positive and negative training examples. = 1 and = 1 are recommended in  buckley et al  1 . 
   in classification  for each test document it simply uses the cosine measure {salton & mcgill  1  to compute the similarity  sim  of  with each prototype vector. the class whose prototype vector is more similar to is assigned to the test document  lines 1 in figure 1 . those documents classified as negative form the negative set rn. 
   in general  it is also possible to use other classification methods  e.g.  naive bayesian  svm  to extract negative data from the unlabeled set. however  not all classification techniques perform well in the absence of a  pure  negative training set. we experimented with svm  naive bayesian and rocchio  and observed that rocchio produces the best results. it is able to extract a large number of true negative documents from u with very high precision. 
   the reason that rocchio works well is as follow: in our positive class based learning  the unlabeled set u typically has the following characteristics: 
1. the proportion of positive documents in u is very small. thus  they do not affect  very much. 
1. the negative documents in u are of diverse topics. thus  in the vector space  they cover a very large region. since the positive set p is usually homogenous  focusing on one topic   they cover a much smaller region in the vector space. assume that there is a true decision surface s that separates positive and negative data. then  the positive prototype vector will be closer to s than the negative prototype vector due to the vector summation in rocchio  line 1 and 1 of figure 1 . because of this reason  when we apply the similarity computation  in line 1 of figure 1  to classify documents  many negative documents will be classified as positive because they are closer to . this explains why rocchio can extract negative documents with high precision  and positive documents with low precision  but very high recall . our experiments show that this is indeed the case. 
　after some reliable negative documents are found from u by rocchio  we can run svm using p and rn. experimental results show that this simple approach works very well. note that we could actually use and in figure 1 as the final classifier without using svm. however  it does not perform well because it tends to classify too many negative documents as positive documents  low precision for positive . svm using rn and p as training data can correct the situation and produce a more accurate final classifier. 
1.1 	method 1: rocchio with clustering 
rocchio is a linear classifier based on cosine similarity. when the decision boundary is non-linear or does not conform to the separating plane resulted from cosine similarity  rocchio may still extract some positive documents and put them in rn. this will damage the svm classifier later. below  we propose an enhancement to the rocchio approach in order to purify rn further  i.e.  to discard some likely positive documents from rn. 
   figure 1 shows a possible scenario where some positive documents in u may still be in rn. and represent the positive and negative prototype vectors respectively. h is the decision hyperplane produced by rocchio. it has the same distance  similarity  to . if a document is located on the right-hand-side of//  it is classified as negative otherwise it is classified as positive. however  the positive and negative classes in the data cannot be separated by h well. in figure 1  the positive documents within the regions 1 and 1 will be misclassified as negative documents. 

figure 1: rocchio classifier is unable to remove some positive examples in u 
   in order to further  purify  rn  we need to deal with the above problem. we propose the following approach. 
   this approach uses clustering to partition rn into many similarity groups  or clusters . it then uses rocchio again to build a classifier using each cluster and the positive set p. the classifier is then applied to identify likely positive documents in the cluster and delete them. the idea is to identify and remove some positive documents in rn in a localized manner  which gives better accuracy. since both the cluster and the positive set are homogeneous  they allow rocchio to build better prototype vectors. 
   the clustering technique that we use is k-means  which is an efficient technique. the k-means method needs the input cluster number k. from our experiments in section 1  we observe that the choice of k does not affect classification results much as long as it is not too small. too few clusters may not be effective because the documents in a cluster can still be diverse. then  rocchio still cannot build an accurate classifier. too many clusters will not cause much problem. 
   this new method can further purify rn without removing too many negative documents from rn. the detailed algorithm is given in figure 1. 
learning 	1    note that in line 1 we still perform the initial rocchio extraction as discussed in section 1.1. thus  the input data for clustering is rn  not u. the reason is that the purity of rn is higher  which allows us to produce better clusters without the influence of too many noisy  positive  documents in u. lines 1 perform the standard k-means clustering of rn  which produces k clusters  n1 n1  ...  nk. based on these clusters  we construct a positive prototype vector pf and a negative prototype vector n for the positive set p arid each cluster  lines 1 and 1 . line 1 starts the extraction. for each document we try to find the nearest positive prototype vector to d1  line 1 . if the similarity sim is less than the similarity between d  and any negative prototype vector n   we put it in our final negative set rn'  line 1 . the reason for using the procedures in lines 1 and 1 is that we want to be conservative in removing likely positive documents from rn  i.e.  not to remove too many negative documents. 
1. perform the algorithm in figure 1 and generate the initial negative set rn; 
1. choose k initial cluster centers randomly from rn; 
1. perform k-means clustering to produce k clusters  
n ; 
1. 
1. 
1.  
1. 1. 
1. 
1.  
figure 1: rocchio extraction with clustering. 
   after rn' is determined  we can build the final classifier using svm  which takes p and rn' as the positive and negative training data respectively. 
1 	step 1: classifier building 
this step builds the final classifier by running svm iteratively with the document sets p and rn or rn' depending on the method used in step 1. let q be the set of the remaining 
unlabeled documents  the algorithm for this second step is given in figure 1. 
1. every document in p is assigned the class label +1; 
1. every document in rn  rn'  is assigned the label - 1 ; 
1. use p and rn  or rn   to train a svm classifier s  with / = 1 initially and  with each iteration  line 1 ; 
1. classify q using s . let the set of documents in that are classified as negative be w  
1. if w = else  
goto  1 ; 
1. use the last svm classifier 	to classify p  
1. if more than positive are classified as negative then use as the final classifier; 
	else use 	as the final classifier; 
figure 1: constructing the final classifier. 
　the reason that we run svm iteratively  lines 1  is that the reliable negative set rn from step 1 may not be sufficiently large to build the best classifier. svm classifiers  s  in line 1  can be used to iteratively extract more negative documents from  the iteration stops when there is no negative document that can be extracted from  line 1 . 
there is  however  a danger in running svm iteratively. 
since svm is very sensitive to noise  if some iteration of 
svm goes wrong and extracts many positive documents from  and put them in the negative set rn  then the last svm classifier will be extremely poor. this is the problem with pebl  which also runs svm iteratively. in our algorithm  we decide whether to use the first svm classifier or the last one  line 1 . basically  we use the svm classifier at convergence  called slast  line 1  to classify the positive set p. if too many positive documents in p are classified as negative  it indicates that svm has gone wrong. we then use the first svm classifier   otherwise  we use as the final classifier. we choose   a s the threshold because we want to be very conservative. since svm is very sensitive to noise  when noise is large  the resulting classifier is often very poor. since our first classifier is always quite strong  even without catching the last classifier which may be better  it is acceptable. 
   note that this strategy is not applicable to pebl because pebl's step 1 extracts too few negative documents from u. 
its first svm classifier is thus very inaccurate. step 1 of our proposed technique is able to extract a large number of negative documents from u. hence  our first svm classifier is always quite good  although it may not be the best. 
   note also that neither the first nor the last svm classifier may be the best. this is the case for both pebl and our algorithm. in many cases  a classifier somewhere in the middle is the best. however  it is hard to catch it. we leave this issue to our future research. 
1 empirical evaluation 
this section evaluates the proposed techniques using the 
reuters-1 text collection   which is commonly used in evaluating text classification methods. we will also compare the proposed techniques with s-em and pebl. 
   the reuters-1 collection contains 1 text documents. we used only the most populous 1 out of the 1 topic categories. table 1 gives the number of documents in each of the ten topic categories. in our experiments  each category is used as the positive class  and the rest of the categories as the negative class. this gives us 1 datasets. 
table 1: categories and numbers of documents in reuters 

　　our task is to identify positive documents from the unlabeled set. the construction of each dataset for our experiments is done as follows: firstly  we randomly select  of the documents from the positive class and the negative class  and put them into p and n classes respectively. the remaining documents from both classes form the unlabeled set u. our training set for each dataset consists of p and u. n is not used as we do not want to change the class distribution of the unlabeled set v. we also used t/as the test set in our experiments because our objective is to recover those positive documents in u. we do not need separate test sets. in our experiments  we use 1 a values to create different settings  i.e.   
http://www.rcscarch.att.com/~lcwis/rcuters1.html 

1 	learning 

1 	evaluation measures 
two popular measures used for evaluating text classification are the f value and breakeven point. f value is defined as  f = wherep is the precision and r is the recall. f value measures the performance of a system on a particular class  in our case  the positive class . breakeven point is the value at which recall and precision are equal. however  breakeven point is not suitable for our task as it only evaluates the sorting order of class probabilities of documents. it does not give indication of classification performance. f value  on the other hand  reflects an average effect of both precision and recall. this is suitable for our purpose  as we want to identify positive documents. it is undesirable to have either too small a precision or too small a recall. 
   in our experimental results  we also report accuracies. it  however  should be noted that accuracy does not fully reflect the performance of our system  as our datasets has a large proportion of negative documents. we believe this reflects realistic situations. in such cases  the accuracy can be high  but few positive documents may be identified. 
1 	experimental results 
we now present the experimental results. we use roc-svm and roc-clu-svm to denote the classification techniques that employ rocchio and rocchio with clustering to extract reliable negative set respectively  both methods use svm for classifier building . we observe from our experiments that using rocchio and rocchio with clustering alone for classification do not provide good classification results. svm improves the results significantly. 
for comparison  we include the classification results of 
nb  s-em and pebl. here  nb treats all the documents in the unlabeled set as negative. svm for the noisy situation  v as negative  performs poorly because svm docs not tolerate noise well. thus  its results are not listed  in many cases  its f values are close to 1 . in both roc-svm and roc-clu-svm  we used the linear svm as  yang & liu  1  reports that linear svm gives slightly better results than non-linear models on the reuters dataset. for our experiments  we implemented pebl as it is not publicly available. for svm  we used the system  joachims  1 . pebl also used 
       s-em is our earlier system. it is publicly available at http://www.cs.uic 	download.html. 
   table 1 shows the classification results of various techniques in terms of f value and accuracy  ace  for a = 1%  the positive set is small . the final row of the table gives the average result of each column. we used 1 clusters  i.e.  k = 1  for /:-means clustering in roc-clu-svm  later we will see that the number of clusters does not matter much . 
   we observe that roc-clu-svm produces better results than roc-svm. both roc-svm and roc-clu-svm outperform nb  s-em and pebl. pebl is extremely poor in this case. in fact  pebl performs poorly when the number of positive documents is small. when the number of positive documents is large  it usually performs well  see table 1 with a = 1% . both roc-svm and roc-clu-svm perform well consistently. we summarize the average f value results of all a settings in figure 1. due to space limitations  we are unable 
learning 
to show the accuracy results  as noted earlier  accuracy does not fully reflect the performance of our system . 

figure 1 average results for all a settings 
from figure 1  we can draw the following conclusions: 
1. s-em's results are quite consistent under different settings. however  its results are worse than roc-svm and roc-clu-svm. the reason is that the negative documents extracted from u by its spy technique are not that reliable  and s-em uses a weaker classifier  nb. 
1. pebl's results are extremely poor when the number of positive documents is small. we believe that this is because its strategy of extracting the initial set of strong negative documents could easily go wrong without sufficient positive data. even when the number of positive documents is large  it may also go wrong. for example  for a = 1%  one f value  for the dataset  trade  is only 1. this shows that pebl is not robust. 
1. both roc-svm and roc-clu-svm are robust with different numbers of positive documents. this is important 
1 

because in practice one does not know how many positive documents are sufficient. using a smaller set of positive documents also reduce the manual labeling effort. 
     from figure 1  we also observe that roc-clu-svm is slightly better than roc-svm in both f value and accuracy  especially when a is small because pure rn is more important in such cases. when a is large  their results are similar. in practice  roc-svm may be sufficient due to its simplicity  efficiency and good results. we also ran one-class svm using the libsvm package 
 http://www.csie.ntu.edu.  it gives very poor results  mostly with f values less than 1 . due to space limitations  we do not list their results here. 
comparison of different number of clusters: as discussed above  the number of clusters used in roc-clu-svm makes little difference to the final classifier. we now provide ex-
periment results in figure 1 to support this claim by using different numbers of clusters  here  a = 1% and for other a values  the results are similar . the cluster number k varies from 1 to 1. we observe that the choice of k has little influence on the results as long as it is not too small. 
figure 1: averaged f values of different clusters 
execution times: our technique consists of two steps: extracting negative documents from u and iteratively running svm. as svm is a standard technique  we will not discuss it here. if we only use rocchio for the first step  it is very efficient because rocchio is a linear algorithm  o luup  . if we use rocchio with clustering  more time is needed because of k-means clustering. the time complexity of k-means is 
1 k* rn *i   where k is the number of clusters   rn  is the size of the reliable negative set  and 1 is the number of iterations. since k and / are normally small  /:-means is generally regarded as a linear algorithm o /rn  . thus  the time complexity of the extraction step of roc-clu-svm is o /uup   since  rn     uup . in our experiments  every dataset takes less than 1 seconds for step one for roc-clu-svm  on pentium 1mhz pc with 1mb memory . 
1 conclusions 
this paper studied the problem of text classification with only partial information  i.e.  with only one class of labeled documents and a set of unlabeled documents. an effective technique is proposed to solve the problem. our algorithm first utilizes the rocchio classifier and/or clustering to extract a set of reliable negative documents from the unlabeled set  and then builds a svm classifier iteratively. experimental results show that the proposed technique is more robust than both s-em and pebl. 
1 
acknowledgement: this research is supported by a research grant from a-star and nus to the second author. 
