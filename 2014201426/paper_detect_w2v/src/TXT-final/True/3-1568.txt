
finding most probable explanations  mpes  in graphical models  such as bayesian belief networks  is a fundamental problem in reasoning under uncertainty  and much effort has been spent on developing effective algorithms for this np-hard problem. stochastic local search  sls  approaches to mpe solving have previously been explored  but were found to be not competitive with state-of-theart branch & bound methods. in this work  we identify the shortcomings of earlier sls algorithms for the mpe problem and demonstrate how these can be overcome  leading to an sls algorithm that substantially improves the state-of-the-art in solving hard networks with many variables  large domain sizes  high degree  and  most importantly  networks with high induced width.
1 introduction
since pearl's classic text  pearl  1   graphical models such as bayesian networks have become the prime representation for uncertainty in ai. this paper deals with the problem of finding the most probable explanation  mpe  for some evidence when reasoning under uncertainty. more specifically  in the light of uncertain knowledge represented as a probabilistic graphical model  this problem is cast as finding the most probable instantiation of all the model's variables v given the observed values e for a subset e   v.
모the mpe problem in graphical models has applications in different fields  such as medical diagnosis  jaakkola and jordan  1   fault diagnosis  rish et al.  1   computer vision  tappen and freeman  1   and prediction of sidechains in protein folding  yanover and weiss  1   to name just a few. consequently  many algorithms have been suggested to solve this problem  but since it is np-hard  many hard problem instances still cannot be solved efficiently. the available algorithms for mpe solving include exact methods like variable elimination  ve   dechter  1   junction tree  jt   cowell et al.  1  and conditioning techniques  pearl  1   but also systematic search algorithms  such as branch & bound  b&b   guided by a mini-buckets  mb  heuristic  dechter and rish  1 . since in practice many applications require efficient online algorithms for networks of high induced width  there is also much research in approximate mpe algorithms  reaching from loopy belief propagation  bp   pearl  1  and generalized bp  yedidia et al.  1  to stochastic local search  sls  algorithms  kask and dechter  1; park  1  and specialized algorithms  such as graph cuts for certain pairwise markov random fields  mrfs  that  for example  occur in computer vision  boykov et al.  1 .
모b&b approaches have recently been shown to be stateof-the-art methods in mpe solving  and it has been claimed that for mpe  b&b algorithms clearly outperform gls  the best performing sls algorithm known so far  in terms of the ability to find high-quality solutions quickly  marinescu et al.  1 . this is in stark contrast to results for numerous other combinatorial optimisation problems  such as weighted max-sat  where sls algorithms clearly define the state-ofthe-art  see  e.g.   hoos and stutzle  1몮   .
모in this work  we analyse the shortcomings of previous sls algorithms for mpe and demonstrate that these weaknesses can be overcome based on the careful consideration of important issues  such as the time complexity of individual search steps  search stagnation and thorough parameter tuning. in particular  we introduce improvements to park's gls algorithm  park  1  that overcome its inferior scaling behaviour with network and domain size. our new algorithm  gls+  clearly outperforms current state-of-the-art mpe algorithms on various types of networks  especially for hard networks with high induced width  and hence establishes stochastic local search as a highly attractive and competitive approach to mpe solving.
모the remainder of this paper is structured as follows. we first introduce some basic concepts and notation in section 1; next  in section 1  we describe gls and gls+ and present some computational results illustrating the improvement of gls+ over gls. in section 1 we present empirical results that establish gls+ as a new state-of-the-art algorithm for finding mpes. we close with some conclusions and a brief outlook on future work in section 1.
1 preliminaries
a discrete bayesian belief network  or bayes net  b is a quadruple hv d g 붯i  where v is an ordered set of random variables  d is an ordered set of finite domains dvi for each vi 뫍 v  g =  v e  is a directed acyclic graph  dag   and 붯 is an ordered set of cpts 뷋v = p v |pa v     specifying the conditional probability distribution of each v 뫍 v given its parents pa v   in g. semantically  a bayes net specifies a joint probability distribution 뷋 over its variables v in factored form: 뷋 = p v  = qv 뫍v 뷋v .
모given a bayes net b = hv d g 붯i and a set of evidence variables e = e  the most probable explanation  mpe  problem is to find an instantiation v = v with maximal probability p v  := q뷋뫍붯 뷋 v = v  over all variable instantiations v consistent with evidence e. while all networks in our experimental analysis are bayes nets  our algorithms are equally applicable to other graphical models  such as mrfs or general factor graphs.
모cpts are a special case of potentials  which are functions that have non-negative entries for any assignment of their variables. one well-known method for solving mpe in general networks is variable elimination  ve   see  e.g.   dechter  1  ; it iteratively eliminates variables v by multiplying all potentials that are defined over v and then maximizing v out of the product thus obtained. once all variables are eliminated  the best assignment can be recovered in linear time. mini-buckets with i-bound ib  mb ib    dechter and rish  1  approximates ve by splitting each product into smaller products with at most ib variables; mb-w s  is a new variant of mb that instead limits the number of entries in each product by s. for constant domain size d  mb ib  and mb-w dib  are equivalent  but for networks with different domain sizes mb-w performed better in our experiments. all of ve  mb  and mb-w employ a min-weight heuristic.
1 sls for mpe: from gls to gls+
probably the most prominent stochastic local search algorithm for inference in bayesian networks is a method called stochastic simulation  pearl  1; kask and dechter  1   also known as gibbs sampling. however  for mpe  this method  as well as simulated annealing  was shown to be clearly outperformed by a simple algorithm called greedy + stochastic simulation  g+sts   kask and dechter  1   which probabilistically chooses between greedy and sampling steps.
모the mpe problem is closely related to weighted maxsat  park  1   and max-csp  marinescu and dechter  1  ; based on this close relationship  park adapted two high-performance max-sat algorithms  dlm and gls  mills and tsang  1   to the mpe problem  park  1 .1 his computational experiments identified gls as the state-of-the-art sls algorithm for mpe solving: he showed that gls and dlm clearly outperform g+sts and that gls performed better than dlm on most instances. however  as shown in  marinescu et al.  1   gls does not reach the performance of current b&b algorithms.
모gls has been applied successfully to many combinatorial problems  including tsp  voudouris and tsang  1   sat and weighted max-sat  mills and tsang  1 . it can be classified as a dynamic local search algorithm  hoos and stutzle  1몮   and uses penalties associated with solution components to guide the search process; these penalties

algorithm 1: gls/gls+ for mpe
gls and gls+ differ in the procedure used to generate an initial solution  the subsidiary local search procedure  and  most importantly  in the evaluation function g v|vi = vi . these differences are explained in the text. the utilities util 뷋 v  are defined in the text.

input: bayes net b = hv d g 붯i  evidence e = e  time bound t  smoothing factor 뷈  smoothing interval n뷈.
output: variable assignment v = v with highest probability q뷋뫍붯 뷋 v = v  found in time t
//=== initialize variable assignment v  penalties 뷂뷋  and local optima counter lo.
1 v 뫹 generateinitialsolution b e 
1 foreach 뷋 뫍 붯 and all instantiations v뷋 = v뷋 do
1  뷂뷋 v뷋 = v뷋  뫹 1
1 # lo  뫹 1
//=== alternate local search and updates of the evaluation function until termination.

are dynamically updated whenever the search reaches a local optimum.
모an outline of the gls algorithm is shown in figure 1. at a high level  after initialising the search and setting all penalties to zero  gls alternates between two search phases: first  a simple iterative improvement search is performed with respect to the evaluation function g that takes into account the current penalty values. after this process has reached a local optimum o of g  certain penalty values are incremented by one. only penalties of solution components present in o can be selected to be incremented; this selection is based on the contribution of the respective solution component to g o  and its current penalty value.  for details  see algorithm 1 or  park  1 .  additionally  all penalties are regularly multiplied by a factor 뷈 뫞 1; this smoothing mechanism prevents the penalty values from growing too large and is performed every n뷈 local optima  where n뷈 is a parameter.
	cpu time  sec 	instance size  n times average k 
	 a  speedup due to modified evaluation function	 b  speedup due to novel caching schemes
figure 1:  a  effect of the modification of the evaluation function in gls+ compared to gls on instance munin1. given are the best  average and worst solution quality over computation time for 1 runs of gls and gls+. the gls+ plot ends after 1 cpu seconds when all its runs have found the optimal solution quality.  b  the speedup achieved by our new caching schemes over the previously best-performing caching scheme on a collection of randomly generated and real-world instances  where instance size is given as the number of variables  n  times the average domain size  k .모in park's gls for mpe  the solution components are partial instantiations of the variables. more specifically  for each potential 뷋 뫍 붯  every instantiation v뷋 = v뷋 of its variables is a solution component.1 the evaluation function to be minimized is defined as g v  = p뷋뫍붯 뷂뷋 v = v   where 뷂뷋 v = v  is the penalty associated with solution component v뷋 = v뷋. it may be noted that this deviates from the standard form of the evaluation function for the general gls algorithm  which also captures the contribution of each solution component to the optimisation objective p v .
모due to this non-standard evaluation function  the sole interaction of objective function and penalties in park's gls for mpe is via the utilities of potentials 뷋 under the current assignment v  which are defined as util 뷋 v  =  뷋 v = v / 1 + 뷂뷋 v = v  . an entry 뷋 v뷋 = v뷋  with high probability is assigned low utility  and its associated penalty 뷂뷋 v뷋 = v뷋  will be increased less often  driving the search towards the partial variable instantiation v뷋 = v뷋 eventually  but possibly only after a considerable delay.
모due to this initial and persistent lack of greediness  we expected the performance of gls for mpe to be boosted significantly by integrating the objective function into the search heuristic. we achieved this by a change in gls's evaluation function. our improved version of gls  which we call gls+  adds the logarithmic objective function to the appropriate penalties  making the new evaluation function to be maximized g v  = p뷋뫍붯 log 뷋 v = v   w뫄뷂뷋 v = v   where w is a weighting factor.1 indeed  this modification of the evaluation function has major consequences on search behaviour and especially boosts the search in early phases of the search. figure 1 a  illustrates this for the real-world network munin1  from the bayesian network repository   where gls+ finds optimal-quality solutions 1 times faster on average.
모gls+ differs from gls in a number of other components  namely the parameter setting  the caching scheme  and the initialization. all of these modifications contribute significantly to gls+'s improved performance  a detailed analysis of their individual importance can be found in  hutter  1  . firstly  a thorough experimental analysis showed that  although park states that  gls had no parameters to tune.   park  1   its performance can be boosted by up to several orders of magnitude by simply changing park's default smoothing value 뷈 = 1 to the constant value 뷈 = 1. for example  for the hailfinder network gls finds the optimal solution 1 times faster with 뷈 = 1 than with 뷈 = 1  and for several random instances this effect is much more pronounced. it is interesting to note that the very small amount of smoothing performed at 뷈 = 1 can be rather important: without smoothing  뷈 = 1   we found rare but conclusive evidence for search stagnation on random networks.
모secondly  the subsidiary local search procedure in gls uses a computationally efficient first-improvement strategy  whereas gls+ employs a more powerful best-improvement procedure in conjunction with newly developed  powerful strategies for caching and updating the effects of variable flips on the evaluation function. while previously used caching schemes only locally update the partial evaluation function value involving variables in the markov blanket of a flipped variable  we developed two substantial improvements. at every step  we maintain the score of flipping each variable to any of its values  caching scheme scores   and on top of that the set of all variables that lead to an improvement in evaluation function value when flipped to some value  caching scheme improving ; since after an initial search phase this quantity is a low constant in practice  this latter caching scheme enables an evaluation of the whole neighbourhood of a search position in constant time. figure 1 b  demonstrates the large performance gains of our new caching schemes over the previous state-of-the-art caching scheme.
모thirdly  gls initializes the search randomly. however  it has been shown that strong initial solutions  such as the ones obtained with mb  lead to much better overall solutions  kask and dechter  1 . consequently  gls+ initializes its search using the mb variant mb-w 1   see section 1   which improves the quality of found solutions considerably and for some instances speeds up the search for optimal solutions by up to two orders of magnitude.
모we also study a version of gls+ that has an additional preprocessing stage based on ve. in this preprocessing  ve is applied until a potential with more than b entries is obtained  where b is a parameter  b = 1 results in the standard
distributionstats
w	optgls+
 default gls
 orig          bbmb static 1 	dynamic 1 모모모aomb static 1 	dynamic 1 random networksbase-line: n =1  k =1  p =1.111 1 1111n=1  k=1  p=1.111 1.1 1 1.1 1 1.1 - 1 - 1 n=1  k=1  p=1.11- 1.1 - 1.1 - 1.1 - 1 - 1 n =1  k=1  p=1.111 1.1 11 1.1 11 1 n =1  k=1  p=1.11- 1.1 1 1.1 1 1.1 1 1 1 1 n =1  k=1  p=1.111 1.1 1 1.1 1 1.1 1 1 1 1 n =1  k=1  p=1.111 1.1 1 1.1 1 1.1 - 1 - 1 grid networksbase-line: n =1  k =1.111 1.1 11 1.1 11n=1  k=1.111 1.1 1 1.1 - 1.1 - 1 - 1 n=1  k=1.11- 1.1 - 1.1 - 1.1 - 1 - 1 n=1  k=1.11- 1 - 1.1 - 1 - 1 - 1 n=1  k=1.11- 1.1 11 1.1 11 1 n=1  k=1.11- 1.1 1 1.1 - 1.1 1 1 1 1 n=1  k=1.11- 1.1 1 1.1 - 1.1 1 1 - 1 n=1  k=1.11- 1.1 - 1 - 1.1 - 1 - 1 table 1: scaling of performance for random networks and grid networks with network size n  domain size k  and number of parents p. for each problem distribution  there are 1 networks. w gives their average induced width  opt the number of networks for which our quasi-optimal solutions are provably optimal  we never found a better solution than the quasi-optimal one . for each algorithm and problem distribution  we list the average time to find a quasi-optimal solution. if an algorithm did not find the quasi-optimal solution for all instances within 1 cpu seconds  we give its average time for its solved instances  and in parentheses its number of unsolved instances  followed by its average approximation quality for these instances.gls+ algorithm  and b =  yields pure ve . the resulting reduced network can then quickly be solved with gls+  and the eliminated variables can be instantiated optimally in linear time like in regular ve.
1 experimental results
we conducted a number of computational experiments to compare the performance and scaling behaviour for the original gls algorithm  gls+  and the current state-of-the-art in mpe solving.1 in  marinescu et al.  1   b&b with static mb heuristic  s-bbmb  and a b&b algorithm with mb tree elimination heuristic  bbbt  were claimed to be the stateof-the-art for mpe solving.  marinescu and dechter  1  then introduced b&b with dynamic mb heuristic  d-bbmb   as well as versions of s-bbmb and d-bbmb that employ an and/or search tree; these are called s-aomb and daomb.
모we used radu marinescu's c++ implementations of sbbmb  d-bbmb  s-aomb  and d-aomb.1 furthermore  we used marinescu's c++ implementation of gls instead of the original java implementation by park  park  1   since the former was orders of magnitude faster than the latter on all instances we tried; our gls+ implementation is also written in c++. we employed a fixed parameter setting of h뷈 n뷈i = h1 1i for gls+ and the overall bestperforming fixed i-bound ib 뫍 {1 1 1} for each
b&b algorithm. the preprocessing stage in gls+ only improves its performance for structured networks. thus  we set b = 1 in the experiments on random instances.
모in the first two experiments  for each network  we executed each algorithm once for a maximum of 1 cpu seconds and call the best solution found in any such run of an algorithm quasi-optimal. for the second and third experiment  we ran the b&b algorithms for a long time to obtain provably optimal solution qualities  which we found to always agree with our quasi-optimal solutions. for a fair comparison  we always report the times each algorithm requires for finding a quasioptimal solution; only for the third experiment we additionally report the time required for proving optimality. finally  we define the approximation quality of an algorithm run as the ratio of the probability of the solution it found and the quasi-optimal probability.
모our first experiment evaluates how the various algorithms scale with important instance characteristics  such as network size  n   domain size of the variables  k   and network density  here controlled by the number p of parents of each node. we created instances with a random network generator provided by radu marinescu  this is  e.g.  described in  marinescu et al.  1    topologically sampled all variables to guarantee non-zero joint probability  and randomly picked 1 evidence variables. table 1 shows that for small and easy networks  identified as  base-line  in the table  the original gls and the b&b algorithms are competitive with gls+  but that when scaling any of n  k  or p  the performance of all algorithms except gls+ degrades rapidly.  marinescu et al.  1  showed that s-bbmb scales much better with domain size k than the original gls algorithm. while this is confirmed by our experiments  see table 1   our results in figure 1 a  show that gls+ substantially outperforms s-bbmb for larger k  and that the relative variability in run-time increases with k for s-bbmb  but remains constant for gls+.
gls+  and s-bbmb 1 
figure 1:  a  cpu time required by gls+ and s-bbmb 1  to solve random network instances to optimality. each point is the result of a single run on one instance. the instances are the same as summarized in table 1. the same cpu-time for both algorithms is indicated by the line.  b  comparison of the quality reached by the three algorithms gls  gls+  and sbbmb 1  within 1 cpu seconds. for all instances  gls+ found the best known solution quality  thus its approximation quality is always 1. each point in the figure is the result of a single run for one instance by gls and s-bbmb 1   while the two lines show the approximation quality of gls+.  c  scaling with induced width for random networks with 1 variables  domain size 1  and maximal node degree 1. for each induced width  based on one run on each of 1 instances  we plot the median runtime for finding the optimal solution. the errorbars are based on the quantiles q1 and q1. we do not employ means and standard deviations since the b&b algorithms did not succeed in finding an optimal solution for every network. for example  for the networks of induced width 1  gls+ took 1 seconds in the worst case  whereas s-bbmb 1  failed to find the optimal solution for 1 of the 1 networks within 1 seconds.모gls+ does not only outperform the other algorithms in terms of runtime for finding quasi-optimal solutions  but also in terms of solution quality found in a given fixed time. this can be seen from table 1 for all of n  k and p  and is visualized in figure 1 b  for scaling n in random grid networks. this scatter plot compares the original gls  gls+  and s-bbmb all given the same maximum computation time; this is possible since gls+ always finds the quasi-optimal solutions  such that its approximation quality is constantly 1. even though s-bbmb 1  scales better than the original gls  gls+ shows even better scaling behaviour. once again  not only the average quality difference to gls+ grows  but for both gls and s-bbmb  the relative variability also increases with network size.
모these scaling results are further extended in our second experiment  where we studied the impact of the induced width of networks on the performance of gls+ vs. gls and the b&b algorithms. for this  we used networks generated with bngenerator 1 which allows to generate networks with a rather accurate upper bound on the induced width. the results in figure 1 c  show that the induced width has a major effect on algorithm performance  and that gls and gls+ scale much better with induced width than the b&b algorithms  we omit s-aomb and d-aomb which were worse than d-bbmb . we attribute this to the b&b algorithm's mb heuristic whose guidance is impaired for high induced widths.
모in our third experiment we studied real-world networks from the bayesian network repository.1 here  gls+ also performs very well  except for large networks with low induced width  which are easily solved with ve. as we show in table 1  a short preprocessing stage  b   1  significantly improves the performance of gls+ for these structured networks  making it faster than all b&b algorithms  with op-
timal i-bound  for all but one network.1 in particular  note that the link network could not be solved by any of the b&b algorithms  while gls and gls+ find the optimal solution in one cpu second. interestingly  mb 1  already finds a tight upper bound on solution quality  in 1 cpu milliseconds   but with feasible i-bounds never even comes close to a tight lower bound. consequently  only a combination of sls and mb can find the optimal solution and prove its optimality  and this within one cpu second .
1 conclusion and future work
in this work  we identified various weaknesses of gls  the previously best-performing sls algorithm for the mpe problem  and introduced gls+  a novel variant of gls that pays more attention to such important concerns as algorithmic complexity per search step  thorough parameter tuning  and strong guidance by the evaluation function. for a wide range of mpe instances  gls+ widely outperforms gls and the best-performing exact algorithms  all of which are b&b algorithms  that defined the state-of-the-art in mpe solving. most importantly  we demonstrated that the performance of gls+ scales much better than gls and b&b algorithms with the network and domain size  as well as with network density and induced width. gls+ also shows the best performance for real-world instances.
모in contrast to recent claims that stochastic local search algorithms are not competitive for mpe solving  marinescu et al.  1   our results establish sls as a state-of-the-art approach for mpe solving that merits further investigation. in particular  the anytime characteristics and excellent scaling

without the preprocessing  b = 1   we performed 1 additional runs of two hours on the instances diabetes and munin1. out of these  eleven runs solved the diabetes network  but the munin1 network was never solved. this highlights the importance of preprocessing for structured networks.
instancestats
n	kwgls+ with b =1optimalgls+ bnrestvegls
 orig staticbbmb ib	dynamic	ibstaticaomb ib	dynamicibbarley1.1111- - 1/11.1.11/11.1.1diabetes1.1 1 1뫟11/1 - 1/1뫟1--1/1뫟1.1.1link1.1111-1 1 1 1 1----munin111.1.11-11/11.1.11/11.1.1munin111.1.11.1.1.1.1.11/-1.1.11/11munin111.1.11.1.1 - 1/11.1.11/11.1.1munin111 1 111/1 - 1/11 1 1.1.11/11table 1: performance on networks from the bayesian network repository. n is the number of variables of the network  k its average domain size  and w its induced width. for gls+  b is the limit on the number of entries for applying ve in the extended version of gls+ and nrest is the remaining number of variables after the preprocessing with that bound; we report the combined time for preprocessing and search. for each network and sls algorithm  if all of 1 runs found the optimal solution within 1 cpu seconds  we give their average runtime. otherwise  we give the average approximation quality in parentheses. for each b&b algorithm  we report the best result for any i-bound in {1 1 1}. if a b&b algorithm found the optimal solution within 1 cpu seconds  we give the time it required to find it  followed by the time it required to prove optimality  if applicable . if it did not find the optimal solution  we give its approximation quality in parentheses. for each network  we highlight the fastest solution time and the fastest proof time. remarkably  the b&b algorithms often prove optimality shortly after finding the optimal solution  which suggests that this task is only slightly harder. the networks alarm  insurance  hailfinder  mildew  pigs  and water are not listed in this table since they are always solved in well below a second by all algorithms except the original gls  which does not solve mildew.behaviour of gls+ suggest its use for large problems with real-time constraints  such as finding the mpe in pairwise mrfs for early computer vision  boykov et al.  1 . we therefore plan a comparative study of gls+  generalized bp and graph cuts  which currently define the state-of-the-art in that domain  tappen and freeman  1 . furthermore  in the near future we plan to implement an extension of gls+ that computes a diverse set of high-quality mpes-the sls paradigm accommodates such an extension easily  for example  by using the best solutions encountered along the search trajectory.
acknowledgments. we would like to thank radu marinescu for providing implementations of gls and of several b&b algorithms  as well as code for the generation of bayes nets. we also thank james park for providing his original gls code.
