 
this paper proposes and develops a new graph-based semi-supervised learning method.  different from previous graph-based methods that are based on discriminative models  our method is essentially a generative model in that the class conditional probabilities are estimated by graph propagation and the class priors are estimated by linear regression.  experimental results on various datasets show that the proposed method is superior to existing graph-based semi-supervised learning methods  especially when the labeled subset alone proves insufficient to estimate meaningful class priors. 
1 introduction 
in many real world classification tasks  the number of labeled instances is very few due to the prohibitive cost of manually labeling every single data point  while the number of unlabeled data can be very large since they are easy to obtain.  traditional classification algorithms  known as supervised learning  only make use of the labeled data  therefore prove insufficient in these situations.  to address this problem  semi-supervised learning has been developed  which makes use of unlabeled data to boost the performance of supervised learning.  in particular  graph-based semi-supervised learning algorithms have proved to be effective in many applications  such as hand-written digit classification  zhu et al.  1; zhu et al.  1   medical image segmentation  grady and funka-lea  1   word sense disambiguation  niu  ji and tan  1   image retrieval  he et al.  1   etc. 
 compared with other semi-supervised learning methods  such as tsvm  joachims  1   which finds the hyperplane that separates both the labeled and unlabeled data with the maximum margin  graph-based semi-supervised learning methods make better use of the data distribution revealed by unlabeled data.  in graph-based semi-supervised learning  a weighted graph is first constructed in which both the labeled and unlabeled data are represented as vertices.  then many of these methods can be viewed as estimating a function on the graph  zhu  1 .  based on the assumption that nearby points in the feature space are likely to have the same label  the function is defined to be locally smooth and consistent with the labeled data.  finally  the classification labels are obtained by comparing the function value and a pre-specified threshold.  for example  in the gaussian random fields and harmonic function method  the learning problem is formulated in terms of a gaussian random field on the graph  and the mean of the field serves as the function  zhu et al.  1 .  another example is the local and global consistency method  in which the function at each point is iteratively determined by both the information propagated from its neighbors and its initial label  zhou et al.  1 .  yet another example is the graph mincut method whose function corresponds to partitioning the graph in a way that roughly minimizes the number of similar pairs of examples that are given different labels  blum and chawla  1 .  in the mincut method  the function can only take binary values. 
 up till now  graph-based semi-supervised learning methods are generally approached from the discriminative perspective  zhu  1  in that the function on the graph corresponds to posterior probabilities in one way or another.  in the discriminative setting  however  the use of unlabeled data does not necessarily guarantee better decision boundaries.  in addition  there is no clear explanation why the function on the graph should correspond to posterior probabilities from statistics point of view. 
 in this paper  we propose a new graph-based semi-supervised learning method from the generative model perspective.  specifically  the class conditional probabilities and the class priors are estimated from the weighted graph.  the potential advantages involve several aspects: first  it can be theoretically justified that in the ideal cases where the two classes are separable  the output functions in terms of certain eigenvectors of the graph converge to the class conditional probabilities as the number of training data goes to infinity.  in non-ideal cases  our functions still provide a good estimate of the class conditional probabilities.  finally  the estimated class priors make use of both the labeled and unlabeled data  which compensate for the lack of label information in many practical situations.  experimental results show that our approach leads to better performance than other existing graph-based methods on a variety of datasets.  hence we can claim both stronger theoretical justification and better empirical results. 
 	the rest of the paper is organized as follows.  in section 1 and section 1  we introduce how to estimate the class conditional probabilities and the class priors respectively.  section 1 deals with the out-of-sample problem  followed by an outline of the algorithm in section 1.  then the experimental results are shown in section 1.  finally  we give conclusion and hint on future work in section 1. 
1 estimating class conditional probabilities 
1 notation 
in a binary classification problem  suppose that we are given a set of n training samples: x1  xn d .  the first nl samples are labeled  including nl1 positive   yi 1  i 1   nl1   and nl1 nl nl1 negative   yi 1  i nl1   nl   samples.  the remaining nu n nl samples are unlabeled.  our goal is to predict the class labels of these nu points by computing the posterior probability  p y xi i . 
 	by bayes rule  we have 
	p x  i | yi  	p y 	i  	 	 1 
p yi | xi p x  i | yi '  p y  i ' 
	yi ' 1	 
 yi is predicted to be 1 iff p yi 1| xi 1 .  in our generative model  in order to calculate p y xi | i   we need to estimate both p x yi i and p y .  in this section  we focus on estimating the class conditional probability p x yi i   and the estimation of p y will be discussed in the next section. 
 	we first form an affinity matrix w	n n	 with 
wij	x xi  j	  where 	xi xj	 is a non-negative function 
measuring the direct similarity between xi and xj .  then define d as the diagonal matrix  where 
	n	1/ 1/1
dii j 1 w iij   1  n   and s d wd .  finally define f and f as two n -dimensional vectors.  the element of f   f   is set to 1 iff the corresponding point is a positive  negative  labeled one. 
1 the ideal case 
to start with  let us first consider the ideal case where the two classes are far apart.  in this case  we have the following equation: 
	 	 1 
where yx is the observed class label of data point x .  based on this assumption  if xi and xj are from two different classes  the corresponding wij  1 .  therefore if we knew the labels of all the samples and put together the samples from the same class  the affinity matrix w   and thus the 
symmetric matrix s would be block-diagonal.  to be specific  let 
	w  	 1 
where w1 and w1 represent the sub-matrices corresponding to the positive and negative samples respectively  and 1 represents zero matrix.  if the total number of positive  negative  samples in the training set is n1   n1    w1  w1   is an n n1   n n1   square matrix.  let d1 and d1 be two diagonal matrices  the diagonal elements of which are the row sums of w1 and w1 .  then s can be written as 
	s	s1	d1/ 1w d1 1/ 1/ 1	1/ 1 	 1 
	1	s1	d1	w d1
 the following theorem connects the class conditional probabilities with the diagonal elements of d . 
theorem 1. if 	xi  xj	xi	xj	n	vn   where 	n and 
vn are positive parameters  and the function  satisfies the following conditions: u 1   u du 1   supu u   lim u id 1ui 1   nlimvn 1   nlimnvn   as 
u
the number of examples n goes to infinity  d nii yi converges to p x yi i . 
 the proof of the theorem is straightforward and therefore we put it in the appendix.  notice that this theorem is similar to a result in kernel density estimation.  the difference is that in kernel density estimation  we only have labeled data from a single class; while in our situation  we have both labeled and unlabeled data  and we could estimate the class conditional distributions of the two classes at the same time.  suppose that the labeled data are noise-free.  according to theorem 1  we can use dii to approximate the class conditional probability of xi given the observed label yi .  how-
ever  for the unlabeled points  we do not know if dii corresponds to p x yi i 1 or p x yi i 1 .  to address this problem  we can make use of the eigenvectors of s . 
 it is easy to show that the largest eigenvalue of s1 and s1 is 1  and if w1 and w1 form a connected graph respectively  
the corresponding eigenvectors would be v d1/ 1 and v d1/ 1  chung  1 .  based on v and v   we can construct two eigenvectors of s with eigenvalue 1: 
	t	t
	v1	vt	1	  v1 vt	 	 1 

where 1 is a zero vector.  notice that if we square v1 and v1 by elements to get v1 and v1   and then add them up  we get 
	v1 v1 d1 d1 d 1 	 1 
 obviously  v1 and v1 correspond to p x yi i 1 and p x yi i 1 respectively  and their non-zero elements are equal to dii . 
 to get v and v   we perform f s f and f s f until convergence.  since the initial value of f is not orthogonal to v1  the elements of f and v1 are non-negative   f  will converge to v1 .  similarly  f will 
                                                                                   1 converge to v1 .  therefore  upon convergence  fi	   fi	  
is in proportion to the class conditional probability of the 
                                     1 positive  negative  class.  after normalizing fi	   fi	  so that it sums to 1  we have an empirical estimation of p x yi	i	1   p x yi	i	1    which converges to its true value as n goes to infinity. 
 figure 1 gives an example of density estimation in the ideal case.  figure 1 a  shows the training data  where the two moons represent two classes  and each class has one labeled example marked as star.  figure 1 b  and 1 c  show the estimated class conditional distributions of the two classes. 

	 b  	 c  
figure 1. density estimation in the ideal case.  a : training data;  b  and  c  class conditional distributions 
1 the general case 
in the general cases  the two classes are not far apart  and we have the following theorem. 
theorem 1. if xi xj satisfies the conditions in theorem 1  as the number of samples n goes to infinity  dii n con-
verges to p x yi	i	1 p y	1	p x yi	i	1 p y	1 
¡¡the proof to this theorem is quite similar to theorem 1.  so we omit the details here.  it can be seen easily that theorem 1 is a special case of theorem 1 when the two classes are far apart  i.e. 
	lim d n p x yii	i	i	1 p y 1   if yi	1 
n	 1  lim d n p x yii i i 1 p y 1   if yi 1 
n
 equation  1   together with the fact that limn n p y1   
leads to theorem 1. 	n
 in the general cases  w tends to form one connected graph instead of two  and s only has one eigenvector that corresponds to eigenvalue 1.  if we still iterate f s f and f s f until convergence  both f and f will converge to the same eigenvector.  on the other hand  the operation of f s f and f s f can be seen as the labeled data gradually spreading their information to nearby points.  if the iteration steps are unlimited  every data point will be equally influenced by the positive and negative labeled data  leading to the same value of f  and f  .  to solve this problem  in our algorithm  we have designed a stopping criterion  and the iteration process is stopped once the criterion is satisfied.  to be more specific  when estimating the class conditional probabilities of the positive class  we could get an estimate of p x yi i 1 in each iteration step 
1
 by normalizing f so that it sums to 1 .  by summing up this probability for negative labeled samples  we have the average likelihood of these samples in the positive class: 
l nl 1 n .  we stop the iteration when the second derivative of l with respect to the iteration steps crosses 1.  this criterion can be justified as follows: in the initial iteration steps  only a few negative data get positive score from their nearby positive labeled points  so the rate at which l increases is very low; as the iteration proceeds  those negative data have accumulated high scores and propagate to the majority of negative points  so the rate gradually increases; finally  as f  begins to converge  its value at each data point becomes stable  so the rate decreases until it reaches 1.  if we plot the curve of l with respect to the number of iteration steps  the shape would be convex first  and then concave until convergence  figure 1 b  .  notice that in the initial iteration steps  the positive points  which are far away from the positive labeled points but connected to them via some kind of manifold  cannot get positive scores.  if the algorithm stops at this stage  it may not fully explore the data distribution and cause misclassification on certain clusters of data.  therefore we choose the transition point between convex and concave as the stopping point in order to trade off between prematurity and excessive propagation.  the stopping criterion for the negative class can be derived 
similarly  i.e. l nl1 p x yi i 1 nl1 .  a key point in our algorithm is that the estimation of the class conditional probabilities of the two classes is independent  i.e. the numbers of iteration steps when the two stopping criterions are satisfied are not necessarily the same.  figure 1 gives an example of density estimation in the general case showing the effectiveness of our criterion.  this example is quite similar to the one shown in figure 1 except that the two classes are not far apart.  figure 1 b  shows the value of l  the upper curve  and l  the lower curve  in each iteration step.  the arrows point to the positions in the curves where the two criterions are satisfied.  figure 1 c  and 1 d  show the estimated class conditional distributions of the two classes.  although there are small gaps in the middle of the distributions  the moon structure is recovered fairly well. 

	 c  	 d  
figure 1. density estimation in the generation case.  a : training data;  b : l and l in each iteration;  c  and  d : class conditional distributions. 
 note that the stopping criterion discussed above is based on simple heuristics.  currently we are trying to design a stopping criterion in a more principled manner. 
1 estimating class priors 
in this section  we focus on estimating the class prior p y .  existing graph-based semi-supervised learning methods only use the labeled set to estimate the class priors  either explicitly  zhu et al.  1  or implicitly  zhou et al.  1 .  obviously  in real applications  the proportion of positive and negative labeled data is often far from the true class priors.  in our algorithm  we use both the labeled and unlabeled data to estimate the class priors.  according to theorem 1  once we have estimated the class conditional probability p x yi i   we can feed them into the following equations and form a linear regression problem  the solution of which is equal to the least squares estimator of p y 1 . 
	dii n	p x yi	i	1 p 	p x yi	i	1	p    i	1 	 n 	 1 
 however  when the number of labeled data is small  the estimated class conditional probabilities may not be very accurate  and thus p  is not very reliable.  to solve this problem  we use a beta distribution as the prior distribution for p y 1   the parameters of which are p  and 1 p  .  then the estimate of p y 1 based on the labeled set: 
	p n 	l1	 1 
	p y 1	  p y 1 p y 1 
1 nl
which is equivalent to smoothing the proportion of the positive and negative samples in the labeled set.  when the number of labeled data is small  unlabeled data can be fully exploited to compensate for the proportion in the labeled set that is not the same as the class priors; when the number of labeled data is large  labeled data will dominate the estimation of the class priors. 
1 prediction of new testing data 
to classify a data point x d that is not present during the training stage  we first calculate its class conditional probabilities via kernel regression: 
	n	x x p x yi	i
p x y	i 1	n	 	 1  x x  i i 1
 using these class conditional probabilities and the class priors obtained during the training stage  we can calculate the posterior probability and make a prediction. 
1 the algorithm 
the procedures for estimating p x yi i and p y are summarized in table 1 and table 1 respectively. 
1. form 	the 	affinity 	matrix w	n n	  	where 
	wij	x xi  j .  calculate d and s . 
1. initialize f and f .  the element of f   f   is set to 1 if the corresponding point is a positive  negative  labeled one  and 1 otherwise. 
1. update f	s f   f	s f . 
1. assign p x yi i 1 fi 1   p x yi i 1   fi  1   and normalize so that  in p x yi i 1   
1
	in p x yi	i	1. 
1
1. calculate the average likelihood of negative  positive  labeled points in the positive  negative  class: 
	l	i nnl	l1p x yi	i	1 nl1   l	inl1p x yi	i	1 nl1  .
go to step 1 unless one of the following conditions is satisfied: 
a. l   l   remains at 1  and f	   f	  has converged; 
b. l   l   does not remain at 1  and the second derivative of l   l   with respect to the iteration steps crosses 1. 
1. output p x yi	i	1 and p x yi	i	1 . 
	table 1. description of estimation for p x yi	i 
1. solve the following linear regression problem for the least squares estimator p  of p y 1 : 
	   p p x y 	i	i	1	p 	p x yi	i	1	dii n i 	1 	 n 
1. calculate the class priors as the smoothed proportion of the positive and negative samples in the labeled set 

1 experimental results 
in this section  we present the comparative experimental results on two datasets: cedar buffalo binary digits database  hull  1   and a document genre-classification dataset  liu et al.  1 .  our algorithm is compared with two other graph-based semi-supervised learning methods: gaussian random fields  zhu et al.  1  and the local and global consistency method  zhou et al.  1 .  we did not compare with supervised learning methods  such as one nearest neighbor  since they have been proved to be less effective than gaussian random fields based on experimental results  zhu et al.  1 . 
 we have designed two kinds of experiments: balanced and unbalanced.  in the balanced case  the ratio of labeled points from each class is always the same as the class priors; in the unbalanced case  if not explained otherwise  we fix the total number nl of labeled points  and perturb the number of positive labeled points around nl 1 with a gaussian distribution of mean 1 and standard deviation nl 1 .  in each experiment  we gradually increase the number of labeled data  perform 1 trials for each labeled data volume  and average the accuracy at each volume point. 
1 cedar buffalo binary digits database 
we first perform experiments on cedar buffalo binary digits database  hull  1  including two classification tasks: classifying digits  1  vs  1   with 1 images in each class; and odd vs even digits  with 1 images in each class  1 images for each digit .  the data we use are the same as those used in  zhu et al.  1 .  here 
average distance between each data point and its 1 nearest neighbors. 
case  while the performance of our algorithm is comparable to the balanced case.  this is because the class mass normalization procedure adopted in gaussian random fields depends on the labeled set only to estimate the class priors; while our algorithm makes use of both the labeled and the unlabeled set to estimate the class priors.  therefore  it is more robust against the perturbation in the proportion of the positive and negative data in the labeled set. 
1 genre dataset 
genre classification is to classify the documents based on its writing styles  such as political articles and movie reviews.  the genre dataset that we use consists of documents from 1 genres  including biographies  b   interview scripts  is   movie reviews  mr   product reviews  pr   product press releases  ppr   product descriptions on store websites  pd   political articles on newspapers  pa   editorial papers on politics  ep   news  n   and search results from multiple search engines using 1 queries  sr .  we randomly select 1 documents from each category to compose the whole dataset of 1 documents.  each document is processed into a  tf.idf  vector  which is generated based on the top 1 most frequent words in this dataset after stemming  with the header and stop words removed.  here 
figure 1. unbalanced classification.  a : pa vs other;  b  b vs other figure 1 a  and 1 b  correspond to the balanced and unbal-
anced cases respectively.  in the balanced case  gaussian random fields is better than our algorithm and the local and global consistency method.  this might be because the function xi xj does not have some of the nice properties required by theorem 1.  however  in the unbalanced case  gaussian random fields tends to suffer a lot.  on the contrary  our algorithm is quite robust despite of the perturbation.  in figure 1  we try to classify pa and b against all the other categories.  in these experiments  the class priors are 1 for the positive class and 1 for the negative class.  however  here we provide equal numbers of positive and negative points in the labeled set.  from the figures  we can see that the performance of our algorithm is rather stable  while the performance of both gaussian random fields and the local and global consistency method is largely affected by the misleading labeled set  since they only depend on the labeled set to estimate the class priors  either explicitly or implicitly. 
1 conclusion and future work 
in this paper  we propose a novel graph-based semi-supervised learning method to estimate both the class conditional probabilities and the class priors.  it is a generative model  in contrast to existing graph-based methods  which are essentially discriminative.  in the ideal case  the estimated class conditional probabilities have been proved to converge to the true value.  in the general case  our algorithm can still output reasonable estimates of the class conditional probabilities.  for data points outside the training set  the class conditional probabilities are estimated via kernel regression.  when estimating the class priors  we effectively use the unlabeled data to make up for the labeled data with unrepresentative class prior distributions.  experimental results on two datasets demonstrate the superiority of our algorithm over recent existing graph-based semi-supervised learning methods  especially when the proportion in the labeled set is not the same as the class priors. 
 in our experiments  we notice that in some cases  adding even a single labeled point into the labeled set brings about significant improvement in classification accuracy; while in other cases  adding many labeled points into the labeled set does not help improve the performance.  currently we are incorporating active learning into our framework.  particularly  we are interested in determining when to invoke active learning  not just which instances to label  in order to achieve the biggest gain while minimizing incremental labeling cost. 
