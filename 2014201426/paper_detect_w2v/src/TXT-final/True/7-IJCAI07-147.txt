
bayesian network classifiers  bnc  have received considerable attention in machine learning field. some special structure bncs have been proposed and demonstrate promise performance. however  recent researches show that structure learning in bns may lead to a non-negligible posterior problem  i.e  there might be many structures have similar posterior scores. in this paper  we propose a generalized additive bayesian network classifiers  which transfers the structure learning problem to a generalized additive models  gam  learning problem. we first generate a series of very simple bns  and put them in the framework of gam  then adopt a gradient-based algorithm to learn the combining parameters  and thus construct a more powerful classifier. on a large suite of benchmark data sets  the proposed approach outperforms many traditional bncs  such as naive bayes  tan  etc  and achieves comparable or better performancein comparison to boosted bayesian network classifiers.
1 introduction
bayesian networks  bn   also known as probabilistic graphical models  graphically represent the joint probability distribution of a set of random variables  which exploit the conditional independence among variables to describe them in a compact manner. generally  a bn is associated with a directed acyclic graph  dag   in which the nodes correspond to the variables in the domain and the edges correspond to direct probabilistic dependencies between them  pearl  1 .
¡¡bayesian network classifiers  bnc  characterize the conditional distribution of the class variables given the attributes  and predict the class label with the highest conditional probability. bncs have been successfully applied in many areas. naive bayesian  nb   langley et al.  1  is the simplest bn  which only consider the dependence between each feature xi and the class variable y. since it ignores the dependence between different features  nb may perform not well on data sets which violate the independence assumption. many bncs have been proposed to overcome nb's limitation.  sahami  1  proposed a general framework to describe the limited dependence among feature variables  called k-dependence bayesian network  kdb .  friedman et al.  1  proposed tree augmented naive bayes  tan   a structure learning algorithm which learns a maximum spanning tree  mst  from the attributes. both tan and kdb have tree-structure graph. k1 is an algorithm which learns general bn for classification purpose  cooper and herskovits  1 .
¡¡the key differences between these bncs are their structure learning algorithms. structure learning is the task of finding out one graph structure that best characterizes the true density of given data. many criteria  such as bayesian scoring function  minimal description length  mdl  and conditional independence test  cheng et al.  1   have been proposed for this purpose. however  it is inevitable to encounter such a situation: several candidate graph structures have very close score value  and are non-negligible in the posterior sense. this problem has been pointed out and presented theoretic analysis by  friedman and koller  1 . since candidate bns are all approximations of the true joint distribution  it is natural to consider aggregating them together to yield a much more accurate distribution estimation. several works have been done in this manner. for example   thiesson et al.  1  proposed mixture of dag  and  jing et al.  1  proposed boosted bayesian network classifiers.
¡¡in this paper  a new solution is proposed to aggregate candidate bns. we put a series of simple bns into the framework of generalized additive models  hastie and tibshirani  1   and adopt a gradient-based algorithm to learn the combining parameters  and thus construct a more powerful learning machine. experiments on a large suite of benchmark data sets demonstrate the effectiveness of the proposed approach.
¡¡the rest of this paper is organized as follows. in section 1  we briefly introduce some typical bncs  and point out the non-negligible problem in structure learning. in section 1  we propose the generalized additive bayesian network classifiers. to evaluate the effectiveness of the proposed approach  extensive experiments are conducted in section 1. finally  concluding remarks are given in section 1.
1 bayesian network classifiers
a bayesian network b is a directed acyclic graph that encodes the joint probability distribution over a set of random variables x =  x1  ¡¤ ¡¤ ¡¤   xd t. denote the parent nodes of xi by pa xi   the joint distribution pb x  can be represented by factors over the network structures as follows:

d
pb x  =p xi|pa xi  .
i=1
¡¡given data set d = { x y } in which y is the class variable  bncs characterize d by the joint distribution p x y   and convert it to conditional distribution p y|x  for predicting the class label.
1 several typical bayesian network classifiers
the naive bayesian  nb  network assumes that each attribute variable only depends on the class variable  i.e 

d
p x y  = p y p x|y  = p y p xi|y . i=1
figure 1 a  illustrates the graph structure of nb.
¡¡since nb ignores the dependencies among different features  it may perform not well on data sets which violate the attribute independence assumption. many bncs have been proposed to consider the dependence among features.  sahami  1  presented a more general framework for limited dependence bayesian networks  called k-dependence bayesian classifiers  kdb .
definition 1: a k-dependence bayesian classifier is a bayesian network which allows each feature xi to have a maximum of k feature variables as parents  i.e.  the number of variables in pa xi  equals to k+1  '+1' means that k does not count the class variable y .
¡¡according to the definition  nb is a 1-dependencebn. the kdb  sahami  1  algorithm adopts mutual information i xi;y  to measure the dependence between the ith feature variable xi and the class variable y  and conditional mutual information i xi  xj|y  to measure the dependence between two feature variables xi and xj. then kdb employs a heuristic rule to construct the network structure via these two measures. kdb does not maximize any optimal criterion in structure learning. hence  it yields limited performance improvement over nb.  keogh and pazzani  1  proposed super-parent bayesian networks  spbn   which assumes that there is an attribute acting as public parent  called super-parent  for all the other attributes. suppose xi is the super parent  denote the corresponding bn as pi x y   we have
	pi x y 	=	p y p xi|y p x i|xi y 

n
	=	p y p xi|y p xj|xi y .	 1 
j=1 ji
it is obvious that spbn structure is a special case of kdb  k=1 . figure 1 b  illustrates the graph structure of spbn. the spbn algorithm adopts classification accuracies as the criterion to select out the best network structure.
¡¡ friedman et al.  1  proposed tree augmented naive bayes  tan   which is also a special case of kdb  k=1 . tan attempts to add edges to the naive bayesian network in order to improve the posterior estimation. in detail  tan first computes the conditional mutual information i xi  xj|y  between any two feature variables xi and xj  and thus obtain a full adjacency matrix. then tan employs the minimum spanning tree algorithm  mst  on the adjacency matrix to obtain a tree-structure bn. therefore  tan is optimal in the sense of mst. many experiments show that tan significantly outperforms nb. figure 1 c  illustrates one possible graph structure of tan.
¡¡both kdb and tan generate tree-structure graph.  cooper and herskovits  1  proposed the k1 algorithm  which adopts the k1 score measure and exhaustive search to learn general bn structures.
1 the structure learning problem
given training data d  structure learning is the task of finding a set of directed edges g that best characterizes the true density of the data. generally  structure learning can be categorized into two levels: macro-level and micro-level. in the macro-level  several candidate graph structures are known  and we need choosing the best one out. in order to avoid overfitting  people often use model selection methods  such as bayesian scoring function  minimum descriptive length  mdl   etc  friedman et al.  1 . in the micro-level  structure learning cares about whether one edge in the graph should be existed or not. in this case  people usually employ the conditional independence test to determine the importance of edges  cheng et al.  1 .
¡¡however  in both cases  people may face such a situation that several candidates  graphs or edges  have very close scores. for instance  suppose mdl is used as the criterion  people may encounter a situation that two candidate bn structure g1 and g1 have mdl score 1 and 1  respectively. which one should be chosen  someone may say that it is natural to select g1 out since it has a bit smaller mdl score  but practice may show that g1 and g1 have similar performance  and g1 may perform even better in some cases. in fact  both of them are non-negligible in the posterior sense.
¡¡this problem has been pointed out and presented theoretic analysis by  friedman and koller  1 . it shows that when there are many models that can explain the data reasonably well  model selection makes a somewhat arbitrary choice between these models. besides  the number of possible structures grows super-exponentially with the number of random variables. for these two reasons  we don't want to do structure learning directly. we hope aggregating a series of simpler and weaker bns together to obtain a much more accurate distribution estimation of the underlying process.
¡¡we note that several researchers have proposed some schemes for this purpose  for examples  learning mixtures of dag  thiesson et al.  1   or ensembles of bayesian networks by model averaging  rosset and segal  1; webb et al.  1 . we briefly introduce them in the following.
1 model averaging for bayesian networks
since candidate bns are all approximations of the true distribution  model averaging is a natural way to combine candidates together for a more accurate distribution estimation.
mixture of dag  mdag 
definition 1: if p x| ¦Èc gc  is a dag model  the following equation defines a mixture of dag model
p x|¦Ès  =  ¦Ðcp x|¦Èc gc   c
where ¦Ðc is the prior for the c-th dag model gc  and ¦Ðc ¡Ý 1  c ¦Ðc = 1  ¦Èc is the parameter for graph gc.

figure 1: typical bayesian network structures¡¡mdag learns the mixture models via maximizing the posterior likelihood of given data set. in detail  mdag combining uses the chesseman-stutz approximation and the expectation-maximization algorithm for both the mixture components structure learning and the parameter learning.
¡¡ webb et al.  1  presented a special and simple case of mdag for classification purpose  called the average one dependenceestimation  aode . aode adopts a series of fixstructure simple bns as the mixture components  and directly assumes that all mixture components in mdag have equal mixture coefficient. practices show that aode outperforms naive bayes and tan.
boosted bayesian networks
boosting is another commonly used technique for combining simple bns.  rosset and segal  1  employed the gradient boosting algorithm  friedman  1  to combine bns for density estimation.  jing et al.  1  proposed boosted bayesian network classifiers  bbn   and adopted general adaboost algorithm to learn the weight coefficients.
¡¡given a series of simple bns: pi x y   i = 1  ¡¤ ¡¤ ¡¤  n  bbn aims to construct the final approximation by linear additive models: p x y  = ni=1 ¦Áipi x y   where ¦Ái ¡Ý 1 are weight coefficients  and i ¦Ái = 1. more generally  the constraint on ¦Ái can be relaxed  but only ¦Ái ¡Ý 1 is kept: f x y  = ni=1 ¦Áipi x y . in this case  the posterior can be defined as follows
	p.	 1 
y exp{f x y  }
for general binary classification problem y ¡Ê {-1}  this problem can be solved by the exponent loss function
	l ¦Á  =  exp{ yf xk y }	 1 
k
via the adaboost algorithm  friedman et al.  1 .
1 generalized additive bayesian networks
in this section  we present a novel scheme that can aggregate a series of simple bns to a more accurate density estimation of the true process. suppose pi x y   i = 1  ¡¤ ¡¤ ¡¤  n are the given simple bns  we considerputting them in the framework of generalized additive models  gam   hastie and tibshirani  1 . the new algorithm is called generalized additive bayesian network classifiers  gabn .
¡¡in the gam framework  pi x y  are considered to be linear additive variables in the link function space:

n
	f x y  =¦Ëi fi pi x y  .	 1 
i
¡¡gabn is an extensible framework since many different link functions can be considered. in this paper  we study a special link function: fi ¡¤  = log ¡¤ . defining z =  x y  and taking exponent on both sides of the above equation  we have
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡n	n	¦Ëi exp f z   = expi ¦Ëi fi z  = i pi  z .
this is in fact a potential function. it can also be written as a probabilistic distribution when given a normalization factor 
n
	1	¦Ëi
	p z  = s ¦Ë z   pi  z  	 1 
i
where s ¦Ë z  is the normalization factor:
	n	n
	s ¦Ë z  =  p¦Ëi i z  = exp¦Ëi log pi z .	 1 
	z	i	z	i=1
the likelihood of p z  is called quasi-likelihood:

n
	l ¦Ë 	=log p zk 
k=1
n

n
=¦Ëi log pi zk    logs ¦Ë zk  k=1 i=1

n
=¦Ë ¡¤ f zk    logs ¦Ë zk  	 1  k=1
where ¦Ë =  ¦Ë1  ¡¤ ¡¤ ¡¤  ¦Ën t  f zk  =  f1 zk   ¡¤ ¡¤ ¡¤   fn zk  t.
1 the quasi-likelihood optimization problem
maximizing the quasi-likelihood  we can obtain the solution of the additive parameters. to make the gam model meaningful and tractable  we add some constraints to the parameters. the final optimization problem turns to be:
	max	l ¦Ë 
	s.t.	 1  1 ¡Ü ¦Ëi ¡Ü 1	 1 
 1  i ¦Ëi = 1
¡¡for equation constraint  the lagrange multiplier can be adopted to transfer the problem into an unconstraint one; while for inequation constraints  classical interior point method  ipm  can be employed. in detail  the ipm utilizes barrier functions to transfer inequation constraints into a series of unconstraint optimization problems  boyd and vandenberghe  1 .
¡¡here  we adopt the most used logarithmic barrier function  and obtain the following unconstraint optimization problem:

	n	n
l ¦Ë rk ¦Á 	=	rklog ¦Ëi  + rk 	log 1   ¦Ëi  i=1	i=1 n
+	¦Á 1  ¦Ëi  + l ¦Ë  i=1
	=	rk	log ¦Ë  + log 1n   ¦Ë   ¡¤ 1n
	+	¦Á 1   ¦Ë ¡¤ 1n  + l ¦Ë  	 1 
where 1n indicates a n-dimensional vector with all elements equal to 1  rk is the barrier factor in the kth step of the ipm iteration  and ¦Á is the lagrange multiplier.
¡¡therefore  in the kth ipm iteration step  we need to maximize an unconstraint problem l ¦Ë rk ¦Á . quasi-newton method is adopted for this purpose.
1 quasi-newton method for the unconstraint optimization problem
to solve the unconstraint problem: max l ¦Ë rk ¦Á   we must have the gradient of l w.r.t ¦Ë.
theorem 1: the gradient of l ¦Ë rk ¦Á  w.r.t ¦Ë is
	 l ¦Ë rk ¦Á 	n
		=	g¦Ë = 	f zk    ep z  f zk  
	 ¦Ë	k=1
	1
+	rk	 	  ¡¤ 1n   ¦Á1n.	 1  ¦Ë	1n   ¦Ë
proof: in equation  1   it is easy to obtain the gradient of the first summation term and non-summation terms. here  we only present the gradient solution of the second summation term  i.e.  logs ¦Ë zk  in l ¦Ë .

=
	 ¦Ë	s ¦Ë z 	 ¦Ë
¡ß s ¦Ë z  =  exp¦Ë ¡¤ f z 
z
 f zk exp¦Ë ¡¤ f zk 
zk
1
 f zk exp¦Ë ¡¤ f zk 
	 ¦Ë	s ¦Ë z  zk
	=	 k p zk f zk  = ep z 	f zk  . 
z
¡¡for computational cost consideration  we did not further compute the second order derivative of l ¦Ë rk ¦Á   while adopted the quasi-newton method  bishop  1  to solve the problem. in this paper  the l-bfgs procedure provided by  liu and nocedal  1  is employed for this task.
1 the ipm based training algorithm
the interior point method starts from a point in the feasible region  sequentially adjusts the barrier factor rk in each iteration  and solves a series unconstraint problem l ¦Ë rk ¦Á   k = 1  ¡¤ ¡¤ ¡¤. the detailed training algorithm is shown in table 1.
table 1: the training algorithm for gabn
input: given training set d = { xi yi }in=1
training algorithm
s1: set convergence precision    1  and the maximal step m; s1: initialize the interior point as ¦Ë =  ¦Ë1 ¡¤¡¤ ¡¤  ¦Ën t   ¦Ëi = 1/n;
s1: generate a series of simple bns: pi x y   i = 1  ¡¤¡¤¡¤  n;
s1: for k = 1 : m
s1: select rk   1 and rk   rk 1  obtain the kth step optimization problem l ¦Ë rk ¦Á ;
s1: calculate g¦Ë and the quasi-likelihood l ¦Ë ;
s1: employ l-bfgs procedure to solve: maxl ¦Ë rk ¦Á ;
s1: test of the barrier term ak = rk	log ¦Ë  + log 1n   ¦Ë   ¡¤ 1n;
¡¡¡¡s1: if ak    jump to s1  else continue the loop; s1: output the optimal parameter ¦Ë   and obtain the final generalized models p z;¦Ë  .1 a series of fix-structure bayesian networks
there are one unresolved problem in the algorithm listed in table 1  which is in the s1 step  i.e  how to generate a series of simple bns as the weak learner. there are many methods for this purpose. in our experiments  we take super parent bn as the weak learner. readers may consider other possible strategies to generate simple bns.
¡¡for a d-dimensional data set  when setting different attribute as the public parent node according to equation  1   it can generate d different fix-structure super-parent bns: pi x y   i = 1  ¡¤ ¡¤ ¡¤  d. figure 1 b  depicts one example of this kind of simple bns. to improve performance  mutual information i xi y  is computed for removing several bns with lowest mutual information score. in this way  we obtain n very simple bns  and adopt them as weak learners in gabn.
¡¡parameters  conditional probabilistic table  learning in bns is common  and thus details are omitted here. note that for robust parameter estimation  laplacian correction and mestimate  cestnik  1  are adopted.
1 discussions
gabn has several advantages over the typical linear additive bn models: boosted bn  bbn . first  gabn is much more computational efficient than bbn. given d-dimensional and n samples training set  it is not hard to prove that the computational complexity of gabn is o nd1 + mnd   where m is the ipm iteration steps. on the contrary  bbn requires sequentially learning bn structures in each boosting step. this leads to a complexity of o knd1   where k is the boosting step  which is usually very large  in 1 magnitude . therefore  gabn dominates bbn on scalable learning task. practice also demonstrates this point.
¡¡furthermore  gabn presents a new direction for combining weaker learners since it is a highly extensible framework. we present a solution for logarithmic link function. it is not hard to adopt other link functions under the gam framework  and thus propose new algorithms. many existing gam properties  optimization methods can be seamlessly adopted to aggregate simple bns for more powerful learning machines.
1 experiments
this section evaluates the performance of the proposed algorithm  compared it with other bncs such as nb  tan  k1  kdb  spbn; model averaging methods such as aode  bbn; and decision tree algorithm cart  breiman et al.  1 .
¡¡the benchmark platform was 1 data sets from the uci machine learning repository  newman et al.  1 . one point should be indicated here: forbncs  when data sets have continuous features  we first adopted discretization method to transfer them into discrete features  dougherty et al.  1 . we employed 1-fold cross-validation for the error estimation  and kept all compared algorithms having the same fold split. the final results are shown in table 1  in which the results by tan and k1 are obtained by the java machine learning toolbox weka  witten and frank  1 .
¡¡to present statistical meaningful evaluation  we conducted the paired t-test to compare gabn with others. the last row of table 1 shows the win/tie/lose summary in 1% significance level of the test. in addition  figure 1 illustrates the scatter plot of the comparison results between gabn and other classifiers. we can see that gabn outperforms most other bncs  and achieves comparable performance to bbn. specially note  the spbn column shows results by the best individual super-parent bn  which are significant worse than gabn. this demonstrates that it is effective and meaningful to use gam for aggregating simple bns.
1 conclusions
in this paper  we propose a generalized additive bayesian network classifiers  gabn . gabn aims to avoid the nonnegligible posterior problem in bayesian network structure learning. in detail  we transfer the structure learning problem to a generalized additive models  gam  learning problem. we first generate a series of very simple bayesian networks  bn   and put them in the framework of gam  then adopt a gradient-based learning algorithm to combine those simple bns together  and thus construct a more powerful classifiers. experiments on a large suite of benchmark data sets demonstrate that the proposed approach outperforms many traditional bncs such as naive bayes  tan  etc  and achieves comparable or better performance in comparison to boosted bayesian network classifiers. future work will focus on other possible extensions within the gabn framework.
