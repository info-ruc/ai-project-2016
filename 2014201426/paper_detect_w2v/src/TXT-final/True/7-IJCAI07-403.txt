
we employed a multilevel hierarchical bayesian model in the task of exploiting relevant interactions among high cardinality attributes in a classification problem without overfitting. with this model  we calculate posterior class probabilities for a pattern w combining the observations of w in the training set with prior class probabilities that are obtained recursively from the observations of patterns that are strictly more generic than w. the model achieved performance improvements over standard bayesian network methods like naive bayes and tree augmented naive bayes  over bayesian networks where traditional conditional probability tables were substituted by noisy-or gates  default tables  decision trees and decision graphs  and over bayesian networks constructed after a cardinality reduction preprocessing phase using the agglomerative information bottleneck method.
1 introduction
in most countries  imported goods must be declared by the importer to belong to one of large set of classes  customs codes . it is important that each good is correctly classified  because each of the customs codes mean not only different customs duties but also different administrative  sanitary  and safety requirements. the goal of this project is to develop a tool that  considering four attributes: declared custom code  dcc   importer  imp   country of production  cp  and entry point in the receiving country  epr   will estimate  for each new example  the probability that it involves a misclassification. such estimates will be used latter by a larger system that allocates human resources for different types of anti-fraud operations.
모our data set has 1 examples of correct classification  which we will call negative examples  and 1 examples of misclassification  positive examples . in this dataset  the first attribute has 1 distinct values  the second  1 values  the third  1 values  and the fourth 1 values.
모with only 1% of positive examples  dataset is imbalanced what is usually handled with different resampling strategies  chawla et al.  1 . however  resampling requires retraining the classifiers for each different assignment of costs for false positives and false negatives. in our context  such costs are not known in advance  priorities changes acording to other anti-fraud demands  and they may vary from example to example  not all false negatives cost the same . thus we cannot train the classifiers for all possible cost assignments in advance.
모on the other hand  if we can produce reliable probability estimates directly from the original dataset the work of the human resource allocation system becomes much easier. it can  for example  at any time  define a selection rate sr that matches the available human resources for the specific task of detecting wrong customs codes considering all other antifraud demands at the moment. the examples to be verified will naturally be the sr examples that are most likely to involve a misclassification. the allocation system may also combine the probability estimates with costs that may vary from to example to example without any retraining. it becomes also unnecessary that customs administration discuss their cost criteria with us. thus we decided to concentrate on bayesian techniques and not to use resampling or any other technique that requires retraining when costs change.
모domain specialists claim that there are combinations of attributes values  some involving all of them  that make the probability of an instance being positive significantly higher then it could be expected looking at each value separately. they call such combinations critical patterns. to benefit from critical patterns we would like to use a bayesian network  bn  pearl  1  where all attribute nodes are parents of the class node. we call such structure the direct bn structure.
모in a bn  considering that xji is a possible value for node xj and 뷇jk is a complete combination of values for 붫j  the set of parents of node xj  the vector  붿jk  such that 붿jki = p xji|뷇jk   contained in the cpt of a node xj  is assessed from the frequencies of the values of xj among the training instances where 붫j = 뷇jk. the distributions of xj given any two different combinations of values for the parents of xj are assumed to be independent and a dirichlet prior probability distribution for 붿jk is usually adopted. applying bayes rule and integrating over all possible values for 붿jk it is found that:
		 1 
where njki is the number of simultaneous observations of xji and 뷇jk in the training set is the value of one of the parameters of the dirichlet prior probability distribution and  the equivalent sample size of the prior probability distribution.
모the dirichlet prior probability distribution is usually assumed to be noninformative  what yields to:
		 1 
where all parameters of dirichlet distribution are equal to a small smoothing constant뷂  andmj is the numberof possible values for node xj. we call this direct estimation  de .
모in the direct bn structure the node whose cpt is to be estimated is the class node and all other attribute nodes are its parents. the conditional probability table  cpt  of the class node in such a structure contains more than 1 뫄 1 parameters. it is clear that for rarely seen combinations of attributes the choice of the structure in the direct bn structure and equation 1 tends to produce unreliable probabilities whose calculation is dominated by the noninformative prior probability distribution. this suggests that using the direct bn structure and traditional cpts we will have overfitting problems.
모instead of the direct bn structure  we can choose a network structure that does not lead to too large tables. this can be achieved limiting the number of parents for a network node. naive bayes  duda and hart  1  is an extreme example where the maximum number of parents is limited to one  the class node is the only parent of any other node . tree augmented naive bayes  tan   friedman et al.  1  adds a tree to the structure of naives bayes connecting the nonclass attributes  and thus limits the maximum number of parent nodes to two. however  limiting the maximum number of parents also limits our ability to capture interactions among attributes and benefit from critical patterns. thus  we would prefer not to do it.
모since the high cardinality of our attributes is creating trouble  it is a reasonable idea to preprocess the data  reducing the cardinality of the attributes. we can use  for example  the agglomerative information bottleneck  aibn  method  slonim and tishby  1  in this task. however  the process of reducing the cardinality of one attribute is blind in respect to the others  except to the class attribute   and thus it is unlikely that cardinality reduction will result in any significant improvement in the ability to capture critical patterns  which always depend on more than one attribute.
모when the number of probabilities to be estimated is too large when compared to the size of the training set and we cannot fill the traditional conditional probability tables  cpts  satisfactorily and  pearl  1  recommends the adoption of a model that resorts to causal independence assumptions like the noisy-or gate. using a noisy-or the number of parameters required to represent the conditional probability distribution  cpd  of a node given its parents  instead of being proportional to the product of the cardinality of all parents attributes  becomes proportional to the sum of their cardinality. however  causal independence assumptions are incompatible with our goal of capturing critical patterns.
모it is possible to use more flexible representations for the conditional probability distributions of a node given its parents  like default tables  dfs   friedman and goldszmidt  1   decision trees  dts   friedman and goldszmidt  1  and decision graphs  dgs   chickering et al.  1 . according to  friedman and goldszmidt  1   using such representations together with adequate learning procedures induces models that better emulate the real complexity of the interactions present in the data and the resulting network structures tend to be more complex  in terms of arcs  but require fewer parameters. fewer parameter may result in smaller overfitting problems. on the other hand  using traditional cpts  we assume that the probability distributions for a node given any two combinations of values for the parents are independent. if some of these distribution are actually identical  dts  dfs and dgs  can reflect it and represent the cpd using a variable number of parameters that is only proportional to the number of actually different distributions.
모using dts  dfs or dgs to represent the conditional distribution of a node given its parents  we assume that the probability distribution of the node given two different combinations of values for the parents may be either identical or completely independent. it is possible that neither of the two assumptions hold.
모in  gelman et al.  1  it is asserted that modeling hierarchical data nonhierarchically leads to poor results. with few parameters nonhierarchical models cannot fit the data accurately. with many parameters they fit the existing data well but lead to inferior predictions for new data. in other words they overfit. in contrast hierarchical models can fit the data well without overfitting. they can reflect similarities among distributions without assuming equality.
모observing an slight modification in equation 1 used in  friedman et al.  1  in the definition of a smoothing schema for tan we can see that the data that is used to estimate the cpt of any node that has at least one parent is hierarchical:
		 1 
where s is a constant that defines the equivalent sample size of the prior probability distribution. we call this almost direct estimation  ade . ade uses the probability distribution assessed in a wider population to build an informative prior probability distribution for a narrower population and so it has a hierarchical nature. such approach was also used  for example  in  cestnik  1 . ade is the consequence of instead of a noninformative dirichlet prior probability distribution  adopting a dirichlet prior probability distribution where 붸jki 뫚 p xji .
모ade gets closer to the true probability distribution  but its discrimination power is not significantly better than de. it is a linear combination of two factors njki/njk and p xji . the second factor is closer to the true probability distribution than its constant counterpart in direct estimation but it is still equal for any combination of values of 붫j and thus has no discrimination power.
모ade jumps from a very specific population  the set of training examples where 붫j = 뷇jk  to a very general population  the whole training set . in contrast  we present a model  that we call hierarchical pattern bayes  hpb   which moves slowly from smaller populations to larger ones benefiting from the discrimination power available at each level.
1 the hierarchical pattern bayes classifier
hpb is a generalization of ade that employs a hierarchy of patterns. it combines the influence of different level patterns in a way that the most specific patterns always dominate if they are well represented and combines patterns in the same level making strong independence assumptions and a calibration mechanism.
모hpb works for classification problems where all attributes are nominal. given a pattern w and a training set of pairs  x c   where c is a class label and x is a pattern  hpb calculates p cr|w  for any class cr where a pattern is as defined below:
definition 1 a pattern is a set of pairs of the form  attribute = v alue   where any attribute can appear at most once. an attribute that is not in the set is said to be undefined or missing.
definition 1 a pattern y is more generic than a pattern w if and only if y   w. if y is more generic than w  we say that w satisfies y .
definition 1 a pattern y is strictly more generic than w if and only if y   w.
definition 1 the level of a pattern w  level w   is the number of attributes defined in w.
definition 1 g w  is the set of all patterns strictly more generic than a pattern w
1 the hierarchical model
hpb calculates the posterior probability p cr|w   using a strategy that is similar to almost direct estimation  but the prior probabilities are considered to be given by p cr|g w  .
모the parameters of the dirichlet prior probability distribution used by hpb are given by: 붸r = s 몫 p cr|g w    where s is a smoothing coefficient. consequently:
		 1 
where nw is the number of patterns in the training set satisfying the pattern w and nwr is the number of instances in the training set satisfying the pattern w whose class label is cr.
모given equation 1  the problem becomes to calculate p cr|g w  . our basic idea is to write p cr|g w   as a function of the various p cr|wj  where the wj are patterns belongingto g w  and calculate each p cr|wj  recursively using equation 1.
definition 1 g w  is the subset of g w  whose elements have level equal to level w    1.
for example  if w is {a = a b = b c = c}  g w  is:
{ {b = b c = c} {a = a c = c} {a = a b = b} }
모we consider that only g w  influences p cr|g w   directly  so that p cr|g w   = p cr|g w  . the influence of the other patterns in g w  are captured by the recursive process. the first step for the decomposition of p cr|g w   in an expression that can be evaluated recursively is to apply bayes theorem:
p cr|g w  =p g w |cr p cr 

p g w  뫚p w1 w1 ... wl|cr p cr where w1 w1 ... wl are the elements of g w .
모then we approximate the joint probability p w1 w1 ... wl|cr  by the product of the marginal probabilities:
		 1 
but apply a calibration mechanism:
		 1 
where b is a calibration coefficient.
모given equations 1 and 1 we need to calculate p wj|cr . applying bayes theorem:
		 1 
모we estimate p cr  using the maximum likelihood approach: p cr  = nr/n  where nr is the number of examples in the training set belonging to class cr  and n is the total number of examples in the training set. if it happens that nr is zero we cannot use equation 1. in this case we just define that p cr|w  is zero for any pattern w.
모we know that when we substitute p wj|cr  by the right side of equation 1 into equation 1 we are able to clear out the factor p wj  because it is identical for all classes  so we do not need to worry about it.
모since wj is a pattern  the estimation of p cr|wj  can be done recursively using equation 1. the recursion ends when g w  contains only the empty pattern. in this case p cr|g w   becomes p cr|{{}}  = p cr .
1 calibration mechanism
in spite of its strong independence assumptions  naive bayes is know to perform well in many domains when only misclassification rate is considered  domingos and pazzani  1 . however  naive bayes is also know to produce unbalanced probability estimates that are typically too  extreme  in the sense that they are too close to zero or too close to one. in the aim of obtaining better posterior probability distributions  calibration mechanisms which try to compensate the overly confident predictions of naive bayes have been proposed  bennett  1; zadrozny  1 .
모using equation 1 we are making stronger independence assumptions than naive bayes. naive bayes assumes that attributes are independent given the class  what is at least possible. equation 1 assumes that some aggregations of attributes are independent given the class. since many of these aggregations have attributes in common we know that such assumption is false. the main consequences of our stronger and unrealistic assumption are even more extreme probability estimates than naive bayes' ones. this is compensated by the calibration mechanism in equation 1. this calibration mechanism is analogous to the one used in  zadrozny  1  in the calibration of decision tree probability estimates.
1 selecting hpb coefficients
equations 1 and 1 require respectively the specifications of coefficients s and b. in the classification of a single instance  these equations are applied by hpb in the calculation of p cr|w  for several different patterns  w. the optimal values of s and b can be different for each pattern.
모in the case of the b coefficients  we use an heuristic motivated by the fact that the level of any pattern in g w  is level w    1. the higher such level is  the more attributes in common the aggregations have  the more extreme probability estimates are and the stronger must be the effect of the calibration mechanism. thus  we made the coefficient b in equation 1 equal to b level w    1  where b is an experimental constant.
모in the case of the s coefficients  we employ a greed optimization approach that starts from the most general pattern family and move toward the more specific ones  where a pattern family is the set containing all patterns that define exactly the same attributes  possibly with different values .
모assuming that the s coefficients have already been fixed for all pattern families that are more generic than a family f  there is a single s coefficient that needs to be specified to allow the use of equation 1 to calculate p cr|w  where w is any pattern belonging to f . we select this coefficient  using leave one out cross validation  in order to maximize the area under the hit curve that is induced when we calculate p cr|w  for all training patterns  w  in f.
1 experimental results
all classification methods were tested by the weka experimenter tool  witten and frank  1  using 1 fold cross validation. we compared classifiers built using the following methods:
  hpb: hpb as described in this paper;
  nb: naive bayes;
  noisy-or: bn with the direct bn structure using noisy-or instead of a cpt;
  tan: tan with traditional cpts;
  ade: almost direct estimation. bn with the direct bn structure and the smoothing schema described in  friedman et al.  1 ;
  de: direct estimation. bn with the direct bn structure and traditional cpts;
  aibn/tan: tan with traditional cpts trained over a dataset where cardinality reduction using aibn was previously applied;
  dg cbm: bn with dgs. complete splits  binary splits and merges enabled;
  dg cb: bn with dgs. complete splits and binary splits enabled;
  dg c: bn with dgs. only complete splits enabled;
  dg cm: bn with dgs. complete splits and merges enabled;
  hc dt: bn with decision trees learned using hill climbing  hc  and mdl as the scoring metric;
  hc df: bn with default tables learned using hc and mdl.
  prior: trivial classifier that assigns the prior probability to every instance.
모all models involving dgs were constructed following  chickering et al.  1 . the models involving dfs and dts were constructed following  friedman and goldszmidt  1  using the mdl scoring metric.
모we tried different parameterizations for each method and sticked with the parameter set that provided the best results  where best results mean best area under the hit curve 1 up to 1% of selection rate  auc1  1. in the y axis  we chose to represent the recall = ntruepositives/npositives  instead of the absolute number of hits  because this does not change the form of the curve and makes interpretationeasier. we represented the selection rate in log scale to emphasizethe beginning of the curves. besides using the hit curve  we compared the probability distributions estimated by the models with the distribution actually found in the test set using two measures: root mean squared error  rmse  and mean cross entropy  mce . figure 1  table 1 and table 1 show our results.
selection of method parameters is explained below:
모the smoothing coefficients employed by hpb are all automatically optimized. such optimization involves a leave one out cross validation that takes place absolutely within the current training set  the 1 fold cross validation varies the current training set  eliminating the possibility of fitting the test set. the b coefficients are defined by the heuristic described in section 1 and by the constant b. we varied b over the enumeration {1 1 1 1 1 1} and sticked with 1  which was the constant that produced the best results in a 1 fold cross validation process. to avoid the effects of fine tuning  we reshuffled the data set before starting another 1 fold cross validation process. the hpb results that we present here came from the second process.

figure 1: selection rate x recall  to avoid pollution we only present curves related to a subset of the tested methods 
the optimization of aibn/tan involves 1 parameters:
the tan smoothing constant  stan   the aibn smoothing constant saibn  and the minimum mutual information constant mmi. varying stan over the enumeration {1 1 1 1 1 1 1} saibn over the enumeration  1 1 1 1  and mmi over the enumeration {1 1 1} we found that the best results are obtained with the triple besttriple =  stan = 1 saibn = 1 mmi = 1 . we did not cover the whole grid but since we did not observe any abrupt variations in hit curves and since we covered all immediate neighbors of the besttriplewe believe that such exhaustivecoveringwas not necessary.
	method	1%	1%	1%	1%	1%

hpb11 11 11 11 11tan11 11 11 11 11aibn/tan11 11 11 11 11nb11 11 11 11 11noisy-or11 11 11 11 11bndg cb11 11 11 11 11bndg cbj11 11 11 11 11bndg c11 11 11 11 11ade11 11 11 11 11de11 11 11 11 11bndg cj11 11 11 11 11hc df11 11 11 11 11hc dt11 11 11 11 11prior11 11 11 11 11table 1: recall for different selection rates with std. dev.
	method	auc	auc1	rmse	mce	parameterization

hpb11 11 11 11b =1tan11 11 11 11s =1aibn/tan11 11 11 11besttriplenb11 11 11 11s =1noisy-or11 11 11 infinitybndg cb11 11 11 11s =1bndg cbj11 11 11 11s =1bndg c11 11 11 11s =1ade11 11 11 11s =1de11 11 11 11s =1bndg cj11 11 11 11s =1hc df11 11 11 11s =1hc dt11 11 11 11s =1prior11 11 11 11table 1: area under curve  accuracy of probability estimates and optimal parametrization
모noisy-or and prior have no parameters. the optimization of all other methods involves only the smoothing constant  which  in all cases  was exhaustively varied over the enumeration {1 1 1 1 1 1 1 1}. this enumeration covers different magnitudes for the smoothing constant and at the same time avoids fine tuning.
모naive bayes and noisy-or are both unable to capture interactions among the attributes and cannot explore critical patterns. this explains their performance in the very beginning of the hit curve. tree augmented naive bayes explores interactions among some attributes and performed better than naive bayes or noisy-or. however  it cannot benefit from some critical patterns involving many attributes that are decisive at the selection rate of 1% and 1%.
모applying cardinality reduction to the attributes before constructing the tan model did not lead to any significant improvements in the hit curve.
모substituting the traditional cpts of bayesian network by decision trees  default tables and decision graphs with binary splits disabled only made the hit curves worse. the learned default tables included very few rows. the learned decision trees and decision graphs involved very few splits. as a consequence  in all cases  the resulting models had little discrimination power.
모using decision graphs with binary splits enabled  the most critical patterns were separated from the others  what resulted in a significant improvement in the beginning of the hit curves. the other patterns were  almost all  left together what resulted in loss of discrimination power for selection rates above 1%.
모hypothesis tests  show that hpb is significantly better than all other classifiers in what regards to auc  auc1  rmse and mce. we also performed hypothesis tests for every selection rate from 1% to 1% in steps of 1%. hpb is significantly better than all other classifiers for every selection rate below 1% with the exceptions that it is not significantly better than: tan in  1% 1%  and  1% 1% ; aibn tan in  1% 1%  and  1% 1% ; bndg cbj at 1%; bndg cb at 1% and 1%.
모hpb benefits from critical patterns involving many or even all attributes but also considers the influence of less specific patterns. as a consequence  it performs well for any selection rate.
1 conclusions
in the domain of preselection of imported goods for verification some combinations of attribute values can constitute critical patterns whose influence over the probability of finding a positive instance is very significant. due to the high cardinality of the attributes in this domain  exploiting such patterns without overfitting is challenging. we addressed the problem using hpb a novel classification method based on a multilevel hierarchical bayesian model.
모hpb was shown capable of capturing the influence of critical patterns without overfitting and without loosing discrimination power after the exhaustion of critical patterns in the test set. hpb resulted in a hit curve that is almost unambiguously better than any of the other methods. hpb also produced better probability estimates according to two accuracy measures  table 1 .
모hpb was only validated in a specialized domain  however  its equations are too simple to reflect any particularities of the domain  except that it is characterized by few high cardinality attributes and relevant interactions among them. thus the use of hpb may be a good option for domains with the same characteristics or  at least  provide some light on how to develop good models for such domains.
모hpb training time is exponential in the number of attributes  linear in the number of training instances and independent of the attributes cardinality. thus hpb is only applicable to domains where there are few attributes  but in such domains it is much faster than methods whose training time depends on the cardinality of the attributes.
모hpb is not a full bayesian model in the sense of  gelman et al.  1   where the parameters associated with a sub-population are assumed to be drawn from a general distribution and the calculation of all involved probability distributions is done at once considering all available evidence.
instead  hpb estimates the probability distributions for the more general populations first and use the results in the estimation of the probability distributions related to the more specific populations. hpb is a generalization of the smoothing techniques used in  cestnik  1  and  friedman et al.  1 . in the sense of  gelman et al.  1   it is an empirical model.
모hierarchical bayesian models have been widely used in the marketing community under the name of hierarchical bayes  allenby et al.  1; lenk et al.  1 . these models have also been used in medical domains  andreassen et al.  1  and robotics  stewart et al.  1 . however  we are not aware of any hierarchical bayesian model that can be employed to handle high cardinality attributes with relevant interactions in a classification problem. this makes hpb relevant. moreover  hpb differs from other models by dealing with a multi level hierarchy recursively and also handling the fact that one sub-population is contained by several overlapping super-populationsand not only by one super-population.
모based on the literature  friedman and goldszmidt  1   one can expect that bayesian networks with default tables  decision trees or decision graphs can emulate the real complexity of the interactions present in the data with without serious overfitting problems. another contribution of this paper is to show that bns with dfs  dts or dgs  in spite of their theoretical motivations  actually do not result in better overall performance than simpler methods like nb or tan  in a practical domain where their abilities are truly necessary.
모preprocessing the data reducing the cardinality of the attributes using the agglomerative information bottleneck method did not result in any significant improvements in the hit curves.
모our present mechanism for selecting the smoothing and the calibration coefficients in hpb equations is too simplistic. we leave its improvement as future work.
