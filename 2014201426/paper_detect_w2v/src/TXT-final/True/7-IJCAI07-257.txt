
currently most word sense disambiguation  wsd  systems are relatively individual word sense experts. scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. word sense transitions are very important. they embody the fluency of semantic expression and avoid sparse data problem effectively. in this paper  hownet knowledge base is used to decompose every word sense into several sememes. then one transition between two words' senses becomes multiple transitions between sememes. sememe transitions are much easier to be captured than word sense transitions due to much less sememes. when sememes are labeled  wsd is done. in this paper  multi-layered conditional random fields  mlcrf  is proposed to model sememe transitions. the experiments show that mlcrf performs better than a base-line system and a maximum entropy model. syntactic and hypernym features can enhance the performance significantly. 
1 introduction 
word sense disambiguation  wsd  is one of the important tasks in natural language processing. given the context of a word  a wsd system should automatically assign a correct sense to this word. wsd has been studied for many years. current word sense disambiguation systems are mainly individual word sense experts  ide et al.  1 . in these systems  there are mainly two categories of the target word's contexts. one category is the words in some windows surrounding the target word  the other category is the relational information  such as syntactic relation  selectional preferences  etc. but senses of contextual words  which may also need to be disambiguated  attract little attention. in this paper  senses of contextual word and the target word are simultaneously considered. we try to incorporate sense transitions between every contextual word and the target word into one framework  and our target is to disambiguate all words through determining word sense transitions.  some scholars have done relative researches on globally modeling word sense assignments. some methods have been proposed  including gambl  a cascaded memory-based classifiers   decadt et al.  1   senselearner  mihalcea and csomai  1   na ve bayes methods  yuret  1 .  
 most of these methods still based on individual word experts while our motivation is to model word sense transitions. this is a different view of globally modeling word sense assignments. in a sentence with no syntactic information  word sense transitions take the form of linear chain. that is  transitions are from left to right  word by word. in a sentence with dependency syntactic information  word sense transitions are from all children to their parent. although linear chain word sense transitions had been studied  by using simulated annealing  cowie et al.  1  and unsupervised graph-based algorithm  mihalcea  1   the number of word sense transitions is so tremendous that they are not easy to be captured. in order to circumvent this shortcoming  we adopt hownet  http://www.keenage.com  knowledge base to decompose every word sense into several sememes  usually no more than 1 sememes . hownet defines a closed set of sememes that are much less than word senses. but various combinations of these sememes can describe various word senses. it is more practical to model sense transitions through sememes than through pure word senses.
 then one transition between two words' senses becomes multiple transitions between sememes. in this paper  we propose multi-layered conditional random fields  mlcrf  to model sememe transitions. as we know  conditional random fields  crf  can label nodes of structures like sequences and trees. consecutive labels in crf also constitute label transitions. therefore  the problem of modeling sememe transitions can also be viewed as sememe labeling problem. 
 there are researches on hownet-based wsd. wong and yang  have done a similar work to ours. just like pos tagging  they regarded wsd as word sense tagging and adopted a maximum entropy approach to tag senses. in section 1  we made some comparisons. the experiments show that mlcrf performs better than a base-line system and wong and yang's maximum entropy model. syntactic and hypernym features can enhance the performance significantly.
1 an introduction of hownet 
hownet  zhendong dong et  1  is a bilingual common-sense knowledge base. one important component of hownet is the knowledge dictionary  which covers over 1 words in chinese and close to 1 english equivalents. we use this knowledge dictionary to generate candidate senses of a word. in hownet  each candidate sense of one word is a combination of several sememes. for example  a sense definition of the chinese word  research institute  is as follows: 
 def=instituteplace  *research  #knowledge  where word sense  def  is split into sememes by commas. the sememes in this example are  instituteplace    research    knowledge . symbols preceding sememes represent relations between the entry word and the corresponding sememe. in this example  symbols are  *  and  # . symbol  *  represents agent-event relation   #  represents co-relation. this word sense  def  can be glossed as:  research institute  is an  instituteplace   which acts as an agent of a  researching  activity  and it has a co-relation with  knowledge . 
 sememes are the most basic semantic units that are non-decomposable. it is feasible to extract a close set of sememes from chinese characters. this is because each chinese character is monosyllabic and meaning bearing. using this closed set of sememes  hownet can define all words' senses through various combinations of these sememes. 
 a word sense is defined by sememes according to an order. the first sememe in a word sense definition represents the main feature of the sense of that word  and this main feature is always the category of that sense. in the example mentioned above  the main feature of the word  research institute  is  instituteplace . other sememes in a word sense are organized by an order of importance from the hownet's point of view. in our current system  symbols representing relations are omitted. 
 totally  hownet contains 1 sememes  which are classified into 1 main classes  including  entity    event    attribute    attribute value    quantity    quantity value    secondary feature . these 1 main classes are further classified hierarchically. in the end  all of 1 sememes are organized as 1 trees. each sememe besides the root sememe has a hypernym  a is a hypernym of b if b is a type of a . for example   instituteplace  has a hypernym  organization    organization  has a hypernym  thing    thing  has a hypernym  entity . the hypernym feature shows usefulness in our word sense disambiguation system. 
1 system description 
1 task definition 
our task is to disambiguate word senses through sememe labeling. here is the formal description of our task. in what follows  x denotes a sentence. the ith word of x is denoted by xi   and the sentence's length is n. according to hownet  the sense of each xi is decomposed into m layers of sememes denoted by yi1  ...yim   where m is an empirical parameter depending on how much layers of sememes bound together can differentiate word senses from each other. please note that yi1  ...yim are ordered by decreasing importance. for example  yi1 is the first sememe in the sense definition of xi . with some simplification of the hownet knowledge dictionary  word senses can distinguish each other by 1 layers of sememes. if sememes are taken as labels  every word xi has 1 layers of labels. word sense disambiguation becomes a 1-layered labeling problem. sememe labeling can be carried on flat sentences  sequences  or dependency syntactic trees. this is illustrated in figure 1. 

figure 1. graphical representation of words and sememe layers.  a  is the sequence representation  and  b  is the tree representation. solid lines show sememe transitions  dashed lines show related links. please note that the direction of sememe transition is different for sequences and trees. for sequences  the direction is from left to right. for trees  the direction is from child node to its parent node. although sememes can depend on any word  higher layer sememes can depend on lower layer sememes  for clarity  we do not show these cross links. for more detailed description  see the feature set of section 1.
 sememes per layer constitute a label configuration of the sentence. these m layer configurations of the sentence are denoted by 1 ... m . our aim is to find: 
	 1...  m	arg max	  1... m |	 	 1 
y1...ym
we propose a layer-by-layer strategy  mlcrf  to solve this equation. current layer sememe labeling can use labeled pre-layer sememes as features. the order of the strategy is bottom up  from 1st layer to mth layer. to introduce mlcrf  we introduce conditional random fields in the following section. 
1 introduction of conditional random fields 
conditional random fields  crf   lafferty et al.  1  are undirected graphical models used to calculate the conditional probability of a set of labels given a set of input values. we cite the definitions of crf in  mccallum  1 . it defines the conditional probability proportional to the product of potential functions on cliques of the graph  
1
	 	|	 	c  c 	c 	 1 
	c	 	 	 
where x is a set of input random variables and y is a set of random labels. c   c  c   is the clique potential on clique c. c y x  is the set of all cliques of the graph. in crf  clique is often an edge with a node on each end. c represents labels of the two end nodes. clique potential is often defined as: 
r
	c  c 	c 	exp 	r fr   c 	c  	 1 
r 1
where fr is an arbitrary feature function over its arguments  r is a learned weight for each feature function  r is the total number of feature functions of the clique. is a normalization factor over all output values   
	'	c  	 1 
two dynamic programming algorithms are employed: viterbi for decoding and a variant of the forward-backward algorithm  sha and pereira  1  for the computing of expectations and normalization. 
 two kinds of crf are used in our system: sequential crf and tree crf. sequential crf is like traditional linear chain crf  while tree crf is slightly different. in linear chain crf  one clique is composed of current word and its pre-word. in tree crf  one clique is composed of current word and one child of it. tree crf adopts sum-product algorithm  pearl  1   which is an inference algorithm in graphical models. cohn and blunsom  have used tree crf to solve semantic role labeling. but the algorithm was not presented. in this paper  we take sum-product algorithm into a forward-backward frame. each node in the tree has several sub forward  backward  vectors. the detailed description of the algorithm is not included in this paper for the limit of the paper length. 
1 multi-layered conditional random fields 
in multi-layered conditional random fields  the conditional probability is as follows: 

	c m	 1 
m
where 	denotes the pair of mth layer labels of the words of clique c.
  represent 1 
1  ...  mth layer probabilities respectively. each layer crf model can be trained individually  using sequential crf or tree crf. 
 sutton et al.  use dynamic crf to perform multiple  cascaded labeling tasks. rather than perform approximate inference  we perform exact inference. to avoid exponential increase  we propose a multi-layered viterbi algorithm for decoding. 
 to illustrate multi-layered viterbi  let us look back on single layer decoding. for single layer  the most probable y is as follows: 
	  argmax  	 	|	   argmax  	 	 	  	 1 
where 	. the denominator	 is omit-
	   	 	  c 	c 
c
ted because is the same for all y. but in multi-layered case  the next layer decoding will use pre layer labels as features  so the denominators are no longer the same.  for the ease of explanation  let us consider linear chain crf of single layer. if the denominator  is not omitted for one layer  it can also be expressed as: 
	sum  1 	sum  n 
sum  1 	 1  sum  1 	sum  n 1 
where i denotes the forward vector at the ith position of the chain. sum   denotes the sum of the vector 's elements. in crf  the sum of the forward vector i 's elements is the sum of all possible paths up to the ith position. 
so sum  n  is equal to 	.
 	then the linear chain probability is as follows: 
	 	|	  	  1  1 	log sum  1  
	  1 	1 	{log sum  1  	log sum  1  } ...
	  n  n 	{log sum  n  	log sum  n 1  }	 1 
equation 1 shows that the linear chain probability can be computed dynamically. at each time step  we can get probability up to that time. based on this single layer equation  we can derive the multi-layered viterbi algorithm. 
 with some simplification of the hownet knowledge dictionary  word senses can distinguish each other by 1 layers of sememes. we present a 1-layer viterbi for sequential crf in figure 1. multi-layered viterbi can be extended from 1-layer viterbi. in figure 1  all forward vectors and transition matrices m are only about second layer. si denotes a combined label of 1 layers at ith position  si 1 denotes the second layer label at ith position. suppose that the label set of each layer contains k elements  then si has k*k kinds of 1-layer combinations. 
 in 1-layer viterbi  we do not need transition m trices a and forward vectors of 1st layer because  is the same for all 1. but for 1nd layer  the normalizing factor is 1   where 1 is the 1st layer sememes that are related to 1st layer decoding. so forward vectors and transition matrices are needed to calculate normalization factor for the 1nd layer  is the forward vector of 1nd layer at ith
figure 1. 1-layer viterbi algorithm for sequential crf. si' 1 record the optimal path. 
　　the time complexity of 1-layer viterbi for one sentence is o j 1t    where j is the number of 1st layer labels  k is the number of 1nd layer labels  t is the length of the sentence. figure 1 mainly deals with sequence. when applying it to trees  sub  s for every tree node should be introduced. in our problem for wsd  si is the candidate sense at position i  which is less than 1-layer combinations of entire labels. 
1 reducing the number of candidate labels 
in common crf  every input variable xi has several candidate labels. there are 1 sememes in hownet. for our wsd problem  it is impractical to treat all these sememes as candidate labels for the training of single layer crf. 
 using the hownet knowledge dictionary we can generate candidate sememes much less. we propose two methods to reduce the number of candidate sememes. one is defspace and the other is exdefspace. in the following  we will introduce how to apply these two methods to first layer crf in detail. 
defspace
for defspace  candidate first layer sememes of a word are the first layer sememes appeared in the definitions of this word in hownet. defspace generates a real space that only includes the labels  sememes  listed in the definitions of the entry word. for example  chinese word fazhan has 1 senses in the hownet dictionary:  causetogrow    grow  and  include . since all these senses are single sememes  candidate sememes of the first layer are these 1 sememes for defspace.
exdefspace
exdefspace is the extended version of defspace. suppose there are two words a and b. according to the hownet dictionary  a has 1 candidate first layer sememes sem 1 and sem 1  b has 1 candidate first layer sememes sem 1 and sem 1. suppose a appears in the training corpus while b does not appears. considering training examples of a with sem 1 or sem 1  for defspace  the training procedure only discriminates sem 1 and sem 1  while sem 1 and sem 1 are not discriminated in this discriminative training. then when b appears in test data  it can't be disambiguated. to overcome this shortcoming of defspace  candidate sememes of a word are extended. for every sememe in hownet  we build a candidate sememe list. concretely speaking  for every sememe s  we search the whole dictionary entries to find sememes of first layer that together with s define a common polysemous word. all such sememes are put into the candidate sememe list of s. then during training  the candidate sememes of a word are generated according to the hand labeled sememe's candidate list. for above example  sem 1 is added to the candidate lists of sem 1 and sem 1. then word a provides a positive example of sem 1 or sem 1  but a negative example of sem 1. also  if one sem 1 is tagged in the training corpus  it provides a negative example of sem 1 or sem 1. 
　　finally  in a sememe's candidate sememe list  there are average 1 candidate sememes. 
　　candidate second layer sememes of a word are generated according to this word's entries in hownet whose first layer sememes are the same as the labeled first layer sememe. this is like defspace used in first layer crf. 
　　in the phase of decoding  multi-layered viterbi   only multiple senses of a word are regarded as candidate senses of this word. no special measure like exdefspace is taken.
1 feature set 
in mlcrf  training is taken layer by layer. for first layer  we use the following factored representation for features. f       p   c q  c'   c   1 
where p   c  is a binary predicate on the input x and current clique c  q  c'   c   is a binary predicate on pairs of labels of current clique c. for instance  p   c  might be   whether word at position i is de .
 table 1 presents the categories of p   c  and q  ' c  .
q  '	c  seqp x c seqq  '	c  treep x c trees i 1 s i-1 s i 1 s i-1trues n 1 s c 1 s c 1 s n 1true  s i 1
  s i-1 s n 1
 s c 1s i 1uni w bi w uni p bi p tri p  from i-1 to i+1 s n 1uni w bi w uni p bi p tri p
 from ff to cl and cr table 1. feature set of sequential crf and tree crf 
 for first layer  a label of the clique c is the first layer sememe. in table 1  s i 1 denotes 1 layer sememe ost f ith word in sequential crf and s n 1 denotes 1st layer sememe of current node n in tree crf.     denotes hypernym  uni  bi and tri denote unigrams  bigrams and trigrams respectively  w denotes word  p denotes pos. the subscript seq and tree represent sequential crf and tree crf respectively. for tree crf  c denotes the child of n in current clique. cl and cr are the left and right sibling of node c. f denotes the parent node of n  ff denotes grandfather of n.
 for second layer  s i 1  s n 1  is replaced by s i 1  s n 1 . p x c  excluding true are appended by s i 1  s n 1 . for true value  when q is about s i 1  s n 1  or s i-1  s c 1   additional feature s i 1  s n 1  is added to p x c .
1 experiments and results 
1 experimental settings 
sememe  hownet  based disambiguation is widely used in chinese wsd such as senseval 1 chinese lexical sample task. but full text disambiguation at sememe level is scarcely considered. furthermore  the full text corpus tagged with wordnet like thesaurus is not publicly available. the sentences used in our experiments are from chinese linguistic data consortium  http://www.chineseldc.org  that contains phrase structure syntactic trees. we can generate gold standard dependency trees using head rules. we selected some dependency trees and hand tagged senses on them with over 1 words for training and over 1 words for testing. there are two taggers to tag the corpus and an adjudicator to decide the tagging results. the agreement figure is the ratio of matches to the total number of annotations  and the figure is 1. this corpus is included in chineseldc. 
 for comparison  we build a base-line system and duplicate wong and yang's method  wong et al.  1 . the base-line system predicts word senses according to the most frequent sense of the word in the training corpus. for words that do not appear in the training corpus  we choose the first sense entry in the hownet dictionary. we also imitate wong's method. their method is at a word level. the sense of a word is the first sememe optionally combined with the second sememe. they defined this kind of word sense as categorical attribute  and also a semantic tag . then an off-the-shelf pos tagger  mxpost  is used to tag these senses.
 in our system  named entities have been mapped to predefined senses. for example  all person named entities have the sense  human| . for those words that do not have entries in dictionary  we define their sense by referring to their synonyms that have entries. in the end  polysemous words take up about 1 of the corpus  and there are average 1 senses per polysemous word. 
　　in the following sections  precisions of polysemous words are used to measure the performance. since all words have entries in dictionary or can be mapped to some senses  recall is the same with precision. note that for single layer sememe labeling  precision is the rate of correct sememes of the corresponding layer. sense precision is the rate of correct senses  not sememes. 
1 results of mlcrf in wsd 
1st layer 1nd layer sensec +h c +hseqdef11 111seqexdef11 - -1 treedef11111treeexdef11--1table 1. pe ormrf ance of mlcrf in wsd. numbers are precisions of polysemous words with respect to first layer  second layer and whole word sense. 
table 1 shows the performance of mlcrf in wsd.  c  represents common features that presented in table 1 except hypernym features.  +h  means common features plus hypernym features. seqdef   seqexdef   denotes sequential 
crf using defspace  exdefspace . treedef  treeexdef   denotes tree crf using defspace  exdefspace . from table 1  we can see that tree crf performs better than sequential crf  which shows that sense transitions in trees are easier to be captured than in sequences. hypernym features enhance performance of tree crf partly because it avoids sparse data problem  while it adds noise to sequential crf. exdefspace performs similarly as defspace  which maybe due to our relatively small corpus. 
 in the test set  there are 1 different types of 1 polysemous words. our best result correctly label 1 word senses that are labeled wrongly by baseline system. of these 1 polysemous words  there are 1 word types  of which 1 word types never occur in the training corpus. although baseline system also wins our method a few words  totally our method improves 1% over baseline. 
 our dependency trees are golden standard. we insist that parsing and wsd should not be processed in a pipe line mode. wsd is not the ultimate object of nlp. it has interplays with parsing  semantic role labeling etc. they can be integrated together if wsd can be fulfilled by globally modeling word senses. there are some methods like re-ranking and approximate parameter estimation that have the potential to solve that integrating. 
 table 1 shows the comparisons between our system and  1  base-line system   1  wong and yang's method. both sequential crf and tree crf perform better than base-line system and wong and yang's method. the syntactic feature enhances performance significantly. sequential crf enhances little over base line partly because sense transitions in sequences are not easy to be captured. the performance of wong and yang's method is below base-line system  which maybe due to its simple features and sparse representation of all possible paths. that is  even pos features are not included in mxpost and no syntactic features are used. moreover  in mxpost  only features that appear more than 1 times in the training corpus are included in the feature set. but candidate labels are all sememes. no special measures like defspace and exdefspace are taken. then the feature set is very sparse to represent all possible paths even we duplicate training corpus 1 times. 
ourwong base-lineseq tree sense acc. 1 1 11table 1. comparisons between our system and wong and yang's method and base-line system.
1 conclusion 
in this paper  we probe into sense transition by decomposing sense into sememes. then sense transition can be replaced by sememe transitions.
 we model sememe transitions by mlcrf. for each layer sememe  sequential crf and tree crf are used. multi-layered viterbi algorithm is proposed at the predicting phase. experiments show that mlcrf performs better than a base-line system and a max entropy model. syntactic and hypernym features can enhance the performance significantly.
acknowledgments 
this work was supported by the natural sciences foundation of china under grant no. 1  1  and the natural science foundation of beijing under grant no. 1. thanks for the helpful advices of professor zhendong dong  the founder of hownet. 
