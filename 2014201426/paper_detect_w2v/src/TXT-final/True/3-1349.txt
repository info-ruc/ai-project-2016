
recently  a number of authors have proposed criteria for evaluating learning algorithms in multiagent systems. while well-justified  each of these has generally given little attention to one of the main challenges of a multi-agent setting: the capability of the other agents to adapt and learn as well. we propose extending existing criteria to apply to a class of adaptive opponents with bounded memory. we then show an algorithm that provably achieves an  -best response against this richer class of opponents while simultaneously guaranteeing a minimum payoff against any opponent and performing well in self-play. this new algorithm also demonstrates strong performance in empirical tests against a variety of opponents in a wide range of environments.
1 introduction
recent work in multi-agent learning has put forth a number of proposals for judging algorithms  bowling and veloso  1; conitzer and sandholm  1; powers and shoham  1 . in addition to arguing the merits of their proposal  each researcher also demonstrated an algorithm meeting their criteria. unfortunately  the algorithms and even the criteria themselves are in general applicable only within a very limited setting. in particular  there has been a focus on designing algorithms that behave well in the presence of stationary opponents  dodging the complexities that arise when the opponent may be adapting to the agent's past play.
﹛the two criteria proposed in  bowling and veloso  1  require that the agent both converge to a stationary policy against some class of opponents and that the agent play a best response if the opponent converges to a stationary policy. if these criteria are satisfied by all the players  this results in a guarantee of ultimately repeatedly playing a nash equilibria of the stage game. they then propose an algorithm that provably meets these criteria in two player normal form games with two actions per player.  conitzer and sandholm 
1  adopt a restatement of these same criteria and prove that their own algorithm achieves these criteria in arbitrary repeated games. note however that neither of these algorithms makes any guarantee about the payoffs achieved by their algorithm against non-stationary opponents and can potentially be exploited arbitrarily by adaptive opponents  as shown in  chang and kaelbling  1 .
﹛in recent work   bowling  1  addresses this vulnerability by adding a requirement that the agent experience zero average regret. in this context  regret is traditionally defined as the maximum payoff that could have been achieved by playing any stationary policy against the opponent's entire history of actual moves minus the actual payoff the agent received. several algorithms have been proven to achieve at most zero regret in the limit  see  hart and mas-colell  1  and  jafari et al.  1  for examples in both game theory and ai .
﹛the work of  fudenberg and levine  1  on 'universalconsistency' is representative of this literature and also points out two limitations of the regret minimization approach as a whole. the first is the inability of no-regret strategies to capitalize on simple patterns in the opponent's play. they address this limitation with a proposal for the stronger concept of 'conditional consistency' and a new algorithm that achieves it in  fudenberg and levine  1 . the second limitation is that while a no-regret algorithm guarantees a minimum payoff against any possible opponent  it ignores the possibility that the sequence of moves played by the opponent is dependent on the agent's own moves. while this assumption is quite justified in games with a large number of players  it becomes a serious liability in repeated interactions with only a few players. while we are not aware of much work dealing explicitly with this limitation   de farias and megiddo  1  address it in the design of their experts algorithm and the rational learning approach of  kalai and lehrer  1  can in principle handle adaptive algorithms of arbitrary complexity as long as they are assigned positive probability in the prior.
﹛to see how the failure to consider adaptive opponents could hurt an algorithm's performance  let us consider a repeated version of the prisoner's dilemma game shown in figure 1. prisoner's dilemma has been extensively studied  axelrod  1  and numerous algorithms proposed that allow two agents to cooperate on the advantageous cooperation outcome without being exploited. the simplest but perhaps
most effective of these is the tit-for-tat algorithm. tit-for-tat starts by cooperating and thereafter repeats whatever action the opponent played last. note that any approach that considers only stationary opponents must always play defect  since this is the unique best response to any stationary opponent and the only strategy that can ever result in no-regret performance. against tit-for-tat this results in a payoff of 1  but the strategy of always playing cooperate would yield a payoff of 1. clearly  a no-regret policy is not the best response in this richer strategy space.
﹛as another example of the advantages of considering adaptive opponents  consider playing the stackelberg game of figure 1 repeatedly. notice that up is a strictly dominated strategy  regardless of what the opponent chooses the row agent would prefer to play down. however  if the opponent is learning  this would presumably prompt them to play left  resulting in a payoff of 1 for the row agent. if it instead played the seemingly suboptimal action of up  the opponent may learn to play right  giving the row agent a higher payoff of 1. we can see that in this instance  teaching can play as much of a role in achieving a desirable outcome as learning. in both of these games some of the most successful strategies are those that have the ability to either cooperate with their opponents or manipulate their opponents as appropriate.
﹛the weaknesses of this reliance on an assumption of stationarity were previously acknowledged in  powers and
shoham  1  and we proposed the following three criteria:1
﹛targeted optimality: against any member of the target set of opponents  the algorithm achieves within   of the expected value of the best response to the actual opponent.
﹛compatibility: during self-play  the algorithm achieves at least within   of the payoff of some nash equilibrium that is not pareto dominated by another nash equilibrium.
﹛safety: against any opponent  the algorithm always receives at least within   of the security value for the game.
﹛one of the key aspects of the proposal is the use of a parameterized target class of opponents against which to achieve optimal performance. while this could also address adaptive agents  the previous work only provides an algorithm for stationary opponents. in this work  we adopt the same criteria and analyze how to develop algorithms that behave well against opponents that can adapt to their past experience.
1 environment
within this paper  we will focus on the class of two-player repeated games with average reward. in this setting the two players repeatedly play a simultaneous move normal form game  represented as a tuple  g =  n a r1...n   where n is the number of players  a = a1 ℅ ... ℅ an  where ai is the set of actions for player i  and ri : a ↙   is the reward function for agent i. after each round  the agents accumulate their reward from the joint outcome and get to observe the prior actions of the other agent. each agent is assumed to be trying to maximize its average reward. two example normal form games with the rewards organized into a table are shown in figure 1. for our purposes we assume that the full game structure and payoffs are known to both agents. finally  although we shall generally refer to the other player as the opponent  we do not mean to imply an adversarial setting  but instead consider the full space of general-sum games.
	cooperatedefect	left right
11111111cooperateup
defectdown
	 a  prisoner's dilemma	 b  stackelberg game
figure 1: example stage games. the row player's payoff is given first  with the column player's payoff following.
1 adaptive opponents
while the goal of this work is to expand the set of possible opponents against which we can achieve a best response  we will need to limit their capabilities in some way. if we consider opponents whose future behavior can depend arbitrarily on the entire history of play  we lose the ability to learn anything about them in a single repeated game  since we will only ever see a given history once. we will therefore assume a limit on the opponent's ability to condition on the history. we propose directly limiting the amount of history available  by requiring that the opponents play a conditional strategy where their actions can only depend on the most recent k periods of past history  fi : o 1 ℅ ... ℅ o k ↙  ai  where o t is the outcome of the game t periods ago. we will assume that the opponents have a default past history they assume at the start of the game. note that even this simple model allows us to capture many methods such as tit-for-tat that current approaches are unable to properly handle.
﹛let's now consider how we could apply the criteria given above to this set of opponents. while the value of the best response to a given conditional strategy is well-defined  it would prove an unreasonable requirement for many possible strategies. let's again consider the prisoner's dilemma game with an opponent that is either playing the grim strategy or always plays cooperate. in the grim strategy the opponent initially starts playing cooperate but switches to playing defect indefinitely if its opponent ever plays defect  a conditional strategy with history 1 . note that no possible learning strategy can achieve the value of the best response against both opponents  since it must play defect at least once to distinguish them  at which point the option of always cooperating with the grim strategy will no longer exist. there are two possible approaches to remedying this problem. one way is to constrain the class of opponents we consider. sufficient requirements for conditional strategies are that either the opponent only condition on the actions of the agent  not its own past actions  or that the policies the opponent plays assign non-zero probability to each action for every past history. an alternative approach would be to relax our best response target. instead of requiring the agent achieve the best value possible by any strategy played from the start of the game  we can set the target to be the highest average value that can be achieved after any arbitrary initial sequence of moves to account for the need for exploration.
﹛even with these restrictions on our target  we can see that in order to guarantee an  -best response with high probability we will require an exponential exploration period  since to find a good outcome an agent needs to sample from the exponential number of histories the opponent considers. furthermore  if we allow the opponent to condition on its own actions  the number of observations required can become unbounded unless we add a requirement of a minimum probability of playing any given action.1
1 a manipulative algorithm
although the metastrategy algorithm we introduced in  powers and shoham  1  is only explicitly designed for stationary opponents  we can use much of the intuition behind the approach to design a new algorithm for our class of adaptive opponents. the idea behind metastrategy is to start with a teaching strategy for an initial coordination/exploration phase and use its payoff and the opponent's play to determine which of three possible strategies to play. if its opponent is consistent with the target class it adopts a best response. if it achieves its target value by getting its opponent to adopt a
best response to its teaching strategy it continues playing it  otherwise it selects a default strategy.1 the algorithm then plays according to this strategy as long as it exceeds its security level  reverting to a maximin policy if its payoff drops.
﹛in figure 1  we show the implementation of this general approach for our target class. the membr strategy calculates a best response strategy against conditional strategies. this approach maintains counts of the opponent's actions after each history of length k  which it uses to calculate the cycle of agent actions with the highest expected reward out of all possible unique agent action sequences  those that don't contain a length k repeated subsequence . given sufficient observations  this lets us guarantee that we achieve an  -best response against any member of our target opponent set.1 we can calculate the probability the opponent's play is consistent with our target set by comparing the observed distribution of play for each history at separate times and measuring the deviation in action profiles. we continue to use the minimax strategy to achieve the security value guarantee. and for the self-play guarantee we can replace the bully-mixed strategy with a generous stochastic version of godfather  littman and stone  1 . godfather was motivated by the folk theorem for repeated games and selects some outcome in the game matrix with greater payoff for each agent than their security set strategy = stochgodfather for 而1 time steps  play strategy for 而1 time steps
if  avgv alue   vgodfather    1 
with probability p  set strategy = membr
﹛play strategy if opponentintargetset   for 而1 time steps  play membr if opponentintargetset  
﹛set beststrategy = membr else set beststrategy = strategy
else if  strategy == stochgodfather and avgv alue   vgodfather    1 
﹛set beststrategy = stochgodfather else set beststrategy = membr while not end of game
if avgv alue   vsecurity    1
﹛play maximin strategy else
play beststrategy
figure 1: the manipulator algorithm
values and plays its portion of the target outcome. if the opponent ever plays an action other than the matching action for the target outcome  the agent plays a strategy that forces the opponent to achieve no more than its security value until the opponent again plays its target action. for our purposes we've created a stochastic version of godfather that selects a mixed strategy for the agent and a target action for the opponent such that the joint strategy gives the opponent a higher expected value than its security value and also denies it any advantageous deviations. this is necessary since we want our godfather algorithm to be implementable by a conditional strategy with history 1. because of this constraint  we need to make sure the opponent can't achieve a net profit by deviating one turn and then playing the target action the next  incurring only one period of punishment. the parameters specified in the algorithm are a function of the desired values for   and 汛 in the theoretical guarantees.  our empirical results used 而1 = 1 而1 = 1 p = 1%  1 =  1 = 1 
﹛an additional advantage of reusing our existing framework is that we can apply the proof used for the metastrategy algorithm with only minor modifications to show theorem 1. the main change is that we must require that the agent play fully mixed strategies during the beginning of the game in order to get sufficient observations to test whether the opponent's play was consistent with the target set of strategies. given this constraint  the proof consists mainly of a long string of applications of the hoeffding inequality  hoeffding  1 .
theorem 1 our algorithm  manipulator  satisfies the three properties stated in the introduction for the class of conditional strategies with bounded memory k  after a training period depending on  where 竹 is the minimum probability the opponent assigns to any action  or 竹 = 1 for opponents that condition only on the agent's actions.

figure 1: average value for last 1k rounds  of 1k  across selected games in gamut. game payoffs range from -1 to 1.1 experimental results
in order to test the performance of our approach we've used the comprehensive testing environment detailed in  powers and shoham  1; nudelman et al.  1 . besides metastrategy  our set of opponents includes bully and godfather from  littman and stone  1   hyper-q  tesauro  1   local q-learning  watkins and dayan  1   smooth fictitious play  fudenberg and levine  1   and wolf-phc  bowling and veloso  1 . we also include random stationary strategies  random   random conditional strategies  stochmem   and membr1 which learns a best response to conditional strategies with history 1.
﹛in figure 1 we show the performance of the most successful of the distinct algorithms across a variety of normal form games. all of the adaptive algorithms fare well against the stationary opponents  random and bully  while manipulator and to a lesser degree  hyper-q  fare the best against the bounded memory adaptive strategies  godfather and stochmem. manipulator and godfather also have an advantage against opponents that learn a best response to conditional strategies  such as membr1 and manipulator itself. for other approaches that fall outside the target sets of either metastrategy or manipulator we can see that metastrategy has a slight advantage  mainly because of its ability to play pure strategies  while manipulator is constrained to explore the opponent's strategy space during the initial coordination period. however  an additional advantage of manipulator in this setting is its ability to perform well in self-play  achieving the highest payoff of all algorithms tested  while metastrategy is limited to the space of stationary policies and misses more complex opportunities for cooperation in some games.
﹛let us now turn to the performance of our new algorithm in different types of games. figure 1 shows the relative reward achieved by the most successful algorithms for a selection of games in gamut averaged across the set of opponents. we can see that manipulator has the best performance in nearly every game. in games like prisoner's dilemma and hawk and dove  godfather is able to perform better by manipulating more of the opponents into yielding  both by sending a clearer message  since it doesn't have to do any exploration  and by waiting longer for its opponent to adapt. this stubbornness  however  proves its undoing in games where it is critical to adapt to the other opponent  such as the dispersion game. metastrategy's strong performance in shapley's game seems to stem from its default fictitious play strategy exploiting localq and wolf-phc. however  we can see the advantage of manipulator over metastrategy in games like prisoner's dilemma and travelers dilemma which have equilibria in the space of repeated game strategies that paretodominate any equilibria of the stage game.
1 discussion

figure 1: percent of maximum value for last 1k rounds  of 1k  averaged across all opponents for selected games in gamut. the rewards were divided by the maximum reward achieved by any agent to make visual comparisons easier.although manipulator demonstrates consistent performance across a wide variety of games  we are by no means claiming that it would be the best approach for all settings. in particular  it doesn't fare nearly as well in the most adversarial games  like matchingpennies  rochambeau  and shapleysgame. this is not surprising since it will be unable to find any deals to offer with its godfather component and its model-based assumption that its opponent is a conditional strategy offers no particular advantage against other adaptive opponents. an alternate approach we considered was adapting a model-free algorithm such as q-learning  watkins and dayan  1  to the multi-agent setting  following in the footsteps of numerous previous researchers attempting to find effective multi-agent learning strategies  e.g.  littman  1; claus and boutilier  1; tesauro  1  . in traditional qlearning the algorithm learns values for each action at each possible state of the world and then chooses the action that maximizes its expected reward. we propose two possible alternatives for dealing explicitly with adaptive opponents. one method is to incorporate the recent history into the state of the world and learn action values for each possible recent history. a second approach is to instead learn values over sequences of actions. when we conducted tests for these two algorithms we found that the approach that conditioned on previous history demonstrated the ability to exploit some of the other adaptive approaches including: local q-learning  smoothfp  and hyper-q  resulting in a higher average value in zero-sum games than any of the previous approaches. however  neither of these model-free methods performed as well in general sum games and both were dominated by manipulator against each opponent when tested on the full set of games. although these approaches also lack the theoretical guarantees of manipulator  they profit from not requiring any advance knowledge about the game payoffs. additionally  the ability to identify settings where individual approaches are particularly effective may lead to more powerful methods using portfolios of algorithms as suggested in  leyton-brown et al.  1 .
﹛finally  it should be pointed out that the model we've put forth for modelling opponents with bounded capabilities is only one of many possible ones. a common approach used in the literature on bounded rationality  neyman  1; papadimitriou and yannakakis  1  is to assume the agents can be modelled by finite automata with k states. note that the automata model is more comprehensive than the set of conditional strategies since any conditional strategy opponent with bounded memory can be modelled by an automata with |a|k states if we allow stochastic outputs  but there exist automata that cannot be modelled by any function on a finite fixed history. in the case of automata with deterministic transitions  we can modify our manipulator algorithm to handle this new class by implementing our version of godfather as a dfa and replacing the best response function. note that learning a best response to an opponent modelled by an unknown finite automata is equivalent to finding the best policy for an unknown pomdp  investigated in  chrisman  1; nikovski and nourbakhsh  1 . while a difficult computation problem  we should be able to achieve the same theoretical properties for this alternate set of opponents given similar constraints to those we placed on the conditional strategies.
in particular  we need to consider how to handle finite automata with multiple ergodic sets of states and automata with arbitrarily small transition probabilities.
1 conclusions and future work
we feel that explicitly addressing the issue of adaptive opponents is a critical element of learning in multi-agent systems. it is this very quality that seems to define the difference between the multi-agent setting and the single-agent one. our algorithm approaches this setting by combining a teaching approach which manipulates adaptive opponents into playing to our agent's advantage with a cooperative/learning approach that adapts itself to its best estimate of its opponent's strategy. our algorithm can be shown to achieve  -optimal average reward against a non-trivial class of adaptive opponents while simultaneously guaranteeing a minimum payoff against any opponent and performing well in self-play  all with high probability. these results translate into good empirical performance in a wide variety of environments. there is clearly more work that can be done  however. as we discussed in the previous section  we've so far only analyzed one possible model for adaptive opponents with bounded memory and are still considering how best to incorporate other approaches that achieve better empirical performance against existing algorithms. additionally  our approach still has significant restrictions on the set of environments it considers. we can immediately identify five limitations of our current approach:
1. single opponent: the criteria are only clearly definedfor games with two players.
1. single state: the criteria are only clearly defined forrepeated games  rather than general stochastic games .
1. average reward: the criteria are defined for games inwhich the agent only cares about the average of its aggregated rewards  rather than a discounted sum .
1. full observability: the agent needs perfect observationsof the opponent's actions from prior moves in the game.
1. known games: the algorithm needs to know all of thepayoffs for each agent from the beginning of the game.
﹛while some of these would only require minor modifications or transformations of the environment  others such as the discounted reward setting require a markedly different way of viewing the problem. we're currently working to test the limits of how much we can relax each of these restrictions in turn and hope our work here may serve as a first step towards a more widely applicable approach.
