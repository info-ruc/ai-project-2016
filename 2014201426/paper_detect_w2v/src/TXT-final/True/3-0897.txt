 
traditional supervised learning deals with labeled instances. in many applications such as physiological data modeling and speaker identification  however  training examples are often labeled objects and each of the labeled objects consists of multiple unlabeled instances. when classifying a new object  its class is determined by the majority of its instance classes. as a consequence of this decision rule  one challenge to learning with labeled objects  or sessions  is to determine during training which subset of the instances inside an object should belong to the class of the object. we call this type of learning 'session-based learning' to distinguish it from the traditional supervised learning. in this paper  we introduce session-based learning problems  give a formal description of session-based learning in the context of related work  and propose an approach that is particularly designed for sessionbased learning. empirical studies with uci datasets and real-world data show that the proposed approach is effective for session-based learning. 
1 introduction 
a typical supervised learning problem is to learn a model from training examples that are usually labeled instances. but for many applications  training examples are labeled objects and each labeled object consists of multiple unlabeled instances. during classification  an object is labeled as class 'a' if the majority of its instances are classified as 'a'.  
¡¡ one application is physiological data modeling  whose goal is to predict context activities of individual users based on their physiological data. typically  physiological signals are measured and recorded continuously for every other second. continuous physiological signals are then divided into a number of sessions. each session is of minutes long and consists of hundreds of records of physiological data. to predict the user's activities for a session  prediction is first made for each record  and the dominant activity predicted for records in the session is then used as the activity for the session. although each session is labeled as a single activity  a user may perform activities other than the labeled one. for example  during a session of 'watching tv'  the user may fall into sleep for a short period of time. more information can be found from the website http://www.cs.utexas.edu/users/sherstov/pdmc/. 
¡¡another application is speaker identification. to determine the speaker for a speech sample that is a minute long  a typical strategy is to first divide the long speech sample into a number of short ones. then  a standard classification model  such as gaussian mixture model  gmm   is applied to determine the speaker identity for each short sample. finally  the dominant speaker that is classified for short samples will be used as the predicted speaker for the long sample. compared with the strategy that extracts a single set of features for the whole long speech sample  this majority vote approach is usually more robust and accurate  reynolds  1 . this is because features extracted from a long speech sample may include significant amounts of background noise  while the noise can be reduced substantially when a long speech sample is divided into short ones.  
¡¡the common characteristics of the above two applications are that  1  training examples are labeled objects  e.g.  sessions of physiological records in physiological data modeling  and a long speech sample in speaker identification ;  1  every object consists of multiple instances that are not labeled; and  1  in predicting an object's class  it is determined by the class that is assigned to the majority of its instances. to distinguish this new type of learning from the traditional supervised learning  we call it 'session-based learning'. 
¡¡the challenge of session-based learning arises from the majority vote strategy that is used to determine the label of an object. this decision rule makes it ambiguous  during training  as to which subset of the instances inside an object should belong to the class of the object - we name it the label ambiguity problem. a straightforward strategy toward this problem is to treat every unlabeled instance within a labeled object as a positive example for the class of the object. in physiological data modeling  every physiological record within a labeled session is treated as a training instance for the activity assigned to the session. in speaker identification  every short sample within a long speech is used as a positive training instance for the speaker of the long speech. we call this simple strategy the 'naive approach'. 

figure 1: a toy example of session-based learning problem with four classes 'a'  'b'  'c'  and 'd'. 
¡¡one obvious problem with the na ve approach is that instances within a single object often belong to multiple different classes  not just to the class of the object. thus  by treating every instance within an object as a positive example for the class of the object  we likely introduce training examples with noisy labels  which  as a result  degrades the quality of classification models. to further illustrate the problem with the na ve approach  let us consider a toy learning problem in figure 1. it has four classes  with class 'a'  'b'  and 'c' centering on the vertices of a triangle  and the fourth class 'd' sitting on the center of the triangle. in traditional supervised learning  training examples are labeled instances. since these four classes are well separated  we would expect that a simple na ve bayes model should work fine for this problem. but  for a session-based learning problem  training examples are labeled objects that consist of instances from different classes. in particular  consider the case when training objects for classes 'a'  'b'  and 'c' are mixtures of instances from these three classes  while training objects for class 'd' only contain instances from 'd'. with appropriate mixtures  the input means of instances from labeled objects for the four classes can stay close to each other  which makes it hard for the na ve bayes method to learn if assigning each instance with its object class. 
¡¡in the following  we first give a formal description of session-based learning  and elaborate on the differences between this new type of learning problem and other related learning problems  such as multiple-instance learning. then  we present a novel approach that is particularly designed for session-based learning. the key idea is to develop an innovative way that handles the label ambiguity problem. finally  we demonstrate the effectiveness of the proposed approach for session-based learning using both uci datasets and data from physiological data modeling. 
1 	formal description of session-based learning 
let 	training 	examples 	be 	denoted 	by 
d ={ o y1  1   o1  y1   ...  on  yn }   where each  o yi  i   is a labeled object and yi ¡Êy is the class label assigned to object oi . domain y is { 1   1} for binary-class classification problems  and  1..k  for multiple-class classification problems where k  k   1  is the number of classes. let c o    be an object-based classification function  which takes an object o as input and outputs its class label. in general  any supervised learning problem can be formulated as an optimization problem: 
	c* = argmin¡Æin=1l c o    i    yi   	 1  
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡c where l is a loss function that determines the amount of punishment when prediction c o  i   is different from yi . 
 for a traditional supervised learning problem  each object only contains a single instance. as a result  training examples can be simplified as d ={ x1  y1   x1  y1   ...  xn  yn  }   where each instance xi ¡Ê d is a vector in a d dimension space. furthermore  the object-based classification function c o    becomes an instance-based classification function f    :x   ¡úd y . thus  a traditional supervised learning 
problem is usually formulated as follows: 
	f * = argmin¡Æin=1l f   xi    yi   	 1a 
f
¡¡in session-based learning  each object consists of multiple unlabeled instances. let the training data be denoted by d ={ x1  y1   x1   y1   ...  xn   yn  }   where each object 
xi = {xi j  }mj=i 1 contains mi different instances. given an instance-based classification function f    :x  d ¡ú y   the decision rule for session-based learning is that  an object xi is labeled as class 'a' if the majority of its instances are classified as 'a'. thus  in session-based learning  the objectbased classification function cs can be written into the following form of the instance-based classification f    x : 
	cs  xi; f   = argmax	f  xi j  	y 	 1  
y¡Êy
where ¦Ä    is a delta function that outputs 1 when the input is positive and zero otherwise.  
 compared to traditional supervised learning  the challenge of session-based learning is due to the label ambiguity problem. although each training object is provided with a class label  the label information of instances within objects is not given. hence  it is difficult to learn an instance-based classification function from labeled objects. in the na ve approach  each instance within an object is treated as a positive example for the class of the object. as a result  a session-based learning problem is simplified as a traditional supervised learning problem: 
	n	mi
f * = argmin¡Æ¡Æl f   xi j     yi   	 1b  f	i= =1 j 1
 the most related work to session-based learning is multiple-instance learning dietterich  et al. 1 . similar to session-based learning  in multiple-instance learning  class labels are assigned to objects that consist of multiple instances  which are called  bags  in multiple-instance learning. in the past  there have been many studies on multipleinstance learning  including the approach of learning axisparallel rectangles  dietterich  et al. 1   the diverse density algorithm  maron and lozano-p¨¦rez 1   the approaches based on support vector machines  andrews  et al. 1  tao  et al. 1   the nearest neighbor approach  amar et.al. 1; wang and zucker  1   and the boosting approach  andrews and hofmann 1 . 
¡¡multiple-instance learning differs from session-based learning in its decision rule. in multiple-instance learning  an object xi is labeled as positive class when at least one of its instances is classified as positive. a negative class is assigned to an object when all of its instances are classified as negative. thus  given the instance-based classification function f    :x  d ¡ú y   the object-based classification function cm for multiple-instance learning is written as: 
	cm xi; f   =     + 1	  ¡Ê  ¡Êji   1...1...mmii     ff  xxi ji j      = +=  1 	 1' 
 
examples for the three different types of learning are shown in figure 1 for comparison. the first column shows traditional supervised learning with instances and their labels. the columns for multiple instance learning and sessionbased learning show how these two different strategies aggregate instances and assign labels to the aggregates. 
¡¡session-based learning is more challenging than multipleinstance learning in the following sense: in multipleinstance learning  when an object is labeled as 'negative'  all of its instances will belong to the negative class. thus  for multiple-instance learning  there is no label ambiguity for negatively labeled objects. in contrast  the label ambiguity problem exists for session-based learning regardless of the sign of labels. for instance  in figure 1  for session-based learning  both the first and second objects are labeled as 'negative'. however  the labels of instances in these two objects are different. the two learning schemes also differ in degrees of difficulty when deciding positive classes for objects: multiple-instance learning adopts the  at-least one  strategy and session based learning uses the majority one.  
1 sboost - an algorithm for session-based learning 
¡¡in this session  we will present a boosting-based algorithm for session-based learning problems of binary classes. the key for designing a learning algorithm for sessionbased learning is to define a simple yet effective loss function l c   xi; f    yi   . a simple choice is 
l c   xi; f    yi   =¦Ä c xi; f   ¡Ù yi   . however  this choice will lead to a non-smooth objective function  which is usually difficult for optimization. hence  we choose to use the exponential loss function to approximate classification er-
trad. supervised learningmultiple-instance learning  session-based learning 1 1 1-1 1 1 1 -1 1 1 1-1 1 1 1-1 1 1 1 1 1 11 1 1-1 1 1 1 1 1 1...... ...... ...... 1 1 1-1 1 1 1 +1 1 1 1-1 1 1 1-1 1 1 1 1 1 11 1 1+1 1 1 1 1 1 1...... ...... ...... 1 1 1+1 1 1 1 +1 1 1 1+1 1 1 1-1 1 1 1 1 1 11 1 1+1 1 1 1 1 1 1figure 1: training examples for traditional supervised learning  multiple-instance learning  and session-based leaning. 
rors  which has been demonstrated to be effective in the adaboost algorithm  freund and schapire  1 .  in particular  in designing objective functions  two types of errors are considered: instance-based errors and sessionbased errors. an instance-based error is a prediction mistake made for an instance  and a session-based error is a prediction mistake made for a session. one key difference between session-based learning and traditional supervised learning is that the former is concerned with both sessionbased errors and instance-based errors while the latter concerns with only instance-based errors. to include both types of errors  given an instance-based classification function h   x   we define the loss function as: l c x h y    i;    i   =
  ¦Ã

	exp	 	     yi ¡Æmi h xi j          ¡Æjm=i1 exp  h x  i j   yi        	 1  
	  mi	j=1	  
         session-based error instance-based error in the above expression  both session-based and instancebased errors are approximated by an exponential function. the loss function is defined as the product of these two errors. in other words  a misclassified instance is important only when its related session is also misclassified. constant ¦Ã in  1  determines the relative importance between the session-based error and the instance-based error. in experiment  a cross validation with 1 split of training data is used to determine appropriate values for ¦Ã. in the following  we will discuss how to efficiently find h   x that minimizes the loss function in  1 . 
 1 boosting-based optimization algorithm 
¡¡with the loss function defined in  1   our goal is then to search for optimal h x  that minimizes the overall cost for the training data  i.e.  
h* = argminerr = argminl c   xi;h y   i  
	h	h
= argminh ¡Æi=1 exp   ¦Ãyi ¡Æmi h xi j          ¡Æjm=i1 exp  h x  i j   yi         1  n	 
	  mi	j=1	  
given:  x1  y1   ...   x m  ym  where xi ={xi j  }mj=i1   xi j  ¡Ê d   yi ¡Ê  { 1} ; weighting constant ¦Ã 
initialize the weight distribution d i j1	m	 
for t = 1 ... t 
1. sample training instances {x1 ... x1 m1 ... xn 1 ... xn m  n } according to dt 1 
1. train a weak classifier ht    :x  d ¡ú  { 1} on sampled examples 
1. compute gi = exp    ymi¦Ã¡Æmj=i1ht 1 xi j        ai = ¡Æmj=i1exp  ht 1 xi j   yi     and 
	 	i
¡¡¡¡¡¡mi	mi bi = ¡Æ y hi  xi j   exp  ht 1 xi j   yi  + y ai i¦Ã¡Æh xi j    for each session. m
	j=1	i	j=1
	  n	 	  
1. let ¦Át = 1+¦Ã  ln      ¡Æ¡Æii==n1 ggii   1++¦Ã¦Ã  aaii + bbii         
1. update the weight distribution d i jt      = gi  exp  ht 1 xi j   yi  +¦Ãa mi	i     where zt is a normaliza-
zt
tion factor  chosen so that dt+1 is sum to 1 . 
1. update the classifier ht    x = ht 1   x +¦Át th    x 
output the final hypothesis: ft    x = argmax t th    x y y¡Ê { 1}
figure 1: description of the sboost algorithm.¡¡an efficient approach for optimizing eq.  1  is to divide it into a series of simple learning problems that do not have the label ambiguity problem and thus can be resolved by traditional supervised learning techniques. we solve the label ambiguity problem by maintaining weights for different instances such that only instances with large weights are assigned to the class of their objects and used for training. since boosting  freund and schapire  1  is a learning algorithm that uses weighted instances to efficiently update classification functions  we design a boosting-based learning algorithm for learning with labeled sessions below.   
¡¡let ht    x be the 'weak' classifier of the t-th iteration that is learned using traditional supervised learning techniques. the combined classifier ht    x for the first t iterations is 
ht     t th      where ¦Át is the combination constant for the t-th iteration. our goal is to find another 'weak' classifier ht+1   x and a constant ¦Át+1 such that the new combined classifier ht+1   x = ht    x +¦Át+1ht+1   x will effectively minimize the function in  1 . given classifier ht+1   x   the objective function in  1  is rewritten as: 
 exp  
	err =¡Æn     	i     ¦Ãmyii ¡Æjm=i1   h xi j    +¦Áh xi j               	 1  
  m	   ¡Á¡Æexp    h x  i j    +¦Áh x  i j     yi    
i=1
	   j=1	  
in the above expression  for the convenience of presentation  we drop the index for ht    x   ht+1   x   and ¦Át+1 . using the convexity of an exponential function  we have  
	n	mi
err ¡Ü¡Æ ¡Ægi	exp  ¦Á  h xi j   +¦Ãh xi k     yi  h xi j   yi    1 
       i=1 mi k j  =1 where gi ¡Ô exp     ymi¦Ãi ¡Æmj=i1h xi j       . then  using inequal-
ity e     1   we can further upper bound  1  by the following expression: 
err err¡Ü upper e¦Á ¦Ã 1+   +e  +¦Á ¦Ã 1   n
=	¡Æa gi	i
	1	i=1
 1  
e¦Á ¦Ã 1+    e  +¦Á ¦Ã 1	 	n	mi	  gi exp  h xi j   yi    
 	1+¦Ã  ¡Æ¡Æi= =1 j 1 yhi  xi j      ¦Ãai	 
	+gi	 
	 	mi	 
where 	ai	h	i j yi . 	define 	weight 
d i j      ¡Ô gi  exp  h xi j   yi  +¦Ãa mii   and rewrite the second term in  1  as: e¦Á ¦Ã 1+    e  +¦Á ¦Ã 1   n mi
	1+¦Ã  ¡Æ¡Æi= =1 j 1 y hi  xi j   d i j      	 1  
clearly  to minimize the objective err in  1   we need to maximize the above expression  1 . the best case is that the output of weak classifier h   x is consistent with yi  for all instances in an object. when this is not the case  the label ambiguity problem is then resolved based on weight d i j      . in particular  instances with large weights are assigned with the class of their objects  while the labels for instances with small weights remain unlabeled. this is because the contribution of an instance to  1  is mainly determined by its weight d i j      . when an instance has a small weight  its contribution to  1  will be ignorable. furthermore  notice that d i j      is proportional to gi   which is related to session-based error. thus  a misclassified instance will not be assigned with a large weight if the related session error is small. finally  the combination constant ¦Á can be obtained by setting the derivative of the upper bound in 
 1  w.r.t. to ¦Á to be zero. that is  
	  n	 
 ¡Ægi   1+¦Ã ai +bi   
	¦Á= 1+¦Ã  ln    ¡Æi=n1 gi   1+¦Ã ai  bi       	 1  
	  i=1	 
where
bi ¡Ô	y h xi	i j	 h	i j yi +		h	i j . 
mi
for later reference  we name this algorithm sboost for 'session-based boosting'. the details are given in figure 1.  
1  experiments 
the goal of this section is to examine the effectiveness of sboost for session-based learning. we compare sboost with the simple na ve approach that treats each instance within an object as a positive example for the class of the object. in particular  the na ve approach is applied to ses-
data set # examples # features spam 1 1 cmc 1 1 german 1 1 table 1: statistics of uci datasets used for synthesized data
 
 positive class negative class # instances 1 1 # objects 1 1 # avg of instances per object 1 1 table 1: statistics for the physiological data 
sion-based learning with both a decision tree and 
adaboost using a decision tree as its base classifier. 
¡¡two types of data are used in experiments to evaluate the effectiveness of sboost:  
1  synthesized data that are generated from binary uci datasets  blake and merz 1  by combining multiple instances into objects. each object consists of ten different instances. to create an object for the positive class  a random number between 1 and 1 is first generated  and the corresponding number of instances from the negative class are randomly chosen and added to the object. the rest of the object is filled out with instances randomly selected from the positive class. a similar procedure is applied to generate objects for the negative class. by doing so  we guarantee that the class of an object is consistent with the dominant class assigned to its instances. the details of uci datasets used in this experiment are listed in table 1. 
1  physiological data that come from the workshop of physiological data modeling at the icml 1. we use the dataset for 'watching tv' with code 1. in the original problem  the number of instances for the negative class is overwhelmingly larger than that for the positive class. since we focus on the study of session-based learning  we intentionally reduce the effect of rare class by randomly selecting parts of negative instances. the resulting data set has 1 sessions with 1 instances. the details of this dataset are listed in table 1.  
¡¡a decision tree  quinlan 1  is used as the baseline classifier throughout the experiments. the session-based classification error  i.e.  the percentage of sessions that are misclassified  is used for evaluation. 1% of data are randomly selected for training and the rest is used for testing. the same experiment is repeated 1 times  and the average session-based classification errors are reported. finally  for both sboost and adaboost  the maximum number of iterations is set to be 1. 
1 results 
the results of three methods  sboost  the na ve approach using a single decision tree  and with adaboost  are shown in table 1. first  we compare the performance of adaboost with that of a decision tree as both adopting the na ve approach to session based learning. we observe that adaboost 
data set sboost adaboost decision tree spam 1  1  1  1  1  1  cmc 1  1  1  1  1  1  german 1  1  1  1  1  1  phys. 1  1  1  1  1 1  table 1: classification errors with variances for the sboost algorithm  the adaboost  and the decision tree  does not guarantee to improve performance over the baseline classifier  a decision tree here . in fact  for the spam dataset and the physiological data  the classification errors of adaboost are even greater than those of a decision tree. this is because the na ve approach treats all instances in an object as positive examples for the class of the object  while in reality  some instances in an object may belong to a class other than the class of the object. hence  the na ve approach introduces noisy labels to training data  which will likely cause adaboost to overfit as observed in previous study of boosting  quanlin  1  dietterich  1  jin et al.  1 .   
¡¡second  comparing sboost with the decision tree  we observe that sboost always outperforms its baseline classifier. according to t-test  it is statistically significantly better than the decision over all the uci datasets with p   1 . thus  sboost does not suffer from the problem of overfitting as adaboost does. to further investigate this issue  we obtain average classification errors of adaboost and sboost in different iterations  respectively  as shown in figure 1. clearly  adaboost tends to overfit the training data after the first several iterations whereas sboost does not. 
1. conclusion 
in this paper  we formulate a new type of learning problem  session-based learning. it differs from the traditional supervised learning in that training examples are labeled objects and each object consists of multiple unlabeled instances. furthermore  session-based learning adopts a different decision rule from that of multiple-instance learning: an object is classified as class 'a' when the majority of its instances are classified as 'a'. this majority vote decision rule causes the 

figure 1: classification errors for physiological data 
label ambiguity problem and has made session-based learning very challenging. we formally describe this new learning problem and propose a novel boosting algorithm  named sboost. empirical studies have demonstrated the effectiveness of the sboost algorithm. 
