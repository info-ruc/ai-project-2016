
behavioral norms are key ingredients that allow agent coordination where societal laws do not sufficiently constrain agent behaviors. whereas social laws need to be enforced in a top-down manner  norms evolve in a bottom-up manner and are typically more self-enforcing. while effective norms can significantly enhance performance of individual agents and agent societies  there has been little work in multiagent systems on the formation of social norms. we propose a model that supports the emergence of social norms via learning from interaction experiences. in our model  individual agents repeatedly interact with other agents in the society over instances of a given scenario. each interaction is framed as a stage game. an agent learns its policy to play the game over repeated interactions with multiple agents. we term this mode of learning social learning  which is distinct from an agent learning from repeated interactions against the same player. we are particularly interested in situations where multiple action combinations yield the same optimal payoff. the key research question is to find out if the entire population learns to converge to a consistent norm. in addition to studying such emergence of social norms among homogeneouslearners via social learning  we study the effects of heterogeneous learners  population size  multiple social groups  etc.
1 introduction
norms or conventions routinely guide the choice of behaviors in human societies. conformity to norms reduces social frictions  relieves cognitive load on humans  and facilitates coordination.  everyone conforms  everyone expects others to conform  and everyone has good reason to conform because conforming is in each person's best interest when everyone else plans to conform   lewis  1. conventions in human societies range from fashions to tipping  driving etiquette to interaction protocols. norms are ingrained in our social milieu and play a pivotal role in all kinds of business  political  social  and personal choices and interactions. they are self-enforcing:  a norm exists in a given social setting to the extent that individuals usually act in a certain way and are often punished when seen not to be acting in this way   axelrod  1 .
　while these aspects of norms or conventions have merited in-depth study of the evolution and economics of norms in social situations  epstein  1; posch  1; young  1; 1   we are particularly interested in the following characterization:  ... we may define a convention as an equilibrium that everyone expects in interactions that have more than one equilibrium.   young  1 . this observation has particular significance for the study of norms1 in the context of computational agents. computational agents often have to coordinate their actions and such interactions can be formulated as stage games with simultaneous moves made by the players  genesereth et al.  1 . such stage games often have multiple equilibria  myerson  1   which makes coordination uncertain. while focal points  schelling  1  can be used to disambiguate such choices  they may not be available in all situations. norms can also be thought of as focal points evolved through learning  young  1 . hence  the emergence of norms via learning in agent societies promises to be a productive research area that can improve coordination in and hence functioning of agent societies.
　while researchers have studied the emergence of norms in agent populations  they typically assume access to significant amount of global knowledge  epstein  1; posch  1; young  1; 1 . for example  all of these models assume that individual agents can observe sizable fraction of interactions between other agents in the environment. while these results do provide key insights into the emergence of norms in societies where the assumption of observability holds  it is unclear if and how norms will emerge if all interactions were private  i.e.  not observable to any other agent not involved in the interaction.
　to study the importantphenomenonof emergenceof social norms via private interactions  we use the following interaction framework. we consider a population of agents  where  in each interaction  each agent is paired with another agent randomly selected from the population. each agent then is learning concurrently over repeated interactions with randomly selected membersfrom the population. we referto this kind of learning social learning to distinguish from learning in iterated games against the same opponent  fudenberg and levine  1 . most of our experiments involve symmetrical games with multiple pure-strategy equilibria with the same payoff. in previous work on learning in games  the opponent is fixed but in our work  the opponentis differentat each iteration. in addition  the opponent may not use the same learning algorithm. it is unclear  a priori  if and how a social norm will emerge from such a social learning framework. our experimental results and concomitant analysis throws light on the dynamics of the emergence of norm via social learning with private interactions. we also investigate a number of key related issues: the effect of population size  number of choices available  multiple populations with limited inter-population interactions  heterogeneouspopulation with multiple learning algorithms  effect of non-learners in shaping norm adoption  norms for social dilemmas  etc.
1 related work
the need for effective norms to control agent behaviors is well-recognized in multiagent societies  boella and van der torre  1; va＞zquez-salceda et al.  1 . in particular  norms are key to the efficient functioning of electronic institutions  garcia-camino et al.  1 . most of the work in multiagent systems on norms  however  has centered on logic or rule-based specification and enforcement of norms  dignum et al.  1; va＞zquez-salceda et al.  1 . similar to these research  the work on normative  game-theoretic approach to norm derivation and enforcement also assumes centralized authority and knowledge  as well as system level goals  boella and lesmo  1; boella and van der torre  1 . while norms can be established by centralized dictat  a number of real-life norms evolve in a bottom-up manner  via  the gradual accretion of precedent   young  1 . we find very little work in multiagent systems on the distributed emergence of social norms. we believe that this is an important niche research area and that effective techniques for distributed norm emergence based on local interactions and utilities can bolster the performance of open multiagent systems. we focus on the importance for electronic agents solving a social dilemma efficiently by quickly adopting a norm. centralized social laws and norms are not sufficient  in general  to resolve all agent conflicts and ensure smooth coordination. the gradualemergenceof normsfrom individuallearning can facilitate coordinationin such situations and make individuals and societies more efficient.
　in our formulation  norms evolve as agents learn from their interactions with other agents in the society using multiagent reinforcement learning algorithms  panait and luke  1; tuyls and nowe＞  1 . most multiagent reinforcement learning literature involve two agents iteratively playing a
stage game and the goal is to learn policies to reach preferred equilibrium  powers and shoham  1 . another line of research considers a large population of agents learning to play a cooperative game where the reward of each individual agent depends on the joint action of all the agents in the population  tumer and wolpert  1 . the goal of the learning agent is to maximize an objective function for the entire population  the world utility.
　the social learning framework we use to study norm emergence in a population is somewhat different from both of these lines of research. we are considering a potentially large population of learning agents. at each time step  however  each agent interacts with a single agent  chosen at random  from the population. the payoff received by an agent for a time step depends only on this interaction as is the case when two agents are learning to play a game. in the two-agent case  a learner can adapt and respond to the opponent's policy. in our framework  however  the opponent changes at each interaction. it is not clear a priori if the learners will converge to useful policies in this situation.
1 social learning framework
the specific social learning situation for norm evolution that we consider is that of learning  rules of the road . in particular  we will consider the problem of which side of the road to drive in and who yields if two drivers arrive at an interaction at the same time from neighboring roads 1. we will represent each interaction between two drivers as a n-person  m-action stage game. these stage games typically have multiple pure strategy equilibria. in each time period each agent is paired with a randomly selected agent from the population to interact. an agent is randomly assigned to be the row or column player in any interaction. we assume that the stage game payoffmatrix is know to both players  but agents cannot distinguish between other players in the population. hence  each agent can only develop a single pair of policies  one as a row player and the other as a column player  to play against any other player from the agent population. the learning algorithm used by an agent is fixed  i.e. an intrinsic property of an agent.
　when two cars arrive at an intersection  a driver will sometimes have another car on its left and sometimes on its right. these two experiences can be mapped to two different roles an agent can assume in this social dilemma scenario and corresponds to an agent playing as the row and column player respectively. consequently  an agent has a private bimatrix: a matrix when it is the row player  one matrix when it is the column player. each agent has a learning algorithm to play as a row player and as a column player and learns independently to play as a row and a column player. an agent does not know the identity of its opponent  nor its opponent's payoff  but it can observe the action taken by the opponent  perfect but incomplete information . the protocol of interaction is presented in algorithm 1.
for a fixed number of epoch do
repeat
remove randomly agents prow and pcol from the population ask each agent to select an action; send the joint action to prow and pcol for policy update;
until all agents have been selected during the epoch ;
algorithm 1: interaction protocol.
　we use three different learning algorithms for learning norms: q-learning watkins and dayan  1  with -greedy exploration  wolf-phc  bowling and veloso  1  and fictitious play  fp . q-learning has been widely used in multiagent systems  but convergesonly to pure strategies. wolfphc  win or learn fast - policy hill climbing  can learn mixed strategies. though wolf is guaranteed to converge to a nash equilibrium of the repeated game in a 1-person  1actions game against a given opponent  it is not clear whether it is guaranteed to converge in social learning. finally  fp is the basic learning approach widely studied in the game theory literature  fudenberg and levine  1 . an fp player uses the historical frequency count of its opponent's past actions and tries to maximizing expected payoff by playing a best response to that mixed strategy  represented by this frequency distribution.
1 results
1 example of a social dilemma
one typical example of the use of norms or convention is to resolve social dilemmas. a straightforward example of this is when two drivers arrive at an intersection simultaneously from neighboring streets. while each player has the incentive of not yielding  myopic decisions by both can lead to undesirable accidents. both drivers yielding  however  also creates inefficiency. ideally  we would like norms like  yield to the driver on right   which serves all drivers in the long run. hence  the dilemma is resolved if each member of the population learns to  yield  as a row  column player and  go  as a column  row  player. the player that yields gets a lesser payoff since it is losing some time compared to the other player. the players know whether they are playing as a row or a column player: the row player sees a car on its right  and the column player sees a car on its left. the action choices for the row player are to go  g  or yield to the car on the right  yl   and they are go  g  or yield to the car on the left  yl  for the column player. we model this game using the payoffs presented in table 1 a . note that for a social norm to evolve  all agents in the population has to learn any one of the following policy pairs:  a   row:g  col:yl   i.e.  yield to the car on the left   or  b   row:yr  col:g   i.e.  yield to the car on the right. we say a norm has emerged in the population when all learners make the corresponding choice except for infrequent random exploration.
　we first note that in iterated play between two players  i.e.  if the population consisted of only two agents  other policy combinations may also emerge. for example  in addition to the above two possible norms  it can be the case that one of the two learners learn to  go  both as row and column player
gyl1g-1  -1  11  1-1  -1yr1  1  11-1  -1  1	 a  social dilemma game	 b  coordination game
table 1: stage games corresponding to social interactions.
and the other player learns to yield in both situations. although not  fair   this situation is possible in our framework since each agent independently learns to play as a row and a column player.
　when a third agent is introduced as the agents do not know the identity of the opponents  no agent can any longer benefit from always choosing  go . this is because all other agents must always  yield  to the  go  agent  and then those agents will receive relatively poor utility when playing each other. as a result  they will also learn to  go . to optimize performance they will have to learn to settle to a norm which everyone else also follows. though we only hoped that this would happen via social learning in a large population  our experimental results show that a uniform norm always emerges in a population of three or more agents. for example  in a population of 1 agents using wolf  we ran 1 runs  and we observed that the population converged to the  yield to the left  norm 1 times  and  yield to the right  norm 1 times. we present the averaged dynamics of the payoffs and the frequency of the joint action during learning in figure 1. from the dynamics we can see that at first the agents avoid the collision and prefer to yield. then  one agent notice that it can exploit this situation by choosing to  go  as the other one is yielding. depending on who notices this first  the population converges to one norm or the other. note that the plot in figure 1 is averaged over all the runs  which explains why the  g yl  and  yr g  appear almost 1% of the time. the presence of the other joint-actions is due to exploration. these results confirm that only private experienceis sufficient for the emergence of a norm in a society of learning agents. this is in contrast with prior work on norm evolution which requires agents to have knowledge about non-local interactions between other agents and their strategies  epstein  1; posch  1; young  1 .
1 influence of population size  number of actions and learning algorithm
the time required for the emergence of a norm in a society of interacting agents  measured by the number of interaction periods before most agents adopt the norm  depends on several factors. here we study the influence of the size of the population  the learning algorithm used  and the number of actions available to the agents.
　first we consider the effect of population size. with a larger population  the likelihood that two particular agents interact decreases. hence the variety of opponents as well as the diversity of personal interaction history increases with the population size and the population takes more time to evolve a norm. in figure 1  we present the dynamics of the aver-
	 1	 1
 1	 1	 1	 1	 1 iterations	iterations
figure 1: social dilemma game with 1 agents using wolf  averaged over 1 runs. the population converges 1 times to  g yl  and 1 times to  yr g .
age agent reward for the social dilemma game in a population of agents using wolf with different population sizes: with more agents  it takes longer for the entire population to converge on a particular norm. it is well-known that tight-knit  small societies  groups  clans develop eclectic norms that are often not found in larger  open societies.
influence of the population size  average over 1 runs  with agents using wolf

figure 1: dynamics of the average payoff of learners using wolf with different population sizes  average over 1 runs .
　next  we consider the effect of the number of actions available to each agent. for the rest of paper  we use the coordination game presented in table 1 b . this stage game models the situation where agents need to agree on one of several equally desirable alternatives. for example  for the two-action case  this game can represent the situation where agents choose on what side of the road to drive. when both agents drive on their left  or on their right  there is no collision  else there is a penalty. the societal norms that we would want to evolve are either driving on the left or driving on the right. the stylized game  representing other  nondriving scenarios  can be expanded to n-actions: the agents receive a payoff of 1 when they choose the same action and a payoff of  1 when their actions differ. in figure 1  we show the dynamics of the probability of an agent choosing action 1 for each learning algorithm in a population of 1 agents for n = 1. then  we ran this experiment with n （ {1 1} in a population of 1 agents using wolf. the results are presented in figure 1. when the number of actions increase  the proportion of joint actions with high payoff decreases. when the agents explore at the beginning  the expected utility is less with a larger game. over time a norm emerges  with the average payoff of the population approaching 1. it takes longer to evolve norms for larger action sets as the space of joint actions increases quadratically.

figure 1: dynamics of the probability to play action 1  each agent is represented by two lines: policy to play as a row and as a column player; darker color represents probability closer to 1 : all agents converge to a probability close to 1  i.e.  chooses action 1.
　finally  we consider the effect of the learning algorithm used by the agents. since there is no clear choice of learn-
dynamics of the payoff of 1 wolf learners with different size of the game

figure 1: dynamics of the payoff of learners using wolf with different game sizes  average over 1 runs .
ing algorithms to use in general  we wanted to evaluate a few representative learning algorithms. we study the influence of the learning algorithms on a population of 1 agents playing the two-action game. when the entire population uses the same learningalgorithm  the populationof q-learners are the quickest to evolve a norm  「 1 iterations   followed by a population of wolf  「 1 iterations   and the population of agents using fp  「 1 iterations . the payoff reached at convergence is different for different algorithms due to different exploration schemes. we also show results of hybrid population using equal proportions of any two or all three of these algorithms. the time taken by mixed groups to evolve norms are in between the time taken by the corresponding homogeneous groups.
dynamics of the average payoff in a population of 1 agents  using different learning algorithms

figure 1: dynamics of the payoff of learners using different learning algorithms  population of 1 agents  average over 1 runs .
1 influence of fixed agents
so far  we have observed that all norms with equal payoffs were evolved roughly with the same frequency over multiple runs. this is understandable because the payoff matrix in table 1 b  does not support any preference for one norm over the other. extraneous effects  however  can bias a society of learners towards a particular norm. for example  some agents may not have learning capabilities and repeat a predetermined action. we study the influence of agents playing a fixed pure strategy on the emergence of a norm. for this study  we use the coordination game of table 1 b  and consider a population with 1 learners  nf = 1 agents playing the fixed strategy 1  driving on the left   and nf agents playing strategy 1  driving on the right . we ran experiments where we add additional agents playing the pure strategy 1. figure 1 presents the percentage of time that the norm  1   i.e.  everyone driving on the left  and  1   i.e.  everyone driving on the right  emerges. note that when there are equal numberof agents playing fixed strategy 1 and fixed strategy 1  one of the two norms emerges with almost equal frequency. it is interesting to note that with only 1 additional agents choosing to drive on the right  the entire population of 1 agents almost always converges to driving on the right! there just

 1	 1	 1	 1	 1	 1 number of additional agents playing fixed strategy 1
figure 1: number of times each norm emerges  average over 1 runs : a small imbalance in the number of agents using a pure strategy is enough to influence an entire population
might therefore be some truth to the adage that most fashion trends are decided by a handful of trend setters in paris!
1 emergence of norms in isolated subpopulations
it is well-documented in societies that isolated populations can be using contradictory norms  e.g.  driving on the  right  or the  wrong  side of the road. we wanted to replicate this phenomenonusing our social learning framework. when two groups of agents interact only infrequently  it is possible that a different norm emerges in each group. in particular  we are interested in studying the degree of isolation required for divergent norms to emerge in different groups. for our experiments  we consider two groups of equal size and a probability p that agents of different groups interact.
　results from this set of experiments are presented in figure 1. we observe that when the probability of interaction is at least 1  a single norm pervades the entire population. in roughly half of the runs all agents learn to drive on the left and for the other half they learn to drive on the right. but for interaction probabilities of 1 and less there are runs where divergent norms emerge in the two groups  corresponding to the white space above the shaded bars in figure 1 . this is a very interesting observation and we are surprised by the relatively high interaction probabilities that could still sustain divergent norms.
1 conclusions
we investigated a bottom-up process for the evolution of social norm that depends exclusively on individual experiences rather than observations or hearsay. our proposed social learning framework requires each agent to learn from repeated interaction with anonymous members of the society. the goal of this work was to evaluate whether such social learning can successfully evolve and sustain a useful social norm that resolves conflicts and facilitates coordination between population members. our experimentalresults confirm
1 groups evolving a norm with different degree of isolation

1 1 1 1 1 1 1 probability of interaction between agents of different groups
figure 1: two groups of 1 agents each evolve norms with different interactions frequencies  average over 1 runs . when the probability of interaction is low  the groups can evolve different norms.
that such distributed  individual learning is indeed a robust mechanism for evolving stable social norms.
　we investigate the effects of population size  number of actions  different learning strategies  non-learning agents  and multiple relatively isolated populations on the speed and stability of norm evolution. we confirmed that even thorny problems like social dilemmas can be successfully addressed by the social learning framework.
　we would like to study other intriguing phenomena like punctuated equilibria in social norm evolution  young  1  within our framework. other interesting experiments include study of spatial distribution of agents and corresponding effects on rate and divergence of norms.
　acknowledgment: us national science foundation award iis- 1 partially supported this work.
