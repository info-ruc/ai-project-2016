 
a method for learning phonetic features from speech data using connectionist networks is described. a temporal flow model is introduced in which sampled speech data flows through a parallel network from input to output units. the network uses hidden units with recurrent links to capture spectral/temporal characteristics of phonetic features. a supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function. 
   a simple connectionist network with recurrent links was trained on a single instance of the word pair  no  and  go   and successful learned a discriminatory mechanism. the trained network also correctly discriminated 1% of 1 other tokens of each word by the same speaker. a single integrated spectral feature was formed without segmentation of the input  and without a direct comparison of the two items. 
1 	introduction 
connectionist networks offer significant advantages in addressing problems of machine perception because of their inherently parallel structure  which is well matched to the biological architecture that has served as their paradigm. their learning capabilities  robust behavior  noise tolerance and graceful degradation are all capabilities which are becoming increasingly well understood and documented mi. 
   the solution of certain perceptual problems requires that the temporal relationships among stimulus characteristics be properly represented. this is especially true in speech recognition  where the relationship between time and frequency is wonderfully complex. in the production of speech  basic speech units  phonemes  are integrated into a smooth sequence  so that the acoustic boundaries can be very difficult to specify. moreover  phonemes are 
 thanks to alex waibel for helpful discussion and analysis. 
   this research was supported in part by darpa grants n1-k-1 and nooo1-k-1  nsf grants dcrr1  dcr1  mcs1-cer  mcs1  1 ro1-hl-1  
u.s. army grants daa1-1-k-1  daab1-k-f1  u.s. air force grant 1-nm-1  ai center grant nsf-mcs-1  u.s. army research office grant aro-daa1-1  lord corporation  rca and digital equipment corporation. 
often co-produced  coarticulated   so that the phonemes exert a strongly context-dependent interaction. thus  the perception of speech depends on the correct analysis of dynamic temporal/spectral relationships. 
   the connectionist network approach is attractive because it offers a computational model which has inherently robust properties. the networks consist of simple processing elements which integrate their inputs and broadcast the results to the units to which they are connected. thus  the network response to input is the aggregate response of many interconnected units. it is the mutual interaction of many simple components that is the basis for robustness. 
   the problem of designing connectionist networks which can learn the dynamic spectral/temporal characteristics of speech has not yet been widely studied. most work in connectionist networks so far has focussed on the static relationship between input/output pairs  such as associative memories  1   various encoding  decoding  parity and addition problems   and mapping from word spelling to phoneme labels . 
   learning to associate static input/output pairs can be accomplished with layered connectionist networks with feedforward links alone. learning pattern sequences requires network state information  which can be provided by feedback from the network output to the input  1 1 1 . the idea of learning pattern sequences has been applied to a speech task using boltzmann machines . 
   the experiments reported here were designed to explore the capabilities of parallel networks to learn dynamic properties of time-varying data. we choose a standard speech recognition problem to test the extent to which a connectionist network could form an internal representation of the temporal/spectral characteristics which distinguish two similar words. a network architecture was selected in which the hidden and output units included self-recurrent links. this approach is distinguished from the pattern sequence approach in that the feedback is internal to the network and distributed. thus  the dynamic response of individual units must be learned in solving the discrimination task. 
	watrous and shastri 	1 

1 	experiment 
the discrimination between the minimal pair  no  and  go* is a typical speech recognition problem  which is included in a standard database for evaluation of speech recognizers   the utterances  no  and  go  share for the major and final portion the voiced phoneme /o/. the  no  utterance is characterized by a lower energy nasal murmur preceding the transition to the back vowel /o/. this nasal murmur has a formant structure which is due to the coupled resonances of the closed oral cavity and open nasal cavity. the  go*1 is distinguished by a very low energy voicing interval during the lingua-palatal closure  a brief burst as the closure is released  and a voiced transition to the full vowel. 
   the distinction between  no 't and  go   therefore  is concentrated in the brief interval of relatively low energy at the beginning of the word. these differences consist in the relative voicing energy  burst spectrum  and formant value and transition pattern. 
1 	data 
the data used for this experimental work consisted of speech data for a single speaker  gd  from texas instruments standard isolated word recognition database . the speech data was played into a commercial speech recognition device  siemens cse 1   where it was passed through a 1-channel filter bank  full-wave rectified  log compressed and sampled every 1 milliseconds. twentysix repetitions of each word comprise the corpus  for a total of fifty-two utterances  1  no  and 1  go  . the filter bank response to the training utterances is shown in figure 
1. 


figure 1:  network configuration showing input  hidden and output layers  
1 	network architecture 
for this initial experiment  a three-layer connectionist network consisting of an input layer  one hidden layer and an output layer was implemented  as shown in figure 1. the sampled speech data flowed through the network in time sequential order. thus  the 1 channel energies were applied to 1 input units  from which activation spread toward the output units simultaneously as the input units were updated by sequential speech samples. this design will be referred to as the temporal flow model  or  more simply as the flow model. 
　　other approaches have used an array of input units  and represented time along one index of the input unit array  1 1 . in this case  time is spatialized across units. 

figure 1:  channel energies for no/go pair  
1 	perception 

the temporal flow model was chosen because it does not require 'chunking' of variable length utterances onto a fixed size network  it avoids the problem of temporal alignment and symmetry  and the temporal flow model seems to be closer to the biological model of speech processing. 
1.1 	unit functions 
the functions which define the unit behavior were chosen from ones in common use in connectionist networks  1 . the unit outputis a nonlinear  sigmoid  function of the unit potential  which is a simple weighted sum of the output values of units connected by afferent links. the weights correspond roughly to the effect of synaptic strengths. the sigmoid function has the desirable properties of a bounded output  non-linear characteristics  and a response threshold. these functions approximate the computational properties of neural cells  and have convenient mathematical properties for the learning algorithm used in this experiment. 

1.1 	back-propagation learning algorithm 
for this experiment  an extended form of the back-propagation learning algorithm was chosen to accommodate networks with recurrent links  1 . 
   the error-propagation algorithm modifies the unit connection weights in order to minimize the mean squared error between the actual and desired output values. the weight change rule can be written as: 

where 1j t - r  is the error signal at unit j at time t - t  with respect to the target values at the output units at time t . this error is given by: 
for a z
   the error signal for an output unit is defined by the difference between the actual and target values  times the unit function slope at time t: 

   the value of r was limited to a small value to limit the recursive computation. the weight changes were made after each time step. these factors introduced approximations into the computation of the gradient. 
1.1 	target function 
the target function for the output units used in the no/go discrimination experiment consisted of a simple ramp. for the output unit which corresponded to the utterance being trained  the ramp increased from a value of 1 to 1 over the duration of the utterance. the other unit was correspondingly decreased from 1 to 1. this represented the intuition that evidence for or against a particular word accumulates over its duration  and reaches a level of confidence after the utterance is completed. 
1 	results 
the parallel connectionist network experiments were conducted on a sequential machine using a network simulator  written specifically for this experiment. the network described previously was trained on a single pair of no/go utterances by a single speaker for 1 training iterations. 
   the value of the squared-error term during learning was observed; it was neither monotonic decreasing nor a smooth function of the number of optimization iterations. this is thought to be due to the local nature of the weight change algorithm  and the limited extent of backpropagation in time. the error value did reach sharplydefined minimum value after 1 iterations; the network at that point was chosen for further study. 

figure 1:  output unit 1 response to no/go pair  
1 	output unit response 
the response of the output units for the network at the selected critical point in the learning process was recorded  and can be seen in figure 1. the output units respond in equal and opposite ways to the input stimuli; in addition  their time response roughly approximates a ramp. since the learned response closely fits the training function  the network exhibits correct discrimination between the pair of items in the training set. 
the significance of this result should not be overlooked. 
first  the local application of a global optimization metric provided a successful path to the desired network response pattern. second  although no segmentation decisions were made  the network was able to form a discriminating spectral feature which was localized in time. third  the approximations of constant weight value  and restrictions to maximum r value in the extended back-propagation algorithm did not prevent convergence to a good solution. fourth  although the shape of the error contour is unknown  it is almost certainly not smooth; consequently  the learning path apparently avoided local minima in arriving at a solution. 
1 	extension to test set 
in order to test the generality and robustness of the internal representations obtained from the training word pair  the network of least squared error value was tested on a set of 1 additional pairs of no/go utterances by the same speaker. using a simple deterministic decision algorithm  the input word could be clearly categorized by the network response. under these conditions  the trained network successfully discriminated all but one of the test cases  1% accuracy . 
the responses of the hidden units were analyzed for the 
1 test utterances as well as the 1 training utterances. in nearly every respect  the hidden unit responses of the test utterances were isomorphic to the response to the training data. a single hidden unit provided the discriminatory response. in the single error case  this unit failed to respond to the input data. the energy levels for this utterance were very low  especially in the mid to upper channels. 
	watroua and 	shastri 	1 

1 	discussion 
although the results of this initial experiment are unexpectedly encouraging  there are several problems which need to be addressed. the stability of the learning algorithm needs to be improved. this could be accomplished through better target functions  greater accuracy in computing the gradient  or improved learning algorithms. these ideas for improvement have been addressed in subsequent work. more powerful optimization algorithms  secondorder iterative methods  have have resulted in stable learning and greatly increased learning speed. 
1 	conclusions 
in conclusion  several interesting results emerge from this experiment. using a connectionist network with a temporal data flow architecture with recurrent hnks  and using an coarse approximation of the desired output as a teaching function  a successful discriminatory mechanism was learned. this discriminatory feature was formed without segmentation and without a direct comparison of the two items. 
　　the discriminatory mechanism turned out to be very robust  even though based on a single training sample. this result is very encouraging for further research with connectionist networks in deriving robust discriminatory features of phonetic classes. 
   obviously  the goal of this research is to structure networks which can learn the complete set of phonetic class discriminations  so that it could support real-time  continuous speech recognition. this requires larger networks  which for efficiency  may need to be partitioned and recombined. initial steps in this direction have been taken by training networks to discriminate the stop consonants in cv words using various vowels. 
