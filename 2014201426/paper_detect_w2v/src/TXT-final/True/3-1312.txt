 
ensemble learning constitutes one of the main directions in machine learning and data mining. ensembles allow us to achieve higher accuracy  which is often not achievable with single models. one technique  which proved to be effective for constructing an ensemble of diverse classifiers  is the use of feature subsets. among different approaches to ensemble feature selection  genetic search was shown to perform best in many domains. in this paper  a new strategy gas-sefs  genetic algorithm-based sequential search for ensemble feature selection  is introduced. instead of one genetic process  it employs a series of processes  the goal of each of which is to build one base classifier. experiments on 1 data sets are conducted  comparing the new strategy with a previously considered genetic strategy for different ensemble sizes and for five different ensemble integration methods. the experiments show that gas-sefs  although being more time-consuming  often builds better ensembles  especially on data sets with larger numbers of features. 
1 introduction 
a popular method for creating an accurate model from a set of training data is to construct a set  ensemble  of classifiers. it was shown that an ensemble is often more accurate than any of the single classifiers in it. the integration of classifiers is currently an active research area in the machine learning and neural networks communities  dietterich  1 . 
﹛both theoretical and empirical research have demonstrated that a good ensemble should include diverse base classifiers. another important issue in creating an effective ensemble is the choice of the function for combining the predictions of the base classifiers. it was shown that increasing the ensemble diversity is not enough to ensure increased accuracy - if the integration method does not properly utilize the ensemble diversity  then no benefit arises from the integration  brodley and lane  1 . 
﹛one effective approach for generating an ensemble of diverse classifiers is the use of feature subsets  or ensemble feature selection  opitz  1 . by varying the feature subsets used to generate the base classifiers  it is possible to promote diversity and produce base classifiers that tend to err in different sub-areas of the instance space. 
﹛feature selection algorithms  including ensemble feature selection  are typically composed of the following components  aha and bankert  1  opitz  1 :  1  search strategy  that searches through the space of feature subsets; and  1  fitness function  that inputs a feature subset and outputs a numeric evaluation. the search strategy's goal is to find a feature subset maximizing this function. 
﹛it is reasonable to include in the fitness function  explicitly or implicitly  both accuracy and diversity. one measure of fitness  which was proposed in  opitz  1   defines fitness fitnessi of classifier i corresponding to feature subset i to be proportional to classification accuracy acci and diversity divi of the classifier: 
fitnessi = acci +汐  divi    1  
where 汐 reflects the influence of diversity. diversity divi is the contribution of classifier i to the total ensemble diversity  which can be measured as the average pairwise diversity for all the pairs of classifiers including i. this fitness function was also used in experiments in  tsymbal et al.  1; 1   and it is used in the experiments in this paper. 
﹛in  tsymbal et al.  1  a genetic search-based strategy ga has been introduced. it uses genetic search for evolving the initial population built with random subspacing. ga was shown to perform best on average with respect to the other three strategies  and two diversity measures were best for ga of the five considered measures: the kappa statistic  and the fail/non-fail disagreement. 
﹛in this paper  we introduce a new genetic search-based strategy for ensemble feature selection  gas-sefs  which  instead of maintaining a set of feature subsets in each generation like in ga  consists in applying a series of genetic processes  one for each base classifier  sequentially. 
﹛the paper is organized as follows. in section 1 the task of ensemble feature selection is considered. in section 1 we present two strategies for genetic ensemble feature selection  ga and gas-sefs  and two diversity measures. in section 1 different methods for ensemble integration are reviewed. in section 1 we present our experiments with the two genetic strategies and conclude in the next section with a summary and assessment of further research topics.  
1 ensemble feature selection and random subspacing  
the task of using an ensemble of models can be broken down into two basic questions:  1  what set of models should be generated ; and  1  how should the predictions of the models be integrated   dietterich  1 . 
﹛one effective approach to ensemble generation is the use of different subsets of features for each model. finding a set of feature subsets for constructing an ensemble is also known as ensemble feature selection  opitz  1 . while traditional feature selection algorithms have the goal of finding the best feature subset that is suitable to both the learning problem and the learning algorithm  the task of ensemble feature selection has the additional goal of finding a set of feature subsets that will promote diversity among the base classifiers  opitz  1 . 
﹛ho  has shown that simple random selection of features may be an effective technique for ensemble feature selection because the lack of accuracy in the ensemble members is compensated for by their diversity. this technique is called the random subspace method or simply random subspacing  rs . 
﹛with rs one may solve the small sample size problem  because the training sample size relatively increases in random subspaces. ho  shows that while most other classification methods suffer from the curse of dimensionality  this method does not. rs has much in common with bagging  skurichina and duin  1   but instead of sampling instances  one samples features. like bagging  rs is a parallel learning algorithm  that is  generation of each base classifier is independent. this makes it suitable for parallel implementation that is desirable in some practical applications. it was shown that  like in bagging  accuracy could be only increased with the addition of new members  even when the ensemble complexity grew  ho  1 . 
﹛rs is used as a base in a number of ensemble feature selection strategies  e.g. gefs  genetic ensemble feature selection   opitz  1  and hc  hill climbing   cunningham and carney  1 . 
1 two strategies for genetic ensemble feature selection 
1 ga and gas-sefs  
the use of genetic search has been an important direction in the feature selection research. genetic algorithms have been shown to be effective global optimization techniques. the use of genetic algorithms for ensemble feature selection was first proposed in  kuncheva  1  and further elaborated in  kuncheva and jain  1 . as the fitness function in  kuncheva  1; kuncheva and jain  1  the ensemble accuracy was used instead of the accuracy of the base classifiers. however  such a fitness function is biased towards some particular integration method  often simple voting . besides  as it was shown e.g. in  kuncheva  1   such a design is prone to overfitting  and some additional preventive measures are needed to be taken to avoid this  as including in the fitness function penalty terms accounting for the number of features . the use of individual accuracy and diversity as in  1  is an alternative solution to this problem. another motivation for this alternative is the fact that overfitting at the level of the base classifiers is more desirable than overfitting of the ensemble itself. it was shown recently in several studies that an ensemble of overfitted members might often be better than an ensemble of non-overfitted members. for example  in  street and kim  1  pruning trees resulted in decreased ensemble accuracy  even though the accuracy of the trees themselves increased. 
﹛the genetic algorithm for ensemble feature selection  ga   tsymbal et al.  1  is based on the gefs strategy of opitz . gefs was the first genetic algorithm for ensemble feature selection that explicitly used diversity in its fitness function. ga begins with creating an initial population with rs. then  new candidate classifiers are produced by crossover and mutation. after producing a certain number of individuals the process continues with selecting a new subset of candidates randomly with a probability proportional to fitness  so called roulette-wheel selection . the process of creating new classifiers and selecting a subset of them  a generation  continues a certain number of times. after a predefined number of generations  the fittest individuals make up the population  which comprises the ensemble. the representation of each individual is a constantlength string of bits  where each bit corresponds to a particular feature. the crossover operator uses uniform crossover  in which each feature of the two children takes randomly a value from one of the parents. the mutation operator randomly toggles a number of bits in an individual. 
﹛instead of maintaining a set of feature subsets in each generation of one genetic process  gas-sefs  genetic algorithm-based sequential search for ensemble feature selection  uses a series of genetic processes  one for each base classifier  sequentially. pseudo-code for gas-sefs is given in figure 1. after each genetic process one base classifier is selected into the ensemble. gas-sefs uses the same fitness function  1   but diversity is calculated with the base classifiers already formed by previous genetic processes instead of the members of current population. in the first ga process  the fitness function has to use accuracy only. gassefs uses the same genetic operators as ga. 
for i=1 to ensemblesize 
  for j=1 to 1 population j =rsm #features ; 
  for j=1 to #generations 
    for k=1 to 1 calculatefitness population k  ;     for k=1 to 1 
    //randomly proportional to log 1+fitness  
         l m =select1 population ; 
        offsprings k =crossover l m ; 
    endfork 
     for k=1 to 1  mutate1 offsprings 1+k  ;             mutate1 offsprings 1+k  ; 
     for k=1 to 1 calculatefitness offsprings k  ; //randomly proportional to fitness 
population=select1 population+offsprings ; 
endforj 
  //according to fitness 
   baseclassifier i =select1 population ;   endfori 
figure 1 pseudo-code for the gas-sefs algorithm 
ga has a number of peculiarities  which we use also in gas-sefs. full feature sets are not allowed in rs nor may the crossover operator produce a full feature subset. individuals for crossover are selected randomly proportional to log 1+fitness  instead of just fitness  which adds more diversity into the new population. the generation of children identical to their parents is prohibited. to provide a better diversity in the length of feature subsets  two different mutation operators are used  mutate1 and mutate1   one of which always deletes features randomly with a given probability  and the other - adds features. 
﹛parameter settings for our implementation of ga and gas-sefs include a mutation rate of 1%  a population size of 1  a search length of 1 feature subsets  the number of new individuals produced by crossover and mutation   of which 1 are offsprings of the current population of 1 classifiers generated by crossover  and 1 are mutated offsprings  1 with each mutation operator . 1 generations of individuals were produced  as our pilot studies have shown that in most cases  with this configuration  the ensemble accuracy does not improve after 1 generations  due to overfitting the training data. 
﹛the complexity of ga does not depend on the number of features  and is o s∩  ngen    where s∩ is the number of individuals in one generation  and ngen is the number of generations  tsymbal et al.  1 . the complexity of gassefs is o s  s∩  ngen    where s is the number of base classifiers. in our experiments  on average  ga and gassefs look through about 1 and 1 feature subsets correspondingly  given that the number of base classifiers is 1  the number of individuals in a generation is 1  and the number of generations is 1 . 
1 diversity measures used in the fitness function 
the fail/non-fail disagreement measure and the kappa statistic were shown to provide the best performance for ga in  tsymbal et al.  1 .  
the fail/non-fail disagreement measure was defined in 
 skalak  1  as the percentage of test instances for which the classifiers make different predictions but for which one of them is correct: 
n1+ n1
div disi j = 1 n1 + n1+ n1    1  
n +
where nab is the number of instances  classified correctly  a=1  or incorrectly  a=1  by classifier i  and correctly  b=1  or incorrectly  b=1  by classifier j. the denominator in  1  is equal to the total number of instances. the fail/non-fail disagreement varies from 1 to 1. 
﹛the kappa statistic was first introduced in  cohen  1 . let nij be the number of instances  recognized as class i by the first classifier and as class j by the second one  ni* is the number of instances recognized as i by the first classifier  and n*i is the number of instances recognized as i by the second classifier. define then 成1 and 成1 as: 
l
成1 = i﹉=1nii   and 成1 = ﹉l     ni*   n*i       1  
	n	i=1 n	n  
where l is the number of classes and n is the total number of instances. 成1 estimates the probability that the two classifiers agree  and 成1 is a correction term  which estimates the probability that the two classifiers agree simply by chance  in the case where each classifier chooses to assign a class label randomly . the pairwise diversity div kappai j is defined as follows: 
div kappai j =成1 成1 .  1  
1 成1
we normalize this measure to vary from 1 to 1. 
1 integration of an ensemble of models 
the challenging problem of integration is to decide which of the classifiers to select or how to combine the results produced by the base classifiers. a number of selection and combination approaches have been proposed. 
﹛one of the most popular and simplest techniques used to combine the results of the base classifiers  is simple voting  also called majority voting   bauer and kohavi  1 . in the voting  the output of each base classifier is considered as a vote for that particular class value. the class value that receives the biggest number of votes is selected as the final classification. weighted voting  wv   where each vote has a weight proportional to the estimated generalization performance of the corresponding classifier  works usually better than simple voting  bauer and kohavi  1 . 
a number of selection techniques have also been proposed to solve the integration problem. one of the most popular and simplest selection techniques is cross-validation majority  cvm  also called single best; we call it simply static selection  ss  in our experiments   schaffer  1 . in cvm  the cross-validation accuracy for each base classifier is estimated  and then the classifier with the highest accuracy is selected. 
﹛the described above approaches are static. they select one model for the whole data space or combine the models uniformly. in dynamic integration each new instance to be classified is taken into account. usually  better results can be achieved if integration is dynamic. 
﹛we consider in our experiments three dynamic techniques based on the same local error estimates: dynamic selection  ds   dynamic voting  dv   and dynamic voting with selection  dvs   tsymbal and puuronen  1 . they contain two main phases. first  at the learning phase  the local classification errors of each base classifier for each instance of the training set are estimated according to the 1 loss function using cross validation. the learning phase finishes with training the base classifiers on the whole training set. the application phase begins with determining k-nearest neighbours for a new instance using a given distance metric. then  weighted nearest neighbour regression is used to predict the local classification errors of each base classifier for the new instance. 
﹛after  ds simply selects a classifier with the least predicted local classification error. in dv  each base classifier receives a weight that is proportional to its estimated local accuracy  and the final classification is produced as in wv. in dvs  the base classifiers with the highest local classification errors are discarded  the classifiers with errors that fall into the upper half of the error interval  and locally weighted voting  dv  is applied to the remaining classifiers. 
1 experimental investigations 
1 experimental setup 
the experiments are conducted on 1 data sets taken from the uci machine learning repository  blake et al.  1 . these data sets include real-world and synthetic problems  vary in characteristics  and were previously investigated by other researchers. the main characteristics of the data sets are presented in table 1. 
table 1 data sets and their characteristics 
data set instances classes features categ. num. balance 1 1 1 1breast cancer 1 1 1 1car 1 1 1 1diabetes 1 1 1 1glass recognition 1 1 1 1heart disease 1 1 1 1ionosphere 1 1 1 1iris plants 1 1 1 1led 1 1 1 1led1 1 1 1 1liver disorders 1 1 1 1lymphography 1 1 1 1monk-1 1 1 1 1monk-1 1 1 1 1monk-1 1 1 1 1soybean 1 1 1 1thyroid 1 1 1 1tic-tac-toe 1 1 1 1vehicle 1 1 1 1voting 1 1 1 1zoo 1 1 1 1 
as in  tsymbal et al.  1; 1   we use simple bayes  sb  as the base classifier in the ensembles. it has been recently shown experimentally and theoretically that sb can be optimal even when the  na ve  feature-independence assumption is violated by a wide margin  domingos and pazzani  1 . second  when sb is applied to the subproblems of lower dimensionalities  the error bias of the bayesian probability estimates caused by the featureindependence assumption becomes smaller. it also can easily handle missing feature values. besides  it has advantages in terms of simplicity  learning speed  classification speed  and storage space. we believe that dependencies and conclusions presented in this paper do not depend on the learning algorithm used and would be similar for most known learning algorithms. 
﹛to evaluate ga and gas-sefs  we have used stratified random-sampling cross validation with 1 percent of instances in the training set. the remaining 1 percent of instances were divided into two sets of approximately equal size  a validation set and a test set . 1 test runs of were made on each data set for each search strategy and diversity.  
﹛four different ensemble sizes have been tested: 1  1  1  and 1. the ensemble size did not exceed 1 due to two main reasons:  1  limitation in computational resources  and  1  it was shown in experiments that for guided ensemble construction such as genetic search the biggest gain is achieved already with 1 base classifiers  and much less classifiers are needed than with unguided ensemble construction such as rs and bagging.  
﹛at each run of the algorithm  accuracies for the five types of ensemble integration are collected: static selection  ss   weighted voting  wv   dynamic selection  ds   dynamic voting  dv   and dynamic voting with selection  dvs . we have collected ensemble characteristics for four numbers of generations: 1  1  1  and 1. 
﹛to reduce the number of possible combinations of parameters  we conducted a separate series of preliminary experiments using the wrapper approach based on cross validation to select the best diversity coefficient 汐 and the number of nearest neighbors in dynamic integration k as in  tsymbal et al.  1 . we have experimented with seven values of 汐: 1  1  1  1  1  1  and 1. seven values: 1  1  1  1  1  1  1   1n  1  n =1 ... 1   were used for k. from the experimental results we could see that the best value of k depended mostly only on the integration method used and on the data set. the best 汐's varied with the search strategy  integration method  and data set used.  
﹛after  the experiments were repeated with the selected values of 汐 and k. although the same data were used for the selection and for the later experiments  we believe that this did not lead to overfitting due to the small number of possible values for 汐 and k. 
﹛the test environment was implemented within the mlc++ framework  the machine learning library in c++   kohavi et al.  1 . a multiplicative factor of 1 was used for the laplace correction in sb as in  domingos and pazzani  1 . numeric features were discretized into ten equal-length intervals  or one per observed value  whichever was less   as it was done in  domingos and pazzani  1 . although this approach was found to be slightly less accurate than more sophisticated ones  it has the advantage of simplicity  and is sufficient for comparing different ensembles of sb classifiers with each other.  
1 experimental results 
to validate our findings  we divided all data sets into two groups: with less than 1 features  1 data sets  group 1   and with greater than or equal to 1 features  1 data sets  group 1 ; and checked all the characteristics for these groups. the ensemble accuracies were nearly the same for the two diversity measures  and the fail/non-fail disagreement was slightly better on average  so we present results for this diversity measure only here. in figure 1 the ensemble accuracies for strategies ga and gas-sefs  over the two groups of data sets and four ensemble sizes are shown averaged over the data sets for the best integration method  dvs . it can be seen from the figure that gas-sefs builds even more accurate ensembles than ga; especially for group 1 including data sets with larger numbers of features. accuracy grows with the ensemble size  but the growth flattens as the number of base classifiers increases.  
 
 

figure 1 ensemble accuracies for two strategies  two groups of data sets  and four ensemble sizes 
in figure 1 ensemble accuracies are shown for the two strategies  five integration methods  and four ensemble sizes on the tic-tac-toe data set  as a representative of group 1 including 1 instances and 1 features. this figure supports our previous findings. besides  it could be seen that dynamic integration  expectedly  outperforms static integration both for ga and for gas-sefs. accuracy grows with the ensemble size and this growth is greater for the best integration methods  ds and dvs in this case .  
 

  figure 1 ensemble accuracies for ga  left  and gas-sefs  right  for five integration methods and four ensemble sizes on the tictac-toe data set 
the difference between the two strategies is clearer for the best integration methods. the dependencies were the same for all the data sets  with sometimes lesser difference between the integration methods. for some data sets dv outperforms ds  which supports the previous findings about behaviour of the integration methods  tsymbal and puuronen  1; tsymbal et al.  1 . 
1 other interesting findings 
selected values of 汐 were different for different data sets  supporting findings in  tsymbal et al.  1; 1 . in general  for both strategies  汐 for the dynamic integration methods is bigger than for the static ones  1 vs 1 on average . gas-sefs needs slightly higher values of 汐 than ga  1 vs 1 on average . this can be explained by the fact that gas-sefs always starts with a classifier  which is based on accuracy only  and the subsequent classifiers need more diversity than accuracy. 
﹛the number of selected features falls as the ensemble size grows  and this is especially clear for gas-sefs  as the base classifiers need more diversity. as a rule  more features are needed in the static integration methods than in the dynamic ones to achieve better accuracy. gas-sefs results in slightly smaller feature subsets on average  1% vs 1% of features for dynamic integration strategies . 
﹛as it was also reported in  tsymbal et al.  1   the selected k-neighbourhood values for dynamic integration change with the integration method. ds needs higher values of k. this can be explained by the fact that its prediction is based on only one classifier being selected  and thus  it is very unstable. higher values of k provide more stability to ds. the average selected k is equal to 1 for ds  and it is only 1 for dv. for dvs  as a hybrid strategy  it is in between at 1. the selected values of k do not change significantly with the change of the search strategy and the ensemble size.  
﹛experimental results for both ga and gas-sefs show that the static integration methods  ss and wv  and the dynamic ds start to overfit the validation set already after 1 generations and show lower accuracies  whereas the accuracies of dv and dvs continue to grow up to 1 generations. this shows the importance of selection of the appropriate integration method for the genetic strategies. 
1 conclusions 
in our paper  we have considered two genetic search strategies for ensemble feature selection. the new strategy  gassefs  consists in employing a series of genetic search processes  one for each base classifier. it was shown in experiments that gas-sefs results in better ensembles having greater accuracy in many domains  especially for data sets with relatively larger numbers of features. gas-sefs is significantly more time-consuming than ga. however  it can be easily parallelized in a multiprocessor setting  and one processor could be used for each offspring in the current generation. 
﹛one of the reasons for the success of gas-sefs is the fact that each of the core ga processes leads to significant overfitting of a corresponding ensemble member  and  as it was shown before  an ensemble of overfitted members is often better that an ensemble of non-overfitted members. 
﹛in  oliveira et al.  1  it was shown that besides the use of weights to combine a number of objectives in the fitness function in genetic algorithms  as the use of 汐 in our case   another common approach that often gives better results for 
single feature subset selection is based on pareto-front dominating solutions. adaptation of this technique to ensemble feature selection is an interesting topic for further research. 
acknowledgments 
this study is supported by science foundation ireland and comas graduate school of the university of jyv skyl   finland. we would like to thank the uci machine learning repository for the data sets  and the mlc++ library for the source code used in this study. 
