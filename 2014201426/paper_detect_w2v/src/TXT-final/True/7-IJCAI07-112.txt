
we present a case-based approach to multilabel ranking  a recent extension of the well-known problem of multilabel classification. roughly speaking  a multilabel ranking refines a multilabel classification in the sense that  while the latter only splits a predefined label set into relevant and irrelevant labels  the former furthermore puts the labels within both parts of this bipartition in a total order. we introduce a conceptually novel framework  essentially viewing multilabel ranking as a special case of aggregating rankings which are supplemented with an additional virtual label and in which ties are permitted. even though this framework is amenable to a variety of aggregation procedures  we focus on a particular technique which is computationally efficient and prove that it computes optimal aggregations with respect to the  generalized  spearman rank correlation as an underlying loss  utility  function. moreover  we propose an elegant generalization of this loss function and empirically show that it increases accuracy for the subtask of multilabel classification.
1 introduction
multilabel ranking  mlr  is a recent combination of two supervised learning tasks  namely multilabel classification  mlc  and label ranking  lr . the former studies the prob-

lem of learning a model that associates with an instance x a bipartition of a predefined set of class labels into relevant  positive  and irrelevant  negative  labels  while the latter considers the problem to predict rankings  total orders  of all class labels. a mlr is a consistent combination of these two types of prediction. thus  it can either be viewed as an extended ranking  containing additional information about a kind of  zero point    or as an extended mlc  containing additional information about the order of labels in both parts of the bipartition   brinker et al.  1 . for example  in a document classification context  the intended meaning of the mlr  eco   is that  for the instance  = document  x  the classes  = topics  politics and economics are relevant  the former even more than the latter  whereas education and sports are irrelevant  the former perhaps somewhat less than the latter.
모from an mlc point of view  the additional order information is not only useful by itself but also facilitates the postprocessing of predictions  e.g.  considering only the at most topk relevant labels . regarding the relation between mlc and mlr  we furthermore like to emphasize two points: firstly  as will be seen in the technical part below  mlr is not more demanding than mlc with respect to the training information  i.e.  a multilabel ranker can well be trained on multilabel classification data. secondly  inducing such a ranker can be useful even if one is eventually only interested in an mlc. roughly speaking  an mlr model consists of two components  a classifier and a ranker. the interdependencies between the labels which are learned by the ranker can be helpful in discovering and perhaps compensating errors of the classifier. just to illustrate  suppose that the classifier estimates one label to be relevant and a second one not. the additional  conflicting  information that the latter is typically ranked above the former might call this estimation into question and thus repair the misclassification.
모hitherto existing approaches operating in ranking scenarios are typically model-based extensions of binary classification techniques which induce a global prediction model for the entire instance space from the training data  har-peled et al.  1; furnkranz and h몮 ullermeier  1몮  . these approaches  briefly reviewed in section 1  suffer substantially from the increased complexity of the target space in multilabel ranking  in comparison to binary classification   thus having a high level of computational complexity already for a moderate number of class labels.
모in sections 1 and 1  we present an alternative framework for mlr using a case-based methodology which is conceptually simpler and computationally less complex. one of the main contributions of this paper is casting multilabel ranking as a special case of rank aggregation  with ties  within a case-based framework. while our approach is not limited to any particular aggregation technique  we focus on a computationally efficient technique and prove that it computes optimal aggregations with respect to the well-known  generalized  spearman rank correlation as an accuracy measure. in section 1  we show that our case-based approach compares favorably with model-based alternatives  not only with respect to complexity  but also in terms of predictive accuracy.
1 problem setting
in so-called label ranking  the problem is to learn a mapping from an instance space x to rankings over a finite set of labels l = {뷂1 ...뷂c}  i.e.  a function that maps every instance x 뫍 x to a total strict order x  wheremeans that  for this instance  label 뷂i is preferred to  ranked higher than  뷂j. a ranking over l can conveniently be represented by a permutation 뷉 of {1...c}  where 뷉 i  denotes the position of label 뷂i in the ranking. the set of all permutations over c labels  subsequently referred to as sc  can hence be taken as the target space in label ranking.
모multilabel ranking  mlr  is understood as learning a model that associates with a query input x both a ranking x and a bipartition  multilabel classification  mlc  of the label set l into relevant  positive  and irrelevant  negative  labels  i.e.  subsets px nx   l such that px 뫌 nx =   and px 뫋 nx = l  brinker et al.  1 . furthermore  the ranking and the bipartition have to be consistent in the sense that 뷂i 뫍 px and 뷂j 뫍 nx implies.
모as an aside  we note that  according to the above consistency requirement  a bipartition  px nx  implicitly also contains ranking information  relevant labels must be ranked above irrelevant ones . this is why an mlr model can be trained on standard mlc data  even though it considers an extended prediction task.
1 model-based multilabel ranking
a common model-based approach to mlc is binary relevance learning  br . br trains a separate binary model mi for each label 뷂i  using all examples x with 뷂i 뫍 px as positive examples and all those with 뷂j 뫍 nx as negative ones. to classify a new instance x  the latter is submitted to all models  and px is defined by the set of all 뷂i for which mi predicts relevance.
모br can be extended to the mlr problem in a straightforward way if the binary models provide real-valued confidence scores as outputs. a ranking is then simply obtained by ordering the labels according to these scores  schapire and singer  1 . on the one hand  this approach is both simple and efficient. on the other hand  it is also ad-hoc and has some disadvantages. for example  good estimations of calibrated scores  e.g.  probabilities  are often hard to obtain. besides  this approach cannot be extended to more general types of preference relations such as  e.g  partial orders. for a detailed survey about mlc and mlr approaches  including case-based methods  we refer the reader to  tsoumakas et al.  1 .
모brinker et al.  presented a unified approach to calibrated label ranking which subsumes mlr as a special case. their framework enables general label ranking techniques  such as the model-based ranking by pairwise comparison  rpc   furnkranz and h몮 ullermeier  1몮   and constraint classification  cc   har-peled et al.  1   to incorporate and exploit partition-related information and to generalize to settings where predicting a separation between relevant and irrelevant labels is required. this approach does not assume the underlying binary classifiers to provide confidence scores. instead  the key idea in calibrated ranking is to add a virtual label 뷂1 as a split point between relevant and irrelevant labels 
i.e.  a calibrated ranking is simply a ranking of the extended label set l 뫋 {뷂1}. such a ranking induces both a ranking among the  real  labels l and a bipartite partition  px nx  in a straightforward way: px is given by those labels which are ranked higher than 뷂1  nx by those which are ranked lower. the semantics of the virtual label becomes clear from the construction of training examples for the binary learners: every label 뷂i known to be relevant is preferred to the virtual label ; likewise  뷂1 is preferred to all irrelevant labels. adding these preference constraints to the preferences that can be extracted for the regular labels  a calibrated ranking model can be learned by solving a conventional ranking problem with c + 1 labels. we have discussed this approach in more detail as we will advocate a similar idea in extending case-based learning to the multilabel ranking scenario.
1 case-based multilabel ranking
case-based learning algorithms have been applied successfully in various fields such as machine learning and pattern recognition  dasarathy  1 . in previous work  we proposed a case-based approach which is tailored to label ranking  hence  it cannot exploit bipartite data and does not support predicting the zero point for the multilabel ranking scenario  brinker and hullermeier  1몮  . these algorithms defer processing the training data until an estimation for a new instance is requested  a property distinguishing them from model-based approaches. as a particular advantage of delayed processing  these learning methods may estimate the target function locally instead of inducing a global prediction model for the entire input domain from the data.
모a typically small subset of the entire training data  namely those examples most similar to the query  is retrieved and combined in order to make a prediction. the latter examples provide an obvious means for  explaining  a prediction  thus supporting a human-accessible estimation process which is critical to certain applications where black-box predictions are not acceptable. for label ranking problems  this appealing property is difficult to realize in algorithms using complex global models of the target function as the more complex structure of the underlying target space typically entails solving multiple binary classification problems  rpc yields c c + 1 /1 subproblems  or requires embedding the training data in a higher dimensional feature space to encode preference constraints  such as for cc .
모in contrast to the model-based methodology which suffers substantially from the increased complexity of the target space in mlr  we will present a case-based approach where the complexity of the target space solely affects the aggregation step which can be carried out in a highly efficient manner.
모the k-nearest neighbor algorithm  k-nn  is arguably the most basic case-based learning method  dasarathy  1 . in its simplest version  it assumes all instances to be represented by feature vectors in the n-dimensional space x = rn endowed with the standard euclidian metric as a distance measure  though an extension to other instance spaces and more general distance measures d 몫 몫  is straightforward. when a query feature vector x is submitted to the k-nn algorithm  it retrieves the k training instances closest to this point in terms of d 몫 몫 . in the case of classification learning  the k-nn algorithm estimates the query's class label by the most frequent label among these k neighbors. it can be adapted to the regression learning scenario by replacing the majority voting step with computing the  weighted  mean of the target values.
모in order to extend the basic k-nn algorithm to multilabel learning  the aggregation step needs to be adapted in a suitable manner. to simplify our presentation  we will focus on the standard mlc case where the training data provides only a bipartition into relevant and non-relevant labels for each instance. later on  we will discuss how to incorporate more complex preference  ranking  data for training.
모let us consider an example  x px nx  from a standard mlc training dataset. as stated above  the key idea in calibrated ranking is to introduce a virtual label 뷂1 as a split point to separate labels from px and nx  respectively  and to associate a set of binary preferences with x. we will adopt the idea of a virtual label but  instead of associating preferences  use a more direct approach of viewing the sequence of the label sets  px {뷂1} nx  as a ranking with ties  also referred to as a bucket order  fagin et al.  1 . more precisely  a bucket order is a transitive binary relation  for which there exist sets b1 ...bm that form a partition of the domain d
 which is given by d = l 뫋 {뷂1} in our case  such that  if and only if there are i j with i   j such that
. using this notation  the mlr sce-
nario corresponds to a generalized ranking setting with three buckets  where b1 = px  b1 = {뷂1} and b1 = nx.
모if the training data provides not only a bipartition  px nx  but also a ranking  with ties  of labels within both parts  this additional information can naturally be incorporated: assume that bi+1 ...bpjx   respectively. then  we can combine this addi-and nx form bucket orders  b1 ...bi 1  and tional information into a single ranking with ties in a straightforward way as  b1 ...bi 1 bi bi+1 ...bj   where bi = {뷂1} represents the split point. note that the following analysis only assumes that the training data can be converted into rankings with ties  with the virtual label specifying the relevance split point. it will hence cover both training data of the standard mlc case as well as the more complex mlr scenario.
모a bucket order induces binary preferences among labels but moreover forms a natural representation for generalizing various metrics on strict rankings to rankings with ties. to this end  we define a generalized rank  i  for each label 뷂i 뫍 d as the average overall position  i  =
l j | within the bucket bj which contains 뷂i. fagin et al.  proposed several generalizations of well-known metrics such as kendall's tau and the spearman footrule distance  where the latter can be written as the l1 distance of the generalized ranks associated with the bucket orders .
모given a metric l  a natural way to measure the quality of a single ranking  as an aggregation of the set of rankings 1 ...횲 is to compute the sum of pairwise distances:
. then  aggregation of rankings leads
to the optimization problem of computing a consensus ranking   not necessarily unique  such that l   = min뷉 l 뷉 .
모the remaining step to actually solve multilabel ranking using the case-based methodology is to incorporate methods which compute  approximately  optimal solutions for the latter optimization problem. as we do not exploit any particular property of the metric l  this approach provides a general framework which allows us to plug in any optimization technique suitable for a metric on rankings with ties in order to aggregate the k nearest neighbors for a query instance x.
모the complexity of computing an optimal aggregation depends on the underlying metric and may form a bottleneck as this optimization problem is np-hard for kendall's tau  bartholdi et al.  1  and spearman's footrule metric on bucket orders  dwork et al.  1 .1 hence  computing an optimal aggregation is feasible only for relatively small label sets {뷂1 ...뷂c}. there exist  however  approximate algorithms with quadratic complexity in c which achieve a constant factor approximation to the minimal sum of distances l for kendall's tau and the footrule metric  fagin et al.  1 .
모while approximate techniques in fact provide a viable option  we will present a computationally efficient and exact method for a generalization of the sum of squared rank differences metric in the following section to implement a version of our case-based multilabel ranking framework.
1 aggregation analysis
the spearman rank correlation coefficient  a linear transformation of the sum of squared rank differences metric  is a natural and well-known similarity measure on strict rankings  spearman  1 . it can be generalized to the case of rankings with ties in the same way as the spearman footrule metric  where  integer  rank values for strict rankings are substituted with average bucket locations. hence  for any two bucket orders  the generalized squared rank difference metric is defined as
	.	 1 
the following theorem shows that an optimal aggregation with respect to the l1 metric can be computed by ordering the labels according to their  generalized  mean ranks.
theorem 1. let 1 ...횲 be rankings with ties on d = {뷂1 ...뷂c}. suppose  is a permutation such that the labels 뷂i are ordered according to   ties are broken
arbitrarily . then 
		 1 
모before we proceed to the formal proof  note that the key point in theorem 1 is that the minimum is taken over sc while it is well-known that the minimizer in rc would be the mean rank vector. for strict rankings with unique mean rank values  the optimal-aggregation property was proved in  dwork et al.  1 . a proof for the more general case of non-unique rank values can be derived from  hullermeier and f몮 urnkranz 몮 1 .
모the following proof is an adaptation of  hullermeier and몮 furnkranz  1몮   where the ranking by pairwise comparison voting procedure for complete strict rankings was analyzed in a probabilistic risk minimization scenario. an essential building block of our proof is the subsequent observation on permutations:
lemma 1   hullermeier and f몮	urnkranz  1몮	  . let mi  i =
1...c  be real numbers ordered such that 1 뫞 m1 뫞 m1 뫞 몫몫몫 뫞 mc. then  for all permutations 뷉 뫍 sc 
	.	 1 
proof  theorem 1 . let us define 1...c. then 

in the last equation  the mid-term equals 1 as
.
furthermore  the last term is a constant 횱 i  1 which does not depend on 뷉. hence  we obtain
.
the proof follows directly from lemma 1.	
모we have proved that an l1-optimal aggregation with respect to the set of permutations can be computed by ordering the labels according to their mean ranks. regarding the complexity  this method requires computational time in the order of o kc + clogc  for computing and sorting the mean ranks  hence  providing a very efficient aggregation technique. note that this method aggregates rankings with ties into a single strict ranking. the related problem of aggregating into a ranking where ties are allowed forms an interesting area of research in itself and for the case of the l1-metric the required complexity is an open question. moreover  multilabel ranking requires predicting strict rankings such that an intermediate aggregation into a ranking with ties would entail an additional postprocessing step and hence forms a less intuitive approach to this problem.
모as stated above  the virtual label 뷂1 is associated with the second bucket b1 = {뷂1} in order to provide a relevance split point. in an initial empirical investigation  we observed that l1-optimal rankings in k-nn multilabel ranking yield good performance with respect to standard evaluation measures on the ranking performance  while the accuracy in terms of multilabel classification measures reached a reasonable  yet not entirely satisfactory level. this observation may be attributed to the fact that the l1-metric penalizes misplaced labels equally for all labels including 뷂1. however  particularly in the context of multilabel classification  뷂1 carries a special degree of importance and therefore misclassifications in the aggregation step should be penalized more strongly. in other words  reversing the preference between two labels is especially bad if one of these labels is 뷂1  as it means misclassifying the second label in an mlc sense.
모to remedy this problem  our approach can be extended in a consistent and elegant manner: instead of a single virtual label 뷂1  we consider a set of virtual labels {뷂1 ...뷂1 p} which is associated with the split bucket bi. in doing so  the theoretical analysis on the aggregation remains valid and the parameter p provides a means to control the penalty for misclassifications in aggregating rankings. note that the computational complexity does not increase as the expansion into a set of virtual split labels can be conducted implicitly. moreover  on computing a prediction  the set of virtual labels can be merged into a single label again in a consistent way as all labels have the same mean rank value.
모to illustrate this  gap broadening  control mechanism  let us take a look at a simple aggregation example with three mlc-induced rankings using a single virtual label:

these bucket orders would be aggregated into a total order such that p =   and n = {뷂1 ...뷂1} as m1 = 1  mean rank of 뷂1  and every other mean rank is greater  including m1 =
1. using a set of two virtual labels  we obtain m1 = m1 = 1  hence  the order of these labels is determined randomly. finally  for three virtual labels  m1 = 1 and m1 = 1 such that the aggregated calibrated ranking corresponds to a multilabel classification p = {뷂1} and n = {뷂1 뷂1 뷂1 뷂1}.
1 empirical evaluation
the purpose of this section is to provide an empirical comparison between state-of-the-art model-based approaches and

figure 1: gap amplification on the  functional  yeast dataset: the estimated hamming loss clearly decreases in the parameter p  which controls the number of virtual labels used for splitting relevant and irrelevant labels.
our novel case-based framework  using the l1-minimizing aggregation technique . the datasets that were included in the experimental setup originate from the bioinformatics fields where multilabeled data can frequently be found. more precisely  our experiments considered two types of genetic data  namely phylogenetic profiles and dna microarray expression data for the yeast genome  consisting of 1 genes.1 every gene was represented by an associated phylogenetic profile of length 1. using these profiles as input features  we investigated the task of predicting a  qualitative   mlr  representation of an expression profile: actually  the profile of a gene is a sequence of real-valued measurements  each of which represents the expression level of that gene at a particular time point. converting the expression levels into ranks  i.e.  ordering the time points  = labels  according to the associated expression values  and using the spearman correlation as a similarity measure between profiles was motivated in  balasubramaniyan et al.  1 .1 here  we further extend this representation by replacing rankings with multilabel rankings. to this end  we use the zero expression level as a natural split point. thus  the sets px and nx correspond  respectively  to the time points where gene x is over- and underexpressed and  hence  have an important biological meaning.
모we used data from eight microarray experiments  giving rise to eight prediction problems all using the same input features but different target rankings. it is worth mentioning that these experiments involve different numbers of measurements  ranging from 1 to 1. since in our context  each measurement corresponds to a label  we obtain ranking problems of quite different complexity. besides  even though the original measurements are real-valued  there are many expression profiles containing ties. each of the datasets was randomly split into a training and a test set comprising 1% and 1%  respectively  of the instances. in compliance with  balasub-

1
this data is publicly available at
http://www1.cs.columbia.edu/compbio/exp-phylo 1
모모this transformation can be motivated from a biological as well as data analysis point of view.
ramaniyan et al.  1   we measured accuracy in terms of the  generalized  spearman rank correlation coefficient  normalized such that it evaluates to  1 for reversed and to +1 for identical rankings  see section 1 .
모support vector machines have demonstrated state-of-theart performance in a variety of classification tasks  and therefore have been used as the underlying binary classifiers for the binary relevance  br  and calibrated ranking by pairwise comparison  crpc  approaches to multilabel learning in previous studies  elisseeff and weston  1; brinker et al.  1 . regarding the associated kernel  we considered both linear kernels  lin  with the margin-error penalty c 뫍 {1 ...1} and polynomial kernels  poly  where the degree varied from 1 to 1 and c 뫍 {1 ...1}. for each parameter  combination  the validation accuracy was estimated by training on a randomly selected subsample comprising 1% of the training set and testing on the remaining 1%. then  the final model was trained on the whole training set using the parameter combination which achieved the best validation accuracy. similarly  the number of nearest neighbors k 뫍 {1 .. 1 .. 1} was determined.
모in addition to the original k-nn mlr approach  we included a version  denoted by the suffix  -r   which only exploits the mlc training data   px nx   and a common extension in k-nn learning leading to a slightly modified aggregation step where average ranks are weighted by the distances of the respective feature vectors to the query vector  referred to as k-nn  .
모the experimental results in table 1 clearly demonstrate that our k-nn approach is competitive with state-of-the-art model-based methods. more precisely  k-nn  and crpcpoly achieve the highest level of accuracy  followed by k-nn with only a small margin. br is outperformed by the other methods  an observation which is not surprising as br only uses the relevance partition of labels for training and cannot exploit the additional rankings of labels. similarly  the mlc versions of k-nn perform worse than their mlr counterparts. moreover  crpc with polynomial kernels performs slightly better than with linear kernels  whereas for br a substantial difference cannot be observed. the influence of gap amplification is demonstrated in figure 1 on an mlc task replicated from  elisseeff and weston  1   where genes from the same yeast dataset discussed above have to be associated with functional categories. moreover  as already anticipated on behalf of our theoretical analysis in section 1  table 1 impressively underpins the computational efficiency of our approach from an empirical perspective.
1 concluding remarks
we presented a general framework for multilabel ranking using a case-based methodology which is conceptually simpler and computationally less complex than previous model-based approaches to multilabel ranking. from an empirical perspective  this approach is highly competitive with state-of-the-art
methods in terms of accuracy  while being substantially faster.
모conceptually  the modular aggregation step provides a means to extend this approach in several directions. for example  ha and haddawy  proposed an appealing
k-nncrpcbrdatasettesttraintesttraintestalpha11111elu11111cdc11111spo11111heat11111dtt11111cold11111diau11111datasetlabelsk-nnk-nn-rk-nn k-nn -rcrpc-polycrpc-linbr-polybr-linalpha1.1.1.1.1.1.1.1.1elu1.1.1.1.1.1.1.1.1cdc1.1.1.1.1.1.1.1.1spo1.1.1.1.1.1.1.1.1heat1.1.1.1.1.1.1.1.1dtt1.1.1.1.1.1.1.1.1cold1.1.1.1.1.1.1.1.1diau1.1.1.1.1.1.1.1.1table 1: experimental results on the yeast dataset using the spearman rank correlation as the evaluation measure.table 1: computational complexity  in seconds  for training and testing on a pentium 1 with 1ghz  where k = 1 for the k-nn approach . the yeast training and test set consist of 1 and 1 instances  respectively.
probabilistic loss on preferences which originates from the kendall tau loss and extends to both partial and uncertain preferences. efficient methods for  approximate  rank aggregation with respect to this measure have not been developed yet but could potentially be plugged into our case-based framework in order to generalize to the uncertainty case. moreover  chin et al.  studied a weighted variant of the kendall tau loss function and proposed an approximate aggregation algorithm which requires polynomial time.
acknowledgments
this research was supported by the german research foundation  dfg  and siemens corporate research  princeton .
references
 balasubramaniyan et al.  1  r. balasubramaniyan  e. hullermeier  n. weskamp  and j몮	org k몮 amper.몮 clustering of gene expression data using a local shape-based similarity measure. bioinformatics  1 :1  1.
 bartholdi et al.  1  j. j. bartholdi  c. a. tovey  and m. a. trick. voting schemes for which it can be difficult to tell who won the election. social choice and welfare  1 :1  1.
 brinker et al.  1  klaus brinker  johannes furnkranz 몮 and eyke hullermeier.몮 a unified model for multilabel classification and ranking. in proceedings of the 1th european conference on artificial intelligence   1.
 brinker and hullermeier  1몮   klaus brinker and eyke hullermeier. case-based label ranking. in몮	proceedings of ecml 1  pages 1  1.
 chin et al.  1  francis y. l. chin  xiaotie deng  qizhi fang  and shanfeng zhu. approximate and dynamic rank aggregation. theor. comput. sci.  1 :1  1.
 dasarathy  1  b.v. dasarathy. nearest neighbor  nn  norms: nn pattern classification techniques  1.
 dwork et al.  1  cynthia dwork  ravi kumar  moni naor  and d. sivakumar. rank aggregation revisited. in world wide web  pages 1  1.
 elisseeff and weston  1  andre elisseeff and jason we-뫣 ston. a kernel method for multi-labelled classification. in advances in nips 1  pages 1  1.
 fagin et al.  1  ronald fagin  ravi kumar  mohammad mahdian  d. sivakumar  and erik vee. comparing and aggregating rankings with ties. in proc. 1rd acm symposium on pods  pages 1  1.
 furnkranz and h몮 ullermeier  1몮   johannes furnkranz and몮 eyke hullermeier. pairwise preference learning and rank-몮 ing. in proceedings of ecml 1  pages 1  1.
 ha and haddawy  1  vu ha and peter haddawy. similarity of personal p