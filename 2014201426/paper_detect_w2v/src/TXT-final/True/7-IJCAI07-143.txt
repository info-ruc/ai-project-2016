
searching the feature space for a subset yielding optimum performance tends to be expensive  especially in applications where the cardinality of the feature space is high  e.g.  text categorization . this is particularly true for massive datasets and learning algorithms with worse than linear scaling factors. linear support vector machines  svms  are among the top performers in the text classification domain and often work best with very rich feature representations. even they however benefit from reducing the number of features  sometimes to a large extent. in this work we propose alternatives to exact re-induction of svm models during the search for the optimum feature subset. the approximations offer substantial benefits in terms of computational efficiency. we are able to demonstrate that no significant compromises in terms of model quality are made and  moreover  in some cases gains in accuracy can be achieved.
1 introduction
linear support vector machines  svms   vapnik  1  have been found to be among the best performers in tasks involving text categorization  joachims  1  lewis et al.  1 . due to the richness of natural language  text categorization problems are characterized by very large numbers of features. even with infrequent features removed  the dimensionality of the attribute space tends to be very high  e.g.  「1   which for some learners poses computational challenges and/or leads to overfitting the training data. in the case of linear svms good performance is in many cases is achieved with little or no feature selection  joachims  1  lewis et al.  1  mladenic et al.  1   although best performance using all features is by no means guaranteed. it has been shown  gabrilovich and markovitch  1  that it is relatively easy to identify text classification problems where best accuracy is achieved with aggressive feature selection.
　even if optimum performance is achieved with no selection  the dependence between the classification accuracy and the number of features used often exhibits saturation whereby the improvement in accuracy due to increasing the number of features beyond a certain count is very small. it is therefore important to be able to estimate at which point the performance curve of the classifier measured against the number of most informative features either achieves a maximum or  levels off . unfortunately  the search for optimum feature settings can be time-consuming due to repetitive model retraining.
　in this work we investigate alternatives to svm model reinduction during feature selection. we are able to demonstrate that the proposed techniques not only result in substantial gains in terms computational efficiency but may actually lead to more accurate solutions and typically lead to equivalent results. as for the feature selection criterion we focus on the feature ranking induced by the svm itself  since it was found that in the text categorization domain it compares favorably  mladenic et al.  1  with the mainstream feature selection criteria such as information gain  rogati and yang  1 .
1 model-driven feature selection for svms
a linear svm creates a classification model by attempting to separate elements of two different classes by a maximum margin  vapnik  1 . for problems that are linearly separable this results in identifying the subsets of both positive and negative examples that lie exactly at the margin - these are called support vectors  svs . it is quite common  however  that the problem is not linearly separable  in which case the support vector set is enriched by those training examples that cannot be classified by the model correctly  the soft-margin svm  cortes and vapnik  1  balances the margin with the total loss over the training data . in either case  the weight vector of the svm solution  derived via convex optimization  is given by a linear combination of the svs  and the output of a the svm to an input vector x is expressed as

where dji is the value corresponding to feature i for the training example j  wi is the weight assigned to the i-th feature by the svm and αj is the lagrange multiplier associated with dj  the value of αj is 1 unless dj is a support vector - otherwise αj   1; for soft-margin svms the lagrange multipliers must satisfy 1   αj ＋ c ; yj （ { 1 +1} is the class label associated with dji.
　svms have proven to be quite robust in dealing with large numbers of features and overfitting tends to be less of an issue compared to other learners applied in the text classification domain. when feature selection needs to be applied svms have been reported to perform well with established feature selection approaches such as ig  χ1 and bns  forman  1 .
　in recent work  mladenic et al.  1  investigated the efficacy of using the feature ranking imposed by a trained svm itself. it was found that ranking according to absolute feature weight values outperforms other established alternatives. its use is justified  mladenic et al.  1  by the fact that the sensitivity of the margin separating the two classes  as determined by a linear svm  to changes in the j-th feature is directly dependent on the sensitivity of the norm of the weight vector to such changes. this can be expressed as  mladenic et al.  1 
therefore  features receiving low absolute weights can be expected to have little effect on the orientation of the optimum hyperplane.
　 mladenic et al.  1  proposed to apply this criterion in the filter fashion  john et al.  1   whereby the ranking is obtained just once and subsequent attempts to identify the optimum subset use the top-n features according to the initial ordering.  guyon et al.  1  followed an alternative wrapper approach  john et al.  1  where  after each subset of the least relevant features is removed  the svm model trained with the remaining ones provides the new ranking  which is then used to identify the next subset of features to remove. as discussed in  hardin et al.  1  an svm may assign low weights to features that are redundant given the presence of other relevant features. the recursive approach of  guyon et al.  1  is thus likely to be more successful in compensating for this effect  especially if it is desired to identify a fairly small set of the most relevant features.
　in the text domain  where the optimum number of features tends to be large  the differences in quality between models utilizing the recursive and non-recursive approaches are rather small  li and yang  1  kalousis et al.  1 . in this work we focus exclusively on the filter variant of svm based feature selection proposed in  mladenic et al.  1 .
1 avoiding model re-induction in svm-based feature selection
traditionally  investigation of the efficacy of different feature subsets relies on inducing a model with each particular choice of the subset and comparing the results achieved. depending on the type of the learner  repetitive model re-induction may carry a more or less significant cost in terms of the computational time required. in the case of linear svms the cost is quadratic in terms of the number of training instances and linear in terms of the number of features used. for massive datasets and rich feature representations  both of which are common in text categorization applications  identifying the optimum can be thus quite expensive.
　published results on svm-based feature selection indicate that svm is good in feature ranking in text applications. one interesting question however is:  how stable is the original solution subject to subsequent feature selection  . put differently  we are interested if the original solution can be significantly reoptimized in the restricted domain of using the top-n features.
　for linear svms  the quality of the solution is primarily determined by the orientation of the hyperplane normal. given the direction of this vector  the final position of the hyperplane  controlled by the bias term b in  1   can be adjusted so as to satisfy a particular utility function  e.g.  misclassification cost  precision or recall .
　for any feature subset one can obtain a projection of the original hyperplane normal onto the reduced feature space  which is obtained by ignoring or  masking  the coordinates corresponding to the removed features. our contribution is the address the following questions:
  is the direction of the projected hyperplane normal substantially different from the direction of the normal induced over the reduced feature representation 
  how does the classification performance of the solution associated with the projected hyperplane normal compare with the solution induced over the reduced feature representation 
  how is the membership of the supportvector set affected by reducing the dimensionality of the training data.
1 normal vector feature masking
let us consider a simple procedure which uses the original svm model to derive one that operates within the reduced space of top-n features. in this approximation the original model is unchanged  except that only those weight vector components which intersect with the top-n feature subset are retained. a corresponding feature masking transformation is applied to the documents. the output of the masked model can be expressed as:

where

and m is the set of features to be masked.
　to gain some insight into the properties of masking let us start with the original model and assume that the feature to be masked has the index of n. given that the original solution is optimal  for a linearly separable problem   it minimizes the
lagrangian

	subject to: for all j	 1 
and thus satisfies the first-order local minimum conditions:
		 1 
let us now mask out the n-th feature of vector w and each of training vector dj  while keeping the lagrange multipliers unchanged  and consider the same optimization problem but in the n   1 dimensional space. notice that the derivatives of the new lagrangian in  1  with respect to w and b remain 1 and thus the original solution when projected on the lowerdimensional space meets the necessary optimality conditions  with respect to w and b  there as well. to be a true solution in the space in which feature n is ignored it also needs to maximize l with respect to α and meet to constraints  1  however. the constraints in  1  were met originally with equality for the sv set and with strong inequality by all other training vectors. for the sake of an argument let us assume that  1  will hold for points outside the sv set. for the true svs the constraints will be violated only for those in which feature n was actually present  given the sparsity of text this may be only a small fraction of the sv set . the amount of the violation for each such sv will be

where it can be seen that a small value of |wn| leads to a small departure from the optimum  w n represents the vector with the n-th feature masked out .
　based on the above we can expect that by keeping the values of lagrange multipliers fixed and masking a low weight feature we can achieve a solution that lies close to the optimum in the reduced-dimensionality space. the validity of such an assumption will increase for features that are infrequent  i.e.  inequality constraints for fewer training points will be affected  and for features assigned low absolute weights  i.e.  the departure from optimality will likely be small .
1 feature masking and document normalization
in the text domain one often transforms the document feature vectors to unit norm  e.g.  according to l1  to compensate for the document length variability  joachims  1  dumais et al.  1  leopold and kindermann  1 . such transformation introduces feature weighting that is uniform for features within a document but varied across documents. the normal-based feature masking preserves the original feature weighting as the less relevant features are removed  which counters to some extent the original length normalization. an alternative that we consider here is to retain the set of svs and the associated lagrange multipliers but renormalize the training vectors after each set of features is removed. the output of such a model is thus given by  assuming l1 length normal-
ization 

note that such renormalization would typically be applied by default when re-inducing the svm model using top-n features.
1 experimental setup
we used linear svm induced with all features as the baseline and as the source of feature ranking. given the original feature set  the methodology was to examine the quality of models using only top n features. to asses the extent and importance of these differences we compared the effectiveness of exact model re-induction and the proposed alternatives over the following document collections:
trec-ap1: this dataset represents a collection of ap newswire articles for which the training/test split described in  lewis et al.  1  was used. the collection consisted of 1 training and 1 test documents divided into 1 categories. each document belonged to one category only.
reuters-rcv1: this collection represents of the most recent and largest corpus used in research involving text categorization. there are 1 training documents and 1 test documents as described in  lewis et al.  1  there are several ways of grouping the documents and we restricted ourselves to the topic ontology using the1 topic categories represented in the training portion of the data  out of the 1 categories total . a document may belong to more than one topic and in certain cases there is a hierarchical relationship where a topic may be considered a subtopic of another.
techtc-1: 1 two-class problems  based on the open directory project1  generated to purposefullyrepresent different challenges for svms as far as feature selection is concerned  gabrilovich and markovitch  1 .
1 document representation
documents were preprocessed by removing markup and punctuation  converting all characters to lower case and extracting feature tokens as sequences of alphanumerics delimited by whitespace. in the case of the rcv1 and
techtc-1 collections  we used the pre-tokenized feature files provided on the corpus websites. in-document term frequency was ignored and each feature received the weight of one if it appeared in a document at least once - otherwise its weight was zero. this binary representation in our experience performs as well as tf-idf when coupled with documentlength normalization. more importantly  in the context of this work  with the binary representation the magnitude of perturbations  1  to the conditions  1  depended primarily on the svm weights and not on independently derived factors  e.g.  tf-idf. l1 length normalization was applied since it has been found to be beneficial in text classification  joachims  1  dumais et al.  1  leopold and kindermann  1 . words which occurred just once in the training corpus were eliminated.
1 experimental procedure
the multi-class categorization problems were broken into a series of one-against-the-rest tasks  where each class was treated in turn as the target with the remaining ones playing the role of the anti-target. the techtc-1 dataset naturally consisted of 1 two-class tasks. unlike trec-ap and rcv1 however the two-class problems here were fairly well balanced  with comparable numbers of training and test examples available for the target and the anti-target.
　classification performance was measured in terms of the area under the receiver operating characteristic  roc  curve  auc  and the best attainable value of f1  as tuned via decision-threshold adjustment. both metrics were estimated over the test data and are reported in terms of the macroaverage  for best f1  and micro-average  for auc  across the categories.
　for each two-class task an svm model was induced using all features  which were then ranked according to their absolute weight values. features not represented in the sv set were removed and we then considered using just a fraction of the top ranking features  with respect to the non-zero weight ones  with values in {1 1 1 1 ... 1 1}.
　in addition to computing the average performance figures for each fraction we also provided the average of the best results on a per category basis  which acknowledgesthat the optimum number of features according to a given performance metric may change from one two-class problem to another.
　in labeling the results the exact reinduction approach is denoted as exact  normal-based feature masking is denoted as mask and the weight re-computation using the masked and renormalized sv set is labeled as sv set.
1 results
1 accuracy effects
table 1 one compares the averageauc and best f1 results for the trec-ap and reuters-rcv1 collections  where the best feature selection results were determined on a per category basis. it can be seen that the best results according these two metrics are numerically very close to each other within each dataset when using the three model feature selection approaches. according to the p-values the differences between the exact and approximate approaches can also be considered statistically insignificant  we used the macro t-test outlined in  yang and liu  1    except for the best-f1 measure and the normal-based masking method for rcv1. over the two collections  best auc performance was achieved with all or almost all features  but the best f1 performance was more varied as illustrated in figure 1 for the case of trec-ap  where it is apparent that reducing the number of features can have a beneficial effect. this exemplifies the fact that optimality of feature selection is dependent on the performance criterion. note that when a high fraction of top ranking features is retained    1%  there is essentially no difference whether an exact or an approximate feature selection method

figure 1: average best-f1 as a function of top-n features for trec-ap.

figure 1: average-best auc across the 1 two-class problems for techtc-1. according to the one-sided paired ttest the difference between the exact and masking approaches is significant  p-value   1   while the difference between the exact and sv set approaches is not  p-value= 1 .
is chosen. for lower feature counts the differences become more pronounced.
　since the techtc-1 collection consisted of problems with balanced numbers of positive and negative examples  in reporting classification accuracy we limited ourselves to the average auc metric. following  gabrilovich and markovitch  1   1-fold cross-validation was used to estimate accuracy and one-sided paired t-test was applied to estimate significance of the differences in classification performance.
　the average best auc results for using all features and the three feature selection methods are shown in figure 1. note that all approachesto optimize the feature set are substantially better than using all features  with the differences being statistically significant  p-values 「 1 . the p-values show that the sv set approach was statistically equivalent to the exact one  although normal-based masking underperformed in this case. given that techtc-1 was specifically designed to illustrate the benefits of feature selection for svm it is not surprising to see the big gains in auc shown in figure 1.
correlation effects
table 1: average best auc and f1 results for the exact and approximate methods for feature selection over the trec-ap and reuters-rcv1 collections. p-values for one-sided pairwise t-test  macro  are also given for determining at which point the differences between the two approximate variants and the exact approach can be considered significant.
best aucbest f1exactmask/p-valsv set/p-valexactmask/p-valsv set/p-valtrec-ap11/11/111/11/1rcv1.1.1.1.1.1.1.1.1.1.1aside from measuring the impact of feature selection on the classification performance it is interesting to investigate the figure 1: pearson correlation coefficient and cosine similarity between the masked original weight vector and the weight vector obtained after svm retraining using the top-n features for trec-ap  top  and rcv1  bottom .
similarity between the weight vectors assigned by the original svm and the svm obtained using a reduced feature representation.
　figure 1 shows the dependence of the weight-vector similarity according to the pearson correlation coefficient and the cosine similarity on the number of top-ranking features used for trec-ap. and rcv1. for both datasets the weight vectors are very close to each other  even when the fraction of top-n features used is small. the direction of the hyperplane normal vector is thus only very weakly dependent on the less relevant features and by projecting the original weight vector onto the sub-space of the top ranking features one obtains a direction that is oriented very close the optimum.
　figure 1 shows set the average overlap between the original set of svs and the one obtained when using top-n features for trec-ap and rcv1. since the overall number of figure 1: overlap between the original set of support vectors and the set obtained when training with reduced feature representation for trec-ap  top  and rcv1  bottom . the fraction  containment  of svs corresponding to the original sv set is also shown.
svs tends to decrease when fewer features are used  they also show the fraction of svs corresponding to the original sv set that are part of the sv set obtained using the reduced feature representation. as can be seen  sv set overlap is quite high  exceeds 1%  as long as a sizable fraction of the original features is used  at least 1% . for smaller values of n  the overlap goes down  but this is mainly due to the decrease in the number of svs in such cases  since the fraction of original svs used remains consistently very high. the overlap and containment of sv sets do not approach 1 as the fraction of features used approaches 1%. this is because the fraction is defined with respect to the set of features that received non-zero weights in the original svm run  i.e.  after discarding the zero-weight ones . it is thus apparent that the use of features that were deemed irrelevant did in fact have an impact on the original sv set selection.
　given that correlation between masked original weight vectors and weight vectors obtained during reinduction is very high despite somewhat lower levels of overlap between the sets of support vectors  it appears that svm training significantly alters the original lagrange multipliers to compensate for the effect of vector length normalization  such normalization will increase the relative contribution of the noneliminated features  and the use of fewer features. this seems to happen without substantially altering the orientation of the optimum hyperplane.
1 conclusions
in this work we stipulated that given the stability and robustness of svms it is unnecessary to re-induce the svm models when searching for optimum feature subset settings. we were able to demonstrate experimentally that feature masking produces results that are equivalent to the ones obtained using the traditional model reinduction while being extremely fast compared to svm retraining. our experiments showed that both orientation of the normal to the hyperplane and the support vector set remain quite stable as the set of active features is reduced by pruning the least relevant ones. this is particularly true for the orientation of the weight vector  which is the reason why feature masking is so effective and provides evidence that in text categorization the direction of the hyperplane normal will be mostly affected by the most relevant features  with the solution only marginally influenced by including the less relevant ones.
　the svm-based feature selection utilizing feature masking is practically very attractive and much faster  e.g.  runtime cost of table-lookup for normal-based masking  than the commonly-used approaches  in which although the original feature ranking is typically quite fast  e.g.  using information gain   the subsequent estimation of performance when using top-n features requires the more expensive model reinduction. by comparison  without model re-induction the search through the model space to identify the optimum feature count is orders of magnitude faster than the traditional approach and in fact this type of feature selection scales on par with some of the most scalable learners such as naive bayes.
