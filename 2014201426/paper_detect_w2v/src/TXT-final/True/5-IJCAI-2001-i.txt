
qualitative probabilistic networks represent probabilistic influences between variables. due to the level of representation detail provided  knowledge about influences that hold only in specific contexts cannot be expressed. the results computed from a qualitative network  as a consequence  can be quite weak and uninformative. we extend the basic formalism of qualitative probabilistic networks by providing for the inclusion of context-specific information about influences and show that exploiting this information upon inference has the ability to forestall unnecessarily weak results.
1	introduction
qualitative probabilistic networks are qualitative abstractions of probabilistic networks  wellman  1   introduced for probabilistic reasoning in a qualitative way. a qualitative probabilistic network encodes statistical variables and the probabilistic relationships between them in a directed acyclic graph. each node a in this digraph represents a variable. an arc a 뫸 b expresses a probabilistic influence of the variable a on the probability distribution of the variable b; the influence is summarised by a qualitative sign indicating the direction of shift in b's distribution. for probabilistic inference with a qualitative network  an efficient algorithm  based upon the idea of propagating and combining signs  is available  druzdzel & henrion  1 .
모qualitative probabilistic networks can play an important role in the construction of probabilistic networks for real-life application domains. while constructing the digraph of a probabilistic network is doable  the assessment of all probabilities required is a much harder task and is only performed when the network's digraph is considered robust. by eliciting signs from domain experts  the obtained qualitative probabilistic network can be used to study and validate the reasoning behaviour of the network prior to probability assessment; the signs can further be used as constraints on the probabilities to be assessed  druzdzel & van der gaag  1 . to be able to thus exploit a qualitative probabilistic network  it should capture as much qualitative information from the application domain as possible. in this paper  we propose an extension to the basic formalism of qualitative networks to enhance its expressive power for this purpose.
모probabilistic networks provide  by means of their digraph  for a qualitative representation of the conditional independences that are embedded in a joint probability distribution. the digraph in essence captures independencesbetween nodes  that is  it models independences that hold for all values of the associated variables. the independences that hold only for specific values are not represented in the digraph but are captured instead by the conditional probabilities associated with the nodes in the network. knowledge of these latter independences allows further decomposition of conditional probabilities and can be exploited to speed up inference. for this purpose  a notion of context-specific independence was introduced for probabilistic networks to explicitly capture independences that hold only for specific values of variables  boutilier et al.  1; zhang & poole  1 .
모a qualitative probabilistic network equally captures independences between variables by means of its digraph. since its qualitative influences pertain to variables as well  independences that hold only for specific values of the variables involved cannot be represented. in fact  qualitative influences implicitly hide such context-specific independences: if the influence of a variable a on a variable b is positive in one context  that is  for one combination of values for some other variables  and zero in all other contexts - indicating independence - then the influence is capturedby a positivesign. also  positive and negative influences may be hidden: if a variable a has a positive influenceon a variable b in some contextand a negative influence in another context  then the influence of a on b is modelled as being ambiguous.
모as context-specific independences basically are qualitative by nature  we feel that theycan and should be capturedexplicitly in a qualitative probabilistic network. for this purpose  we introduce a notion of context-specific sign. we extend the basic formalism of qualitative networks by providing for the inclusion of context-specific information about influences and show that exploiting this information upon inference can prevent unnecessarily weak results. the paper is organised as follows. in section 1  we provide some preliminaries con-
cerning qualitative probabilistic networks. we present two examples of the type of information that can be hidden in

qualitative influences  in section 1. we present our extended formalism and associated algorithm for exploiting contextspecific information in section 1. in section 1  we discuss the context-specific information that is hidden in the qualitative abstractions of two real-life probabilistic networks. in section 1  we briefly show that context-specific information can also be incorporated in qualitative probabilistic networks that include a qualitative notion of strength of influences. the paper ends with some concluding observations in section 1.
1	qualitative probabilistic networks
a qualitative probabilistic network models statistical variables as nodes in its digraph; from now on  we use the terms variable and node interchangeably. we assume  without loss of generality  that all variables are binary  using a and a몬 to indicate the values true and false for variable a  respectively. a qualitative network further associates with its digraph a set of qualitative influences  describing probabilistic relationships between the variables  wellman  1 . a qualitative influence associated with an arc a 뫸 b expresses how the values of node a influence the probabilities of the values of node b. a positive qualitative influence  for example  of a on b  denoted s+ a b   expresses that observing higher values for node a makes higher values for node b more likely  regardless of any other influences on b  that is 
pr b | ax  뫟 pr b | ax몬   
for any combination of values x for the set x of parents of b other than a. the '+' in s+ a b  is termed the influence's sign. a negative qualitative influence s   and a zero qualitative influence s1  are defined analogously. if the influence of node a on node b is non-monotonic or unknown  we say that it is ambiguous  denoted s  a b .
모the set of influences of a qualitative probabilistic network exhibits various properties  wellman  1 . the symmetry property states that  if s붻 a b   then also s붻 b a   붻 뫍 {+   1  }. the transitivity property asserts that a sequence of qualitative influences along a chain that specifies at most one incoming arc per node  combine into a single influence with the  -operator from table 1. the composition property asserts that multiple influences between two nodes along parallel chains combine into a single influence with the -operator.
 
table 1: the  - and -operators.
모a qualitativenetworkfurthercapturesqualitative synergies between three or more nodes; for details we referto  druzdzel & henrion  1; wellman  1 .
모for inference with a qualitative network  an efficient algorithm is available  druzdzel & henrion  1 . the basic idea of the algorithm is to trace the effect of observing a node's value on the other nodes in the network by message passing between neighbouring nodes. for each node  a node sign is determined  indicating the direction of change in the node's probability distribution occasioned by the new observation given all previously observed node values. initially  all node signs equal '1'. for the newly observed node  an appropriate sign is entered  that is  either a '+' for the observed value true or a ' ' for the value false. each node receiving a message updates its node sign and subsequently sends a message to each neighbour whose sign needs updating. the sign of this message is the  -product of the node's  new  sign and the sign of the influence it traverses. this process is repeated throughout the network  building on the properties of symmetry  transitivity  and composition of influences. since each node can change its sign at most twice  once from '1' to '+' or ' '  and then only to ' '  the process visits each node at most twice and is therefore guaranteed to halt.
1	context-independent signs
context-specific information cannot be represented explicitly in a qualitative probabilistic network  but is hidden in the network's qualitative influences. if  for example  the influence of a node a on a node b is positive for one combination of values for the set x of b's parents other than a  and zero for all other combinations of values for x  then the influence of a on b is positive by definition. the zero influences are hidden due to the fact that the inequality in the definition of qualitative influence is not strict. we present an example illustrating such hidden zeroes.

figure 1: the qualitative surgery network.
example 1 the qualitative network from figure 1 represents a highly simplified fragment of knowledge in oncology; it pertains to the effects and complications to be expected from treatment of oesophageal cancer. node l models the life expectancy of a patient after therapy; the value l indicates that the patient will survive for at least one year. node t models the therapy instilled; we consider surgery  modelled by t  and no treatment  modelled by t몬  as the only alternatives. the effect to be attained from surgery is a radical resection of the oesophageal tumour  modelled by node r. after surgery a life-threatening pulmonary complication  modelled by node p  may result; the occurrence of this complication is heavily influencedby whether or not the patient is a smoker  modelled by node s.
모we consider the conditional probabilities from a quantified network representing the same knowledge. we would like to note that these probabilities serve illustrative purposes only; although not entirely unrealistic  they have not been specified by domain experts. the probability of attaining a radical resection upon surgery is pr r | t  = 1; as without surgery there can be no radical resection  we have pr r | t몬  = 1.
from these probabilities we have that node t indeed exerts a positive qualitative influence on node r. the probabilities of a pulmonary complication occurring and of a patient's life expectancy after therapy are  respectively 

pr p ss몬t11t몬11
pr l pp몬r11r몬11from the left table  we verify that both t and s exert a positive qualitative influence on node p. the fact that the influence of t on p is actually zero in the context of the value s몬for node s  is not apparent from the influence's sign. note that this zero influence does not arise from the probabilities being zero  but rather from their having the same value. from the right table we verify that node r exerts a positive influence on node l; the qualitative influence of p on l is negative. 
the previous example shows that the level of representation detail of a qualitative network can result in information hiding. as a consequence  unnecessarily weak answers may result upon inference. for example  from the probabilities involved we know that performingsurgeryon a non-smokerhas a positive influence on life expectancy. due to the conflicting reasoning chains from t to l in the qualitative network  however  entering the observation t for node t will result in a ' ' for node l  indicating that the influence is unknown.
모we recall from the definition of qualitative influence that the sign of an influence of a node a on a node b is independent of the values for the set x of parents of b other than a. a ' ' for the influence of a on b may therefore hide the information that node a has a positive influence on node b for some combination of values of x and a negative influence for another combination. if so  the ambiguous influence is non-monotonic in nature and can in fact be looked upon as specifying different signs for different contexts. we present an example to illustrate this observation.

figure 1: the qualitative cervical metastases network.
example 1 the qualitative network from figure 1 represents another fragment of knowledge in oncology; it pertains to the metastasis of oesophageal cancer. node l represents the location of the primary tumour that is known to be present in a patient's oesophagus; the value l models that the tumour resides in the lower two-third of the oesophagus and the value 몬l expresses that the tumour is in the oesophagus' upper onethird. an oesophageal tumour upon growth typically gives rise to lymphatic metastases  the extent of which are captured by node m. the value m몬 of m indicates that just the local and regional lymph nodes are affected; m denotes that distant lymph nodes are affected. which lymph nodes are local or regional and which are distant depends on the location of the tumour in the oesophagus. the lymph nodes in the neck  or cervix  for example  are regional for a tumour in the upper one-third of the oesophagus and distant otherwise. node c represents the presence or absence of metastases in the cervical lymph nodes.
pr c llm11m몬11모we consider the conditional probabilities from a quantified network representing the same knowledge; once again  these probabilities serve illustrative purposes only. the probabilities of the presence of cervical metastases in a patient are 몬
from these probabilities we have that node l indeed has a negative influence on node c. the influence of node m on c  however  is non-monotonic:
pr c | ml    pr c | ml몬    yet pr c | m몬l    pr c | m몬몬l 
the non-monotonic influence hides a '+' for the value l of node l and a ' ' for the context 몬l. 
모from the two examples above  we observe that contextspecific information about influences that is present in the conditional probabilities of a quantified network cannot be represented explicitly in a qualitative probabilistic network: upon abstracting the quantified network to the qualitative network  the information is effectively hidden.
1	context-specificity and its exploitation
the level of representation detail of a qualitative probabilistic network enforces influences to be independent of specific contexts. in this section we present an extension to the basic formalism of qualitative networks that allows for associating context-specific signs with qualitative influences. in section 1  the extended formalism is introduced; in section 1  we show  by means of the example networks from the previous section  that exploiting context-specific information can prevent unnecessarily weak results upon inference.
1	context-specific signs
before introducing context-specific signs  we define a notion of context for qualitative networks. let x be a set of nodes  called the context nodes. a context cx for x is a combination of values for a subset y   x of the set of context nodes. when y =    we say that the context is empty  denoted ; when y = x  we say that the context is maximal. the set of all possible contexts for x is called the context set for x and is denoted cx. to compare different contexts for the same set of context nodes x  we use an ordering on contexts: for any two combinations of values cx and c1x for y   x and
y 1   x  respectively  we say that cand cx and c1x specify the same combination of values for y 1.
모a context-specific sign now basically is a sign that may vary from context to context. it is defined as a function 붻 : cx 뫸 {+   1  } from a context set cx to the set
of basic signs  such that for any two contexts cx and c1x with c we have that  if 
{+   1}  then 붻 cx  뫍 {붻i 1}. for abbreviation  we will write 붻 x  to denotethe context-specificsign 붻 that is defined on the context set cx. note that the basic signs from regular qualitative networks can be looked upon as context-specific signs that are defined by a constant function.
모in our extended formalism of qualitative networks  we assign context-specific signs to influences. we say that a node a exerts a qualitative influence of sign 붻 x  on a node b  denoted s붻 x  a b   where x is the set of parents of b other than a  iff for each context cx for x we have that
  붻 cx  = + iff pr b | acxy  뫟 pr b | a몬cxy  for any combination of values cxy for x;
  붻 cx  =   iff pr b | acxy  뫞 pr b | a몬cxy  for any such combination of values cxy;
  붻 cx  = 1 iff pr b | acxy  = pr b | a몬cxy  for any such combination of values cxy;
  붻 cx  =   otherwise.
note that we take the set of parents of node b other than a for the set of context nodes; the definition is readily extended to apply to arbitrary sets of context nodes  however. contextspecific qualitative synergies can be defined analogously.
모a context-specific sign 붻 x  in essence has to specify a basic sign from {+   1  } for each possible combination of values in the context set cx. from the definition of 붻 x   however  we have that it is not necessary to explicitly indicate a basic sign for every such context. for example  consider an influence of a node a on a node b with the set of context nodes x = {d e}. suppose that the sign 붻 x  of the influence is defined as
 
the function 붻 x  is uniquely described by the signs of the smaller contexts wheneverthe largercontextsare assigned the same sign. the function is therefore fully specified by

모the sign-propagation algorithm for probabilistic inference with a qualitative network  as discussed in section 1  is easily extended to handle context-specific signs. the extended algorithm propagates and combines basic signs only. before a sign is propagatedoveran influence  it is investigatedwhether or not the influence's sign is context-specific. if so  the currently valid context is determined from the available observations and the basic sign specified for this context is propagated; if none of the context nodes have been observed  then the sign specified for the empty context is propagated.
1	exploiting context-specific signs
in section 1 we presented two examples showing that the influences of a qualitative probabilistic network can hide context-specific information. revealing this hidden information and exploiting it upon inference can be worthwhile. the information that an influence is zero for a certain context can be used  for example  to improve the runtime of the sign-propagationalgorithm because propagationof a sign can be stopped as soon as a zero influence is encountered. more importantly  however  exploiting the information can prevent conflicting influences arising during inference. we illustrate this observation by means of an example.
example 1 we reconsider the qualitative surgery network from figure 1. suppose that a non-smoker is undergoing surgery. in the context of the observation s몬 for node s  propagating the observation t for node t with the basic signpropagationalgorithm results in the sign ' ' for node l: there is not enough information present in the network to compute a non-ambiguous sign from the two conflicting reasoning chains from t to l.
모we now extend the qualitative surgery network by assigning the context-specific sign 붻 s   defined by

to the influence of node t on node p  that is  we explicitly include the information that non-smokingpatients are not at risk for pulmonary complications after surgery. the thus extended network is shown in figure 1 a . we now reconsider our non-smoking patient undergoing surgery. propagating the observation t for node t with the extended signpropagation algorithm in the context of s몬 results in the sign ' +   +    1     ' = '+' for node l: we find that surgery
is likely to increase life expectancy for the patient. 

	 a 	 b 
figure 1: a hiddenzero revealed  a   anda non-monotonicity captured   b   by a context-specific sign.
모in section 1 we not only discussed hidden zero influences  but also argued that positive and negative influences can be hidden in non-monotonic influences. as the initial ' 's of these influences tend to spread to major parts of a network upon inference  it is worthwhile to resolve the non-monotonicities involved whenever possible. our extended formalism of qualitative networks provides for effectively capturing information about non-monotonicities  as is demonstrated by the following example.
example 1 we reconsider the qualitative cervical metastases network from figure 1. we recall that the influence of node m on node c is non-monotonic since
pr c | ml    pr c | ml몬   and pr c | m몬l    pr c | m몬몬l 
in the context l  therefore  the influence is positive  while it is negative in the context 몬l. in the extended network  shown in figure 1 b   this information is captured explicitly by assigning the sign 붻 l   defined by
  =  
to the influence of node m on node c. 
1	context-specificity in real-life networks
to get an impression of the context-specific information that is hidden in real-life qualitative probabilistic networks  we
# influences with sign 붻:
	+	 	1 totalalarm	1	11oesophagus	1	11table 1: the numbers of influences with '+'  ' '  '1' and ' ' signs for the qualitative alarm and oesophagus networks.
computedqualitative abstractions of the well-known alarmnetwork and of the network for oesophageal cancer. the alarm-network consists of 1  mostly non-binary  nodes and 1 arcs; the number of direct qualitative influences in the abstracted network - using the basic definition of qualitative influence - therefore equals 1. the oesophagus network consists of 1  also mostly non-binary  nodes and 1 arcs. table 1 summarises for the two abstracted networks the numbers of direct influences with the four different basic signs. the numbers reported in table 1 pertain to the basic signs of the qualitative influences associated with the arcs in the networks' digraphs. each such influence  and hence each associated basic sign  covers a number of maximal contexts. for a qualitative influence associated with the arc a 뫸 b  the number of maximal contexts equals 1  the empty context  if node b has no other parents than a; otherwise  the number of maximal contexts equals the number of possible combinations of values for the set of parents of b other than a. for every maximal context  we computedthe proper  contextspecific  sign from the original quantified network. table 1 summarises the number of context-specific signs covered by the different basic signs in the two abstracted networks. from the table we have  for example  that the 1 qualitative influences with sign '+' from the alarm network together cover 1 different maximal contexts. for 1 of these contexts  the influences are indeed positive  but for 1 of them the influences are actually zero.

# cx with sign 붻1:alarm+ 1 total+1-1-1붻:  1-
-1 -1 --
-1 111total111# cx with sign 붻1:oesophagus+ 1 total+1-1-1붻:  1-
-1 -1
--
-1 111total111
table 1: the numbers of contexts cx coveredby the '+'  ' '  '1' and ' ' signs and their associated context-specific signs  for the qualitative alarm and oesophagus networks.
모for the qualitative alarm-network  we find that 1% of the influences are positive  1% are negative  and 1% are ambiguous; the network does not include any explicitly specified zero influences. for the extended network  using contextspecific signs  we find that 1% of the qualitative influences are positive  1% are negative  1% are zero  and 1% remain ambiguous. for the qualitative oesophagus network  we find that 1% of the influences are positive  1% are negative  and 1% are ambiguous; the network does not include any explicit zero influences. for the extended network  using context-specific signs  we find that 1% of the qualitative influences are positive  1% are negative  1% are zero  and 1% remain ambiguous.
모we observe that for both the alarm and the oesophagus network  the use of context-specific signs serves to reveal a considerable number of zero influences and to substantially decrease the number of ambiguous influences. similar observations were made for qualitative abstractions of two other real-life probabilistic networks  pertaining to wilson's disease and to ventricular septal defect  respectively. we conclude that by providing for the inclusion of context-specific information about influences  we have effectively extended the expressive power of qualitative probabilistic networks.
1	extension to enhanced networks
the formalism of enhanced qualitative probabilistic networks  renooij & van der gaag  1   introduces a qualitative notion of strength of influences into qualitative networks. we briefly argue that the notions from the previous sections can also be used to provide for the inclusion and exploitation of context-specific information about such strengths.
모in an enhanced qualitative network  a distinction is made between strong and weak influences by partitioning the set of all influences into two disjoint subsets in such a way that any influence from the one subset is stronger than any influence from the other subset; to this end a cut-off value 붸 is used. for example  a strongly positive qualitative influence of a node a on a node b  denoted s++ a b   expresses that
pr b | ax    pr b | ax몬   뫟 붸
for any combination of values x for the set x of parents of b other than a; a weakly positive qualitative influence of a on b  denoted s+ a b   expresses that
1 뫞 pr b | ax    pr b | ax몬   뫞 붸
for any such combination of values x. the sign '+ ' is used to indicate a positive influence whose relative strength is ambiguous. strongly negative qualitative influences s    and weakly negative qualitative influences s   are defined analogously; a negative influence whose relative strength is ambiguous is denoted s  . zero qualitative influences and ambiguous qualitative influences are defined as in regular qualitative probabilistic networks. renooij & van der gaag  1  also provide extended definitions for the - and  -operators to apply to the double signs. these definitions cannot be reviewed without detailing the enhanced formalism  which is beyond the scope of the present paper; it suffices to say that the result of combining signs is basically as one would intuitively expect.
모our notion of context-specific sign can be easily incorporated into enhanced qualitative probabilistic networks. a context-specific sign now is defined as a function 붻 : cx 뫸 {++ +  +         1  } from a context set cx to the extended set of basic signs  such that for any two contexts cx and c1x with cx   c1x we have that  if the sign is strongly positive for c1x  then it must be strongly positive for cx  if the sign is weakly positive for c1x  then it must be either weakly positive or zero for cx  and if it is ambiguously positive for c1x  then it may be  strongly  weakly or ambiguously  positive  or zero for cx. similar restrictions hold for negative signs. context-specific signs are once again assigned to influences  as before.
모for distinguishing between strong and weak qualitative influences in an enhanced network  a cut-off value 붸 has to be chosen in such a way that  basically  for all strong influences of a node a on a node b we have that |pr b | ax    pr b | ax몬  | 뫟 붸 for all contexts x  and for all weak influences we have that |pr b | ax    pr b | ax몬  | 뫞 붸 for all such contexts. if  for a specific cut-off value 붸  there exists an influence of node a on node b for which there are contexts x and x1 with |pr b | ax    pr b | ax몬  |   붸 and |pr b | ax1    pr b | ax몬 1 |   붸  then signs of ambiguous strength would be introduced into the enhanced network  which would seriously hamper the usefulness of exploiting a notion of strength. a different cut-offvalue had better be chosen  by shifting 붸 towards 1 or 1. unfortunately  붸 may then very well end up being 1 or 1. the use of context-specific information about qualitative strengths can now forestall the necessity of shifting the cut-off value  as is illustrated in the following example.

figure 1: context-specific sign in an enhanced network.
example 1 we reconsider the surgery network and its associated probabilities from example 1. upon abstracting the network to an enhanced qualitative network  we distinguish between strong and weak influences by choosing a cut-off value of  for example  붸 = 1. we then have that a pulmonarycomplicationafter surgerystronglyinfluences life expectancy  that is  s   p l . for this cut-off value  however  the influence of node t on node p is neither strongly positive nor weakly positive; the value 붸 = 1 therefore does not serve to partition the set of influences in two distinct subsets. to ensure that all influences in the network are either strong or weak  the cut-off value should be either 1 or 1.
모for the influence of node t on node p  we observe that  for 붸 = 1  the influence is strongly positive for the value s of node s and zero for the context s몬. by assigning the contextspecific sign 붻 s  defined by
 
to the influence of node t on node p  we explicitly specify the otherwise hidden strong and zero influences. the thus extended network is shown in figure 1. we recall from example 1 that for non-smokers the effect of surgery on life expectancy is positive. for smokers  however  the effect could not be unambiguously determined. from the extended network in figure 1  we now find the effect of surgery on life expectancy for smokers to be negative: upon propagating the observation t for node t in the context of the information s for node s  the sign ' + +  ++    ' = ' ' results for node l. 
1	conclusions
we extended the formalism of qualitative probabilistic networks with a notion of context-specificity. by doing so  we enhanced the expressive power of qualitative networks. while in a regular qualitative network  zero influences as well as positive and negative influences can be hidden  in a network extended with context-specific signs this information is made explicit. qualitative abstractions of some real-life probabilistic networks have shownthat networks indeedcan incorporate considerable context-specific information. we further showed that incorporating the context-specific signs into enhanced qualitative probabilistic networks that include a qualitative notion of strength renders even more expressive power. the fact that zeroes and doublesigns can be specified contextspecifically allows them to be specified more often  in general. we showed that exploiting context-specific information about influences and about qualitative strengths can prevent unnecessary ambiguous node signs arising during inference  thereby effectively forestalling unnecessarily weak results.
references
 boutilier et al.  1  c. boutilier  n. friedman  m. goldszmidt  and d. koller. context-specific independence in bayesian networks. proceedings of the twelfth conference on uncertainty in artificial intelligence  1  pp. 1 - 1.
 druzdzel & henrion  1  m.j. druzdzel and m. henrion. efficient reasoning in qualitative probabilistic networks. proceedings of the eleventh national conference on artificial intelligence  1  pp. 1 - 1.
 druzdzel & van der gaag  1  m.j. druzdzel and l.c. van der gaag. elicitation of probabilities for belief networks: combining qualitative and quantitative information. proceedings of the eleventh conference on uncertainty in artificial intelligence  1  pp. 1 - 1.
 renooij & van der gaag  1  s. renooij and l.c. van der gaag. enhancing qpns for trade-off resolution. proceedings of the fifteenth conference on uncertainty in artificial intelligence  1  pp. 1 - 1.
 wellman  1  m.p. wellman. fundamental concepts of qualitative probabilistic networks. artificial intelligence  vol. 1  1  pp. 1 - 1.
 zhang & poole  1  n.l. zhang and d. poole. on the role of context-specific independence in probabilistic inference. proceedings of the sixteenth international joint conference on artificial intelligence  1  pp. 1 - 1.
max-norm projectionsfor factored mdps
   carlos guestrin computer science dept.
모stanford university guestrin cs.stanford.edu모모daphne koller computer science dept.
 stanford university koller cs.stanford.eduronald parr computer science dept.
duke university parr cs.duke.eduabstract
markov decision processes  mdps  provide a coherent mathematical framework for planning under uncertainty. however  exact mdp solution algorithms require the manipulation of a value function  which specifies a value for each state in the system. most real-world mdps are too large for such a representation to be feasible  preventing the use of exact mdp algorithms. various approximate solution algorithms have been proposed  many of which use a linear combination of basis functions as a compact approximation to the value function. almost all of these algorithms use an approximation based on the  weighted  -norm  euclidean distance ; this approach prevents the application of standard convergence results for mdp algorithms  all of which are based on max-norm. this paper makes two contributions. first  it presents the first approximate mdp solution algorithms - both value and policy iteration - that use maxnorm projection  thereby directly optimizing the quantity required to obtain the best error bounds. second  it shows how these algorithms can be applied efficiently in the context of factoredmdps  where the transition model is specified using a dynamic bayesian network.
1	introduction
over the last few years  markov decision processes  mdps  have been used as the basic semantics for optimal planning for decision theoretic agents in stochastic environments. in the mdp framework  the system is modeled via a set of states which evolve stochastically. the key problem with this representation is that  in virtually any real-life domain  the state space is quite large. however  many large mdps have significant internal structure  and can be modeled compactly if the structure is exploited in the representation.
모factored mdps  boutilier et al.  1  are one approach to representing large  structured mdps compactly. in this framework  a state is implicitly described by an assignment to some set of state variables. a dynamic bayesian network  dbn   dean and kanazawa  1  can then allow a compact representation of the transition model  by exploiting the fact that the transition of a variable often depends only on a small number of other variables. furthermore  the momentary rewards can often also be decomposed as a sum of rewards related to individual variables or small clusters of variables.
모even when a large mdp can be represented compactly  e.g.  in a factored way  solving it exactly is still intractable: exact mdp solutionalgorithms require the manipulation of a value function  whose representation is linear in the number of states  which is exponential in the number of state variables. one approach is to approximate the solution using an approximate value function with a compact representation. a common choice is the use of linear value functions as an approximation - value functions that are a linear combination of basis functions.
모this paper makes a twofoldcontribution. first  we provide a new approach for approximately solving mdps using a linear value function. previous approaches to linear functionapproximationtypically have utilized a least squares   -norm  approximation to the value function. least squares approximations are incompatible withmost convergence analyses for mdps  which are based on max-norm. we provide the first mdp solution algorithms - both value iteration and policy iteration - that use a linear max-norm projection to approximate the value function thereby directlyoptimizingthe quantity required to obtain the best error bounds.
모second  we show how to exploit the structure of the problem in order to apply this technique to factored mdps. our work builds on the ideas of koller and parr  1; 1   by using factored  linear  value functions  where each basis function is restricted to some small subset of the domain variables. we show that  for a factored mdp and factored value functions  various key operations can be implemented in closed form without enumerating the entire state space. thus  our max-norm algorithms can be implemented efficiently  even though the size of the state space grows exponentially in the number of variables.
1	markov decision processes
a markov decision process  mdp  is defined as a 1-tuple where: is a finite set of states; is a set of actions; is a reward function ir  such that represents the reward obtained by the agent in state
after taking action ; and is a markovian transition model where represents the probability of going from state to state with action .
모we will be assuming that the mdp has an infinite horizon and that future rewards are discounted exponentially with a discount factor . a stationary policy for an mdp is a mapping   where is the action the agent takes at state . the policy is associated with a value function ir   where is the discounted cumulative value that the agent gets if it starts at state . the value function for a fixed policy is the fixed point of a set of equations that define the value of a state in terms of the value of its possible successor states. more formally  we define: definition 1 the dp operator    for a fixed stationary policy is:
	is the fixed point of	:	.	
모the optimal value function is also defined by a set of equations. in this case  the value of a state must be the maximal value achievable by any action at that state. more precisely  we define:
definition 1 the bellman operator 	  is:
	is the fixed point of	:	.	
모for any value function   we can define the policy obtained by acting greedily relative to . in other words  at each state  we take the action that maximizes the one-step utility  assumingthat represents our long-termutilityachieved at the next state. more precisely  we define greedy
the greedy policy relative to the optimal value function
is the optimal policy greedy . there are several algorithms to compute the optimal policy  we will focus on the two most used: value iteration and policy iteration.
모value iteration relies on the fact that the bellman operator is a contraction - it is guaranteed to reduce the max-norm     distance between any pair of value functions by a factor of at least . this property guarantees that the bellman operator has a unique fixed point  puterman  1 . value iteration exploits this property  approaching the fixed point through successive applications of the bellman operator: . after a finite number of iterations  the greedy policy greedy will be the optimal policy.
모policy iteration iterates over policies. each iteration consists of two phases. value determination computes  for a policy   the value function   by finding the fixed point of: . policy improvement defines the next policy as greedy . it can be shown that this process converges to the optimal policy.
1	solving mdps with max-norm projections
in many domains  the state space is very large  and we need to perform ourcomputationsusingapproximate value functions. a very popular choice is to approximate a value function using linear regression. here  we define our space of allowable value functions ir via a set of basis functions
               . a linear value functionover is a function that can be written as for some coefficients . we define to be the linear subspace of ir spanned by the basis functions . it is useful to define an matrix whose columns are the basis functions  viewed as vectors. our approximate value
function is then represented by	.
linear value functions: the idea of using linear value functions for dynamic programming was proposed  initially  by bellman et al.  and has been further explored recently  tsitsiklis and van roy  1; koller and parr  1; 1 . the basic idea is as follows: in the solution algorithms  whether value or policy iteration  we use only value functions within . whenever the algorithm takes a step that results in a value function that is outside this space  we project the result back into the space by finding the value functionwithinthe space which is close to . more precisely:
definition 1 a projection operator	is a mapping
ir . is said to be a projection w.r.t. a norm if: such that	. 
모unfortunately  these existing algorithms all suffer from a problem that we might call  norm incompatibility.  when computing the projection  they all utilize the standard projectionoperator with respect to norm or a weighted norm. onthe otherhand  most ofthe convergence and erroranalyses for mdp algorithms utilize max-norm. this incompatibility has made it difficult to provide error guarantees.
모in this section  we propose a new approach that addresses the issue of norm compatibility. our key idea is the use of a projection operator in norm. this problem has been studiedinthe optimizationliteratureas the problem offinding the chebyshev solutionto an overdetermined linear system of equations  cheney  1 . the problem is defined as finding such that:
 1 
we will use an algorithm due to stiefel   that solves this problem by linear programming:
variables:
minimize:
	subject to:	and	 1 
for the solution of this linear program  is the solution of eq.  1  and is the projection error. note that this lp only has variables. however  there are constraints  which makes it impractical for large state spaces. in the remainder of this section  we will discuss how this projection addresses the norm incompatibility problem.
in section 1  we will show that  in factored mdps  all the constraints can be represented efficiently  leading to a
tractable algorithm.
approximate value iteration: the basic idea of approximate value iterationis quite simple. we define an projection operator that takes a value function and finds that minimizes . this is an instance of eq.  1  and can be solved by eq.  1 . the algorithm alternates applications of the bellman operator and projection steps :

and
in standard value iteration  we only need to perform the

first step. however  may not be in   so we need to add the second step  the projection step  to the process. we can analyze this process  bounding the overall error between our approximate value function and the optimal value function . this analysis shows that the overall error depends on the single-step max-norm projection errors

. thus  using the max-norm
projection  we can minimize these projection errors directly  we omit the analysis for lack of space. note that  in approximate value iteration  the error introduced by successive approximations may grow unboundedly. as we will show  this cannot happen in approximate policy iteration.
approximate policy iteration: as we discussed  policy iteration is composed of two steps: value determination and policy improvement. our algorithm performs the policy improvement step exactly. in the value determination step  the value function is approximated through a linear combination of basis functions. consider the value determination for a
policy	. define	  and
. we can now rewrite the value
determination step in terms of matrices and vectors. if we
view and as -vectors  and as an matrix  we have the equations: . this is a system of linear equations with one equation for each state  which can only be solved exactly for small . our goal is to provide an approximate solution  within . more precisely  we want to find:
this minimization is another instance of an	projection
 eq.  1    where and   and can be solved using the linear program of eq.  1 . thus  our approximate policy iteration alternates between two steps:
greedy
모for the analysis  we define the projection error for the policy iteration case  i.e.  the error resulting from the approximate value determination step:
	.	unlike in approxi-
mate value iteration  the error is bounded in this case:
lemma 1 there exists a constant such that for all iterations of the algorithm. 
finally we define discountedaccumulatedprojectionerror

as	. lemma 1 implies that

the accumulated error remains bounded:	.
모we can now bound the error in the value function resulting from approximate policy iteration:
theorem 1 in the approximate policy iteration algorithm  the distance between our approximate value function at iteration and the optimal value function is bounded by:


모in words  the difference between our approximation at iteration and the optimal value function is bounded by the sum of two terms. the first term is present in standard policy iteration and goes to zero exponentially fast. the second is the discounted accumulated projection error and  as the theorem shows  is bounded. this second term can be minimized by choosing as the one that minimizes . therefore  by per-
forming max-norm projections  we can make the bound on the theorem as tight as possible.
1	solving factored mdps
1	factored mdps
our presentation of factored mdps follows that of  koller and parr  1 . in a factored mdp  the set of states is described via a set of random variables   where each	takes on values in some finite domain
모모모모. a state defines a value for each variable . we define a state transition model using a dynamic bayesian network  dbn   dean and kanazawa  1 . let denote the variable at the current time and the variable at the next step. the transition graph of a dbn is a two-layer directed acyclic graph whose nodes are . we denote the parents of in the graph by parents . for simplicity of exposition  we assume that parents ; i.e.  all arcs in the dbn are between variables in consecutive time slices.  this assumption can be relaxed  but our algorithmbecomes somewhat more complex.  each node is associated with a conditional probability distribution  cpd 
parents . the transition probability is then defined to be   where is the value in of the variables in parents .
모consider  for example  the problem of optimizing the behavior of a system administrator maintaining a network of computers. each machine is connected to some subset of other machines. in one simple network  we might connect the machines in a ring  with machine connected to machines and .  in this example  we assume addition and subtraction are performed modulo  . each machine is associated witha binary random variable   representingwhether it has failed. the parents of are     . the cpd of is such that if true  then true with high probability; i.e.  failures tend to persist. if false  then is a noisy or of its two other parents; i.e.  a failure in either
of its neighbors can independently cause machine	to fail.
모we can define the transitiondynamics of an mdp by defining a separate dbn model for each action
 . however  in many cases  different actions have very similar transition dynamics  only differing in their effect on some small set of variables. in particular  in many cases a variable has a default evolution model  which only changes if an action affects it directly  boutilier et al.  1 . therefore  as in  koller and parr  1   we use the notion of a default transition model . for each action   we define to be the variables in the next state whose local probabilitymodel is different from   i.e.  those variables such that parents parents .
모in our system administrator example  we have an action for rebooting each one of the machines  and a default action for doing nothing. the transition model described above corresponds to the  do nothing  action  which is also the default transition model. the transitionmodel for is different from only in the transition model for the variable   which is now true with some small fixed probability  regardless of the status of the neighboring machines.
모finally  we need to provide a compact representation of the reward function. we assume that the reward function is factored additively into a set of localized reward functions  each of which only depends on a small set of variables.
definition 1 a function	is restricted to a domain
if ir. if is restricted to and   we will use as shorthand for where is the part of the instantiation that corresponds to variables in .
let be a set of functions  where each is restricted to variable cluster . the reward function for state is defined to be ir. in our example  we might have a reward function associated with each machine   which depends on .
모factorization may allow us to represent large mdps very compactly. however  we must still address the problem of solving these mdps. solution algorithms rely on the ability to represent value functions and policies  the representations which requires the same number of parameters as the size of the space. one might be tempted to believe that factored transition dynamics and rewards would result in a factored value function  which can thereby be represented compactly. unfortunately  even in factored mdps  the value function rarely has any internal structure  koller and parr  1 .
모kollerand parr suggest that there are many domains where our value function might be  close  to structured  i.e.  well-approximated using a linear combination of functions each of which refers only to a small number of variables. more precisely  they define a value function to be a factored  linear  value function if it is a linear function over the basis   where each is restricted to some subset of variables . in our example  we might have basis functions whose domains are pairs of neighboring machines  e.g.  one basis function which is an indicator function for each of the four combinations of values for the pair of failure variables.
모as shown by koller and parr  1; 1   factored value functions provide the key to performing efficient computations over the exponential-sized state sets that we have in factored mdps. the key insight is that restricted domain functions  includingour basis functions  allow certain basic operations to be implemented very efficiently. in the remainder of this section  we will show that this key insight also applies in the context of our algorithm.
1	factored max-norm projection
the key computational step in both of our algorithms is the solution of eq.  1  using the linear program in eq.  1 . in our setting  the vectors and are vectors in ir   where is our state space. in the case of factored mdps  our state space is a set of vectors which are assignments to the state variables . we can view both and as functions of these state variables  and hence also their difference. thus  we can define a function such that . note that we have executed a representation shift; we are viewing as a function of   which is parameterized by .
모the size of the state space is exponential in the number of variables. hence  our goal in this section is to optimize eq.  1  without explicitly considering each of the exponentially many states. the key is to use the fact that has a factored representation. more precisely  has the form
             where is a subset of . for example  we might have which takes value in states where true and false and otherwise. similarly  the vector in our case is also a sum of restricted domain functions. thus  we can express as a sum   where may or may not depend on . in the future  we sometimes drop the superscript when it is clear from context. we tackle the problem of a factored solution to the lp in eq.  1  in two steps.
maximizing over the state space: first  assume that and are given  and that our goal is simply to compute
  i.e.  to find the state
over which	is maximized. recall that	.
we can maximize such a function using non-serial dynamic programming  bertele and brioschi  1  or cost networks  dechter  1 . the idea is virtually identical to variable elimination in a bayesian network. we review this construction here  as it is a key component in our solution lp. our goal is to compute
where is the instantiation of the variables in in the assignment . the key idea is  rather than summing all functions and then doing the maximization  we maximize over variables one at a time. when maximizing over   only summands involving participate in the maximization. for example  assume
we therefore wish to compute:
we can first compute the maximum over	; the functions and	are irrelevant  so we can push them out. we get
the result of the internal maximization depends on the values of ; i.e.  we can introduce a new function whose value at the point is the value of the internal expression. our problem now reduces to computing
having one fewer variable. next  we eliminate another variable  say   with the resulting expression reducing to:
where
finally  we define
the result at this point is a number  which is the desired maximum over .
모in general  the variable elimination algorithm maintains a set of functions  which initially . the algorithm then repeats the following steps:
1. select an uneliminated variable	;
1. take all	whose domain contains	.
1. define a new function and introduce it into . the domain of is .
모the computational cost of this algorithm is linear in the number of new  function values  introduced in the elimination process. more precisely  consider the computation of a new function whose domain is . to compute this function  we need to compute different values. the cost of the algorithm is linear in the overall number of these values  introduced throughout the algorithm. as shown in  dechter  1   this cost is exponential in the induced width of the undirected graph defined over the variables   with an edge between and if they appear together in one of the original functions .
factored lp: now  consider our original problem of minimizing	over . as ineq.  1   we want toconstruct a linear programwhich performsthisoptimization. however  we want a compact lp  that avoids an explicit enumeration of the constraints for the exponentially many states. the first key insight is that we can replace the entire set of constraints - for all states - by the equivalent constraint   orequivalently  . the second key insight is that this new constraint can be implemented using a construction that follows the structure of variable eliminationin cost networks.  an identical construction applies to the complementary con-
straints:	. 
모consider any function used within  including the original 's   and let be its domain. for any assignment to   we introduce a variable into the linear program whose value represents . for the initial functions   we include the constraint that . as is linear in   this constraint is linear in the lp variables. now  consider a new function introducedinto byeliminatinga variable . let be the functionsextracted from   and let be the
domain of the resulting . we introduce a set of constraints:
where is the domain of and denotes the value of the instantiation restricted to . let be the last function generated in the elimination  and recall that its domain is empty. hence  we have only a single variable . we introduce the additional constraint .
모to understand this construction  consider our simple example above  and assume we want to express the fact that
모모모모모. we first introduce a set of variables for every instantiationof values to the variables
모모모. thus  if and are both binary  we would have four such variables. we would then introduce a constraint defining the value of appropriately. for example  for our above  we would have and . we would have similar variables and constraints for each and each value in . note that each of the constraints is a simple equality constraint involving numerical constants and perhaps the weight variables .
모next  we introduce variables for each of the intermediate expressions. for example  we would have a set of variables ; for each of them  we would have a set of constraints
one for each value of . we would have a similar set of constraint for in terms of and . note that each constraint is a simple linear inequality.
모it is easy to show that minimizing  drives down  the value of each variable   so that
we can then prove  by induction  that must be equal to . our constraints on ensure that it is greater than this value  which is the maximum of over the entire state space. the lp  subject to those constraints  will minimize   guaranteeing that we find the vector
that achieves the lowest value for this expression.
returningto our original formulation  we have that
is	in one set of constraints and	in the other.
hence our new set of constraints is equivalent to the original set: and . minimizing finds the that minimizes the norm  as required.
1	factored solution algorithms
the factored max-norm projection algorithm described previously is the key to applying our max-norm solution algorithms in the context of factored mdps.
approximate value iteration: let us begin by considering the value iteration algorithm. as described above  the algorithm repeatedly applies two steps. it first applies the bell-

man operator to obtain . let be the stationary policy greedy . note that corresponds to the maximizing policy used in the bellman operator at iteration   i.e.  for iteration . thus 

we can compute	by first computing
and then performing a backprojection operation for this fixed

policy  i.e. 	. assume  for the moment  that is a factored transition model; we discuss the computation of the greedy policy and the resulting transition model below. as discussed by koller and parr   the backprojection operation can be performed efficiently if the transition model and the value function are both factored

appropriately. furthermore  the resulting function is also factored  although the factors involve larger domains.
모to recap their construction briefly  let be a restricted domain function with domain ; our goal is to compute
   . we define the back-projection of	through	as the set of parents of	in the transition graph	; parents	. it is easy to show that:
	  where	is the value of	in	.
thus  we see that is a function whose domain is restricted to . note that the cost of the computation depends linearly on   which depends on  the domain of   and on the complexity of the process dynamics.

모therefore  is composed of the sum of two factored functions: the reward function which is assumed to be factored  and the backprojected basis functions which are also factored  as we have just shown. finally  the second step in approximate value iteration is to compute the projec-

tion	  i.e.  find	that minimizes

	.	as both	and	are
factored  we can perform this computation using the factored lp discussed in the previous section.
approximate policy iteration: policy iteration also iterates through two steps. the policy improvement step simply computes the greedy policy relative to . we discuss this step below. the approximate value determination step computes:
again  assuming that	is factored  we can conclude that
is also a matrix whose columns corre-
spond to restricted-domain functions. the target corresponds to the reward function  which is assumed to be factored. thus  we can again apply our factored lp.
모in our discussion so far  we assumed that we have some mechanism for computing the greedy policy greedy   and that this policy has a compact representation and a factored transition model. as shown in  koller and parr  1   the greedy policy relative to a factored value function has the form of a decision list. more precisely  the policycan be writ-
ten in the form	  where each
is an assignment of values to some small subset of variables  and each is an action. the optimal action to take in state is the action corresponding to the first event in the list with which is consistent.
모koller and parr show the greedy policy can be represented compactly using such a decision list  and provide an efficient algorithm for computing it. unfortunately  as they discuss  the resulting transition model is usually not factored. thus  we cannot simply apply our factored lp construction  as suggested above. however  we can adapt it todeal withthisissue.
모the basic idea is to introduce cost networks corresponding to each branch in the decision list. let be the set of states for which is the first event in the decision list for which is consistent. recall that our lp construction defines a set of constraints that imply that for each state . instead  we will have a separate set of constraints for the states in each subset . for each state in   we know that action is taken. hence  we can apply our construction above using - a transition model which is factored by assumption - in place of the non-factored .
모the only issue is to guarantee that the cost network constraints derived from this transition model are applied only to states in . specifically  we must guarantee that they are applied onlyto states consistent with   but not to states that are consistent with some for . to guarantee the first condition  we simply instantiate the variables in to take the values specified in . that is  our cost network now considers only the variables in   and computes the maximum only over the states consistent with . to guarantee the second condition  we ensure that we do not impose any constraints on states associated with previous decisions. this is achieved by adding indicators for each previous decision   with weight . these will cause the constraints associated with to be trivially satisfied by states in for . note that each of these indicators is a restricted domain function of and can be handled in the same fashion as all other terms in the factored lp. thus  for a decision list of size   our factored lp contains constraints from cost networks.
모it is instructive to compare our max-norm policy iteration algorithm with -projection policy iteration algorithm of koller and parr  in terms of computational costs per iteration and implementation complexity. computing the projection requires  among other things  a series of dot product operations between basis functions and backprojected basis functions . these expressions are easy to compute if refers to the transition model of a particular

figure 1: network topologies tested.
action . however  if the policy is represented as a decision list  as is the result of the policy improvement step  then this step becomes much more complicated. in particular  for every point in the decision list  for every pair of basis functions and   and for each assignment to the variables in
  it requires the solutionof a counting
problem which is -complete in general. although  koller and parr show that this computation can be performed using a bayesian network  bn  inference  the algorithm still requires a bn inference for each one of those assignments at each point of the decision list. this makes the algorithm very difficult to implement efficiently in practice.
모the max-norm projection  on the other hand  relies on solving a linear program at every iteration. the size of the linear program depends on the cost networks generated. as we discuss  two cost networks are needed for each point in the decision list. the complexity of each of these cost networks is approximately the same as one of the bn inferences in the counting problem for the projection. overall  for each point in the decision list  we have a total of two of these  inferences   as opposed to one for each assignment of for every pair of basis functions and . thus  the max-norm policy iteration algorithm is substantiallyless complex computationallythan the approach based on -projection. furthermore  the use of linear programming allows us to rely on existing lp packages  such as cplex   which are very highly optimized.
1	experimental results
the factored representation of a value functionis most appropriate in certain types of systems: systems that involve many variables  but where the strong interactions between the variables are fairly sparse  so that the decoupling of the influence between variables does not induce an unacceptable loss in accuracy. as argued by herbert simon  in  architecture of complexity   many complex systems have a  nearly decomposable  hierarchic structure   with the subsystems of such systems interacting only weakly between them. we selected to try our algorithm on a problem that we believe characterizes this type of structure.

	 a 	 b 	 c 
figure 1:  a  fitting a polynomial to the running time of the unidirectional ring;  b  relative error to optimal value function and comparison to projection;  c  for large models  measuring bellman error after convergence.모the problem relates to a system administrator who has to maintain a network of computers; we experimented with various network architectures  shown in fig. 1. machines fail randomly  and a faulty machine increases the probability that its neighboring machines will fail. at every time step  the sysadmin can go to one machine and reboot it  causing it to be working in the next time step with high probability. each machine receives a reward of 1 when working  except in the ring  where one machine receives a reward of 1  to introduce some asymmetry   a zero reward is given to faulty machines  and the discount factor is . note that the additive structure of the reward function makes it unsuitable for the tree-structured representation used by boutilier et al. .
모the basis functions included independent indicators for each machine  with value 1 if it is working and zero otherwise  each  a restricted domain function of a single variable   and the constant basis  whose value is 1 for all states.
모we implemented the factored policy and value iteration algorithms in matlab  using cplex as the lp solver. in our experiments  most of the time was spent in matlab generating the lp constraints; cplex is a remarkably efficient and reliable solver  allowingeven large lps tobe solved very quickly. we present only results for policy iteration. the time per iteration was about equal for policy and value iteration  but policy iterationconverged in many fewer iterations  only about 1 or 1 iterations in the models we tested.
모to evaluate the complexity of the algorithm  tests were performed with increasing number of states  that is  increasing number of machines on the network. fig. 1 shows the running time for increasing problem sizes  for various architectures. the simplest one is the  star   where the backprojection of each basis functionhas domain restricted to twovariables and the largest factor in the cost network has domain restricted to two variables. the most difficult one was the  bidirectional ring   where factors contain five variables.
모note  the number of states is growing exponentially  indicated by the log scale in fig. 1   but running times increase only logarithmically in the number of states  i.e.  polynomially inthe number of variables. this is illustratedbyfig. 1 a   where we fit a 1rd order polynomial to the running times for the  unidirectional ring . note  the problem size grows quadratically with the number of variables: adding a machine to the network also adds the possible action of fixing that machine. for this problem  the computation cost of our algorithm empirically grows approximately as . for further evaluation  we measured the error in our approximate value function relative to the true optimal value function . note that it is only possible to compute for small problems; in our case  we were only able to go up to 1 machines. for comparison  we also evaluated the error in the approximate value function produced by the -projection algorithm of koller and parr. as we discussed above  the projections in factored mdps by koller and parr  are difficult and time consuming; hence  we were only able to compare the two algorithms for smaller problems  where an equivalent -projection can be implemented using an explicit state space formulation. results for both algorithms are presented in fig. 1 b   showing the relative error of the approximate solutions to the true value function for increasing problem sizes. the results indicate that  for larger problems  the max-norm formulation generates a better approximation of the true optimal value function than the -projection. here  we used two types of basis functions: the same single variable basis  and pairwise basis  which also includes indicators for neighboringpairs of machines. as expected  pairwise basis generated better approximations.
모for these small problems  we can also compare the actual value of the policy generated by our algorithm to the value of the optimal policy. here  the value of the policy generated by our algorithm is much closer to the value of the optimal policy than the error implied by the difference between our approximate value functionand . for example  for the  star  architecture with one server and up to 1 clients  our approximation with single variable basis functions had relative error of   but the policy we generated had the same value as the optimal policy. in this case  the same was true for the policy generated by the projection. in an  unidirectional ring  with1 machines and pairwise basis  the relative error between our approximation and was about   but the resulting policy only had a loss over the optimal policy. for the same problem  the approximation has a value function error of   and a true policy loss was . in other words  both methods induce policies that have lower errors than the errors in the approximate value function  at least for small problems . however  our algorithm continues to outperform the algorithm  even relative to actual policy loss.
모for the large models  we can no longer compute the correct value function  so we cannot evaluate our results by computing . however  the bellman error  defined as
  can be used to provide a bound:	  williams and
baird  1 . thus  we use the bellman error to evaluate our answers for larger models. fig. 1 c  shows that the bellman error increases very slowly with the number of states.1
모finally  we can look at the actual policies generated in our experiments. first  we noted that these tended to be short  the lengthof the final decision list policy grew approximately linearly with the number of machines. furthermore  the policy itself is often fairly intuitive. in the  ring and star  architecture  for example  the decision list says: if the server is faulty  fix the server; else  if another machine is faulty  fix it.
1	conclusions
in this paper  we presented new algorithms for approximate value and policy iteration. unlike previous approaches  our algorithms directly minimize the error  and therefore have better theoretical performance guarantees than algorithms that optimize the -norm.
모we have shown that the error can be minimized using linear programming  and provided an approach for representing this lp compactly for factored mdps  allowing these algorithms to be applied efficiently even for mdps with exponentially large state spaces. our algorithms are more efficient and substantially easier to implement than previous algorithms based on the -projection.
모we have presented results on a structured problem  representing a simplified version of a maintenance task. our results show that our methods scale effectively to very large factored mdps  and that our approach can exploit problemspecific structure to efficiently generate approximate solutions to complex problems.
모the success of our algorithmdepends on our abilityto capture the most important structure in the value function using a linear factored approximation. this ability  in turn  depends on the choice of the basis functions and on the properties of the domain. the algorithm currently requires the designer to specify the factored basis functions. this is a limitation compared to other algorithms  e.g.   dearden and boutilier  1    whichare fullyautomated. however  ourexperiments indicate that a few simple rules are often quite successful in designing a basis. first  we ensure that the reward function is representable by our basis. a simple basis that  in addition  contains a separate set of indicators for each variable is often successful. we can also add indicators over pairs of variables; most simply  we can choose these according to the dbn transition model  where an indicator is added between variables and each one of the variables in parents   thus representing one-step influences. this procedure can be extended  adding more basis to represent more influences as required. thus  the structure of the dbn gives us indications of how to choose the basis functions. other sources of prior knowledge can also be included for further specifying the basis.
모the quality of our approximation also depends stronglyon the structure of the domain. as we discussed above  our approximationis most successful in systems where variables are tightlycoupled to only a small number of other variables. we believe that  for systems of this type  an approximation as a factored linear value function can be very accurate.
모there are many possible extensions to this work. in particular  we hope to extend our algorithms to utilize other types of structure in the representation. one interesting direction involves factored action models  where multiple actions are taken simultaneously. another involves the use of asymmetries  context-specificity  in the value function  as in the work of dearden and boutilier   providing a complementary source of structure to the factorization used in our work.
acknowledgments: we are very grateful to dirk ormoneit and uri lerner for many useful discussions. this work was supported by the onr under the muri program  decision making under uncertainty  and by the sloan foundation.
references
 bellman et al.  1  r. bellman  r. kalaba  and b. kotkin. polynomial approximation - a new computational technique in dynamic programming. math. comp.  1 :1  1.
 bertele and brioschi  1  u. bertele and f. brioschi. nonserial dynamic programming. academic press  new york  1.
 boutilier et al.  1  c. boutilier  t. dean  and s. hanks. decision theoretic planning: structural assumptions and computational leverage. journal of artificial intelligence research 1.
 cheney  1  e. w. cheney. approximation theory. chelsea publishing co.  new york  ny  1nd edition  1.
 dean and kanazawa  1  t. dean and k. kanazawa. a model for reasoning about persistence and causation. computational intelligence  1 :1  1.
 dearden and boutilier  1  r. dearden and c. boutilier. abstraction and approximate decision theoretic planning. artificial intelligence  1 :1  1.
 dechter  1  r. dechter. bucket elimination: a unifying framework for reasoning. artif. intel.  1-1 :1  1.
 koller and parr  1  d. koller and r. parr. computing factored value functions for policies in structured mdps. in ijcai  1.
 koller and parr  1  d. koller and r. parr. policy iteration for factored mdps. in proc. of uncertainty in ai  uai-1   1.
 puterman  1  m. puterman. markov decision processes: discrete stochastic dynamic programming. wiley  new york  1.
 simon  1  herbert a. simon. the sciences of the artificial. mit press  cambridge  massachusetts  second edition  1.
 stiefel  1  e. stiefel. note on jordan elimination  linear programming and tchebycheff approximation. numerische mathematik  1 - 1  1.
 tsitsiklis and van roy  1  j. n. tsitsiklis and b. van roy.
feature-based methods for large scale dynamic programming. machine learning  1-1  1.
 williams and baird  1  r. williams and l. baird. tight performance bounds on greedy policies based on imperfect value functions. tech. report  northeastern univ.  massachusetts  1.

uncertainty and probabilistic reasoning
factored markov decision processes

solving factored mdps via non-homogeneous partitioning
kee-eung kim and thomas dean
department of computer science
brown university
providence  ri 1 kek tld  cs.brown.edu

abstract
this paper describes an algorithm for solving large state-space mdps  represented as factored mdps  using search by successive refinement in the space of non-homogeneous partitions. homogeneity is defined in terms of bisimulation and reward equivalence within blocks of a partition. since homogeneous partitions that define equivalent reduced state-space mdps can have a large number of blocks  we relax the requirement of homogeneity. the algorithm constructs approximate aggregate mdps from non-homogeneous partitions  solves the aggregate mdps exactly  and then uses the resulting value functions as part of a heuristic in refining the current best non-homogeneous partition. we outline the theory motivating the use of this heuristic and present empirical results and comparisons.
1	introduction
markov decision processes  mdps  employing representations that factor states  actions and their associated transition and reward functions in terms of component functions of state variables  fluents  have surfaced as plausible models for planning under uncertainty  boutilier et al.  1 . since the number of states in these factored mdps  fmdps  is exponential in the number of fluents  traditional iterative methods that require enumerating states are typically not effective. in this paper  we solve fmdps by constructing and refining non-homogeneous partitions of the state space. in a homogeneous partition  any two states in a block have the same reward and the same distribution with respect to transitions to other blocks. non-homogeneous partitions allow variation among the states in a block with regard to rewards and block transition distributions. from a non-homogeneous partition  we construct an aggregate mdp by averaging the transition probabilities and the rewards within blocks.
모the algorithm described in this paper solves fmdps by successive refinement in the space of non-homogeneous partitions. we use factored representations to encode the partitions and the aggregate mdps we construct from them. we can solve the aggregatemdps in time polynomialin the number of blocks in the partition. the resulting optimal policies  optimal for the aggregate mdps  also serve as policies for the original mdp and the value function for the optimal policy in the aggregate mdp serves as an estimate for the value of the policy in the original mdp. given this estimate and the current partition  we choose the refinement that yields the greatest improvement and iterate. in the remainder of this paper  we present some background  provide a theoretical motivation for our refinement heuristic  and describe the results of a series of experiments.
1	factored mdps  fmdps 
definition 1  fmdp  an fmdp is defined as a tuple where is a vector of fluents which collectively define the state. we use to denote the set of values for . is the sample space of when is considered as a random variable. thus  the state space for is . we use lower case letters to denote a particular instantiation of the fluents  and and denote  respectively  a fluent and its value at a particular time . is the set of actions.
모is the set of transition probabilities  represented as the set of conditional probability distributions  one for each action and fluent:
pa
	where pa	denotes the set of parents of
in the graphical model  see below . note that pa	.
모모모모모is the reward function. without loss of generality  we define the reward to be determined by both state and action    .
모figure 1 shows an example of a graphical model for the dynamics governing an action in a toy robot domain with five boolean variables. the robot has to deliver a cup of coffee whenever requested. unfortunately  the coffee bar is outside the building and the robot receives a small amount of punishment if it gets wet. each fluent denotes a particular aspect of
	rt 	p u ut 	p w t
1 1.1 hct  p wc wct  1
	1	1
모모모모모모모모모모모모모1	1	1 t	t + 1
figure 1: graphical representation of an fmdp describing a toy robot domain.
the world: the weather outside being rainy      the robot having an umbrella      the robot being wet      the robot holding coffee      and someone wanting coffee    . note that the probabilisticeffect of each fluent at time is conditioned on a small number of fluents  the set of parent fluents  e.g.  pa   at time . the conditional probabilities are stored as trees rather than tables for further savings in the size of the representation. thus  we complete the description of the state dynamics by specifying conditional probabilitytrees  cpts   boutilier et al.  1 for each fluent and action. similar representations have been used to model markovian processes with factored state spaces  e.g.  the two-stage temporal bayesian network  1tbn   dean and kanazawa  1  and the dynamic bayesian network  dbn   forbes et al.  1 . the reward function is also designated in terms of trees.
모fmdps exploit conditional independence among the fluents given their parents in the graphical model to achieve economy of representation. however  the underlying structure in the domain that makes compact representations possible does not lead necessarily to an efficient fmdp algorithm. there are algorithms modeled after classical mdp algorithms that use dynamic programming to iteratively update a value function represented in a factored form; however  the size of the value functions so represented can easily explode. there are also algorithms that compress the value functions in an effort to prevent an explosion  e.g.  using a tree representation for value functions with pruning techniques  boutilier and dearden  1   or using algebraic decision diagrams  adds  for further reduction  hoey et al.  1 . these algorithms are examples of value function approximation  vfa  algorithms applied to fmdps  gordon  1; tsitsiklis and van roy  1; koller and parr  1 .
1	stochastic bisimulation equivalence
dean and givan  introduce the notion of stochastic bisimulation homogeneity for fmdps. it is an extension of the state equivalence relation for minimizing finite state automata  fsa  to that of probabilistic fsas. dean and givan's model minimization algorithm for fmdps partitions the state space into stable blocks.
definition 1  stable block and homogeneous partition 
a block	of a partition	is said to be stable with respect to a block of and an action if and only if every state in has the same transition probability of ending up in block by action . mathematically 
               such that where
we say that is stable if is stable with respect to every block of and action in . is said to be homogeneous if and only if every block is stable.
모given a homogeneouspartition of an fmdp  we can define an aggregate mdp that is equivalent to the original fmdp. every state within a block of a homogeneous partition has the same state dynamics with respect to any policy  i.e.  the transition probability of moving into a block is the same for every state in the same block. thus  if every state within a block of a homogeneous partition has the same reward  the partition yields a reduced model of the original fmdp.
모givan et al.  introduce -homogeneity for finding an approximately homogeneous partition with few blocks. we say that a block is -stable with respect to and if
such that
if every block of partition is -stable with respect to every other block of and every action  we say is an homogeneous partition. qualitatively speaking  by relaxing the equality to be approximately equal with error within   we get a smaller partition than if we required strict homogeneity. the error in the optimal value function computed from an homogeneous partition is discussed in the work of singh and yee  and white and eldeib . most of the previous work on computing approximatesolutions of fmdps follow similar analyses  boutilier and dearden  1; st-aubin et al.  1 .
1	non-homogeneous partitions for fmdps
there are two important questions remaining to be answered concerning partition refinement techniques for fmdps. first  what class of functions should we allow to represent block formulae  if we restrict the representational power of the formulae too much  we may end up with large partitions but with easily manipulable block formulae. if we allow arbitrary functions  manipulating block formulae is np-hard  see  goldsmith and sloan  1  for recent analysis.  though we can compute the coarsest homogeneous partition which may be small. second  what if the coarsest homogeneous partition is still exponentially large compared to the description size of the fmdp  calculating -homogeneous partition helps  but note that we don't have a fine control over the size of the partition - all we know is that increasing will generally reduce the size of the partition and decreasing it will generally do the opposite.
모based on the above observation  we describe a new state aggregation algorithm that does not search for the coarsest homogeneous partition. in fact  the technique is not driven

figure 1: chopping the block with respect to   or . note that the blocks resulting from the chop operator are not stable in general.
by the notion of stochastic bisimulation homogeneity. given an initial non-homogeneous partition of the state space  the algorithm iteratively decides which block to refine so that it produces at the end an approximately optimal policy without generating a huge partition. thus  given a partition  the algorithm has to determine which block to refine with respect to which fluent. note that the partitions we consider are not -homogeneous given that the transition probabilities can widely differ.
모we show that the optimal value functions calculated on non-homogeneouspartitions providethe basis for an effective heuristic for selecting the block-fluent pair to chop. first  we define how we construct the mdp from a non-homogeneous partition.
definition 1  mdp from non-homogeneous partition 
given an fmdp and a non-homogeneous partition of the state space   the aggregate mdp induced by   denoted as   is defined
as


for all and . denotes the optimal value function of   which is a mapping from to . note that
for any and in the same block of   and	.
모given a partition   the algorithm selects block and fluent for generating a refined partition . has the same blocks as except for which is replaced by chop . figure 1 illustrates how the chop operator generates refined blocks of . recalling that is the number of fluents in the domain  we note that there are at most different refined partitions that can be obtained by the chop operator. for each refined partition generated by the chop operator  the algorithm constructs and calculates the optimal value function .
모we will present the algorithm shortly. but first  we would like to know how good it is to use to solve the original fmdp. we prove that given any non-homogeneous partition

figure 1: value functions for an mdp constructed from a non-homogeneouspartition. the figure illustrates an example where the mdp has two states and the partition has one block. the dashed line is the set of representable approximate value functions.  and .
모  the optimal value function of   which we define as   is within a bounded distance from the true value function . to this end  we introduce a definition and a theorem from gordon  that are used in proving theorem 1.
definition 1  averager  an approximationfunction is an averager if  given the target vector   the approximation function generates defined as
where is a constant and and are non-negative constants such that	.
theorem 1  gordon  let be the optimal value function for an mdp   the update  bellman backup  operator for value iteration 
and the mapping for an averager. let be any fixed point of . given   the iteration of converges to a value function so that
   . approximate value iteration using returns which satisfies	.
theorem 1  bounded distance between	and	 
given an fmdp and a partition of the state space   the optimal value function of given as and the optimal value function of given as satisfy the bound on the distance
		 1 
where	 	being any value function such that for any block	and any	and	in	 
 .
proof recall from the definition that	 
and


we define another value function

to calculate the maximum distance between and   we define an averager as follows:
		for	.
note that each fixed point of assigns the same value to states in the same block. for the example shown in figure 1  the straight dotted line satisfying is the set of fixed points for since and belong to the same block. since is a refinement of the reward partition  is the fixed point of the mapping   and applying the result from gordon   we have that
		 1 
where is the distance between and the closest fixed point of . the distance between and is calculated as follows: for any  




since the above inequality holds for	  we have
		 1 
1. the number of iterations   initial partition of the state space  reward partition   and the optimal value function for .
1. for each block and fluent   compute where is the same as except is replaced by chop .
1. for each	  compute	and then select
.
1. if step 1 has been run	times  halt and output	.
1. set	and go to step 1.figure 1: the algorithm for finding a non-homogeneous partition for an approximately optimal policy
by combining equation 1 and equation 1  we have shown
that

there are two remarks on the bound in the above theorem.
first  that appears in the first additive term in equation 1 measures how fine is the partition . is the minimum error that we can achieve by assigning values to the components of   with the restriction that the values should be the same for the components that belong to the same block in . thus  we naturally expect that a finer partition will achieve a smaller   although it depends on how the state space is partitioned. at least  given a partition and its refinement   and letting and be the
of and respectively  we can say that . note also that becomes 1 for a homogeneous partition. second  in the second additive term also vanishes as the partition becomes finer. although there is no guarantee that the error is monotonically smaller for a finer partition and larger for a coarser partition  at least this error serves as an upper bound on the distance.
모the algorithm that we describe in the following selects the best refined partition from the current partition given by the formula
theorem 1 indicates that by choosing a refinement	of
maximizing the distance between and there is reason to believe that we are making significant progress in approaching . this is the same sort of argument used to justify the use of discretization strategies for continuous statespace mdps.
모the algorithm runs iteratively by refining the partition for a pre-determined number of iterations so that we end up with a fixed-size policy. figure 1 shows the algorithm. note that constructing can be done efficiently. the new transition probability matrix typically has many components
c = x1 뫇 x1
	x1	c' = x1 뫇 x1 뫇 x1
 p  	t  c a c'  = 1
1	1	x1	p'
	figure 1: calculating	with decision trees.
the same as . by reusing the components already calculated  we can construct the s without computing transition probabilities and rewards for all blocks. for example  if the partition was obtained by chopping a block
with respect to the fluent	so that	 
we have
the remaining components are computed without explicitly enumerating all of the states in the blocks. figure 1 shows how we calculate the component using decision trees when or . to compute the component  we first graft cpts and multiply the probabilities in the terminal nodes. since we are only interested in the transition probability related to blocks and   we can eliminate the branches of the constructed tree that do not intersect with block . this greatly reduces the size of the tree. the final step is to take the average of the items in the terminal nodes in the tree  weighted by the sizes of the blocks that the terminal nodes represent.
	the reward function	for	is obtained similarly.
1	experiments
the test problems used in our experiments are adopted from hoey et al.  and involve domains with 1 to 1 binary variables  fluents . the initial probabilities are given as a uniform distribution. for each domain  we show the performance of the policy derived from the non-homogeneous partition and the cumulative elapsed time at each iteration. we use cplex 1 for calculating optimal value functions of aggregate mdps induced by non-homogeneous partitions  and the cudd package  somenzi  1  to implement structured versions of iterative algorithms  similar to spudd  hoey et al.  1   for evaluating the policies and calculating the optimal value functions. all experimentsare run on a sun ultra1 with 1mb of memory.
모figure 1 through figure 1 illustrate the performance of the algorithm in figure 1 on three benchmark domains. we ran the algorithm for 1 iterations  except for the coffee domain since it consisted of only 1 states. for each figure  the graph on the left side shows the actual performance  actual value of the optimal policy calculated from the aggregated

figure 1: coffee domain  1 variables  1 states . the nonhomogeneouspartitioningalgorithm found the optimal policy after 1 chops  totaling 1 blocks in the partition. the optimal value is 1.

figure 1: expon domain  1 variables  1 states . after 1 chops  1 blocks   the non-homogeneous partitioning algorithm yields an approximately optimal policy. the optimal value is .

figure 1: factory domain  1 variables  1 states . after 1 chops  1 blocks   the non-homogeneous partitioning algorithm yields an approximately optimal policy. the optimal value is .
mdp  and the heuristic value  the optimal value of the aggregated mdp  after each iteration. the values are obtained assuming uniform starting probabilities on the states. the graphs on the right sides show the plots of cumulative elapsed time  in seconds  after each iteration. note that in some domains  there is a small gap between the actual performance and the heuristic value even when the policy from the aggregated mdp reached the optimal performance. this is due to the fact that the evaluation is donethrough an iterative method
with the stopping condition	  which we set to 1.
모the optimal value function of the expon domain  figure 1  has thousands of internal nodes even when represented as an add. the non-homogeneous partitioning algorithm is not able to find the optimal policy with partition size less than or equal to 1  however  the policy at the end of 1th itera-

figure 1: distance plots between two value functions of successive aggregate mdps   in figure 1 . from left to right: coffee  expon  factory.
tion performs 1 times better than the initial policy from the aggregate mdp induced by the reward partition. the size of the add representation for the optimal value function is also quite large in the case of the factory domain  figure 1 . after 1 chops  totaling 1 blocks  the policy from the aggregated mdp has a value of 1 which is 1% of the optimal value.
모we also experimented on the linear domain  1 variables  1 states . the optimal policy of the aggregate mdp from the reward partition yields the optimal value  i.e.  the optimal policy was obtained without any split. it takes 1 seconds for 1 iteration. when the algorithm finds out that every refinement of the current partition yields
                 then it knows the current optimal policy from the aggregated mdp is indeed optimal. meanwhile  our implementation of the structured value iteration using cudd package  which is not as fully optimized as spudd  takes 1 seconds  1 leaves .
모figure 1 shows the distances between two value functions of successive aggregate mdps for each domain. we excluded the linear domain since the distances were zero from the onset. note that in the coffee domain  the distance decreases sharply after the optimal policy from the aggregate mdp is actually optimal. however  the distances after that are not zero  which implies the partition is still nonhomogeneous. we do not observe such behavior in the expon and factory domain since the algorithm was not able to yield the optimal policy. although the distances between consecutive value functions are large  we note that the actual performance does not change much in the two domains. it is natural that a big difference in value functions does not necessarily imply a big difference in the actual performance. sometimes  a block containing states with highly varing transition probabilities is not so important to reach for optimal performance  hence chopping the block does not greatly improve the actual performance of the policy.
모we also compare the performance of non-homogeneous partitioningalgorithmto apricodd  st-aubinet al.  1 . figure 1 and figure 1 summarize the comparison of the two algorithms on the larger domains  expon and factory. by trial and error  we tuned the pruning parameter of the apricodd algorithm so that the size of the approximate value function from the apricodd algorithm is comparable to that of the non-homogeneous partition at the end of 1 chops. we used  sliding-tolerance  pruning technique with different parameters. we allow two to three times more nodes in the decision diagrams than the number of blocks in the non-homogeneous partition. often  increasing the pruning
perf.blockstimenon-homogeneous1 %11 sperf.nodesleavestimeapricodd  1 1 %11 sapricodd  1 1 %11 sfigure 1: comparison of the non-homogeneous partitioning algorithm and the apricodd on expon domain. the number inside the parenthesis is the pruning parameter for the  sliding-tolerance  pruning. the perf. is the ratio between the performance of the optimal policy and that of the approximate policy assuming uniform starting distribution on states. we also present the number of nodes and leaves in the add representation of approximate value function from the apricodd.
perf.blockstimenon-homogeneous1 %11 sperf.nodesleavestimeapricodd  1 1 %11 sapricodd  1 1 %11 sfigure 1: comparison of the non-homogeneous partitioning algorithm and the apricodd on factory domain.
parameter so that we get smaller value functions from apricodd results in non-converging behavior. in fact  apricodd on the expon domain with pruning parameter 1 does not converge. in this case  we stopped the algorithm when the value functions between consecutive iterations were oscillating  1 iterations . note that we are still allowing the apricodd to search in the space of a much richer representation for value functions - an add with 1 nodes can represent a much finer partition than a partition with 1 blocks. in terms of the number of blocks  the non-homogeneous partitioning algorithm performs comparable to the apricodd algorithm. note also that our implementation of apricodd is not fully optimized  and that the running times may be significantly greater than those reported by the original authors.
모we are currently exploring heuristics for efficiently selecting block-fluent pairs to chop. we note that solving continuous mdps faces a similar problem  and we are experimenting on discretization heuristics such as munos and moore  munos and moore  1 . a preliminary result shows that this heuristic is highly effective  yielding approximately 1-fold speed up in some domains. a preliminary report on this experiment appears in  kim  1 .
모the above experiments show two advantages of using the non-homogeneous partitioning algorithm. first  while in some domains the coarsest homogeneous partition may be quite large  it may not be critical to compute a homogeneous partition to obtain an optimal  or near optimal  policy. structured value iteration algorithms such as spudd or apricodd may incur a large cost in representing an optimal or near optimal value function  whereas the non-homogeneous partitioning algorithm does not necessarily face such a problem. second  the algorithm provides a new approach to finding an approximatelyoptimal policy  which allows us to specify the desired size of the policy a priori.
1	conclusion and related work
to achieve economy of representation many algorithms aggregate states that have the same  or roughly the same  value; they do so using a variety representations ranging from simple set representations  bertsekas and castan on  1   decision trees  boutilier et al.  1; boutilier and dearden  1  and algebraic decision diagrams  hoey et al.  1; st-aubin et al.  1   to linear combinations of simple basis functions  koller and parr  1  and neural networks  bertsekas and tsitsiklis  1 .
모while all that is necessary is that the states that are grouped together have the same value  it is often the case that the aggregated states have other properties in common. structured value iteration  boutilier et al.  1  and structured model reduction  dean and givan  1  group states according to stochastic bisimulation equivalence. this is a sufficient but not necessary criterion and in some cases it can result in partitions that are larger than strictly necessary. in the case of  exact  structured value iteration the leaves of the decision tree constitute the blocks of a partition that is reward and transition homogeneous. the structured model reduction algorithm successively refines an initial partition by splitting non-homogeneousblocks and is guaranteed to terminate with the coarsest homogeneous partition; unfortunately  this partition could have a number of blocks exponentialin the number of fluents. this paper provides a method for splitting blocks that provides a global perspective  rather than considering each block in isolation  and for which the objective is to find a non-homogeneous partition that yields an aggregate mdp and corresponding optimal value function that is  close  to the original mdp and its optimal value function. the preliminary experimental results in this paper indicate that our algorithm achieves good performance using a compact representation encoded in terms of a non-homogeneous partition that is a fraction of the size of the representations required by other approaches.
모finding a non-homogeneous partitioning algorithm with a richer representation for blocks  such as adds  seems promising  and this task remains as our future work.
references
 bertsekas and castan on  1  dimitri p. bertsekas and david a. castan on. adaptive aggregation for infinite horizon dynamic programming. ieee transactions on automatic control  1 :1  1.
 bertsekas and tsitsiklis  1  dimitri p. bertsekas and john n. tsitsiklis. neuro-dynamic programming. athena scientific  1.
 boutilier and dearden  1  craig boutilier and richard dearden. approximatingvalue trees in structured dynamic programming. in proceedings icml-1  1.
 boutilier et al.  1  craig boutilier  richard dearden  and moise뫣s goldszmidt. exploiting structure in policy construction. in proceedings ijcai-1  pages 1  1.
 boutilier et al.  1  craig boutilier  thomas dean  and steve hanks. decision-theoretic planning: structural assumptions and computational leverage. journal of artificial intelligence research  1  1.
 dean and givan  1  thomas dean and robert givan. model minimization in markov decision processes. in proceedings aaai-1  1.
 dean and kanazawa  1  thomas dean and keiji kanazawa. a model for reasoning about persistence and causation. computational intelligence  pages 1  1.
 forbes et al.  1  jeff forbes  tim huang  keiji kanazawa  and stuart russell. the batmobile: towards a bayesian automated taxi. in proceedings ijcai-1  1.
 givan et al.  1  robert givan  sonia leach  and thomas dean. bounded-parametermarkov decision processes. artificial intelligence  1-1  1.
 goldsmith and sloan  1  judy goldsmith and robert h. sloan. the complexity of model aggregation. in proceedings aips-1  1.
 gordon  1  geoffrey j. gordon. stable function approximation in dynamic programming. technical report cmu-cs-1  school of computer science  carnegie mellon university  1.
 hoey et al.  1  jesse hoey  robert st-aubin  alan hu  and craig boutilier. spudd: stochastic planning using decision diagrams. in proceedings uai-1  1.
 kim  1  kee-eung kim. representations and algorithms for large stochastic planning problems. phd thesis  brown university  1. in preparation.
 koller and parr  1  daphne koller and ronald parr. computing factored value functions for policies in structured mdps. in proceedings ijcai-1  1.
 munos and moore  1  re뫣mi munos and andrew moore. variable resolution discretization for high-accuracy solutions of optimal control problems. in proceedings ijcai1  1.
 singh and yee  1  satinder p. singh and richard c. yee. an upper bound on the loss from approximate optimalvalue functions. machine learning  1-1  1.
 somenzi  1  fabio somenzi. cudd: cu decision diagram package release 1.1. department of electrical and computerengineering university of colorado at boulder  1.
 st-aubin et al.  1  robert st-aubin  jesse hoey  and craig boutilier. apricodd: approximate policy construction using decision diagrams. in proceedings nips1  1.
 tsitsiklis and van roy  1  john n. tsitsiklis and benjamin van roy. feature-based methods for large scale dynamic programming. machine learning  1-1  1.
 white and eldeib  1  chelsea white and hany eldeib. markov decision processes with impricise transition probabilities. operations research  1   1.
symbolic dynamic programming for first-order mdps
craig boutilier
dept. of computer science
university of toronto
toronto  on  m1s 1 cebly cs.toronto.eduray reiter
dept. of computer science
university of toronto
toronto  on. m1s 1 reiter cs.toronto.edubob price
department of computer science
university of british columbia
vancouver  bc  v1t 1 bprice cs.toronto.eduabstract
we present a dynamic programming approach for the solution of first-order markov decisions processes. this techniqueuses an mdp whose dynamics is represented in a variant of the situation calculus allowing for stochastic actions. it produces a logical description of the optimal value function and policy by constructing a set of first-order formulae that minimally partition state space according to distinctions made by the value function and policy. this is achieved through the use of an operation known as decision-theoretic regression. in effect  our algorithm performs value iteration without explicit enumeration of either the state or action spaces of the mdp. this allows problems involvingrelational fluents and quantificationto be solved without requiring explicit state space enumeration or conversion to propositional form.
1	introduction
markov decision processes  mdps  have become the de facto standard model for decision-theoretic planning problems. however  classic dynamic programming algorithmsfor mdps  puterman  1  require explicit state and action enumeration. for example  the classical representation of a value function is as a table or vector associating a value with each system state; these are produced by iterating over the state space. since state spaces grow exponentially with the number of domain features  the direct application of these models to ai planning problems is limited. as a consequence  much mdp research inaihas focussed onrepresentations andalgorithms that allow complex planning problems to be specified concisely and solved effectively. techniques such as function approximation  bertsekas and tsitsiklis  1  and state aggregation  boutilier et al.  1  have proven reasonably effective at solving mdps with very large state spaces.
모one such approach with a strong connection to classical planning is the decision-theoretic regression  dtr  model  boutilier et al.  1a . the state space of an mdp is characterized by a number of random variables  e.g.  propositions  and the domain is specified using logical representations of actions that capture the regularity in the effects of actions. for instance  bayesian networks  decision trees  algebraic decision diagrams  adds   and probabilisticextensions ofstrips canall beused toconciselyrepresent stochasticactionsinmdps. these representationsare exploitedintheconstructionof a logical representationof the optimal value function and policy  thereby obviating the need for explicit state space enumeration. this process can be viewed as automatic state space abstraction and has been able to solve fairly substantial problems. for instance  the spudd algorithm  hoey et al.  1  has been used to solve mdps with hundreds of millionsof states optimally  producing logical descriptions of value functions that involve only hundreds of distinct values. this works suggests that very large mdps  if described in a logical fashion  can often be solved optimally by exploiting the logical structure of the problem.
모unfortunately  existing dtr algorithms are all designed to work with propositional representations of mdps  while many realistic planning domains are best represented in firstorder terms  exploiting the existence of domain objects  relations over those objects  and the ability to express objectives and action effects using quantification. existing dtr algorithms can only be applied to these problems by grounding or  propositionalizing  the domain.1 unfortunately such an approach is impractical: the number of propositionsgrows veryquicklywiththe number ofdomain objects and relations  and even relatively simple domains can generate incredibly largenumbersofpropositionswhengrounded. the numberof propositionshas a dramatic impact on the complexity of these algorithms. specifying and reasoning with intuitivelysimple domainpropertiesinvolvingquantificationbecomes problematicina propositionalsetting. forinstance  a simple objective such as  e.g.  we want some widget at factory 1  becomes the unwieldy   where the are  relevant  constants  e.g.  widget-1 is at factory 1  or ... . thus grounding our domain description deprives one of the naturalness and expressive power ofrelational representationsand quantificationinspecifyingdynamics andobjectivefunctions. finally  existing dtr algorithms require explicit action enumeration when performing dynamic programming  which is also problematic in first-order domains  since the number of ground actions also grows dramatically with domain size.
in this paper we address these difficulties by proposing
a decision-theoretic regression algorithm for solving firstorder mdps  fomdps . we adopt the the representation for fomdps presented in  reiter  1; boutilier et al.  1b   in which stochastic actions and objective functions are specified using the situationcalculus. we derive a version of value iteration  bellman  1  that constructs first-order representations of value functions and policies by exploiting the logical structure of the mdp. the algorithm constructs a minimal partitioning of state space  represented by a set of first-order formulae  and associates values  or action choices  with each element of the partition.
모as a consequence  our dynamic programming algorithm solves first-order mdps without explicit state space or action enumeration  and without propositionalizingthe domain. furthermore  the technique we propose can be used to reason purely symbolically about value and optimal action choice. our model can be viewed as providing a tight  seamless integration of classic knowledge representation techniques and reasoning methods with solution algorithms for mdps.
모this paper should be viewed as providing the theoretical foundations for first-order decision-theoretic regression. we are encouraged by the success of dtr methods for propositional mdps  where it has been demonstrated that many mdps have value functions and policies that can be represented very concisely using logical techniques. we have no doubt that the use of relations and quantification will ultimately enhance these methods tremendously.
모we review mdps insection 1  and brieflydescribe our representation of fomdps in section 1. we derive our symbolic dynamic programming technique in detail in section 1 and discuss various implementation issues in section 1. we conclude with a discussion of future directions.
1	markov decision processes
we begin withthe standard state-based formulationof mdps. we assume that the domain of interest can be modeled as a fully-observablemdp  bellman  1; puterman  1  with afiniteset ofstates andactions . actionsinducestochastic state transitions with denotingtheprobabilitywith which state is reached when action is executed at state . we also assume a real-valued reward function   associating with each state its immediate utility .1
모a stationary policy describes a particular course of actionto be adoptedby an agent  with denoting the action to be taken in state . the decision problem faced by the agent in an mdp is that of forming an optimal policy that maximizes expected total accumulated reward over an infinite horizon  i.e.  the agent acts indefinitely . we compare policies by adopting expected total discounted reward as our optimalitycriterion  wherein future rewards are discounted at a rate   and the value of a policy   denoted   is given by the expected total discounted reward accrued  that
is 	. policy	is optimal if
for all and policies . the optimal value function is the value of any optimal policy.
모value iteration  bellman  1  is a simple iterative approximation algorithm for constructing optimal policies. it proceeds by constructinga series of -stage-to-govalue functions . setting   we recursively define -stage-togo q-functions:
 1 
and value functions:
 1 
the q-function denotes the expected value of performing action at state with stages to go and acting optimally thereafter. the sequence of value functions produced by value iteration converges linearly to . for some finite   the actions that maximize eq.  1  form an optimal policy  and approximates its value. we refer to puterman  for a discussion of stopping criteria.
모the definition of a q-function can be based on any value function. we define exactly as in eq.  1   but with arbitrary value function replacing on the right-hand side. denotes the value of performing at state   then acting in such a way as to obtain value subsequently.
1	first-order representation of mdps
most planning domains are specified in terms of a set of randomvariables  whichjointlydeterminethestateofthesystem. for example  the system state may be the assignment of truth values to a set of propositional variables. in addition  these variables may themselves be structured  built from various relations  functions  and domain objects  that naturally lend themselves to a first-order representation. representing and solvingmdps under such circumstances is generally impractical usingclassic state-based transitionmatrices and dynamic programmingalgorithms. the difficultylies inthe need toexplicitly enumerate state and action spaces. state spaces grow exponentiallywiththenumberofpropositionalvariables need to characterize the domain. furthermore  in a first-order domain  the number ofinducedpropositionalvariables can grow dramatically with the number of domain objects of interest.1moreover  we are often interested in solving planning problems with infinite domains.
모several representations for propositionally-factoredmdps have been proposed  including probabilistic variants of strips and dynamic bayes nets  boutilier et al.  1 . first-order representations have also been proposed for mdps  including those of poole   and geffner and bonet . in this paper we adopt the first-order  situation calculus mdp representation developed by reiter   and by boutilier et al.  1b  for use in the dtgolog framework. this model has several unique features that make dynamic programming techniques viable. we first review this representational language and methodology  and then show how stochastic actions can be represented in this framework. we also introduce some notation to ease the specification of mdps.
1	the situation calculus
the situation calculus  mccarthy  1  is a first-order languageforaxiomatizingdynamicworlds. inrecent years  ithas been considerably extended beyond the  classical  language to include processes  concurrency  time  etc.  but in all cases  its basic ingredients consist of actions  situations and fluents.
actions
actions are first-order terms consisting of an action function symbol and its arguments. for example  the action of putting block on the table might be denoted by the action term puttbl .
situations
a situation is a first-order term denoting a sequence of actions. these are represented using a binary function symbol do: do denotes the sequence resulting from adding the action to the sequence . the special constant denotes the initialsituation namely the empty action sequence. thus  do is like lisp's cons and is like lisp's . in a blocks world  the situation term
모do stack do puttbl do stack denotes the sequence of actions
	stack	puttbl	stack
foundational axioms for situations are given in  pirri and reiter  1 .
fluents
relationswhose truthvalues varyfrom statetostate are called fluents  and are denoted by predicate symbols whose last argument is a situation term. for example  bin is a relational fluent meaning that in that state reached by performing the action sequence   box is in paris.
axiomatizing a domain theory
a domain theory is axiomatized in the situationcalculus with four classes of axioms  pirri and reiter  1 :
1. action precondition axioms: there is one axiom for each action function   with syntactic form poss
	here 	is a formula with free variables among
	these characterize the preconditions of action	.
1. successor state axioms: there is one such axiom for each fluent   with syntactic form do
	where	isaformulawithfreevariables among
모모모these characterize thetruthvaluesofthe fluent in the next situationdo in terms of the current situation   and they embody a solutiontothe frame problem for deterministic actions  reiter  1 .
1. unique names axiomsfor actions: these state that the actions of the domain are pairwise unequal.
1. initial database: this is a set of first-order sentences whose only situation term is and it specifies the initial state of the domain. the initial database will play no role in this paper.
regression in the situationcalculus
the regression of a formula through an action is a formula that holds prior to being performed iff holds after . successor state axioms support regression in a natural way. suppose that fluent 's successor state axiom is do . we inductively define the regression of a formula whose situation arguments all have the form do as follows:
	regr	do
	regr	regr
regr regr regr regr	regr
1	stochastic actions and the situation calculus
for the purposes of representing probabilisticuncertainty  the above ontology and axiomatization for the situation calculus might appear to be inadequate  because all actions must be deterministic. one can see this requirement most clearly in the syntactic form of successor state axioms where a fluent's truth value in the next situation is uniquely determined by the current situation; thus  the next state is uniquely determined by the present state and the action performed. how then can stochastic actions be represented in the situationcalculus  the trick is to decompose stochastic actions into deterministicprimitivesundernature'scontrol-shechoosesthe deterministic action that actually gets executed  with some specified probability when an agent performs a stochastic action. we then formulate situationcalculus domain axioms using these deterministic choices  bacchus et al.  1; reiter  1; boutilier et al.  1b .
모we illustrate this approach with a simple example in a logistics domain consisting of cities  trucks  and boxes: boxes can be loaded onto and unloaded from trucks  and trucks can be driven between cities.
nature's choices for stochastic actions: for each stochastic action we must specify the deterministic choices available to nature. for instance  the stochastic loadaction can succeed  denoted by loads  or fail  loadf :
	choice load	loads	loadf
similarly the stochastic unloadand drive actionsalso decompose into successful or unsuccessful alternatives chosen by nature with known probabilities.
choice unload unloads unloadf choice drive drives drivef
probabilities for nature's choices: for each of nature's choices associated with action   we specify the probabilityprob withwhichit ischosen  given that was performed in situation :
prob loads	load prob loadf	load
	prob unloads	unload
	rain	rain
	prob unloadf	unload
	prob unloads	unload
prob drives	drive prob drivef	drive
here we see that unloading is less likely to succeed when it is raining.
action preconditions for deterministic actions:
poss loadsbintinposs loadfbintinposs unloadsonposs unloadfonposs drivestrueposs driveftruenature's choicesfor actionneed not have com-mon preconditions  but often they do  as above.
successor state axioms:
bin	do tin	unloads
	bin	loads
	tin	do	drives
	tin	drives
on do loads on	unloads
	rain do	rain
there are two important points to note about this example:
1. by virtue of decomposing stochastic actions into deterministic primitives under nature's control  we get perfectly conventional situation calculus action precondition and successor state axioms that do not refer to stochastic actions. stochastic actions have a status different from deterministic actions  and cannot participate in situation terms.1
1. nowhere do these axioms restrict the domain of discourse tosome prespecified set of trucks  boxes  orcities. there are even models of these axioms with infinitely many-even uncountably many-individuals. if one were to solve an mdp for which this axiomatization is valid  one would obtain  in fact  a solutionthat applies to an entireclass ofmdps witharbitrarydomainsof trucks  boxes and cities.
1	some additional notation
in what follows we use the notion of a state formula    whose only free variables are non-situationvariables and a situation variable . intuitively  a state formula refers only to propertiesofthesituation . aset ofstateformulae
partitions state space iff	  for all  	  and	.
the case notation
to simplify the presentation  we introduce the notation case
as an abbreviation for the formula
where the are state formulae andthe are terms. we sometimes write this case . often the will be constants and the will partition state space. we introduce the following operators on case statements  whose use will be important in the next section :
case	case case
case	case case
case	case case
case	case case
representing probabilities with case notation
let be a stochastic action type with possible outcomes . we assume the probabilities of these outcomes are specified using case notation. specifically  the choice probabilities for are given as: prob	case
where the partitionstate space  and is the probabilityof choice being realized under condition when the agent executes stochastic action .
모our unload stochastic action above is represented in case notation as:
prob unloadsunloadcase rainrainprob unloadfloadcase rainrainnotice that when the probability of nature's choice is situation-independent   e.g.  as in loads   then only a single  case  is present  e.g.  true  .
specifying rewards and values with case notation
an mdp optimization theory contains axioms specifying the reward function. in their simplest form  reward axioms use the function to assert costs and rewards as a function of the action taken  properties of the current situation  or both  note that the action taken can be recovered from the situation term . in what follows  we assume a simple  state-based  rewardmodel inwhichonlyrelationalfluentsdeterminereward  andwe assume thatthisrewardfunctionisspecified usingcase notation:
case
where the	partitionstate space. for example  rewarding the presence of some box in paris can be specified using case	bin
bin
the restrictionto state-based reward is simply to keep the expositionsimple. action costs are easily modeled and are used in our prototype implementation.
모we also use the case notation to represent value functions in a similar fashion  concisely writing in the form case
this use of case statements can be viewed as embodying a form of state space abstraction: rather than assigning values on a state-by-state basis  we distinguish states according to the conditions . those states satisfying can be treatedas an abstractstate. inthisway  we can oftenrepresent value functions and policiesand q-functionssimilarly without state enumeration  exploiting the logical structure of the function. this is similar to the abstraction models discussed in  boutilieret al.  1   but withthe abilityto partitionstate space using first-order formulae.
1	dynamic programming with fomdps
logical representations for mdps provide natural and compact specifications of planning domains  obviating the need for explicit state space enumeration. logical descriptions exploitingregularitiesinvaluefunctionsandpoliciescan alsobe very compact. solving an fomdp can be made much more efficient if the logical structure of value functions can be discovered through inference using the the logical mdp specification  withexpected value computations performed once per abstract state instead of once per state. thus a dynamic programming algorithm that works directly with symbolic representations of value functions offers great potential computational benefit. in this section  we generalize the notion of decision-theoretic regression from propositional mdps to fomdps  and construct a first-order value iteration algorithm.
1	first-order decision-theoretic regression
suppose we are given a value function . the first-order decisiontheoretic regression  fodtr  of throughactiontype is a logical description of the q-function .
in other words  given a set of abstract states corresponding to regionsofstatespace where isconstant  we wishtoproduce a corresponding abstraction for . this is analogous to classical goal regression  the key differences being that action is stochastic.
모let be a stochastic actionwithcorrespondingnature's choices . ignoring preconditions momentarily 
is defined classically as
since different successor states arise only through different nature's choices  the situation calculus analog of this is:
	prob	do	 1 
as described earlier  we assume that the functions	  and	are all described with case state-
ments. respectively denote these by rcase	  pcase
and vcase . then after substituting these case expressions intoeq.  1  and appealing to the case additionand multiplication operators of section 1  we obtain rcase
	pcase	vcase do
the only problem with this expression is that the formula vcase do refers not to the current situation   but to the future situation do   but this is easily remedied with regression:
rcase
	pcase	regr vcase do
we emphasize the critical nature of this step. the representational methodology we adopt-treating stochastic actions using deterministic nature's choices-allows us to apply regression directly to derive properties of the pre-action state thatdetermine thevalue-relevant properties of thepost-action state. specifically  classical regression can be applied directly
to the case statement vcase do because the are deterministic.
모because sums and products of case statements are also case statements  the above expression for is a case statement  say case   that characterizes the q-function for action with respect to . thus from a logical description of we can derive one for . conceptually thiscanbe viewedas transformingtheabstractionofstate space suitable for into one suitable for . it is not hard to show that if the state formulae in 's case statement partition the state space  then so do the defining . this is key to avoiding state and action enumeration in dynamic programming.
	the	above	derivation	ignores	action	preconditions.
to handle preconditions  can no longer be treated as a function  but must be represented by a relation
모모모모모모  meaning that	's q-value in	is	.	this relation holds only if poss	holds for at least one of 's choices	; otherwise the	-value is undefined:
	poss	case
since poss can be distributed into the case statement  by conjoining it with the    the result is again a case statement for the q-relation.
	as an example consider value function	:
	case	bin	rome	bin	rome
that is  if some box is in rome  value is 1; otherwise value is 1. suppose that reward is identical to and our discount rate is . we use the unload action  described above  to illustrate fodtr. the regression of through unload results in a case statement  after simplification  
denotingunloadwith four elements:binromeraintin	romeon	bin	romeraintin	rome	on	bin	rome
	tin	rome	on
모모모모모모모모bin	rome and the associated q-values:	;	;	;
모모모. before simplification  the case statement consisted of 1 formulae  two of which were inconsistent and two pairs of which had identical q-values.
모an important property of fodtr is that it not only produces an abstraction of state space to describe   it also abstracts the action space as well. with a small number of logical formulae  it captures the q-values for each situation and each instantiation of . while state space abstraction has been explored in the context of decision-theoretic regression for propositional representations of mdps  little work has focused on abstracting the action space in this way.
모finally  although our example works with specific numerical values inthe case statements  purelysymbolic descriptions of value can also be reasoned with in this way. for example  if the q-value of action drive depends on the weight of truck in the current situation  the value term in a case statement can be made to depend on this property of the situation  i.e.  weight  . this can prove especially useful for reasoning with continuous  or hybrid  state and action spaces.
1	symbolic dynamic programming
value iteration consists of setting and repeatedly applying eq.  1  and eq.  1  until a suitable termination condition is met. since is described symbolically and fodtr can be used to implement eq.  1  logically  we need only derive a  logical implementation  of eq.  1  in order to have a formofdynamic programmingthat can compute optimalpolicies for fomdps without explicit state or action enumeration  together witha method for terminationtestingand policyextraction .
모in what follows  we assume that all values occurring in the case statements for are numerical constants  which means that the case statements for   and all have this property.
	suppose we have computed	-stage-to-go q-relations
             one for each action type   of the form case   where the are numerical constants.
letting denote the -stage-to-go value function  eq.  1  can be written
 1 
we assume that some stochastic action  e.g.  a deterministic no-op  is executable in every situation  so that will be a function.  ifnot we can easily define itas a relation.  we now derive a series of expressions for the r.h.s. of this equivalence. assuming domain closure for action types  i.e.  all actions are instances of some   we have
to minimize notational clutter  represent this generically by we are supposing that we have already determined the values for each action type   in the form of a case statement:
 1  substitute eq.  1  into the previous expression to get
since the are constants  we can distribute the existential quantifiers into the case expression:
writing	as	  and recallingthe definitionof
the case union operator	of section 1  we have
suppose	has the form
. therefore 
this simplifies to
recalling the definition of the case notation  we get
the only remaining task is to characterize the expressions in terms of the case statement for . suppose this case statement is:
then it is easy to show that
substitutingthis last expression for
in the above expression for	gives us
next  because the and are numerical constants  we can distribute the universal quantifier as an existential quantifier in the antecedent of the implications  to get
next  recalling how the were introduced by unioning the case expressions for all the -values  we get
finally  we can again exploit the fact that the s are numerical constants  as opposed to symbolic terms   and therefore can be compared. this allows us to write our final expression for :
모모모모모모모모모... case
if we modify the definition of the operator so that it sorts the rows according to their values  and merges rows with identical values  we get the pleasing expression
 1  case	...
 this determines a simple case statement that completely defines the value function given the logical description of the relations . together with the fodtr algorithm for producing q-relations  this provides the means to construct the sequence of value functions that characterize value iteration in a purely symbolic fashion  eliminating the need for state and action enumeration. it is not hard to show that the case conditions defining partition state space.
모finally  notice that we obtained the case expression  1  by a sequence of equivalence-preserving transformations from the definition  1  of the q-function  suitably modified to accommodate actionpreconditions  andthe definition 1 of the value function. therefore  we have:
theorem 1 the case expression  1  is a correct representation for relative to the specifications  1  and  1  for the q-function and value function respectively.
모with these pieces in place  we can summarize first-order value iteration as follows: given as input a first-order representation of  a case statement  and our action model  we set   and perform the following steps until termination:
1. for each action type compute the case representation of  using as in eq.  1  .
1. compute the case representation of  using the as in eq.  1  .
1. increment	.
termination of first-order value iteration is straightforward.
given the case statements and for value functions and   we form and simplify the resultingcase statement byremoval ofanyinconsistentelements. if each case hasa valuetermlessthat specifiedthreshold   value iteration terminates. extraction of an optimal policy is also straightforward: one simply needs to extract the maximizing actions from the set of q-functions derived from the optimal value function. the optimal policy will thus be represented symbolically with a case statement.
1	an illustration
to givea flavor of theform offirst-ordervalue functions consider an example where the reward function is given by three statements:
	paris	typea
	paris	typea
paris
that is  we want a box of type a in paris  but will accept a box of another type if a type a box is unavailable. actions include the load  unload  and drive actions described above. we include action costs: the action unload has cost   load has cost   and drive has cost . the optimal one-stage policy chooses only unloadingor no-op  since with only one stage to go  drivingand loadinghave no value . our algorithmderives the followingconditionsfor unload to be executed:
	on	tin	paris
	bin	paris
	typea	rain
	bin	paris	typea
thus a box is unloaded if there is a box on some truck in paris  and there is no box currently in paris  or is a type a box and it's not raining  and there's no type a box in paris. no-opisexecuted if thenegationof the conditionabove holds  since for a one-step backup there is no value yet discovered for driving or loading . it is important to note that this partitioningremains fixed  as does thepartitioningfor theresultant value function  regardless of the number of domain objects and extraneous relations in the problem description. thus we get stronger abstractionthan wouldbe possibleusing a propositionalized version of the problem. also note that this describes the conditionsunder which one performs any instance of the unload action. in this way our algorithm allows for action abstraction  allowing one to produce value functions and policies without explicit enumeration of action instances.
1	a  very  preliminary implementation
we have implemented  in prolog  the basic bellman backup operator  i.e.  single iterations of one-step value iteration  defined by eq  1 . the implementation is based entirely on a rewrite interpreter that applies programmer specified rewrite rules to situation calculus formulae until no further rewrites are possible. the program first computes the case statements for the q-values for all the stochastic actions. next  from these it computes the pairs required by the case statement  1   and finally  the case statement of  1  itself. throughout  logical simplification is applied  also specified by rewrite rules  to all subformulas of the current formula.
모from a practical point of view  the key component in efficientlyimplementingfirst-order dtr is logical simplification to ensure manageable formulae describing the partitions. our current implementation performs only the most rudimentary logicalsimplificationanddoesnotalwaysproduceconcisedescriptions of the cases within partitions. neither can it eliminate all inconsistent partitions. the main reason forthese limitations is that the current implementation lacks a first-order theorem-prover. for the example mdps we have looked at  sophisticated theorem-proving appears not to be necessary  but simple-minded simplification rules that don't know very much about quantifiers are simply too weak.
모we ran value iteration to termination under our implementation using the reward function that gives a reward of 1 for having any box in paris  and zero reward otherwise  for simplicity  it is treated as a terminal reward  and is received only once . because our simplifier did not include a theoremprover  some of the intermediate computations were handedited to further simplify the resulting expressions. we obtained the following optimal value function:
bin parisrainon
bin paristinrainon
bin paristinrainonbin parisontinrainbintinonbin parisrainon
bin parisontinbin parison	rain	bin	tin
we emphasize again that this value functionapplies no matter how many domain objects there are.
모ouralgorithmis not competitivewithstate of theart propositionalmdp solvers  largelybecause solverssuch as spudd  hoey et al.  1  use very efficient implementations of logical reasoning software. we are currently developing a version of the fodtr algorithm that uses a first-order theoremprover to enhance its performance. of course  at another level  one can argue that propositional mdp solvers cannot even get off the groundwhen  even trivial planningproblems have a large number of domain objects.
모an important issue we hope to address in the near future is theuse ofhybridrepresentationsofmdpsand valuefunctions that allow one to adopt efficient data structures like adds or decision trees  but instantiate these structures with first-order formulae. this would allow the expressive power of our firstorder model  but restrict the syntactic form of formulae somewhat so that simplification and consistency checking could be implemented more effectively for  typical  problem instances.
1	concluding remarks
we have described the first approach for solving mdps specified in first-order logic by dynamic programming. by the careful integration of sophisticated kr methods with classic mdp algorithms  we have developed a framework in which mdps can be specified concisely and naturally and solved without explicit state and action enumeration. indeed  nothing in our model prevents its direct application to infinite domains. furthermore  it permits the symbolic representation of value functions and policies.
모a number of interesting directions remain to be explored. as mentioned the practicalityofthisapproachdepends onthe use of sophisticated simplification methods. we are currently incorporatingseveral of these intoour implementation. other dynamicprogrammingalgorithms e.g.  modifiedpolicyiteration can be implemented directly withinour framework. approximationmethodsbased onmergingpartitionswithsimilar valuescan also be appliedwithease. finally the investigation of symbolic dynamic programming to continuous and hybrid domains offers exciting possibilities.
acknowledgements: this research was supported by nserc and iris project bac  dealing with actions.  thanks to the referees for their helpful suggestions on the presentation of this paper.
references
 bacchus et al.  1  f. bacchus  j. y. halpern  and h. j. levesque. reasoning about noisy sensors in the situation calculus. ijcai-1  1  montreal  1.
 bellman  1  r. e. bellman. dynamic programming. princeton university press  princeton  1.
 bertsekas and tsitsiklis  1  d. p. bertsekasandj.. n. tsitsiklis. neuro-dynamic programming. athena  belmont  ma  1.
 boutilier et al.  1  c. boutilier  t. dean  and s. hanks. decision theoretic planning: structural assumptions and computational leverage. j. art. intel. res.  1-1  1.
 boutilier et al.  1a  c. boutilier  r. dearden  and m. goldszmidt. stochasticdynamicprogrammingwith factoredrepresentations. art. intel.  1-1  1.
 boutilier et al.  1b  c. boutilier  r. reiter  m. soutchanski and s. thrun. decision-theoretic  high-level agent programming in the situation calculus. aaai-1  1  austin  tx  1.
 geffner and bonet  1  h. geffner and b. bonet. highlevel planning and control with incomplete information using pomdps. fall aaai symp. on cognitive robotics  orlando  fl  1.
 hoey et al.  1  j. hoey  r. st-aubin  a. hu  and c. boutilier. spudd: stochastic planning using decision diagrams. uai-1  1  stockholm  1.
 mccarthy  1  j. mccarthy. situations  actions and causallaws. tech. report  stanford univ.  1. repr. semantic information processing  m. minsky ed.   mit press  cambridge  1  1.
 pirri and reiter  1  f. pirri and r. reiter. some contributionsto the metatheory of the situation calculus. jacm  1 :1  1.
 poole  1  d. poole. the independent choice logic for modelling multiple agents under uncertainty. art. intel.  1-1 :1- 1  1.
 puterman  1  m. l. puterman. markov decision processes: discrete stochastic dynamic programming. wiley  1.
 reiter  1  r. reiter. the frame problem in the situation calculus: a simple solution  sometimes  and a completeness result for goal regression. in v. lifschitz  ed.  artificial intelligence and mathematical theory of computation  papers in honor of john mccarthy   1. academic press  1.
 reiter  1  r. reiter. knowledge in action: logical foundations for describing and implementing dynamical systems. mit press  cambridge  ma  1.

uncertainty and probabilistic reasoning
markov decision processes

adaptive control of acyclic progressive processing task structures
stephane cardon뫣	abdel-illah mouaddibshlomo zilbersteinrichard washingtoncril-iut de lens-universite뫣 d'artoiscomputer science deptnasa ames research ctrrue de l'universite뫣  s.p. 1university of massachusettsms 1 moffet field1 lens cedex franceamherst  ma 1 usaca 1  usacardon  mouaddib  cril.univ-artois.frzilberstein cs.umass.edurwashington arc.nasa.gov
abstract
the progressive processing model allows a system to trade off resource consumption against the quality of the outcome by mapping each activity to a graph of potential solution methods. in the past  only semi-linear graphs have been used. we examine the application of the model to control the operation of an autonomous rover which operates under tight resource constraints. the task structure is generalized to directed acyclic graphs for which the optimal schedule can be computed by solving a corresponding markov decision problem. we evaluate the complexity of the solution analytically and experimentally and show that it provides a practical approach to building an adaptive controller for this application.
1	introduction
planetary rovers are unmanned vehicles equipped with cameras and a variety of scientific sensors. they have proved to be a cost-effective mechanism in space exploration and will continue to play a major role in future nasa missions  washington et al.  1 . despite dramatic improvements in capabilities and computationalpower  space systems continueto operate with scarce resources such as power  computer storage  and communicationbandwidth. they also have a limited life span. as a result  it is necessary to maximize operation time during periods of no communication with the control center. it is also necessary to manage resources carefully when deciding what activity should be performed and how to best accomplish the rover's tasks. the main objective of this work is to develop planning and monitoring algorithms that can optimize scientific return when operating with limited resources.
모for this purpose  the progressive processing model provides a suitable framework  mouaddib and zilberstein  1; 1 . progressive processing allows a system to trade off computational resources against the quality of the result similar to other resource-bounded reasoning techniques such as  flexible computation   horvitz  1   anytime algorithms   dean and boddy  1; zilberstein  1    imprecise computation   hull et al.  1; liu et al.  1  and  design-to-time   garvey and lesser  1 . the specific characteristic of progressive processing is the mapping of the computational process  or a plan  into a hierarchical graph of alternative methods. each method has an associated probabilistic description of the resource/quality tradeoff it offers.
모in previous work  mouaddib and zilbertstein  mouaddib and zilberstein  1; 1  presented a solution to the control problem of progressive processing. this problem is to decide at run-time which subset of methods should be executed so as to maximize the overall utility of a system. the reactive controller is adaptive and takes into account the remaining resources and possible changes in the environment. this is achieved by reformulating the control problem as a markov decision process  mdp . an efficient implementation that can handle run-time modifications of the overall plan has been implemented  mouaddib and zilberstein  1 . in the past  only semi-linear task structures have been used. that is  each step of a task could be implemented by one of several alternatives  after which the next step is considered.
모the rover application  however requires several extensions of the previously developed controller. figure 1 shows a sample plan that illustrates the problem. in the plan  the rover locates an object and aims the camera  possibly after moving closer to the object. it can take a picture at one of three different resolutions  and can compress it using either high or low compression methods. in between  depending on the analysis of the image  the rover may decide to investigate the chemical components of the object. this plan presents several new requirements that have not been previously addressed:
task inter-dependency: task execution may depend on the outcome of previous actions. for example  the ability to investigate the chemical components depends on the results of image analysis.
non linearity: the overall task structure becomes a directed acyclic graph. execution follows a linear path through the graph.
multiple resources: the controller must optimize its operation with respect to multiple resources.
모the contribution of this paper is three fold. first  we generalize progressive processing to non-linear task structures with one resource and construct an optimal controller for such plans. second  we analyze the complexityof the solutionboth analytically and empirically and show that it is an effective approach. third  we examine the implications of increasing
unit 1

figure 1: planetary rover's plan
the number of resources and size of the plan on the complexity. we address this problem by introducing the notion of opportunity cost that can be estimated quickly for large  nonlinear plans.
1	progressive processing control problem
the work presented in this paper builds on the concepts of progressive processing units and plans. in this section  we define these notions and define our problem.
1	preliminary definitions
progressive processing is a reasoning technique that creates an incremental solution of a problem by using a hierarchy of steps  where each step contributes to the solution. this hierarchy can be viewed as the reasoning scheme which leads us to the solution. we denote by pru  progressive processing unit  this hierarchy. notice that we may associate a utility to the execution of a pru's step. this utility is an abstract measure of the step's quality in the solution.
모with this intuitive definition  a rover's plan may be seen as an ordered set of prus. more generally  the plan may be non-linear  i.e.  a directed acyclic graph of prus. one unit has many successors. in this graph  a step will be considered for executionwhen the succession of steps executed so far has sufficient quality to enable this step.
모for the example of planetary rovers  we have three prus. the first corresponds to going to a site  taking a picture and analyzing this image in order to find rocks. the second depends on the analysis results. if the site contains rocks  the rover may activate its apxs  alpha proton x-ray spectrometer  in order to determine the chemical components of the rocks. the last pru corresponds to compressing the scientific data  images and chemical components .
모we may transform this plan to a linear plan but  in that case  we lose generality and the inter-unit dependency information. in fact  the non-linearity of a plan and the notion of dependence allow for more realistic applications.
모the planetary rover's problem  or progressive processing problem control  is to choose the set of executing tasks that maximize the global utility while respecting resource constraints.
1	formal definitions
we formalize these definitions  focusing on one resource: operation time.
definition 1 a plan is a directed acyclic graph of progressive processing units  .
example: one plan is figure 1  where	is unit 1 	is unit 1 and	is unit 1.
definition 1 a progressive processing unit  pru     consists of a set of steps 	  a deadline and a set of successors 
모모모모모모모모모모모모모모모모모모모모 	where and	is the utility interval corresponding to minimal and maximal utilities required to move to the next unit	.
example: pru has three steps: is step 1  is step 1 and is step 1. it has two successors: and .
definition 1 a step of a unit     consists of a set of tasks 	and a userspecified attribute  skippable  which indicates if this step may be skipped or not. we denote by the skippable attribute  and when is skippable and 1 otherwise. we define to be the special step  with no tasks  that is used to show the beginning of executing a unit.
example: step 1 of unit 1 has three tasks. the first is  picture low resolution   the second is  picture medium resolution  and the third is  picture high resolution . this step can be skipped if we don't want to take a picture at a particular site.
definition 1 a task of a step of a unit     consists of an executable module  a utility  or quality  and a discrete probability distribution of resources consumed when the module is executed. the number of elements of is
.
the probabilities can be determined from ground experimental rovers and simulations.
example: task 1 of step 1 of unit 1  consists of taking a mediumresolution picture of the site. the distribution of resource consumption for this task  as used in our experiments  is	.
definition 1 the progressive processing control problem is to select at run-time the set of tasks that maximizes the global utility.
1	selecting tasks
when the rover has completed step	 meaning that a task of that step has been executed or that the step has been
skipped   it has three possible decisions:
	it can execute one task	of the next step
if this step exists 
it can skip the next step	  if it exists and it is skippable  it can move to a accessible successor unit	.
모the optimal decision is the one that maximizes the global utility. the sequence of decisions will determine the set of tasks executed. the global utility is the cumulative reward of the executed tasks  reward is measured by the qualities associated to the tasks .
모in this context  the progressive processing control problem is a state space where a transition from one state to an another is the decision made by the rover. the state represents the last executed step and the remaining resources. in fact  the choice of the next action  when the rover is in a particular state  depends only on the currentstate. this is the markovproperty. so  the progressive processing problem control may be seen as a problem of controlling a markov decision process.
1	markov decision process controller
we now define the corresponding markov decision process.
states
a markov decision process is a graph of states such that:
definition 1 a state    consists of the last executed step and the remaining resources.
definition 1 the accumulated quality is the sum of qualities of all executed task.
definition 1 quality dependency of units: a state is the successor of when the accumulated quality of belongs to the utility interval and . we say in this case that is an accessible state from .
with the quality dependency  we express the dependence between the problems  including the precedence constraint  quality not null or belonging to a certain interval .
example: to go to site 1  the rover should have executed
      analysis of taken picture . so  we give a high reward to and   where .
definition 1 a state is terminal when all the resources have been fully elapsed  or when there is no accessible successor.
transitions
we have three possible transition types:	 	  and	.
definition 1 a transition  type   is probabilistic. it consists of executing a task of the next step . we denote it	  where represents the resources consumed for the execution of .
definition 1 a transition	 type	  is deterministic.
it consists of skipping the next step	.	we denote it
.
definition 1 a transition	 type	  is deterministic.
it consists of passing to an another unit.	we denote it
	  where	is an accessible suc-
cessor of	.
transition rules
now  let us define transition rules.
모pr state state   transition   is the probabilityof moving from to when taking transition .
	let	be the current state.
1. transitions
	let	be the resources consumed by task	.
 a   
 
 b   
모모모모모모모모  where	is accessible from .
1. transitions	 if	  i.e.  the step is skippable 
.
1. transitions
  where
is accessible from .
1	state's value
the value associated to each state is a measure of utility of this state in the mdp. this value is calculated recursively by using the value of successor states.
모for this  we denote   states accessible from and an accessible state for which the associated task is in unit .
transitions
transitions
transitions
so  the value of one state is:
 1 
1	performance of the mdp controller
we have implemented the execution model described above  the policy-constructionalgorithm  and a simulator that allows plans to be executed and visualized in the rover domain. the performance of the resulting system is evaluated using the simulator. this section illustrates the results obtained using the system.
1	analysis
because of the one-to-one correspondence between the states of mdp and the states of the control  the optimal solution of the mdp is the optimal control.
claim 1  mdp size  the number of states in the mdp is bounded by:
 1 
proof: note that two states with same remaining resources and associated step generatethe same successors. so  to avoid an exponential complexity  we must notice already created states.
모since consumable resources are bounded  as well as the number of steps  we obtain a bounded number of states. moreover  two states are different if and only if the associated steps are different or the remaining resources are different. so  we have the bound:	.
claim 1  solution time  the time elapsed to calculate the optimal policy for the mdp is bounded by:
 1 
proof: the time consumed to solve the mdp is bounded by the number of transitions multiplied by the cost of searching already created states  if we don't want an exponential complexity .
모the number of transitions from a state is bounded by the number of transitions of each type: of type   and one each of type and . so  for that one state  the number of transitions generated is bounded by:
모therefore  the bound on the number of transitions in the entire mdp is  using equation 1 :
모finally  the search of already created states involves states in the worst case: since we store states by unit and by step  only those states associated with the specified unit and step need to be checked.
so  we obtain the following bound:
in conclusion  our controller has a complexity of
모모모모모in space and	in policyconstruction time. we also notice that if the deadline is not a function of n  we obtain a linear complexity.
1	experimental results
we take a non-linear plan composed of	units and of depth
     . each unit is composed of four steps. the first step consists of going to a site  the second of arming the camera  the third of taking the photo  low  medium or high resolution  and the last of compressing the obtained scientific data.
모for our experiments  we set the average time to execute this unit to be 1 seconds. so  the deadline of each unit in plan is 1 seconds multiplied by the depth of this unit in the plan. if we assume that the number of nodes at each level of the plan increases by at least a constant factor   then the depth of the plan is   and we obtain the complexity in space and in time.
모however  we notice that we use exactly the resources needed to execute a task. what happens if we consume resources in fixed-size packets instead of exactly the needed resources 
value  size=1 value  size=1 error1.1.1.1%1.1.1.1%1.1.1.1%1.1.1.1%1.1.1.1%table 1: value computed as a function of	  using packets of resources.
모table 1 shows state values computed for packet sizes of 1 and 1; little error is incurred by increasing the packet size. figure 1 shows policy-creationtime for plans of realistic size. using a packet size of 1  the time used to solve the mdp is divided by 1 and the number of states created is divided by 1. in fact  we can generalize this result:
claim 1 if is the number of states created and is the time consumed to create these states  for packet size equal to 1   then  is the maximal number of created states and the maximal time elapsed  for packet size equal to  .
     and	are bounds since equations 1 and 1 are bounds for	and	.
proof: we suppose that and are number of created states and time consumed for packet size equal to 1. we now take packets of resources.
solution time  in seconds 
1
1
1
1
1
1
1
	1	1	1
figure 1: solution time for packet size equal to 1 and 1
모notice that size of packet has no influence on the size of the problem . moreover  it leaves the number of steps in unit and number of tasks in step invariant.
모so  the packet size proportionally reduces the deadline of each unit    . therefore  equation 1 implies that  is the maximal number of created states and equation 1 implies that  is the maximal time consumed to create these states.
모the last term of equation 1 can be changed by packets  but it is just a constant.
1	dynamic operation and scalability
in the previous section  we presented an optimal solution to the control problem of acyclic progressive processing task structures. in this section  we extend the applicability of the solution to situations in which the global policy cannot be computed at run-time either by the control center or the rover itself. there are two primary motivations to avoid run-time construction of the policy.
1. dynamic operation: we want to be able to modify the plan at run-time  e.g.  insert or delete a pru  without necessarily recomputing the control policy.
1. scalability: we want to be able to track more resources  and the complexity of global policy construction grows exponentially with the number of resources.
모while we focus in this paper on the dynamic operation  scalability is also enhanced by the proposed solution. our approach is to exploit the fact that the units of the plan are largely independent. we try to capture the dependency of the execution of each unit on the remaining plan using a notion similar to opportunitycost  zilberstein and mouaddib  1 .
1	dynamic control with one resource
definition 1 let represent the optimal local value of the state after step with remaining resources
.
모note that is the same as for a modified plan in which unit has no successors. is simply the value ofthe best policy for pru ignoring the remaining plan  hence it is called local value.
claim 1 the global value function can be reformulated as follows.
 1 
proof: this is an immediate result of the definition of   and   and the additivity of the utility.
모for each pru  we can easily compute the local value function and construct a local policy off-line. the global value function  and policy  can be constructed at run-time  if we can obtain a fast estimate of .
1	estimating
one way to think about is as the value of terminating the work on a certain pru with some level of resources  then using the resources for the remaining plan. this is exactly what we need to know. estimating is equivalent to estimating the global value of the remaining plan. we have examined several approximation schemes for . due to space limitations  we only sketch one simple approach.
construct an optimal local value function and local policy for each pru  done once  off-line .
for each level of rr  compute the expected value and expected resource consumptionby the optimal local policy  done once  off-line .
go over the plan graph  backwards  from the leaves and compute an expected value for each node  for each level of at that node. the backup is relatively simple: maximizing for each level of at that node the combined parent/child value. in this process  it is necessary to take into account the difference between the deadlines of each parent/child pair. the quality dependency constraints are enforced with respect to the expected quality of the parent.
모this algorithm is linear in the size of the graph. the computational savings and sub-optimality of the outcome are the result of using precalculated expected resource consumption and value for each pru.
1	conclusion
we have presented a solution to the problem of adaptive control of acyclic progressive processing tasks. this approach  which relies on solving a corresponding mdp  generalizes earlier work on mdpc  mouaddib and zilberstein  1 ; it permits for the first time the treatment of non-linear progressive processing task structures. moreover  our approach addresses effectively the high degree of uncertainty regarding resource consumption. in that sense  it improves on existing models for resource-bounded reasoning such as  imprecise computation   hull et al.  1; liu et al.  1  and  design-to-time   garvey and lesser  1 . finally  the model captures inter-task quality dependency similar to the enable and disable relationships in the  design-to-time  framework  garvey et al.  1 .
모we also address in the paper a potential deficiency of the approach which may require a large amount of memory and time to create and solve the mdp and to store the resulting policy. using packets of consumable resources  we managed to reduce the size of the mdp with a small loss of quality  1% for packet size equal to 1 . finally  we address the issue of dynamic operation and scalability by approximating the solution of the mdp.
모the solution presented in this paper includes time as the only resource. future work will allow us to represent a vector of resources  mouaddib  1  such as storage capacity and power. an additional challenge is to develop more precise estimates of the value function and adapt the approach to the case of multiple resources. a related project is using reinforcement learning to estimate  bernstein and zilberstein  1 .
모more accuratemodels of resourceusage can be obtainedby using results from actual rover testing or high-fidelity simulations. we plan to make use of ongoing development of a high-fidelity rover simulation at nasa ames research center to refine our models to make them more accurate and applicable to realistic rover problems  with the goal of testing our approach on actual rover hardware.
모the pru used in our experiments cf. 1 can be extended to solve a satellite scheduling problem. with this approach  more units can be scheduled than by traditional methods. indeed  at this moment  a satellite in orbit around the world schedules the equivalent of only about 1 units. with our approach  it would be able to schedule 1  thus increasing its productivity 1-fold.
acknowledgements
support for this work was provided in part by the national science foundation under grants iri-1  iri-1  and int-1  and by a project of plan etat/nord-pasde-calais  and by iut de lens.
references
 bernstein and zilberstein  1  daniel s. bernstein and shlomo zilberstein. reinforcement learning for weaklycoupled mdps and an application to planetary rover control. in uai  submitted for publication   1.
 dean and boddy  1  t. dean and m. boddy. an analysis of time-dependent planning. in seventh national conference on artificial intelligence  pages 1  1.
 garvey and lesser  1  a. garvey and v. lesser. desingto-time real-time scheduling. ieee transaction on systems  man  and cybernetics  1-1  1.
 garvey et al.  1  a. garvey  m. humphrey  and v. lesser. task interdependencies in desing-to-time real-time scheduling. in aaai  pages 1  1.
 horvitz  1  e. j. horvitz. reasoning about beliefs and actions undercomputationalresourceconstraints. in workshop uai-1  pages 1  1.
 hull et al.  1  d. hull  w. c. feng  and j. w. s. liu. operating system support for imprecise computation. in aaai fall symposium on flexible computation  pages 1- 1  1.
 liu et al.  1  j. liu  k. lin  w. shih  j. chung a. yu  and w. zhao. algorithms sor scheduling imprecise computations. ieee computer  1-1  1.
 mouaddib and zilberstein  1  a-i. mouaddib and s. zilberstein. handling duration uncertainty in meta-level control of progressiveprocessing. in ijcai  pages 1  1.
 mouaddib and zilberstein  1  a-i. mouaddib and s. zilberstein. optimal scheduling of dynamic progressive processing. in ecai  pages 1  1.
 mouaddib  1  a. i. mouaddib. optimizing multi-criteria decision quality in a progressive processing system. in aaai symposium on realtime autonomous agent  pages 1  1.
 washington et al.  1  r. washington  k. golden  j. bresina  d. e. smith  c. anderson  and t. smith. autonomous rovers for mars exploration. in proceedings of the 1 ieee aerospace conference  1.
 zilberstein and mouaddib  1  s. zilberstein and a-i. mouaddib. reactive control of dynamic progressive processing. in ijcai  pages 1  1.
 zilberstein  1  s. zilberstein. using anytime algorithms in intelligent systems. ai magazine  1-1  1.

an improved grid-based approximation algorithm for pomdps
rong zhou and eric a. hansen
computer science department
mississippi state university
mississippi state  ms 1
rzhou hansen  cs.msstate.eduabstract
although a partially observable markov decision process  pomdp  provides an appealing model for problems of planning under uncertainty  exact algorithms for pomdps are intractable. this motivates work on approximation algorithms  and grid-based approximation is a widely-used approach. we describe a novel approach to grid-based approximation that uses a variable-resolution regular grid  and show that it outperforms previous grid-based approaches to approximation.
1	introduction
a partially observable markov decision process  pomdp  models planning problems for which actions have stochastic effects and sensors provide imperfect and incomplete state information. originally developed in the operations research community  this model has been adopted by the ai community as a framework for research in planning under uncertainty and related problems of reinforcement learning.
모a standard approach to solving a pomdp is to transform it into an equivalent  fully observable markov decision process with a state space that consists of all probability distributions over the core states of the pomdp  and to solve the pomdp in this form. for a pomdp with core states  the transformed state space is the -dimensional simplex  or belief simplex. although a continuous state space such as this presents a computational challenge  discrete approximations of continuous state spaces are a natural and oftenused approximation technique. grid-based approximation was the first approach adopted for solving pomdps  and it is still widely-used  drake  1; kakalik  1; eckles  1; lovejoy  1; brafman  1; hauskrecht  1; 1 . a finite grid is placed over the belief simplex  values are computed for points in the grid  and interpolation is used to evaluate all other points in the simplex. this is closely related to grid-approximation techniques for other continuous mdps  munos & moore  1 .
모different approaches to constructing a grid have been explored. lovejoy  1  describes a fixed-resolution regular grid  in which the points of the grid are spaced in a regular pattern and divide the belief simplex into equal-sized subsimplices. this allows a very efficent interpolation algorithm based on the concept of triangulation. however  the size of the grid grows exponentially with any increase in resolution. hauskrecht  1  1  and brafman  1  propose nonregular grids that allow the points of the grid to be spaced unevenly in the belief simplex  in order to approximate the contours of the value function as economically as possible. however  interpolation algorithms for non-regular grids are much less efficient. in this paper  we develop a variableresolution regular grid that combines the strengths of both approaches. it allows the resolution of the grid to be adjusted in different regions of the belief simplex  but in a regular pattern that makes it possible to generalize the efficient interpolation algorithm for regular grids. the result is a higher quality approximation at less computational cost.
모the paper is organized as follows. we begin with a review of the pomdp model  the grid approximation approach  and lovejoy's interpolation algorithm for regular grids. we generalize the interpolation algorithm for use with a variableresolution regular grid  and discuss methods for adjusting the resolution of the grid to achieve the best quality-time tradeoff for the approximation. we evaluate this approach analytically and experimentally  and show that it outperforms previous grid-approximation methods.
1	background
we consider a discrete-time pomdp with a finite set of states
   a finite set of actions   and a finite set of observations . each time period  the environment is in some state   the agent takes an action for which it receives an immediate reward with expected value   the environment makes a transition to state with probability   and the agent observes with probability . let denote a vector of state probabilities  called a belief  or information  state  where denotes the probability that the environment is in state . if action is taken and observation follows  the successor belief state  denoted   is determined by revising each state probability as follows 

where	and	the
denominator	is	a	normalizing	factor
	.	a	pomdp	is
solved by finding a rule for selecting actions  called a policy  that optimizes a performance objective. we assume the objective is to maximize expected total discounted reward over an infinite horizon  where is the discount factor . an optimal policy has a value function that satisfies the bellman optimality equation 
where	. exact algorithms for solving this optimality equation are intractable for all but trivial problems. this motivates work on approximation methods.
1	grid-based approximation
let	denote a grid that contains a finite set of belief states 
       . a belief state is a convexcombination of the belief states in grid if
for all
where	is a vector of size	 	for all	  and
모모모모모모. for each belief state in the grid  we store a value denoted . the convex interpolation function 
defines a continuous and piecewise-linear value function over all belief states  given values for the belief states in the grid. to compute values for the belief states in the grid  we use value iteration with the update rule 
which converges to a unique fixed-point solution that is guaranteed to be an upper bound on the optimal value function.
모a belief state can be expressed as many different convex combinations of grid points. the convex combination that provides the best upper bound can be found by solving the following linear program.
variables:	for minimize:
	constraints:	for all
for
using linear programming to find the best interpolation can be very expensive. however  any convex combination provides an upper bound  and fast methods for finding a nonoptimal convex combination can produce good upper bounds. design of a grid-approximation strategy requires addressing two questions. what interpolation method provides a good tradeoff between time and quality  what belief states should be included in the grid  every grid must contain the corner points of the belief simplex to ensure that a convex combination can always be found. different strategies for adding other points to a grid have been explored. we begin with a review of a fixed-resolution regular grid and the efficient interpolation algorithm it supports. in the rest of the paper  we develop a variable-resolution generalization of this.
1	a fixed-resolution regular grid
lovejoy  1  constructs a regular grid as follows. let be a positive integer that represents the resolution of the grid. the set of belief states in the grid is defined as 

where denotes the set of -vectors of non-negative integers. the grid divides the belief simplex into a set of equalsize sub-simplices. note that when   the grid contains only the corner points of the simplex. the number of points in the grid is given by the formula:

모the advantage of a regular grid is that it allows an elegant and efficient method of interpolation. to evaluate a belief state requires finding the vertices of the smallest subsimplex that contains   and then finding the coefficients of interpolation . lovejoy defines a second grid of integer
vectors 
and convertsa belief state into an integer vector in order to perform interpolation. because of a one-to-one correspondence between grid points in and   the subsimplex that contains can be used to determine the sub-simplex that contains . here  we summarize the steps of the interpolation algorithm. we refer to lovejoy  1  for a detailed explanation.
1. given a belief state   create an -vector such that for .  this transforms probability
	density function	into cumulative distribution function	. 
1. let be the largest integer -vector such that for all .
1. let	be an	-vector such that	for all
.
1. let be an -vector that contains a permutation of the integers that orders the components of in descending fashion  so that	.
1. find the vertices of the sub-simplex in that contains   as follows:
for
if
otherwise
by the isomorphism between and   this identifies the corresponding vertices of the sub-simplex in that contains .
1. find the barycentric coordinates for the interpolation  as follows:
for
1. let	.
the complexity of each of these steps is   with the possible exception of step 1 which requires sorting the elements of an -vector. although sorting has worst-case complexity
모모모모모  we can improve this by noting that the difference vector is uniformly distributed over . bucket sort is ideally suited for input data that is uniformly distributed between 1 and 1  and has only linear average-case complexity. using bucket sort  the average-case complexity of the interpolation algorithm is . we note the important fact that the complexity of interpolation does not depend on the size of the grid.
모we store the grid using a hash table. our hash function uses lexicographical ordering of the integer vectors in to map each vector in to a unique integer from to	. the hash function is perfect if the size of the hash table is equal to or greater than	  which is the total number of possible grid points.  the hash function assumes a maximum resolution  .  because the number explodes quickly as and
   increase  it is usually impossible to have a hash table this large. we resolve collisions by associating a linked list with each slot  and use the hash code to uniquely identify the grid point in the list.
1	a variable-resolution regular grid
the disadvantage of a fixed-resolution grid is that increasing the resolution of the value function approximation in one region of the belief simplex requires increasing its resolution everywhere. this causes an exponential explosion in the size of the grid  and makes this approach infeasible for all but small problems. to address this problem  lovejoy  1  suggested a variable-resolution generalization of his method that would  maintain the simplicity of the freudenthal triangulation  but allow the mesh to vary on different portions of .  however  neither lovejoy nor anyone else developed this extension.
모in the rest of this paper  we develop this variable-resolution generalization of lovejoy's regular grid. we discuss how to generalize the freudenthal interpolation and how to select the appropriate resolution for different regions of the simplex. in allowing the resolution of the grid to vary  we adopt the rule that the resolution is always a positive integer power of 1. this ensures that vertices of low-resolution sub-simplices are also vertices of higher-resolution sub-simplices  and still useful when resolution is increased. from now on  we let denote the highest resolution anywhere in the grid.
1	interpolation
smallest complete sub-simplex to perform interpolation for belief state   we search for the smallest  i.e.  highestresolution  sub-simplex containing that is complete  i.e.  all its vertices are in the grid . the lowest possible resolution of the grid is 1  and the highest possible resolution    is given.  we must know in order to create the hash function for the hash table used to store the grid.  for any resolution  the algorithm summarized in section 1 identifies the vertices of the sub-simplex containing   and we can check whether all of these are in the grid. thus  we can use binary search to find the resolution between 1 and with the smallest complete sub-simplex containing . because the grid always includes the corner points of the belief simplex  the search is guaranteed to find a complete sub-simplex at some resolution. the complexity of this interpolation algorithm is   where the factor reflects the complexity of binary search and the fact that only resolutions that are powers of two must be considered.
virtual points each point in a regular grid is a vertex in many different sub-simplices. in a variable-resolution grid in which not every part of the grid is refined to the same resolution  this means that there will be many sub-simplices for which some but not all of their vertices are grid points. the smallest complete algorithm considers sub-simplices containing at different resolutions  until it finds the smallest complete one. in its search  it always considers the sub-simplex at the next higher resolution. even though this sub-simplex is not complete  many of its vertices may be present in the grid. it can be advantageous to use these higher-resolution grid points in interpolation  and it is possible to do so by  filling in  the missing vertices with what we call virtual vertices. any vertex that is missing from a sub-simplex is a belief state for which we can create an upper bound value by performing interpolation  since the missing vertex is contained in a lowerresolution sub-simplex. we call this computed value a virtual vertex. by using virtual vertices to fill in the missing vertices of the incomplete sub-simplex at the next higher resolution after the smallest complete one  we can improve the interpolated value. the cost for this improvement is an increase in the worst-case complexity of interpolation by a factor of compared to the smallest complete algorithm  since in the worst case every vertex may be missing from a sub-simplex . but this complexity factor can be improved by not creating virtual vertices for an incomplete sub-simplex with more than maxvirtual vertices missing. this limits the increase in worst-case complexity to a factor of maxvirtual . in practice  we found that another technique significantly reduces the average-case complexity of virtual interpolation without adversely affecting the result. for an incomplete subsimplex  let sum denote the sum of the barycentric coordinates for vertices that are present.  note that sum for an incomplete sub-simplex.  we don't perform virtual interpolation for a sub-simplex for which sum threshold. adjusting this threshold lets us adjust a time-quality tradeoff for this method of interpolation. we report experimental results for different threshold values in section 1.
1	refining the grid
we now discuss the question of how to refine the grid  that is  how to selectively adjust the resolution of the grid in different regions of the belief simplex. a natural strategy is to do so in a way that reduces the error of the approximation. this requires a method for estimating the error. since our gridbased approximation is an upper bound on the optimal value function  the error can be estimated by measuring its distance from a lower-bound function.
모lovejoy  1  discusses how to use a grid to compute a lower-bound function  as well as an upper-bound function. the lower-bound function is represented by a set of
	-vectors 	  with the value of be-
lief state	computed as follows 
one vector is associated with each grid belief state  and is computed by value iteration using the following update rule.
1. for each action	and observation	  let
1. for each action	  let
1. let	for	.
the complexity of performing this update for every grid belief state is . although value iteration using this update is guaranteed to produce a value function that is a lower bound on the optimal value function  it may not converge to a fixed point. hauskrecht  1  1  and zhang et al.  1  propose refinements of this update that guarantee convergence  but they fail to bound the size of or the complexity of the update. so we do not use their refinements.
모given these upper and lower-bound grid-based value functions  lovejoy describes an algorithm that computes a uniform error bound  which is the maximum difference between the upper and lower-bound functions for anybelief state . unfortunately  it has complexity   and our experience indicates that it is intractable for problems with more than nine or ten states. however  lovejoy notes that it is easy to compute an approximate error bound  defined as the maximum difference between the upper and lower-bound functions for any grid belief state . this is guaranteed to be tighter than the uniform bound  and we can use it to refine the grid.
모once we have identified the grid point s  with the largest error  what belief states should we add to the grid in order to reduce the error at these points  the value of a grid belief state depends on the interpolated value of its successor belief states  so refining the grid at the successor belief states will reduce the error at the grid point. for a successor belief state   we increase the resolution of the grid by adding the vertices of the -resolution sub-simplex containing to the grid  where is the resolution of the smallest complete subsimplex containing in the current grid. this adds at most points to the grid  which is the maximum number of missing vertices in the higher-resolution sub-simplex.
모this first approach to refining the grid tries to improve the grid-based approximation everywhere  without considering reachability from a start state. if the starting belief state for a pomdp is given  a policy can be found using forward search  satia & lave  1; washington  1; hansen  1; hauskrecht  1 . this focuses computation on regions of the belief simplex that are reachable from the starting belief state. upper and lower-bound functions can be used to prune branches of the search tree  as well as to compute an error bound for the starting belief state  by backing-up upper and lower-bound values from the leaves of the search tree to the root . this suggests a more focused approach to refining grid-based upper and lower-bound functions: attempt to reduce the error bound of the belief state at the root of the search tree. this can be done by refining the grid for belief states on the fringe of the search tree that contribute most to the value of the starting belief state.
모although we choose to focus on grid-refinement strategies that attempt to reduce the error bound of the approximation  other strategies are possible. for example  given upper and lower-bound value functions  it is sometimes possible to identify the optimal action for a belief state using action elimination. it may be desirable to refine the grid at points where the optimal action is uncertain  and by the same reasoning  unnecessary to refine it at points where the optimal action can be determined  regardless of the error bound at these points. hauskrecht  1  1  and brafman  1  suggest other heuristics for choosing points to add to a  non-regular  grid. these heuristics consider such factors as reachability  likely improvement of corner points of belief simplex  and likely effect on the policy  and can be viewed as complementary to our strategy of reducing an error bound.
1	comparison to non-regular grids
a variable-resolution regular grid is not the only way to allow the resolution of a grid to vary in different regions of the belief simplex. another approach is a non-regular grid that allows the points of the grid to occur anywhere in the belief simplex  and not necessarily in a regular pattern that allows for triangulation. hauskrecht  1  1  and brafman  1  propose non-regular grids  and we now compare this approach to our variable-resolution regular grid.
모a drawback of non-regular grids is that interpolation is more difficult. hauskrecht proposes an interpolation algorithm that creates a convexcombination using one point in the grid and corner points of the belief simplex. because it tests each point in the grid in order to select the one that gives the best interpolated value  its complexity is . brafman proposes an interpolation algorithm that relies on the grid belief states being sorted in decreasing order of entropy. given a belief state for which interpolation is to be performed  brafman's algorithm searches the list of grid belief states until a belief state is found that assigns positive probability only to states to which assigns positive probability. the maximum coefficient for which is calculated  and the process is repeated with instead of until interpolation is complete.  searching a list sorted by entropy has the effect of prefering low-information belief states that are likely to be closer to .  the algorithm has worst-case complexity   and requires an initial sort with complexity . although both interpolation algorithms are simple  their complexity depends on the size of the grid. this is a serious drawback that limits the size of non-regular grids in practice.
1	value iteration
although the complexityof interpolation in regular grids does not depend on the size of the grid  the complexity of value iteration in both regular and non-regular grids does. values for grid belief states are computed using value iteration  with values initialized to known upper bounds  such as their completely observable values . in a variable-resolution regular grid  the compexity of the first iteration of value iteration is
	  where	is the number of grid
points  is the number of successor belief states of each grid point  is the worst-case complexity of computing a successor belief state by bayesian conditioning  and reflects the added complexity of interpolation. because both successor belief states and barycentric coordinates used for interpolation can be cached for re-use  the complexity of subsequent iterations of value iteration is only   where is the complexity of computing an interpolated value using the cached barycentric coordinates. in practice  caching successor belief states and barycentric coordinates dramatically improves the running time of value iteration.
모because the same successor belief states and barycentric coordinates are used in each iteration of value iteration  this grid-based approximation is a finite-state mdp and convergence to a fixed-point solution in polynomial time is guaranteed  hauskrecht  1 . in non-regular grids  the barycentric coordinates used for interpolation are re-computed each iteration. because the complexity of interpolation in nonregular grids depends on   the complexity of each iteration of value iteration in a non-regular grid is quadratic in . this underscores the advantage of using a regular grid.
모in addition to limiting the complexity of interpolation  it is helpful to limit the factor   which is the number of times interpolation  and other calculations involved in computing a backup  are performed in each iteration of value iteration. the obvious way to limit this factor is to limit the size of the grid. this adjusts a tradeoff between quality of approximation and grid size  and a variable-resolution grid lets us space the points of the grid to achieve the best possible approximation for a given grid size. action eliminations can be used to limit . as for   it may be reduced by ignoring some  or all  observations  and evaluating the successor belief states of actions. if grid-based interpolation is used to evaluate the successor belief states of actions  however  the gridbased approximation may no longer be an upper bound. the upper bound guarantee is lost because information provided by the observation is ignored.  the upper bound guarantee can be restored by using the corner points of the belief simplex to perform interpolation  since this is equivalent to assuming perfect information  but this impairs the quality of the approximation. if always done  the grid-based approximation becomes no better than the completely observed heuristic.  finally  we note that it is possible to improve a grid-based approximation without adding points to the grid. instead of performing a conventional backup using one-step lookahead  the value of a grid point can be improved further by using a deeper lookahead search that we call a multi-step backup. using branch-and-bound or ao* search  the upper-bound function can be used to prune the search tree and the depth of the lookahead can adjust a tradeoff between search complexity and improvement of of value. we report some experimental results for this technique in the next section.
1	performance results
we experimentally evaluate our grid-based approximation algorithm using several test problems from the literature. these include the machine maintenance problem that is lovejoy's  1  only test example; the four most-difficult-to-solve problems from a test set used by cassandra et al.  1   see table 1 ; the 1-state  1-action  1-observation gridworld navigation problem used as a test example by hauskrecht  1  1 ; and the 1-state  1-action  1-observation gridworld navigation problem introduced by littman et al.  1  and used as a test example by brafman  1 .
interpolation we first compare the time-quality tradeoff for different methods of interpolation in a variable-resolution regular grid. we use the five small test problems in table 1 for this comparison  so that we can also compute an optimal interpolation using linear programming. figure 1 shows the performance of interpolation using virtual points relative to performance using the smallest complete subsimplex method alone. we measure improvement as a percentage of the difference between the interpolated value using the smallest complete method and the optimal interpolated value. virtual point interpolation is tested with three different sum threshold values; 1  1  and 1. as expected  a smaller sum threshold achieves better quality at the cost of speed. for these problems  virtual point interpolation takes only slightly longer than the smallest complete sub-simplex method.  interpolation using linear programming runs an average of 1 times slower.  for problems with larger state spaces  virtual point interpolation takes relatively longer be-

figure 1: time-quality tradeoff for interpolation using virtual points with different sum thresholds. results are averaged over the five test problems in table 1.
fixed resolutionvariable resolutionproblemerrormub timelb timemub timelb timemachine maintenance11.11.1.11.1.1network monitoring11.11.1.11.1.1shuttle docking11.11.1.11.1.1navigation11.11.1.11.1.1aircraft identification11.11.1.11.1.1table 1: a comparison of fixed and variable-resolution grids for 1 small test problems  showing the grid size  as well as maximum resolution and time  in cpu seconds  used to compute upper and lower-bound grid approximations  in order to achieve the same error bound.cause its worst-case complexity is quadratic in the size of the state space instead of linear  but reasonable performance can still be achieved by adjusting the sum threshold. optimal interpolation using linear programming is infeasible for larger problems.
모interpolated values are typically close to optimal. there appear to be two reasons for sub-optimality. first  interpolation using a sub-simplex is less flexible than interpolation using a convex combination.  every sub-simplex is a convex combination  but not every convex combination is a subsimplex.  as a result  better interpolated values can sometimes be found using convex combinations that do not correspond to sub-simplices. the second reason is more problematic. although the interpolation function is piecewise linear and continuous  there is no guarantee that it is convex. for most problems  we found that it is convex  or nearly so . however  we sometimes found non-convexities. when they exist  the  nearest  grid points do not necessarily give the best interpolation. we found non-convexities in two of our test problems: the network monitoring problem from cassandra's test set and hauskrecht 1-state grid navigation problem. the large error bound for the network monitoring problem in table 1 reflects this. note that these non-convexities occur in both fixed-resolution and variable-resolution regular grids.
모one explanation for non-convexity is that  localized  transitions may prevent values from propagating throughout the grid. that is  non-convexities may occur when improved values in one area of the grid do not improve values in another area of the grid because there is not a chain of successor belief states between them through which they can propagate.

figure 1: performance of multi-step backups for hauskrecht's maze navigation problem. results are for a fixed-resolution regular grid with   which has 1 grid points.
for both test problems with non-convexities  we found that multi-step backups corrected the problem.  multi-step backups are described in the last paragraph of section 1.  instead of refining the grid at points with the largest error bound  we performed multi-step backups at these points. for the network monitoring problem  using multi-step backups with a depth bound of ten reduced the error bound from 1 to 1 after 1 cpu seconds. figure 1 shows the positive effect of multi-step backups on hauskrecht's 1-state maze problem.  for this problem  the upper bound score is defined as the average value of a fixed set of 1 randomly generated belief states plus the corner points  a metric hauskrecht uses.  for the other test problems  multistep backups do not improve the value function as much as grid refinement does. as a rule of thumb  when the error bound for a problem remains large despite grid refinement  multi-step backups may correct the problem.
refining the grid table 1 compares the performance of a fixed-resolution grid and a variable-resolution grid on five small test problems. each problem is solved using a fixedresolution regular grid  and the error bound is measured. then a variable-resolution grid is used to find a solution with the same error bound. we use virtual interpolation with sum = 1  for the variable-resolution grid. the results show that a variable-resolution approximation can achieve the same accuracy with a much smaller grid.
모figure 1 shows how the error bound decreases as the size of grid increases for the aircraft identification problem. re-

figure 1: decrease in error in a variable-resolution regular grid  as function of grid size. results are for aircraft identification problem.

figure 1: upper bound value as a function of grid size. results are for 1-state hallway navigation problem.

figure 1: average interpolation time as a function of grid size. results are for 1-state hallway navigation problem.
sults are similar for other problems. using the error bound to determine where to refine the grid requires a lower bound function  but lovejoy's grid-based lower bound function can be time-consuming to compute. for the 1-state hallway navigation problem  computing an -vector for each and every grid point is prohibitive. but it is not necessary. in our experiments with the hallway problem  we got good results using a lower-bound function consisting of only ten -vectors. each iteration  we computed an -vector for the ten grid points with the largest error bound.
comparison to non-regular grids figure 1 shows how the quality of the upper bound function improves as the size of the grid increases for the 1-state hallway navigation problem. it also shows that a better upper bound function can be computed using a variable-resolution regular grid than using a non-regular grid. the reason for this is that faster interpolation makes it possible to use a much larger grid. figure 1 shows average interpolation time as a function of grid size for our regular grid  compared to brafman's and hauskrecht's non-regular grids  and dramatically underscores the advantage of regular grids. figure 1 compares the time it takes value iteration to converge for each of these methods  as a

figure 1: time for value iteration to converge as a function of grid size. results are for 1-state hallway navigation problem.

figure 1: increase in value iteration time in a fixed-resolution regular grid with m = 1  as a function of state space size.
function of grid size. although the variable-resolution regular grid is much larger  it takes less time to compute. brafman's value iteration algorithm is faster than hauskrecht's  even though his interpolation algorithm is slower   because it disregards observations and only considers the successor belief states of actions. even with this simplification  value iteration in brafman's non-regular grid is slower than value iteration in a regular grid. moreover  as noted earlier  ignoring observations means that his grid-based approximation is not guaranteed to be an upper bound  although the values it computes for this problem are  in fact  upper bounds .
scalability although a variable-resolution regular grid allows much larger grids  and thus better approximations   the size of the state space remains a problem in scaling-up this approach. figure 1 compares the time it takes to compute a regular grid for a succession of pomdp problems with increasing state space size.  all problems have 1 actions and 1 observations.  besides confirming that the size of the state space is the primary factor that limits the scalability of grid-based approximation  this gives some idea of the range of problems for which grid-based approximation is currently feasible. we have found that almost all of the running time of value iteration is spent performing interpolation or computing successor belief states using bayesian conditioning  and the complexity of both depends on the size of the state space. this points to where further work on grid-based approximation is needed. in order to improvethe efficiency of interpolation and bayesian conditioning for problems with larger state spaces  we need to integrate grid-based approximation with various techniques for state abstraction that have been explored for mdps and pomdps  boutilier et al.  1 .  state abstraction would also make it possible to compute lovejoy's grid-based lower-bound function more efficiently. 
1	discussion
pomdps are notoriously difficult to solve  and finding even a bounded-optimal solution is an intractable problem  lusena et al.  1 . grid-based approximation algorithms have polynomial-time complexity in the size of the grid and the number of states  actions  and observations  hauskrecht  1 . although there is no guarantee on the quality of the approximation that can be achieved in polynomial time  a well-designed grid-based approximation can be adjusted in several ways to optimize a time-quality tradeoff.
모this paper introduces a variable-resolution regular grid that allows the resolution of the grid to be adjusted in different regions of the belief simplex  in order to approximate the contours of the value function as efficiently as possible. although non-regular grids also allow this  interpolation in nonregular grids is more difficult and its complexity is a function of the size of the grid. in a variable-resolution regular grid  the complexity of interpolation is independent of the size of the grid. thus  it is feasible to use much larger grids with this approach in order to achieve better approximations.
모our experiments indicate that it is easier to scale up this approach to large grids than to large state spaces  and the size of the state space remains the most prohibitive factor in the complexity of grid-based approximation. to use grid-based approximation to solve pomdps with large state spaces  we need to integrate it with various techniques for state abstraction that have been explored for mdps and pomdps. this is a promising direction for future research.
acknowledgments
we thank tony cassandra and milos hauskrecht for making available their test problems  and the anonymous reviewers for helpful comments. this research is supported in part by the national science foundation under grant iis-1.
references
 boutilier et al.  1  boutilier  c.; t. dean; and s. hanks. 1. decision-theoretic planning: structural assumptions and computational leverage. journal of artificial intelligence research 1: 1.
 brafman  1  brafman  r. 1. a heuristic variable grid solution method for pomdps. in proceedings of the fourteenth national conference on artificial intelligence  aaai-1   1. providence  ri.
 cassandra et al.  1  cassandra  a.; littman  m.; and zhang  n. 1. incremental pruning: a simple  fast  exact algorithm for partially observable markov decision processes. in proceedings of the thirteenth annual conference on uncertainty in artificial intelligence  1.
 drake  1  drake  a. 1. observation of a markovprocess through a noisy channel. ph.d. thesis  electrical engineering department  m.i.t.  cambridge  ma.
 eckles  1  eckles  j. 1. optimum replacement of stochastically failing systems. ph.d. thesis  department of engineering-economic systems  stanford university.
 hansen  1  hansen  e. 1. solving pomdps by searching in policy space. in proceedings of the fourteenth conference on uncertainty in artificial intelligence  uai1   1. madison  wi.
 hauskrecht  1  hauskrecht  m. 1. incremental methods for computing bounds in partially observable markov decision processes. in proceedings of the fourteenth national conference on artificial intelligence  aaai-1   1. providence  ri.
 hauskrecht  1  hauskrecht  m. 1. value-function approximations for partially observable markov decision processes. journal of artificial intelligence research 1-1.
 kakalik  1  kakalik  j. 1. optimum policies for partially observable markov systems. technical report 1  operations research center  massachusetts institute of technology  cambridge  ma.
 littman et al.  1  littman  m.; a. cassandra; and l. kaelbling. 1. learning policies for partially observable environments: scaling up. in proceedings of the twelth international conference on machine learning  1.
 lovejoy  1  lovejoy  w. 1. computationally feasible bounds for partially observed markov decision processes. operations research 1-1.
 lusena et al.  1  lusena  c.; j. goldsmith; and m. mundhenk. 1. nonapproximability results for markov decision processes. technical report  computer science department  university of kentucky.
 munos & moore  1  munos  r. and a. moore. 1.
variable resolution discretization for high accuracy solutions of optimal control problems. in proceedings of 1th international joint conference on artificial intelligence.
 satia & lave  1  satia  j. and lave  r. 1. markovian decision processes with probabilistic observation of states. management science 1 :1.
 washington  1  washington  r. 1. bi-pomdp: bounded  incremental partially-observable markov-model planning. in proceedings of the fourth european conference on planning  ecp-1 .
 zhang et al.  1  zhang  n.; s. lee; and w. zhang. 1. a method for speeding up value iteration in partially observable markov decision processes. in proceedings of the 1th conference on uncertainty in artificial intelligence.

uncertainty and probabilistic reasoning
uncertainty

weakening the commensurability hypothesis in possibilistic qualitative decision theory
adriana zapico
departamento de computacio뫣n
universidad nacional de r뫣 o cuarto
campus de la u.n.r.c 1 r뫣 o cuarto
argentina zapico dc.exa.unrc.edu.ar

abstract
models for qualitative decision under uncertainty assuming that uncertainty is of possibilistic nature have been proposed in 1's. in them  as in the classical approach of expected utility theory  two alternatives emerge: a` la von neumann and morgenstern  vnm  or a` la savage. in the first one  uncertainty and preferences on consequences are measured on finite scales. decisions are ranked in terms of the ranking induced  on their associated possibility distributions on consequences  by qualitative utility functions. to define these utilities  an hypothesis of commensurability  i.e.
the existence of an onto order-preserving mapping linking both scales of uncertainty and preference  is assumed. this hypothesis forces us to restrict to decision problems where the cardinality of the uncertainty values set is greater than or equal to the cardinality of the preference set. this point has been attacked in the possibilistic a` la savage approach  but as far as we know  it is an open question in the qualitative vnm approach. as a first step to weaken the commensurability hypothesis in this model  the preference orderings resulting of applying the qualitative criteria without requiring to be onto are characterized. these orderings may be not continuous  but they satisfy a relaxed continuity axiom.
1	introduction
the basic references in classical decision theory characterizing preference relations under uncertainty and the rationality hypothesis are von neumann and morgenstern's expected utility theory eut   and the version of savage . both approaches assume that uncertainty is represented by probability distributions. von neumann and morgenstern assume a probability distribution encoding uncertainty on situations. then  as each decision induces a probability distribution on the set of consequences   each decision is identified with its associated probability distribution on . for sorting decisions  associated distributions are ranked in terms of their expected value with respect to decision maker's  dm's for short  preferences on consequences. on the other hand  savage  proposes a somewhat different framework for eut  axiomatically characterizing the preference relation on acts of decision makers that behave as eut agents.
모the classical axiomatic frameworks of utility theory have actually been questioned rather early  e.g.  allais  1; ellsberg  1  . another problem with eut is that it needs numerical probabilities for each state and numerical utilities for all possible consequences  but this assumption may be too strong if there is only incomplete or poor available information. as doyle and thomason  comment in a recent paper  there are many experiences showing that usually people explain and make their decisions with partial  generic and  uncertain  information. hence  a qualitative approach may give tools for representing this decision making behaviour. they summarize main proposals on qualitative decision theory. among them  we find those models that use possibility theory as uncertainty formalism in which  analogously to classical eut  two alternatives emerge: a la von neumann and morgenstern`	  initiated by dubois and prade in and latter developedand extended in  dubois et al.  1; zapico  1   and a la savage`   this proposal is summarized in  dubois et al.  1; 1 .
모here  we are interested in the possibilistic qualitative counterpart to vnm's eut. in this approach  both dm's preferences on consequences and uncertainty are graded on finite ordinal sets  i.e. scales equipped with the maximum  minimum and an order-reversing operations  that are commensurate. as each decision is linked with its associated possibility distribution on consequences  then for ranking decisions the associated distributions are ranked by qualitative utility functions -expressed in terms of maximum  minimum and reversing operations. to define these utilities an hypothesis of commensurability  i.e. the existence of an onto order-preserving mapping linking both scales of uncertainty and preference  is assumed. this forces us to restrict to decision problems where the cardinality of the uncertainty set is greater than or equal to the cardinality of the preference set  unfortunately in several problems this cardinality relation is not satisfied.
모the commensurability hypothesis has been a point of interest for researchers in the possibilistic a` la savage approach  e.g.  fargier and perny  1   but as far as we know  it has not been attacked in the qualitative von neumann and morgenstern style. here  we propose to weaken the commensurabilityhypothesisfor this approachby not requiring the linking mapping scales involved to be onto. that is  we will characterize the preference ordering resulting of applying these qualitative criteria only requiring to be an order-preserving mapping  in particular we show that not requiring to be onto results in a relaxation of the continuity property of the preference relations on distributions. on the other way  replacing in the axiomatic framework the usual continuity axiom by the relaxed one is not enough for guaranteeing the relation is representable by the qualitative criteria  an axiom establishing how some special mixtures are handled is required.
모this weakening of the commensurability hypothesis will allow us to deal with other types of problems  in particular  those in which the cardinalityof the preferencevalues set may be greater than the cardinality of the uncertainty set.
모in the next section we summarize the possibilistic qualitative decision theory  pqdt . our proposal on weakening the commensurability hypothesis is introduced in section 1  including the representation theorem for characterizing the preference relations representable by these  more general  utility functions. the respective extension of the model for involving non-normalized distributions are considered in it as well. finally  in last section we include some concluding remarks.
1	background on pqdt
let us define the pqdt framework. denotes a finite linear plausibility scale  where and is a finite set of consequences  and denotes the set of consistent possibility distributions on over   i.e.
:
for the sake of simplicity  we shall use for denoting both a subset and the normalized possibility distribution on such that if and otherwise.
hence  we can consider	as included in	.
모the so-called possibilistic mixture  possibilistic lottery   the qualitative counterpart of the probabilistic lottery  is an operation defined on that combines two possibility distributions and into a new one  denoted     with and defined as:
with this definition  and taking into account the properties of maximum and minimum  the  reduction of lotteries  holds 
i.e.
모모is a finite linearly ordered scale of preference or utility   with and is the reversing involution in . as usual  we assume as working hypothesis the existence of a preference function representing dm's preference on consequences  i.e. there exists a function : that assigns a preference level of to each consequence of such that if and only if is at least as preferred as let : be an order preserving function relating and such that
모in such a framework  assuming also that is onto  dubois et al.  have characterized the preference relations induced by the utility functions
where . this criterion requires to make  at least to some extent  all plausible consequences to be preferred ones1. since in some problems this criterion may be too conservative  it may be interesting to consider an optimistic behaviour  like requiring to make at least one of the good consequences highly plausible- at least to some extent. with this goal  dubois et al.  considered the utility function:
which is dual to	. notice that	restricted to
coincides with
모dubois et al. have shown that1 has an interesting property: preserves the possibilistic mixture  in the sense that it holds that:
모they also provide the following set of axioms for preference relations on   with the max-min possibilistic mixture as the internal operation on   to characterize the ordering induced by    . is a total pre-order reflexive transitive and total .
this axiom guarantees the existence of maximal and of minimal elements on	.	we denote by  and  a maximal and a minimal ones respectively. for each	  let		 and		 .
if
	s.t.	.
establishes that distributions are preferentially equivalent
to having a	-level of uncertainty with respect to 
theorem 1 a preference relation satisfies axioms if  and only if  there exist a finite linearly ordered utility scale   with
and	  a preference function	s.t. an onto order-preserving function	  in such a way that	iff
for characterizing the optimistic behaviour representable by
  it is necessary to change the uncertainty aversion
axiom	by an uncertainty-prone postulate:
	: if	then
and to adequate the continuity axiom	replacing it with:
s.t.
모in the next section  the proposal for weakening the commensurability hypothesis is introduced.
1	a first approach with a weaker commensurability hypothesis
let us remark that the difference with the works developed previously in  dubois et al.  1; 1  is that now is not required to be onto. that is  given   an orderpreserving mapping such that   for any
consider the qualitative utility functions
where	observe that	and
restricted to	coincide with the preference function
it is interesting to notice the following properties.
lemma 1 these functions still preserve the mixture:
in particular  we have:
corollary 1 it remains true thatis themedian of three terms including
1. if	then
1. if	theni.e.1	the roles of the onto condition and of the continuity axiom
let us observe that the onto condition of is a basic requirement involved in the characterization of the relations  since the fact of allowing to be a non-onto mapping results in that the continuity axiom may not be true.
example: for instances  consider consequences are either totally possible or impossible  i.e. while preferences are measured on   being the set of consequences   with dm's
preference on consequences		
.	since	and	  it is obvious that
that is  the ordering
induced by coincides with the maximin criterion while the ordering induced by coincides with the maximax one. observe that for there is no such that is preferentially equivalent to
hence  it is clear that the continuity property for relations on may be lost if is non-onto. for facing this point  a relaxed continuity axiom is proposed. the underlying idea is to relax the continuity of the preference in the sense that there exists a subset on such that either the distributions are preferentially equivalent to individual consequences on this subset  or  the distributions are preferentiallyequivalentto having a -level of uncertainty with respect to  as usual.
모on the other way  having a relation satisfying axioms and the relaxed continuity one  see in section 1  is not enough for guaranteeing is representable by as it is shown following.
example:	consider the following framework:


and consider the  reflexive and transitive  relation	s.t.  also satisfying

all other distributions are taken equivalent to  this relation satisfies axioms and . but it is not representable by   since although    we have	 .
1	axiomatic setting proposed
the above discussion has led us to propose this new set of axioms for preference relations on   with the max-min mixture as the internal operation: is a total pre-order .
: if
모모모모모모모: there exists a subset1s.t. all maximal and all minimal elements
ofare in the complement of  andeithers.t.
:ors.t.1. ifthenif if ifor	1. if	then
	if	or
if if
모this axiom establishes how the relation handles  reduces  lotteries involving consequences that are not preferentially equivalent to binary lotteries of  and . now  we introduce some results that will be useful latter.
lemma 1 axioms a1  a1  a1   a1rc and axmix imply : if then there is s.t.
dubois et al.  1  lemma 1  have shown:
lemma 1 if	verifies axioms	and	then:


 and  are also the minimal and maximal elements of
1	representation of pessimistic qualitative utilities
next  we show that the preference ordering on
induced by the qualitative/ordinal pessimistic utility satisfies the above set of axioms.
lemma 1
letbe the preference ordering oninducedby	i.e. 	iff
satisfies axioms setthenproof: is easily verified  is a consequence of maximum and minimum being non-decreasing functions.
results from the fact that preserves max-min possibilistic mixtures  while is not difficult to verify taking into account lemma 1. now  we check axiom
모모모. if is onto  and reduces to hence  we are in the case detailed in section 1. if is non-onto  let	as
모모모모모  and	and	if	is a maximal or a minimal element of	then	we have to prove that	either	s.t. or	s.t.
by definition of	  s.t.
hence 
if	then with	.
otherwise 	now  either or not. in the first option 	s.t.
while in the second one 
and
모now  it is shown in the following representation theorem that the preference orderings satisfying	can always be represented by a pessimistic qualitative utility
theorem 1 a preference relation on satisfies axiom set if  and only if  there exist
 i  a finite linearly ordered utility scale	with and
 ii  a preference function	:	s.t.
 iii  an order-preserving1 function	:	s.t. and
in such a way that	iff
proof: the  if  part corresponds to the preceding lemma. as for the  only if  part  we go on structuring the proof  in the following three steps:
1. the utility scale and an order-preserving function from to are defined.
1. a function	:	representing	i.e. s.t.
	iff	is introduced.
1. the preference function	:	is the restriction of to	and	 	being the reversing
	involution on	it is proved that
now  we develop these steps.
1. as usual  stratifies in a linearly ordered set of classes of equivalently preferred distributions  
   iff we define with the natural  linear  order iff denote by 1 and 1 the maximum and minimum elements of    and 
define the order reversing function : as guarantees that reverses the order. let be the reversing involutionin the linking-scales mapping is defined as
1. now  we define	in three steps.
 a  for all
 b  secondly 	let
	it is easy to check that	iff
and analogously  that restricted to
	distributions of type	represents
 c  finally  since	guarantees that	either
	s.t.	or	s.t.	we define
if	s.t. if	s.t.
notice that  because of	 	is well defined.
1. now  : is defined as notice that  and  it remains to prove that
         =	with this goal  we will prove the following equalities:
	by	there are several alternatives for
 a  s.t.		 and
 b  s.t.	and
 c  s.t.	and
 d  s.t.	and
it is not difficult to verifythat applying and reducing lotteries  in each case we obtain that
in particular  we have that
this may be easy generalized to
now  we verify
	as	is normalized 	s.t.
	without	loss	of	generality	we	assume
then  let
	since	we	have:
1
this ends the proof of the theorem.
1	representation of optimistic qualitative utilities
for modeling an optimistic dm's behaviour  we consider the
axioms set	=	 
as usual being		   with
	: if	then
       : there exists a subset1	s.t. all maximal and all minimal elements of	are in its complement  and	either	s.t. or	s.t.
:
1. if	then
	if	or
if if
1. if	then
	if	or
if if

1
note thatso1
if  then axiomis recovered.모we have analogous results to lemmas 1 and 1  but for spaces reasons they are omitted here. the respective representation theorem of optimistic utility is:
theorem 1 a preference relation on satisfies axioms set if  and only if  there exist
 i  a finite linearly ordered utility scale	with and
 ii  a preference function	:	s.t.
 iii  an order preserving function	:	such that
and
in such a way that	iff
1	a particular case: maximin and maximax decision criteria
let us consider again the simplest scale of uncertainty  that is  consequences can be either fully possible or fully impossible. this is a very particular case since for any preference scale the only requirement to be fulfilled by a mapping : is that and in this framework is just the power set and the resulting utility functionals are
and
leading to the well-known maximin and maximax decision models. now  it is very easy to check that  in order to fully characterize a preference relation on induced by these and the above axioms simplify to these ones:
	:	is a total pre-order 
	: if	then
	: if	then
	:	s.t.
	: if	then
actually  in this setting  axiom is redundant since it is a logical consequence of the remaining axioms. moreover  as we are working with a finite set is a consequence of the axiomatic frameworks a` la savage of these maximax and maximin criteria are provided in  brafman and tennenholtz  1; 1 .
모dubois et al.  shown the necessity of extending the model to non-normalized possibility distributions  because of case-based decision-making problems or to be able of facing problems where different  partially inconsistent  information sources about the situation are available etc. to include these problems  they extended the model summarized in section 1. analogously  we extend our above proposal and provide corresponding characterizations of the orderings induced by suitably modified utility functions.
1	utilities for non-normalized distributions
now  we consider the set of non-necessarily normalized distributions on        with values on the finite uncertainty scale  keeping the usual definition of possibilistic mixture  as the working set of possibilistic lotteries. the utility functionals are extended to evaluate nonnormalized distributions as well. indeed  given a mapping
:	s.t.	 	 :


where is the height of the distribution   and is a normalized version of
defined as if   and as   otherwise. notice that when the original expression is retrieved.
to characterize the preference orderings induced by

in	 	  we extend	and	respectively  with the usual  dubois et al.  1; godo and zapico  1  additional axiom:
:
obtaining the following representation theorems.
theorem 1 a preference relation	on	 	  satisfies
resp.
if  and only if  there exist
 i  a linearly ordered and finite preference scale with	and
 ii  a preference function	:	s.t. u	 1 	u	 1  
 iii  an order-preserving mapping	:	and
in such a way that	iff

	 	iff	respectively .

1	conclusions and future works
as it has been mentioned  to weaken the onto condition of the linking-scales mapping allows us to work with decisionmaking problems where the cardinality of the uncertainty values set may be not greater than or equal to the cardinality of the preference set.
모it has been shown that this onto condition in the commensurability hypothesis is directly related with the continuity property of the preference relation on .
for characterizing those preference relations representable by the qualitative utility functions where is not required to be onto  we have proposed to include a relaxed continuity axiom and an axiom for handling some special mixtures.
	it is true  that given	and	non-onto  we may extend
to some and define   with onto  in a such a way that	. but the
point  for us  is that while the ordering induced by in satisfies continuity  the ordering induced by in the original distributions set may not satisfy continuity.
모this is a first approach to the goal of weakening commensurability. a next step is to extend the analysis to the general utility functions proposed by dubois et al. . however  this extension seems non-trivial since the onto conditions is required to guarantee the well-definedness of these generalized utility functions. another point is to consider the utilities when the sets values of uncertainty and preferences are non-linear lattices  zapico  1 .
acknowledgments
the author wishes to thank the valuable comments of llu뫣 s godo and of the anonymous referees .
references
 allais  1  m. allais. le comportement de l'homme rationneldevantle risque: critiquedes postulats et axiomes de l' e뫣cole am뫣ericaine. econometrica  1 :1 1.
 brafman and tennenholtz  1  r.i. brafman and m. tennenholtz. on the foundations of qualitative decision criteria. in 1th nat. conf. on a.i. aaai'1   1.
 brafman and tennenholtz  1  r.i. brafman and m. tennenholtz.on the axiomatization of qualitative decision criteria. in 1th nat. conf. on a.i. aaai'1  1  1.
 doyle and thomason  1  j. doyle and r. thomason. background to qualitative decision theory. ai magazine  1 :1  1.
 dubois and prade  1  d. dubois and h. prade. possibility theory as a basis for qdt. in 1th int. joint conf. on a.i.  ijcai'1   1  1.
 dubois et al.  1  d. dubois  h. prade  and r. sabbadin. decision under qualitative uncertainty with sugeno integrals: an axiomatic approach. in 1th int. fuzzy syst. assoc. cong.  ifsa'1   vol. i  1  1.
 dubois et al.  1  d. dubois  h. prade  and r. sabbadin. qdt with sugeno integrals. in 1th conf. on uncertainty in a.i.  uai'1   1  1.
 dubois et al.  1  d. dubois  l.godo  h. prade  and a. zapico. on the possibilistic-based decision model:from decision under uncertainty to case-based decision. int. j. of uncert.  fuzz. and know-based syst.  1 : 1  1. short version in:making decision in a qualitative setting:from decision under uncertainty to case-based decision. in 1th int. conf. on principles of know. repres and reasoning  kr'1  1 - 1  1.
 godo and zapico  1  l.godo and a.zapico.	on the
possibilistic-based decision model: characterisation of preferences relations under partial inconsistency. in applied intelligence 1 :1  1.
 ellsberg  1  d. ellsberg. risk  ambiguityand the savage axioms. quarterly j. of economics  1-1  1.
 fargier and perny  1  h. fargier and p. perny.
qualitative models for decision under uncertainty without the commensurability assumption. in 1th conf. on uncertainty in a.i.  uai'1   1  1.
 savage  1  l. j. savage. the foundations of statistics. dover  new york  1.
 von neumann and morgenstern  1  j. von neumann and o. morgenstern. theory of games and economic
behavior. princeton univ. press  1.
 zapico  1  a. zapico. axiomatic foundations for qualitative/ordinal decisions with partial preferences. in 1th int. joint conf. on a.i.  ijcai'1   1.
a fuzzy modal logic for belief functions
llu뫣 s godo
ai research institute
iiia - csic
1 bellaterra  spain godo iiia.csic.espetr hajek뫣
institute of computer science
academy of sciences 
1 prague  czech republic
hajek cs.cas.czfrancesc esteva
ai research institute
iiia - csic
1 bellaterra  spain esteva iiia.csic.esabstract
in this paper we introduce a new logical approach to reason explicitly about dempster-shafer belief functions. we adopt the following view: one just starts with boolean formulas and a belief function on them; the belief of is taken to be the truth degree of the  fuzzy  proposition standing for   is believed . for our complete axiomatization  hylbert-style  we use one of the possible definitions of belief  namely as probability of  modal  necessity. this enables us to define a logical system combining the modal logic s1 with an already proposed fuzzy logic approach to reason about probabilities. in particular  our fuzzy logic is the logic    which puts lukasiewicz and product logics together.
1	introduction
reasoning under uncertainty is a key issue in many areas of artificial intelligence. from a logical view point  uncertainty basically concerns formulas  describing a situation  that can be either true or false  but their truth value is unknown due to incompleteness of the available information. when this uncertainty can be measured  then one may assign to propositions the degree of belief of being true. among the different models of numerical uncertainty  in the sense of belief  that have been proposed  probability theory is undoubtedly the most relevant  however other interesting models have been also proposed trying to remedy different shortcomings of the pure bayesian approaches. in particular  demspter-shafer belief functions  shafer  1  provide a more general framework to assess uncertainty to events or propositions  generalizing not only the probabilistic model but also for instance the possibilistic model. in this framework propositions can be attached two uncertainty degrees  a lower and an upper estimate  and the lack of evidence on the truth of a proposition does not necessarily induce evidence for the falsity of that proposition.
모one may find in the literature a number of uncertainty logics. as for probability  let us mention  keisler  1  for a deep investigation of probabilistic quantifiers  probability of is a fixed parameter  and  fagin et al.  1; halpern  1  for logics to reason explicitly about probabilities; see also  nilsson  1; bacchus  1  for other probabilistic logical formalisms. as for other types of belief  see e.g.  dubois et al.  1  for a full overview on possibilistic logic  and  fagin and halpern  1; saffiotti  1; sossai et al.  1  for different proposals of belief function logics. another type of approaches  that may be called comparative  deal with a binary modality   the formula meaning  belief of is belief of  ; this was adopted e.g. in  boutilier  1; bendova and ha뫣jek  1   possilibity theory  and  harmanec and ha뫣jek  1   dempster-shafer belief . let us remark that  as it is obvious  the above list of references of relevant works is far from being exhaustive  it is only for exemplifying purposes.
모it is of worth noticing that all the above uncertainty logics  and in many others  are built on the basis that the representation of situations is done by means of two-valued boolean  propositions: they can only be either true or false. however  when we need to deal with propositions representing gradual notions   e.g. high temperature  low income  it is natural to move from two-valued classical logic to a many-valued logical framework where propositions may be attached intermediate truth degrees. this is the realm of fuzzy logic1  which is completely different from the one of uncertainty logics  although in both propositions are usually attached with numbers between 1 and 1. it has been repeatedly stressed that truth degrees in fuzzy logic must be carefully and clearly distinguished from belief degrees  probabilities  or more generally belief degrees of dempster-shafer theory . fuzzy logics are logics of comparative truth and are mainly understood as truth-functional: the truth degree of a compound formula  conjunction  disjunction  implication ...  is determined by the truth degrees of its components via truth functions. on the other hand  uncertainty is basically concerned with degrees of belief that two-valued propositions are true. belief degrees are obviously not truth-functional. this does not prevent to find proposals to extend the various notions of uncertainty to fuzzy propositions and hence to find in the literature also fuzzy belief logics of various kinds  for instance let us mention the paper  ha뫣jek et al.  1  fuzzifying the comparative modality for possibilistic measures over a finitely valued  ukasiewicz logic .
모in this paper we introduce a new logical approach to reason explicitly about dempster-shafer belief functions which combines classical uncertainty measures  probability and belief functions  with elements of fuzzy logic: the basic observation is that  uncertainty  or belief is itself a gradual notion  e.g. a proposition may be totally  quite  more or less  or slightly certain  in the sense of probable  possible  believable  plausible  etc. . then  one just starts with boolean formulas and a probability on them; the probability of is taken to be the truth degree of the fuzzy proposition is probable . this was already used in  ha뫣jek et al.  1  and continued in  ha뫣jek  1; godo et al.  1  to define a fuzzy logic theory for probabilistic reasoning. here we generalize this approach to deal with belief functions  i.e. we take the following identity belief degree of = truth degree of  
as our working assumption  where stands for the fuzzy proposition   is believed . for getting a complete axiomatization of  being  highly  believed  we use one of possible definitions of dempster-shafer belief  namely as probability of necessity. this enables us to combine the approach of  godo et al.  1  with the modal logic s1. in particular  our fuzzy logic is the logic     putting  ukasiewicz and product logic together   which provides a nice and powerful logical setting where arithmetical operations are implicitly available.
모we are forced to have extensive preliminaries: section 1 contains preliminaries on belief functions and modal logic s1  section 1 preliminaries on the logic    and a variant of it suitable to handle infinite theories. section 1 is devoted to the definition of our fuzzy belief logic fb     for regular belief functions and main  completeness  results  while section 1 contains an approach to handle dempster combination rule in our fuzzy logic setting. finally  section 1 discusses a generalization to deal with general belief functions  i.e. belief functions with possibly positive. due to space limitation proofs are not included here but can be found in  godo et al.  1 .
1	preliminaries i
we survey here some basic facts on belief functions.  the basic monograph is  shafer  1 .  we are interested in belief functions on propositional formulas in finitely or countably infinitely many propositional variables. starting from the classical definition of dempster  using what we call dempster spaces  we explain a presentation using kripke models of modal logic as well as a definition using a superadditivity condition. all results of this section  except the last one to the best of our knowledge  are already known modulo possibly a different notation.
definition 1 a dempster space  or d-space  is a structure where is a mapping of into the power set of and is a finitely additive probability on an algebra of subsets of for
put	 if this set is measurable . is the belief function given by d. d is regular if for each	then	is a regular belief function
모needless to say  a function mapping  some  subsets of into is a belief function iff it is a belief function given by some d-space. note that it may be given by various d-spaces; only properties independent of the choice of the underlying d-space are of interest.
모let var be a set of propositional variables and let  value of a propositional variable in a world . the pair  or just is an evaluated d-space. for each formula built from our propositional variables is the truth value of in defined in the obvious way. the belief of is defined as
a function is a belief function on formulas if for some evaluated d-space actually we shall deal only with total belief functions on formulas  i.e. with those belief functions for which the sets of worlds are -measurable for any for-
mula	. evidently  if	and	are logically equivalent then
	  i.e.	respects logical equiva-
lence.
모now let us introduce some basic concepts of modal logic. formulas of modal propositional logic are built form propositional variables using connectives and the modality
모 necessarily ; if is a formula then is a formula. a kripke model for the logic  briefly a -kripke model  is a structure where is an equivalence
relation on	 reflexive  symmetric and transitive  and
is an evaluation of propositional variables as above.	the truth value of	is defined as follows:	if for each	 	implies	a formula	is an	-tautology if	for each	-kripke model and each	is true in each world
of each	-kripke model . the following are axioms of
axioms of classical propositional logic plus
 k 
 t   1 
 b 
where	is read  possibly	  and stands for	.
deduction rules are modus ponens and necessitation  from deduce is complete with respect to the given semantics; a formula is provable in iff it is a -tautology. a formula is without nested modalities if no box occurs in the scope of other box  box is applied only to box-free formulas . in each formula is logically equivalent to a formula without nested modalities. this  together with the known normal form theorem of propositional logic  gives the following important fact.
fact. if the set var of propositional variables is finite then there is a finite set of formulas such that each formula is  over logically equivalent to a formula from .
a	-probabilistic kripke model is a pair	  or just   where is a -kripke model and is a probabilistic  finitely additive  measure on an algebra of subsets of can be seen as an evaluated d-space
	where	 the
-class of	observe that then
모모모모모모모  or briefly   which tells us that the belief of is the probability of the necessity of .
모note that the belief function given by a -probabilistic kripke model is regular  thanks to reflexivity of . general  regular and non-regular  belief functions may be obtained e.g. from probabilistic kripke models of provability logic gl  see  ha뫣jek  1 . we shall come back to general belief functions later in section 1.
모now assume be finite non-empty. a basic belief assignment on is a mapping such that is regular if the belief
function given by	is defined by
this is indeed a belief function; moreover  given an evaluation   the corresponding belief function on formulas 	  is regular iff it is given by a -probabilistic kripke model; namely by a model where
	 1 	 
 1 
 1  iff	and
 1  is any probability on	such that
this construction is from  harmanec et al.   . conversely  each belief function on a finite set is given by a basic belief assignment defined as follows  shafer  1 :
note that if var  the set of propositional variables  is finite and is a belief function on the formulas built from var  then we may assume that is given by a d-space with  the set of all -tuples of zeros and ones . then if regular  is given by a finite -probabilistic kripke model.
모let be an evaluated d-space. for each and each tuple of formulas   the belief function given by satisfies the inequality  see shafer  1  :
we shall call it the generalized super-additivity condition  gsa for short  since it reduces to the usual super-additivity condition for and when and are mutually exclusive. moreover  if is finite and is a function assigning numbers from to formulas such that
 1 
 1 
    1  respects logical equivalence  and
    1  satisfies the gsa condition  then is a regular belief function and the formula above defines a regular basic belief assignment defining consequently 	is given by a finite	-probabilistic kripke model.
모our last aim in this section is to prove that this result can be extended to the case of var being countable.
theorem 1 if is countable and is a function satisfying	the superadditivity condition  sa  and respects logical equivalence then there is a -probabilistic kripke model k defining .
an sketch of the proof goes as follows.	let
모모모모모모모모  and for each   let be the set of formulas in the variables . let the restriction of to and let be the corresponding -probabilistic kripke model. assume the sets to be pairwise disjoint and define the structure as follows:
1. .
1.   if	and	;
  otherwise.
1. .
1. for each   put if there exists such that for all   otherwise we let undefined. observe that trivially and
.
then one can prove that	is a	-
probabilistic kripke model such that for each modality-free formula	.
모this result is in the same spirit as the one by fagin and halpern  fagin and halpern  1  theorem 1  connecting belief functions and inner measures induced by probability measures when the set var of propositional variables is finite.
1	preliminaries ii
here we survey the underlying system of fuzzy logic we shall use for our belief function logic.
모   is a logic  putting  ukasiewicz and product logics together   introduced in  esteva et al.  1 . formulas are built from propositional variables and the truth constants and  using connectives     ukasiewicz and goguen implication and product conjunction  with the following truth functions1:


	 	
	 	=
=
=
evaluations of propositional variables into  1  1  which are extended to arbitrary formulas using these rules will be called   -evaluations. other definable connectives are:
	 	is	 	 
is	 	 	  is	 	 	 	 
  is       is	   
is	 	 	 	  is	  is	   
with the following truth functions associated  as the reader easily verifies :
	 	=	 
=
=
	 	=
=
=
=
=
definition 1
 i   	is the logical system whose axioms and rules are as follows1:
   	axioms of  ukasiewicz logic  for	 	 ;  	 	axioms for product logic  for	 ;
   	 	   	 	 	 
	   1 	 
the deduction rules are modus ponens and necessitation for : from infer .
 ii   	 is the logic obtained from  	by adding a truth

constant  together with the axiom: 	 	 .
in this setting  a theory is just a set of formulas. a   evaluation is a model of if for each . the notion of proof in    is as usual.
theorem 1    is strongly complete for finite theories with respect to the given semantics. that is  if is finite then
	iff	for any evaluation	model of	.
it is interesting to remark that  as shown in  esteva et al.  1   rational truth constants  for each rational are definable in    and the logic proves their basic properties. thus the following definition is sound.
definition 1 rational   logic  r  for short  is the logic obtained from    by adding the following infinitary rule:

	from	  for each	  derive	.	 ir 
the notion of proof in r  is as follows. if is a theory over r    then the set of all provable formulas from
is the smallest theory containing as a subset  containing all axioms of r  and closed under all deduction rules. for simplicity we shall denote by . we shall keep the notation to denote finitary deduction  i.e. deduction under   .
theorem 1  strong completeness  for any theory over r  and any formula   iff is 1-true in all  1 
1 -valued models of	.
모to close this section let us mention that in  godo et al.  1  the logics    and r  have been used to build the corresponding fuzzy probability logics fp      fp r   . in the next section we shall elaborate the corresponding belief logics fb      fb r   .
1	a logic for belief functions
after all these preliminaries we are prepared to define our logic. we distinguish two kinds of formulas:
 1  boolean formulas  or s1-formulas  are built from propositional variables using connectives
 say  and the modality as described above. modality-free formulas do not contain a formula is closed if each propositional variable is in the scope of a modality. closed formulas without nested modalities are called normal; they are just built form formulas of the form modality free using the connectives above.
 1  p-formulas  or many-valued formulas . atomic pformulas have the form   read  probably    where
is a normal s1-formula. p-formulas are built from atomic p-formulas using the connectives of     i.e.    

and truth constants	and . we shall use	for
where is a modality-free formula; reads   is believed . formulas built from these using   -connectives and truth-constants are called belief formulas or b-formulas; they are of course particular p-formulas.
examples:	is a boolean  modality free formula 
is a normal s1-formula 
is a p-formula  and   is a b-formula  equivalently expressed as   .
from a knowledge representation point of view  we want to
point out the rich expressivity b-formulas provide with. as a matter of fact  we can express for instance simple numerical assignments like and by formulas

	and	; a positive belief	by
모모모모; comparisons of beliefs like by the formula ; or even more complex conditions like	by the formula

.
semantics. our language is interpreted in the obvious way in
 -probabilistic kripke models for each boolean and each is defined  and is 1 or
1 . moreover  for each normal s1-formula	the probability
is well-defined; then the truth
value of	in k is defined as
this extends to other p-formulas using truth connectives of
   thus is well defined for each p-formula in particular  is defined for each modality free formula an obvious alternative semantics for b-formulas is given by an arbitrary regular belief function bel on modality-free formulas; take and extend using truth functions of connectives. since each belief function on formulas is given by a -probabilistic kripke model this reduces to the former case.
	a p-formula	is a 1-tautology  or just tautology  of
fb     if for each -probabilistic kripke model k. it follows using  ha뫣jek  1  that the following formulas are tautologies  for any normal s1-formulas
 fp1 	 
 fp1 	 	 
 fp1 	 	 	 
these three axioms axiomatize the  fuzzy  notion of being probable. axiom  fp1  says that if both and are probable  then is probable as well. axiom  fp1  says that is probable iff is not probable. finally  it is easy to check  by means of the truth function of   that axiom  fp1  expresses the condition that the probability of is the sum of the probabilities of and minus the probability of	.
definition 1
axioms of fb  	  and fb r    are the following:
 i    for each s1-formula	provable in s1
 ii    for each normal s1-formula provable in s1  call this axiom  fb1  
 iii  the schemes  fp1 - fp1  above for p-formulas  iv  axioms of  	 for p-formulas.
deduction rules of fb     are those of     modus ponens and -necessitation  and deduction rules of fb r    are those of r   the above plus the infinitary rule  ir  .
this is a recursive axiom system since provability in s1 is decidable. alternatively one may take axioms and rules of s1  including classical propositional logic  for s1-formulas and add the rule  from infer   for a normal s1-formula  in this way one can obtain a system with finitely-many axiom schemes and rules.
모interestingly  one can easily check that fb     proves the following formulas being modality free boolean :
 1 	 

 1 

 1 moreover  ifare boolean modality free  then
 
is a derived rule in fb    . the reader may notice that these theorems and rule correspond to the four properties characterizing belief functions on formulas  see section 1   in particular  i  above corresponds to the gsa condition. indeed  in semantic terms  i  expresses the inequality	  but by the inclusion-exclusion rule for probabilities the last term is
	  and since	is
logically equivalent to	  one has
.
모after this preparation we can immediately state and prove two completeness theorems for belief theories  i.e. sets of bformulas. recall that -probabilistic kripke models that are models of a belief theory are in the obvious correspondence with regular belief functions bel that are models of i.e.
such that	for each axiom
theorem 1  completeness 
 1  strong completeness of fb r   . let be a belief theory and a belief formula.  i.e. proves over iff for each belief function bel which is a model of  iff for each probabilistic kripke model of
 1  strong finite completeness of fb    . let be a finite belief theory and a belief formula. proves over    iff for each belief function bel which is a model of
1	about dempster's rule
in this section we shall give some hints about how our logi-
cal framework is powerful enough to enable the modelling of regular belief functions combination by means of the wellknown dempster's rule  although in a bit cumbersome way and provided we work with a finite set of propositional variables.
모let us briefly recall that if and are two regular basic belief assignments on a finite   then the  regular  basic belief assignment corresponding to the dempster
rule combination of	and	is defined for as
where	  and
.
모the key point is that  in a such a finite framework  given a belief function on formulas  we can access to its corresponding mass distribution. similarly to the fact that the belief of a boolean modality-free formula can be obtained as the probability of the modal formula   the corresponding mass assignment for can be obtained as the probability of another related modal formula . indeed  let be a -probabilistic kripke model and let be its induced belief function  i.e. . call its corre-
sponding mass distribution. assume to be decomposed in a disjunction of maximally elementary conjunctions  mec's for short   that is    where each is of the form
for some . notice that since we have propositional variables  we have mec's  call them . then one can prove the following lemma.
lemma 1 for each modality free boolean formula	  let stand for
then	.
모now  since we are able to identify mass distributions  and since dempster's rule defines a new mass from two masses and with products and sums  available in the   and r  logics   the rest is easy  although a bit cumbersome as already mentioned. one has to introduce three pairs of modalities   for   and construct formulas in the obvious way. belief formulas now will be built from atomic belief formulas of the form   called   using connectives and truth-constants from   . analogously to the simple case  we can restrict ourselves to belief kripke models of the form   where the
   's are belief functions on . for each non-modal formula   let us denote by   the following  cumbersome  formula:
where	are such that
. notice that  due to lemma 1  the truth value
of	in the model	is
then let fb      be like fb     but repeating fb     axioms for each pair of modalities and having

	 	 	 dr1 
for each non-empty	and

	 	 dr1 
as additional axioms;	analogously with fb  r 	and
fb r 	 .
corollary 1 let be a belief theory over fb  r  and let be belief formula. then  iff
for each triple of belief functions model of and with . in particular 

모모 	iff	  for each triple	model of	.
1	final remarks
in this final section we like to briefly comment about the extension of the approach developed in the previius sections to deal with  general  belief functions. by general we mean regular as well as singular  i.e. belief functions such that can be positive. a -space as defined in definition 1 yields general belief functions; if is singular  nonregular   its regularization is defined as
there is no room for full details so we shall only sketch how an extension of our approach covering this can be done.
모the main point is that when dealing with modal logic we have to allow possible worlds satisfying  where is the truth constant for falsity   i.e. to allow for worlds such that for no . there are several logics admitting this but to be able to generalize directly our proofs we have to keep the logic as near to s1 as possible. our proposal is motivated semantically: call a -probability kripke model where are as above and is a symmetric transitive relation  not necessarily reflexive. the modal logic whose models are structures of the form   where
is symmetric and transitive  is known as kb1  a weaker logic than s1  where axiom t is dropped .
모so swichting our base modal logic from s1 to kb1  the reader may go through sections 1 and 1 generalizing from s1 to kb1 and from regular belief functions to general belief functions. then define the logics fb     and fb  r  modifying the definitions of fb-logics as follows: axioms sub  i  are all kb1-tautologies and sub  ii  all formulas where is any normal kb1-tautology. finally  the completeness theorem 1 generalizes in the obvious way by replacing fb by fb and deleting all occurrences of the word  regular  . it is important to observe that our generalized fuzzy logic of belief functions can express the regularization of a singular belief function  namely one can check that expresses the regular-
ized belief of	.
모finally let us comment how this general approach also covers general belief functions combination by means of demspter's rule  actually in a simplified way. in such a setting  the  non-normalized  dempster's combination of two masses and is defined for any as
where obviously now may be positive  even if and were regular. then one can just follow the same approach as in section 1 and define the logics fb      and
fb  r  as fb      and fb  r  respectively but replacing axioms  dr1  and  dr1  just by this one
모모모모모모모모모모모모모모 	 	 dr1'  for each	  with the convention that if	  then	is taken as the truth constant	. finally one can show that corollary 1 still remains valid if we replace there normalized demspter combination	by the non-normalized version	.
acknowledgements.
petr ha뫣jek acknowledges partial support by the grant no. iaa 1 of the grant agency of the academy of sciences of the czech republic. lluis godo and francesc esteva acknowledges partial support of the bilateral project csiccnr  logics for evidential theory . the authors also thank the anonymous referees for their useful comments and criticisms.
references
 bacchus  1  bacchus f. representingand reasoningwith probabilistic knowledge. mit-press  cambridge massachusetts  1.
 bendova and ha뫣jek  1  bendova뫣  k.  ha뫣jek  p. possibilistic logic as tense logic. in:  piera et al.  ed.  proc. of quardet'1  barcelona 1.
 boutilier  1  boutillier c. modal logics for qualitative possilibity and beliefs  in:  dubois et al.  ed.  uncertainty in artifical intelligence viii  morgan-kaufmannpubl. 1  1.
 dubois et al.  1  dubois d.  lang  j.  prade h. possibilistic logic. in:  gabbay et al.  ed.  handbook of logic in artificial intelligence and logic programming  vol. 1  1. oxford up  1.
 esteva et al.  1  esteva f.  godo l. and montagna f. the   and    logics: two complete fuzzy logics joining  ukasiewicz and product logic. archive for mathematical logic 1  1  1.
 fagin and halpern  1  fagin r.  halpern j.y. uncertainty  belief and probability. computational intelligence 1  1  1.
 fagin et al.  1  fagin r.  halpern j.y.  and megiddo n. a logic forreasoning about probabilities information and computation 1  1  1.
 godo et al.  1  godo l.  esteva f.  ha뫣jek p. reasoning about probabilities using fuzzy logic. neural network world 1  1  1.
 godo et al.  1  godo l.  ha뫣jek p. and esteva f. a fuzzy modal logic for belief functionsm  extended version . iiia research report 1  1. available at http://www.iiia.csic.es/publications/reports/1/.
 ha뫣jek  1  ha뫣jek p.	metamathematics of fuzzy logic  kluwer 1.
 ha뫣jek  1  ha뫣jek p. getting belief functions from kripke models. int. j. general systems  1  no. 1  1   1.
 ha뫣jek et al.  1  ha뫣jek p.  godo l. and esteva f. fuzzy logic and probability. in proc. of uai'1  pp. 1  1.
 ha뫣jek et al.  1  ha뫣jek p.  harmancova뫣 d.  esteva f.  garcia p.  godo l. on modal logics of qualitative possibility in fuzzy setting. in proc. uai'1  morgan-kaufmann pp. 1  1.
 halpern  1  halpern j. y. an analysis of first-order logics of probability in proceedings of the international joint conference on artificial intelligence  ijcai'1   pp. 1  1.
 harmanec and ha뫣jek  1  harmanec  d.  ha뫣jek  p. a qualitative belief logic. int. journ. uncertainty  fuzziness and knowledgebased systems 1  1   1.
 harmanec et al.    harmanec d.  klir g.j. and resconi g. on modal logic interpretation of demspter-shafer theory of evidence. int. j. of intelligent systems 1  1.
 keisler  1  keisler j. probability quantifiers. in  j. barwise and s. feferman  ed.  model-theoretic logics springer-verlag new york 1  1
 nilsson  1  nilsson n.j. probabilistic logic. artificial intelligence 1  1  1.
 shafer  1  shafer g. a mathematical theory of evidence. princeton univ. press 1.
 saffiotti  1  saffiotti  a. a belief function logic. proc. of the 1th aaai conf. san jose  ca  pp. 1  1
 sossai et al.  1  sossai c.  p. bison p.  chemello g. and trainito n. merging probability and possibility for robot localization. in ijcai-1 workshop on reasoning with uncertainty in robot navigation stockholm  sweden  august 1  1.

uncertainty and probabilistic reasoning
probabilistic reasoning

ibal: a probabilistic rational programming language
avi pfeffer
division of engineering and applied sciences
harvard university avi eecs.harvard.edu

abstract
in a rational programming language  a program specifies a situation faced by an agent; evaluating the program amounts to computing what a rational agent would believe or do in the situation. this paper presents ibal  a rational programming language for probabilistic and decision-theoretic agents. ibal provides a rich declarative language for describing probabilistic models. the expression language allows the description of arbitrarily complex generative models. in addition  ibal's observation language makes it possible to express and compose rejective models that result from conditioning on the observations. ibal also integrates bayesian parameter estimation and decisiontheoretic utility maximization thoroughly into the framework. all these are packaged together into a programming language that has a rich type system and built-in extensibility. this paper presents a detailed account of the syntax and semantics of ibal  as well as an overview of the implementation.
1	introduction
in a rational programming language  a program specifes a situation encountered by an agent; evaluating the program amounts to computing what a rational agent would believe or do in the situation. rational programming combines the advantages of declarative representations with features of programming languages such as modularity  compositionality  and type systems. a system designer need not reinvent the algorithms for deciding what the system should do in each possible situation it encounters. it is sufficient to declaratively describe the situation  and leave the sophisticated inference algorithms to the implementors of the language.
모one can think of prolog as a rational programming language  focused on computing the beliefs of an agent that uses logical deduction. in the past few years there has been a shift in ai towards specifications of rational behavior in terms of probability and decision theory. this paper presents ibal  a probabilistic rational programming language. ibal  pronounced  eyeball   stands for integrated bayesian agent language. as its name suggests  it integrates various expects of probability-based rational behavior  including probabilistic reasoning  bayesian parameter estimation and decisiontheoretic utility maximization.
모ibal makes four main contributions. the first is a highly expressive language for representing probability models  that is significantly more expressive than previous languages. second  ibal integrates a language for probabilistic modeling  bayesian learning and decision theory under a single coherent semantic framework. third  it provides a unified inference engine for solving reasoning  learning and utility maximization problems  that generalizes algorithms for many standard kinds of models. finally  ibal is packaged together into a usable programming language with features such as a strong type system and built-in extensibility.
모ibal is designed with three kinds of users in mind. the first is the system modeler  who may not be an expert in probabilistic reasoning. for this type of user the basics of the language should be reasonably easy to learn  and it should be fairly easy to come up with decent models for many domains. this kind of user will benefit greatly from a good selection of libraries implementing standard kinds of models. also  a good default inference algorithm is needed that can be expected to do reasonably well on a large number of models.
모the second kind of user is a modeling expert  who understands well the inference algorithms for probabilistic reasoning. for the expert user  the language should provide the power to carefully tweak the model being used  and to control what inference algorithm is used to evaluate different parts of the model.
모the third kind of user is the ai researcher  who may want to introduce new kinds of probabilistic models and new inference algorithms. ibal makes it easy to introduce new models via libraries  and the implementation framework provides support for building new algorithms and extending the inference capabilities of the system.
모space limitations prevent a full description of both the language and the implementation in this paper. the next section provides a fairly detailed account of the language and its semantics. section 1 presents an extended example  showing how the various components of ibal can be used together to provide a declarative implementation of a fairly complex decision-theoretic agent. section 1 presents an overview of the main features of the ibal implementation.
1	genealogy and related work
the most direct presucrsor to ibal is the language of  koller et al.  1   hereinafter referred to as kmp1. kmp1 is a lisp-like language extended with a flip construct to describe random events. ibal extends kmp1 in a number of powerful ways. ibal's basic definition and expression language for describing generative probabilistic models is similar to kmp1  but significantly richer  particularly in its use of higher-order functions. in addition  ibal uses observations to condition distributions  which allows it to easily describe a much richer class of models. ibal also integrates decisions and learning into the framework  which are not provided by kmp1.
모ibal is also indebted to recent work integrating probabilistic and first-order representation languages. two recent strands in that direction are relational probability models  pfeffer et al.  1   and stochastic logic programs  muggleton  1 .
모other projects have tried to integrate an expressive modeling language with at least some aspect of learning or decision theory. another project similar in spirit to ibal is dtgolog  boutilier et al.  1   which integrates decision theory into the golog action cacululus. it is based on logicprogramming rather than the functional programming framework ibal uses. also  its roots can be traced back to markov decision processes  while ibal's roots are in bayesian networks. as a result  the two systems have quite a different approach to inference; for example  there is no notion of variable elimination in dtgolog. also  dtgolog does not integrate learning into the framework.
모 mcallester  1  presents a language based on kmp1 for describing decision problems and the policies used by agents  and for calculating the expected utilities of the agents. unlike ibal  there is no attempt to solve the decision problem and compute the optimal policies.  cumby and roth  1  integrates learning and reasoning in an expressive  compositional language  but one that is not probabilistic.
1	the ibal language
ibal provides a declarative language for describing probability distributions  parameter estimation problems and utility maximization problems. the top level language component in ibal is a block. a block consists of a sequence of declarations. there are a number of kinds of declarations  including definitions stating how values of things are stochastically generated; observations stating that some property holds of generated values; priors describing the prior probability distributions over learnable parameters; decisions describing the decisions that an agent makes and the information it has; rewards describing the rewards an agent receives; and pragmatics  containing control knowledge on how to perform inference in a block. i will describe each of these in turn. for the sake of presentation i will present the semantics of the language incrementally  as i discuss each kind of declaration. however  the discussions of semantics are somewhat technical  and can be skipped on first reading.
1	definitions
a definition states how the value associated with a name is generated. a name is a symbol such as ; a chain is a sequence of names such as . a definition has the form   where is an expression with one of the forms:
 constant   chain 
 function 	'	where	is a symbol conditional 	if	then	else
	where	stands for a pattern  discussed below dist 	dist	:	:
	where the	are probabilities summing to 1 block 	where	is a block application 
a pattern defines a condition that may be satisfied by a value. the notation stands for the predicate that is true iff the value of expression satisfies the pattern . a pattern may specify equality to a constant  a chain  it may be the negation of a pattern  or the conjunction or disjunction of two patterns.
모in addition to the above expression syntax  the language provides plenty of syntactic sugar  such as case statements and easier syntax for defining functions. there is also a type system  discussion of which will be omitted for lack of space.
모the intuitive meaning of a definition is that it defines a stochastic experiment generating the value of . consider a simple example:
fair  	= dist  1 : 'h  1 : 't  biased   = dist  1 : 'h  1 : 't 
pick   = dist  1 : fair  1 : biased  coin = pick   x = y = coin   ; z = coin  
모intuitively  fair and biased are functions that return either 'h or 't with appropriate probabilities. the function pick is a higher-order function that returns either the fair or biased function. the value of coin is generated by applying pick; it is either fair or biased. the expression defining x is a block expression; x is a data structure with components y and z  both generated by applying the value of coin. chains x.y and x.z are mutually dependent on whether coin is fair or biased  but they are conditionally independent given coin.
모ibal inherits from kmp1 the idea of using laziness to allow infinitely long experiments to be defined  only some of whose results may be needed. for example:
real   =	first = dist  1 : 'zero  1 :'one  rest = real  
less than half = real.first	'zero
모here  real defines a uniform distribution over real numbers between 1 and 1  represented by their binary expansion. executing real involves an infinite recursion  but only the first bit is needed to determine the value of lessthanhalf  which is the value of a boolean predicate that looks only at real.first. we can think of executing real lazily  to get the single bit needed.
모i will now make these intuitions precise. there are four kinds of values in ibal:  1  symbols such as heads and true;  1  a special undefined value  denoted by ;  1  complex values that consist of an environment  which maps names to values;  1  closures  denoted	consisting of formal paramaters	  body	and environment .
모a chain can be viewed as representing a function on values. formally  if is a chain and a value  we define as follows: if is empty  . otherwise  if is not complex 
모모모모. otherwise  let be   and be the value associated with in ; then . to determine whether a value satisfies a pattern  we need an environment assigning values to the variables appearing in the pattern. formally  we define   meaning that satisfies in   by
if	is the symbol	;	  if	and	; and if	and	.	for conjunctive
or disjunctive is defined in the obvious way. note that cannot satisfy any pattern.
모in order to generate the value of an expression   we need an environment binding the free variables of   and a source of randomness  which is provided by an infinite sequence of i.i.d. real numbers . formally  we will define a function   meaning the value generated for in environment   given random choices as in . the notation is the subsequence of of elements with index congruent to modulo . this device allows us to split into multiple independent subsequences. hd and tl indicate the head and tail of . for constant  chain  function  conditional and dist expressions  is defined by:
if	if then	if else	if
모모모모모모모모모모모모모모where dist : : tl where	hd
모the value of a block expression is complex  and a definition within the block results in the component mapping to the value of . each definition is evaluated in an environment including bindings for names appearing previously in the block. furthermore  if is a lambda expression  the binding for is also added to the environment in which is evaluated  so that is bound in the resulting closure. this allows recursive functions to be defined. formally:
:
where if	is a lambda
and otherwise
모the final case to define is function application. to determine the value of   we first compute the value of . if it is not a closure  the result of the application is undefined. otherwise we evaluate the body in the environment formed by extending the closure environment by binding each formal parameter	to the value of	. formally:
if
otherwise
where
모note that the preceding definitions elegantly take care of the issue of infinite experiments with finite observations  such as the lessthanhalf example above  without needing to make explicit use of laziness in the semantics. the rule for generating the value of an expression only uses the chains that appear in it. furthermore  a block expression always returns a complex value.
모the above semantics is an operational semantics  showing how a program consisting of definitions defines a random experiment for generating values. we also provide a denotational semantics in terms of a probability measure over values. the underlying probability space consists of countable sequences of i.i.d. real numbers generated uniformly from  1 . a program defines a function from sequences to values  by   where is the empty environment. if is a measurable set of sequences  and is the image of under   then is measurable and pr pr .
모natural properties of values that we would like to talk about are in fact measurable. for example  any property of the form is measurable. we can see this by defining a depth-bounded evaluation function . its definition is the same as above  except that is always   and uses for evaluating the body of function applications. in other words  values that require a recursion to a depth greater than will be undefined. now  a program defines a sequence of functions . it is clear that the set is measurable since it only requires looking at a finite subsequence of to determine if it is in . therefore the set	is measurable. furthermore  the above argument also suggests an anytime approximation algorithm for computing pr . since the are non-decreasing  and their union is   the probabilities of the are a non-decreasing sequence whose limit is pr pr .
1	observations
the language described so far is similar to that of kmp1  albeit with a richer syntax and type system  and a more refined semantics. it can express many common models  such as bayesian networks  relational probability models  stochastic logic programs  hidden markov models  dynamic bayesian networks and stochastic context free grammars. all these models are generative in nature  defining an experiment that stochastically generates values for variables. the richness of the model is encoded in the way the values are generated.
모another flavor of probability model is a rejective model. in a rejective model  the data is generated by a very simple process  e.g. uniformly  but data that fails to satisfy certain constraints may be rejected. the richness of the model is encoded in the rejection process. a good example of a rejective model is a product of experts  poe   hinton  1 . in a poe  a datum is generated uniformly  and then passed to a set of probabilistic experts. each expert accepts with some probability that depends on a property of . the data is accepted only if all experts accept it. the probability of any datum is proportional to .
모ibal is able to express rejective models by making observations an integral part of the language. an observation is a declaration of the form . recall that this is the syntax used for boolean predicates. an observation is simply a statement that a certain predicate is true.
thus  for example  the general schematic form of a poe
model	is	expressed	as
generate  	=	 * produce a uniform datum * 
x	=	generate  
 * for each expert   the following code *  expert  x 	=	...  * return 'accept or 'reject * 
expert	'acceptfollows.모another kind of model that can be expressed using observation declarations is a markov random field  mrf . an mrf is an undirected analogue of a bayesian network  but it can also be viewed as a type of poe. an interesting effect is at work here. ibal's expression language defines directed  generative models  but the observation language implements the undirected notion of a constraint. as a result  ibal is able to express both directed and undirected models  and combinations of the two. it is important to stress that observations are an integral part of the language  and not something pasted onto a model after the fact in order to condition it. they can occur within blocks and functions  and therefore they can be composed together  just like generative definitions. all the power of a modular  functional language is thereby extended to rejective models. of course  ibal also allows rejective models to be combined with generative models. for example  one natural way to build a language model is to use a stochastic context free grammar as the initial generator of sentences  and then use probabilistic constraints to express global properties like agreement and sentence length.
모in defining the semantics of observationsin ibal  one subtle point must be stressed. an observation in a block can only condition variables defined within the block  not free variables. as far as a containing block is concerned  the definition of a contained block is considered to be a black box. it simply defines a distribution overthe value of the block  given values for the free variables. the containing block need not concern itself with whether this distribution is defined generatively or rejectively. failure to enforce this rule would be a serious violation of modularity.
모we get the right effect simply by modifying the definition of for the case of block expressions. now  in addition to defining values for each of the components of the block  it will also make sure that all the observations in the block are satisfied. if they are not  the generated values for the block are rejected  and the process is repeated. the resulting distribution defined by the block is conditioned on the observations being satisfied. it is  of course  possible to define a set of observations that fail with probability 1  in which case the attempt to generate a value for the block will go on for ever. in that case  the value of the block is . note that because the rejection/repetition process is contained within the block itself  only the value of the block itself is conditioned by the observations  not the free variables.
1	learning
observations provide the basis for integrating learning  in the form of bayesian parameter estimation  into the ibal framework. unknown probability parameters are specified using prior declarations  which have the form learn dirichlet	.
모a prior declaration of this form achieves two things. first  it defines a probabilistic parameter   and specifies a dirichlet prior over the parameter. the are positive real numbers  specifying the the hyperparameters of the dirichlet. second  a prior declaration also creates a definition for   equivalent to dist . we can view an ibal program with prior declarations as specifying a joint model  that defines a joint probability distribution over the model parameters and the value returned by the program. observations condition the joint model in the standard bayesian way. let us refine the coins example from earlier by adding priors and observations.
fair  =result = dist  1 : 'h  1 : 't biased  =learn result = dirichlet  1 : 'h  1 : 't pick  =learn result = dirichlet  1 : fair  1 : biased coin=pick  .resultx=y = coin  .result ; z = coin  .resultx.y'ha fair coin is known to produce 'h with probability 1. the probability of 'h for a biased coin is unknown  but its prior is peaked around 1  while the prior over which coin gets picked is uniform. as before  coin is the result of picking a coin  and x.y and x.z are two tosses of coin. we also have an observation that x.y came out 'h. this observation has multiple effects. first  because 'h is more likely for a biased coin  the probability that coin is biased is increased  which in turn increases the probability that x.z is 'h. the observation also conditions the probability parameters. because coin  a result of applying pick  is likely to have turned out biased  we will get a posterior over the pick parameter that is more weighted towards a biased result. furthermore  because coin may have been biased  and because a toss of coin came out 'h  the posterior for the biased parameter is also weighed slightly more strongly towards heads.
모with its learning component  ibal is able to do parameter estimation for many common models  including hidden markov models  stochastic context free grammars and probabilistic relational models. furthermore  learning in ibal is not just  added on  to the probabilistic representation language  but is thoroughly integrated into the language. as a result  the benefits of compositionality and modularity are obtained for representing learning tasks. in particular  ibal is good at representing a cumulative learning framework  in which smaller models are learned and then used as components of larger learning problems. just as observations only condition values within their scope  they are only used to learn about model parameters within their scope. thus a compositional learning process can be specified by providing a nested scope containing all the data and parameters for a learning subproblem  and a containing scope that uses the results of the subproblem.
모in defining the semantics of learning in ibal  we need to get a subtle point correct. if a prior declaration appears in the body of a function  the same parameter values should be used every time the function is applied. this is fundamental to learning - different observations of the same function are all observations about the same parameter! this means that in terms of the generative semantics  the parameter value is not returned by each application of the function  but rather it is generated when the function is defined  and stored as part of the closure representing the value of the function object.
모to achieve this effect  we make the following definition. the parameters directly inside a block are the parameters of prior declarations defined in the block  that are not nested in the body of a lambda expression. in the generative process  the values of the parameters directly inside the body of a lambda are generated at the time the closure is created  and these values are used for all future applications of the closure.
모the remainder of the semantics stays basically the same as before. when a definition resulting from a prior declaration is encountered  the relevant parameter values are looked up in the environment and used to choose which branch is taken  in the same way as for a dist expression.
1	decisions and utilities
the representation of decision problems in ibal is geared towards two popular models: influence diagrams  ids  and markov decision problems  mdps . ibal can easily represent these and other models  including various kinds of structured mdps. a decision declaration in ibal has the form choose from given
this specifies the name of the decision variable  its range   and the information available to the decision maker  the chains  called the informational parents of  . a block may contain multiple decision declarations  in which case we enforce the noforgetting rule of ids  that the informational parents of later decisions always include those of earlier decisions  as well as the decisions themselves. a reward declaration is either receive case of : : or receive . the first form states that the reward depends on the value of   and it is the real number associated with the first pattern that the value satisfies. the second states that the reward is times the reward of   where is a positive real number. a block may have multiple reward declarations  in which case the total reward is the sum of the individual rewards. for example  a typical mdp is schematically represented as follows:
mdp s  =	 * takes current state as argument *  transition s a  = ...  * returns next state *  reward s a  =	reward case  s a  of   's1 'a1  : 1  ... 
choose a from a1 a1 given s nextstate = transition s a  currentreward = reward s a  futurereward = mdp nextstate  reward 1 currentreward reward 1 futurereward
모three points must be made about the representation of decision problems in ibal. the first is that each block constitutes a distinct decision problem. a block with decisions is viewed as identifying an implicit agent who makes the decisions and receives the rewards mentioned in the block itself.
decisions and rewards in nested blocks implement the notion of delegation; decisions in the nested block do not consider their effects on the containing block. the reason for enforcing this interpretation is similar to the reason observations don't condition values outside their scope; the alternative would result in a serious loss of modularity. if an agent in a nested block had to be concerned about rewards in the calling block  the decisions for the nested block would no longer be determined solely by its free variables. therefore a program calling the nested block could no longer treat it as a black box.
모the second point to clarify is that it is assumed that the values of free variables are always known to an agent making the decisions in a block. once again  failure to enforce this restriction would result in a modularity problem. if the agent does not know the values of the free variables  it must know the distribution over those values in order to make a rational decision. but then  the distribution defined by a block depends not only on the values of the free variables  but on their distribution. this is an unfortunate restriction  since it prevents ibal from being capable of representing pomdps. the situation can perhaps be salvaged  by borrowing the idea of belief state from pomdps  and adding it as an extra implicit input to the block.
모the third point is the relationship between observations and decisions in a block. we assume that all observations are known to the decision maker  even if they appear lexically subsequent to the decision. the reason is that the observations are viewed simply as part of the definition of the probability distribution over the block's values  and we make the assumption that the decision maker knows the correct distribution. a result of this assumption is that we disallow observations statement to mention variables that depend  directly or indirectly  on the decisions in a block. this restriction prevents the semantics of learning and decisions from interfering with each other.
모in defining the semantics  we begin with utilities. with every value	we associate a utility	. for non-complex values	. for complex values	is determined by the reward declarations in the block in which	was created. recall that a complex value is created by	. we extend the definition of	so that it also produces after producing	. we set	to be the sum over reward declarations	in	of	  defined as follows. if	is reward case	of	  if	and	for if	for
where is the environment resulting from extending with the bindings in . if is reward   .
모now for decisions. a strategy for block is a decision for each decision statement and each value of the informational parents of . given an environment in which to evaluate   and a particular strategy   we define a function
모모모  whose meaning is the utility for   given strategy   bindings of free variables in   and random choices specified by . holding   and fixed  this function is a random variable on the space of sequences. taking the expectation of this function over sequences produces the expected utility; the optimal strategy for is then the one that maximizes this expected utility. the definition of now proceeds by first choosing  and then processing the block as before  using to choose the values of each decision variable given its informational parents.
1	pragmatics
the ibal language allows a wide variety of different kinds of models to be expressed. there is no single best inference algorithm to use for all models. while ibal implements a good default algorithm that should work well in many situations  it also allows the possibility of using other methods. the programmer can specify control knowledge on how to solve a program in the form of pragmatic declarations. these could either specify what algorithm to use  or details of how to use a particular algorithm  such as specific elimination orderings for variable elimination.
모because pragmatic declarations are part of a block  they only specify how to do inference in that block and its nested blocks. furthermore  pragmatic declarations in a nested block can override those in a containing block. these features allow fine  modular  control over how inference is done  allowing complex models to be built in which different inference methods are used for different components.
모it is anticipated that pragmatic declarations will often be used by library designers. the user of a library is shielded from thinking about how the library models will be solved. this modularity of inference methods may turn out to be as important as modularity of representation in building complex probabilistic agents.
1	example
in this section  i illustrate how a declarative high-level representation language that unifies probabilistic reasoning  decision theory and parameter estimation can simplify and integrate the implementation of sophisticated rational agents. consider the task of implementing an automated receptionist agent. the job of the agent is to receive spoken requests over the phone and to respond to them appropriately. requests include asking for directions and talking to a particular person; responses include giving directions  connecting to an extension  and asking the caller to repeat the request.
모in a decision-theoretic design for this agent  the agent receives a utility based on how well its response matches the ac-
tual request of the user. of course  the agent does not observe the actual user request  but only a sequence of signals. in the decision model  there will be a prior distribution over requests and a conditional distribution of signals given requests.
모with existing tools  it is difficult to implement this decision-theoretic design using a single  coherent model. instead  a typical implementation might consist of several components  each of which is probabilistic  but which are stitched together in an ad-hoc manner. there will typically be a speech-recognition component that determines the likely words that generated the received signal; a language model that can determine the probability that a particular request generated a particular sentence; and a high-level influence diagram for deciding what to do. the results of the speechrecognizer and request generator are fed into the language model. the results of the language model are fed into the influence diagram. the overall result may not be a coherent probabilistic model. furthermore  it may be hard to tailor the components for their use in this application. for example  getting the speech recognizer to recognize unusual names of individuals in the company might require an extra engineering effort.
모with ibal  the whole application can be described using a single declarative model. at a high-level  the generative model consists of three steps: a request generation function  a function that generates sentences based on requests  and a function that generates phoneme sequences from sentences. for the decision-making component  the agent is given the phoneme sequence  and chooses a response. the agent's utility is determined from the request and the response. the highlevel code looks like this:
request = makerequest   sentence = makesentence request  phonemes = makephonemes sentence  choose response.type from givedirections  connect  repeat choose response.who from fred  wilma receive match request  response 
모i will now describe  in turn  the three generative steps and the utility computation. the function makerequest produces a request. a request has a type field  whose value is either getdirections or talk. a talk request also has a who field  whose value is a person. a person is a complex value with three fields: name  title and extension. in a typical programming language  the type of name would be string  because what we typically want to do with names is compare them  read them and print them. in this application  however  the most important thing about a name is how it tends to be pronounced. therefore the type of name is a function that stochastically generates a phoneme sequence. the same is true for title. the makerequest function is as follows:
makerequest   =
learn who =  1 : fred  1 : wilma  learn type =  1 : getdirections; 1 : talk 
모generating the request involves choosing two things: the type of the request  and the person  if the request is to talk to a person. both choices are made learnable. the agent has lots of past experience about what kinds of requests tend to be generated. these are expressed by observation statements as follows:
r1 = makerequest   r1 type	getdirections r1 = makerequest   r1 type	talk
r1 who	wilma
...
모the makesentence function produces a sentence from a request. a sentence is a list of words  each of which is a function that stochastically produces a list of phonemes when activated. here i illustrate with an extremely simple sentence generation model; in a real application  a more complex grammar model might be used. even this simple example illustrates the power of passing around data structures that contain functions in fields. the sentence generator simply slots the given person's name or title in the appropriate place.   is the list containing   and
is the list concatenation operator. 
makesentence request  =
	if request type	getdirections
then  can;you;give;me;directions  else  connect;me;to 
 dist  1:request.who.name  1:request.who.title  
모the makephonemes function takes a sentence and produces a list of phonemes. since each word in the sentence is a function that generates phoneme sequences  makephonemes just executes each of the words in the sentence  and concatenates the results. the individual word models would probably be hidden markov models  and may be part of a speech recognition library. unusual names of people may not be in the generic hmm library  but the library may provide a function that creates a new  trainable hmm that can be learned for a particular name.
makephonemes sentence  =
if sentence isempty
then 
else head sentence      makephonemes tail sentence  
모finally  the utility model takes the request and response  and produces a utility of 1 if the request matches the response. if the response is repeatplease  the utility is 1. an incorrect response has utility -1.
match request response  =
correct =
 request type	getdirections response type	givedirections 
 request type	talk response type	connect
	request who	response who 
receive if correct
then 1 else if response type repeat
then 1 else -1
1	implementation overview
the inference tasks in ibal are to estimate the learnable parameters  to compute the utility maximizing decisions  and to solve for the conditional distribution over various chains  given values for other chains. the tasks are accomplished in that order. since decisions cannot influence observations  they are irrelevant to the learning task. once the parameters have been learned  the expected utility for any strategy can be computed and the decision task can be solved. once the decisions have been fixed  the distribution over all values is known and the probabilistic reasoning task can be solved.
모ibal is implemented in objective caml  a variant of ml. the design of the implementation is divided into four parts. the first component is a frontend consisting of a parser  type checker  and translator  whose job is to produce code in shallow form. in shallow form  nested subexpressions are replaced by chains  and only simple patterns are allowed. the second component is a set of support modules. these include a ubiquitous polymorphic container type implementing efficient maps from chains to values of any kind  as well as a general implementation of events  which are measurable sets of values  and factors  which are measurable functions from values to elements of a ring. events and factors support a very generalized version of the variable elimination algorithm.
모the third component implements the main line of inference. the main line begins with code in shallow form  and proceeds through a sequence of steps. the first step is domain generation in which the support of each chain used in the program is computed. domain generation uses rules such as the following: for a definition dist
               is the union of the . domain generation also uses observations to restrict the domains. domain generation may be followed by an optional constraint propagation step  which further restricts the domains  by propagating observations back through the definitions in the program. constraint propagation may be worthwhile since it is cheaper than the full-scale variable elimination process performed later.
모the second step is to compute the needed chains that are actually relevant to answering a query. these include all chains that influence query variables or observed variables  directly or indirectly. this computation step is particularly important in recursive models such as stochastic grammars. it deduces  for example  that in order to determine whether the first word of a sentence is  the   only the first non-terminal need be expanded at any stage  assuming there are no productions .
모the key step in the main line  factor production  converts each definition in the program into a set of factors. the goal is to produce factors that are as simple as possible; dummy variables may be introduced to achieve this. for example  for a definition   suppose and are complex  with and needed. two separate factors will be produced  one enforcing that and are equal  and the other for and
모. for a definition dist   a dummy variable is introduced  to represent which branch is actually taken. the dummy variable serves to separate the from each other; without the dummy variable  they would all have to appear in the same factor. a set of factors is produced for each branch   saying that if takes the value   then the components of and must have the same value  but if
takes any other value this branch is irrelevant and there is no constraint on and . an additional factor saying that takes value 1 with probability   ...  with probability is also produced.
모the final solution step is variable elimination  as is commonly used for bn inference. the set of factors computed by factor production represents a sum of products expression. all variables except for query variables and free variables in a block are eliminated from the factors  and the result is the conditional probability of the query variables given the free variables. all solution steps except for the final variable elimination are recursive; a recursive call is used to process nested blocks and function applications.
모the final component of the implementation is the glue that holds everything together. each of the solution steps is turned into a dynamic programming algorithm  where all recursive calls are looked up in a cache before solving them explicitly. also  an iterative deepening strategy is used to provide an anytime approximation algorithm for queries that may not terminate. these recursion strategies are implemented in a generic  modular way. for example  a dynamic function is provided that takes a recursive algorithm and produces a dynamic programming version of the same algorithm. the glue is also responsible for handling pragmatics.
모the glue is implemented using a programming technique in which a recursive function takes a special argument . when recurses  rather than calling itself directly  it calls
 . thus is a function with a  hole   which needs to be filled in. all the recursion strategies can be implemented as ways of filling in the hole. the simplest way to fill in the hole is to plug into itself  which gives the basic recursion pattern. but another possibility is to define a function cached which looks for a result in a cache before calling   calls if the result is not found  and stores the result of calling in the cache. if cached is plugged into itself  the result is a dynamic programming algorithm based on
 . similarly  depth-bounded produces a version of that stops recursing after a recursion depth of   returning instead. recursion strategies can be composed using the method of functions with holes. this provides a nice  modular way to deal with multiple inference methods. each inference method is invoked by a pragmatics handler that recognizes a particular type of pragmatic declaration. the pragmatic handlers can be installed on top of each other  so that every recursive call results in the all the handlers being checked. the inference methods themselves do not need to know what handlers are used for the recursive calls.
모all three inference problems use dynamic programming with the main line of inference described above. decision making is based on id algorithms  using backward induction to solve the decisions in a block from the bottom up. for mdps  the algorithm reduces to value iteration  because of the dynamic programming component  with reachability analysis  because of the domain generation . for the probabilistic reasoning task  all reasoning is directed towards a particular query; pruning of the problem to those variables required for the query is accomplished by computing the needed chains. for probabilistic reasoning tasks  the algorithm reduces to regular variable elimination for standard bns  while the dynamic programming makes it equivalent to forward-backward for hmms and the inside algorithm for scfgs. finally  parameter estimation uses a general version of the em algorithm to compute maximum a-posteriori parameter values. this is only an approximation to the true bayesian posterior  of course. the e step consists of using the main inference line to compute how many times each branch was taken for each prior declaration. these are the expected sufficient statistics. the m step combines the expected sufficient statistics with the dirichlet hyperparameters to produce new parameter values that have maximum posterior probability given the expected sufficient statistics. again  the combination of variable elimination with dynamic programming results in the algorithm reducing to the typical em used for bns  baum-welch for hmms  and the outside algorithm for scfgs.
모i do not claim that ibal will be an efficient inference algorithm for all applications. some problems are inherently hard  or require special methods  and a variety of algorithms is needed. i do believe that ibal integrates a number of widely used methods  and thereby provides a good default algorithm. furthermore  the support modules and glue make it easier to extend the system with alternative methods.
1	conclusion and future work
ibal is a rich declarative programming language for describing probabilistic models  decision theoretic situations  and bayesian parameter estimation problems. this paper has presented a syntax and semantics for the language  and an overview of an implementation with probabilistic reasoning  utility maximization and parameter learning.
모future work on ibal is divided into implementation enhancements that are practically useful but not too theoretically challenging  and more significant extensions to the expressive power of the language. plans in the first category include implementing a variety of approximate solution methods; providing libraries to implement common kinds of models  and useful structures such as sets of objects; and extending the type system to include algebraic data types and polymorphic types  modeled on ml.
모one of the main future tasks in the second category  is to support the learning of model structure as well as parameters. in keeping with the philosophy that everything that can be done in ibal should be thoroughly integrated  this will require a much richer language with which to express priors  including priors over structure. another important extension is to provide a way to describe how an ibal agent interacts with its environment; in other words  to provide a declarative description of the i/o capaboilities of the agent. a final task is to allow multiple agents into the ibal framework. not only would this allow ibal to be used for modeling gametheoretic situations  it would also provide a way to describe agents' models of other agents. while these tasks present a range of interesting and challenging issues  ibal provides a good foundation with which to begin tackling them.
references
 boutilier et al.  1  c. boutilier  r. reiter  m. soutchanski  and s. thrun. decision-theoretic  high-level agent programming in the situation calculus. in aaai  1.
 cumby and roth  1  c. cumby and d. roth. relational representations that facilitate learning. in kr  1.
 hinton  1  g. hinton. training products of experts by minimizing contrastive divergence. technical report  gatsby computational neuroscience unit  1.
 koller et al.  1  d. koller  d. mcallester  and a. pfeffer. effective bayesian inference for stochastic programs. in aaai  1.
 mcallester  1  d. mcallester. bellman equations for stochastic programs. revision of talk at lpnmr-1  1.
 muggleton  1  s. muggleton. stochastic logic programs. journal of logic programming  1. accepted subject to revision.
 pfeffer et al.  1  a. pfeffer  d. koller  b. milch  and k.t. takusagawa. spook: a system for probabilistic object-oriented knowledge representation. in uai  1.
approximate inference for first-order probabilistic languages
hanna pasula and stuart russell
computer science division  university of california
soda hall  berkeley  ca 1 pasula russell  cs.berkeley.eduabstract
a new  general approach is described for approximate inference in first-order probabilistic languages  using markov chain monte carlo  mcmc  techniques in the space of concrete possible worlds underlying any given knowledge base. the simplicity of the approach and its lazy construction of possible worlds make it possible to consider quite expressive languages. in particular  we consider two extensions to the basic relational probability models  rpms  defined by koller and pfeffer  both of which have caused difficulties for exact algorithms. the first extension deals with uncertainty about relations among objects  where mcmc samples over relational structures. the second extension deals with uncertainty about the identity of individuals  where mcmc samples over sets of equivalence classes of objects. in both cases  we identify types of probability distributions that allow local decomposition of inference while encoding possible domains in a plausible way. we apply our algorithms to simple examples and show that the mcmc approach scales well.
1	introduction
recent work in ai has made clear the advantages to be derived from combining probability theory with  at least some of  the expressive power of first-order logic  wellman et al.  1 . we will call languages that exhibit such a combination first-order probabilistic languages  fopls . the ability to handle objects  relations  and quantification gives such languages a huge advantage in representational efficiency in certain situations. to take a purely logical example: the rules of chess can be written in about one page of prolog but require perhaps millions of pages in a propositional language. a recent thesis on first-order probabilistic languages  pfeffer  1  describes a battlespace management system involving potentially thousands of objects whose relationships are unknown and changing.  pasula et al.  1  describe a freeway traffic surveillance involving probabilistic inference about the identities and properties of thousands of vehicles. these applications would be infeasible without some ability to specify and reason with fopl knowledge bases.
모the semantics of fopls are based on the idea that each modelof a fopl knowledgebase shouldbe viewedas a probability measure over the possible worlds  logical models  defined by the constant  function  and predicate symbols of the knowledge base  halpern  1 . although  wildly undecidable  in full generality  highly restricted fopls appear to be practical  especially with finite models. two threads have arisen  based on semantic networks  e.g.   koller and pfeffer  1   and logic programming  e.g.   sato and kameya  1  .
모in this paper  we focus on the family of relational probability models  rpms   pfeffer  1   although our ideas apply equally to other languages. rpms  like semantic networks  are based on classes containing instances  with each instance possessing attributes.  see section 1 for details.  rpms allow one to specify probability distributions over the attribute values of an instance  either directly or via inheritance from classes. these distributions may depend on other attribute values of the instance or of other instances. for example  a phd student's success may depend on the fame of his or her advisor.
모 pfeffer et al.  1  describe an exact inference algorithm for rpm knowledge bases called structured variable elimination  sve . roughly speaking  sve applies the variable elimination algorithm  zhang and poole  1  to a dynamically constructed bayesian network whose nodes are all those groundpropositionalvariables defined by the knowledgebase and relevant to the current query. sve derives an efficient variable ordering from the structure of the knowledge base and reuses computation results where possible. it is often able to answer queries involving hundreds of variables in a few seconds.
모despite sve's excellent performance its runtimeis at least exponential in the size of the largest clique in the optimal triangulation of the network. the expressive power of rpms makes it very easy to construct knowledge bases whose corresponding bayesian networks have very large cliques. for example   pfeffer  1  describes a model for matches in a sports league. the knowledge base consists of a single  generic conditional distribution for the outcome of a match given the quality of the two teams  a single prior distribution for the quality of the teams  and the results of some matches. if every team plays every other  then the team qualities form a clique in the correspondingbayesian network and the inference cost is exponentialin the numberof teams. similar problems will arise in any application in which there are complex relationships among large numbers of objects-i.e.  precisely those domains for which fopls are really necessary.
모the situation is exacerbated when the rpm language is extendedto allow for structural uncertainty-i.e. uncertainty about which probabilistic dependencies actually exist. we consider two forms of structural uncertainty:
reference uncertainty: the value of a relational attribute may be uncertain-e.g.  we may not know which of two professors is the advisor of a certain student;
identity uncertainty: we may not know whether two objects in the knowledgebase are the same-e.g.  when we see a red bus at two different camera locations on the freeway.
each of these extensions may lead to bayesian networks whose size alone causes difficulties and whose high connectivity makes exact inference completely impractical. for example  the inference problem in the freeway surveillance application of  pasula et al.  1  is known to be #p-hard  i.e.  almost certainly exponential in the number of vehicles.
모the solution proposed by  pasula et al.  1  is to use a markov chain monte carlo  mcmc  algorithm  see section 1 for details . the algorithm samples from possible matchings among vehicles  converging  in some cases polynomially  to approximately correct probabilities. often a few hundred samples suffice for a state space of states. the states being sampled are essentially the possible worlds defined by the constant symbols  observed vehicles  and predicates  equality  of the knowledge base. this approach- sampling possible worlds with mcmc-can be turned into a general inference algorithm for first-order probabilistic languages  as suggested by  russell  1; pfeffer  1 .
모in this paper  we investigate mcmc on possible worlds as an inference method to handle reference uncertainty  section 1  and identity uncertainty  section 1 . we show how the possible worlds may be constructed dynamically and how the transition probabilities may be computed efficiently. for the case of transitions involving a referentially uncertain relational attribute value  we identify a large family of conditional distributions that render the calculation independent of all but the particular values involved in the transition. we illustrate the algorithms using simple examples and give experimental results suggesting that the algorithms scale well.
1	relational probability models
the following definitions are adapted from  koller and pfeffer  1 . a relational probability model  in its most basic form  consists of
a set of classes denoting sets of objects  related by subclass/superclass relations.
a set of named instances denoting objects  each an instance of one class.
a set of complex attributes denoting functional relations. each complex attribute has a domain type and a range type .

figure 1: the simple rpm defined in the text  with its associated conditional probability models.

figure 1: bayesian network structure generated from the rpm in figure 1. the advisor relationship  being certain  doesn't appear.
a set of simple attributes denoting functions. each simple attribute has a domain type and a range that is a finite  enumerated set of values .
a set of conditional probability models for the simple attributes. is the set of 's parents  each of which is a nonempty chain of  appropriately typed  attributes   where is a simple attribute. probability models may be attached to instances or inherited from classes.
for now  we will assume that the values of all the complex attributes are known  no reference uncertainty   and that every instance is distinct  unique names assumption  hence no identity uncertainty . thus  a possible world is defined by the values of the instance variables-the simple attributes for all named instances.
모consider the following very simple rpm. there are two classes  student and professor  and two instances  student and prof . there is one complex attribute  advisor  mapping student to professor  and the advisor of student is prof . there are three simple attributes  all boolean:
success of a	and fame and $$  funding level  of a
         . for any student   success has one parent  advisor fame  with an appropriate conditional distribution. for any professor   fame has no parents and a simple prior distribution  while $$ depends on fame. figure 1 shows the knowledge base with probability distributions and figure 1 shows the bayesian network structure for the attribute variables definable from the knowledge base.
1	markov chain monte carlo algorithms
mcmc  gilks et al.  1  generates samples from a posterior distribution over possible worlds by defining a markov chain whose states are the worlds and whose stationary distribution is . in the metropolis-hastings method  henceforth m-h   transitions in the markov chain are constructed in two steps:
given the current state   a candidate next state is generated from the proposal distribution   which may be  more or less  arbitrary.
the transition to is not automatic  but occurs with an acceptance probability defined by

it is not necessary that all the variables of state be updated simultaneously  in a single transition function. singlecomponentm-h alters each variablein turn. it is also possible to factor into separate transition functions for various subsets of variables. provided that is defined in such a way that the chain is ergodic  this transition mechanism defines a markov chain whose stationary distribution is .
모the gibbs sampling algorithm for bayesian networks  pearl  1  is a special case of metropolis-hastings in which the proposal distribution samples a single variable using the distribution mb   where mb
denotes the current values of the variables in the markov blanket of  its parents   children   and children's other parents . in this case  the acceptance probability is always 1. one can show easily that
	mb	 1 
gibbs sampling is very simple and also local: transitions are generated referring only to parts of the model directly connected to the variable in question. hence  the cost per transition is typically independent of model size. m-h sampling is also typically local because all the parts of the model that are not changed by the transition cancel in the ratio . in particular  if the proposal concerns a single variable   this ratio reduces to mb mb   where is the proposed value of and is its current value. the m-h algorithm  unlike gibbs  has the added advantage that the transition may often be computed without referring to the other values of at all  as we will see.
1	reference uncertainty
reference uncertainty arises whenever relations among objects  as described by complex attribute values  are not known with certainty. for example  we may be unsure as to which of three professors is student 's advisor. we need to be able to describe this uncertainty and to specify the dependencies that influence it. the following definitions are adapted from  pfeffer  1 :
with each complex attribute   we associate a simple reference attribute   such that

figure 1: bayesian network structure with reference uncertainty about student advisor. the reference attribute is shown in a double oval.
is a finite  enumerated set of named instances.  where no confusion arises  we may drop the and use the attribute name itself.  dependencies are expressed as before by a conditional distribution
	. the parents of	are those
attributes or attribute chains that influence the choice of an instance as the value of attribute .
reference uncertaintymodifies the definition of attribute chains. suppose that an attribute depends on the parent chain . any of the complex attributes in the chain may be uncertain. then the parent variables for are all the instance variables reached by the chain for all possible combinations of values of all the uncertain complex attributes  as well as all the reference variables for those attributes.
the simple example of figure 1 can be extended to include reference uncertainty  if student advisor is unknown. we define a reference attribute student advisor with range  say  prof prof prof . the choice of advisor depends  generically  on the funding  prof $$  of each candidate  which depends  generically  on prof fame. this gives the instance-variable network structure shown in figure 1. obviously  when reference attributes have many possible values  very large implicit network structures can result.
1	exact inference with reference uncertainty
as mentioned above  the runtime of any variable elimination algorithm is exponential in the size of the largest clique in the optimally triangulated graph. looking at figure 1  it is apparent that a straightforward application leads to two  large  cliques: one containing student success and its parents  and one containing student advisor and its parents. in general  these will have and members respectively  where is the number of possible values for the reference attribute. thus  inference cost grows exponentially with this number.
모 pfeffer  1  observes that at least one of these cliques- the one associated with student success-can be decomposed. given a known value for student advisor   student success does not depend on the fame of other professors  figure 1 . this is an extreme form of context-specific independence  and allows the -variable factor to be replaced by a product of 1-variable factors in the variable elimination process. this decomposition applies in general

figure	1:	the	network	structure	conditioned	on
student	advisor	prof .
to variables that have referentially uncertain parents.
모unfortunately  the clique associated with the reference attribute itself- student advisor in this case-cannot be decomposed. we are left with a network structure that seems  in general  to be intractable for exact inference. the situation becomes worse still as more complex interactions occur among several reference attributes.
1	mcmc inference with reference uncertainty
in our approach  we extend the mcmc algorithm commonly used on bayesian networks by augmenting the markov chain state space to include reference variables denoting relational uncertainty  and defining appropriate transition functions. simple attributes  such as student success  use gibbs sampling as usual-and context-specific independence can sometimes be used to simplify those steps further. reference attributes  such as student advisor   depend on a slightly more involved m-h step.
the gibbs steps
in what follows  we abbreviate student	to	  advisor to
  and so on. let us consider an extended version of fig-
ure 1 with potential advisors. let the current value of be . suppose we want to apply gibbs sampling to  prof fame   where . by equation  1   the sampling
distribution is
$$
$$
$$
the first step of this derivation uses the fact that depends only on the fame of 's advisor    and not on other professors; the second step simply observes that is constant w.r.t. . thus  when
the reference attribute is instantiated  sampling operates exactly as if the links from non-selected parents were nonexistent. the same holds when sampling a child of the reference variable  e.g.   . thus  while reference variables remain constant  the network has a simplified form  such as that shown in figure 1. mcmc sampling on this simpler network should converge quickly. when the value of the reference variables changes  however  so does the effective structure of the network. we now address this issue.
the metropolis-hastings steps
gibbs sampling for a reference variable with values involves considering possible network structures  so we apply m-h sampling instead. m-h proposes a single new value for the reference variable  and then decides whether to accept it.  we can ignore the proposal distribution  which we will assume for now is uniform and hence cancels.  for the transition from to   then  we need  from
equation  1   and simplifying based on known values of the reference variable  the ratio
	$$	$$

	$$	$$
at first sight  it seems that calculating this ratio requires accessing the current values of $$ $$  i.e.  the funding levels of all possible candidate professors  even though the transition involves just two of them.  this is because the probability of picking any one advisor does depend on the funding levels of all candidates.  in turn  this requires that all those nodes be constructed and instantiated  which we prefer to avoid if possible.
모it so happens  fortunately  that conditional distributions for  selecting  a value for a relational attribute  given properties of a set of candidates  may have some structural properties that simplify the task. suppose  for example  that for all  
$$
	$$	$$ 
$$
for some arbitrary function . then  in the transition probability ratio given above  the summations $$ cancel  leaving
$$

$$
which does not mention any values for the reference variable besides and . the property of conditional distributions that we require is satisfied by some well-known conditional distributions  including the softmax distribution:

softmax is indeed a reasonable model for selecting an advisor based on funding.
the complete algorithm
once we put the two types of steps together  the complete algorithm alternates between ordinary gibbs steps on a simplified network and m-h steps altering the network structure. it should be noted that different network structures may result in different sets of variables being  at any given time  relevant or irrelevant to the query. our approach restricts computation to variables that are strictly relevant  zhang and poole  1  according to the current structure.
모a further possible enhancement is lazy construction of the bayesian network. the network can  e.g  be grown  from the query outwards   with nodes added as they are needed to sample a node currently in the network.  structural variables are  like others  instantiated on creation  yielding a network

figure 1: mcmc convergence on a simple example with three professors and one student. the exact probability is 1  horizontal line . each point represents the estimate from one of 1 markov chains after a given number of samples  log scale . the dotted line shows the average of the 1 individual estimates.
in the usual simplified format.  if necessary  nodes may then be discarded when they become irrelevant to the query under the current network structure. by using an  intelligent  m-h proposal  as discussed in section 1  we can also focus computation on variables that are  important  to the query- perhaps visiting only an infinitesimal fraction of the potentially relevant variables-without sacrificing the guarantee of convergence to correct probabilities in the limit. this should permit us to apply our approach to very large  or even infinite  knowledge bases.
1	experimental results
to check our algorithm  we applied it first to the knowledge base with reference uncertainty  no identity uncertainty  and three professors. in this case  the algorithm generates and evaluates the network shown in figure 1. we set values for fame or funding for each professor  and queried the posterior probability of student success. in this tiny model  we can compute the exact value for comparison. the mcmc estimates in figure 1 clearly converge to the correct value.
모we also tested the scalability of the algorithm by trying it out on various types of networks of increasing size. as an example  let us consider networks with professors and students  each of whom could have any one of the professors as advisor. we specified the success  or otherwise  of students and queried the success of the last. figure 1 shows the inference cost as a function of the total network size. because we cannot compute the exact values for all   we use a standard convergence diagnostic due to  gelman  1   checking against the exact values for small . the resulting graph appears to be linear in the size of the network. it would appear that  for this type of network  the sampling algorithm scales well. note also that cost is measured in terms of singlevariable state transitions performed by the algorithm  hence the number of transitions per variable to reach convergence is approximately constant  regardless of network size. this

figure 1: mcmc convergence as a function of network size. the
 -axis measures the number of state transitions required to reach a preset convergence threshold for a standard diagnostic. the straight line is a regression fit to the data  which is averaged over 1 trials.
held in all our experiments  including experiments  not reported here  on genetic inheritance with uncertain parentage  where the number of cascaded reference variables increases with .
1	identity uncertainty
standard relational probability models  pfeffer  1  incorporate the unique names assumption  i.e.  the assumption that each instance present in the knowledge base corresponds to a different object in any given possible world. when this assumption is removed  we must consider the possibility that several instances may denote the same object. identity uncertainty is especially prevalent in settings where an agent perceives multiple objects over time  pasula et al.  1   but also occurs in almost all real databases where  duplicate  records abound.
모with identity uncertainty  a  possible world  must specify not only the attribute values for all objects  but also the mapping from instances in the knowledge base to objects in the possible world.  without identity uncertainty  this mapping is one-to-one and hence no distinction between instances and objects is needed.  we represent this mappingusing an equivalence relation-a set of equivalence classes  each of which contains all the instances that co-refer in that particular possible world.1
모consider the following example  a simplified version of the data association problem studied by  pasula et al. 
1 . there are two classes 	and	.
there is one complex attribute 		  which maps an	to the	that generated it.	each of a vehicle reports a	  which depends
 through a probabilistic model of the sensing process  on the of the corresponding . instances are added to the kb as follows: whenever a vehicle is detected at some

figure 1: bayesian network structure for the two-vehicle case  where the vehicles are currently assumed to be one and the same object.
sensor  an is instantiated with its colour set to the measured value  and with a new instance attached. in this domain  inference involves reasoning about which s are  in fact  identical.1
모figure 1 shows the bayesian network structure generated when the knowledge base contains observations and of vehicles and   and when the equivalence
relation on instances is set to  i.e.  and are the same object .
1	defining a probability distribution
each possible world now contains an equivalence relation as well as the attribute values  and the knowledge base must now specify a unique probability distribution over this enlarged space of worlds. we can write
	attribute values	attribute values	 1 
given each fixed   identity uncertainty is eliminated  and the probability distribution over the attributes can be defined as before. all that remains is to specify the prior . when doing so  we can use the fact that instances in disjoint classes cannot co-refer-for example  vehicles cannot be observations. can thus be factored into terms dealing with each of the classes. even then  however  the state space of each factor is exponential in the number of objects in its class. fortunately  there are more compact ways of expressing the
 -distribution for each class. the simplest is to give an explicit distribution over the number of objects in each class  which can be done by adding a number attribute to classes in the knowledge base. in more complex situations  such as full data association  the -distributions are specified implicitly in conjunction with object arrival and detection models  pasula et al.  1 . the general topic of modular specification of -distributions is likely to require a good deal of further research.
1	inference with identity uncertainty
in the presence of identity uncertainty  exact inference calls for a summation over all possible values of . this is clearly infeasible when is large-and is exponential in the number of instances in identity-uncertain classes. mcmc permits us to replace the summation with a sample. the algorithm now works as follows. in addition to  normal transitions   which run as before given a fixed   the algorithm performs  identity transitions  in which changes. because of the large state space of   the latter type of transition uses m-h steps. moreover  these steps propose changes not only to   but also to some of the conventional attributes.
모the reason for this is that an identity transition can propose  among other things  that two currently separate instances be grouped together in the same equivalence class. clearly  if these two instances currently have different values for the same attribute  the probability of the destination state will be zero. hence  identity transitions will have a chance of occurring only when the instances  line up -that is  a sequence of normal transitions causes all of their attribute values to match exactly. when instances have many attributes  line-up may be very rare  resulting in a low acceptance ratio and a slow algorithm. moreover  there are cases in which waiting for line-up makes the markov chain non-ergodic-the algorithm can be trapped in a subset of possible s with no escape.
identity transition proposals
consider a current assignment	  and a proposed assignment
모. let be the set of unobserved attributes of all the objects in affected by the transition  and let be the unobserved attributes of the corresponding objects in . let be the set of all other attributes within the markov blankets of the attributes in or : this may include observed attributes  as well as the unobserved attributes of other objects not affected by the transition. note that all attributes in remain fixed throughout the transition.
each identity transition proceeds as follows:
   is chosen using a proposal . this proposal can take manyforms: onesimple possibility is to suggest that a randomly selected instance move from one equivalence class to another. another method might suggest merging two equivalence classes  or splitting an equivalence class in two.
	values for all of	are chosen using a proposal
모모모모모모모. some possible choices for will be explained below.
the proposed change is accepted with probability

which is derived from equation  1  using equation  1  and the two-step nature of our proposal.
모as an example  let us consider the world state portrayed in figure 1. in this situation 
a split can now be proposed as follows:
to ensure that the chain is ergodic  new values must then be proposed by for at least the newly created attributes.
choosing the attribute values
one possible proposal picks a value uniformly at random from the values available at each attribute. this approach is simple to implement and yields an acceptance ratio that is easy to calculate. the terms reduce to .
this is the algorithm that we test experimentally below.
모unfortunately  this proposal may suggest very unlikely value combinations for the attributes  resulting in low acceptance ratios. we might be better off with a more intelligent proposal mechanism  such as likelihood weighting which generates samples from together with weights . since

the fraction in the acceptance ratio can now cancel to give

this ratio is no longer so simple  as performing the summations may be quite time-consuming. fortunately  whenever the set of uncertain attributes can be split into several d-separated sets  the summations can be rewritten as products of summations over these sets. if the sets are all small  this algorithm may be feasible.
모yet another approach is to run a  local  mcmc algorithm inside to generate attribute values that reflect the current values of and hence are likely to be accepted.  when this method is used  care must be taken to ensure that the chain is run until convergence: stopping it prematurely may result in biased answers. 
1	experimental results
to check our identity uncertainty algorithm  we first performed experiments analogous to those in section 1. we began with a problem that was small enough for exact calculation: it consisted of five observations with known attribute values  and queried the posterior probability of the vehicle generating the first observation. figure 1 shows that our algorithm's estimates converge to the correct value over time.
모we then tested the scalability of the algorithm by  once more  applying it to randomly generated networks of increasing size  and measuring time to convergence using a standard diagnostic. the results shown in figure 1 were obtained by constructing larger and larger sets of vehicle-observation pairs  and requesting a posterior distribution over the number of vehicles. as before  the algorithm appears to scale well.
모finally  we performed an additional experiment to demonstrate that our algorithm works as expected. it should be true that  if observations are generated from a specific set of vehicles  increasing the number of observations in the knowledge base will enable us to model characteristics of that set of vehicles with increasing accuracy. figure 1 shows that adding more and more observations does enable us to infer the number of vehicles responsible for their generation.

figure 1: mcmc convergence on a simple example with five observed vehicles. the exact probability is 1  horizontal line . each point represents the estimate from one of 1 markov chains after a given number of samples  log scale . the dotted line shows the average of the 1 individual estimates.

figure 1: mcmc convergence as a function of network size. the
 -axis measures the number of state transitions required to reach a preset convergence threshold for a standard diagnostic. the straight line is a regression fit to the data  which is averaged over 1 trials.

figure 1: distributions over the number of objects in the network. the prior distribution is shown; note that it insists that there are at least two vehicles. observations are then generated from exactly two vehicles. when given just two observations  the algorithm converges to a distribution close to the prior  as expected. as more observations are added  the result moves closer to the true value of two vehicles.
1	conclusions and further work
we have proposed an algorithmic approach for inference in first-order probabilistic languages  based on markov chain monte carlo. our preliminary investigations suggest that the approach is very promising. we believe that it can significantly increase the expressive power of languages that can be considered  practical  for knowledge representation and reasoning under uncertainty. for example  the lazy exploration approach should make it possible to handle infinite recursive models  including temporal models  with somewhat greater generality than so far envisaged  koller and pfeffer  1 .
모the metropolis-hastings algorithm allows for a wide variety of proposal distributions. in particular  intelligent proposals can be used to focus computation. one can easily construct data-driven and query-driven proposals  analogous to forward and backward chaining in logical systems  that essentially result in  activation  spreading outwards from query and evidence variables. these concepts can be subsumed by a general mechanism for proposing transitions based on the expected value of computation  russell and wefald  1 . we can also insert arbitrary domain-specific knowledge into the proposal mechanism-for example  proposing candidate advisors based on the student's research interest. such knowledge-based proposals  which result in faster convergence  can also be learned via so-called adaptive proposals- that is  allowing the proposal distribution to change over time based on the algorithm's experience in generating samples and computing acceptances.
모clearly  we have just scratched the surface of this topic. a large effort is needed to apply first-order probabilistic languages to real problems  in order to identify useful language features  common representation structures  and their effect on inference. having a flexible and general-albeit sometimes slow-mcmc inference engine should help with this task. finally  we also need to develop complexity results for approximate inference  using tools such as those provided by  jerrum and sinclair  1 .
references
 gelman  1  andrew gelman. inference and monitoring convergence. in w. r. gilks  s. richardson  and d. j. spiegelhalter  editors  markov chain monte carlo in practice  pages 1. chapman and hall  london  1.
 gilks et al.  1  w.r. gilks  s. richardson  and d.j. spiegelhalter  editors. markov chain monte carlo in practice. chapman and hall  london  1.
 halpern  1  j. y. halpern. an analysis of first-order logics of probability. artificial intelligence  1 :1  1.
 jerrum and sinclair  1  m. jerrum and a. sinclair. the markov chain monte carlo method. in d. s. hochbaum  editor  approximation algorithms for np-hard problems. pws publishing  boston  1.
 koller and pfeffer  1  d. koller and a. pfeffer. probabilistic frame-based systems. in proceedings of the fifteenth national conference on artificial intelligence  aaai-1   madison  wisconsin  july 1. aaai press.
 koller and pfeffer  1  d. koller and a. pfeffer. semantics and inference for recursive probability models. in proceedings of the seventeenth national conference on artificial intelligence  aaai-1   austin  texas  july 1. aaai press.
 pasula et al.  1  hanna pasula  stuart russell  michael ostland  and ya'acov ritov. tracking many objects with many sensors. in proceedings of the sixteenth international joint conference on artificial intelligence  ijcai-1   stockholm  sweden  august 1. morgan kaufmann.
 pearl  1  judea pearl. probabilistic reasoning in intelligent systems: networks of plausible inference. morgan kaufmann  san mateo  california  1.
 pfeffer et al.  1  a. pfeffer  d. koller  b. milch  and k.t. takusagawa. spook: a system for probabilistic object-oriented knowledge representation. in uncertainty in artificial intelligence: proceedings of the fifteenth conference  stockholm  august 1. morgan kaufmann.
 pfeffer  1  avrom j. pfeffer. probabilistic reasoning for complex systems. phd thesis  stanford university  stanford  california  1.
 russell and wefald  1  stuart j. russell and eric h. wefald. do the right thing: studies in limited rationality. mit press  cambridge  massachusetts  1.
 russell  1  stuart russell. expressive probability models in science. in proc. of the 1nd int'l conf. on discovery science  tokyo  japan  december 1. springer verlag.
 sato and kameya  1  t. sato and y. kameya. prism: a symbolic-statistical modeling language. in proceedings of the fifteenth international joint conference on artificial intelligence  ijcai-1   pages 1  nagoya  japan  august 1. morgan kaufmann.
 wellman et al.  1  m. p. wellman  j. s. breese  and r. p. goldman. from knowledge bases to decision models. knowledge engineering review  1 :1  march 1.
 zhang and poole  1  n.l. zhang and d. poole. exploiting causal independence in bayesian network inference. journal of artificial intelligence research  1-1  1.
knowledge processing under information fidelity 
wilhelm r dder 
fernuniversit t gesamthochschule in hagen 
모모d-1 hagen  germany wilhelm.roedder fernuni-hagen.de  
 
abstract 
xspirit is a professional expert system-shell for knowledge acquisition  inference and response using conditional logic and probability. composed conditionals on propositional variables with finite domain are the communication tool between the user and the knowledge base  making the process of acquisition  inference and query comfortable and intelligible. xspirit allows partial rather than complete information about the knowledge domain and supplements missing parts by the principle of information fidelity. by virtue of evident temporary information  knowledge undergoes a well-defined adaptation process  respecting this principle again. the construction and transformation of probability distributions as developed here  allow acquired knowledge  remaining uncertainty and strength of inference to be measured in the information units  bit . xspirit allows large-scale applications with hundreds of composed conditionals and umpteen variables.  
1 introduction 
humans memorise and deduce facts  in a conditional environment .  if a then b  is a constituent of human knowledge. such knowledge pieces together with concatenation mechanisms build a highly netted memory  the knowledge base. once situativ things become evident  our knowledge undergoes a complex adaptation process resulting in conclusions. conclusions cause reactions or response. 
 probability distributions on event fields are a suitable means to model such deduction processes in a knowledge base. bayes nets are the graphical structure which might help to organise knowledge and the corresponding distributions  and make them manageable  cf.  lauritzen and spiegelhalter  1  or  pearl  1 . bayes nets allow to break down the global distributions to local marginals  at the cost of conditional independence assumptions which rarely meet reality. inference in bayes nets then is the technically complex but unpretentious calculation of conditioned probabilities. we do not follow this idea  but want to enable the user to formulate any local knowledge pieces without forcing him to construct a directed acyclic graph. after an automatic aggregation of these pieces to a knowledge base he can adapt this knowledge to any situativ modification. 
 thus  different from the established form of a probability model  we shall discuss an inference process which   allows partial rather than complete information about the probability distribution. the missing parts are completed by the principle of information fidelity which turns out to coincide with the principle of minimum relative entropy. 
  permits inference from virtual evidence. vague assumptions about the actual situation can be processed as well as sure things. 
  allows the user to  communicate  with the distribution in the comfortable language of composed conditionals. 
 there is a lot of related work to our paper in literature. to minimize relative entropy when adapting the dependency structure in a distribution to new information was discussed already in early works  jaynes  1    cheeseman  1   and justified axiomatically  shore and johnson  1    paris and vencovsk뺙  1    csisz뺙r  1    kernisberner  1 . even the idea to use such adaptation for inference is not new  see  cheeseman  1    h뺙jek et al.  1    goldszmidt et al.  1    r dder and meyer  1 .  
 all probabilistic inference presented here  however  is of purely information theoretic nature. the quantity of acquired knowledge can be measured in  bit   as well as the strength of evidence. knowledge is reduced uncertainty  inference means uncertainty adaptation to new information. and even query and response are information demand and supply.  
 in section 1 we develop the language of composed conditionals as a communication tool between user and knowledge base  in section 1 we analyse the relations between probability and information  in section 1 we go through the entire process of knowledge acquisition  inference and response  as realized in the expert system shell xspirit. a small example emphasises the conceptual difference between bayes nets and the inference method developed here. section 1 gives a short technical introduction to the shell and invites the reader to test it. 
1. conditionals and probability 
1 conditionals in a three-valued logic 
concerning the semantics we consider a population of individuals or objects with their respective properties. our intention is the description of relations between these properties. as far as the syntax concerns we model the knowledge domain as a finite set of finite-valued variables v = {v1 ... vl} and the respective event algebra f on the elementary events v = v1...vl  here vl are values of vl. as usual   is the all- and   the empty event. 
 each event from f can be identified in a natural way with a propositional sentence  built from literals vl = vl by the connectives negation  conjunction and disjunction  cf.  r dder and kern-isberner  1 . simple conjuncts of such literals we often write as unordered tuples of the respective values  v1...vl  for instance. hence each event may be expressed in an intelligible way  which we shall demonstrate by example 1 below. we do not distinguish between events and propositional sentences  further on. 
 generic events from f or the corresponding propositions we denote as capital letters  indexed if necessary: a  b  ...   
ai  bi  ...   ei  fi. if v   a   we call the proposition a true in the world v and false  otherwise: a v  = t or a v  = f. if a is true  false  on the entire    we write a = t  a = f . the set of all propositional sentences forms the language l. 
 calabrese  calabrese  1  and earlier de finetti  definetti  1  go a step further and define conditionals b|a for b and a from l. calabrese in  calabrese  1  and  calabrese  1  derives a rich language of composed conditionals. we go through that in more detail now. the exact definition of a conditional reads 
        t	if	v   ab  = a뫇 b  b a    v =   f	if	v   ab  = a뫇 b  	 1  
	  u	if	v   a.
 barring means negation and t rue   f alse   u ndefined  are values in a three-valued logic  cf.  rescher  1 . in this logic the connectives  뫇 뫈 apply for t  f as usual and for u as follows: 
t 뫇u = t뫈 u = t   f 뫇u = f 뫈 u = f   u 뫇u = u 뫈 u = u   u = u. 	 1  
 the truth-value undefined does not change neither t  f nor u under conjunction or disjunction. 
 the set of all conditionals b |a is the language l|l. identification of b |  with b embeds l in l|l. similar to above we write b |a = t |a  = f |a  if the conditional is true  false  on the whole a. 
following calabrese further  we compose conditionals by 
  뫇 뫈 and | to get a very rich linguistic structure for the communication between the user and the knowledge base  as shall be developed in the remainder of this section.  
 with the conventions in reschers three-valued logic  1   the author in  calabrese  1  defines pointwise: 
    ba 뫇 dc     v =  ba    v 뫇 dc    v   
    ba 뫈  dc     v =  ba    v 뫈 dc    v   	 1  
   ba    v =  b a    v . 
 by these pointwise attributions any conjunct  disjunct or negation of conditionals is well-defined also globally. 
 let the conditional operator | apply as in the following scheme: 
     read e.g. f |u = f . 	 1  
with this convention a pointwise definition of a conditioned conditional reads: 
    b | a  d |c      v =  b a    v   dc     v . 	 1  
 pointwise attribution of truth-values again permits the global definition of conditioned conditionals on  . more on the hierarchy of composed conditionals the reader might find in calabrese's original paper  as well as the proof of the following theorem 1. theorem 1 permits the reduction of composed conditionals to simple ones. 
theorem 1  composed conditionals  	 
with the conventions in  1    1  and the definitions in  1    1  the following equations hold: 
   b a 뫇  dc =   b 뫈 a   d 뫈 c    a뫈c   
   ba 뫈  dc =  ab뫈cd   a뫈c   
   b a    dc = b a d 뫈c  . 
composed conditionals are reducible to simple ones  but in their composed form allow an immense linguistic richness  as demonstrates the following example. 
example 1  linguistics and composed conditionals   with the use of mnemonic letters the following propositions should be self explicatory: 
    m  c1 뫇c1   뫇  c1 뫇c1   m      v   
in the world v machine m is o.k. iff the component c1 is o.k. and c1 is o.k. 
     g 뫈 a 뫇 b g    s     v   
in the world v herr schmid is german or austrian  if he is german he comes from berlin. 
    c 뫈  v  c   | l     v   
in the world v mr. lee is chinese  if not  he is vietnamese. 
 the reader might notice that we can make global considerations rather than  in the world v   too. in a certain population or knowledge context  the machine m is o.k. iff c1 and c1 are o.k. with a probability of 1%  say. probabilities of conditionals are constituents of our aiconcept. more on that in the next sections  cf. also  r dder  1 . because of theorem 1 it suffices to consider simple conditionals of the form b |a further on. bear in mind  however  that they represent the total hierarchy of arbitrarily composed conditionals and their respective linguistic pendants.  
 in what follows we define sets of conditionals with certain properties. these properties make them  a conditioning partition  on  .  
definition 1  c-independent and disjoint conditionals  
i.  	b |a is  conditionally or c-independent of d |c  iff  b |a | d |c  = b |a  and c-dependent otherwise. ii.  b |a and d |c are disjoint iff abcd = f. 
definition 1  conditional basic set cbs  	 
i.  a set s of conditionals is sufficient for    if any v     can be expressed as a conjunct of elements from s.  
ii.  a conditional basic set cbs is a set of conditionals sufficient for   such that  for any b |a  d |c 뫍 cbs one of three statements holds:    b |a and d |c are disjoint     b |a is c-independent of d |c     d |c is c-independent of b |a. 
 the following example gives an idea of conditional basic sets. 
example 1  conditional basic sets  i.  	{v = v1...vl} is a cbs. 
ii.  {v1}뫋{v1 v1}뫋...뫋{vl vl 1...v1}is a cbs. note that conditionals  more to the right  are c-independent of those  more to the left   if they include no distinct values vl 뫛 vl'. if they do include distinct values  they are disjoint. any v can be resolved into  factors  v = v1 뫇 v1 v1 뫇...뫇 vl vl 1...v1 . to show this  use 
 1  and  1 . 
iii.  {vl...v1}뫋{vl...vl+1 | vl... }v1 is a cbs  as shows a similar reasoning like that in ii . 
iv.  ii  or iii  for an arbitrary permutation l1...ll of 1....l is a cbs. 
 conditional basic sets will serve as a means for measuring the intrinsic uncertainty in a probability distribution on   and this measure will turn out to be independent of the special choice of the cbs. 
1 the probability of conditionals 
a probability measure p on   can be generalized to conditionals by the natural requirement  that for the conjunct of c-independent conditionals the probabilities should multiply. as b |a is c-independent of a and because  b |a  뫇 a = b a  it follows immediately p b a  = p  b |a 뫇a  = 
p b |a    p a  and hence p b |a  = p b a  / p a   if only p a    1. the probability of a conditional is its conditioned probability. 
from the logical factorisation  v = v1 뫇 v1 v1  뫇...뫇 vl vl 1...v1   cf. example 1   
we get the well-known probabilistic factorisation p v  = p v1  p v1 v1  ... p vl vl 1...v1  . 
probabilistic independence reflects the logical-based cindependence  as it should be. 
1. probability  uncertainty and information 
1 basic considerations  
entropy first was established by the physicists carnot and clausius in thermodynamics. it measures the portion of thermal energy which cannot be transformed back into mechanical energy. when the engineer shannon  shannon  1  founded information theory  he looked for an adequate denotation of h   p =  뫉v p   v  ld p   v   where ld 
is the dual logarithm and 1  ld 1 = 1. in this theory signals v vary in a finite alphabet v 뫍 v and h measures the average information rate which can be transmitted from a source via a channel to a receiver. john v. neumann suggested to name it entropy  following an anecdote in the scientific american  horgan  1  because  shannon would have an advantage in debates about this theory.  
 in the remainder of this section we shall study entropy further. 
 if we learn that a proposition a with probability p a  becomes true  we receive the information -ld p a   bit . information is reduction of uncertainty  any textbook of information theory goes through a proof of this fact. we learn that a conditioned proposition b |a becomes true if ba becomes true  cf.  1 . though the reduced uncertainty is not -ld p ba  but rather  -ld p ba  -  -ld p a  . this result comes from the additivity of uncertainty reduction for cindependent conditionals. we put it as a lemma and leave the obvious proof to the reader. 
lemma 1  information of c-independent conditionals  if for any c-independent b |a of d |c or d |c of b |a  the uncertainty reduction inf  b |a 뫇 d |c  equals inf  b |a  + inf  d |c   it follows inf  b |a  = -ld p ba  -  -ld p a  . 
 let us now consider a similar situation as before  but - for whatever reason - our situativ conviction of b |a to become true  has changed from p b |a  to q b |a . this change is 
change of uncertainty and the difference equals  	 
 inf =  ld p b | a    ld q b | a   = ld  q b | a / p b | a    cf. figure 1. 
	-ld p b|a 	q b|a 
	b|a is true	 
figure 1: sequential information gain 
note that the expectation of this uncertainty change is 
q ba  ld  q b | a / p b | a    as q ba  is the probability of b |a to become true in the light of our new conviction. 
1 information adaptation 
in this section we study the adaptation of a distribution p to new information  supplied in the form of conditionals from l|l and their respective desired probabilities. to distinguish between such conditionals and the elements of a cbs  we use lowercase letters f |e for the latter. there should be no problems with this notation.  
 the following theorem shows that the overall expected uncertainty change does not depend on a specific cbs. 
theorem 1  unique uncertainty change  	 
let cbs and cbs' be two arbitrary conditional basic sets with generic conditionals f |e and f ' |e'   respectively. let q and p be two distributions on    such that q is totally continuous with respect to p. then the following equality holds  1   ld  1  =1 :  
 뫉q  fe  ld  q  f | e / p  f |e   = f |e뫍cbs
 	뫉q  f 'e'  ld  q  f '| e' / p  f '| e'  . f '|e'뫍cbs' proof: 
뫉q  fe  ld  q  f | e / p  f | e   =
f |e뫍cbs
 뫉 뫉    q v     ld  q  f | e / p  f | e   = f |e뫍cbs  v  fe  
 
뫉q v     뫉 ld  q  f | e / p  f | e     .
	v	  f |e  with v  fe	 
for each v the inner summation runs through conditionals  which are not disjoint and hence pairwise cindependent  one independent of the other . because a cbs is sufficient we even have   v = 뫇 f | e . 
v  fe
as furthermore logical independence implies probabilistic independence we get   
 
뫉q v  ld 뫊  q  f | e / p  f | e   v f|e with 뫇 f |e=v
 
	and finally 뫉v뫍 q v  ld  q v / p v  . 	 
 
the cbs was chosen arbitrarily  which completes the proof. 
 
 the basic idea of the proof was to show that either side of the equation equals 뫉v뫍 q v  ld  q v / p v  . 
this expression is known as relative entropy r q p  between p and q or directed divergence from p to q. expected uncertainty change from p to q in any cbs equals the relative entropy between p and q. 
1
if p = p is the uniform distribution  h q  = ld | | - 
1
r q p   is the  absolute  entropy of q. thus the uncertainty 
1
change from p to q is also ld | | - h q . both r and h measure in  bit . 
 as mentioned at the beginning of the present section we now study the problem of adapting the distribution p to new information supplied as a set of conditionals with desired probabilities xi 뫍  1 :   ={bi | ai xi }. 
 we demand this adaptation to be such that the former uncertainty structure in p is preserved as far as possible  i.e. we solve 

p = arg min r q p  s.t. q bi | ai  = xi   i = 1...i  	 1  

and call p the    -adaptation of p. as  1  preserves the expected uncertainty reduction for any cbs as far as possible  it obeys the principle of information fidelity. there are axiomatic accesses to conditional knowledge adaptation  cf.  csis뺙r  1    shore and johnson  1    paris and vencovsk뺙  1    kern-isberner  1 . the objective of our approach is to focus upon a cautious information work up. with the presentation of all logical  probabilistic and information-theoretic prerequisites we are now ready to develop knowledge processing as realized in xspirit. 
1 knowledge acquisition    inference and response 
probabilistic knowledge processing in xspirit is performed in the four steps: initialisation  knowledge acquisition  inference and response. we shall go through these steps in detail now. 
initialisation 
here we commit ourselves to a knowledge domain by defining the variables vl and their respective values vl . this determines   = {v} and the event algebra f. the uniform p1 on f means absolute ignorance on f  as the variables vl are mutually independent from each other. 
knowledge acquisition  
the knowledge engineer supplies  composed  conditionals and their respective desired probabilities   = {bi|ai  xi    i = 1...i}. bi|ai  xi   i = 1...i is the imperative to adapt p1 to 
  . as developed in the last section we solve  
p* = arg min r q p1  s.t. q bi | ai  = xi   i = 1...i.  1  p* is the adapted knowledge following the principle of 
                               1 information fidelity. r p*  p   measures the knowledge increment in  bit . remind that minimizing relative entropy is equivalent to maximizing entropy  and verify the equation 
	1
h p   - r p*  p   = h p* . actual uncertainty is maximum uncertainty minus acquired knowledge. 
inference 
the user supplies  composed  conditionals and their respective temporarily evident probabilities   = {fj|ej  yj    j = 1...j}. fj|ej  yj   j = 1...j is the imperative to find a probability distribution which meets evidence. evidence is not additional knowledge to be acquired  but describes a temporary reflection about things that might happen.  if very likely with y = 1 % a person's nationality were german  which conclusions could we infer from that   evident conditionals and their probabilities in almost all cases are contradictory to the knowledge supplied by   ! of course in our knowledge domain there is no 1 % probability to be german  in general. because of the observations in the previous section we calculate 
p   = arg min r q p   s.t. q fj | e j   = y j   j = 1...j.  1  
 p** has minimal directed divergence from p*  subject to evidence. among all q  p** is the distribution with the minor change of expected uncertainty relative to p*. r p** p*  is the strength which evidence puts upon p* to infer p**  and measures in  bit . 
response 
the user wants  in the evident situation  the knowledge base to answer a question  which also might be of the conditional form  h |g . the response is p** h |g . response is inferred from the knowledge base   and from evidence  . 
 while  1  and  1  is inference  as  former  knowledge is adapted to new information in a sophisticated transformation process  response is not. response is the calculation of a conditional probability and hence the mere evaluation of a distribution. 
 
 w. r dder and g. kern-isberner in  r dder and kernisberner  1  discuss the example lea somb뺝 of people in a fictitious society  which originally was presented in  somb뺝  1 . 
 
example 1  lea somb뺝   
in a fictitious society we consider the properties: to be a student or not s = s / s ; to be young or not y = y / y ; to have the marital status single  married or corporate life m = s  m  c; to be parent or not p = p/ p . the following information is given: 
  about 1 % of all students are young  
  about 1 % of all young people are single  
  1 % of singles are young  
  1 % of young people study  
  members of the society which live in corporate lives  are young with 1 %    students with children  in 1 % of all cases are married or live in corporate lives. 
 in the notation of the present paper we have the conditionals: 
y = ys = s    .1  	m = sy = y   .1  	y = y m = s   .1  
s = sy = y    .1  	y = ym = c    .1  	m = s s = s 뫇 p = p    .1 .
 
 after solving  1  to determine p*  we want to make conclusions about lea somb뺝 who evidently is a student and has a child. is she young  calculating p** in  1  under certain evidence and evaluating p** y=y  yields 1 %. lea somb뺝 very likely is young. 
 the information in p* only counts 1  bit   the remaining uncertainty about the fictitious society is high  namely 1  bit . imposing evidence on p* results in an information flow of 1  bit . all results were calculated with the shell xspirit. 
 besides of its simplicity example 1 is not accessible for bayes nets  for two reasons: 
  information about the society is incomplete inasmuch as it does not fully determine a probability distribution    supplied conditioned probabilities do not build a directed acyclic graph. 
 you can handle virtual evidence and augment the number of variables and conditionals almost arbitrarily. xspirit is able to treat hundreds of composed conditionals on umpteen variables  as we shall relate in the next section. 
1 the shell xspirit 
 the shell xspirit 1  a java-version  is a professional tool for knowledge processing under information fidelity. to acquire knowledge the user defines variables and supplies conditionals in a comfortable syntax. the syntax permits conjunctions  disjunctions and negations of literals and their respective composed conditionals  in an arbitrary hierarchy. variables might be boolean  nominal and ordinal. the attributes might be associated with real numbers  utilities  allowing the calculation of expected utility for respective decision models. the shell supports an interactive process to untie inconsistent information if provided by the user. for an application to on-line banking and credit worthiness confer  kulmann and reucher  1   for a business-to-business-approach see  kulmann and r dder  1 . 
 once all conditionals and their respective probabilities are supplied to the system  xspirit automatically generates a 
suitable graphical structure  a hypergraph  and builds the knowledge base p* by virtue of corresponding marginal distributions on the hyperedges -sometimes called legs  local events groups . in the presence of evidence xspirit supplies p** and evaluates any query  asked by the user. all communication is performed in the rich syntax of composed conditionals. p* and p** are calculated by a generalised iterative proportional scaling  ips  procedure  whose convergence was proved by csisz뺙r  csisz뺙r  1 . the system at any time informs about the amount of information already acquired and the remaining uncertainty in  bit . for more mathematical details see  r dder and meyer  1 . 
 bayes nets are importable and xspirit also provides a frequentistic learning process. if knowledge about the domain under consideration is available as a sample of elementary events rather than estimated probabilities xi   the shell adopts this information in a separate module. 
 the reader interested in the shell xspirit might visit the homepage http://www.xspirit.de. 
references 
 calabrese  1  p. g. calabrese: deduction and inference using conditional logic and probability  in: conditional logic in expert systems  i. r. goodman  m. m. gupta  h. t. nguyen and g. s. rogers  editors . elsevier science publishers b. v.  1 - 1   1 . 
 calabrese  1  p. g. calabrese: theory of conditional information with applications  in: special issue on conditional event algebra  ieee transactions on systems  man  and cybernetics  vol. 1  no. 1  1 - 1   1 . 
 cheeseman  1  p. cheeseman: a method of computing generalized bayesian probability values for expert systems  proc. 1th int. joint conf. on ai  ijcai - 1   karlsruhe   1 . 
 csisz뺙r  1  i. csisz뺙r: i-divergence geometry of probability distribution and minimisation problems. the annals of probability  1  no.1  1 - 1   1 .  
 csisz뺙r  1  i. csisz뺙r: why least squares and maximum entropy  an axiomatic approach to inference for linear inverse problems  the annals of statistics  1  1   1 - 1   1 .  
 definetti  1  b. de finetti: induction and statistics  wiley  new york   1 . 
 goldszmidt et al.  1  m. goldszmidt  p. morris  j. pearl: a maximum entropy approach to nonmonotonic reasoning  ieee transactions on pattern analysis and machine intelligence  1  no. 1  1   1 . 
 h뺙jek et al.  1  p. h뺙jek  t. havr뺙nek  r. jirousek: uncertain information processing in expert systems  crc-press   1 . 
 horgan  1  j. horgan: claude e. shannon  unicyclist  juggler and father of information theory  profile in: scientific american  1 - 1   1 . 
 jaynes  1  e.t. jaynes: where do stand on maximum entropy   in: the maximum entropy formalism  r.d. levine and m. tribus  editors   cambridge mass.  mitpress   1 . 
 kern-isberner  1  g. kern-isberner: characterising the principle of minimum cross-entropy within a conditional-logical framework  artificial intelligence  vol. 1  1 - 1   1 . 
 kulmann and r dder  1  f. kulmann and w. r dder: probabilistische modellbildung auf der basis von scoring-schemata  proc. gor operations research  dresden  1   1 . 
 kulmann and reucher  1  f. kulmann and e. reucher: computergest뺯tzte bonit tspr뺯fung bei banken und handel  die betriebswirtschaft  dbw   1  1   1 . 
 lauritzen and spiegelhalter  1  s.l. lauritzen and d. j. spiegelhalter: local computations with probabilities in graphical structures and their applications to expert systems  journal of the royal statistical society  1  1   1 - 1   1 . 
 pearl  1  j. pearl: probabilistic reasoning in intelligent systems  morgan kaufmann  san mateo cal.   1 . 
 paris and vencovsk뺙  1  j.b. paris  a. vencovsk뺙: a note on the inevitability of maximum entropy  int. j. of approximate reasoning  1  1   1 . 
 rescher  1  n. rescher: many-valued logics  mc craw-hill  new york   1 . 
 r dder  1  w. r dder: conditional logic and the principle of entropy  artificial intelligence  1  1 - 1   1 . 
 r dder and kern-isberner  1  w. r dder and g. kernisberner: l뺝a somb뺝 und entropie-optimale informationsverarbeitung mit der expertensystem-shell spirit  or spektrum  1  1 - 1   1 . 
 r dder and meyer  1  w. r dder and c.-h. meyer: coherent knowledge processing at maximum entropy by spirit  proceedings 1th conference on uncertainty in artificial intelligence  e. horvitz and f. jensen  editors   morgen kaufmann  san francisco cal.  1 - 1   1 . 
 shannon  1  c. e. shannon: a mathematical theory of communication  bell system tech. j.  1  1 - 1  part i   1 - 1  part ii    1 . 
 shore and johnson  1  j. e. shore and r. w. johnson: axiomatic derivation of the principle of maximum entropy and the principle of minimum cross entropy  ieee trans. information theory  1  1   1 - 1   1 . 
 somb뺝  1  l. somb뺝: schlie en bei unsicherem wissen in der k뺯nstlichen intelligenz  vieweg  braunschweig  wiesbaden   1 .  

constraints as data: a new perspective on inferring probabilities
manfred jaeger
mpi informatik
stuhlsatzenhausweg 1  1 saarbru몮cken  germany jaeger mpi-sb.mpg.deabstract
we present a new approach to inferring a probability distribution which is incompletely specified by a number of linear constraints. we argue that the currently most popular approach of entropy maximization depends on a  constraints as knowledge  interpretation of the constraints  and that a different  constraints as data  perspective leads to a completely different type of inference procedures by statistical methods. with statistical methods some of the counterintuitiveresults of entropy maximization can be avoided  and inconsistent sets of constraints can be handled just like consistent ones. a particular statistical inference method is developed and shown to have a nice robustness property.
1	introduction
probabilistic representations of uncertainty usually consist of a single probability distribution over a large  but finite  domain of possible states . it is thus required to assign a probability value to each state . usually  a direct  full assessment of all these values is very difficult or impossible. all one usually is able to obtain are partial descriptions of by constraints of e.g. the form
            	  or  	and	are independent   where	are subsets of	. such constraints can be derivedby knowledgeelicitation from an expert  by direct observations of the domain  or by any other information gathering process.
a	set	of	constraints	defines	the	set of probability measures on	that are con-
sistent with the constraints. very rarely will consist of a single probability distribution. instead  it will either contain more than one element  or be empty  i.e.  the constraints are inconsistent . a fundamental problem in probabilistic reasoning then is to select from the admissible set a distribution sel as the best guess for the true distribution the constraints describe.
모this measure selection problem is well studied in the literature  particularly for the case where the constraints are linear and consistent. it is almost unanimously suggested that in this case one should select the distribution with maximal entropy from  shore and johnson  1; lemmer and barth  1; jaynes  1; cheeseman  1; paris and vencovska뫣  1; r몮odder and meyer  1 . a more general class of constraints is considered by drudzel and van der gaag  who then employ the center of mass selection rule  according to this rule one selects the center of mass of the admissible region .
모in this paper we propose a new selection rule which is radically different from either maximum entropy or center of mass. it is motivated by the observation that in spite of the very compelling justifications it has been given  shore and johnson  1; jaynes  1; paris and vencovska뫣  1   maximum entropy selection has some rather counterintuitive properties. these are illustrated by the following examples.
example 1 overhearingtwo strangers talking at an airport  we hear the first one saying   jones got at least 1% of the votes    and the second replying   smith didn't get any less than 1% either  . before the two disappear in the crowd  we also hear them both agreeing on the fact that if anyone else had bothered to run for mayor  then neither smith nor jones would have had a chance of winning the election. suppose  now  that we need to assess the probability smith of an arbitrary voter in the unnamed home town of the two strangers having voted for smith. the information we have establishes a lower bound of 1 and an upper bound of 1 on smith . moreover  we have learned that the relevant underlying state space only consists of smith and jones. if we base our probability assessment on entropy maximization  then we will obtain smith . intuitively  this assessment appears to be overly optimistic from smith's point of view.
example 1 for the construction of a medical diagnosis system ten different experts are asked for bounds on the two crucial conditional probabilities stylosis
polycarpia   and xylopserosis anameae . assume that 1 and 1 are the greatest lower bound and smallest upper bound  respectively  mentioned by any expert for
모. having complete confidence in the experts  we will then take it as given that the true value for lies in the interval  1 1 . let  1 1  be the correspondingly defined interval for . applying maximum entropy to find the best values for and for our expert system  we will determine
             . this appears somewhat counterintuitive because we have chosen the same value for both probabilities  even though the information provided would seem to indicate a smaller value for than for .
모the reasons why the maximum entropy solution appears counterintuitive in the two examples are very similar. in the first example  an equal percentage of 1% of votes for both smith and jones seems implausible  because the constraints are highly asymmetric. experience tells us that the disparity of the given lower bounds probably reflects a similar disparity of the actual values  which will rather be assumed to be approximately 1% for jones and 1% for smith. such an assessment could be based on a natural explanation for how the constraints were generated in the first place: one might suspect  for instance  that the constraints report the partial count of 1% of the votes  among which 1% were found to be for jones  and 1% for smith. in the second example it appears unlikely that the experts would systematically state larger upper and lower bounds for than for if these two probabilities were really the same.
모in both examples we have thus argued that the maximum entropy distribution is a counterintuitive solution of the selection problem  because the given constraints are unlikely to be observed when this is the true distribution. underlying this argument is a view of constraints that is fundamentally different from the view which  implicitly  underlies the use of the maximum entropy principle: entropy maximization is predicated on the view that the given constraints are just a description of a state of knowledge: the knowledge that the true distribution is a member of the admissible region defined by the constraints. we call this the constraints as knowledge perspective. in our examples - and  we would claim  in most cases where we encounter the measure selection problem - the given constraints are not only a description of our knowledge  they also are the source of our knowledge. they thereby carry not only the face-value information consisting of a restriction of the admissible region; they also carry the metainformation consisting of the fact that we observed exactly these constraints. this meta-information is relevant for the solution of the measure selection problem as it allows us to reason about the likelihood of observing the given constraints for different true distributions. we call the view of constraints that tries to take into account this meta-information the constraints as data perspective: constraints are seen as randomly sampled pieces of information. the distribution of this constraint data  the constraint distribution  is in part determined by the true distribution on the domain  the domain distribution   which we want to determine. in other words  the domain distribution is a parameter of the constraint distribution. our problem thus becomes a statistical one: to infer a parameter of a distribution from random samples drawn from that distribution.
모all statistical methods rely in part on considerations of likelihood. the most direct way to use likelihood is by maximum likelihood inference: select the parameter that gives highest probability to the observed sample. the measure selection rule we develop in this paper is likelihood maximization for the observed constraints. the main problem we face in a formal development of this intuitive principle is that statistical methods usually require a specific model on how the distribution of observed data depends on the parameter of interest  i.e. the stipulation of some underlying parametric family. our goal  however  is to define a general rule for measure selection that does not require any knowledge about the random mechanism that produces the constraints. our approach towards solving this dilemma is that of robust statistics: we do proposea specific model for the randomgenerationof constraints  but this model is chosen such that in the long run it will lead to correct inferences for a fairly wide class of constraint distributions.
모the constraints as data perspective coupled with statistical approaches to measure selection permits us to handle inconsistent sets of constraints just like consistent ones. our statistical model for the constraint observation only must allow for the observation of wrong constraints  i.e. constraints not satisfied by the true distribution  as an erroneous assessment given by an expert  the premature and incorrect report of an election result  etc. . such a model then assigns nonzero likelihoods to inconsistent sets of constraints  and a maximum likelihood solution can be found just as for consistent constraint sets.
모the idea of measure selection by likelihood maximization for the observed constraints was already expressed in  jaeger  1   but no concrete formalization of the idea was developed. the view of constraints as data has also been taken in somewhat different form by dickey   who proposed a model in which partial specifications of a probability distribution were treated as random variables with a distribution depending on . a major difference between dickey's and our work is that dickey does not consider partial specifications by arbitrary linear constraints  but only by values for a fixed set of  aspects  of . it is interesting to note that dickey takes it for granted that in most cases the specified aspects will overdetermine the model  i.e. be inconsistent  whereas authors in artificial intelligence assume underdetermined models.
모in this paper we can only give an overview of our maximum likelihood approach to measure selection. goal of this paper is to convey the main ideas  and to provide some insight into the feasibility of their mathematical development. more technical details  including proofs of the theorems here stated  will be given in a full technical paper.
1	the constraint sample space
to treat constraints as random samples we have to view them as elements of some sample space on which probability distributions can be defined. throughout we assume that the constraints refer to a distribution on a domain of elements. the set of all these distributions can be identified with
a linear constraint then has the general form
 1 
we	could identify this constraint with	its	parameters
             and thus take as our sample space. however  this would mean to view two equivalent constraints like and as different sample points. as it does not seem sensible that our method should depend on such representational variants of constraints  we prefer to distinguish constraints only according to the subsets of distributions they define. this can be done by writing constraints in a normal form
 1 
where is an element of the dimensional unit sphere
as every linear constraint  1  can be transformed into a unique normal form  1   we henceforwardidentify linear constraints with points   and model randomly observed constraints by probability distributions on .
모in the binomial case      a constraint  1  is a  nontrivial  lower bound on iff and ; it is a  nontrivial  upper bound iff and . the following definition generalizes this classification of constraints.
definition 1 a sign vector is any vector with components in . for we define sign as or   depending on whether     or . the sign vector sign for is the vector sign .
each sign-vector	of length	defines a sector	in	:
	sign	 1 
모the intuition behind this definition is that sectors contain constraints of the same qualitative type. the classification of constraints according to sectors gives rise to the following coarser  three-way distinction: a constraint is vacuous iff sign for all  a vacuous constraint is satisfied by all  ; is a support constraint iff sign for all  a support constraint is satisfied by all
whose set of support is a subset of sign  ; is proper iff sign and sign for some  a proper constraint divides the interior of   i.e. there exist int that satisfy   and int that do not satisfy
 .
모figure 1 illustrates constraints from different sectors. shown in the figure is the polytope with its 1 vertices corresponding to domain distributions that assign unit mass to one of the states in . six different constraints from three different sectors are representedby the halfplanes of points satisfying the constraint. halfplanes are shown by their boundary line  and a shading that indicates to which side of the boundary the halfplane extends.
모for the rest of the paper we make two simplifying assumptions. assumption 1: all constraints in the observed sample are proper. assumption 1: the model we want to determine lies in the interior int of . the two assumptions are somewhat connected. non-proper constraints are essentially constraints on the set of support of . thus  both assumptions will be satisfied if in an initial inference step we use all observed non-proper constraints to determine a set of support for our model  and then use the method we shall develop on the remainingproperconstraints to determine with figure 1: constraints from different sectors
that set of support. with these assumptions  the measure selection problem consists of finding selection rules in the following sense.
definition 1 a selection rule is any function that for every maps any tuple     of proper constraints to a set sel int .
this definition of a selection rule is not directly tied to constraints as data  and is very similar to paris and vencovska뫣's  notion of an inference process. it is very general in two respects: first  it is not required that sel consists of a unique point. while it is obviously desirable that a selection rule yields unique solutions as often as possible  one needs to take the possibility into account that no principled statistical method can guarantee unique solutions in all cases. second  it is not demanded that sel be a subset of  the distributions on that actually satisfy the constraints . such a demand  which is natural fromthe constraints as knowledgeperspective  is not required from the constraints as data perspective. to see why  recall that in order to deal with inconsistent constraint sets  and also for greater realism  we should work with probabilistic models according to which it is possible to observe false constraints. this means that even for consistent constraint sets we must take the possibility into account that it contains false constraints  and that therefore the true domain distribution does not belong to .
1	invariance and equivariance
a selection rule in the sense of definition 1 is a maximum likelihood selection rule  if it takes the following form: for every   and for every int a probabilitydistribution on is defined  and for a sample
of constraints we select the distributions in int that maximize the likelihood of the sample:
	sel	argmax	 1 
where	is the density function of	. usually  constraints will be assumed to be independent  so that with
 1 
모whenever some information about the random process that generates the constraints is available  then one will choose in  1  a family that is a plausible model for this random process. the question we shall be concerned with  however  is what to do when no particular information about the generation of the constraints is available. thus  we address the same inference problem as addressed by maximum entropy inference: input of the selection problem is a set of constraints  and nothing else.
모the justification of the maximum entropy principle  in broad outline  takes the following form: because there is no information except the constraints  one should select the domain distribution that encodes the least additional information beyond the face-value information provided by the constraints. minimal information content  in turn  is realized by distributions that  roughlyspeaking  maximize independencies and uniformity. our approach to dealing with the lack of information is somewhat similar  only applied to the constraint distribution: because we have no information on the family   we should assume the least specific structure of this family. in particular  we will assume that the constraints are independent  and that the family is homogeneous in   in a sense that will be formalized by the notion of -invariance  which is developed in this section.
모we derive the concept of -invariance from the semantic concept of a random constraint generating mechanism that works uniformly for all . additional support for the invariance assumption on will be given by the observation that maximum likelihood selection with respect to invariant families is -equivariant  and that this can be understood as the formalization of the intuitive principle that a uniformshift applied to all constraints should induce a similar shift of the selected distributions  cf. example 1 .
모to motivate the concept of a random constraint generating mechanism that works uniformly for all   reconsider example 1  and the subsequently given explanation of how the constraints might have been generated from a partial count of 1% of the votes. if the true distribution is indeed
	smith	jones	  then the
observation of the constraints follow as a result of a sequence of chance events: the partial count of 1% of the votes becomes known  this partial count happens to be an accurate projection of the full count  and two strangers happen to mention these partial counts in their conversation. this sequence of chance events does not depend on the distribution . if the true distribution was   then the same sequence of events would occur with the same likelihood  but now generate the constraints .
모generalizing from this example  we obtain the  yet informal  notion of a random constraint generating process that works uniformly for all : the constraint generating mechanism will produce a constraint when the true domain distribution is with the same likelihood as it will produce a constraint corresponding to when the true distribution is . to make this idea precise  we have to find a transformation on constraints that maps every to a corresponding   such that an observation of under the true distribution corresponds to an observation of under . the correspondence expressed by should preserve elementary qualitative properties of constraints. two natural preservation conditions are:
sector preservation: maps every sector bijectively onto itself.
implication preservation: for all	 	:
 1 
모sector preservation means that two corresponding constraints should be of the same qualitative type  as expressed by their membership in a sector. implication preservation says that logical relationships between constraints should be preserved. implication preservation is equivalent to the conjunction of two simpler conditions: for all
   and	 consistency preservation .
모the following definition introduces a class of transformations that satisfy both properties.
definition 1 let . the transformation	is defined by

we write	for the set	.
모it is obvious that transformations satisfy sector preservation. they also satisfy a slightly strengthened version of implication preservation. for this  denote by the set of all real solutions of  1   without the restriction to solutions  so that  . in analogy to
 1  we can then define global implication preservation of by the condition
 1 
with condition  1  we look at constraints as defining sets of real numbers  not sets of probability distributions. in our context condition  1  seems to be the more pertinent one. we nevertheless here introduce the global version  1   because with this version we can prove the following representation theorem.
theorem 1 let   . preserves sectors and is globally implication preserving iff .
모the theorem does not hold for . the proof is by reduction to a classical representation result in projective geometry which characterizes mappings that preserve collinearity. we may conjecture that the theorem also holds when the condition of global implication preservation is replaced by implication preservation in our preferred sense  1 . a proof of this modified theorem appears to be considerably harder  however.
모in light of theorem 1 we see the transformations as the adequate realization of the concept of correspondence of constraints. to relate this correspondence to different domain distributions  we define dual transformations on
.
first transformation
second transformation
third transformation
	figure 1:	-transformations and equivariant selection
definition 1 let . the transformation is defined by

we write	for the set	.
the mapping is dual to in that it is the only transformation of such that for all   :
 1 
모our initial intuition of corresponding observations of constraints now can be phrased as follows: the observation of constraint under the true domain distribution corresponds to the observation of constraint under the true domain distribution . figure 1 shows three different transformations of a set of three constraints  and the dual transformations of one probability measure inside the admissible region of the constraints. each of the three sets of constraints can be transformed into any of the other two sets by unique . the dual transformations at the same time transform the indicated points in into each other.
모with the transformations and we can now finally formalize the idea of a constraint generating mechanism that works uniformly for all :
definition 1 let int be a family of distributions on . the family is called -invariant if for all and all int :
 1 
모when the distributions are represented by densities relative to a suitably chosen underlying measure on   then  1  can be expressed by the condition
 1 
by using such appropriate density functions  it is immediate
that the maximum likelihood selection rule given by  1  and  1  becomes -equivariant in the sense of the following definition.
definition 1 a selection rule sel is called g-equivariant iff for samples of constraints  and every
sel	sel	 1  note that the concept of -equivariance does not  in turn  rely on maximum likelihood selection rules. indeed  independently from the constraints as data interpretation  equivariance captures the idea that when the given constraints undergo a shift in one directions  then the selected distributions should undergo a similar shift. in figure 1  a equivariant rule would have to select the distribution indicated by a cross given the solid constraints iff it selects the distribution indicated by a diamond given the dashed constraints iff it selects the distribution indicated by a box given the dotted constraints.
모a second homogeneity assumption one will make about in the absence of any information to the contrary is permutation invariance: if is any permutation of   then for all
 1 
maximum likelihood selection with respect to a permutation invariant family leads to a permutation equivariant selection rule:
	sel	sel	 1 
1	robust estimation
in the previous section we have argued that when no particular information about the constraint generating family is given  then reasonable assumptions on this family are and permutation invariance. these assumptions alone are not nearly sufficient to identify a unique such family: if is any distribution on that satisfies for all and all permutations   then gives rise to a - and permutation invariant family by letting
모모모모  where is the uniform domain distribution  and   where is the transformation uniquely determined by . conversely  every - and permutation invariant family is uniquely determined by its member   which has to satisfy
.
모in the following  we define a particular family by way of defining . the motivation for this family comes out of the robustness of maximum likelihood selection with respect to this family  theorem 1 . can be thought of as a mixture of multivariate laplace distributions that are separately defined on each sector. the usual multivariate laplace distribution on has a density that depends on the euclidean distance between and the mean of the distribution. to define laplace-like distributions on the sectors of   we first introduce a suitable metric on sectors:
definition 1 let	 	. define
	log		 1 
a density now is defined as a function of the distance between and a reference constraint   which can be thought of as the mean constraint in sector .
모in order for to satisfy for all permutations   the have to be chosen such that
모모모for all sign vectors and permutations . apart from this condition  no restriction has to be imposed on the choice of the in orderto obtainour robustness result. we therefore only assume at this point that some have been appropriately fixed  and define
	exp	 1 
the function is the density of a probability distribution   which induces a - and permutationinvariant family
     . the maximum likelihood selection rule sel	based on this family is distinguished by the following robustness property.
theorem 1 let . let be a - and permutation-invariant family of probability distributions on properconstraints such that for all propersectors
. let denote the distribution of an infinite sequence of independentconstraints drawn according to .
then
	lim	sel	 1 
the theorem says that in the long run we will select with probability 1 the correct distribution by using sel   even when the constraints are actually generated according to dis-
tributions . the conditions and make sure that with probability 1 sel will be a unique point for all sufficiently large . to obtain an analogous result for a mild additional condition on must be added. the proof of theorem 1 follows the proof of a general robustness result given as theorem 1 in  huber  1 .
모theorem 1 provides a good justification for using sel on  large  samples. it does not provide any guarantee that sel will show a sensible behavior on small samples. in particular  the behavior on small samples can be strongly affected by the special choice of the reference constraints . thus  the definition of sel and theorem 1 do not yet provide a full answer to the measure selection problem from the constraints as data perspective. to extend these first results towards a fully satisfactory solution  one will have to develop suitable criteria by which to judge the performance of a maximum likelihood selection rule on small samples  and to specialize or modify the definition of sel to obtain a selection rule that performs well according to these criteria  but retains the asymptotic behavior  1  .
1	conclusion
we have seen that an interpretation of constraints as data  not as knowledge  leads to a completely new perspective on the measure selection problem. this perspective calls for statistical methods of parameter estimation as the tool for measure selection. the key problem we then face is that statistical methods call for a statistical model for the data generation  but that  according to the traditional problem statement that we deal with  no information about the appropriate statistical model is given. we have argued that in the absence of any such information - and permutation invariance are natural homogeneity assumptions for the constraint distributions. is a relatively simple - and permutation invariant family of constraint distributions that leads to a robust maximum likelihood selection rule.
모future work will have two major directions: first  the definition of sel will be refined in order to obtain a sensible small sample behavior of the selection rule. second  it will be explored to which degree the assumptions made in theorem 1 on the constraint generating family can be relaxed without loosing  1  for sel .
