 
we present a new general framework for online probabilistic plan recognition called the abstract hidden markov memory model  ahmem . the new model is an extension of the existing abstract hidden markov model to allow the policy to have internal memory which can be updated in a markov fashion. we show that the ahmem can represent a richer class of probabilistic plans  and at the same time derive an efficient algorithm for plan recognition in the ahmem based on the raoblackwellised particle filter approximate inference method. 
1 	introduction 
the ability to perform plan recognition can be very useful in a wide range of applications such as monitoring and surveillance  decision supports  and team work. however the plan recognizing agent's task is usually complicated by the uncertainty in the plan refinement process  in the outcomes of actions  and in the agent's observations of the plan. dealing with these issues in plan recognition is a challenging task  especially when the recognition has to be done online so that the observer can react to the actor's plan in real-time. 
¡¡the uncertainty problem has been addressed by the seminal work  charniak and goldman  1 which phrases the plan recognition problem as the inference problem in a bayesian network representing the process of executing the actor's plan. more recent work has considered dynamic models for performing plan recognition online  pynadath and wellman  1; 1; goldmand et a/.  1; huber et ai  1; albrecht et a/.  1 . while this offers a coherent way of modelling and dealing with various sources of uncertainty in the plan execution model  the computational complexity and scalability of inference is the main issue  especially for dynamic models. 
inference in dynamic models such as the dynamic 
bayesian networks  dbn   nicholson and brady  1  is more difficult than in a static model. inference in a static network utilizes the sparse structure of the graphical model to make it tractable. in the dynamic case  the dbn belief state that we need to maintain usually does not preserve the conditional independence properties of the single time-slice network  making exact inference intractable even when the dbn has a sparse structure. thus  online plan recognition algorithms based on exact inference will run into problems when the belief state becomes too large  and will be unable to scale up to larger or more detailed plan hierarchies. 
¡¡in our previous work  we have proposed a framework for online probabilistic plan recognition based on the abstract hidden markov models  ahmm   bui et a/.  1 . the ahmm is a stochastic model for representing the execution of a hierarchy of contingent plans  termed policies . scalability in policy recognition in the ahmm is achieved by using an approximate inference scheme known as the raoblackwellised particle filter  rbpf   doucet et al 1. 
it has been shown that this algorithm scales well w.r.t. the number of levels in the plan hierarchy. 
¡¡despite its computational attractiveness  the current ahmm is limited in its expressiveness  in particular  its inability to represent an uninterrupted sequence of plans and actions. this is due to the fact that each policy in the ahmm is purely reactive on the current state and has no memory. this type of memoryless policies cannot represent an uninterrupted sequence of sub-plans since they have no way of remembering the sub-plan in the sequence that is currently being executed. in other words  the decision to choose the next sub-plan can only be dependent on the current state  and not on the sub-plans that have been chosen in the past. other models for plan recognition such that the probabilistic state dependent grammar  psdg   pynadath and wellman  1; pynadath  1  are more expressive and do not have this limitation. unfortunately  the existing exact inference method for the psdg in  pynadath  1  has been found to be flawed and inadequate  bui  1 . 
the main motivation in this paper is to extend the existing 
ahmm framework to allow for policies with memories to be considered. we propose an extension of the ahmm called the abstract hidden markov memory model  ahmem . the expressiveness of the new model encompasses that of the 
user modeling 	1 psdg  pynadath and wellman  1   thus the new model removes the current restriction of the ahmm. more importantly  we show that the rbpf approximate inference method used for the ahmm can be extended to the more general ahmem as well  ensuring that the new generalized model remains computationally attractive. to the best of our knowledge  we are the first to provide a scalable inference method for this general type of hierarchical probabilistic plan hierarchy. 
¡¡the paper is structured as follows. section 1 provides a more detailed discussion of the ahmm  psdg and related models for online probabilistic plan recognition. the ahmem is introduced in section 1 and the algorithms for plan recognition are presented in section 1. experimental results with a prototype system are provided in section 1. finally  we conclude and discuss directions for future work in section 1. 
1 related models for online probabilistic plan recognition 
in the ahmm  bui et al  1; 1   an agent's probabilistic plan is modeled by an abstract markov policy  amp . an amp is an extension of a policy in markov decision processes  mdp  defined within a subset of the environment state space so that it can select other more refined amps and so on to form a hierarchy of policies. the amp is thus similar to a contingent plan that prescribes which sub-plan should be invoked at each applicable state of the world. the noisy observation about the environment state can be modelled by making the state  hidden   similar to the hidden state in the hidden markov models  rabiner  1 . the stochastic process resulting from the execution of an amp is termed the abstract hidden markov model. intuitively  the ahmm models how an amp causes the adoption of other policies and actions at different levels of abstraction  which in turn generate a sequence of states and observations. in the plan recognition task  an observer is given an ahmm corresponding to the actor's plan hierarchy  and is asked to infer about the current policy being executed by the actor at all levels of the hierarchy  taking into account the sequence of observations currently available. this problem is termed policy recognition  bui et al  1 . 
¡¡scalability of policy recognition in the ahmm is achieved by using a hybrid inference method  a variant of the rao-blackwellised particle filter  rbpf   doucet et al  1 . when applied to dbn inference  raoblackwellisation  casella and robert  1  splits the network into two sets of variables: the set of variables that need to be sampled  termed the rao-blackwellising  rb  variables   and the set of remaining variables whose belief state conditioned on the rb variables need to be maintained via exact inference  termed the rao-blackwellised  rb  belief state . the rbpf thus allows us to combine sampling-based approximate inference with exact inference to achieve efficiency and improve accuracy. 
¡¡the probabilistic state-dependent grammar  psdg   pynadath  1; pynadath and wellman  1  can be described as the probabilistic context free grammar  pcfg   jelinek et al  1   augmented with a state space  and a state transition probability table for each terminal symbol of the pcfg. in addition  the probability of each production rule is made state dependent. as a result  the terminal symbol now acts like primitive actions and the non-terminal symbol chooses its expansion depending on the current state. the ahmm is equivalent to a special class of psdg where only production rules of the form x  yx and a   are allowed. 
the first rule models the adoption of a lower level policy y by a higher level policy a'  while the second rule models the termination of the policy x. the psdg model considered by pynadath and wellman allows for more general rules of the form x  i.e.  the recursion symbol must be located at the end of the expansion. thus in a psdg  a policy might be expanded into a sequence of policies at the lower level which will be executed one after another before control is returned to the higher level policy. 
¡¡although more expressive than the ahmm  the existing computational method for inference with the psdg remains inadequate. pynadath proposed an exact method for updating the belief state of the psdg in a  compact  closed form. the proposed algorithm seemingly gets around the exponential blow up in the size of the belief state. unfortunately  the derivation of the algorithm is based on a flawed assumption that the higher levels in the belief state are independent of the lower levels given the current level. for more details about the flaw in the inference algorithm for psdg  interested readers are referred to  bui  1. 
¡¡the ahmm  psdg  and the proposed ahmem are related to the hierarchical abstract machines  ham   parr  1  used in abstract probabilistic planning. in this model  the policy is represented by a stochastic finite automaton  which can call other automata at the lower level. despite their representational similarity  the computational techniques for ahmem and related models are intended for plan recognition whereas the ham model is used for speeding up the process of finding an optimal policy for mdp. 
¡¡if we ignore the state dependency  the dbn structure of the ahmem and psdg is similar to the structure of the hierarchical hidden markov model  hhmm   fine et al  1; murphy and pashkin  1 . however  while the hhmm is a type of probabilistic context free grammar  pcfg   the ahmem and psdg are not due to the state dependency in the model. 
1 	the abstract hidden markov memory models 
this section introduces the abstract hidden markov memory models  ahmem   an extension of the ahmm where the policy can have internal memory. our main aim is to construct a general model for plan recognition whose expressiveness encompasses that of the current ahmm and psdg models  while retaining the computational attractiveness of the ahmm framework. we first define the ahmem in subsection 1. the dbn structure of the new model is given in subsection 1. 
1 	the model 
1 	user modeling consider an mdp-like model with s representing the states of the environment  and a representing the set of primitive actions available to the agent. each action a € a is defined by its transition probability from the current state s to the next state s': oa s s' . the set of abstract policies will include every primitive actions. furthermore  the ahmm  bui et al  1  defines higher level abstract policies on top of a set of policies as follows: 
¡¡an amp as defined above is purely reactive  in the sense that it selects the next policy at the lower level based only on the current state s. this restricts the set of behaviours that an amp can represent. for example  it will not be able to represent a plan consisting of a few sub-plans  one followed by another regardless of the state sequence. to represent this kind of plans  the agent needs to have some form of internal memory to remember the current stage of execution. let m be a set of possible internal memory states. we first extend the definition of our policy to include a memory variable which takes on values in m and is updated after each stage of execution of the policy.1 
definition 1  abstract markov policy with memory 
 ampe  . let ii be a set of ampes  an ampe  * over ii is defined as a tuple 	where: 
would stop if the current state is d and the current memory value is 1. 
 is the policy selection prob-
ability  a   .s  m    is the probability that selects the policy   at the state s and memory value  
 1  is the initial distribution of is the probability that the ini-
tial memory is m if  * commences at state s. 
 is the memory transiis the probability that the distributions  i.e.  l and 	1. 
¡¡when an ampe is executed from a state .s  it first initialises its memory value m according to the distribution 
       then a policy at the lower level will be selected according to the distribution this policy will be executed until it terminates at some state at the new state .s'  the policy itself will terminate with probability 
         if it does not terminate  the memory variable will be given a new value m' according to the transition probability then a new policy at the lower level  is selected with probability  and so on. 
¡¡using the ampes  we can construct a hierarchy of abstract policies in the same way as in the ahmm. we start with a set of primitive actions = a  and for k = 1   . . .   k build a set of new ampes on top of then  if a top-level policy  is executed  it invokes a sequence of level- k-l  policies  each of which invokes a sequence of levei- k-1  policies and so on. a level-1 policy will invoke a sequence of primitive actions which leads to a sequence of states. we can then introduce the hidden states and model the noisy observation of the state by an observation model  the dynamic process of executing a top-level ampe is termed the abstract hidden markov memory model  ahmem . 
¡¡some special cases of the ahmem are worth mentioning here. first  the ahmm itself is a special ahmem. the memoryless policies of the ahmm are equivalent to ampes where the dependency on the memory variable is ignored  e.g. when m is a singleton set . the class of psdg considered in  pynadath  1  can also be easily converted to an ahmem. the terminal symbols in the psdg are equivalent to the primitive actions. each non-terminal symbol is equivalent to a memoryless policy. in addition  each sequence encountered on the rhs of a production is equivalent to a 
policy whose memory m taking on the values 1   . . .  n. y then simply selects the  memoryless  policy  
¡¡note that in the ahmem definition  we assume a balanced policy hierarchy for the ease of presentation  all the actions must appear at the same bottom level . however  we can also specify an unbalanced hierarchy by introducing some dummy policies which arc equivalent to primitive actions at the higher levels in the hierarchy. 
1 	dbn representation of the ahmem 

next memory value is m' given that the current memory value is m and the current state is .s. 
¡¡subsequently  we will drop the subscript confusion about the policy in the context. note that all the states in d   s are called terminal states and thus  
 also  the policy selection  memory 
initial and transition probability have to be proper probability 
   'one can argue that wc can always incorporate the memory variable in the environment state  and hence we gain no extra representational power just by introducing the memory variables. however  incorporating the memory variables in the state variable would blow 
up the size of the state space and thus defeat our purpose of keeping the model computationally feasible. 
user modeling 	1 


policy termination and selection 
the policy termination and selection model for the a h m e m is essentially the same as in the ahmm  except for the dependency on the value of the current memory variable  we note that the same context specific independence  boutilier et a/.  1  properties of the ahmm still hold. for example  if 
     = f then = f and becomes independent of the remaining parents of this variable. now consider the variable 
	= f then 	and 	is independent of the 
remaining parents; otherwise   
	and 	is independent of  
memory update 
consider the variable there are two parents of this node that act like context variables: and if = f  the policy at the lower level has not terminated and memory 
is not updated at this time. thus  and becomes independent of all the remaining parents. if = t 
1 	independence properties in the ahmem 
even though ampes are more expressive than memoryless policies  they remain  autonomous   in the sense that the higher layers have no influence over the state of an ampe during its execution. the only way the higher layers can influence the current state of an ampe is through the conditions at the start: either through the starting state or the starting time. thus  the conditional independence theorem for policies in the ahmm still holds in this more general setting. we state the theorem for the ahmem below. the proof for the ahmm  bui et ai  1  can be directly extended to this general case using the context specific independence properties described in the previous subsection. 
note that k n o w i n g i s equivalent to 
knowing precisely when each of the policies starts and ends. therefore  given  the starting time and state of every current policy are known. the following corollary is thus a direct consequence of theorem 1. 
corollary 1. let ct represent the conditional joint distribution  then ct has the following 
bayesian network factorization: 

¡¡ct thus also has the following undirected network representation. we first form the set of cliques:  
note that the set of 
cliques co-k form a chain of cliques in this order  therefore we term ct the policy-clique chain. this extends the concept of the policy chain in the memoryless case of the ahmm  bui et al  1 . ct can be factored into the product of poten-
network factorization  the potentials are said to be in canonical form. any potential representation of the clique chain can be canonicalized by first perform message passing  exact inference  to compute the marginal at each clique. the canonical form can then be computed directly from these marginals. later on we will use the undirected representation of ct for exact inference  and the canonical form  directed representation  of ct for obtaining samples from the joint distribution using simple forward sampling. 
1 	approximate inference for ahmem 
in this section  we look at the online inference problem in the ahmem. assume that at time f  we have a sequence of observations about the environment state 
= we need to compute the belief state of the dbn which is the joint distribution of all the current variables given this observation sequence: 
  from this  we can answer 
various queries about the current status of the plan execution. 

1 	user modeling 

for example  the marginal probability of tells us about the current policy the actor is executing at some level k; the probability p r t e l l s us about the current stage of execution of a policy the probability pr  tells us if  will end after the current time  etc. 
¡¡since there is no compact closed form representation for the above belief state  exact inference in the structure of the ahmem is intractable when k is large. however  theorem 1 suggests that we can apply the raoblackwellised particle filter  rbpf  to this problem in a similar way as in the ahmm  bui et al.  1   i.e. by using rt as the rao-blackwellising  rb  variables. the raoblackwellised  rb  belief state is then similar to the original belief state of the ahmem  except that now are known: = pr 	note that the rb belief state can be obtained directly from the policy-clique chain ct by adding in the network representing the conditional distribution of ot and   - 
pr 	pr oi|st ct seefig.1 . 
¡¡two main steps in the rbpf procedure are:  1  updating the rb belief state using exact inference  and  1  sampling the rb variable rt from the current rb belief state. 
1 updating the rb belief state 
fig. 1 shows the modified 1-time-slice dbn when the rb variables are known  we note that all the nodes from above level / remain unchanged  while all the links across time slices from level / and below can be removed. this greatly simplifies the network structure  allowing the updating operations to be performed efficiently. 
¡¡since the bt can be obtained directly from ct  all we have to do is to compute  from ct. the procedure for updating as usual has two stages:  1  absorbing the new evidence  i.e. from ct  we need to compute - pr  and  1  projecting to the new time slice  i.e. from we need to compute ct+ . 
¡¡in principle  we apply a simplified version of the junctiontree algorithm uensen  1  on the undirected network representation of ct to perform the update step. this is in fact a generalization of the arc-reversal procedure which operates on directed network representation of the policy chain in the ahmm  bui et al  1 . the algorithm for updating the rb belief state is given in fig. 1. the absorbing step involves simply incorporating the evidence likelihood into the potentials of ct to obtain the potentials for ct+. the projecting step involves first adding time-slice t + 1 to ct+  see fig. 1   then marginalizing the now redundant variables in the old time slice. the marginalization is done by performing message passing between the cliques of the 1-time-slice network shown in fig. 1. 
¡¡since the potentials of ct from level / + 1 to level k' stay unchanged  the complexity of the algorithm is 1  where / is the highest level of termination. furthermore  if one of these potentials is in canonical form  it remains in canonical form after the updating procedure. 
1 	rbpf for ahmem 
the full rbpf algorithm for policy recognition in the ahmem is provided in fig 1. the general structure is the 

figure 1: two time slices of the rb belief state 
same as the rbpf procedure for ahmm  bui et al  1 . at each time step t  the algorithm maintains a set of n samples  each consists of a value for st~i and / /   i   and a parametric representation of ct the differences are in the details of how to obtain new samples and update the rb belief state. 
¡¡to obtain each new sample  st lt   we first need to canonicalize the potentials of ct. however  assuming that ct-  is in canonical form  we only have to canonicalize the potentials of ct between level 1 and level i + 1 with complexity o 1 . thus the complexity of the sampling step is o nl . since the complexity of the updating step is also o nl   the overall complexity of the algorithm at each time step is o nl . furthermore  the distribution for i usually decays exponentially  thus the average complexity is only o n . 
¡¡if at some time t  an estimation e.g. pr required  we need to compute h = for each sample. this involves performing message passing for the entire chain ct. thus the complexity for this time step is 1 nk . other types of queries are also possible  as long as the probability required can be computed from the rb belief state. for example  we can ask the question: if the actor is currently executing  what is the current stage of execution of this policy  to answer this query  we need to compute the conditional probability this can easily be achieved by replacing the ft function in the algorithm with 
1 	experimental results 
we have implemented the above algorithm in a surveillance domain to demonstrate its working in a practical application. the environment consists of a spatial area which has two separate rooms and a corridor monitored by a set of 1 cameras  fig. 1 . the monitored area is divided into a grid of cells  and the cell coordinates constitute the overall state space 1. the coordinates returned by the cameras are modelled as the noisy observations of the true coordinates of the tracked person  and over time  provide the sequence of observations o  ;t. 

user modeling 	1 


t 
¡¡at the top-level  each policy models a type of activity performed by the person. for example  print is a policy that figure 1: rbpf for ahmem invoked by the print policy. these results are obtained by running the rbpf algorithm with 1 samples. the average processing time for each observation is approximately 1 seconds on a 1ghz desktop machine. for a more detailed description of this surveillance system  readers are referred to  nguyen et al.  1 . figure 1: the environment and a person's trajectory cies to have internal memories which are updated in a markov fashion. this allows the ahmem to represent a richer set of hierarchical probabilistic plans  including those belonging to the class of psdg previously considered. furthermore  figure 1: memory transition of the policy  print  1 user modeling figure 1: updating c
involves a sequence of going to the computer and the printer  possibly going to the paper store if the printer is out of paper  and exiting. note that this policy cannot be represented in the ahmm framework. in the ahmem  this can be represented by a policy whose memory transition model is given in fig. 1. the policies at the lower level  such as going to a computer  model the person's trajectories toward a set of special landmarks in the environment. these policies are constructed as memoryless policies in the same way as in the ahmm  bui etal  1 . 
¡¡the result of querying the top-level policy for the trajectory in fig. 1 is given in fig. 1. the system correctly identifies the most probable policy as print. the result of querying the memory variable of this policy is given in fig. 1. the system correctly identifies the sequence of lower-level policies 
1 	conclusion 
in conclusion  we have presented the abstract hidden markov memory models  ahmem   a general framework for representing and recognizing probabilistic plans online. 
the new framework extends the ahmm by allowing the poli-


time  seconds  
figure 1: querying the top level policies 

time  seconds  
figure 1: the progress of executing the policy  print' 
we have shown that the rao-blackwellised particle filter  rbpf  approximate inference method used for the ahmm can be extended to the ahmem  resulting in efficient and scalable procedures for plan recognition in this general setting. our work demonstrates the advantage of phrasing probabilistic plan recognition as inference in dbn so that suitable approximate methods can be employed to cope with the complexity issue. a similar approach can be applied to more general models to consider more complex plan constructs such as multi-agent plans  interleaving plans etc. 
