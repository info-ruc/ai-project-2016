acquisition of moving objects and hand-eye coordination 
r. t. chien and v. c. jones 
coordinated science laboratory 
university of illinois 
urbana  illinois 1 u.s.a. 
tract 	1. three world models of dynamic vision 


     many applications for robotic systems require the ability to handle moving objects. a method for providing a robot with real-time visual processing of moving objects is presented. by appropriately isolating the tracking task. it is possible to reduce the tracking problem to two dimensional pattern recognition. numerous objects  potentially 
moving randomly  can be tracked while identifying one object at a time. the tracking ability can also be used to provide a robot with visual feedback  in real-time  for hand-eye coordination. 
1. introduction 

     many potential applications for robotic systems  ranging from surveillance to semiconductor fabrication to extraterrestrial exploration  require ability to work with moving objects. many industrial processes which should be automated are not because parts come down the assembly line with random orientation  indeterminate velocity  or otherwise unpredictable location. while current industrial robots cannot cope with this worst of randomness  a robot with dynamic vision could; even if only used to restore orientation for further processing by conventional automation techniques. 
     detailed traffic monitoring  is another of those tedious tasks begging to be automated. the work required to analyze several hours worth of aerial photographs taken at the rate of one  or more  per second is staggering. 
     the difficulty is the  one-shot  approach to vision. most effort in computer vision has been expended on analyzing a single picture to obtain all desired data. this approach is adequate only as long as the world is static enough for the results obtained to be valid by the time they are available. clearly  what is needed is some way to dynamically update the world model  possibly even as i t is still being built. 
     this is a presentation of an approach to avoid the  one-shot  problem by tracking significant  or potentially significant  features until their place in the world model can be determined. in many applications  it is possible to use standard  slow  recognition techniques even though the world is not static. visual feedback for hand-eye coordination can also be attained using this technique. 
1 this work supported by the joint services electronics program  u.s. army  u.s. navy and u.s. 
air force  under contract number daab-1-c1. 
     the problems encountered in using robot vision in a dynamic environment are widely varied. however  robots must be able to handle motion in the field of view in order to handle virtually any dynamic problems. 
     consider a stationary robot looking at a moving object. the object's location is continuously changing  so that by the time the object is identified  the robot may no longer know where it is. in addition  the object may be changing orientation or appearance. only an extremely limited class of objects looks identical whether 
viewed from the front  side or back  e.g. spheres . 
     if the robot is moving and the rest of the world is not  the problem is very different. here it is undesirable for the robot to require lengthy pauses to determine its new location and the location of surrounding objects each time i t moves. this problem of a moving visual scene is not restricted to mobile robots. any vision system 
which is not totally fixed presents this problem. current systems  must use sophisticated software and highly calibrated hardware just to pan the camera over a scene. hence  the usual solution    has been to simply make sure that every possible point of interest is within the camera's view  and only look at portions of the picture. while this method insures easy calibration  it leads to tremendous waste in picture resolution. 
     finally  there is the problem of the moving robot in a dynamic environment. here  not only is the robot's field of view constantly changing  but features may move within the field of view. if the robot cannot keep track of significant features in real time  the situation is virtually 
hopeless. 
vision hardware 

     one of the fundamental problems of computer vision is getting a visual image into the computer f1 . there are two basic mechanisms currently in use: image dissectors and vidicons  tv cameras . 
     the image dissector  given its extremely high resolution combined with random access of any point in its field of view  at first glance appears ideal for digital computer work. unfortunately  i t is this random access ability that creates problems. if enough time is allowed to average the input at each input location  for a reasonable signal to noise ratio   there is no way to input a picture to a computer in a reasonable amount of time   hence  one cannot take advantage of the image dissector's abilities for real time vision. 
     the vidicon does not have the problem of input noise inherent in the image dissector. the input from a point in the field of view is effectively averaged over the entire frame period instead of only during the actual time the point is being interrogated. however  vidicons have problems of their own. not the least is the available resolution. standard vidicons are only capable of about 1 by 1 lines of information. while this at first seems quite high  i f the camera is required to cover the entire field of view  by the time the field is partitioned to look at an object one twentieth the width of the entire field  resolution is decreased to 1 by 1 lines. equally bad  brightly l i t images in vidicons tend to spread  or bloom  over adjacent areas. this apparent enlargement of bright objects can be disastrous to many recognition schemes  and may even obscure nearby objects so much they cannot be seen at a l l . it is possible  through the use of a special  anti-comettail  gun  to greatly reduce blooming. however  this is only at the cost of eliminating the linear response characteristic of the vidicon  a step which introduces problems of its own. 
     in real time vision systems  i t seems necessary to use vidicons for image input despite their limitations. consequently  pan-tilt heads and zoom lenses are also desirable  unless the robot's 
visual abilities are to be restricted to objects which fill a relatively large portion of the total 
field of view. 
1. the basic system 

     division of labor is used to obtain real time visual processing in a dynamic environment. if arbitrary features in a scene can be tracked in real time  most dynamic vision processes can be handled adequately by what would normally be considered off-line  not real time  methods. 
     a simple example is recognition and acquisition of a moving object by a robot. a typical approach would be to take a picture of the work-
space and apply high powered heuristics to locate and identify the object. after deciding the desired object had been located  the robot would reach out and grab i t   while the vision system continued taking and analyzing pictures to maintain knowledge of the current location of the object. even for a simple domain of objects  the recognition algorithm would be complex. not only would the size and orientation of the object be subject to variations  but the appearance of the hand in the same frame as the object further complicates matters   considering that most recognition programs require minutes of computer time  1    1   several orders of magnitude of speedup are required to make even this trivial example realistic. 
     by using a high speed feature tracker with no recognition ability  the same task can be accomplished quite simply. as soon as an object is detected in the field of view  two processes are initiated. the tracker is pointed at some feature of the object and instructed to keep track of i t . simultaneously  a picture of the object is given to a recognition program to identify. the two processes then run in parallel  the tracker taking pictures and updating the object's position  while the recognition program continues to grind away at the original stored picture. when the recognition program finally determines what it is looking at  the tracker uses this knowledge to aid its tracking  while the robot uses the continuously updated position information provided by the tracker to guide its pickup of the object. since the tracker is continuously following the object  confusion with the hand  as long as it did not obscure the object  is not a problem. 
     the crucial factor has been the development of an adequate tracker. the tracker is caught in a conflict between the requirement for real time feature location and identification  typically 1 milliseconds  and the necessity of accurately determining the latest position of numerous 
features while having only minimal knowledge of their characteristics. however  the application of a simple constraint allows the resolution of this conflict  and in fact requires a rapid frame rate to accurately identify features. 
     the simple example just discussed actually requires three discrete modules. aside from the tracker and recognizer working in parallel  there is also a supervisor  or monitor  to coordinate everything and provide contact with the outside world  i.e. accept commands  take pictures and control the robot . the supervisor would also handle such tasks as locating new objects appearing in the field of view  determining the task to be accomplished  converting tracker coordinates to robot  real space  coordinates  etc. the tracker does not burden itself with knowledge of the real world  instead it works with two dimensional features  blobs  patterns  1   lines  1    1    in picture coordinates  a task possible in real time. meanwhile  the recognizer only needs to work on one object at a time  and knows that when i t finally succeeds  the supervisor will still know where the object is  since the tracker has been keeping track of i t   
	1. 	the tracker 

     the design of a digital computer program to scan 1 picture points  find  and accurately identify a feature in that picture in real time 
would seem an impossible task considering a typical computer would require sixty milliseconds to look just once at every point in the picture. 
brute force techniques are obviously not suitable. since full pictures cannot be handled  a l l number crunching is done on windows extracted f rom fu1 pictures. ail the software has been standardized to 1 by 1 windows. windows are obtained by specifying their location and size  i.e. number of points in the f u l l picture corresponding to a single point in the window . since windows contain only 1 points  complex operations can be applied to entire windows at reasonable cost in computer time. 
1      windows alone do not solve the problem. assuming a window has been taken and a feature found  how can that feature be positively identified  there is no guarantee the feature being tracked is in that window. the identification of features problem has been resolved by a relatively simple constraint. the assumption is made that features are far enough apart and  or  predictable 
enough in location to be distinguisable solely on the basts of position. while this may initially appear a severe constraint on the applicability of the system  it normally is not. real objects have physical limits on velocity and acceleration  even when moving randomly. 
　　basically this means objects cannot collide  or in three dimensions  obscure one another. in the event of a collision  one of the features being tracked will be either lost or confused with the other  obscuring  feature. this situation cannot be handled by the tracker  but the tracker can warn the supervisor that something is amiss and higher level help is required. 
1. demonstration systems 

　　to demonstrate the abilities of systems based on the method developed  three sample systems are presented. all of these have been implemented on the combined computer facilities of the advanced automation lab in coordinated science laboratory. 
traffic monitoring 

　　the program traffic  as its name implies  is a traffic montior. the camera is aimed out of the window  about 1 meters above ground level  at the roads outside the building. figure 1 is a typical view  taken by the camera and reconstructed by the csl graphics display unit. in addition to the crossroads  there are sidewalks and pedestrians  as well as numerous parked cars. 
　　moving objects are detected by the supervisor on the pdp-1. while the tracker on the pdp-1 follows them  the supervisor extracts a window containing just one object and transmits it to the pdp-1. there it is given to the recognizer to determine its identity  i.e. car  truck  bicycle  etc.   
　　concurrently  a schematic representation of the intersection showing thelocation and status of vehicles being tracked maintained on the graphics display  figure 1 . note that one car will be lost when i t disappears behind the truck. vehicles which have not yet been identified are displayed as   -    speed is indicated by the amount o f exhaust. 
stacking 

　　the applicability to hand-eye coordination is illustrated by the program stack. several cubes are scattered about the robot's worktable. these are located visually and stacked one at a time on the base cube  figure 1 . visual feedback  using front and side views via the mirror  is used to 
guide the hand for precise alignment of the cubes. 
　　the system functions quite simply. ignoring the mirror  the approximate locations and orientations of the base and stacking cubes are deter-
mined visually. the robot then orients the wrist and grabs the cube to be stacked. as it moves the cube to over the base cube  it rotates the wrist to the estimated orientation of the base block  using care that the fingers do not obscure any of the vertical edges used for alignment. 
     the supervisor then locates the front and right edges of the cubes and the mirror images of the front edges. the mirror provides the equivalent of a second camera for complete three dimensional determination of the scene. the correspondence between direct and mirror views need only be set up once  it is then maintained by the tracker. 
     once the tracker settles on the edges  the supervisor uses the pan-tilt head to center the stack and zooms in for a closer view  figure a . the hand then moves parallel to the table top and the wrist rotates until the corresponding edges of both cubes are exactly aligned. the hand is then lowered while keeping the edges exactly aligned until contact is made. note that once the edges of the cube are being tracked  there is no requirement for the base cube or vision system  mirror  to remain stationary  let alone for even mediocre calibration of the robot arm. stacking accuracy is limited by the picture sampling width  1 points per line   in this case about 1 inch error with 1 inch cubes. 
insertion 

     insert has the capability of inserting the correct peg in an arbitrary hole where the hole is in a fixture held by an uncooperative human being  figure 1 . while the robot is moving to insert the peg  the human being is trying to prevent it  keeping the fixture flat on the table and not moving faster than the robot can . 
     the system is actually divided into three steps: detection  approach  and insertion. the detection phase is straightforward. as soon as a hand appears in the field of view  the tracker is set to follow i t . simultaneously  the recognizer is invoked to determine the contents of the hand. the hand is tracked until the recognizer has finished. once recognized  the tracker is reset to utilize this knowledge and track the fixture itself. if the robot has a suitable peg for insertion  the approach phase is entered. 
     the approach phase uses the known and visual size of the fixture to estimate its position in three dimensional space. after the hand has retrieved the correct peg  it is moved to a position six inches above and behind the fixture. care must be exercised by the robot during this time to prevent obscuring the fixture  and losing it. 
     once in position  the hand mark is tracked  and the arm moved to always keep it in the same visual position relative to the fixture. the pan-tilt head is used to center the fixture and the supervisor zooms in for a closer look. when the image becomes sufficiently large  the tracker is shifted from the hand mark to the actual pin tip. when the camera is fully zoomed in  control is passed to the insertion phase. 
     the insertion phase  figure 1  uses the known vertical distance from pin tip to hole to determine the relative error in position. the robot moves to keep the pin tip over the center of the hole and correctly rotated  if necessary  for insertion. simultaneously  of course  the 

1 
pan-tilt head is continuously adjusted to keep the hole centered. when the pin tip has been lowered to ten millimeters above the fixture  it is inserted. 
