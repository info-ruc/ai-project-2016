
despite significant algorithmic advances in recent years  finding optimal policies for large-scale  multistage stochastic combinatorial optimization problems remains far beyond the reach of existing methods. this paper studies a complementary approach  online anticipatory algorithms  that make decisions at each step by solving the anticipatory relaxation for a polynomial number of scenarios. online anticipatory algorithms have exhibited surprisingly good results on a variety of applications and this paper aims at understanding their success. in particular  the paperderives sufficient conditionsunder which online anticipatory algorithms achieve good expected utility and studies the various types of errors arising in the algorithms including the anticipativity and sampling errors. the sampling error is shown to be negligible with a logarithmic number of scenarios. the anticipativity error is harder to bound and is shown to be low  both theoretically and experimentally  for the existing applications.
1 introduction
online stochastic algorithms for solving large multistage stochastic integer programs have attracted increasing interest in recent years. they are motivated by applications in which different types of requests arrive dynamically  and it is the role of the algorithm to decide which requests to serve and how. unlike traditional online algorithms  these applications assume that the uncertainty is stochastic and that distributions of the requests are given.
모consider the packet scheduling problem from  chang et al.  1 . a router receives a set of packets at each time step and must choose which packet to serve. packets can be served only for a limited time and they are characterized by a value. the goal is to maximize the values of the served packets. the packet distributions are specified by markov models whose states specify arrival frequencies for the packet type.
모online reservation systems  benoist et al.  1  are another such application. customers place requests in real time for some service at a fixed date. the resources are modeled by a multiknapsack constraint.  think of tour operators requesting rooms in hotels for a group: the choice of a specific hotel is not pertinent for the group but all group members must be allocated to the same hotel . customers must be immediately notified of acceptance or rejection of their requests  and accepted requests must be satisfied. accepted requests must also be assigned to a specific resource at reservation time and this choice cannot be reconsidered. the goal is to maximize the profit of the served requests which come from different types with different characteristics and arrival frequencies.
모online multiple vehicle routing with time windows  bent and van hentenryck  1  captures an important class of applications arising in transportation and distribution systems. in these problems  a fleet of vehicles serve clients which are located in many different locations and place request for service in real-time in specific time windows. clients must be immediately notified of acceptance or rejection of their requests  and all accepted requests must be satisfied. routing decisions however can be delayed if necessary. the goal is to maximize the number of satisfied requests.
모all these problems share several characteristics. first  they can be modeled as multistage integer stochastic programs. second  the number of stages is large. in packet scheduling  time is discrete by nature  and experiments were made with 1 stages. for the two other applications  time is continuous but a reasonable discretization of time would require 1 stages. third  the set of feasible decisions at each stage is finite. finally  these applications require fast decision-making. these characteristics prohibit the use of a priori methods for  partially observable  markov decision processes and for stochastic programs. indeed   chang et al.  1  and  benoist et al.  1  have shown that  po mdps do not scale on these applications. moreover successful algorithms for 1-stage stochastic optimization  such as the sample average approximation method  are shown to require a number of samples exponential in the number of stages  shapiro  1   precluding their use on these applications.
모interestingly  high-quality solutions to these applications have been obtained by online algorithms that relax the nonanticipativity constraints in the stochastic programs. these online anticipatory algorithms make decisions online at a time t in three steps:
1. sample the distribution to obtain scenarios of the future;
1. optimize each scenario for each possible decision;
1. select the best decision over all scenarios.
it is clear that this strategy is necessarily suboptimal  even with many scenarios. however  experimental results have been surprisingly good  especially with the regret algorithm  bent and van hentenryck  1; hentenryck et al.  1  which is an efficient way of implementing step 1. our goal in this paper is to demystify these results by providing a theoretical analysis of these algorithms. section 1 describes the model and the algorithm. section 1 analyses the performance of the online anticipatory algorithm and isolates two fundamental sources of error: a sampling errorand a quantity called the global anticipatory gap which is inherent to the problem. section 1 shows how to bound the anticipatory gap theoretically and experimentally. section 1 analyzes the effect of approximating the optimization problem. section 1 compares the anticipatory gap to the expected value of perfect information. section 1 presents directions for future research.
1 model and algorithm
we consider finite stochastic integer programs of the form
  
where 뷅 is a stochastic process  with 뷅t being the observation at time t   with 뷅1 being deterministic   st =  x1..t 1 뷅1..t  is the state at time t  x maps states to non-empty subsets of a finite set x of decisions  so the max's are well-defined   and f is the utility function bounded by fmax. we denote respectively x and 뷅 the vectors x1..t and 뷅1..t .
모a decision process is a stochastic process x such that  t : xt 뫍 x st . we can assume that the computation of each xt requires exactly one random variable 붺t. these variables are independent and independent of 뷅.
모in practice  decisions cannot be made based on future observations. a decision process x is non-anticipative if xt is a deterministic function of 붺1..t and 뷅1..t  that is  if x is adapted to the filtration ft =  붺1..t 뷅1..t  . we can rewrite the stochastic program as
q = max{e f x 뷅   | x non-anticip. dec. proc.}.
a scenario is a realization of the process 뷅. the offline problem is the problem a decision maker would face if  in a given state st  the future observations are revealed; we define
o st xt 뷅  = max{f y 뷅  | y dec. proc. y1..t = x1..t} 
o st 뷅  = max{f y 뷅  | y dec. proc. y1..t 1 = x1..t 1} =	max o st x 뷅 .
x뫍x st 
note that these two problems are deterministic.
모finally  the expected value of the clairvoyant  evc  is defined as the expected utility of a clairvoyant decision maker  that is  evc = e o s1 뷅  . the problems discussed in the introduction all fit in this model: in particular  the utility is bounded thanks to capacity constraints. the model can also be generalized to the case in which f x 뷅  has finite first and second moments for every x.
모the anticipatory algorithm studied here is algorithm makedecision  parametrized by the number of scenarios m  whose successive decisions form a non-anticipative process:

function makedecision st  붺t 

use 붺t to compute scenarios 뷅1 ...뷅m where 뷅1i..t = 뷅1..t foreach
   g x  뫹 m xt 뫹 argmax
1 analysis of the anticipatory algorithm
we compare the performance of the anticipatory algorithm with the offline  a posteriori solution in the expected sense  as is typically done in online algorithms  borodin and el-yaniv  1 . in other words  for the decision process x produced by the anticipatory algorithm  we bound evc   e f  x 뷅    which we call the expected global loss  egl .
1 local and global losses
we first show that the egl is the sum of the expected losses of the stages.
definition 1 let st be a state. the expected local loss of decision x 뫍 x st  is defined as
붟 st x  = e o st 뷅    o st x 뷅 |st  .
note that conditioning on a state st does not provide any information on 붺t: when reading an expression of the form e ...|st   keep in mind that there is uncertainty on 뷅t+1 ... 뷅t and on 붺t ... 붺t.
lemma 1  global loss = sum of local losses  for	any decision process x 
                           t evc 
t=1
proof. let ct be the random variable o st xt 뷅  and at =
e ct   f x 뷅  . then at = 1 and  for t   t 
at = e ct   ct+1 + ct+1   f x 뷅  
= e ct   ct+1  + at+1
= e 붟 st+1 xt+1   + at+1.
the last equality comes fromdecomposingand re-assembling amongst all possible values of xt. finally evc  
e f  x 뷅   = e c1   f  x 뷅   = a1.	
1 decomposition of the local loss
we now show that the local loss at a state st consists of a sampling error and the anticipatory gap.
definition 1 the anticipatory gap of a state st is defined as
	붟g st  =	min	붟 st x .
x뫍x st 
the choice error of x wrt st is defined as
붟c st x  = 붟 st x    붟g st .
the anticipatory gap is inherent to the problem and independent of the decision process x. an equivalent definition is
 .
this expression shows that this gap can be interpreted as the cost of commuting of e and max. we now bound 붟c st x .
lemma 1  sampling error  let xt be computed by the anticipatory algorithm using m samples per decision. let st be a state and x be argmaxe o st x 뷅 |st    break ties arbitrarily . then
  
where  st x  is the standard deviation of o st x 뷅     given st.
proof. here all probabilities and expectations are implicitly conditional on st. the left-hand side can be decomposed as
.
due to the argmax in makedecision  the event xt = x implies. therefore p xt = x  뫞
.	since f is bounded  o st x 뷅   
 has a finite expectation and variance. now 

and  by the central limit theorem  this difference is normally distributed for m large enough  with mean  붟c st x  and variance m1  st x 1. finally  if x 몲 n 뷃 1  with 뷃   1  then  chernoff bound . 
1 performance of the algorithm
we now assemble the previous results.
definition 1 the global anticipatory gap of the problem is
gag  .
once again  this quantity is inherent to the problem.
theorem 1 the expected global loss of the anticipatory algorithm satisfies
egl 뫞 gag 
where m is the number of samples per decision and
붟c st x 1
k = min 1 . st x뫍x st  1 st x 
붟c st x  1
proof. we have
	t	t
egl.
	t=1	t=1
the term gag comes from
  
and the global sampling error satisfies
.

모an important consequence of this theorem is that the sampling error can be made smaller than some constant a by choosing m 뫟 1/k log 1/at |x|fmax .  shapiro  1  argues that the saa method does not scale to multistage problems  because the number of samples to achieve a given accuracy grows exponentially with t. the anticipatory algorithm only requires m to grow logarithmically with t|x|  which makes it highly scalable. of course  it only produces highquality decisions when the anticipatory gap is small.
1 bounding the global anticipatory gap
this section provides theoretical and experimental results on the anticipatory gap  explaining why anticipatory algorithms are effectivein the applications mentionedin the introduction.
1 theoretical proof on packet scheduling
we first show how to compute an upper bound on gag for a simplified version of the packet scheduling problem. suppose that there are k types of packets whose values are v1   ...   vk respectively. at each step from 1 to t   1  a packet of type i arrives with probability pi. all these random variables are independent. each packet has a lifetime of 1  meaning a packet received at time t can be scheduled either at time t or at time t+1. the utility is the sum of scheduled packets over the t stages. all packets take a single time step to serve. for convenience  we introduce a packet type 1 with value v1 = 1 and probability p1 = 1. it should be clear that this problem satisfies all assumptions above. in particular  the utility is bounded  1 뫞 f 뫞 tvk .
모why is the gag small on this problem  we show that 붟g is rarely high  inducing a small gag. for st a state and x y 뫍 x st   we say that x dominates y if o st x 뷅  뫟 o st y 뷅  almost surely given st. studying 붟g st  only requires to focus on non-dominateddecisions: there are at most two non-dominateddecisions for a given state  which consists of scheduling
  the most valuable packet  of type i  received at time t 1 and not already scheduled; or
  the most valuable packet  of type j  received at time t.
moreover  if i 뫟 j  then choosing j is dominated  since i is more valuable and will be lost if not chosen now. also  if i   j but the second most valuable packet received at t is of type k 뫟 i  then choosing i is dominated. if one of these two conditions holds  a decision dominates all the other ones  and thus 붟g st  = 1.
모suppose now that st does not satisfy any of them. by the dominance property  scenarios can be partitioned into those where scheduling i  resp. j  is the unique offline  optimal decision and those on which there is a tie. introduce the random variable yt  taking values i  j or 뫐 in these three respective cases. we then have
붟 st i  = e o st 뷅    o st i 뷅 |st  
모모모모= e o st 뷅    o st i 뷅 |st yt = j p yt = j  and symmetrically
붟 st j  = e o st 뷅    o st j 뷅 |st  
= e o st 뷅    o st j 뷅 |st yt = i p yt = i .
now  if i is scheduled and the optimal offline solution was to schedule j  then the loss cannot exceed vj   vi  since the rest of the optimal offline schedule is still feasible. hence e o st 뷅    o st i 뷅 |st yt = j  뫞 vj   vi. moreover  for the optimal offline schedule to choose j at time t  it is necessary to have a packet of value greater than vi arriving at t+1 and thuswhere qk = 1 pk.
finally  we find
.
the other case is harder to study  but a trivial upper bound is 붟 st j  뫞 vi. now it remains to bound the expectation of 붟g st  = min 붟 st i  붟 st j   by enumerating the possible values of i and j and weighting each case with its probability of occurrence. this weight is  in fact  bounded by the product of the probabilities of the 1 following events:
  a packet of type i arrived at time t   1;   the most valuable packet arrived at t is of type j;
  no packet of type i 뫞 k   j arrived at time t.
here is a numerical example. suppose there are 1 types of packets  with the following values and emission probabilities:
type111value111prob1..1.1.1.1the upper bound on 붟g depending on i and j  is
1
1.11.1.11.1.1.111	 i  j 
we find that e 붟g st   뫞 .1. on the other hand  a simple lower bound on the evc is the expectationof the value of the most valuable packet arriving at each stage multiplied by the number of stages. in this case  that leads to evc 뫟 1t. as a result  the ratio of gag over evc on this problem is less than 1%. because this analysis is not tight - especially the lower bound of the evc-  the anticipatory algorithm is likely to have an even better expected global loss. this analysis also hints at why online anticipatory algorithms are so effective on packet scheduling and why they outperform competitive algorithms on these instances.
1 discussion on practical problems
the previous section shows how to bound the gag on a particular problem: study dominance properties between the decisions  bound the loss of making a non-optimal in the offline sense  decision  and bound the probability of this event. we are currently applying this method on more complex problems but proofs quickly become very cumbersome. as an alternative  we discuss another  empirical way to argue that the gag of a problem is small.
모we have emphasized in the theoretical discussion the importance of bounding the probability that the chosen decision is not a posteriori optimal. we call p o st xt 뷅  = o st 뷅 |st   the consensus rate. this quantity can be estimated easily during the computation of an anticipatory algorithm. it suffices to count how many scenarios make the same decision  i.e. 
.
kindly gave us some of these statis-
tics on online reservation systems: they depict the consensus rate  min/max/avg  as a function of the number of scenarios.

on this class of instances  there are 1 possible decisions in each state. therefore  one could expect an average consensus rate of 1%  but it is actually much higher  at about 1%. moreover the maximal offline loss of a bad decision can easily be bounded in this problem. by markov inequality  the gag is low. similar observations were made in the packet scheduling problem  where the measured average consensus was about 1%  and in the vehicle routing problem  where the rate varies among the stages  exhibiting an increasing trend from 1 to 1%. this argument  however  would be useless when fmax is high  e.g. problems with high penalty states.
모more generally  the following theorem gives a way to measure the anticipatory gap of a state.
theorem 1 let st be a state. define as
  
then this is a strongly consistent estimator of 붟g st   i.e. 
.
proof. apply the strong law of large numbers to o st x 뷅  for all x and conclude with the finiteness of x st .  no additional cost. 붟g can be computed by the online anticipatory algorithm at
1 approximating the offline problem
theorem 1 explains why the anticipatory algorithm provides good results when the gag is small. however  practical implementations use a variant of algorithm 1 in which o is rethree applications mentioned earlier which use an approxi- placed by a fast approximation o. this is the case for the
mating technique called regret  bent and van hentenryck  1; hentenryck et al.  1 . the regret algorithm can be seen as a discrete version of sensitivity analysis: instead of computing o st x 뷅  for each for each x 뫍 x st   the idea is to compute x  = argmaxx o st x 뷅i   first and then to compute a vector of approximations using x .	each entry in this vector is derived by approximating the loss of selecting a specific x in x st  instead of the optimal decision x .	as a result  the regret algo i  o 뫞 o and  ii  maxx o st x 뷅i   =
모모모모모모모모. see  bent et al. for a discussion on the complexity of computing this approximated vector.
모it is not easy to provide tight theoretical bounds on the expected global loss for the regret approximation. we thus measured the empirical distribution of  on online stochastic reservation systems from  hentenryck et al.  1 . the differenceis zero in 1% of the cases and its mean is very small  around .1 while the typical values of o are in the range  1   although it can occasionally be large. this intuitively justifies the quality of regret  whose expected global loss is not significatively different from the anticipatory algorithm for the same sample size.
모finally  recall that  on online reservation systems  the consensus rate is very high on average. let 
argmax and let the consensus rate be 붸. by properties of regret  the approximated  score  of decision x may only exhibit errors in  1   붸 m scenarios and hence will be very close to 붸m
               . moreover  other decisions have an approximated score where  almost all  the terms of the sum has a negative error. therefore the approximated decision is biased toward consensual decisions and a high consensus rate tends to hide the approximation errorsof regret.
모in summary  a high consensus rate not only makes the ag small but also allows regret to produce decisions close in quality to the exact anticipatory algorithm. this does not mean that a brutal regret approximation  e.g.  assigning zero to each non-optimal decision  would be as effective  bent and van hentenryck  1 .
1 gag versus evpi
this section studies the relationships between the anticipatory gap and the expected value of perfect information  evpi . since these concepts are seemingly close  it is useful to explain how they differ and why we chose to introduce the notion of anticipatory gap.
모consider the following two maps assigning values to states: the offline value and the online value of state st  respectively denoted by 뷋 st  and 뷇 st  and defined by 뷋 st  = max{e f x 뷅 |st  | x dec. proc.} 뷇 st  = max{e f x 뷅 |st  | x non-anticip. dec. proc.}.
note that 뷋 st  뫟 뷇 st  for all state st. the difference 붾 st  = 뷋 st    뷇 st .
is the  local  expected value of perfect information  evpi . the expected value of perfect information of the problem is 붾 s1   that is  the advantage  in the expected sense  of a clairvoyant over a non-clairvoyant  both with infinite computational resources . the next lemma relates the offline problem and 뷋 and shows that the operators max and e commute for clairvoyant decision processes.
lemma 1 for any state st  뷋 st  = e o st 뷅 |st .

figure 1: low evpi but high global anticipatory gap
proof. let x be a decision process maximizing e f x 뷅 |st . then for all  and  as
e is non-decreasing 
뷋 st . inversely  let x뷅 be a decision process maximizing f x 뷅  with . define x by aggregation: x does the same thing as x뷅 on the scenario 뷅. then
.	
모in two-stage stochastic programming  a low evpi makes the problem much easier because an optimal decision is also a good one for each specific scenario  birge and louveaux  1  ch. 1 . however  this is no longer true in the multistage case. consider the three-stage problem depicted in figure 1. black dots represent decisions variables x1 and x1. stochastic variables 뷅1 and 뷅1 have no influence and are not represented. the white dot represents 뷅1 which take values 1 and 1 with equal probability. leaves are tagged with their utilities and a is large positive number. the value of the evpi and the anticipatory gap 붟g for each state are the following:

	state	root state	x1 = 1
/

on this problem  the evpi is 붼: an optimal solution has a score of 붼  whatever the scenario. the expected value of the optimal policy is zero. however  the online anticipatory algorithm always chooses x1 = 1 and thus has an expected utility of 1 붼   a . therefore anticipatory algorithms may behave poorly even with a low evpi. moreover  in this case  the inequality of theorem 1 is tight when m converges to +  since the gag equals 1 붼 + a .
모the phenomenon comes from the fact that the evpi of the problem is low although the evpi of the node  x1 = 1  is 붼   1 붼   a  = 1 붼 + a  and thus much larger. this does not contradict the super-martingale property of  dempster  1  because dempster considers optimal decision processes  which is not the case of anticipatory algorithms. as a result  the expected global loss of the anticipatory algorithm cannot be bounded by the root evpi. the example may suggest that the maximum of the evpis at each node of the tree gives an upper bound of the egl  but this is not true either. figure 1 presents a stochastic program  where 'sub' are clones of the problem in figure 1  with variables indices shifted. on this problem  the optimal solutions to the scenarios have an expected expected utility of 붼  and those of the anticipatory algorithm  with m =   have expected utility 1  1a+붼 ; the egl thus equals 1 a+붼 . by theorem 1  the gag is not smaller than the egl  m = オ no sampling error . as a result  the gag is greater than the maximum of the evpi over all nodes  which is equal to 1 붼 + a .

figure 1: gag higher than max of the evpis
모finally  the following theorem gives one more reason why the concept of anticipatory gap is of interest.
theorem 1 for any state st  we have 붾 st  뫟 붟g st  and there exist cases in which the inequality is strict.
proof. 붾 st  = 뷋 st    뷇 st . recall that 뷇 st  is the optimal expected utility of a non-anticipative decision process given st. because of non-anticipativity and because x st  is finite  there exists an optimal decision such that
. now
.
and thus  using lemma 1 
붾 st  뫟 e o st 뷅 |st     max e o st x 뷅 |st  
모모모모모모모모모모모모x뫍x st  뫟 붟g st .
this proves the inequality. the second part of the theorem is proven by the example of figure 1  on which the root node s1 satisfies 붾 s1  = 붼   1 = 붟g s1 . 
1 conclusion and research perspectives
anticipatory algorithms have been shown experimentally to be successful in tackling a variety of large multistage stochastic integer programs which were outside the scope of a priori methods such as  po mdps and multistage stochastic programming. this paper studied the performance of anticipatory algorithms in terms of their sampling error and the anticipatory gap of the problems. it showed that  whenever the anticipatory gap is small  anticipatory algorithms are scalable and provide high-quality solutions in the expected sense with a logarithmic number of samples in the problem size. the paper also studied how to bound the anticipatory gap both theoretically and experimentally  showing that a simple packet scheduling problem admits a small anticipatory gap and providing experimental evidence on several large multistage stochastic programs. finally  the paper indicated that the anticipatory gap is an important concept and studied its relationships with the expected value of perfect information.
모there are many research directions opened by this research. first  it is desirable to to deepen the understanding of the problem features  both combinatorial and statistical  which lead to small  or large  anticipatory gaps. second  it is also important to study novel anticipatory algorithms for applications with non-negligible anticipatory gaps. here an interesting direction is to borrow ideas from real-time dynamic programming  barto et al.  1; paquet et al.  1 .
indeed  because the estimation of 뷋 st  obtained by relaxing non-anticipativity constraints is an upper bound of its online value 뷇 st  with high probability  a rtdp approach can produce increasingly tighter approximations of the optimal policy until decision time. despite negative complexity results   kearns et al.  1    we believe that  if the gag is not too large  high-quality decisions could be obtained in reasonable time. indeed  since the far future is unlikely to be as important as the near future for the current decision  we may hope that small trees will be sufficient for many applications.
acknowledgments
this research is partly supported by nsf award dmi-
1 and onr depscor award n1.
thanks to the reviewers for their interesting suggestions.
