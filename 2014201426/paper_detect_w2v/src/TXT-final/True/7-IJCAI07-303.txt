
there is a growing interest in intelligent assistants for a variety of applications from organizing tasks for knowledge workers to helping people with dementia. in this paper  we present and evaluate a decision-theoretic framework that captures the general notion of assistance. the objective is to observe a goal-directed agent and to select assistive actions in order to minimize the overall cost. we model the problem as an assistant pomdp where the hidden state corresponds to the agent's unobserved goals. this formulation allows us to exploit domain models for both estimating the agent's goals and selecting assistive action. in addition  the formulation naturally handles uncertainty  varying action costs  and customization to specific agents via learning. we argue that in many domains myopic heuristics will be adequate for selecting actions in the assistant pomdp and present two such heuristics. we evaluate our approach in two domains where human subjects perform tasks in game-like computer environments. the results show that the assistant substantially reduces user effort with only a modest computational effort.
1 introduction
the development of intelligent computer assistants has tremendous impact potential in many domains. a variety of ai techniques have been used for this purpose in domains such as assistive technologies for the disabled  boger et al.  1  and desktop work management  calo  1 . however  most of this work has been fine-tuned to the particular application domains. in this paper  we describe and evaluate a more comprehensive framework for intelligent assistants.
모we consider a model where the assistant observes a goaloriented agent and must select assistive actions in order to best help the agent achieve its goals. to perform well the assistant must be able to accurately and quickly infer the goals of the agent and reason about the utility of various assistive actions toward achieving the goals. in real applications  this requires that the assistant be able to handle uncertainty about the environment and agent  to reason about varying action costs  to handle unforeseen situations  and to adapt to the agent over time. here we consider a decision-theoretic model  based on partially observable markov decision processes  pomdps   which naturally handles these features  providing a formal basis for designing intelligent assistants.
모the first contribution of this work is to formulate the problem of selecting assistive actions as an assistant pomdp  which jointly models the application environment and the agent's hidden goals. a key feature of this approach is that it explicitly reasons about the environment and agent  which provides the potential flexibility for assisting in ways unforeseen by the developer. however  solving for such policies is typically intractable and we must rely on approximations. a second contribution is to suggest an approximate solution approach that we argue is well suited to the assistant pomdp in many application domains. the approach is based on explicit goal estimation and myopic heuristics for online action selection. for goal estimation  we propose a model-based bootstrapping mechanism that is important for the usability of an assistant early in its lifetime. for action selection  we propose two myopic heuristics  one based on solving a set of derived assistant mdps  and another based on the simulation technique of policy rollout. a third contribution is to evaluate our framework in two novel game-like computer environments using 1 human subjects. the results indicate that the proposed assistant framework is able to significantly decrease user effort and that myopic heuristics perform as well as the more computationally intensive method of sparse sampling.
모the remainder of this paper is organized as follows. in the next section  we introduce our formal problem setup  followed by a definition of the assistant pomdp. next  we present our approximate solution technique based on goal estimation and online action selection. finally we give an empirical evaluation of the approach in two domains and conclude with a discussion of related and future work.
1 problem setup
we refer to the entity that we are attempting to assist as the agent. we model the agent's environment as a
markov decision process  mdp  described by the tuple
  where w is a finite set of world states  a is a finite set of agent actions  a is a finite set of assistant actions  and is a transition distributions that represents the probability of transitioning to state w given that  is taken in state w. we will sometimes use t w a  to denote a random variable distributed as t w a 몫 . we assume that the assistant action set always contains the action noop which leaves the state unchanged. the component c is an action-cost function that maps to real-numbers  and i is an initial state distribution over w.
we consider an episodic setting where at the beginning of each episode the agent begins in some state drawn from i and selects a goal from a finite set of possible goals g. the goal set  for example  might contain all possible dishes that the agent might prepare. if left unassisted the agent will execute actions from a until it arrives at a goal state upon which the episode ends. when the assistant is present it is able to observe the changing state and the agent's actions  but is unable to observe the agent's goal. at any point along the agent's state trajectory the assistant is allowed to execute a sequence of one or more actions from a ending in noop  after which the agent may again performan action. the episode ends when either an agent or assistant action leads to a goal state. the cost of an episode is equal to the sum of the costs of the actions executed by the agent and assistant during the episode. note that the available actions for the agent and assistant need not be the same and may have varying costs. our objective is to minimize the expected total cost of an episode.
모formally  we will model the agent as an unknown stochastic policy 뷇 a|w g  that gives the probability of selecting action a 뫍 a given that the agent has goal g and is in state w. the assistant is a history-dependent stochastic policy  that gives the probability of selecting action
given world state w and the state-action trajectory t observed starting from the beginning of the trajectory. it is critical that the assistant policy depend on t  since the prior states and actions serve as a source of evidence about the agent's goal  which is critical to selecting good assistive actions. given an initial state w  an agent policy 뷇  and assistant policy 뷇 we let  denote the expected cost of episodes that begin at state w with goal g and evolve according to the following process: 1  execute assistant actions according to 뷇 until noop is selected  1  execute an agent action according to 뷇  1  if g is achieved then terminate  else go to step 1.
모in this work  we assume that we have at our disposal the environment mdp and the set of possible goals g. our objective is to select an assistant policy 뷇 that minimizes the expected cost given by  where g1 is an unknown distribution over agent goals and 뷇 is the unknown agent policy. for simplicity we have assumed a fully observable environmentand episodicsetting  however these choices are not fundamental to our framework.
1 the assistant pomdp
pomdps provide a decision-theoretic framework for decision making in partially observable stochastic environments.
a pomdp is defined by a tuple
s is a finite set of states 
is a transition distribution  c is an action cost function  i is an initial state distribution  o is a finite set of observations  and 뷃 o|s  is a distribution over observations o 뫍 o given the current state s. a pomdp policy assigns a distribution over actions given the sequence of preceding observations. it is often useful to view a pomdp as an mdp over an infinite set of belief states  where a belief state is simply a distribution over s. in this case  a pomdp policy can be viewed as a mapping from belief states to actions. actions can serve to both decrease the uncertainty about the state via the observations they produce and/or make direct progress toward goals.
we will use a pomdp model to address the two main challenges in selecting assistive actions. the first challenge is to infer the agent's goals  which is critical to provide good assistance. we will capture goal uncertainty by including the agent's goal as a hidden component of the pomdp state. in effect  the belief state will correspond to a distribution over possible agent goals. the second challenge is that even if we know the agent's goals  we must reason about the uncertain environment  agent policy  and action costs in order to select the best course of action. our pomdp will capture this information in the transition function and cost model  providing a decision-theoretic basis for such reasoning.
모given an environment mdp   a goal distribution g1  and an agent policy 뷇 we now define the corresponding assistantpomdp.
모the state space is w 뫄g so that each state is a pair  w g  of a world state and agent goal. the initial state distribution i assigns the state  w g  probability i w g1 g   which models the process of selecting an initial state and goal for the agent at the beginning of each episode. the action set is equal to the assistant actions a  reflecting that assistant pomdp will be used to select actions.
모the transition function t assigns zero probability to any transition that changes the goal component of the state  i.e.  the assistant cannot change the agent's goal. otherwise  for any action a except for noop  the state transitions from  w g  to  with probability. for the noop action 
t simulates the effect of executing an agent action selected according to 뷇. that is  noop  is equal to the probability that.
모the cost model c reflects the costs of agent and assistant actions in the mdp. for all actions a except for noop we have that. otherwise we have that
 noop  = e c w a    where a is a random variable distributed according to 뷇 몫|w g . that is  the cost of a noop action is the expected cost of the ensuing agent action.
모the observation distribution 뷃 is deterministic and reflects the fact that the assistant can only directly observe the world state and actions of the agent. for the noop action in state  w g  leading to state   the observation is where a is the action executed by the agent immediately after the noop. for all other actions the observation is equal to the w component of the state  i.e. . for simplicity of notation  it is assumed that the w component of the state encodes the preceding agent or assistant action a and thus the observations reflect both states and actions.
모we assume an episodic objective where episodes begin by drawing initial pomdp states and end when arriving in a state  w g  where w satisfies goal g. a policy 뷇 for the assistant pomdp maps state-action sequences to assistant actions. the expected cost of a trajectory under 뷇 is equal to our objective function from the previous section.
thus  solving for the optimal assistant pomdp policy yields an optimal assistant. however  in our problem setup the assistant pomdp is not directly at our disposal as we are not given 뷇 or g1. rather we are only given the environment mdp and the set of possible goals. as described in the next section our approach will approximate the assistant pomdp by estimating 뷇 and g1 based on observations and select assistive actions based on this model.
1 selecting assistive actions
approximating the assistant pomdp. one approach to approximating the assistant pomdp is to observe the agent acting in the environment  possibly while being assisted  and to learn the goal distribution g1 and policy 뷇. this can be done by storing the goal achieved at the end of each episode along with the set of world state-action pairs observed for the agent during the episode. the estimate of g1 can then be based on observed frequency of each goal  perhaps with laplace correction . likewise  the estimate of 뷇 a|w g  is simply the frequency for which action a was taken by the agent when in state w and having goal g. while in the limit these estimates will converge and yield the true assistant pomdp  in practice convergencecan be slow. this slow convergencecan lead to poor performance in the early stages of the assistant's lifetime. to alleviate this problem we propose an approach to bootstrap the learning of 뷇.
모in particular  we assume that the agent is reasonably close to being optimal. this is not unrealistic in many application domains that might benefit from intelligent assistants. there are many tasks  that are conceptually simple for humans  yet they require substantial effort to complete. given this assumption  we initialize the estimate of the agent's policy to a prior that is biased toward more optimal agent actions. to do this we will consider the environment mdp with the assistant actions removed and solve for the q-function q a w g . the q-function gives the expected cost of executing agent action a in world state w and then acting optimally to achieve goal g using only agent actions. we then define the prior over agent actions via the boltzmann distribution. in our experiments  we found that this prior provides a good initial proxy for the actual agent policy  allowing for the assistant to be immediately useful. we update this prior based on observations to better reflect the peculiarities of a given agent. computationally the main obstacle to this approach is computing the q-function  which need only be done once for a given application domain. a number of algorithms exist to accomplish this including the use of factored mdp algorithms  boutilier et al.  1   approximate solution methods  boutilier et al.  1; guestrin et al.  1   or developing specialized solutions.
모action selection overview. let ot = o1 ... ot be the observation sequence from the beginning of the current trajectory until time t. each observation provides the world state and the previously selected action  by either the assistant or agent . given ot and an assistant pomdp the goal is to select the best assistive action according to the policy.
모unfortunately  exactly solving the assistant pomdp will be intractable for all but the simplest of domains. this has led us to take a heuristic action selection approach. to motivate the approach  it is useful to consider some special characteristics of the assistant pomdp. most importantly the belief state corresponds to a distribution over the agent's goal. since the agent is assumed to be goal directed  the observed agent actions provide substantial evidence about what the goal might and might not be. in fact  even if the assistant does nothing the agent's goals will often be rapidly revealed. this suggests that the state/goal estimation problem for the assistant pomdp may be solved quite effectively by just observing how the agent's actions relate to the various possible goals. this also suggests that in many domains there will be little value in selecting assistive actions for the purpose of gathering information about the agent's goal. this suggests the effectiveness of myopic action selection strategies that avoid explicit reasoning about information gathering  which is one of the key pomdp complexitiescomparedto mdps. we note that in some cases  the assistant will have pure informationgathering actions at its disposal  e.g. asking the agent a question. while we do not consider such actions in our experiments  as mentioned below  we believe that such actions can be handled via shallow search in belief space in conjunction with myopicheuristics. with the abovemotivation  our action selection approach alternates between two operations.
모goal estimation. given an assistant pomdp with agent policy 뷇 and initial goal distribution go  our objective is to maintain the posterior goal distribution p g|ot   which gives the probability of the agent having goal g conditioned on observation sequence ot. note that since the assistant cannot affect the agent's goal  only observations related to the agent's actions are relevant to the posterior. given the agent policy 뷇  it is straightforward to incrementally update the posterior p g|ot  upon each of the agent's actions. at the beginning of each episode we initialize the goal distribution p g|o1  to g1. on timestep t of the episode  if ot does not involve an agent action  then we leave the distribution unchanged. otherwise  if ot indicates that the agent selected action a in state w  then we update the distribution according to p g|ot  =  1/z  몫 p g|ot 1  몫 뷇 a|w g   where z is a normalizing constant. that is  the distribution is adjusted to place more weight on goals that are more likely to cause the agent to execute action a in w.
모the accuracy of goal estimation relies on how well the policy 뷇 learned by the assistant reflects the true agent. as described above  we use a model-based bootstrapping approach for estimating 뷇 and update this estimate at the end of each episode. provided that the agent is close to optimal  as in our experimental domains  this approach can lead to rapid goal estimation  even early in the lifetime of the assistant.
모we have assumed for simplicity that the actions of the agent are directly observable. in some domains  it is more natural to assume that only the state of the world is observable  rather than the actual action identities. in these cases  after observing the agent transitioning fromwe can use the mdp transition function t to marginalize over possible agent actions yielding the update 
.
모action selection. given the assistant pomdp m and a distribution over goals p g|ot   we now address the problem of selecting an assistive action. for this we introduce the idea of anassistantmdprelative to a goal g and m  which we will denote by m g . each episode in m g  evolves by drawing an initial world state and then selecting assistant actions until a noop  upon which the agent executes an action drawn from its policy for achieving goal g. an optimal policy for m g  gives the optimal assistive action assuming that the agent is acting to achieve goal g. we will denote the q-function of m g  by qg w a   which is the expected cost of executing action a and then following the optimal policy.
모our first myopic heuristic is simply the expected q-value of an action over assistant mdps. the heuristic value for assistant action a in state w given observations ot is 

g
모and we select actions greedily according to h. intuitively h w a ot  measures the utility of taking an action under the assumption that all goal ambiguity is resolved in one step. thus  this heuristic will not select actions for purposes of information gathering. this heuristic will lead the assistant to select actions that make progress toward goals with high probability  while avoiding moving away from goals with high probability. when the goal posterior is highly ambiguous this will often lead the assistant to select noop.
모the primary computational complexity of computing h is to solve the assistant mdps for each goal. technically  since the transition functions of the assistant mdps depend on the approximate agent policy 뷇  we must re-solve each mdp after updating the 뷇 estimate at the end of each episode. however  using incremental dynamic programming methods such as prioritized sweeping  moore and atkeson  1  can alleviate much of the computational cost. in particular  before deploying the assistant we can solve each mdp offline based on the default agent policy given by the boltzmann bootstrapping distribution described earlier. after deployment  prioritized sweeping can be used to incrementally update the qvalues based on the learned refinements we make to 뷇.
모when it is not practical to solve the assistant mdps  we may resort to various approximations. we consider two approximations in our experiments. one is to replace the user's policy to be used in computing the assistant mdp with a fixed default user's policy  eliminating the need to compute the assistant mdp at every step. we denote this approximation by hd. another approximation uses the simulation technique of policyrollout  bertsekas and tsitsiklis  1  to approximate qg w a  in the expression for h. this is done by first simulating the effect of taking action a in state w and then using 뷇 to estimate the expected cost for the agent to achieve g from the resulting state. that is  we approximate qg w a  by assuming that the assistant will only select a single initial action followed by only agent actions. more formally  let c몬n 뷇 w g  be a function that simulates n trajectories of 뷇 achieving the goal from state w and then averaging the trajectory costs. the heuristic hr is identical to h w a ot  except that we replace qg w a  with the expectation. we can also combine both of these heuristics  which we denote by hd r. finally  in cases where it is beneficial to explicitly reason about information gathering actions  one can combine these myopic heuristics with shallow search in belief space of the assistant mdp. one approach along these lines is to use sparse sampling trees  kearns et al.  1  where myopic heuristics are used to evaluate the leaf nodes.
1 experimental results
we conducted user studies and simulations in two domains and present the results in this section.
1 doorman domain
in the doorman domain  there is an agent and a set of possible goals such as collect wood  food and gold. some of the grid cells are blocked. each cell has four doors and the agent has to open the door to move to the next cell  see figure 1 . the door closes after one time-step so that at any time only one door is open. the goal of the assistant is to help the user reach his goal faster by opening the correct doors.
모a state is a tuple  where s stands for the the agent's cell and d is the door that is open. the actions of the agent are to open door and to move in each of the 1 directions or to pickup whatever is in the cell  for a total of 1 actions. the assistant can open the doors or perform a noop  1 actions . since the assistant is not allowed to push the agent through the door  the agent's and the assistant's actions strictly alternate in this domain. there is a cost of  1 if the user has to open the door and no cost to the assistant's action. the trial ends when the agent picks up the desired object.
모in this experiment  we evaluated the heuristics hd and hr. in each trial  the system chooses a goal and one of the two heuristics at random. the user is shown the goal and he tries to achieve it  always starting from the center square. after every user's action  the assistant opens a dooror does nothing. the user may pass through the door or open a different door. after the user achieves the goal  the trial ends  and a new one begins. the assistant then uses the user's trajectory to update the agent's policy.
모the results of the user studies for the doorman domain are presented in figure 1. the first two rows give cummulative results for the user study when actions are selected according to hr and hd respectively. the table presents the total optimal costs  number of actions  for all trials across all users without the assistant n  and the costs with the assistant u  and the average of percentage cost savings  1- u/n   over all trials and over all the users1. as can be seen  hr appears to have a slight edge over hd.

figure 1: doorman domain.
모to compare our results with a more sophisticated solution technique  we selected actions using sparse sampling  kearns et al.  1  for depths d =1 and 1 while using b=1 or 1 samples at every step. the leaves of the sparse sampling tree are
totaluseraveragetimeheuristicactionsactions1    u/n pernuactionhr11 11hd11  11hr11  11d = 1  b = 11.1  11d = 1  b = 11.1  11d = 1  b = 11.1  11d = 1  b = 11.1  11figure 1: results of experiments in the doorman domain. the first half of the table presents the results of the user studies while the lower half presents the results of the simulation.
evaluated using hr. for these experiments  we did not conduct user studies  due to the high cost of such studies  but simulated the human users by choosing actions according to policies learned from their observed actions. the results are presented in the bottom half of figure 1. we see that sparse sampling increased the average run time by an order of magnitude  but is able to produce a reduction in average cost for the user. this is not surprising  for in the simulated experiments  sparse sampling is able to sample from the exact user policy  i.e. it is sampling from the learned policy  which is also being used for simulations .it remains to be seen whether these benefits can be realized in real experiments with only approximate user policies.
1 kitchen domain
in the kitchen domain  the goals of the agent are to cook various dishes. there are 1 shelves with 1 ingredients each. each dish has a recipe  represented as a partially ordered plan. the ingredients can be fetched in any order  but should be mixed before they are heated. the shelves have doors that must be opened before fetching ingredients and only one door can be open at a time.

figure 1: the kitchen domain. the user is to prepare the dishes described in the recipes on the right. the assistant's actions are shown in the bottom frame.
모there are 1 different recipes. the state consists of the location of each of the ingredient  bowl/shelf/table   the mixing state and temperature state of the ingredient  if it is in the bowl  and the door that is open. the state also includes the action history to preserve the ordering of the plans for the recipes. the user's actions are: open the doors  fetch the ingredients  pour them into the bowl  mix  heat and bake the contents of the bowl  or replace an ingredient back to the shelf. the assistant can perform all user actions except for pouring the ingredients or replacing an ingredient back to the shelf. the cost of all non-pour actions is -1. experiments were conductedon 1 human subjects. unlike in the doorman domain  here it is not necessary for the assistant to wait at every alternative time step. the assistant continues to act until the noop becomes the best action according to the heuristic.
모since this domainhas much biggerstate-space than the first domain  both heuristics use the default user's policy. in other words  we comparehd and hd r. the results of the user studies are shown in top half of the figure 1. hd r performs better than hd. it was observed from the experiments that the hd r technique was more aggressive in choosing non-noop actions than the hd  which would wait until the goal distribution is highly skewed toward a particular goal. we are currently trying to understand the reason for this behavior.
totaluseraveragetimeheuristicactionsactions1    u/n pernuactionhd r11 11hd11 11hd r11  11d = 1  b = 11.1  11d = 1  b = 11.1  11d = 1  b = 11.1 11d = 1  b = 11.1  11figure 1: results of experiments in the kitchen domain. the first half of the table presents the results of the user studies while the lower half presents the results of the simulation.
모we compared the use of sparse sampling and our heuristic on simulated user trajectories for this domain as well see bottom half of figure 1 . there is no significant difference between the solution quality of rollouts and sparse sampling on simulations  showing that our myopic heuristics are performing as well as sparse sampling with much less computation.
1 related work
much of the prior work on intelligent assistants did not take a sequential decision making or decision-theoretic approach. for example  email filtering is typically posed as a supervised learning problem  cohen et al.  1   while travel planning combines information gathering with search and constraint propagation  ambite et al.  1 .
모there have been other personal assistant systems that are explicitly based on decision-theoretic principles. most of these systems have been formulated as pomdps that are approximately solved offline. for instance  the coach system helped people suffering from dementia by giving them appropriate prompts as needed in their daily activities  boger et al.  1 . they use a plan graph to keep track of the user's progress and then estimate the user's responsiveness to determine the best prompting strategy. a distinct difference from our approach is that there is only a single fixed goal of washing hands  and the only hidden variable is the user responsiveness. rather  in our formulation there are many possible goals and the current goal is hidden to the assistant. we note  that a combination of these two frameworks would be useful  where the assistant infers both the agent goals and other relevant properties of the user  such as responsiveness.
모in electric elves  the assistant is used to reschedule a meeting should it appear that the user is likely to miss it. since the system monitors users in short regular intervals  radical changes in the belief states are usually not possible and are pruned from the search space varakantham et al.  1 . in a distinct but related work  doshi  1   the authors introduce the setting of interactive pomdps  where each agent models the other agent's beliefs. our model is simpler and assumes that the agent is oblivious to the presence and beliefs of the assistant. relaxing this assumption without sacrificing tractability would be interesting.
모our work is also related to on-line plan recognition and can be naturally extended to include hierarchies as in the hierarchical versions of hmms  bui et al.  1  and pcfgs  pynadath and wellman  1 . blaylock and allen describe a statistical approach to goal recognition that uses maximum likelihood estimates of goal schemas and parameters  blaylock and allen  1 . these approaches do not have the notion of cost or reward. by incorporating plan recognition in the decision-theoretic context  we obtain a natural notion of assistance  namely maximizing the expected utility.
모there has been substantial research in the area of user modelling. horvitz et.al took a bayesian approach to model whether a user needs assistance based on user actions and attributes and used it to provide assistance to user in a spreadsheet application horvitz et al.  1 . hui and boutilier used a similar idea for assistance with text editing hui and boutilier  1 . they use dbns with handcoded parameters to infer the type of the user and computed the expected utility of assisting the user. it would be interesting to explore these kind of user models in our system to determine the user's intentions and then compute the optimal policy for the assistant.
1 summary and future work
we described the assistant pomdp as a model for selecting assistive actions. we also described an approximate solution approach based on iteratively estimating the agent's goal and selecting actions using myopic heuristics. our evaluation using human subjects in two game-like domains show that the approach can significantly help the user. one future direction is to consider more complex domains where the assistant is able to do a series of activities in parallel with the agent. another possible direction is to assume hierarchical goal structure for the user and do goal estimation in that context. our framework can be naturally extended to the case where the environment is partially observable to the agent and/or the assistant. this requires recognizing actions taken to gather information  e.g.  opening the fridge to decide what to make based on what is available. another important direction is to extend this work to domains where the agent mdp is hard to solve. here we can leverage the earlier work on learning apprentice systems and learning by observation  mitchell et al.  1 . the user's actions provide training examples to the system which can be used for learning.
acknowledgements
this material is based upon work supported by the defense advanced research projects agency  darpa   through the department of the interior  nbc  acquisition services division  under contract no. nbchd1. any opinions  findings  and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of darpa.
