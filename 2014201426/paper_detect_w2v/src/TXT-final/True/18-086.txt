f i r s t r e s u l t s i n 	r o b o t 	r o a d - f o l l o w i n g 
richard wallace  a n t h o n y stentz 
charles t h o r p e   hans moravec 
william w h i t t a k e r   takeo kanade 
robotics institute  carnegie-mellon university 

a b s t r a c t 
 the new carnegie mellon autonomous i and vehicle group has produced the first demonstrations of road following robots. in this paper we first describe the robots that are part of the cmu autonomous land vehicle project. we next describe the vision system of the cmu alv. we then present the control algorithms  including a simple and stable control scheme for visual servoing. finally we discuss our plans for the future. 
introduction 
 cmu has formed the autonomous land vehicle  alv  group to develop a perceptive outdoor robot. we have produced the first demonstrations of an autonomous vehicle able to follow a road using a single on board black and white television camera as its only sensor. our robot has made several successful runs over a curving 1 meter path  and 1 meter segments of staright sidewalk  moving continuously at slow speeds  by tracking the edges of the road. 
 the research described in this paper is a first complete system  covering everything from low-level motor drivers to the top-level control loop and user interface. we took a  depth first  approach to building our testbed: we picked one rough design and built all the pieces of a functioning system  rather than spending a lot of time at the beginning exploring design alternatives. 
 related research at the unversity of maryland  has focused on the problem of visually finding and tracking roadways. the  bootstrapping  phase of the maryland road finding program  in which the robot detects a road on start-up with no a priori position information  currently has no counterpart in our system. our vehicle is always started with an orientation more or less aligned with the direction of the road and with knowledge of an initial road model. the maryland road finding module is expected to be tested soon on an alv built at martin marietta denver aerospace. 
in this paper we first describe the robots that are part of the cmu 
autonomous land vehicle project. we next describe the vision 
currently  this project is funded in part by carnegie-mellon university  by the 
office of naval research under contract number n1-k-1  by the 
western pennsylvania advanced technology center  by defense advanced 
research projects agency  dod . arpa order no 1  monitored by the air 
force avionics laboratory under contract f1 k 1. and by denning 
mobile robotics. inc richard waace thanks nasa for supporting him with a nasa graduate student researchers program fellowship grant. 
system of the cmu alv. we then present the control algorithms  including a simple and stable control scheme for visual servoing. rinally  we discuss our plans for the future. 
terregator and neptune 
 no mobile robot system is complete without a mobile robot. the primary vehicle of the cmu alv project is the terregator  built in the civil engineering department. the design and construction of the lerreyator  for terrestrial naviya/o/  is documented in . it is a 1 wheeled vehicle  1 inches long by 1  wide by 1  tall. all wheels are driven  with one motor for the 1 left wheels and one for the 1 right wheels. shaft encoders count wheel turns  but the vehicle skid steering introduces some indeterminacy. 
 the terregator is untethered. power is provided by an onboard generator. communications with a host computer are via a bidirectional 1 baud radio link for vehicle status and commands  and a 1 megahertz microwave link for television signal from the vehicle to a digitizer. a remote vax 1 runs programs for symbolic processing of visual data and navigation. a grinnell gmr 1 attached to the vax computes low- level visual operations such as edge detection. a motorola 1 on the terregator translates steering commands from the vax into wheel velocities for the left and right wheels. 
earlier work also used the tethered robot neptune  built by the 
mobile robot lab. neptune is a simple tricycle  with a powered and steered from wheel and two passive wheels in the rear. its sensors consist of two cameras  for stereo vision work   plus a ring of 1 sonars. while it was intended primarily for indoor work  it has large enough wheels to run outside on gentle terrain. with suitable modifications  an umbrella taped to the camera mast   it even has limited all-weather capability. 
 our first successful continuous motion road following was achieved with neptune running in our lab on a road marked with black electrical tape on the floor. this 1 meter road had one left turn and one right turn  which neptune navigated successfully. at the end of the road  neptune made a sharp right turn and drove around in circles. 



since images are digitized frequently  the appearance of the road edges does not change appreciably across successive images; consequently  searching the entire image is unnecessary  in order to constrain the search  the program maintains a model of the road i he model contains the position and orientation of the left and right road edges seen in a recent image. the program uses these model edges to generate two small suhimage rectangles in which to search for the left and right road edges. since the approximate direction of each road edge is known a priori  the program uses directed curve tracing to reduce processing time and to preclude spurious edges generally the program finds more than one edge in each subimago rectangle. the model is used to select the pair of extracted edges most likely to be road enges. i ins new pair replaces the old pair in the model. from the model pair  the program computes a center line  the vehicle's drift from the center line  and a steering command to bring the vehicle closer to the center line. as the vehicle executes a steering command another image is digitized and the cycle repeats. figure 1 depicts the program control flow. in the remainder of the paper we explain each component of the program in greater detail. 
constraining the search 
 each time the program digitizes an image it chooses two subimage rectangles to constrain the search for left and right edges the representation of the rectangle is two horizontal and two vertical bounding line segments. the vehicle always  looks  a fixed distance ahead; therefore  the placement in the image of the horizontal bounding segments is predetermined and remains fixed across successive images. the placement of the segments is partly determined by two parameters selected manually: the height of the rectangle  typically 1 to 1 pixels  and rectangle overlap  that is  the percentage of the road in a rectangle seen in the preceding image  typically 1% . these two parameters present important trade offs: if a large height is chosen  the extracted road edges will be longer  thus providing more accurate information about the road; however  the processing time will be increased  and the road will be scrutinized less often. if a large overlap is chosen  more information is available from the previous image and spurious edges are less likely to deceive the algorithm; however  the vehicle's speed must be slowed to enable such overlap. the two parameters  coupled with the vehicle's speed  the image processing time  and the camera's tilt determine the placement of the horizontal bounding segments in the image. 
 the vertical bounding segments change from image to image. the program selects bounding segments so that the road edges  based on predictions from the model and a preset error tolerance  will appear within the rectangle. this error tolerance arises from two sources; first  the program obtains its estimates of the vehicle's motion by dead reckoning  which is somewhat inaccurate. second  the program assumes the road is straight  that is  predictions are made by linearly extending the road edges. road curvature introduces a discrepancy between these predictions and the actual road; consequently  the .ectangle must be wide enough to see the road edge within a preset tolerance. 
selecting the best edges 
 the line finding routine generally returns more than one line from each rectangle. the program passes these lines through a number of filters to determine which  if any  are road edges. the new road edges are used to plan a path for the vehicle and to update the model the 1 best left and right edge candidates  based on weights supplied by the line finding routine  are retained  and the rest are discarded the program assumes that the camera's calibration  position  and orientation with respect to the road are known  that the ground is locally level and that all candidate edges arise from ground features. these assumptions 
r. wallace et at. 1 
allow the program to project each candidate edge into a unique line in the ground plane we establish a nighthanded coordinate system with the vehicle at the origin and the xy plane on the ground  with the positive x axis directed to the right of the vehicle and the positive y axis directed forward f or each transformed edge  the program calculates the following parameters: the perpendicular distance r measured from the origin to the edge and the angle 1 measured from the positive x axis the differences in r and 1 between each transformed candidate edge and the corresponding model edge are calculated  call these values dr and 1 respectively . the quantity dr is the difference in displacements of the vehicle from the model edge and from the candidate edge. 1 he quantity 1 is the angle between the model edge and the candidate edge test runs have shown that the vehicle tends to remain aligned with the center line; most of the error is in the form of lateral drift from this line. hence  dr provides the most information for evaluating candidate edges. the quantity 1 tends to be small  less than 1 degrees ; consequently  an early filter uses it to eliminate spurious edges. after this round of edge elimination  one of three cases remains: 
1. all edge candidates have been eliminated 
1 all edge candidates have been eliminated for a particular road edge  either left or right  
1. at least one edge candidate remains for both the left and right road edge 
¡¡in the first case  the program obtains no new information and the vehicle continues to execute the path planned from the previous image. in the second case  only one road edge is visible. the other road edge is occluded  shadowed  or poorly defined. suppose for example the program found a set of candidate road edges on the right side but none on the left. from the candidate edges on the right side the program selects the one with the minimum dr value. it inserts this new edge into the model  retains the old model edge for the left side  and generates a new steering command. in the third case  both road edges are visible. the program selects one edge from each list of road edges  left and right  by comparing each left edge to each right edge candidate and choosing the pair that minimizes the difference in their dr values  that is  it selects the two edge candidates that differ from their corresponding model edge in the same way. figure 1 illustrates road edge selection in this case. this decision is based on the observation that vehicle motion error and road curvature shift the location of each edge in the image in the same way. the program inserts the two new road edges into the model and plans a new path. 
line and edge extraction 
 at the lowest levels of the vision system for our vehicle  the edge and line extraction modules  we found that for detecting road edges we could rely on the principle  almost anything works in the simple cases   i hat is  any of a number of simple edge and line finding techniques could be used to extract road edges in various situations. our approach then was to try everything we tested various edge and line finding programs on static road images and on images acquired by the vehicle in actual runs. simple techniques proved adequate in many situations we encountered. 
¡¡the basic approach of all the vision modules we tried was to find the left and right boundaries of the road and represent them as lines. therefore  the task of the low level vision modules is to find line segments which are plausible candidate road edges. we sought to make only the most general assumptions about what might constitute a road in an image. the technique used to extract road edges and represent them as lines depends on 

whether we think of a road as an intensity change from background  a texture change  a color change or a combination. we experimented with 1 methods for extracting road edges from images and three methods for fitting lines to the edges. the seven techniques we used to find edges in road images were: 
1. correlation. assuming that a road edge is a more or less vertical feature in a subimage it can be followed by selecting a small sample patch of the edge and correlating this on a row-by-row basis with the subimage. where the correlation is strongest in each row a road edge element is assumed the result is a list of points where the road edge appears in each row a line can be fit to these directly. the correlation approach worked very well when the sample road edge patch was hand selected. 
1. dog operator. a difference of gaussian edge operator was tried at a wide range of spatial resolutions on road images. road edges tend to be low spatial frequency signals so large dogs were required to find them directly. two dimensional dog filters tended to break up the road edges even at low frequencies one dimensional dog operators applied horizontally in the image produced more connected road edge pieces  since the road boundaries were almost vertical features in the image. high spatial frequency dog operators can be used as the basis of a texture-based segmentation of road images  however. 
1 temporal edge detector. subtracting two successive image frames is an inexpensive method for detecting image features that change from one moment to the next. if a vehicle is traveling down an ideal road  where the intensity of the road is uniform  the intensity of the surrounding region is uniform and the road edges are straight and parallel  then the difference of two successive road images is zero. when the vehicle begins to turn left or right off the road  however  simple image differencing finds the road edges. this strategy was used in one experiment to servo neptune visually down a hallway. here the road edges were particularly distinct so the idealness assumption was more or less satisfied. 
1. roberts operator a  x1 roberts edge operator was sufficient to find road edges where they were relatively well defined intensity step functions  such as when the vehicle traveled down a hallway or when we artificially marked the road edges with tape. 
1. intensity segmentation. a simple binary intensity segmentation of the road image works in many cases where the road is a set of pixels most of whose intensities are grouped together in the image histogram. we used a simple segmentation technique based on classifying all the pixels in the bottom 1% of the histogram as one region and those in the upper 1% as another standard procedures for expanding and shrinking the resulting segments to join closely spaced segments and eliminate small ones are applied. road edges are assumed to lie along the boundaries of the resulting regions. 

1 texture segmentation texture based segmentation often proves better than intensity based segmentation for mad edges where the road is relatively smooth and the surrounding region is not  such as when the road is asphalt against a grass background. a simple texture operator which we have found useful in detecting road edges is one which counts the number of edges per unit area and classifies all those areas where the edge count is high as a single region. 
1 row integration summing the intensities columnby-column in a set of scanlmos in the image results in a single scanline intensity image where the road is roughly a one dimensional box function  given that the road is a more or less vertical feature and the road and surrounding area each have fairly uniform but different intensities. finding the boundaries of the box amounts to finding the average position of the left and right road edges over the scanlines summed. repeating the procedure for another set of rows in the image locates another pair of road edge points which can be joined with the first to approximate the road boundaries as line segments. 
the three line-extraction techniques we used were: 
1 least sqaures line fitting when we had only one possible line in an edge image  such as the result of running a correlation operator over the rows or collecting a number of road edge points by row integration  a line could be fit to the points by least squares. 
1. muff transform. a modified hough  muff  transform was used to fit lines to edge data where the edge extractor returned points that could plausibly be parts of several lines. the hough transform has been used to detect road edges in other road finding programs 
. the muff transform uses a new parameterization tor lines in images. the muff transform has several implementational advantages over the conventional p-1 parameterization. the details and implementation of the muff transform are presented elsewhere . 
1. line tracing. most of the subimages we processed to find lines were bands about 1 pixels tall and 1 pixels wide. a simple raster tracking algorithm found in  proved sufficient to trace the road edges. basically  if an edge point p above some high threshhold d is found while scanning the subimage  then we search on scan lines below for connected edge points above some lower threshhold /. the last such point found in the subimage is called 1 and we assume pq is a line segment the line tracing procedure is much like the inverse of a bresenham algorithm for drawing lines  with the similar limitation that we can find lines  that are only with 1 degrees of vertical. we find lines more than 1 degrees from perpendicular and lines with gaps by searching in a neighborhood below an edge point for the next adjacent edge point. strictly speaking  our tracing program returns the endpoints of a curve which may not necessarily be a line  but over the small distances in the subimages we search for lines we have found this fast tracing procedure yields an adequate approximation. the line tracing procedure was used in all of the real time continuous motion runs of our vehicle under vision control. 
r. wallace et al. 1 
 a combination of three factors enabled us to reduce the image processing time for each image sample to about   seconds. first  special image processing hardware in our grimmelll gmr 1 display processor was used for the low level correlation and convolution second  only small subimages  1 by 1 pixels  were searched for road edges by the line finding routines. third  selection from among the possible set of candidate road edges of the actual road edges was accomplished by simple means  q.v  . 
 the next step in our plans for development of low level roadfinding vision is to integrate several types of feature detectors in a blackboard data structure. we want to evaluate  the success of combining intensity  texture and color edge and region features to find road edges. earlier we said that we relied on the principle  almost anything works in simple cases . for complicated cases. such as we have encounteied in actual outdoor road scenes  we have found that none of the techniques we have tried always works. we believe that a combination of techniques will enable us to find road edges reliably in a wide range of situations. 
control 
 the control procedure translates the visual measurements into vehicle motor commands that  if successful  keep the vehicle moving along the road. we evaluated a half-dozen approaches experimentally with our vehicles and analytically. one approach  servoing to keep the road image centered in the forward field of view  excelled in all the measures  by such a margin that we feel it deserves to be considered a fundamental navigational principle for mobile robots. 


 let x represent the shortest distance between the center of our vehicle and the centerline of a straight road 1 is the angle between the heading of the robot and the road direction  i.e. when 1 = 1 the robot is driving parallel to the road. suppose the vehicle travels at a constant scalar velocity v  and that control is achieved by superimposing a steering rate. 1 / dt  where t is time  on top of the forward motion. if there is no slippage  the following kinematic relationship will hold: 
 1  
 the general problem for continuous road following is to find a steering function f such that by setting 1 / dt = f x 1  the vehicle approaches the road center. we tried several functions and noticed a number of recurring problems. estimating 1 and x from the image requires both a precise calibration of the camera and accurate determination of the position and orientation of the road edges in the image. both are difficult to achieve in practice  and the high noise level in these quantities made most of our functions unstable. a second problem led directly to our solution. the road image sometimes drifted out of the camera's 1 degree field of view  and in the next sampling period the program would fail to find a road  or  worse  identified some other feature  like a door edge  as road. the obvious solution was to servo to keep the road image centered. experimentally this approach was a stunning success. besides helping the vision  it seemed to be insensitive to even large calibration errors and misestimates of the road parameters. 
 the theoretical analysis was remarkably sweet also  and bore out the empirical observations. a first order analysis  where we assume the road image is kept perfectly centered  gives the relation 
		 1  
 where r is the distance in front of the robot where a ray through the camera image center intersects the ground  i.e. the range at which we do our road finding . the parameter r can be changed by raising or lowering the camera  changing its tilt  or by using a different scanline as the center of the region in which road edges are sought. 
equation  1  can be substituted into  1  to give 
		 1  
which can be solved directly  giving 
 1  

 where x1 is the initial value of x when t = 1. so to first order the vehicle approaches the centerline of the road exponentially with time. 
 a more detailed analysis considers the actual servo loop behavior. the displacement of the road centerline image from the center of the forward field of view is proportional to 
 1  

servoing the steering rate on  1  sets 
 1  
 where g is the servo loop gain. the full behavior of the robot can be found by solving  1  with  1  simultaneously. these equations are made linear and easily solvable by the substitution 1 - sin 1  giving 
 1  

 by co incidence or cosmic significance of all the servo functions we considered  only this one yielded a fully general analytic solution. 
 the solution has three cases distinguished by the sign of the expression 
		 1  
 in all cases the solution converges to x - 1  q  and 1  = 1 exponentially with time. when g   1v/r the convergence is a decaying oscillation - the sluggish steering causes repeated overshoots of the road center. when g   1v/r the solution contains a second exponential  and the robot approaches the road center more slowly. when g = 1v/r  the critically damped case  we have the fastest convergence and no overshoot  and the behavior is given by the equations 
 1  
		 1  
 the gain sets the turn rate required of the robot. note that to retain the critically damped situation while increasing v without changing g  it is necessary only to increase r  i.e. arrange to have the vision look further ahead. 
 the method is successful for several reasons it keeps the road in view at all times. because the system always converges  errors in g or camera calibration do not jeopardize performance. because the parameter being servoed is the most robust direct measurable  namely road position in the image  the noise problems of the other approaches are almost totally eliminated. in particular  1  or q  and x though they occupy a central position in the theoretical analysis  need never be calculated in the actual servo loop. 
conclusions 
 we have developed a vision and control system for a mobile robot capable of driving the vehicle down a road in continuous motion. the system has been tested on two mobile robots  neptune and the terregator  in both indoor  hallway and artificial road  and outdoor  asphalt paths in a park and cement sidewalk  environments. in our best run to date the terregator traversed a 1 meter outdoor path at 1 cm/sec. image processing time has been reduced to 1 sec/image. 
 failure modes of our vehicle have included driving off the road  driving into trees and walls  and driving around in circles. such failures were mostly due to bugs in our programs  imprecise calibration procedures  and limitations of current hardware  e.g.  b&w camera with narrow angle lens   not fundamental limitations of the techniques used. 

r. wallace et al. 1 
future work 
 there are several areas that we plan to address. first is the construction of a true testbed. this involves mostly software engineering  such as cleaning up and documenting the interfaces between vision and control. this will enable us to try other vision methods  such as texture and color operators. 
 further work will require the use of a map  along with program access to a magnetic compass and a gyro. the map will list road direction  width  appearance  and intersections  which will provide strong cues to both the image processing and the navigation system. the compass  along with the map information  will help predict road location in the image. this will become increasingly important as we venture onto curved and hilly roads  and as we encounter intersections and changes in the road surface. 
the next step is obstacle avoidance  which will require limited 
1d processing. projects in the cmu mobile robot laboratory have already demonstrated obstacle avoidance with sonar  and stereo cameras ; we intend to integrate these into the testbed. later work may add a laser rangefinder and programs to handle that data. 
 finally  as the testbed becomes more complicated  system control will become a major issue. we plan to work on a blackboard system with cooperating and competing knowledge sources. all the data  from the lowest level signals to the highest level models and maps  will be on the blackboard and available to ail processes. 
a c k n o w l e d g e m e n t s 
 we would like to thank first of all pat muir for his work on analysis of the control of the terregator. many thanks also to mike blackwell  microprocessor hacker extrodinaire  kevin dowling  tender of robots  and john bares  prime mover of the terregator  without whom the experiments described here would have been much more difficult and much less fun. thanks also to gregg podnar and bob spies for video and digitization work. finally  we would like to express our appreciation to raj reddy for his support and encouragement. 
