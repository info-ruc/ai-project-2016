 
decision tree grafting adds nodes to an existing decision tree with the objective of reducing prediction error. a new grafting algorithm is presented that considers one set of training data only for each leaf of the initial decision tree  the set of cases that fail at most one test on the path to the leaf. this new technique is demonstrated to retain the error reduction power of the original grafting algorithm while dramatically reducing compute time and the complexity of the inferred tree. bias/variance analyses reveal that the original grafting technique operated primarily by variance reduction while the new technique reduces both bias and variance. 
1 	introduction 
decision committee techniques  notably adaboost  freund & schapire  1  and bagging  breiman  1  have demonstrated spectacular success at reducing decision tree error across a wide variety of learning tasks  quinlan  1; bauer & kohavi  in press . these techniques apply a base learning algorithm multiple times to form a committee of classifiers. all committee members vote to classify an unseen case. the success of these approaches has demonstrated that there is room to improve upon the average case prediction performance of standard decision tree learning algorithms. however  decision committees deliver this improvement at a cost. whereas a single decision tree provides a model that is straight forward to interpret  comprehension of a decision committee requires juxtaposition of all constituent models. this is infeasible in non-trivial cases. while it is possible to construct a single decision tree that expresses the model inherent in a decision committee  for even small committees there is an exponential increase in the size of the derived single tree model relative to the unmodified model of the base learning algorithm  quinlan  1 . for any but the most simple of domains  this will reduce ease of comprehension critically. 
¡¡decision tree grafting has been presented as a technique that obtains some of the benefit of a decision committee while creating only a single tree  webb  1 . 
1 	machine learning 
grafting is applied as a postprocess to an inferred decision tree. it identifies regions of the instance space that are not occupied by training examples  or occupied only by misclassified training examples  and considers alternative classifications for those regions. support for those classifications is obtained by considering alternative branches that could have been selected at ancestor nodes to the leaf containing the region in question. if those alternative branches indicate stronger support for an alternative classification than that assigned to the region by the current tree  a new branch is grafted to the tree that imposes the new classification on the region. while the increase in tree complexity of this technique is much lower than that of forming a single tree from a committee  the average increase in accuracy is also lower. also  initial techniques were limited in application to continuous valued attributes. this paper presents extensions to decision tree grafting that extend grafting to discrete valued attributes; dramatically reduce induction time; reduce the complexity of inferred trees; and increase average prediction accuracy. it also provides a bias/variance analysis of grafting's error reduction  demonstrating that the original algorithm reduced error primarily through variance reduction while the new algorithm reduces both bias and variance. 
1 	previous grafting algorithms 
the first decision tree grafting algorithm  was developed to investigate the utility of occam's razor  webb  1 . it was not initially conceived as a practical learning technique. the success of the technique at reducing prediction error led to further development aimed at creating a practical learning algorithm. this led to  a postprocessor for c1 release 1  that demonstrated frequent  if modest  reductions in prediction error from that of the unmodified c1 over a wide cross-selection of learning tasks  webb  1 . 
¡¡appendix a presents the grafting algorithm extended in a straight-forward manner to handle discrete valued attributes. this algorithm investigates in turn each leaf of an existing tree. for each leaf  l  and attribute a  it climbs the tree investigating at ancestor nodes for i alternative cuts on a that would project across the region of the instance space encompassed by l. it as-

sesses for each such alternative cut the evidence that it would have presented about the class of objects that fall within its projection across l  if selected instead of the cut actually imposed at the ancestor. if that evidence provides stronger support for an alternative classification to the support for the original classification at the leaf  the new cut is saved for possible imposition. when all such cuts have been identified at ancestors of l  they are ordered and imposed on l from best supported to least. support for a classification from a leaf or cut is obtained by applying the laplace accuracy estimate 

where pos x c  is the number of members of the set of training examples x that belong to class c. 
¡¡while c1++ is reasonably consistent at reducing the error of classical decision tree induction  the technique has a number of deficiencies. one of these is its computational complexity. the complexity of the algorithm is 1 l - d. a.t   where l is the number of leaves in the initial tree  d is the maximum depth of the tree  a is the number of attributes  and t is the number of training cases. where these values are large  application of the algorithm can become infeasible. much of the efficiency of decision tree learning is derived from the manner in which it partitions the data so that  for the majority of processing  at the leaves   only very small subsets of the training data need be considered. in contrast  c1+ and c1++ require consideration of all training data that reaches each ancestor of each leaf  once for each attribute for each leaf. hence  the entire training set  the data at the root  must be processed once for each combination of attribute and leaf. 
¡¡a further concern is that grafting considers very large numbers of possible grafts  which must substantially increase risk of overfitting the data. that is  there is a substantial risk of finding grafts that appear good by chance. this can be viewed as a form of oversearch effect  quinlan & cameron-jones  1 . 
¡¡finally  one of the justifications for grafting is that it allows non-local information  examples from outside the leaf  to be considered when deciding how to classify regions within the leaf that are occupied by few or no training examples. however  the manner in which external examples may be used is determined by where in the tree they are located relative to a leaf  rather than where 
in the instance space they fall. hence  it is likely that an example  e  that is very close to a region  r  being investigated will have little influence on the classification that is selected if e happens to be divided from r by a cut close to the root of the decision tree. a training case separated by a single cut closer to the leaf / that contains r will have greater influence on the class selected for r  as it will be considered at every ancestor of l as the algorithm climbs the tree. a training case x that fails all tests leading to l will have as much influence as a case y that fails only the test at the root of the tree  even though y may directly adjoin r and x fall at the oppo-

figure 1: example instance space as partitioned by c1 

figure 1: all-tests-but-one partition for highlighted leaf 
site end of the instance space. both will be considered once only  when the algorithm considers cuts that could have been imposed at the root of the tree. 
1 	grafting from the all-tests-but-one partition 
grafting requires a means of estimating the accuracy of a potential new leaf that contains few or no training cases. quinlan  uses the context to estimate the accuracy of a leaf with few training cases. this context is called herein the all-tests-but-one-partition  atbop . the atbop of a leaf l is the region of the instance space that is separated from the region covered by i by no more than one decision surface. it will be reached by any case that fails no more than one test on a path from the root of the tree to l this region is illustrated in figures 1 and 1. the first figure  replicated from webb   provides a representation of a simple two attribute instance space projected as a two-dimensional geometric space. objects of three classes are represented by *  o  and o  where each of these symbols represents a different class. the dashed lines represent the decision boundaries created by the decision tree  presented in table 1  generated by c1 release 1 for this training data. points on a vertical line are included in the region to its left and points on a horizontal line are included in the region below. the region projected in figure 1 is the atbop for the leaf highlighted in figure 1. as these figures illustrate  the 
	webb 	1 

table 1: c1 decision tree for example instance space 

atbop for a leaf i is the region formed by removing all and only the decision surfaces that enclose /. 
¡¡this paper explores the use in decision tree grafting of estimation within the atbop instead of estimation at ancestor nodes. 
¡¡using the atbop to evaluate the evidence supporting alternative classifications of a region within a leaf has a number of advantages over using ancestor nodes. computational requirements are greatly reduced. only one set of data is considered at any leaf /  the cases in the atbop of i   rather than one set for each ancestor of l further  that data set cannot be larger and will usually be considerably smaller than the largest data set used by the original technique-all training data  considered at the root. a further advantage is that the order in which decision boundaries have been imposed by the learning algorithm will not affect the outcome  a cut at the root is treated identically to the deepest cut within a tree. this appears better motivated than the original technique. finally  the number of alternative cuts that are evaluated is considerably reduced  decreasing the opportunity for chance overfitting of the training data. 
¡¡the resulting algorithm is identical to c1++  as presented in appendix a  except that steps 1a and 1a are each replaced by n: n is the all-test-but-one-partition of l 
1 	experimental evaluation 
to evaluate the utility of the new grafting technique  four algorithms-c1  c1+  c1++  and c1a- were compared on 1 data sets from the uci repository  blake  keogh  & merz   1 . these data sets include all of those employed in previous grafting research  webb  1; 1  augmented by a wide cross-
selection of discrete valued data sets. note that the data sets employed in the earlier studies slightly disadvantage grafting as they include the discordant and sick data sets that were specifically added because they were variants of hypo  the only data set on which c1 outperformed c1x in the first experiment conducted. these are the only data sets selected due to specific predictions about the performance of grafting  and those predictions were that grafting would not perform well. 
¡¡all four algorithms were implemented as a single modified version of c1 release 1 that incorporates all variations as options. in each ease grafting was applied to pruned decision trees  as previous evaluation has suggested that this provides the best average-case perfor-
1 	machine learning 
mance  webb  1 . all experiments were run on a sun ultra 1 model 1 with dual 1 mhz cpus. 
   the performance of an algorithm on a data set was evaluated by ten rounds of three-fold cross validation. in each round  the data were divided into three subsets. for each subset s in turn  a decision tree was learned from the remaining two subsets and evaluated by application to s. this procedure was used to support estimation of bias and variance. variance measures the degree to which the predictions of different learned classifiers vary as a result of differences in the training data. variance impacts on error  as if different predictions  of a single test item  are made as a result of different training data  not all can be correct  and so errors must result. bias measures the error due to the central tendency of the learning algorithm. kohavi & wolpert's  bias/variance decomposition was selected for this study as it is both applicable to multiple class domains and close to the original bias/variance decomposition for numeric regression  geman  bienenstock  & doursat   1 . some analyses also consider intrinsic noise  the minimum possible error for a domain  but this research followed kohavi & wolpert by aggregating this quantity into bias. the use of ten rounds of three-fold cross validation replicated kohavi & wolpert's bias/variance estimation technique except that cross-validation was substituted for random sampling  ensuring that all available items were used the same number of times for training  1 times . each item was also used the same number of times for testing  1 . this uniformity in the number of times an item was utilized in each role can be expected to produce greater consistency across different runs of the procedure. under kohavi & wolpert's original procedure  a given training example x might be used any number between zero and 1 times for training and any of zero  1  1  or 1 times for testing. this variability can be expected to increase the  statistical  variance of the measures obtained for accuracy  bias  and  learning  variance. 
¡¡table 1 presents the mean error  number of declassifications divided by total classifications  of the thirty trees learned and evaluated in this manner for each combination of learning algorithm and domain. the last row of this table presents the mean error across all data sets. as can be seen  all grafting techniques narrowly outperform plain c1 on this latter measure. however  this measure should be treated as indicative only  as it is not clear to what extent error rates across different data sets are commensurable. 
¡¡table 1 presents specific comparisons of each pair of learning algorithms. each algorithm is represented by three rows in the table. the first row  labeled r  presents the geometric mean error ratio of each other algorithm against the nominated algorithm. this is the geometric mean1 of c/r  where c is the error for the algorithm 
1
the geometric mean of a set of values x1 to xn is 
 this provides a better aggregate evaluation of a set of ratio outcomes than arithmetic mean as if the geo-

table 1: error by data set 
data set c1 c1+ c1++ c1a anneal audio autos balance-scale breast cancer slov. breast cancer wise. cleveland credit  aust.  credit  german  discordant 
echocardiogram 
glass heart hepatitis horse-colic house-votes-1 hungarian 
hypo 
iris 
kr-vs-kp labor-neg 
lenses lymphography new-thyroid pima diabetes primary tumor promoters 
segment sick sonar soybean large splice junction 
tic-tac-toe waveform 1 
1 
1 1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 1 
1 
1 
1 1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 	all data sets 	1 	1 	1 	1 
to which a column relates  and r is the error for the algorithm to which the row relates. a value below 1 indicates an advantage to the algorithm for the column. a value above 1 indicates an advantage to the algorithm for the row. the row labeled s indicates the win/draw/loss record. the three numbers for each entry indicate the number of data sets for which c   r  c = r  and c   r  respectively. the row labeled p indicates the two-tailed1 binomial probability of obtaining the relevant win/loss outcome by equiprobable chance. 
¡¡c1+ achieves lower error than c1 for twice as many data sets as the reverse  but the geometric mean error ratio very slightly favors c1. it is notable  however  that this latter result can be attributed mainly to the single hypo data set  for which the error differs by only 
metric mean indicates an advantage in the error for algorithm a over that for algorithm b then it will also indicate a disadvantage for 1 over a. arithmetic mean does not have this desirable antisymmetry with respect to ratios of outcomes. 
¡¡¡¡1 two tailed tests are used for consistency throughout because predictions were not made for the outcomes of some pairwise comparisons. 
table 1: summary of relative error 

1  but c1's error is so low that this results in a very high error ratio. if hypo were excluded  the geometric mean error ratio would be 1 in favor of c1+. while the win/draw/loss record is not statistically significant  many of the data sets do not contain continuous value attributes  denying c1+ a chance to alter the performance of c1. 
¡¡both c1++ and c1a outperform c1 on both geometric mean error ratio and win/loss/draw record  the latter advantage being statistically significant at the 1 level on a two-tailed sign test for c1a but not c1++1. c1a outperforms c1++ on geometric mean error ratio  but c1++ achieves lower error than c1a for more data sets than the reverse. this latter advantage is not statistically significant at the 1 level  however. these results do not suggest that either c1a or c1++ holds a strong advantage in general error performance over the other. 
¡¡table 1 presents the relative bias performance of the algorithms. only aggregate results are presented due to space constraints. this table follows the format of table 1 with an additional row labeled mean that corresponds to the final row of table 1. while the means across all data sets vary only slightly  the other comparative statistics suggest that c1 enjoys a slight general advantage over c1+ and c1++  although not statistically significant at the 1 level  and that c1a enjoys a small but consistent and significant  at the 1 level  advantage over c1 and a small advantage over c1+ and c1++ that approaches significance at the 1 level. 
¡¡table 1 presents the algorithms' relative variance performance. all the grafting algorithms enjoy a small but consistent and significant advantage over c1 in this respect. both c1+ and c1++ enjoy an advantage over c1a  this being statistically significant at the 1 level for c1++. there is a straight forward explanation for this outcome. the original grafting techniques allow cuts that could have been imposed high in the tree to be superimposed further down the tree. if small variations in the training data lead c1 to different selections of at-
¡¡¡¡1  as it was predicted that c1++ would outperform €1  it could be argued that a one-tailed sign test should be employed  in which case the outcome of 1 would be significant at the 1 level. 
	webb 	1 

table 1: summary of relative bias 

table 1: summary of relative variance 

tribute for nodes high in a tree  variance is likely to be enhanced. the superimposition of those alternative cuts at lower levels in the tree by grafting will counteract this upwards influence on variance. c1a  by not considering the data at ancestor nodes has less opportunity to superimpose such cuts  but may do so as a consequence of supporting evidence for the cut in the atbop. 
¡¡table 1 analyses the comparative complexity  measured by number of nodes  of the trees produced by each algorithm. all the grafting algorithms consistently increase complexity  as they must . c1a consistently produces less complex trees than the previous grafting algorithms. the mean complexity of the trees produced by c1a is approximately twice that of c1 whereas the ratio for c1++ is more than 1 to 1. 
¡¡a major advantage of c1a is a reduction in computational complexity in comparison to the original grafting algorithms. the average compute times for the four systems are c1: 1; c1+: 1  c1++: 1; and c1a: 1 cpu seconds. for the grafting algorithms these times include both induction of the initial tree and postprocessing to produce the final tree  and hence are consistently greater than those for c1. times exclude reading the training or test data from disk but include application of the classifier to the test data and minor overheads associated with measuring bias and variance. for no data set did c1a more than triple the compute time of c1. the greatest increase in compute 
1 	machine learning 
table 1: summary of relative complexity 

time due to the other grafting algorithms was for the segment data set for which both required 1 times the compute time of the original c1. 
1 	summary and conclusions 
grafting from the atbop has demonstrated a number of advantages over previous grafting algorithms. without signficantly affecting error performance  grafting from the atbop dramatically reduces compute times and the size of inferred trees. grafting has not previously been evaluated in terms of bias and variance. the current studies revealed that the previous grafting techniques operated primarily be variance reduction. grafting from the atbop is slightly less effective at variance reduction than the previous techniques  but introduces a compensating bias reduction effect. the bias/variance operational profile of the original grafting techniques is similar to that of bagging in that it reduces variance only  bauer & kohavi  in press . in contrast  atbop grafting has a bias/variance reduction profile similar to boosting in that it reduces both bias and variance  bauer & kohavi  in press . while the error reduction effect of grafting is of much smaller magnitude than that of bagging or boosting  atbop grafting produces a single decision tree which will usually be much more straight forward to interpret than the committees of decision trees produced by boosting and bagging. in consequence  grafting deserves serious consideration for machine learning applications where it is desirable to minimize error while producing a single comprehensible classifier. 
a 	c1++ algorithm 
let ca1es n  denote the set of all training cases that can reach node n  unless there are no such cases in which case cases n  shall denote the set of all training cases that can reach the parent of n. 
let value a  x  denote the value of attribute a for training case x. 
let po1 x c  denote the number of objects of class c in the set of training cases x. 
let class x  denote the class of object x. 
let laplace x  c  =  where x is a set of training 

cases  is the number of training cases and c is a class. let upperlim n a  denote the minimum value of a cut c on continuous attribute a for an ancestor node x of n with respect to which n lies below the branch of x. if there is no such cut  upperlim n  a  = this determines an upper bound on the values for a that may reach n. let lowerlim n  a  denote the maximum value of a cut c on continuous attribute a for an ancestor node x of n with respect to which n lies below the branch of x. if there is no such cut  lowerlim n  a  = this determines a lower bound on the values for a that may reach n. let prob x n p  be the probability of obtaining x or more positive objects in a random selection of n objects if the probability of selecting a positive object is p. 
to post-process leaf i dominated by class c 
1. initialize to {} a set of tuples t containing potential tests. 
1. for each continuous attribute o 
 a  find values of n: n is an ancestor of i 
 cases n  & v = value a x  k v   min value a y : y  cases l  k class y  = c  k v   lowerlim l  a  
k: k is a class 
that maximize = laplace {x: x  cases n  k value{a x  k value a  x    lowerlim l  a } k . 
 b  add to t the tuple  n  a  v  k  
 c  find values of n: n is an ancestor of i 
v:  : x € cases{n  k v = value a  x  & v   
 max value a y : y  cases l  k class y  = c  k upperlim l  a  
k: k is a class 
that maximize = laplace {x: a; cases n  & value a  x:    v & value a  x  upperlim l  a } k   
 d  add to t the tuple 
1. for each discrete attribute a for which there is no test at an ancestor of l 
 a  find values of n: n is an ancestor of / v. v is a value for a 
k: k is a class 
that maximize  = laplace {x: x  cases n  k value a x  = v} k . 
 b  add to t the tuple  n  o  v  k    =  
1. remove from t all tuples  n a v k ¡ê x;  such that laplace cases l  c  or prob x n laplace ca1es /  c   1. 
1. remove from t all tuples  n  a  v  c    x  such that there is no tuple  n'  a' v'    x'  such that k' c k  
1. for each  n a v k  x  in t ordered on ¡ê from highest to lowest value if x is then 
 a  replace i with a node t with the test a v. 
 b  set the  branch for t to lead to a leaf for class k. 
 c  set the   branch for t to lead to i. 
else if x is   then 
 a  replace i with a node t with the test a   v. 
 b  set the   branch for t to lead to a leaf for class k. 
 c  set the   branch for t to lead to i. 
else  x must be =  
 a  replace i with a node t with the test a = v. 
 b  set the = branch for t to lead to a leaf for class k. 
 c  set the branch for t  implemented as a c1 subset branch  to lead to i. 
