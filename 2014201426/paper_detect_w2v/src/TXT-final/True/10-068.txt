: pecos is a system that synthesizes programs 	other module of the synthesis phase  an efficiency expert 
from abstract specifications through the use of a knowledge 	known 	as 	libra  is 	used to select from 	among these 
base of programming rules. 	the rules deal explicitly with 	alternatives. 
various 	aspects 	of 	the 	programming 	process  	including 
the development of pecos has required research along four 
intermediate-level constructs and certain design decisions. 
	directions: 	 1  the codification of a body of knowledge 
programs 	are 	specified 	as 	abstract 	algorithms 	in 	a 
	about programming; 	 1  the development of a model of 
high-level language for the domain of symbolic computation. 
program construction appropriate for that knowledge;  1  
programs 	are 	constructed 	by 	a 	process 	of 	gradual 
the design of a representation scheme; and  1  the design 
refinement.- pecos retrieves and applies relevant rules until 
	and 	implementation 	of 	a 	system 	that 	can 	apply 	the 
the original abstract description has been refined into a 
	knowledge 	to 	produce 	implementations 	of 	abstract 
concrete program. 	when several rules are relevant in the 
	algorithms. 	no one aspect can be totally isolated from any 
same situation  each is applied separately  enabling pecos 
	of 	the 	others  	but 	this 	paper will 	focus 	on 	the 	last. 
to construct a variety of programs for a given abstract 
overviews of the knowledge base and the model of program 
specification. 
construction will be given  followed by a brief description of 
	the 	representation 	scheme. 	finally  	pecos's 	control 
keywords; 	automatic programming  program refinement  
structure will be discussed and an example of pecos in 
programming 	knowledge  	rules 	about 	programming  
operation will be given. 
knowledge-based systems. 

1. introduction 
pecos is a system that uses a knowledge base of programming rules to synthesize programs from descriptions of abstract algorithms  barstow 1a . the rules embody knowledge about a variety of techniques for implementing data structures and algorithms. this allows pecos to construct several distinct implementations of the same abstract algorithm. the implementation techniques dealt with by the rules include linked list and array representations of sets  simple table-oriented techniques for representing correspondences  and several types of enumeration algorithms with varying degrees of efficiency. 
programs are specified in a high-level language whose constructs are drawn from the area of symbolic programming. these include such information structures as sets and correspondences and such operations as testing for membership in a set and computing the inverse of a correspondence. programs are synthesized by a process of gradual refinement. pecos repeatedly retrieves and applies rules from its knowledge base until the original abstract description has been refined into a concrete implementation in the target language. currently the target language is lisp  in particular  a subset of interlisp  teitelman 1  . when several rules are relevant in the same situation  pecos applies each separately  resulting in the synthesis of a distinct program for each rule. pecos can thus construct several implementations from one specification. 
this ability is central to the role of pecos as the coding 
expert of the psi    program synthesis system  green 
1 1   . loosely speaking  the coding expert's task is to enable the construction of a variety of implementations of the specifications produced by   ' s acquisition phase1. the 
1
  pecos's specification language is that used by ty's program model builder  mccuna 1 . 
	auto. 	prok. 
1 
1. the knowledge base 
most of pecos's abilities are based on its access to a large store of rules about programming. a sample of these rules is presented below. for clarity  the rules are given in english; a brief discussion of the internal representation will be given later. 
rule1: 	a 	collection 	may 	be 	refined 	into 	an 	explicit collection. 
rule pi a sequential collection may be refined into a linked list. 
rules: if a linked list is represented as a lisp list  a test of whether an item is stored in an element cell of the list may be refined into a call to the lisp function member. 
rule1-. if a collection is represented as a correspondence of items to boolean values  a test of whether an item is an element of the collection may be refined into a retrieval of the value corresponding to the item. 
rules: if there is an ordering relation for the elements of a sequential collection  a test of whether an item is stored in some location in the collection may be refined into an enumeration of the elements in the collection  in which the enumeration order is based on the relation and in which the enumeration may be terminated early if an item is found which follows the target item according to the relation. 
rule1: if the enumeration order is the same as the stored order of a collection  the state of the enumeration may be saved as a position in the collection. 
rule1: if the enumeration order is ordered by an ordering relation and the stored order in a sequential collection is ordered by the same relation  the enumeration order is the same as the stored order. 
rule1: if there is an ordering relation for the elements of a sequential collection  the elements may be stored according to that relation. 
rule1: an ordering relation for integers is  less than.  
＊1: 	b a r s t o w 

rule 1: a scheme for remembering a value computed in one place and used elsewhere is to bind the value to a variable. 
rule 1: if a value is remembered as the value of a variable whose name is x  the value may be accessed by retrieving the value of x. 
several observations about the rules are in order. first  note that the rules mention various programming concepts explicitly. the most abstract concepts are those included in pecos's specification language  e.g.   collection   
 correspondence    is-element  . the most concrete are those used in the target language  e.g.   lisp list    member function  . some concepts  e.g.   linked list    position in a sequential collection   represent intermediate notions  more concrete than the specification language and more abstract than the target language. other concepts  e.g.   enumeration order    scheme for remembering a value   represent design decisions involved in symbolic programming. among the benefits of such abstraction levels are significant amounts of knowledge-sharing  e.g.  knowledge about the common aspects of linked lists and arrays is dealt with at the level of sequential collections  and relatively simple extensibility  different abstractions provide a variety of hooks for new rules . 
second  note that refinement plays a central role in the rules. rule1  for example  describes  linked list  as a refinement of  sequential collection.  the essence of such a relationship is that the refined concept  linked list  is a more concrete notion than the abstract concept  sequential collection   pecos uses such rules to drive the refinement process which produces the final program: the application of rule1 to a given sequential collection represents a decision to implement it as a linked list. the validity of such an implementation is often dependent on other parts of the program. for example  rule1 specifies that a 
 get-correspondent  operation is a refinement of an  is-element  operation only if the collection has been refined into a correspondence. 
third  note that the rules are all relatively small. one major motivation for designing the rules this way is that individual rules are intended to reflect individual decisions and choices  rather than groups of choices  thereby making it easier to focus on the nature of each decision. a second motivation is that the rules are then relevant in wide ranges of situations. 
finally  note that some of the rules deal with programming in general and others are specific to programming in lisp. for example  rule1 could be considered a general rule while rule1 could be considered a lisp rule. one of the benefits of such a separation is that the replacement of the lisp rules by rules for some other language might enable pecos to write programs in that language. a set of rules for sail  an algol-iike language  is currently being developed. a few simple sail programs have been written using a combination of previously developed general rules and the newly developed sail rules  ludlow 1 . 
considerable effort has gone into the codification of this knowledge base in the hope of developing a set of rules reflecting notions and decisions inherent in the programming process  although often in the guise of hidden assumptions . taken together  the rules may be considered to be a step toward a  science  of programming: they are a coherent collection of facts  specifying certain concepts and relationships between them  which describe part of the process of programming within the domain of symbolic computation. the explication of such knowledge has been a major goal of this line of research  green and barstow 1  1 . the pecos system demonstrates that such a 
body of knowledge can provide the basis of an automatic programming system; in fact  the rules were developed with this application in mind. however  it is hoped that the concepts and relationships are sufficiently relevant to other aspects of programming that other applications may be found. an interesting possibility might be to investigate their usefulness in the teaching of programming. for the interested reader  a description of the entire collection of rules may be found elsewhere  barstow 1a . 
1. a refinement model of program synthesis 
pecos constructs programs through the application of rules from its knowledge base. this process is based on a model of program synthesis as gradual refinement. this may be simply illustrated as a sequence of program descriptions: 

each description in the sequence is slightly more refined 
 concrete  than the previous description. the first is the program description in the specification language and the last is the implementation in the target language. such sequences will be referred to as refinement sequences and the individual descriptions will be referred to as program descriptions. the process of deriving one description from the previous one will be referred to as a refinement step. 
we may illustrate this through a very simple example  more typical cases often require hundreds of refinement steps . suppose the original specification is: 
is x  integer  an element of y  collection of integers   
 in order to focus on the refinement process  we will use english phrases for program descriptions. the internal form of these descriptions is a collection of nodes and properties as will be discussed in more detail in the next section.  the first refinement step might be to decide to implement the collection  y  as an explicit collection  as opposed to some form of implicit representation  such as upper and lower bounds : 

the next step might refine the  is-element  operation. an appropriate refinement for explicit sets would be: 
 is x  integer  explicitly an element of y  explicit collection of integers   
the next step might be to represent the explicit collection as a stored collection  as opposed to markings on the elements  for example : 
             is x  integer  explicitly an element of y  stored collection of integers   with a corresponding refinement for the operation: 
is x  integer  stored as an element of y  stored collection of integers   

auto. prop.-1: barstow 
1 

we thus have a sequence of twelve refinements taking the original specification and producing a lisp expression that implements it. note that the refinements at each step involve only very small changes. this is characteristic of the programming rules upon which pecos is based. a deliberate effort has been made to identify and separate out decisions that are often implicit. thus  the complete refinement of the collection y took five steps  each corresponding to a particular design decision. note also that some steps may require other steps to have been made previously. for example  the refinements which led from the  is-element  operation to a call to member each required the data structure to have been refined in a particular way. although it is quite common for the refinements of operations and data structures to proceed in figure 1 
fnch box is a node. the labels inside the boxes are the concepts and the labeled arrows are properties. in the refinement sequence presented earlier  the arguments to the is-element operation were given merely as x and y. in general  such arguments are links to nodes representing operations which return values to be used as the arguments.  the values are indicated by the result-data-structure property.  in this case  one of 
1
 read  is x an element of the inverse mapping of y under the correspondence z   

a u t o . 	p r o g . - 1 : 	b a r s t o w 
1 

these 	operations 	is 	a 	remembered-value 	node  
specifying that the value to be used is computed elsewhere in the program. the other operation is an inverse node. note that the result of the inverse operation is represented explicitly by the collection node  although it is only implicit in the corresponding english description. this node represents the data structure passed from the inverse operation to the is-element operation. rules for refining inverse generally have conditions on the data structure produced  to insure that the representation is what the refinement produces. likewise  rules for refining is-element generally have conditions on the argument data structure in order to insure that the refinement can accept that representation for its arguments. since the same node  the collection node  is used in both cases  the two refinements are coordinated. 
there are only a few types of changes that may be made in a single refinement step: the introduction of new nodes  the addition of properties to existing nodes  and the refinement of one node into another1. other types of changes  e.g.  modification of an existing property  may be imagined  but no use for them has yet been found. 
1. rules in the knowledge base 
the internal representation of the programming rules is based on two observations. first  the changes made in each refinement step are generally quite small and are of certain specific types. second  the rules all have the form of condition-action pairs  where the conditions are patterns to be matched against subparts of descriptions  and the actions are instances of the types of modifications just mentioned. these observations lead to a classification of rules into a set of rule types: 
refinement rules refine one node pattern into another. the refined node is typically created at the time of rule application. these are the rules which carry out the bulk of the refinement process  and are by far the most common type. rule1  rule1. rull1. and rules are refinement rules. 
property rules attach a new property to an already existing node. rule1 and rule1 are property rules. property rules are often used to indicate explicit decisions which guide the refinements of distinct but conceptually linked nodes. for example  rule1 specifies how references to a data structure are to be refined. 
query rules are used to answer queries about a particular description. such rules arc normally called as part of the process of determining the applicability of other rules. rule1 is a query rule. 
the internal representation of the rules is based on this categorization of rule types: 
 ref- 	 node pattern  	 refincment node   prop-  property name   node pattern   value  
　 query-  query pattern   query answer  where ref-  prop-  and query- are tags indicating rule type. in ref- and prop- rules  each  node pattern  consists of a  conccpt  and an  applicability pattern . 
1
  although it is not strictly accurate   refinement  may bo considered to mean the replacement of the more abstract node by the refined node. for various reasons  pecos actually retains the abstract node as well as the refinement node. 
several other issues have arisen in the design of the rule base organisation  including an indexing scheme for efficient rule retrieval  the details of the pattern matcher  and the breakdown of the condition patterns into separate parts for different uses. detailed discussions of these issues may bo found elsewhere  barstow 1a  1b . 
it should be noted that several kinds of rules are not easily expressed in the current formalism. for example  more general inferential rules  such as the test constructor used by manna and waldinger  manna and waldinger 1   and rules about certain kinds of data flow  such as merging an enumeration over a set with the enumeration that constructed it  can not be described conveniently with the current set of rule types. it is not clear how difficult it would be to extend pecos to handle such cases. 
1. control structure 
pecos uses a relatively simple task-directed control structure to develop refinement sequences for a given specification: in each cycle  a task is selected and a rule applied to the task. when several rules are applicable  a refinement sequence can be split  with each rule applied in a different branch. pecos thus actually creates a tree of descriptions in which the root node is the original specification  each leaf is a program in the target language  and each path from the root to a leaf is a refinement sequence*. while working on a given task  subtasks may be generated; these are added to the agenda and considered before the original task is reconsidered. 
there are three types of tasks  corresponding to the types of changes involved in refinement steps and to the different rule types : 
 refine n  specifies that node n is to be refined. 
 property p n  specifies that property p of node n is to be determined. 
 query rel argl argz ...  specifies that the query  rel argl arg1 ...  must be answered. 
when working on a task  relevant rules are retrieved and tested for applicability. for example  if the task is  refine 1  and node 1 is an is-element node  then all rules of the form  ref- is-element... .. .  will be considered. when testing applicability  it may be necessary to perform a subtask. for example  an argument may need to be refined in order to determine if it is represented as a linked list. this is  in fact  quite common: the refinement of one node is often the critical factor making several rules inapplicable to a task involving another node. 
with its current knowledge base  pecos usually succeeds at achieving each task.  hence  most refinement sequences lead to complete programs.  the major reason for this lies in the knowledge base: there are refinement rules for each intermediate level concept  and property rules for each of the necessary properties. however  some sequences lead to situations where no rules are applicable  and pecos abandons such linos of development. 
in the interests of brevity  further details of the control structure and context mechanism used for the tree of descriptions will not be considered here. more detailed discussions may be found elsewhere  barstow 1a  1b . 
fl
  the nature of such refinement trocs is central to the practicality of this approach and will be discussod in section 1. 

auto. 	p r o g . - 1 : 	barstow 

1. pecos in operation 
lot us now look at pecos implementing a different algorithm for the 1s-element operation  in order to illustrate the nature of the tasks and the way that pecos focuses design decisions. recoil that the original specification was; 
is x  integer  an element of y  collection of integers   
we will pick up the refinement process at the point where y has been refined into a sequential-collection  and where the 1s-element operation has been refined into an is-stored-in-a-location operation. the node and pioperty representation is given in figure 1  with several nodes omitted in order to emphasize the important ones . the numbers on the left will be used to refer to the nodes. 
the 	task 	is 	now 	 refine 	1   	that 	is  	to 	refine 	the is-stored-in-a-location 	node. 	in 	the 	example 	in section 1  we followed the path where this was refined into a test on whether x was stored in an element cell of y. another refinement rule  rule1  is applicable here  but only if there is an ordering relation for the elements of the sequential 	collection 	y  	so 	a 	subtask 	is 	established:  property ordering-relation 1 . 	one rule for this task  rulf1  is applicable only if the items are integers  as is the case here. 	pecos applies this rule and less-than is attached as the ordering-relation property of node 1. 	having achieved the subtask  pl cos reconsiders the original task  refine 1   and rules is now applicable. this produces 	an 	enumerate-items 	node  	where 	the enumeration is constrained to be in the order specified by the ordering relation associated with the elements of y  less-than  and where the search may be terminated prematurely 	 before 	all 	the 	elements of 	y 	have 	been examined  if an element is found which follows x under the same 	ordering relation. 	constraints sue!  	as 	these are specified 	by 	attaching 	properties 	to 	the 
enumerate-items node  as illustrated in figure 1. 
in turn  the enumerate-items node  1  is refined into a loop structure which uses an enumeration-state node to determine how far the enumeration has proceeded. the refinement of this node coordinates the refinements of the parts of the program that refer to it  initialization  termination test  incrementation   as illustrated in figure 1. note how nodes 1  1  and 1 all have links to node 1. this is typical of the way that pecos focuses design decisions: the refinements of the various parts are all dependent on a 
single node. 
when 	pecos 	tries to refine the enumeration-state node  rule1 is applicable only if the elements are to be enumerated in the order in which they are stored. 	this causes 	a 	subtask 	to 	be 	generated: 
 query stored-enumeration-order 1 . 	the 
enumeration-order has already been constrained to be 
ordered by the less-than relationship. all that is needed  then  is to determine whether the sequential collection is also ordered. the subtask for this is:  property stored-order 1 . two rules are applicable here  and we will follow rule1  which specifies that the items may be ordered if there is an ordering relation for them. since an ordering relation  less-than  has already been found  rule1 can be applied  resulting in  ordered less-than  as the stored-order property of node 1  a 
	auto. 	p r o r . - 1 : 	barstow 
1 

further specification on the representation of y1. now rule1 can provide the answer  true  to the query  and rule 1produces a p1sit1n-1 t-c1llectj1n node as a refinement of node 1. the rest of the refinement path is relatively straightforward. note  however  that at this point 
y still has not been refined further than a sequential collection. aji of the rules that have been used are independent of the particular implementation chosen for the sequential collection; linked lists and arrays would work equally well. this is an example of the v/ay the rules in the knowledge base have been explicitly designed to operate at the highest possible level of abstraction. before the program can be completed  of course  this decision must be made  and we will assume that  again  a linked list is chosen. this is the last interesting aspect of the refinement sequence. for the curious  the final program looks like the following  with mnemonic labels for increased readability : 

if the is-element operation were embedded in the specification of a jarger program  the decision to represent y as an ordered sequential collection would presumably have wider ramifications  possibly including the necessity of sorting y. this is characteristic of the programs that pecos writes: multiple representations are often considered for what in the original specification was the same data structure  with the program converting representations as necessary. 
1. a space of partially implemented programs 
one of the motivations for this work is the observation that different implementations of abstract concepts are appropriate in different situations. pecos's refinement tree represents a space that can be searched for such an  appropriate  implementation. in fact  the ability to construct such a space was one of the primary goals in developing pecos's rules1. since determining the appropriate implementation involves exploring this space  the sizo of pecos's refinement trees is a critical issue. 
1
 note that when that the decision is taken the other way 
 to leave the elements unordered   the refinement path requires the construction of a considerably more complicated enumeration  one which searches the collection at each stage for the smallest element  then compares that to x  and so on. the result is an implementation of the is-element operation that requires on the order of n1 comparisons! 
1
  note that the nodes of this space all represent somantically correct programs; that is  they all implement the original specification  although at varying levels of detail. the use of refinement rules enables pecos to proceed directly toward correct implementations  without any need for testing different combinations of primitive constructs to see if they are correct. 
one aspect of the size is the length of the individual refinement sequences  i.e.  the number of rule applications . this varies greatly  but preliminary experiments indicate that about two rule applications are required per cons cell in the s-expression for the final program. thus  if the correct choice can be made without expanding any alternative paths  the cost of constructing the best implementation is roughly linear with the size of the final program. this cost may be reduced through the use of special purpose rules to handle frequently occurring cases. such rules can collapse an entire sequence of rule applications into a single rule. while the rules to derive the sequence must still be available for use in other situations  one can frequently get by with a  short-cut  rule. pecos currently has a few such rules and more will be added soon. 
in an effort to reduce the entire space  pecos currently postpones consideration of all choicepoints for as long as possible1. this technique  together with the use of several levels of abstraction  results in trees that tend to be  skinny  at the top and  'bushy  at the bottom. in the membership test  for example  by delaying the refinement of the sequential collection  into either a linked list or an array   1  1 refinement steps were made before working on the choice; following the split  1 further steps were required in the linked list case and 1 steps in the array case. had the split been made when the task was first considered  splitting would have occurred after 1 steps  leaving 1 left in the linked list case and 1 in the array case. thus  delaying the choicepoint cut the total number of rule applications in half. 
the techniques mentioned above are intended to reduce the total size of the space. another simple technique helps to reduce the amount of the space that must be searched. when two parts of the program arc totally independent  and both involve choices  the entire space represents the cross-product of the alternatives in  the two cases. however  under the assumption that the independence of the parts enables a choice for one to be made independently of any work that has been done on the other  the exploration of some of the paths can be avoided. the basic idea is that one of the choicepoints be carried through to conclusion and one of its alternatives be chosen before considering the other choicepoint. then the alternatives for the second choicepoint need only be considered in the chosen path for the first. 
although these simple techniques can help to reduce the size of the space  the problem of actually making choices still remains. perhaps the greatest benefit of the use of abstraction and refinement is the way that such choice-making is facilitated. since each step is quite small  the effects of alternative branches can be relatively easily understood. it is usually unnecessary to complete all implementations in order to determine which alternative is preferable. in addition  the uniform representation of descriptions in refinement sequences facilitates the use of analytic planning techniques based on cost estimators for the unrefined parts. work on libra ol' 's efficiency expert  has included the development of such heuristic and analytic techniques for making choices  kant 1 . pecos and libra have been interfaced and now operate as ty's synthesis phase. a discussion of their interaction  as well as a detailed example of their joint operation  is available elsewhere  barstow and kant 1   
　　several other artificial intelligence programs have employed similar tactics  e.g.  noah  sacerdoti 1  . 

a u t o . 	p r o p . - 1 : 	b a r s t o w 
1 

1. discussion 
the use of large amounts of detailed knowledge has become on increasingly important paradigm for developing artificial intelligence systems. however  most approaches to automatic programming have concentrated on the development of relatively general techniques  e.g.  synsys  manna and waldinger 1  . pecos's rules enable the construction of larger and more varied programs  but the price paid is a loss of generality. the  ultimate  automatic programming system will presumably employ both techniques. although current trends in programming methodology emphasize abstraction and refinement  most automatic programming work has not used intermediate level concepts extensively. low's data structure selection system involves mapping directly from abstract concepts to machine-level constructs  low 1   most problem-solvers deal directly with the primitive operations of the target language. 
currently  may 1   there are about two hundred rules in pe cos's knowledge base. in the next few months  this number will increase to about three hundred. this enlargement of the rule base will concentrate on higher level knowledge  e.g.  alternative set representations   as the current rules involving low level knowledge should be able to handle most of the low level details. even with its current rule base  pecos is able to construct a variety of implementations for specifications of many different programs. in theory  pecos could construct implementations for any program expressible in the high-level specification language. in practice  time and space limitations prevent the consideration of programs whose specification is more than about  a page.  the number of implementations possible for a given program specification varies considerably with many factors  for instance  whether a particular primitive is known to be numeric or symbolic   so the best indication is perhaps to take a simple example. pecos can implement the  is-clcmcnt  program considered in this paper in about a dozen ways  with the principle variations being the use of linked lists  arrays  or very simple hash tables for the data structures  and the use of several types of enumeration for the algorithms. the most complex program that pecos has implemented is one involving manipulation of an inverted correspondence  i.e.  indexed by range values . the specification was about a half page in length and the implementation was about 1 lines of interlisp code. 
the development of pecos represents the final stage in an experiment investigating a knowledge-based approach to automatic program construction. the essence of this approach   involves the identification of concepts and decisions involved in the programming process and their codification into individual rules. these rules are then represented in a form suitable for use by an automatic programming system. the benefits of this approach seem to lie in the variety of programs  with respect to both data structures and algorithms  that can be implemented. a side benefit is that the rules themselves constitute a detailed explication of knowledge about symbolic programming. pecos demonstrates that such a knowledge base can serve as the basis of an automatic programming system capable of constructing a variety of implementations for abstract specifications. 
1. acknowledgements 
cordcll green  my thesis advisor  and the other members of the ty project have been a source of motivation and focus for this work. interaction with elaine kant and her work on libra has been especially beneficial. juan ludlow's development of rules for sail exposed some of the hidden assumptions in my rules. brian mccune contributed greatly to the design of the specification language. randy davis and jorge phillips have provided very helpful comments on earlier drafts of this paper. 
1  references 
 barstow 	1a  
barstow  david r. automatic construction of algorithms and data structures using a knowledge base of programming rules  forthcoming ph.d. thesis  stanford university  1. 
 barstow 1b  
barstow  david r. a knowledge base organization for rules about programming. workshop on pattern-directed inference systems  may 1/  to appear in sigart  june 1 . 
 barstow and kant 1  
barstow  david r.  and kant  elaine. observations on the interaction of coding and efficiency knowledge in the psi program synthesis system. proceedings of the second international conference on software engineering  san erancisco  california  october 1  pages 1. 
 green 1  
green  cordcll. the design of the psi program synthesis system. proceedings of the second international conference on software engineering  san francisco  california  october 1  pages 1. 
 green and barstow 1  
green  c. cordell  and barstow  david r. a hypothetical dialogue exhibiting a knowledge base for a program understanding system  in elcock  e. w.  and michie  d.  eds. . machine representations of knowledge. ellis horwood ltd. and john wylie  1. 
 green and barstow 1  
green. c. cordell  and barstow  david r. some rules for the automatic synthesis of programs. advance papers of the fourth international joint conference on artificial intelligence. tbilisi  georgia  ussr  september 1  pages 1. 
 kant 1  
kant  flaine. 1he selection of efficient implementations for a high lei/el language. proceedings of acm sigart-sigplan symposium on artificial intelligence and programming languages  august 1. 
  l o w 1  
	low  	j. 	automatic 	coding: 	choice 	of 	data 	structures. 
stanford university  computer science department  aim-1. august 1. 
 ludlow 1  
ludlow  juan. masters project. stanford university  1. 
 manna and waldinger 1  
manna  zohar and waldinger  richard. the automatic sytithcsis of recursive programs. proceedings of acm sigart-sigplan symposium on artificial intelligence and programming languages  august 1. 
 mccune 1  
mccune  brian p. the psi program model builder: synthesis of very high-level programs. proceedings of acm sigart-sigplan symposium on artificial 
intelligence and programming languages  august 1. 
 sacerdoti 1  
	sacerdoti earl. 	the nonlinear nature of plans. 	advance 
papers of the fourth international joint conference on 
artificial intelligence. tbilisi  georgia  ussr  september 1  pages 1. 
 teitelman 1  
tcitelman  warren. interlisp reference manual. xerox 
palo alto research center  palo alto  california  december 1. 

auto. 	pro/*.-1: 	barstow 
1 

informality in program specifications 
robert balzer  neil goldman and david wile 
information sciences institute 
university of southern california 
1 admiralty way 
marina del rey  california 1 
abstract 
　　this paper is concerned primarily with  1  the procedure by which process-oriented specifications are obtained from goal-oriented requirement specifications and  1  computer-based tools for their construction. it first determines some attributes of a suitable process-oriented specification language  then examines the reasons why specifications would still be difficult to write in such a language. the key to overcoming these difficulties seems to be the careful introduction of informality based on partial  rather than complete  descriptions and the use of a computer-based tool that uses context extensively to complete these descriptions during the process of constructing a 
　　well-formed specification. some results obtained by a running prototype of such a computer-based tool on a few informal example specifications are presented and  finally  some of the techniques used by this phototype system are discussed. 
/. 	introduction 
　　a critical step in the development of a software system occurs when its goal-oriented requirements specification is transformed into a process-oriented form that specifies how the requirements are to be achieved. only after this transformation has occurred can the feasibility of the system be analyzed and the consistency of the process specification with the requirements be verified. the key to this transformation is expressing the process-oriented specification abstractly so that its functionality is completely determined while the class of possible implementations remains broad. 
　　we believe that such abstract process-oriented specifications are the key to rationalizing the software development process. such specifications are  in reality  programs written in a very high level abstract programming language. as such  they could provide an effective interface between the two major software concerns: functionality and efficiency. these concerns should be decoupled so that the functionality of a system can be addressed before its efficiency has been considered. once functionality has been accepted  it can be preserved while the system is optimized. thus  since the abstract process-oriented specification is a program  its consistency with the requirements could be formally verified  informally argued  or tested by actually executing the specification. furthermore  the end user could be given hands-on experience exercising the specification to see if it behaved as intended. deviations and/or inconsistencies could be corrected in the specification before any implementation occurred. 
　　once the system's functionality has been accepted by the user  the efficiency of the system in meeting its performance requirements remains an issue. such efficiency must be gained without altering the system's accepted functionality. we have argued elsewhere  that a computer-based tool can 
note: this research was supported by the defense advanced research projects agency  darpa  under contract no. 
dahc1 c 1  arpa order no. 1  program codes 1 and 1. 
be built which guarantees maintenance of functionality while a program is optimized without sacrificing the programmer's ingenuity or initiative in determining how best to achieve 
efficiency. 
　　in this paper we are concerned primarily with the procedure by which such process-oriented specifications are obtained and with computer-based tools for their construction. we will begin by determining some attributes of a suitable process-oriented specification language  then examine why specifications would still be difficult to write in such a language. we will argue that the key to overcoming these difficulties is the careful introduction of informality based on partial  rather than complete  descriptions and the use of a computer-based tool which utilizes context extensively to complete these descriptions during the process of constructing a well formed specification. we will then present some results obtained by a prototype of such a computer-based tool on a few informal example specifications. finally  we will discuss some of the techniques used by this prototype system. 
1. attributes of suitable process-oriented specification languages 
　　as stated above  a suitable process-oriented specification must completely define functionality  represent a broad class of possible implementations  and be executable. 
　　how can we obtain such a language  we begin by noting that a suitably abstract programming language is a 
　　specification language. several recent languages almost meet the above requirements for an executable specification language. they have arisen from two separate disciplines: 
　　1. specification languages. languages  such as rsl  psl  etc.  designed specifically for specification  describe a system in terms of data flows and processing units but do not functionally define the processing. such languages can provide a simulation of the described system down to some level of detail  but cannot describe or simulate its full functionality. 
　　1. abstract programming languages. spawned by dijkstra's notions of structuring  a generation of programming languages  clu  alphard  euclid  pearl  has bloomed which isolate the definition of data objects  and the operations allowed on them  from their use and manipulation in the program. the result is the ability to use abstract program entities which model those that occur in the application being programmed. these entities are defined in terms of more computer-science-oriented entities  which are  in turn  defined in terms of more primitive ones  until the primitive objects and operations of the language are reached. without the successive refinements of the abstract objects and operations  these languages would be suitable for specification  except that they would then lose their property of executability. their executability has been gained at the expense of complete specification of implementation  down to the base level of the language . 
what is clearly needed  then  is a language which can fully specify a system functionally without fully specifying its implementation. what are the required properties of such a language  
　　first  it must be able to define and manipulate application-oriented objects  as is done by the abstract programming languages . second  the description of these objects and operations must be in terms of some formalism that does not require successive refinement to gain functionality and that does not overly constrain the 

auto. 	pror:. - 1 : 	r a l z e r 
1 

implementation. this is the key issue that would enable specification and programming languages to be unified. 
　　three formalisms have been proposed for this role: sets  axiomatic specification  and relational data bases. 
　　one of the earliest efforts is jack schwartz's setl language. sets are the single abstract type allowed for which multiple implementations exist. all the operations on sets can deal with any of the implementations. thus  users need not be concerned with any of these implementations while specifying the manipulations to be performed on their sets. because functionality was completely captured by the setl definitions of sets  implementation did not have to be considered. however  such implementation-free functionality existed only for sets and was not extensible. 
　　more recently  guttag  horowitz  and musser  have discussed an axiomatic specification technique in which the functional behavior of new abstract objects are axiomatically defined by algebraic equations. these algebraic equations act as functional requirements which any implementation of the objects and operations upon them must satisfy. furthermore  they provide a way of executing programs using the operations directly without providing any implementation. whenever an operation is performed on an object  the  state  of that object is transformed by applying the algebraic equation for that operation to the existing  state.  the resulting state is just another expression in the algebra. as more and more operations are performed  these states become more complex. however  the states can be simplified by general rules of the algebra such as and a false - false  or by using the equations for the abstract objects as rewrite rules  such as for a stack  pop push a x  = a. such equivalence rules are part of the functional definition of the operations on the abstract objects. if the axiomatic functional definitions are complete  then specifications in this language can be directly executed while no implementation need be selected and the choice of possibilities has not been constrained. these axiomatic functional definitions provide a user the capability of adding arbitrary new abstract types to the language that can be manipulated in an implementation independent way. this extensible capability is exactly analogous to setl's built-in capability to manipulate sets in an implementation-independent way. 
　　finally  we have languages in which the  state  is represented by a series of assertions in a relational data base  rather than by an expression  and in which the effects of an action are expressed as a series of additions or deletions to the data base rather than as an equation to be applied to the  state.  the big difference between these two approaches is that in the axiomatic approach the functional definitions are expressed as interactions between the operations on a data type and hence do not rely on any more primitive notions. in the relational approach  as in setl  each operation is functionally defined in terms of how it affects a built-in primitive notion  the relational data base. 
　　the self-defining  or closed  property of axiomatic definitions would seem to favor that approach because each abstract object and its operations can be considered in isolation without relying on outside semantics and without specifying any constraints on the implementation. unfortunately  this property comes at the expense of expressing the behavior of objects entirely in terms of the operations upon them and the need to express this behavior in the form of algebraic equations so that the equivalence of alternative sequences of operations can be formed  e.g.  the pop push a x  - a equivalence cited earlier for stacks . 
　　in the relational approach  rather than stressing a closely knit set of types and operations on them  objects are perceived entirely in terms of their relationships with each other and a set of primitive operations which allow these relationships to be built and destroyed and to be extracted. non-primitive operations exist on the objects  but they merely alter the set of relationships that exist between the objects. this view allows incremental elaboration of objects  their relationships with each other  and operations upon them. most importantly  this approach enables objects and operations to be modeled almost exactly as they are conceived by the user in his application  as measured by how they are expressed in our most unconstrained form of communication-natural language . 
　　this latter property is the reason we have selected the relational approach: we feel it minimizes the difficulty that a user would have in constructing an operational specification. 
1. why 	operational specifications are hard to 
construct 
　　unfortunately  even when the user's difficulties in constructing operational specifications are minimized by the use of the relational approach  the task remains burdensome and error-prone  primarily because although a suitable language has been chosen  it is still formal. each reference to an object or action must be consistent and complete. the large number of interacting objects  actions  and relationships require the user to do a great deal of  error-prone  clerical bookkeeping which impedes his attention to the specification itself and reduces its reliability. 
　　suppose we constructed a computer aid which relieved the user of these clerical chores. how would the specification task be altered  we begin by considering how people specify software systems when unconstrained by computer formalisms. 
1. semantic constructs 	in natural language 
specification 
　　we studied many actual natural language software specifications. the main semantic difference between these specifications and their formal equivalent is that partial descriptions instead of complete descriptions are used. when such partial descriptions are understood it is because they can be completed from the surrounding context. the partial descriptions focus both the writer's and the reader's attention on the relevant issues and condense the specification. furthermore  the extensive use of context almost totally eliminates bookkeeping operations from the natural language specification. these are some of the properties we find so useful in natural language specifications and which we so sorely lack in formal specification languages. 
　　we have evidence  see sections 1 and 1   in the form of a running prototype system that these properties can be added to a previously formal specification language and that a computer tool can complete the partial description from the existing context. such a capability is not totally new; it already exists in limited form. 
a u t o . 	p r o r   - 1 : 	b a l z e r 
1 　　most programming languages use the context provided by declarations to complete partial descriptions of the operations to be performed on those objects  e.g.  add becomes either integer-add or floating-add  depending on the declared attributes of its operands . the codsyl dbdtg report  goes further in the use of context by completing partial references to an item by use of the  current  instance of that item as established by some other statement in the program. data base declarations are also used to determine how various program variables are to be used in completing partial descriptions of data base items. 
　　these uses of context in programming languages have been accepted  and even championed  because for each use  the context-providing mechanisms are well-defined  the completion rules are simple and direct  and only a single interpretation is valid. 
　　the context mechanisms we are proposing here are much more complex  the context generated much more diffuse  and a given partial description may produce zero  one  or several valid interpretations. zero valid interpretations means that the partial description is inconsistent with the existing context. a single valid interpretation means that the partial description can be unambiguously completed through use of the existing context. multiple valid interpretations indicate that sufficient context does not exist to complete the description and that interaction with the user is required to resolve the ambiguity. 
　　our work should be viewed as an effort to provide more general context mechanisms to resolve the ambiguity introduced in the specification by partial descriptions. if  as we believe  such mechanisms can be provided  would they be a beneficial addition to specification languages  
1. 	desirability 	of 	informality 
　　we recognize that our approach is controversial and apparently opposes the current trend to make program specifications more and more formal and to introduce such formalisms earlier in the development cycle. we believe closer examination will reveal that our approach is not only compatible with the desire for increased formalism  but a necessary adjunct to it. 
　　attention has been focused on formalisms for program specification to the exclusion of concern with the difficulty and reliability of creating such formal specifications and with maintaining them during the program life-cycle. our approach specifically addresses these issues. 
　　first  it should be recognized that informality will always exist during the formulation of a specification. the issue is whether the informal form is explicitly entered into the computer and transformed  with the user's help  into the formal specification  or whether it exists only outside the computer system in someone's head or written somewhere in unanalyzable form. we should consider  then  the feasibility and the desirability of a computer-based tool to aid in the transformation of an informal specification into a formal one. 
　　let us begin with the question of feasibility. while the results presented in the next section are preliminary and the examples chosen far smaller and simpler than real specifications  we are optimistic about continued progress and ultimate practicality of this approach. however  since these results are far from conclusive  we invite the reader to reach his own conclusions after considering the examples of the next section and the description of the prototype system which follows them. 
　　assuming for the moment that such a system is feasible  we consider its desirability. informal specifications have three obvious advantages. first  they are more concise than formal specifications and focus both the specifier's and the reader's attention. they are more concise because only part of the specification is explicit; the rest is implicit and must be extracted from context. attention is focused on the explicit information and  therefore  away from the implicit information  which increases both the readability and the understandability of the specification. 
　　the second advantage is that informal specifications which employ partial rather than complete descriptions are a familiar  in fact normal  mode of communication. this reduces the training requirements of users  permits a wider set of users  and reduces dependence on the judgment and accuracy of intermediaries. 
　　the final advantage deals with the maintainability of the system. since about 1. of the total life cycle costs of large systems are for maintenance  any improved capabilities in this area are very significant. as we have argued elsewhere   the main deterrent to maintainability is optimization. optimization spreads information throughout a program and increases its complexity through increased interactions among the parts. both of these optimization effects greatly impede the ability to alter the program. an obvious solution is to alter an unoptimized specification and then reoptimize the program. no cost-effective and reliable technology currently exists for such reoptimization  though one has been proposed en 
　　a similar situation exists between the informal and formal specifications. the creation of a formal specification involves spreading implicitly specified information throughout the specification and increasing the complexity by structuring the specification into parts and establishing the necessary interfaces between them. as before  both of these formalization effects greatly impede the ability to modify the specification. again  a solution is obvious: modify the informal specification and retransform it into a revised formal specification. under the assumed feasibility of our approach  this solution would be possible and would greatly simplify maintaining the formal specification of the system. 
　　we now consider three possible disadvantages of a computer-based tool to aid in transforming an informal specification into a formal one. the first possible disadvantage is that the informal constructs will be misunderstood by the computer tool. this is entirely possible  just as it is when a human intermediary interprets an informal specification. while the computer tool cannot match human performance in understanding the informal specification  it operates much more methodically. it can question the user when it detects that there are alternative interpretations of some statement. it can record and make explicit all assumptions it makes in transforming the formal specification. it can paraphrase the informal specification to verify that its interpretation is accurate  the current prototype system records its assumptions and interacts with the user to determine the correct interpretation of unresolved ambiguities  but does not yet contain any paraphrase capabilities . thus  feedback and interaction with the user can eliminate the problem of possible misinterpretation of the informal specification. 
　　the second possible disadvantage is that the computer-based tool will decrease the reliability of the transformation to a formal specification. if the informal specification exists only outside the computer system  then we must rely on the accuracy of the user or  more often  on some trained intermediary to accurately transform it into a formal specification. this transformation depends upon properly understanding the informal specification  see previous paragraph   then restating it in the required formalism. once the proper understanding has been obtained  the restatement involves moving information from one place to another and changing its form. history would indicate that such clerical bookkeeping transformations are error-prone and can always be done more reliably by a computer tool. hence  once the 

a u t o . 	p r o r . - 1 : 	b a l z e r 
1 

correct interpretation has been obtained through the use of context and interaction with the user  the restatement of the informal specification into the required formalism can be more reliably performed by the computer-based tool than by the user or his intermediary. therefore  reliability would be improved rather than reduced by such a tool once understanding was obtained. 
　　understanding  rather than reliability  thus emerges as the key feasibility issue. one way to improve understanding is to increase the interaction with the user. this leads to the third possible disadvantage: that the required volume of interaction will abrogate the advantage of informality. we do not expect this to be an issue with the current system or its successors  since we feel that its current performance level  as evidenced in the following section  indicates that the required interaction rate would be sufficiently small to prevent annoying or sidetracking the user. 
　　thus  we conclude that the availability of such a computer-based tool would be highly desirable because it would simplify the creation of a formal specification while increasing the reliability of the formulation process; improve the maintainability of the formal specification; reduce special training requirements; and expand the base of potential users. the question of feasibility  which remains as the paramount issue  rests clearly on the ability to correctly interpret an informal specification. we therefore now present some preliminary results obtained by the prototype system and describe its operation so that the reader can observe its performance level and judge for himself the generality of its context resolution mechanisms and therefore its feasibility. 
1. 	results 
　　this section presents two examples successfully handled by the prototype system. the examples were extracted from actual natural language specification manuals  and the results illustrate the power of the system's context mechanisms. however  our system is a prototype and  as such  it is far from complete. new examples currently expose new problems which are resolved by adding new capabilities to the system. therefore  until some measure of closure is obtained  it should not be assumed that the prototype will correctly process new examples of the same  complexity  as earlier examples. our goal is to add each new capability in as general a form as possible so that when it is used in new examples it will function correctly. in this way we expect to  grow  the system as more complex and varied examples are tried. 
　　for each of the examples  we present three figures: the actual parenthesized version of the informal input currently used by the system  to avoid syntactic parsing problems  ll   a manually marked version which indicates some of the informalities to be resolved by the system  and a stylized version of the formal output program produced by the system. 
　　the first example is a system which automatically distributes messages to offices on the basis of a keyword search of the text of the message. figure 1 gives the informal natural language description. figure 1 indicates some of the imprecisions contained in this example which must be resolved to obtain the system's formalization of this specification as an operational program  figure 1 . 
　　to give some measure of the amount of imprecision in this example and  therefore  the amount of aid provided by the system  we have compiled the following statistics: 
number of missing operands 	＊ 1 number of incomplete references 	＊ 1 number of implicit type conversions 	  1 number of terminology changes 	  1 number of refinements or elaborations - 1 number of implicit sequencing decisions - 1 actual input for message processing example 
   messages   received  from  the  autodin-asc      are processed  for  automatic distribution assignment   
   the message   is distributed  to  each   assigned   office   
   the number of  copies of  a message    distributed  to 
 an office      is   a function of  whether   the office   is assigned  for    action   or   information        
   the rules for   editing   messages     are   :   replace  
 all line-feeds  with  spaces     save   only  alphanumeric characters  and  spaces      eliminate   all redundant spaces     
    to edit   the text portion of  the message     is  
 necessary   
  then  the message   is searched  for  all keys   
  when   a key   is located  in  a message     perform   the action   associated  with  that type of  key       
   the action for  type-1 keys    is   :  if   no office   has been assigned  to  the message  for   action      the 
 action  office from  the key    is assigned  to  the message  for   action      if   there is  already  an 
 action  office for  the message      the  action  office from  the key    is treated  as  an  information  
office       label 1ffs1  all  information  offices from 
 the key     are assigned  to  the message   if   ref 1ffs1 
they  	 have 	 not  	 already  	been 	assigned  	for 
   action   or   information        
   the action for  type-1 keys    is   :  if   the key   is   the first type-1 key   found  in  the message     then 
  the key   is used  to   determine   the  action  office      otherwise  the key   is used  to   determine   only 
 information  offices      
figure 1 
　　to illustrate how context is used to complete the partial descriptions in the example  we consider a few cases: 
1. partial sequencing. distribution is never explicitly invoked in the informal specification. however  the first sentence indicates that assignment is performed to enable the distribution. hence  distribution should be explicitly invoked after assignment. 
1. missing operand. sentence two indicates that the message should be distributed to certain offices-those that are  assigned.  but  as can be determined from other usages in the informal specifications  offices can be  assigned  to either messages or keys. this missing operand can be resolved by remembering that assignment 

auto. 	p r o r . - 1 : 	ral1 r 
1 


was performed to enable distribution. hence  distribution must use some result of the assignment process. assignment  from the last two input sentences  assigns offices to the current message. hence  distribution must consume offices assigned to that message. 
   incomplete reference. sentence four says to replace all line feeds with spaces. first  replace requires a third operand  some set in which the replacement will occur. context indicates that this missing operand should be the text of the message parameter of edit. second  the use of a plural in the operand of an action which expects a singular operand  indicates an implicit loop. hence  we have   for all line feeds  replace the line feed by a space in the text of the message.  now  which line feeds are we concerned with  only those in the text of the message because they are the only ones which can be replaced. hence  completing the partial reference  we have  for all line feeds in the text of the message  replace the line feed by a space in the text of the message.  
　　it should be noted that of the approximately 1 decisions which had to be made for this example  all but one were resolved correctly by the prototype system. the message it distributed is the edited one  with all punctuation removed  rather than the original unedited one. the cause of the error is that the system does not understand the difference between an object being changed and its participating in relations with other objects; therefore  it has no concept of the original state of an object and hence does not consider this as a possible completion of any partial reference. 
program created by prototype system 
 whenever 	  r e c e i v e message fror autodin-asc by safe  
d1 odi t 	t e x t of message  
  s e a r c h text of message for  create the set of keys   
  d i s t r i b u t e - p r o c e s s f l 	message   
  d i s i r i b u t e - p r o c e s s # l 	 message  
 for all   o f f i c e s assigned to message for anything  
	  d i s t r t b u t e - p r o c e s s # 1 	message 	o f f i c e       
  d i s t r i b u t e - p r o c e s s # 1 	 message 	o f f i c e   
 1 	  f u n c t i o n a l 	 boolean 	 assigned o f f i c e to message for a c t i o n     
	 boolean 	 assigned o f f i c e to message for 	i n f o r m a t i o n       
tiries   d i s t r i b u t e a copy uh1ch is a copy of messago and located at s a f e from safe to l o c a t i o n of o f f i c e       
  e d i t 	  t e x t   
 for all 	i i n e - f e e d s 	in text 
	  r e p l a c e 	l i n e - f e e d in text by  create set of spaces    
 keep 	 union 	 create the set of alphanumberlc ch ara ct e r s in t e x t   
　 create the set of spaces in t e x t     from t e x t   
 for all spaces 	in text ano redundant 	in text 
 remove space from t e x t     
 whenever 	  l o c a t e a key in text of message at position anything  
do  case 	 type of key  
	  t y p e - 1 	  t y p e - 1 - a c t i o n message key   
	  t y p e - 1 	  t y p e - 1 - a c t i o n message k e y         
  t y p e - 1 - a c t i o n 	 message key  
 if 	 not 	 exists a c t i o n o f f i c e for message   
	then 	  a s s i g n the a c t i o n o f f i c e f l for key 
to message for a c t i o n   
else 	  t r e a t 	a c t i o n o f f i c e # 1 for key 
	as 	i n f o r m a t i o n o f f i c e # 1 for key 
	in 	 if 	 not 	 assigned o f f i c e # 1 to message 
	for a c t i o n or 	i n f o r m a t i o n     
	then 	 assign o f f i c e # 1 to message for i n f o r m a t i o n         
 for all 	  o f f t c e # 1 assigned to key for information  
	 if 	 not 	 assigned o f f i c e # 1 to message 
	for a c t i o n or 	i n f o r m a t i o n   
	then 	  a s s i g n o f f i c e # 1 to message for 	i n f o r m a t i o n         
  t y p e - 1 - a c t i o n 	 message key  
 if key - 	  k e y # i uh1ch is 	 search history for first 
  l o c a t e t y p e - 1 key#l in text of message at p o s i t i o n any    then  determine the a c t i o n o f f i c e for message 
	by 	  t y p e - 1 - a c t i o n message key   
	else 	 determine only the 	i n f o r m a t i o n o f f i c e for message 
	by 	 if 	 exists a c t i o n o f f i c e for message  
	then 	  t r e a t 	a c t i o n o f f i c e # l for key 
	as 	i n f o r m a t i o n o f f i c e # l for key 
	in 	 if 	 not 	 assigned o f f i c e #1 to message 
	for a c t i o n or 	i n f o r m a t i o n     
	then 	 assign o f f i c e ' l to message for 	i n f o r m a t i o n         
	 for all o f f i c e # 1 assigned to key for 	information  
 if  not  assigned o f f i c e # 1 to message for a c t i o n or i n f o r m a t i o n     
then  assign o f f i c e # 1 to message for i n f o r m a t i o n             
figure 1 
　　this capability can clearly be added to the system  but the important point is that interpretation errors will occur  just as they do when human intermediaries are used to produce the formal specification. it is therefore essential to provide extensive feedback and assumption-testing facilities so that such errors  when made  can be discovered and corrected by the user. 
　　the second example is from a system for scheduling a satellite communication channel by multiplexing it among several users  subscribers . it specifies the component of the system which receives a schedule  sol  from the controller of the satellite channel and extracts from it the portions of the 

	auto. 	p r o * . - 1 : 	balzer 
1 
next transmission cycle which have been reserved for a particular subscriber and those portions available to any user  rats . this information is placed in a transmission schedule used by another component to actually utilize the channel during the allowed periods. figure 1 gives the informal natural language description. figure 1 indicates some of the imprecisions contained in this example which must be resolved to obtain the system's formalization of the specification as an operational program  figure 1 . in addition to the process description of figure 1  we have assumed that the formulas referenced and a structural description of the objects of the domain have been separately specified. 
　　the relevant portions of these specifications are that the sol is an ordered set of subscriber and rats entries. each subscriber entry has subscriber identifier and transmission length fields  while a rats entry has only the latter. the transmission schedule is a set of entries  each of which is composed of an absolute transmission time and a transmission length. one of these entries is the primary entry of the transmission schedule. finally  formulas 1 and 1 both take an sol entry as input and produce  respectively  a relative and an absolute transmission time. 
       using the same measures of imprecision as in the first example  we find that this example has about half as many 
imprecisions. 
number of missing operands 	  	1 number of incomplete references 	- 	1 
	number of implicit type conversion 	＊ 	1 
	number of terminology charges 	= 	1 
	number of refinement or elaboration 	- 	1 
	number of implicit sequencing decisions 	= 	1 
　　the example is interesting as a test of the generality of the mechanisms which worked on the first example  and because of the new issues it raises. we will examine each of these to illustrate the range of capabilities added to the prototype to enable it to correctly understand this example and produce the operational program of figure 1. 
 build-transmission-schedule  sol subscriber  
 create transmission-schedule  
 search sol for a subscriber-entry such that sid of subscriber equals sid of subscriber-entry  
 if  locate a subscriber-entry such that sid of subscriber equals sid 
of subscriber-entry in sol  
then 
 make  result-of  formula-1 subscriber-entry   be the relative-transmission-time of subscriber  
 make  result-of  formula-1 subscriber-entry   be the clock-transmission-time of subscriber   
 for all rats which are in sol 
do  make  result-of  formula-1 rats   
be the relative-transmission-time of rats  
 make  result-of  formula-1 rats   
　　　　　be the clock-transmission-time of rats    for all clock-transmission-time of rats 
do  make clock-transmission-time be the transmission-time of  create transmission-entry   
 add transmission-entry to transmission-schedule    
 whenever  make time be the clock-transmission-time 
of subscriber  
do  make time be the transmission-time 
of  create transmission-entry   
 add transmission-entry to transmission-schedule  
 make transmission-entry be the primary-entry 
of transmission-schedule   
figure 1 
auto. prog:.-1: balzer 
1 

　　1. scope of conditional. in natural language communication the end of a conditional is almost never explicit. instead  context must be used to determine whether subsequent statements are part of the conditional. in sentence three of the example  the input to formula 1 is the sol entry found in the previous sentence. thus  sentence three is really part of the conditional statement. 
　　1. implicit formation of relations. in sentence two  the relative transmission time produced by formula 1 is supposed to be associated with the subscriber. since that association is not established elsewhere  it is implicitly being established here. hence this passive construct must be treated as an active one. 
　　1. implicit creation of outputs. in a similar fashion  various sentences establish associations with a transmission schedule  the output of this example  but an instance of one is never explicitly created. such usage indicated that an implicit creation of the output is required. 
　　a. expectation failure. in addition to process and structural statements  a specification normally contains expectations about the state of the computation at some point which provide context for people to explain why something is being done or some properties of its result. they also provide some redundancy against which an understanding of the specification can be checked. in the example  one of these expectations  that all of the components of the entries of the output have been produced  fails  which indicates either a misunderstanding of the specification or an inconsistency or incompleteness. in this case  both our example and the actual specification from which it was drawn are incomplete; they fail to describe how the length field of the entries of the transmission schedule are calculated from the inputs. 
1. description oe the prototype system 
　　the prototype system is structurally quite simple. it has three phases  linguistic  planning  and meta-evaluation  which are sequentially invoked to process the informal specification. each phase uses the results of the previous phases  but no capability currently exists to reinvoke an earlier phase if a difficulty is encountered. hence  either ambiguity must be resolved within a phase or the possibilities passed forward to the next phase for it to resolve. 
　　we will describe the prototype system by working backward from the goal through the phases  in reverse order  toward the input to expose the system design and provide context for understanding the operation of each phase. 
　　the goal of the system is to create a formal operational specification from the informal input  which means that it must complete each of the partial descriptions in the input to produce the output. in general  each partial description has several different possible completions  and a separate decision must be made for each partial description to select the proper 
completion for it. 
　　based on the partial description and the context in which it occurs  an a priori ordered set of possible completions is created for each partial description. but one decision cannot be made in isolation from the others; decisions must be 
consistent with one another and the resulting output specification must make sense as a whole. since the output is a program in the formal specification language  it must meet all the criteria for program well-formedness. fortunately  programs are highly constrained objects  one reason they are 
	a u t o . 	p r o * . -
1 
so hard to write   so there are many well-formedness criteria which must be satisfied. 
　　this provides a classical backtracking situation   since there are many interrelated individual decisions that in combination can be either accepted or rejected by some criteria  the well-formedness rules . in such situations  the decisions are made one at a time in some order. after each decision the object  program  formed by the current set of decisions is tested to see if it meets the criteria  well-formedness rules . if it does  then the next decision is made  and so on  until all the decisions have been made and the result accepted. if at any stage the partially formed result is rejected  then the next possibility at the most recent decision point is chosen instead and a new result formed and tested as before. if all possibilities have been tried and rejected for the most recent decision point  then the state of the decision-making process is backed up to that existing at the previous decision point and a new possibility chosen. this process will terminate either by finding an acceptable solution  formal specification  or by determining that none can be found. the resulting object  program  is an acceptable solution  formal specification  for the problem  informal specification . 
　　the order in which decisions  rather than the order of alternatives within a decision  are made should be chosen to maximize early rejection of infeasible combinations of decisions. this requires that the rejection criteria can be applied to partially determined objects. the preferred decision order is clearly dependent on the nature of the acceptance/rejection criteria. 
　　we now let the nature of the well-formedness criteria determine the structure of the prototype system so that the early rejection possibilities inherent in the criteria can be utilized. the criteria fall into three categories: dynamic state-of-computation criteria  global reference criteria  and static flow criteria. each of these categories must be handled differently. 
　　the dynamic state-of-computation criteria are based only on the current  state  of the program and its data base  e.g.   the constraints of a domain must not be violated  and  it must be possible to execute both branches of a condition  . they require that all decisions that affect the computation to that point  but not beyond  must be made before the criteria can be tested. thus  if decisions could be made as they are needed by the computation of the program and the program  state  examined at each stage of the computation  then the dynamic state-of-computation criteria could be used to obtain early rejection of infeasible decisions. 
　　this is exactly the strategy adopted in the design of the prototype system. however  since no actual input data is available for the program to be tested  and since the program must be well-formed for a variety inputs  symbolic inputs rather than actual inputs are used. instead of actual execution  the program is symbolically executed on the inputs  
which provides a much stronger test of well-formedness than would execution on any particular set of inputs. 
　　however  completely representing the state of the computation as a program is symbolically executed is very difficult  e.g.  determining the state after execution of a loop or a conditional statement  and more detailed than necessary for the well-formedness rules. therefore  the prototype system uses a weaker form of interpretation  called meta-evaluation  which only partially determines the program's state as computation proceeds  e.g.  loops are executed only once for 
1 : b a q l z e r 
some  generic  element  and the effects of then and else clauses are marked as possible  but are not conditioned by the predicate of the if . this meta-evaluation process is much easier to implement and still provides a wealth of run-time context used by the acceptance/rejection criteria to determine program well-formedness. 
　　the global referencing criteria  such as  parameters must be used in the body of a procedure   test the overall use of names within the program and thus cannot be tested until all decisions have been made. they are tested only after the meta-evaluation is complete. 
　　the final category of criteria  static flow  e.g.   items must be produced before being consumed  and  outputs must be produced somewhere    are more complex. the 
meta-evaluation process requires a program on which to operate  which may contain partial descriptions that the meta-evaluation process will attempt to complete by backtracking. this program  outline  is created from the informal input for the meta-evaluation process by the flow analysis  or planning  phase  which examines the individual process descriptions and the elaborations  refinements  and modifications of them in the input  then determines which pieces belong together and how the refinements  elaborations  and modifications interact. it performs a producer/consumer analysis of these operations to determine their relative sequencing and where in the sequence any unused and unsequenced operations should occur. this analysis enables the planning phase to determine the overall operation sequencing for the program outline from the partial sequencing information contained in the input. it uses the data flow well-formedness criteria and the heuristic that each described operation must be invoked somewhere in the resulting program  otherwise  why did the user bother to describe it   to complete the partial sequence descriptions. 
　　if the criteria are not sufficiently strong to produce a unique program outline  the ambiguity must be resolved either by interacting with the user or by including the alternatives in the program outline for the meta-evaluation phase to resolve as part of its decisionmaking process. in the prototype system  the meta-evaluation phase is prepared to deal with only minor sequencing alternatives such as the scope of conditional statements  if a statement following a conditional assumes a particular value of the predicate  it must be made part of one of the branches of the conditional.  and demons  are all situations which match the firing pattern of a demon intended to invoke it or only those which arise in some particular context  and if so what context  . major sequencing issues-such as whether one statement is a refinement of another or not-that cannot be resolved by the planning phase must be resolved by the user before the meta-evaluation phase. 
　　both the planning and meta-evaluation phases use a structural description of the application domain to provide context for their program execution  and inference rules which define relation inter-dependencies in the process domain. this structural base is the application-specific foundation upon which the planning and meta-evaluation phases rest  and must be provided before they are invoked. it contains all the application-specific contextual knowledge. it augments the system's built-in knowledge of data flow and program well-formedness and enables the system to be specialized to a particular application and to use this expertise in conjunction with its built-in program formation knowledge to formalize the input specification. 
　　the construction of a suitable application-specific structural base is itself an arduous  error-prone task. furthermore  our study of actual program specifications indicated that most of the structural information was already informally contained in the program specification. we therefore decided to allow partial descriptions in the specification of the structural base and to permit such descriptions to be intermixed with the program specification. 
　　since we are concerned only with the semantic issues raised by using partial descriptions in the program specification  the system uses a parenthesized version of the natural language specification as its actual input to avoid any syntactic parsing issues. this parenthesized input does not affect the semantic issues we have discussed. 
　　the first tasks  then  of the system are to separate the process descriptions from the structural descriptions  to convert both to internal form  and to complete any partial structural descriptions. these tasks comprise the system's linguistic phase  which precedes the other two. 
　　if a formal structural base already exists for some application  then  of course  it is loaded first and is augmented by and checked for consistency with any structural statements contained within the program specification. 
　　thus  in chronological order  rather than the reverse dependence order used above   the system's basic mode of operation consists of reading an input specification  separating it into structural and processing descriptions; completing the structural descriptions and integrating the result into any existing structural base; determining the gross program structure by producer/consumer analysis during the planning phase; and  finally determining the final program structure through meta-evaluation. 
