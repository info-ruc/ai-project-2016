 
in this paper  we address an issue that arises when the background knowledge used by explanationbased learning is incorrect. in particular  we consider the problems that can be caused by a domain theory that may be overly specific. under this condition  generalizations formed by explanation-based learning will make errors of omission when they are relied upon to make predictions or explanations. we describe a 
technique for detecting errors of omission  assigning blame for the error of omission to an inference rule in the domain theory  and revising the domain theory to accommodate new examples. 
1 introduction 
an important issue arises when the background knowledge for explanation-based learning is acquired via empirical methods: inaccuracies in the background knowledge create inaccuracies in the  compiled  theory. in this paper  we describe a technique for detecting errors in the compiled theory and revising the background knowledge. we consider the case in which the domain theory may be overly specific. such domain theories are formed by one-sided inductive learning algorithms . in a system such as occam pi  which integrates empirical and explanation-based learning  two kinds of examples can be distinguished: 
foundational. examples from which background domain knowledge can be learned. for example  occam is presented with examples of parents helping their children and examples of strangers not assisting children. from these foundational examples  it acquires a schema that indicates that parents have a goal of preserving the health of their children. this schema explains why a parent pays a ransom to a kidnaper. 
this research is supported in part by a faculty 
research grant from the university of california  irvine. 
performance. these training examples are instances of the performance task. in occam  one performance task is to infer the goals of the participants in kidnaping episodes. for this performance task  examples of actual kidnaping incidents are used as performance examples. because occam has learned the relevant background knowledge  it can use explanationbased learning on these performance examples. 
   note that the classification of an example as a performance or foundational example is relative to a particular performance task. for example  if the performance task were to predict who might be a potential candidate for purchasing ransom insurance  kidnaping incidents would serve as foundational examples. the foundational examples differ from the performance examples in that they can be viewed as a subproblcm of the performance task. 
   in previous work   we have described a technique for detecting errors in the background theory when foundational examples are misclassified and we have shown how these changes can be propagated to the compiled theory. in this paper  we demonstrate a 
   procedure which can detect errors in the compiled theory when performance examples are misclassified  and assign blame for the error to an inference rule in the background theory. the technique relies upon recording the dependency between a feature present in a generalization formed by explanation-based learning and the inference rule in the domain theory that is responsible for including the feature in the generalization. 
   an error of omission in the compiled theory occurs when an performance example is not correctly classified or explained. for example  a kidnaping schema might indicate that a ransom will be demanded of a wealthy parent of the hostage. this schema will not be able to explain a case in which the ransom note is sent to a wealthy grandparent. from an example such as this  it is desirable that a learning system be able to notice that its rule which indicates that parents have a goal of preserving the health of their children is 
	pazzani 	1 
overly specific and to generalize this rule to include grandparents. such revisions of the domain theory distinguishes this work from previous work  which corrects only the compiled theory. we investigate errors of omission rather than errors of commission for two reasons: 
errors of omission are the only kind of error which occurs in one-sided algorithms that incrementally generalize their hypothesis as little as possible to account for new positive examples. this class of learning algorithms has been the subject of much recent theoretical analysis  e.g.  l    and has been proposed as a model which can account for some varieties of human learning  . 
blame assignment for errors of omission in compiled theories can be done incrementally as new examples are encountered. two recent techniques which assign blame to inference rules in the background theory   1j non-incrementally process a collection of examples when errors of commission are detected. when there is an error of commission  at least one rule must be modified by including an additional condition to prevent. however  any rule which played a part in the explanation is a potential candidate for modification. correlational techniques are used to assign blame to a particular rule and to construct a description of the condition which must be added. in contrast  with errors of omission  a condition must be dropped from at least one rule that was used to chain together an explanation. this condition can be found by recording a dependency between rules in the domain theory and features in the compiled theory. 
1 errors of omission in occam 
occam maintains a hierarchy of schemata that can be used to make predictions or give explanations. a schema which applies to a new experience is found by a memory search that starts at a very general node in memory  e.g.  coercion . by following indices  e.g.  threat = refuse to sell  from the general schemata  more specific schemata that provide additional inferences and expectations  e.g. economic-sanctions  are retrieved. in this extension to occam  there are two types of indices: 
predictive. a predictive index is traversed to find a schema that describes the result of a given event. explanatory. an explanatory indexed is traversed to find a schema that describes the cause  i.e.. the explanation  of a given event. 
   these indices are similar in spirit to the predictive and predictable features used in unimem . however  there is one important difference. in unimem  this information is determined empirically: the predictive features are those characteristic features of a generalization that are unique  or nearly unique  to that generalization. the predictable features appear in many generalizations. the idea is that the predictive features are likely to be the cause of the predictable features . in occam  this information is derived analytically. the predictive features are those that appear in the antecedent of an inference rule which is used to create a generalization with explanation-based learning. the explanatory features appear in the consequent of an inference rule. figure 1 illustrates one inference rule which is used in part of an explanation to explain why the us grain embargo with the soviet union failed. 

figure 1. an inference rule that indicates that an increased demand for a commonly available commodity by a country with a strong economy enables a country which exports the commodity to sell the commodity at a greater than market rate. 
   when a generalization of the us grain embargo is constructed by explanation-based learning  it is indexed by predictive features  e.g.  economich e a l t h of t h e t a r g e t c o u n t r y = s t r o n g   a v a i l a b i l i t y o f t h e commodity 
= common . the schemata formed from generalizing the us grain embargo can be retrieved via predictive indices to make predictioas about future or hypothetical cases. for example  occam answers the question  what would happen if the us refused to sell computers to south korea  if south korea continues to export automobiles to canada    by retrieving the generalization via predictive indices. the generalization will also be indexed by explanatory features  e.g.  p r i c e of t h e c o m m o d i t y = -
1 	machine learning  market . explanatory indices could be followed to find an answer to a question such as  what could cause the price of oil to rise  . similar distinctions have been made by pearl  who distinguishes causal and evidential support and by tversky and kahneman  who find that people treat causal and diagnostic evidence differently. 
1 detecting errors of omission 
in occam  when a new event is encountered  predictive indices are followed to determine if there is a schema which can predict the observed result of the new event. if the observed result is predicted  then there is nothing that can be learned from the new event. if there is no schema which predicts the result  occam attempts to create one via explanation-based learning.1 if this fails  because the domain theory cannot explain the result of the new event  then occam checks for an error of omission. schemata are retrieved by following explanatory indices. if a single schema is retrieved which could explain the event  then this schema may be making an error of omission. that is  the conditions which predict the outcome of the schema may be overly-specific. overly-specific predictive indices would prevent the schema from being retrieved by predictive indices  but not by explanatory indices. the general idea is that the current domain theory does not predict the outcome of the new event. however  the current domain theory predicts some conditions  which differ from the current event  under which that outcome could occur. it may be the case that a slight modification of these conditions could account for the new event. the search for a modification focuses on the differences between the new event and the schema retrieved by explanatory indices. 
1 blame assignment 
after an error of omission has been detected and a schema which could be modified is retrieved  an attempt is made to account for the differences between the new event and the existing schema. the following process isolates one rule in the domain theory which can be modified to account for the new event: 
1. rederive the justification for the current schema by reexplaining how the generalized event of the schema results in the outcome of the schema. this explanation process differs from the standard explanation-based learning algorithms   in one crucial aspect: a dependency is recorded between each feature which is included in the generalized event and the inference rule s  in the domain which are responsible for including the feature in the generalized event. there is a trade-off between storing this dependency with every schema that is created and rederiving this dependency when 
　　　1. explanation-based learning in occam is similar to other ebl algorithms  e.g.    . rules in occam are stored as schemata in memory. predictive indices can be followed to retrieve a rule whose antecedent matches an explanatory goal. a complex event is explained by chaining together several schemata. explanation-based learning creates a new schema so that in the future  an explanation can be found by memory search rather than chaining. 
a possible error of omissions has been detected. we rederive this information  because the psychological evidence does not indicate that people are aware of the assumptions behind their beliefs . 
1. the features which differ between the retrieved schema and the new event are found. 
1. the inference rules which arc responsible for the features which differ are collected. 
1. if exactly one inference rule is responsible for all the differences  e.g.  if there was only one difference   then it assumed that this inference rule is making an error of omission. recall that occam learns by observation. if it had a teacher that could indicate the correct explanation  then it would be possible to correct all the inference rules responsible for every difference. without a teacher  it is necessary to find an explanation that is  close  to explaining the new event. here   close  meaas that there is only one schema that is retrieved via explanatory indices and there is only one inference rule responsible for all the differences between the retrieved schema and the new event. in effect  when an error of omission in a compiled theory occurs  this step determines that an inference rule in background theory is committing an error of omission. 
1. a new training example for the inference rule in the background theory is derived from the new event. the general idea here is that the new event is a performance example. a foundational example is constructed from the performance example so that the background theory can be revised. this is accomplished by exploiting variable constraints between the inference rule used in the explanation and the generalization produced by explanation-based learning. 
1 1 blame assignment: an example 
in this section  we provide an example of the way that blame assignment works. the antecedent of a rule which indicates that refusing to sell a commodity results in an increased demand for the commodity was modified so that the countries which sell and buy the product must have free economies  see figure 1 . 
　　occam is presented with the following example and a new schema is created with explanation-based learning: 
australia and france  1 
in 1  australia refused to sell uranium to france  unless france ceased nuclear testing in the south pacific. france paid a higher price to buy uranium from south africa and continued nuclear testing. 
	pazzani 	1 


figure 1. an overly specific inference rule that indicates that a country with a free economy refusing to sell an exported commodity to a country with a free economy that imports the commodity will result in an increased demand for the commodity. 
   the outcome of this event  france possessing the uranium  is explained by chaining together three inference rules. refusing to sell the uranium results in an increased demand for the uranium  sec figure 1   an increased demand for the uranium enables a 
   country to sell the uranium at a higher price  see figure 1   and selling the uranium results in france possessing the uranium. the generalization indicates that three features of the target  i.e.  france  are essential: 
	the target imports the commodity. 	 the rule in 
figure 1 is responsible for this constraint.  
the target has a strong economy.  figure 1  the target has a free economy.  figure 1  
   by keeping track of the source of each feature included in a generalization  occam can assign blame when an error of omission occurs. consider what happens when occam is presented with the example of the us grain embargo with the soviet union: 
us and ussr  1 
in 1  the us refused to sell grain to the soviet union if the soviet union did not withdraw its troops from afghanistan. the soviet union paid a higher price to buy grain from argentina and did not withdraw from afghanistan. 
   there is no schema in memory which could predict the observed outcome of this incident and an explanation cannot be created for the outcome by chaining together political or economic rules from the domain theory  since the soviet union does not have a free economy . however  the schema created from the sanction incident involving australia and france can be retrieved by explanatory indices. the following trace shows occam detecting and assigning blame for this error of omission: 
1 	machine learning 

   in this example  occam retrieves a schema by following explanatory indices  determines that the only difference is the type of economy of the target and finds the rule which indicates that the target must have a free economy. next  it creates a foundational example from the performance example so that the rule may be revised. in effect  what has occurred is that a performance example of the soviet union thwarting a sanction attempt by bidding the price of the grain up is converted to a foundational example of the us refusing to sell grain to the soviets resulting in an increased demand for the grain by the soviets. an inference rule in the domain theory can be revised to account for the derived foundational example. 

1 correcting the domain theory. 
once a foundational training example has been constructed  the inference rule in the domain theory which is responsible for error of omission can be revised. the central problem addressed in this paper is the assignment of blame for an error of omission. a number of techniques could be used to revise the domain theory. if the learning system is acquiring deterministic causal rules and there is no noise in the data  then an inference rule can be revised as soon as one exception is seen. although  we acknowledge that this situation rarely occurs in human learning or in real-world domains  we make this assumptions in the experimental study described in section 1. this is  in part  necessitated by the small number of economic sanctions incidents that have occurred. an error of omission is caused by an inference rule being too specific. this is corrected by generalizing the antecedent inference rule as little as possible to accommodate the training example. those features that are present in the inference rule but are not present in the new example are dropped. the following trace continues the example from section 1.1: 

   the feature that was removed by the new example indicated that the target of the coercion must have a free market economy. once the domain theory has been revised  the schema formed via ebl with the old theory is deleted  and a new  more accurate schema is created. in the current example  this schema still contains an erroneous feature that indicates that the actor of the coercion  i.e.  the country that refuses to sell the product  have a free market economy  since this was part of the initial domain theory and no counter example has been seen. 
1 experimental results on occam 
to understand how much of an improvement can be gained by the algorithms described in this paper  we intentionally introduced errors into occam's background knowledge. every inference rule which indicates the result of threat  e.g.  reftising to sell a product  or cutting of economic aid  was modified to also include the features which are present in the other inference rules in this class. in effect  this simulates an approach in which the potentially relevant features are known a priori  but the contexts in which the features are relevant is not known  cf.  . figure 1 illustrates the accuracy of occam  occam with the erroneous knowledge base  and occam using the algorithms for detecting and correcting errors omission on the erroneous knowledge base. we measured the accuracy of predicting the outcome on unseen cases after 1 and 1 training examples. the values are averaged over 1 runs. in each run  a collection of cases was selected randomly from a database of 1 cases. the remaining cases and five hypothetical cases analyzed by a political analyst at the rand corporation were used as test examples. while this may seem like a small number of training examples  there have not been many major economic sanctions incidents. it is exactly in this type of situation that knowledge-intensive strategies are best suited. 
   the figure clearly shows that the algorithm described in this paper increases the accuracy of predicting the outcome of new cases. with 1 training examples  correction improves the performance significantly  p .1  r 1 =1 . the difference after 1 examples is significant at the .1 level. the correction strategy does not achieve the same performance as occam with a correct domain theory because in some cases  there are no precedents which contain the necessary features to force the revisions of the incorrect domain theory. 
1 conclusion 
we have described a technique for detecting errors of omission in compiled theories  assigning blame for the error of omission on a rule  or rules  in the domain theory and revising the domain theory to account for the new example. although we have explored this technique in the context of explanation-based learning  it appears to be applicable to case-based reasoning systems which make use of domain knowledge  as well as rule-based expert systems . one advantage of this technique that we have not yet demonstrated is that a more accurate domain theory can increase the accuracy of explanation-based learning of concepts other than the one in which the error was detected. this will occur when two compiled concepts share some foundational background knowledge. 
acknowledgements 
i'd like to thank wendy sarrett and kamal ali for comments on an earlier draft of this paper. 
	pazzani 	1 
 
 
 
 
 
 
 
