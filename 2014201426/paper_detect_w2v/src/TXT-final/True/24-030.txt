 
this paper describes c a b o t   a case-based system that is able to adjust its retrieval and adaptation metrics  in addition to storing cases. it has been applied to the game of othello. experiments show that c a b o t saves about half as many cases as similar systems that do not adjust their retrieval and adaptation mechanisms. it also consistently beats these systems. these results suggest that existing case-based systems could save fewer cases without reducing their current levels of performance. they also demonstrate that it is beneficial to distinguish failures due to missing information  faulty retrieval  and faulty adaptation. 
1 	introduction 
many case-based reasoning  cbr  systems are designed to solve new problems by adapting solutions to similar  previously solved  problems. the proposed solution to a new problem can be inappropriate for a variety of reasons: l  the system lacks a similar past case  1  the  wrong  case was retrieved  or 1  the retrieved case was not adapted properly to the new problem. most casebased and instance-based systems are designed to handle 
just one or two of these errors. however  it is rare for a c b r system to handle all three. 
   it is important to distinguish between errors with differing causes because they require different solutions. it may be possible to compensate for an error in the retrieval mechanism by adding more information or altering the adaptation mechanism  but the resulting system might do more work or save more information than would otherwise be necessary. for example  the c h e f system  hammond  1  may expend effort adapting a 
   retrieved case when a different case could be adapted with less effort. compensating for an error  rather than dealing with its causes  may also hamper future performance. 
* supported by a grant from digital equipment corp. 
+ supported by a grant from gte laboratories inc. 
   ++ supported by grants from the air force office of sponsored research under contract afosr-1; the national science foundation  contract iri-1; and gte laboratories incorporated. 
   the work presented in this paper is based upon two hypotheses. the first is that it is useful for a case- or instance-based system to distinguish among errors due to a lack of information  a faulty retrieval mechanism  and a faulty adaptation mechanism. the second hypothesis is that a problem-solving system can profitably use feedback from its environment to distinguish among the three types of errors. once this distinction can be made  it is assumed that error correction can be done using a standard machine learning technique for supervised learning. 
   these hypotheses have been tested in c a b o t   a hybrid case-based reasoning/machine learning system that both reasons from cases and uses an inductive learning algorithm to adjust its retrieval and adaptation mechanisms. c a b o t has been tested against a non-learning system  two case-based systems with fixed retrieval and adaptation mechanisms  and two inductive learning systems. results from these tests suggest that existing casebased and instance-based systems could save fewer cases without reducing their current levels of performance. 
1 	case-based reasoning 
although case-based reasoning systems vary significantly  those that are used for problem-solving share a common approach. when a new problem is encountered  the system first retrieves one or more cases that are similar to the new problem. typically no case matches the new problem exactly  so the system must adapt one of the retrieved solutions to the new problem. finally  the system may add the new problem and its solution to the case base  and it may adjust its indices. 
   instance-based learning algorithms also share this approach to problem-solving  although they are sometimes considered distinct from case-based systems. when a 
   new problem is encountered  one or more similar instances are retrieved. if their classifications differ  the differences must be resolved to provide a classification for the new instance. finally  the system may add a new instance to its instance base. the primary differences between case-based systems and instance-based learning algorithms are the tasks to which they are applied  and the complexity of the information that they store; cases are typically more complex than instances. 
　the simplest approach to error-correction is to save all information  whether or not an error was made. the 
	gallon  fawcett  and rissland 	1 

1. retrieve the stored problem state most like the current problem state  according to the retrieval metric. 
1. select from the available successor states the one most like the retrieved successor  according to the selection metric. 
1. implement the selected action. 
1. receive feedback from the oracle about which successor was best. 
1. if c a b o t did not select the best successor  perform error-correction: 
 a  try to adjust the retrieval metric. if successful  go to 1. 
 b  try to adjust the selection metric. if successful  go to 1. 
 c  add the current problem state and the best successor to the case base. go to 1. 
figure 1: c a b o t ' s problem-solving cycle. 

g i n a program  de jong & schultz  1  for playing othello adopts this approach  saving every board that it sees. one might expect g i n a eventually to become swamped with cases. instead  the number of unique boards encountered by g i n a in its play with opponents eventually stabilized at a small fraction of the number of possible boards. 
   aha & kibler  1  have experimented with a family of instance filtering algorithms. the proximity algorithm saves all instances  while the ntgrowth algorithm discards instances that would be classified correctly or that appear to be noisy  i.e.  their classifications conflict with the rest of the data . in their tests on noisy data  the two algorithms were roughly equal; thus  equal performance was gained from fewer instances. 
   both cyrus  kolodner  1  and p r o t o s  bareiss & porter  1  dynamicauy adjust the structure of their case bases  and hence their retrieval mechanisms. cyrus tries to maintain an appropriate organization of cases based upon an internal metric  expressed in terms of the number of adherents and exceptions to a given memory organization packet  mop . in contrast  protos is given feedback by a benevolent teacher. the teacher provides the correct answer and may explain the relevance of individual features. the teacher also approves or rejects changes that protos proposes  which prevents p r o t o s from making serious mistakes. this feedback also enables protos to prune its case-base by merging cases. 
   e a c h  salzberg  1  also stores exemplars  generalized  representative instances  to perform classification. when e a c h encounters a new instance  the distance to each stored exemplar is measured. the closest exemplar determines the classification that e a c h predicts. e a c h generalizes and specializes its exemplars in response to classification successes and failures. in addition  e a c h uses a weighted distance function  similar to c a b o t ' s   which is adjusted after every classification error. thus e a c h tunes its retrieval mechanism in response to limited feedback from its environment. 
   c h e f  hammond  1  adjusts its adaptation mechanism and also its retrieval mechanism by changing the way cases are indexed. c h e f receives very detailed feedback from a simulator  which it can analyze to identify the reason that an adapted case fails to meet its goals. after it repairs the adapted case  c h e f constructs demons that prevent it from making similar adaptation mistakes in the future. c h e f also indexes the repaired case according to the failures that the case avoids  as well as the goals the case satisfies. 
   adding a case  adjusting the retrieval mechanism  and adjusting the adaptation mechanism are all ways of coping with failure. few of the systems above make clear distinctions among causes of failure  and therefore it is rarely clear which mechanism should be adjusted. 
   most systems that can adjust their retrieval or adaptation strategies depend upon detailed feedback from the problem-solving environment. requiring detailed feedback limits the environments in which those systems can operate. in contrast  c y r u s optimizes an internal metric that does not use feedback from the environment. however  the inability to use feedback prevents a system from adjusting to the environment in which it operates. therefore  an important research question is how a system can adjust both its retrieval and adaptation mechanisms using only limited feedback from the environment. the following sections present a system that addresses this question. 
1 	cabot 
the c a b o t program was developed to investigate the problem of using limited feedback from a problemsolving environment to distinguish different types of errors in case-based reasoning. c a b o t is designed for state-space search  where states are represented as feature vectors and problem-solving consists of repeatedly selecting and then executing one of a set of actions. figure 1 outlines c a b o t ' s problem-solving cycle. cases consist of pairs of states   where  the child  is the desired successor to parent . 
   state-space search is performed by starting at an initial state and repeatedly selecting successor states until a goal state is reached. the object of c a b o t ' s reasoning is solely to determine  at each state in the search  the best successor state. c a b o t does this by performing a restricted form of adaptation called selection: when a case is retrieved  it is used to select one from a set of successor states. by improving the quality of decisions at each step in the search  c a b o t improves its problem-solving performance. 

   the feedback available to c a b o t is qualitative. a domain-dependent oracle identifies the best successor state  but it does not explain its reasoning  nor does it score or order the rest of the available successors. c a b o t ' s task is to use this limited feedback to improve its problem-solving performance. when c a b o t ' s selection differs from that of the oracle  its decision is considered a failure that it must correct. c a b o t first tries to adjust its retrieval metric. if the retrieval metric cannot or should not be changed  c a b o t tries to adjust its selection metric. changes to the retrieval and selection metrics are both subject to an assured consistency condition that only allows changes if the two metrics remain consistent with each other. c a b o t checks for assured consistency by testing any changes on a random sample of problems that it has previously encountered; if a change to one of the metrics decreases accuracy  the change is discarded. if neither metric can be changed  c a b o t adds a new case. 
the next three sections discuss each step in detail. 
1 	r e t r i e v a l 
retrieval consists of identifying the case  
whose  is closest to the current problem state tance is determined by a weighted distance function dr 
 the retrieval metric  whose weight vector can be adjusted during error correction. no similarity threshold is used; retrieval always returns a case. 
1 	s e l e c t i o n 
it is rare for the retrieved successor state to exactly match any of the available successors of the current problem state. however  the exact pair of states is less important than their relationship; the pair of states encodes a transformation of the problem that presumably reduces the distance to a goal. selection consists of identifying the successor state whose transformation of the current problem is most similar to the transformation represented by the retrieved case. 
   the transformation of the problem from one state to another is represented by the difference between their 
feature vectors. the vector  represents the change in feature values that should ideally occur between the current problem state and the selected successor state. the change that actually occurs between the current problem state and each successor state  is 
represented as  these relationships are illustrated in figure 1. 
selection consists of identifying the successor state 
whose  distance is determined by a weighted  signed distance function ds  the selection metric   whose weight vector ws can be adjusted during error correction  ds is defined as follows: 

ties are broken arbitrarily. no similarity threshold is used; selection always selects a successor state. 

figure 1: the relationships among cases in retrieval and selection. is the successor chosen by c a b o t . is the successor chosen by the oracle. 
1 	e r r o r c o r r e c t i o n 
after c a b o t selects a successor state and carries out the associated action  it asks the oracle to identify the desired action. if the oracle agrees with c a b o t ' s choice  no error correction is performed. otherwise  c a b o t considers its choice to be wrong and tries to correct itself. error correction consists of the following: 
1. c a b o t checks to see if it retrieved the wrong case. it does so by conducting retrieval again with the added constraint that the retrieved case must cause it to select the desired successor state. the resulting 
case .  is called a near miss. if no such case is found  c a b o t proceeds to step 1. 
if a near miss is found  c a b o t tries to adjust the weighted euclidean distance function dr so that the near miss state is closer than the retrieved state to the current problem state. the desired distance relationship is: 

this relationship may be expressed in terms of the 

rection rule  nilsson  1  to equation 1. finally  c a b o t verifies that the retrieval and selection metrics remain consistent  as discussed earlier. if they do  error correction terminates. otherwise  the adjustments to the retrieval weight vector are discarded  and c a b o t proceeds to step 1. 
1. c a b o t assumes that it retrieved the right case  but that the selection metric is wrong. it tries to adjust the selection weight vector  in a manner similar to that used for the retrieval weight vector. adjustment is based upon the desired relationship: 

	callan  fawcett  and rissland 	1 
c a b o t verifies that the retrieval and selection metrics remain consistent  as discussed earlier. if they do  error correction terminates. otherwise  the adjustments to the selection weight vector are discarded  and c a b o t proceeds to step 1. 
1. c a b o t assumes that its error was due to missing information  and adds a new case. the new case consists of the current problem state and the desired successor state. 
   weights used in the retrieval and selection metrics  are initially set to 1 and are adjusted by the error correction process. c a b o t attempts to adjust its retrieval metric  step 1  before attempting to adjust its selection metric  step 1  because retrieval determines the cases that are used in selection. this ordering is designed to make the retrieval metric converge before the selection metric. 
1 	an experimental domain: othello 
c a b o t has been been tested on othello  a two-player board game. players alternate moves  and there are usually 1 moves in a game. although the rules of othello are simple  the search space contains approximately 1 nodes  and the strategies can be quite complex. othello was selected as a domain for c a b o t because of its large search space  because it has been used as a domain by researchers in artificial intelligence  rosenbloom  1; lee & mahajan  1; de jong & schultz  1   and because of the availability of immediate feedback after every move. problem-solving in othello consists of deciding which move to make next; on each cycle  c a b o t selects a move  makes it  receives feedback  and possibly performs error correction. 
   the othello oracle makes decisions by conducting a minimax search  using alpha-beta pruning  to a depth of 1-ply and then evaluating search states with a polynomial evaluation function. the oracle shifts to exhaustive search for the last 1 moves of the game. 
   raw board positions are often considered too specific a representation for othello game states  so it is common to represent them with vectors of more abstract features. a feature is a numeric function that measures some important characteristic of a board position. c a b o t and its opponents use a set of features common to o t h e l l o playing programs: mobility  potential mobility  corner squares  x squares  etc.  rosenbloom  1  hereafter   board  will be used to denote this feature vector  rather than the actual raw board configuration. the othello oracle uses a larger and more comprehensive set of features than is used by c a b o t . 
1 	l a t c b r : a p u r e c a s e - b a s e d o p p o n e n t 
one of c a b o t ' s opponents is a case-based reasoning system that uses a very simple version of a claim-lattice  rissland & ashley  1  to determine the best move. this opponent  called l a t c b r   orders the cases into a lattice in which the cases closest to the root are those with maximal subsets of features exactly matching the current problem state. l a t c b r contains no built-in indices to prune this set  nor does it have any way of judging which of the nodes at a given level are best for the player. it therefore treats all nodes occurring closest to the root as equal  and discards the rest. the cases contained in these nodes are all considered retrieved. 
   the claim-lattice was designed for comparing and contrasting competing alternatives. it does not offer a straightforward mechanism for selecting a single alternative  nor is there any a priori reason for selecting one alternative as best. the approach adopted for l a t c b r is to adapt each case to the current situation by identifying the move that it matches most closely  and then make the choice that is recommended by the majority of the cases  with ties broken randomly. the degree of match between a case and a given move is determined by the number of features on which they exactly match. 
   l a t c b r does not attempt to correct either its retrieval function or its adaptation function when it makes a mistake. when the oracle disagrees with its chosen move  l a t c b r adds a new case. 
1 	p i l : a p u r e i n d u c t i v e l e a r n i n g o p p o n e n t 
the pil opponent uses an inductive learning algorithm to improve a heuristic evaluation function h over search states. pil selects the successor to the current problem state for which h is greatest; ties are broken arbitrarily. 
   after each selection  pil asks the oracle to identify the desired successor. if the desired successor differs from pil's selection  pil considers its choice to be wrong and tries to correct h. it does so by adjusting h so that the desired successor state is rated more highly than all other successor states. 
   pil's evaluation function h is a weighted sum of feature values. weights in the weight vector  are initially set to 1. qualitative feedback can be used correct the weights  as shown below: 
		 1  
pil adjusts the weight vector by applying the absolute correction rule  nilsson  1  to equation 1. 
1 	experiments and results 
four experiments were run to determine the relative effectiveness of the pure case-based   l a t c b r     adaptive case-based   c a b o t     and pure inductive learning  pil  move selection strategies. the experiments were designed to examine three characteristics of each strategy: 
  effectiveness: how effective was it for playing othello  effectiveness was measured by the win/loss ratio  and by the average discs taken per game. 
  d e c i s i o n a c c u r a c y : how often did it agree with the oracle's choice  
  g r o w t h of case base: for the case-based systems  how quickly did the case base grow  


　　　　　　　　　　number of games figure 1: accuracy of decision-making. 

number of games 
figure 1: growth of case base during game playing. 

   each tournament began with empty case bases and unweighted linear threshold units  i.e.  all weights set to 1 . at the beginning of each game  one player was randomly selected to make the first move. each player was permitted to learn from both its own moves and its opponent's moves throughout the tournament. by training on both sets of moves  a learning program receives balanced training because it can train on better game states than it might achieve on its own. 
   games were played until either 1 games had been played  or until the oracle's best game had been encountered. the latter stopping condition is important for case-based competitors. if a case- or instance-based game-playing program is permitted to observe an opponent that follows a fixed strategy  it will eventually memorize that opponent's best game. once the opponent's best game is memorized  the case-based program cannot lose more than 1% of the subsequent games. when both opponents are case-based and both are learning from a fixed oracle  they will eventually begin playing a single game repeatedly. all of the tournaments reported below were ended if this condition occurred. the results from the four tournaments are summarized in tables 1 and 1. 
   table 1 shows the number of games won  lost and tied by each playet during the four tournaments. it demonstrates that c a b o t ' s performance is superior to that of the pure case-based system   l a t c b r   and the pure inductive learning system  pil  in playing o t h e l l o . it also shows that the performance of pil is superior to the performance of l a t c b r . this result is unexpected because the training data are not linearly separable. we expected pil to perform more poorly  and are unable to explain why it did not. the performance of c a b o t against the oracle is not surprising. however  it is interesting to note that it took c a b o t just 1 games to discover the oracle's best game. after that point  the two opponents began playing the same game repeatedly  each losing exactly 1% of the games. 
   o t h e l l o players are rated both on how often they win and on the magnitude of the score. table 1 reports the average number of discs won by each player in each tournament. the figures confirm that the hybrid system is better at selecting moves than either of its pure opponents. 
figure 1 shows the decision accuracy of c a b o t and 
l a t c b r . decision accuracy is the percentage of time that a player's chosen move is judged to be the best by the oracle. this graph shows a running average of decision accuracy through 1 games  and confirms that c a b o t ' s performance is better than l a t c b r ' s because its decisions tend to be more accurate. the training data for both systems are identical  so the only factor that can account for this difference is c a b o t ' s improved retrieval and adaptation strategies. the improvement in c a b o t ' s retrieval and adaptation strategies occurs quickly  usually within the first game or two. in contrast  l a t c b r is initially error-prone but its accuracy continues to improve as its case base grows. figure 1 shows that the difference between the two accuracies decreases as the number of games increases. this confirms the intuition that having enough cases is asymptotically as good as having a smaller number of cases with better retrieval and adaptation strategies. 
   superior performance is one result of adjusting retrieval and adaptation strategies. another result is the difference in the growth of the case base. figure 1 shows 
	callan  fawcett  and rissland 	1 

this growth during the c a b o t vs l a t c b r tournament. the c a b o t case base tends to be about half the size of the l a t c b r case base. neither case base shows any sign of stabilizing. continued growth may be due to the small number of games that have been played. the search space for othello is large  about 1   legal boards   so it may be unrealistic to expect either case base to stabilize within 1 games. 
   results vary from tournament to tournament  because of the effects of learning  and because the oracle makes random choices when it has two or more equally good moves. we have observed variation in the specific numbers of games won and lost by each move selection strategy. however  the relative performance of each strategy is consistent. c a b o t is superior both to l a t c b r and p1l  and pil is superior to l a t c b r . 
   we have also tested c a b o t against opponents that use other inductive learning techniques to guide move selection. one of these opponents used the id1 algorithm for building decision trees  quinlan  1 . one problem with using id1 for this task is that the othello boards are described by numeric features  whereas id1 is intended for symbolic attributes. we experimented with several methods of converting numeric data to symbolic attributes  including quinlan's method  1   vapnik's method  1   and our own manual method. none of these methods for creating symbolic attributes was both computationally feasible and more effective than the weighted sum evaluation function of the pil system. 
1 	conclusions 
c a b o t was developed to investigate the hypothesis that case-based reasoning systems would benefit from the ability to dynamically adjust their retrieval and adaptation mechanisms. c a b o t reasons from cases for the purpose of guiding state-space search. state-space search is different than traditional c b r tasks because the problem-solver's task is to repeatedly identify the successor state to explore next. as a result  c a b o t performs a restricted form of adaptation  in which the retrieved case is used to select one of a set of known alternatives. 
   one of the assumptions of this work is that feedback from the environment is necessary for making proper adjustments to retrieval and selection mechanisms. the feedback provided to c a b o t and its opponents consists of the best move as identified by the oracle. however  no information is provided that might suggest why one move was better than another. this feedback is quite weak when compared with the detailed information available to systems like c h e f  hammond  1  and p r o t o s  bareiss & porter  1 . 
   the empirical results reported in this paper confirm several hypotheses. first  they demonstrate that the ability to tune retrieval and selection mechanisms leads to both a smaller case base and better performance. they suggest that traditional c b r systems may be saving more cases than they really need. second  they demonstrate that detailed feedback from the environment  while useful  is not always necessary to make these adjustments. sometimes it is enough just to know the desired result. finally  the performance of c a b o t when played against pure inductive and pure case-based learning opponents illustrates that a hybrid architecture can overcome the weaknesses of opponents of each type. 
acknowledgements 
support for this work was provided by the office of naval research through a university research initiative program  under contracts n1-k-1 and n1-k-1. comments by david aha and bernard silver were especially valuable. we thank jeff clouse for providing the oracle program. 
