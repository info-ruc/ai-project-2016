
in this paper  we outline the development of a system that automatically constructs ontologies by extracting knowledge from dictionary definition sentences using robust minimal recursion semantics  rmrs   a semantic formalism that permits underspecification. we show that by combining deep and shallow parsing resources through the common formalism of rmrs  we can extract ontological relations in greater quality and quantity. our approach also has the advantages of requiring a very small amount of rules and being easily adaptable to any language with rmrs resources.
1 introduction
ontologies are an important resource in natural language processing. they have been shown to be useful in tasks such machine translation  question answering  and word-sense disambiguation  among others where information about the relationship and similarity of words can be exploited. while there are large  hand-crafted ontologies available for several languages  such as wordnet for english  fellbaum  1  and goitaikei for japanese  ikehara et al.  1   these resources are difficult to construct and maintain entirely by hand. they are  however  of proven utility in many nlp tasks  such as pp-attachment  where results using wordnet approach human accuracy  1% vs 1%   while the best methods using automatically constructed hierarchies still lag behind  at 1%   pantel and lin  1 . therefore  there is still a need to improve methods of both fully-automated and supervised construction of ontologies.
　there is a great deal of work on the creation of ontologies from machine readable dictionaries  a good summary is  wilkes et al.  1    mainly for english. recently  there has also been interest in japanese  tsurumaru et al.  1; tokunaga et al.  1; bond et al.  1 . most approaches use either a specialized parser or a set of regular expressions tuned to a particular dictionary  often with hundreds of rules. in this paper  we take advantage of recent advances in both deep parsing and semantic representation to combine general purpose deep and shallow parsing technologies with a simple special relation extractor.
　our basic approach is to parse dictionary definition sentences with multiple shallow and deep processors  generating semantic representations of varying specificity. the semantic representation used is robust minimal recursion semantics  rmrs: section 1 . we then extract ontological relations using the most informative semantic representation for each definition sentence.
　in this paper we discuss the construction of an ontology for japanese using the the japanese semantic database lexeed  kasahara et al.  1 . the deep parser uses the japanese grammar jacy  siegel and bender  1  and the shallow parser is based on the morphological analyzer chasen.
　we carried out two evaluations. the first gives an automatically obtainable measure by comparing the extracted ontological relations by verifying the existence of the relations in exisiting wordnet  fellbaum  1 and goitaikei  ikehara et al.  1  ontologies. the second is a small scale human evaluation of the results.
1 resources
1 the lexeed semantic database of japanese
the lexeed semantic database of japanese is a machine readable dictionary that covers the most common words in japanese  kasahara et al.  1 . it is built based on a series of psycholinguistic experiments where words from two existing machine-readable dictionaries were presented to multiple subjects who ranked them on a familiarity scale from one to seven  with seven being the most familiar  amano and kondo  1 . lexeed consists of all open class words with a familiarity greater than or equal to five. an example entry for the word {′ doraiba．  driver  is given in figure 1  with english glosses added. the underlined material was not in lexeed originally  we add it in this paper. doraiba．  driver  has a familiarity of 1  and three senses. lexeed has 1 words divided into 1 senses and defined with 1 definition sentences.
　useful hypernym relations can also be extracted from large corpora using relatively simple patterns  e.g.   pantel et al.  1  . however  even a large newspaper corpus does not include all the familiar words of a language  let alone those words occurring in useful patterns  amano and kondo  1 . therefore it makes sense to extract data from machine readable dictionaries  mrds .
　
 
headword
 pos{′
noundoraiba-
lexical-typenoun-lex		 
familiarity	1  1 	   	 s1 f1/wh1/	    
 
	  definition  	/            
	 	  
 	     
  
  
    
 
	 	 
	，	 	 
	s1  /k/ u/1d/1/	 
 definition 
someone who drives a car 
  
    	 
	  sense 1	hito  person 	  	 
	 	1:driveri   1:person 	 
  
	 person1 	 
  
  
	 	s1 ＞ / /	/1/fi /x/g/ 
          in golf  a long-distance club 
	 	 definition	 s1   /} /	/ 
	 	  
	 	  
	  sense 1	  	a number one wood . 
 
  
	 	  hypernym	 1 kurabu  club  
  
	 	 wordnet sense	driver1    club1  
	domain	＞ 1 gorufu  golf 
figure 1: entry for the word doraiba-  driver  from lexeed  with english glosses 
hook h1  propositionmrel h1 h1:  qeq h1: h1 
jidoushan h1 x1: 
udefrel h1 x1: 
rstr h1 h1:  body h1 h1:  qeq h1: h1 
untens1 h1 e1:present: 
arg1 h1 x1: 
     arg1 h1 x1:  hiton h1 x1: 
　　　ing h1: h1:  udefrel h1 x1: 
rstr h1 h1:  body h1 h1:  qeq h1: h1 
propositionmrel h1 h1:  qeq h1: h1 
unknownrel h1 e1:present: 
arg1 h1 x1: hook h1 
jidoushan h1 x1 
orel h1 u1 
untens h1 e1 
sururel h1 e1  hiton h1 x1 rmrs from jacy  deep rmrs from chasen  shallow jidosha wo unten suru hito．	''a person who drives a car  lit: car-acc drive do person ''
real predicates are shown in bold font.
figure 1: deep and shallow rmrs results for doraiba．1
　
1 parsing resources
we used the robust minimal recursion semantics  rmrs  designed in the deep thought project callmeier et al.   with tools from the deep linguistic processing with hpsg initiative  delph-in: http://www.delph-in.net/ .
robust minimal recursion semantics
robust minimal recursion semantics is a form of flat semantics which is designed to allow deep and shallow processing to use a compatible semantic representation  while being rich enough to support generalized quantifiers  frank  1 . the full representation is basically the same as minimal recursion semantics  copestake et al.  1 : a bag of labeled elementary predicates and their arguments  a list of scoping constraints  and a handle that provides a hook into the representation. the main difference is that handles must be unique  and there is an explicit distinction between grammatical and real predicates.
　the representation can be underspecified in three ways: relationships can be omitted  such as message types  quantifiers and so on ; predicate-argument relations can be omitted; and predicate names can be simplified. predicate names are defined in such a way as to be as compatible as possible among different analysis engines  e.g. lemma-pos-sense  where sense is optional and the part of speech  pos  is drawn from a small set of general types  noun  verb  sahen  verbal noun  ...  . the predicate untens is less specific than untens1 and thus subsumes it. in order to simplify the combination of different analyses  the results are indexed to the position in the original input sentence.
　examples of deep and shallow results for the same sentence  k u1jidosha wo unten suru hito．  a person who drives a car  lit: car-acc drive do person   are given in figure 1  omitting the indexing . real predicates are prefixed by an underbar    . the deep parse gives information about the scope  message types and argument structure  while the shallow parse gives little more than a list of real and grammatical predicates with a hook.
deep parser  jacy and pet 
the japanese grammar jacy  siegel and bender  1  was run with the pet system for the high-efficiency processing of typed feature structures  callmeier  1 .
shallow parser  based on chasen 
the part-of-speech tagger  chasen  matsumoto et al.  1  was used for shallow processing of japanese. predicate names were produced by transliterating the pronunciation field and mapping the part-of-speech codes to the rmrs super types. the part-of-speech codes were also used to judge whether predicates were real or grammatical. since japanese is a head-final language  the hook value was set to be the handle of the rightmost real predicate.
1 ontology construction
as outlined in section 1  our approach to ontology construction is to process a definition sentence with shallow and deep parsers and extract ontological relations from the most informative rmrs output. here  we describe the algorithm used to extract ontological relations from an rmrs structure:
1. let pi be the number of real predicates in the defining sentence
  if pi = 1  there is a unique real predicate  return: hsynonym: headword  predicatei
1. initialize a stack of semantic relations to be processedwith the semantic relation from the defining sentence's hook  the highest scoping handle 
1. pop a semantic relation from the stack and check itagainst special predicates that require additional processing
  when a relation indicating coordination or conjunction is found  locate all of its arguments and push them onto the stack for processing   if a special predicate is found  extract its relations and add them to the stack
  else if the current semantic relation is a real predicate  add it to list of extracted semantic heads repeat until stack is empty
1. return the ontological relations in the list of extractedsemantic heads in the form: hrelation: headword  semantic headi
　step 1. checks for a synonym relation  shown by a defining sentence containing a genus term with no differentia. such a sentence will have a semantic representation with only a single real predicate.
　in step 1.  for more complicated defining sentences  we try and find the genus term  looking first at the predicate with the widest scope. this is given by the rmrs's hook. the default ontological relation for the genus term is a hypernym.
　step 1. processes each semantic relation in the stack by searching for special predicates that require additional processing in order to retrieve the semantic head. special predicates include explicit relation names  such as ryaku  abbreviation   and some grammatical predicates. this step identifies and processes special predicates  adding any results to the stack of unprocessed semantic relations. if a relation is not identified as being a special predicate  and it is a nongrammatical predicate  then it is accepted as a semantic head  and it is added to the list of extracted relations. step 1 is repeated until the stack is empty.
　special predicates also give information about type of ontological relation that has been identified. they can confirm an implicit hypernym such as with isshu  one type  in japanese or identify an entirely different relation  as in the case of the relation part  which identifies a meronym relationship in english or meisho  honorific name  identifying a name relation in japanese. specials predicates can also extract non-ontological relations such as domain.
　step 1. returns the list of all non-grammatical predicates once all semantic heads have been processed for special relations and no new results are produced.
　this processing is following in the long tradition of parsing such special relationships  also called  empty heads    function nouns  or  relators    guthrie et al.  1; wilkes et al.  1 . our main innovation is to extract them from the semantic representations produced by a deep and shallow
special predicate japaneseontological relationisshu n 1hypernymhitotsu n 1hypernymsoushou n 1hyponymryakushou s 1abbreviationryaku s 1abbreviationkeishou n 1name:honorificzokushou n 1name:slangmeishou n 1namebubun n 1meronymichibu n 1meronymtable 1: special predicates and their associated ontological relations
parsing  rather than using regular expressions or specially designed parsers.
1 results and evaluation
we summarize the relationships acquired in table 1. the first two lines show thesaurus type relations: hypernyms and synonyms. the remaining four lines show other relations: abbreviations  names  meronyms and domains. hypernyms and synonyms are by far the most common relations: fewer than 1% of entries are marked with an explicit relationship.
　results are shown for lexeed using only the rmrs produced by chasen  using the results for jacy  and using the deepest possible result  jacy if it exists  backing off to chasen .
results for chasen
relation	noun	sahen	verb	other	total
hypernym1	1	111synonym1	1 11total1	1	1
results for jacy11relationnounsahenverbothertotalhypernym11111synonym11 1 1 1abbreviation11domain11other11total1
resu 1 lts for dee1 pest11relationnoun	sahen	verbothertotalhypernym11111synonym11 1 1 1abbreviation11domain11other11total11111table 1: results of ontology extraction  lexeed 
　as one would expect  the word based analysis using chasen finds more actual relationships  but does not provide enough information to find anything beyond implicit hypernyms and synonyms. the grammar based analyses have lower coverage  but allow us to extract some of the knowledge given explicitly in the lexicon.
　although the vast majority of relations extracted are hypernym and synonym relations  we extract several other kinds of knowledge  and are thus are closer to an ontology than a simple thesaurus.
　we carried out two evaluations. the first was an automatic evaluation  comparing our extracted triples hrelation: word1  word1i with existing resources. the second was a small scale hand evaluation of a sample of the relations.
1 verification with hand-crafted ontologies
because we are interested in comparing lexical semantics across languages  we compared the extracted ontology with resources in both the same and different languages.
　we verified our results by comparing the hypernym links to the manually constructed japanese ontology gt. it is a hierarchy of 1 semantic classes  defined for over 1 nouns  ikehara et al.  1 . the semantic classes are mostly defined for nouns  and verbal nouns   although there is some information for verbs and adjectives. senses are linked to gt semantic classes by the following heuristic: look up the semantic classes c for both the headword  wi  and genus term s   wg . if at least one of the index word's classes is subsumed by at least one of the genus' classes  then we consider the relationship confirmed  1 .
	  ch cg  : {ch   cg;ch （ c wh ;cg （ c wg }	 1 
　in the event of an explicit hyponym relationship indicated between the headword and the genus  the test is reversed: we look for an instance of the genus' class being subsumed by the headword's class  cg   ch .
　to test cross-linguistically  we looked up the headwords in a translation lexicon  alt-j/e  ikehara et al.  1  and edict  breen  1   and then did the confirmation on the set of translations ci   c t wi  . although looking up the translation adds noise  the additional filter of the relationship triple effectively filters it out again.
　for example  for {′ 1 doraiba-1  driver1   gt does not find any relationship  as it does not have the golf club semantic class label for {′ doraiba-. however  looking up t  {′   gives {driver  screwdriver} and the extracted hypernym is  1 kurabu  club . wordnet recognizes that driver1 is a kind of wood1 which is a kind of club1  using senses and relations from wordnet 1  fellbaum  1  . we thus simultaneously confirm the link is good; find an appropriate translation for this sense of {′  and its hypernym ; and link these to the appropriate wordnet synsems.
　the results of the evaluation for lexeed are shown in table 1. parts of speech are classified as  noun    verb    sahen  verbal noun   and  other . relations verified in either gt or wordnet are classed as verified. in these results  we only extract relations from the first definition sentence for each headword or when there is evidence of a synonym relation  as other definition sentences often provide clarification of the first sentence and may not contain useful ontological information. however  extracting relations from all definition sentences results in a net loss in confirmation of less than five percent while extracting over 1 additional relations. this suggests that even secondary definition sentences contain information that can be exploited in building ontologies.
　our results using jacy achieve a confirmation rate of 1% for nouns only and 1% overall  besting tokunaga et al.   who reported 1% for nouns only. our method also allows us to extract multiple relations from a single sentence by processing coordinate clauses. this allowed us to extract an extra 1 relations. using the deepest rmrs parse available  we confirmed 1% of the noun relations and 1% overall. this is comparable to our results reported in  bond et al.  1  with one important difference: we are now extracting over 1 more confirmed ontological relations.
   gt and wordnet both lack complete cover - over half the relations were confirmed with only one resource. this shows that the machine readable dictionary is a useful source of these relations. cross lingual checking was surprisingly effective  resources in one language can be used to bootstrap those in another  as seen in the euro wordnet project.
1 human evaluation
one problem with using existing ontological resources to verify new relations is that only relations which are subsumed by the ontology being used for comparison can be verified. this poses a considerable problem for researchers who wish to extract new relations: be it from domains where such resources are unavailable  or in cases where existing resources are limited in scope  such as for verbs. in this case  it makes more sense to evaluate a selection of the results retrieved by hand than to rely completely on existing ontologies for verification.
　in this spirit  we conducted a hand-verification of a selection of our automatically acquired relations. 1 relations were selected using a stratified method over the entirety of our results  every 1th relationship  ordered by link-type and then headword . in this evaluation we only consider synonyms and any relationships extracted from the first sentence: the second and subsequent definition sentences tend to contain other non-hypernomic information. the results were then evaluated by native speakers of japanese were given the definition word  the semantic head we retrieved  and the posited relation type and asked to evaluate if the relation was accurate. they had access to the original lexicon.
　the human judges found the relations presented to them to be accurate 1% of the time. in the 1 relations that were judged unacceptable  it was also determined that a relation did exist in 1 cases  but it was incorrect  i.e. a synonym in place of a hypernym and so on . these errors had three sources: the most common was a lack of identified explicit relationships; the next was lack of information from the shallow parse and the last was errors in the argument structure of the deep parse. tokunaga et al.  report slightly higher results for extracting noun relationships only  1% .
1 discussion and future work
we were able to successfully combine deep and shallow processing to extract ontological information from lexical resources. we showed that  by using a well defined semantic representation  the extraction can be generalized so much that it can be used on very different dictionaries from different languages. this is an improvement on the common approach to using more and more detailed regular expressions  e.g. tokunaga et al.  . although this provides a quick start  the results are not generally reusable. in comparison  the chasen-rmrs engine is immediately useful for a variety of tasks  such as question answering and information extraction  callmeier et al.  1 
　the other innovation of our approach is the cross-lingual evaluation. as a by-product of the evaluation we enhance the existing resources  such as gt or wordnet  by linking them  so that information can be shared between them. further  we hope to use the cross-lingual links to fill in gaps in the monolingual resources. finally  we can trivially extract links from the gt ontology to wordnet  thus combining two useful resources and allowing us to compare them in detail.
　there are several areas to address in future work as we continue to pursue ontology acquisition. first  and foremost  in order to increase the quality of the ontological relations that are extracted  we need to improve the quality of the parsers to generate rmrs. with our hpsg parsers  this can be addressed by adjusting the parse ranking model to take into account the special features of dictionary definition sentences. in addition we are currently increasing the coverage by adding in treatments of more grammatical phenomena.
　another area of great interest is the acquisition of other ontological relations. for example  if we extend our special predicates to include the relation produced by various forms of negation  we may be able to extract antonym relations.
　finally  we would also like to use the links created during evaluation which link our ontologies to hand-crafted ontologies  to furtherlink together senses of words across languages. this kind of cross-lingual ontology would be of great use in applications like machine translation.
1 conclusion
we have demonstrated how deep and shallow processing techniques can be used together to enrich the acquisition of ontological information by constructing an ontology for japanese. our approach requires few rules and is thus easy to maintain and expand  and it can be easily extended to cover any language that has rmrs resources. in future research  we plan to extend our processing to retrieve more ontological relations and to investigate means of improving the accuracy of output of both deep and shallow processors.
