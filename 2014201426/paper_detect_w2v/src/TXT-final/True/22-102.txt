 
we investigate the problem of learning dnf concepts from examples using decision trees as a concept description language. due to the replication problem  dnf concepts do not always have a concise decision tree description when the tests at the nodes are limited to the initial attributes. however  the representational complexity may be overcome by using high level attributes as tests. we present a novel algorithm that modifies the initial bias determined by the primitive attributes by adaptively enlarging the attribute set with high level attributes. we show empirically that this algorithm outperforms a standard decision tree algorithm for learning small random dnf with and without noise  when the examples are drawn from the uniform distribution. 
1 	introduction 
the goal of a system that learns from examples is to improve classification performance on new instances of some target concept  after observing a relatively small number of examples. a key technique is to formulate a simple explanation  hypothesis  of the training examples using a concept description language. then  a new instance is classified according to the induced hypothesis. 
   the simplest way to describe an instance is by a measurement vector on a set of predefined attributes. an attribute comprises a  possibly  relevant characteristic of the target concept  and an attribute measurement reveals the state of the attribute in the observed instance. such description is called attribute-based. 
   a frequently studied class of target concepts in attribute-based learning is the class of target concepts that can be represented by small disjunctive normal form  dnf  formulae  valiant  1  haussler  1  michalski  1 . while general purpose learning algorithms have been proposed for certain subclasses of simple dnf formulae  valiant  1   the general problem is still open. in this paper  we investigate the problem of learning small dnf concepts using decision trees as the concept representation language  breiman et a/.  1  quinlan  1 . 
   concepts with a small dnf description do not always have a concise decision tree representation when the tests at the decision nodes are limited to single attributes. the representational complexity is due to the fact that the tests to determine if an instance satisfies a term have to be replicated in the tree. we call this representational shortcoming the replication problem. the problem is reduced  and  eventually disappears   as we show in section 1  by using high level attributes as tests in the tree. the complexity of classification rules derived from decision trees have been also addressed in  breiman et a/.  1  quinlan  1b . 
   for learning  appropriate high level attributes may be defined a priori or dynamically by the learning algorithm. when the features are defined a priori  they are usually determined as combinations of the primitive attributes of a fixed type and size  e.g.  conjunctions up to a fixed size  breiman et a/.  1  valiant  1  rivest  1 . in general  only a small number of features defined are meaningful for learning. a higher percentage of relevant features can be obtained  for example  by using knowledge about the concept at hand. 
   on the other hand  when the features are defined by the learning system  no background knowledge about the target concept is needed. the capability of a learning algorithm to enlarge adaptively the initial attribute set is called dynamic bais  utgoff and mitchell  1 . genera  purpose learning systems with this capability have been proposed  for example  in  schlimmer  1  muggleton  1 . 
   in this paper  we present a novel heuristic for a decision tree algorithm to modify the initial bias determined by the primitive attributes by dynamically introducing high level attributes. the learning algorithm we present  called fringe   builds a decision tree using the primitive attributes  and analyses this tree to find candidates for useful features. then  it redescribes the examples using the new attributes  in addition to the primitive attributes  and builds a new decision tree. this process is iterated until no new candidate attributes are found. 
   the performance of fringe was tested in several synthetic boolean domains. our results show that fringe improves upon the classical decision tree approach when the target concept has a small dnf description  even in the presence of noise. however  when a target concept has no small dnf description   e.g.  
pagallo 
the majority function  or when the attribute values are not correlated to the classification  e.g.  the parity function   fringe cannot improve significantly on the poor performance of the classical approach. 
1 	definitions 
in this section we introduce the notation and definitions that formalize the basic notions we use in this paper. to simplify our presentation we limit ourselves to the case where all attributes are boolean. 
a decision tree is a tree with labelled nodes and edges. 
in our setting  an internal node defines a binary test on an attribute. for example  the root node of the tree in figure 1 defines the test is  x1 = 1  . each edge represents an outcome of the test. we use the convention that the left edge represents the negative outcome  and the right edge represents the positive outcome. the label of a leaf represents the class label that is assigned by the tree to any example that reaches this node. 
   the size of a concept description with respect to a representation language is the number of bits required to write down the description. for simplicity  we mear sure the size of a decision tree by the number of internal nodes. 
   finally  we introduce some terminology to refer to combinations of boolean attributes. a literal is an attribute or its complement. a term is a conjunction of literals  a clause is a disjunction of literals. in general  a feature is any boolean combination of the attributes obtained by applying boolean operators to the primitive attributes. we use the term variable to refer to a primitive attribute or to a feature. we use    + and  bar  to denote the boolean operators and  or and not respectively. 
1 	decision trees 
the common procedure to learn a decision tree from examples is based on successive subdivisions of the sample. this process aims to discover sizable subsets of the sample that belong to the same class. the tree structure is derived by a top-down refinement process on the sample. 
   initially  we begin with an empty decision tree and a sample set. a test is applied to determine the best attribute to use to partition the sample. following  breiman et a/.  1  quinlan  1    we measure the merit of an attribute with respect to the subdivision process by the mutual information between the class and the attribute. then  the best attribute according to this measure is placed at the root of the tree  and the examples are partitioned according to their value on that attribute. each subset is assigned to one subtree of the root node  and the procedure is applied recursively. the subdivision process on one branch of the tree ends when no attribute provides information about the class label  i.e.  every attribute has zero mutual information. this leaf is labelled with the common class of these examples. 
   to avoid overtraining  the tree grown by the subdivision process described above is pruned back. we use the reduce error pruning method proposed by quinlan in  quinlan  1b . 

figure 1: smallest decision tree for x1   x1 + x1   x1   x1. 
1 	the replication problem 
a decision tree representation of a boolean function with a small dnf description typically has a peculiar structure: the same sequence of decision tests leading to a positive leaf is replicated in the tree. to illustrate the problem  consider the boolean function x1'x1 + x1-x1-x1. 
   the smallest decision tree for the function has the following structure  see figure 1 : each decision test in the rightmost path of the tree is an attribute of the shortest term in the formula. the path leads to a positive leaf and it corresponds to the truth setting for the first term. the left branch from each of the nodes is a partial truth assignment of the attributes that falsifies the shortest term. then  to complete the function representation  we have to duplicate the sequence of decisions that determine the truth setting of the second term on the left branch of each of the nodes. we call this representational shortcoming the replication problem. in general  there are shared attributes in the terms  so some replications are not present  but the duplication pattern does occur quite frequently. 
   due to the replication problem  while learning a decision tree  the partition strategy has to fragment the examples of a term into many subsets. this causes the algorithm to require a large number of examples in order to ensure that the subsets are large enough to give accurate probability estimates; otherwise the subdivision process branches incorrectly or terminates prematurely. 
   one way to solve the replication problem is to use conjunctions of the primitive literals at the decision nodes. to illustrate this solution  consider the two term formula discussed above. assume we use the conjunction x1 - x1 as the test at the root node. now  the right branch leads to a positive leaf that corresponds to the examples that satisfy the first term. the left branch leads to a representation of the second term  see figure 1 . we can represent this term by single attributes as before  but now the replication problem has disappeared. if  in addition  we have a feature for the second term  x1   x1   x1  we obtain a decision with only two internal nodes. 
but  how to define appropriate features while learning 
   

from examples  one approach is to determine them a priori as combinations of the primitive attributes of a fixed type and size  breiman et a/.  1  rivest  1 . a second approach is to discover them after learning a decision tree. this schema has been used by quinlan to infer production rules from a decision tree  quinlan  1a . quinlan's method generates a production rule from each path in the decision tree  and then simplifies them by removing irrelevant conditions  attributes  in the rule. 
   there are two main differences between this approach and the fringe algorithm we define in the next section. first  quinlan s method is a simplification procedure  so to find an accurate set of production rules the initial decision tree has to already contain all the relevant patterns. in view of the replication problem  this may require a large tree and a large training sample. for fringe only some patterns have to be present in the initial tree  other patterns may surface later as the iteration proceeds. second  quinlan's method maintains rules for both classes when learning a single concept. this representation may not be as efficient as fringe's representation for concepts with a small dnf description. 
1 	learning features 
the fringe learning algorithm uses the following iterative schema to define appropriate features. the algorithm begins with a set v of primitive attributes  and creates a decision tree for a set of examples  choosing its decision variables from the set v. then  a find-feature procedure generates new features as boolean combinations of the variables that occur near the fringe of the tree. we describe this heuristic in more detail below. the set of new features is added to the variable set  the examples are reexpressed using this expanded set of variables  and the execution of the decision tree algorithm and the find-feature procedure is repeated. we will call a single execution of both processes an iteration. the iterative process terminates when no new features are added to the variable set  or a maximum number of variables is reached. 
   the find-feature heuristic was originally designed to discover relevant conjunctions of the target concept. to see this  observe that a decision tree defines an equivalent dnf expression. each path from the root node to a positive leaf defines a term as follows: initially  set the term to the constant value 1; then  for each node in the path form the conjunction of the current term and the attribute at the node if the path proceeds to the right of the node  otherwise form the conjunction of the current term and the negation of this attribute. 
   the find-feature procedure defines a feature for each positive leaf as follows: initialize the feature the constant value 1  and consider the path of length two from the leaf. then  for each node on the path  form the conjunction of the current feature and the attribute at the the node if the path proceeds to the right  otherwise form the conjunction of the current feature and the negation of the attribute. a leaf node with distance less than two from the root does not define a feature. so  the find-feature heuristic simply defines features by applying the term formation rule to the fringe of the tree. for example  the find-feature heuristic applied to the decision tree in figure 1 would produce the features: x1   x1  x1   x1  x1 - x1  one feature appears twice . they correspond to the positive leaf nodes taken from left to right. 
   in each iteration  the find-feature heuristic forms very simple combinations: namely  conjunctions of two literals. the creation of longer terms  or more complex features  occurs adaptively through the iterative process. initially  the variable set contains only the attributes. after the k-i   iteration  the variable set may contain features of size up to 1*. for example  after the second iteration  a feature is either a term of size up to 1  or a conjunction of two clauses  each of size 1 or 1. negated features include clauses of size up to 1  and disjunctions of two terms of size 1 or 1. in the limit  the find-feature procedure has the capability of generating any boolean function of the literals  since the negation and and operators are a complete set of boolean operators. 
   figure 1 illustrates the learning performance results for a single execution of fringe on a small random dnf  dnf1  see section 1  and how it compares to a strategy where features are proposed at random. the random proposal heuristic works as follows. a feature is defined as the conjunction of two elements chosen at random from the current variable set. moreover  the random proposal heuristic adds in each iteration the same number of features as fringe did. so  both methods work with the same number of variables in each iteration. 
   the graph shows the change in percentage error and in size of the hypothesis generated by the two methods as the iteration proceeds. the error is measured on a sample drawn from the uniform distribution independently of the training sample. more details on the experimental design are given in section 1. 
   the shape of the error and size graphs for fringe  given in figure 1  are typical. after the first few iterations  a very accurate hypothesis is usually developed  
pagallo 
   

iteration 
figure 1: performance comparison between fringe and random as defined by:     solid line: % error for fringe;    dashed line : % error for random; d  solid line: hypothesis size for fringe; ¡õ   dashed line : hypothesis size for random. the right scale measures % error  the left scale measures the hypothesis size by the number of internal nodes 
and the remaining few steps are used to reduce the size of the representation by introducing more meaningful features. eventually  the process ends because the findfeature procedure cannot discover any new features  and any further iterations would produce the same hypothesis. 
the random guess heuristic was not successful at all. 
the small fluctuations occur because by chance a few of the random combinations are significant. 
   another interesting aspect of fringe's learning behavior is the form of the final hypothesis. in this example  the test target concept has 1 terms in its smallest dnf description and the final tree has 1 decision nodes. the decision variable at each node is exactly one conjunction in the original target concept. hence  the complexity of the final decision tree is the same as the complexity of the smallest dnf' representation for the concept. a similar behavior was observed for all dnf concepts  in the absence of noise  whose representation uses terms of approximately the same length. for dnf concepts with terms much longer than the average  the final hypothesis tends not to include them. however  the final hypothesis is still very accurate because the examples that only satisfy these long terms are rare when the examples are drawn from the uniform distribution. 
   to end this section  we discuss briefly how the findfeature procedure can be generalized to accommodate continuous attributes  and multiple concepts. in a decision tree a test for a continuous attribute defines a binary partition of the attribute range  breiman et a/.  1 . therefore  it can be thought of as a binary attribute  and the find feature procedure can be applied without modifications. 
   so far  only the positive leaves determine candidate features. a dual heuristic can be defined for cnf  conjunctive normal form  concepts by looking at the negative leaves  and by using disjunctions instead of conjunctions. moreover  a symmetric heuristic could be obtained by applying the original find-feature procedure to the positive as well as to the negative leaves. this last heuristic can also be used in the multiple class case  by applying the feature formation rule to all leaves  regardless of their class label. 
1 	experiments 
the performance of a learning algorithm can be measured in terms of the classification accuracy on unseen instances from the target concept and in terms of the size of the final hypothesis. the goal of our experiments was to test how well the fringe algorithm does using these two criteria  and how well it compares with a decision tree algorithm with respect to the same criteria. 
   the algorithms were tested on five domains. they are: small random dnf  multiplexor  parity  majority and prob-disj. in addition  we tested the performance of the algorithms in the presence of random noise in both attributes and classification. 
   we present in table 1 a concise description of the test functions by listing the total number of attributes  the number of terms  the average term length and the standard deviation from the average of the term length  in the smallest dnf description of the target concepts. the first four test functions  dnfl - dnf1  are small random dnf functions  pagallo and hausslcr  1   mult 1 is multiplexor on 1 attributes  wilson  1   par1 and par1 are parity four and five attributes respectively  pagallo and hausslcr  1   majll is majority out of 1 attributes  subutai and tesauro  1   and prdsj is a short hand for prob-disj  quinlan  1b . for all test functions except prdsj additional irrelevant attributes have been added. 
   we executed ten independent runs for each test function. the following design criteria for the sample sets were used for all domains except prdsj. for this domain we used the specifications given in  quinlan  1b  to facilitate the comparison of results. 
   in each execution  the learning and testing tasks were performed on two sets of examples independently drawn from the uniform distribution. the learning set was randomly partitioned into two subsets  training and pruning sets  using the ratios 1 and1  breiman et a/.  1 . the training set was used to generate a consistent hypothesis. the pruning set was used to reduce the size of the hypothesis and  hopefully  to improve its classification performance on new instances of the target concept. 
   let n be the number of attributes and k be the number of literals needed to write down the smallest dnf description of the target concept. let e be the percentage error that we wish to achieve during the testing task. the number of learning examples we used is given by the 
   
¡¡this formula represents roughly the number of bits needed to express the target concept  times the inverse of the error. this is approximately the number of examples given in  vapnik  1  blumer et a/.  1  which would suffice for an ideal learning algorithm that only considers hypotheses that could be expressed with at most the number of bits needed for the target  concept  and always produces a consistent hypothesis. qualitatively  the formula indicates that we require more training examples as the complexity of the concept increases or the error decreases. in our experiments we set e = 1%. we used 1 examples to test classification performance. 
   table 1 presents the results we obtained with the decision tree algorithm and fringe for each target concept. the table reports the average percentage error and the average tree size obtained over ten executions of the algorithms. the percentage error is the number of classification errors divided by the size of the test sample. the deviation of the actual results from the average error is within 1% for the decision tree algorithm and within 1%. for fringe. the size of the tree is measured by the number of internal nodes. 
   fringe discovered the exact concept for dnfl  dnf1  dnf1  multill and par1. the small error for dnp1 and dnf1 results from the fact that one or two terms in the concepts are much longer than the average length. so  examples that satisfy only the longer terms are very rare  when the examples are drawn from the uniform distribution. fringe did not include a description for the longer terms in the final hypothesis. 
   majority and parity concepts  with a large number of inputs  are hard for decision trees. majority is hard because there are a large number of terms in the smallest dnf representation of the concept  and hence there are a large number of terms is the smallest decision tree for the concept. parity is hard when the examples are drawn from the uniform distribution because the attributes are not correlated with classification  pagallo and haussler  1 . fringe found an exact representation for par1  but it could not improve significantly on the poor per-
formance of the decision tree algorithm for par1. both methods performed poorly on m a j l l . 
   the results for prdsj are  to within experimental error  comparable to the results reported in  quinlan  1b . 
   as an example of the sensitivity of the algorithms to noise we test them on the concept dnf1. figure 1 summarizes the performance results for the noise experiments with different levels of class and attribute noise. the average percentage error was taken over five execution of each algorithm. a noise level of n percent means that  with probability n percent the true value was flipped. for attribute noise  each attribute was corrupted independently by the same noise level. in all experiments  the learning and testing sets were generated as for the noise free case  and the results are an average over five executions. the learning and testing sets were corrupted using the same type and level of noise. the table shows that f r i n g e is more sensitive to attribute noise than class noise  as is the decision tree algorithm. 
   another informative measure of the performance of a learning system is how the classification accuracy varies as a function of the number of learning instances. this variation is described by a learning curve. figure 1 compares the learning curves for the decision tree algorithm and f r i n g e on dnf1. the percentage error is measured on an independent test set  and it is the average of five executions for each algorithm. f r i n g e finds an exact representation for the target concept using a learning set of 1  or more  examples. a sample size of 1 examples is the size of the learning set predicted by formula  1  for t = 1%. 
1 	conclusions 
in this paper we view the problem of learning from examples as the task of discovering an appropriate vocabulary for the problem at hand. we present a novel algorithm based on this approach. the algorithm uses a decision 
pagallo 
   

tree representation and it determines an appropriate vocabulary through an iterative process. the method accommodates target concepts that are described by discrete as well as continuous attributes  and it can be used to learn multiple concepts simultaneously. 
   we also show empirically that this method compares very favorably on boolean domains to an implementation of a standard decision tree algorithm for noise-free as well as noisy problems. since the difficulties of the decision tree method arise from a representational limitation  we expect that any reasonable implementation of this algorithm will give similar results. 
currently we are testing fringe on natural domains. 
so far  we have compared the methods on the mushroom 
 schlimmer  1   hypothyroid  quinlan  1b   and led domains  breiman et ai  1   the latter with 1% attribute noise. the hypotheses generated by the decision tree method were already very accurate  less than 1% error  in the first two domains  and they were close to optimal  1% error   breiman et al  1   in the led domain. the hypotheses generated by fringe were more concise and at least as accurate. 
acknowledgments 
i would like to thank david haussler for helpful discussions of the material presented here  and improvements to an earlier version of this paper. i also thank r. buntine  t. dietterich  j. schlimmer and p. utgoff for helpful comments to a technical report that preceded this paper. the support of the onr grant n1-k-1 is gratefully acknowledged. 
