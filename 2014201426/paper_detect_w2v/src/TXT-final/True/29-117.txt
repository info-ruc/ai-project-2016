 
a survey is given on two decades of developments in the field  encompassing an increase in computing power by four orders of magnitude. the '1-d approach' integrating expectation-based methods from systems dynamics and control engineering with methods from ai has allowed to create vehicles with unprecedented capabilities in the technical realm: autonomous road vehicle guidance in public traffic on freeways at speeds beyond 1 km/h  on-board-autonomous landing approaches of aircraft  and landmark navigation for agv's  for road vehicles including turn-offs onto cross-roads  and for helicopters in low-level flight  real-time  hardware-in-the-loop simulations in the latter case . 
1 	introduction 
road vehicle guidance based on video-signal processing has been picked up independently in japan  tsugawa et al.  
1   in europe  meissner  1   and in the usa  klass  1 . while in japan analog signal processing has been used and  quasi-steady  al-methods predominated in the us  recursive estimation methods well known from systems engineering have been extended to image sequence processing at the author's institute  ubm ; the resulting method had been dubbed '1-d approach'  in contrast to the 1-d  1d  and 1-d methods under discussion then  disregarding time as the fourth independent variable in the problem domain. the numerical efficiency and compactness in state representation of recursive estimation which directly allowed control applications for generating behavioral capabilities  finally  led to its wide-spread acceptance in the vision community. artificial neural nets  ann  also found wide acceptance in the usa  pomerleau  1  and around the globe even though image resolution used  about 1 pixel = lkpel   usually  was much less than with recursive estimation  1 kpel per image  even at a higher image rate . 
   both methods allowed road vehicles to run autonomously along highways and other types of roads up to rather high speeds  initially on empty roads only  dickmanns and zapp  1  pomerleau  1  but finally in normal freeway traffic also  dickmanns et al.  1  pomerleau  1 ; however  while ann's stayed confined to either lateral  pomerleau  1; mecklenburg et al.  1  or longitudinal control  fritz  1  at a time  the other mode had to be controlled by a human driver   the 1d approach allowed to detect  track and determine the spatio-temporal state  position and velocity components on a 1d surface  relative to about a dozen other objects in a range of up to 1 meters in front of and behind the own vehicle pickmanns  1a . the two final demonstrator vehicles in the european project prometheus: vita 1 of daimlerbenz and vamp of ubm  ulmer  1; dickmanns et al.  
1   may well be considered as the first two road vehicles of a new species capable of understanding  part of  their environment and of reacting properly to the actual needs on their own  completely autonomous . 
   dynamic remote sensing for intelligent motion control in an environment with rapidly changing elements requires the use of valid spatio-temporal models for efficient handling of the large data streams involved. other objects have to be recognized with their relative motion components  the near ones even with high precision for collision avoidance; this has to be achieved while the own vehicle body carrying the cameras moves in an intended way and is  simultaneously  subject to perturbations hardly predictable. 
   for this complex scenario  inertial sensing in addition to vision is of great help; negative angular rate feedback to a viewing direction control device allows to stabilize the appearance of stationary objects in the image sequence. measured accelerations and velocities will  via signal integration  yield predictions for.translational and rotational positions affecting the perspective mapping process. these predictions are good in the short run  but may drift slowly in the long run  especially when inexpensive inertial sensors are used. these drifts  however  can easily be compensated by visual interpretation of static scene elements. 
	dickmanns 	1 
1 	simultaneous representations on differential and multiple integral scales 
combined use of inertial and visual sensing is well known from biological systems  e.g. the vestibular apparatus and its interconnections to eyes in vertebrates. in order to make optimal use of inertial and visual signals  simultaneous differential and integral representations on different scales both in space and in time are being exploited; table 1 shows the four categories introduced: the upper left corner represents the point 'here and now' in space and time where all interactions of a sensor or an actuator with the real world take place. inertial sensors yield information on local accelerations  arrow 1 from field  1  to field  1  in the table  and turn rates of this point. within a rigid structure of an object the turn rates are the same all over the body; therefore  the inertially measured rate signals  arrow 1 from field  1  to  1   are drawn on the spatial object level  row 1 . 
   the local surface of a structure may be described by the change of its tangent direction along some arc length; this is called curvature and is an element of local shape. it is a geometrical characterization of this part of the object in differential form; row 1 in table 1 represents these local spatial differentials which may cause specific edge features  straight or curved ones  in the image under certain aspect conditions. 

table 1: differential and integral representations on different scales for dynamic perception 
1 	invited speakers    single objects may be considered to be local spatial integrals  represented in row 1 of table 1   the shapes of which are determined by their spatial curvature distributions on the surface; in connection with the aspect conditions and the photometric properties of the surface they determine the feature distribution in the image. since  in general  several objects may be viewed simultaneously  also these arrangements of objects of relevance in a task context  called 'geometrical elements of a situation'  are perceived and taken into account for behavior decision and reactive control. for this reason  the visual data input labeled by the index 1 at the corresponding arrows into the central interpretation process  field  1   has three components: 1a  for measured features not yet associated with an object  the so-

called detection component; 1b  the object-oriented tracking component with a strong predictive element for efficiency improvement  and 1c  the perception component for the environment which preshapes the maneuver space for the self and all the other objects. looked at this way  vision simultaneously provides geometrical information both on differential  row 1  and integral scales  rows: 1 for a single objects  1 for local maneuvering  and 1 for mission performance . 
   temporal change is represented in column 1 which yields the corresponding time derivatives to the elements in the column to the left. because of noise amplification associated with numerical differentiation of high frequency signals  this operation is usable only for smooth signals  like for computing speed from odometry; especially  it is avoided deliberately to do optical flow computation at image points. even on the feature level  the operation of integration with a smoothing effect  as used in recursive estimation  is preferred. 
   in the matrix field  1  of table 1 the key knowledge elements and the corresponding tools for sampled data processing are indicated: due to mass and limited energy availability  motion processes in the real world are constrained; good models for unperturbed motion of objects belonging to specific classes are available in the natural and engineering sciences which represent the dependence of the temporal rate of change of the state variables on both the state- and the control variables. these are the so-called 'dynamical models'. for constant control inputs over the integration period  these models can be integrated to yield difference equations which link the states of objects in column 1 of table 1 to those in column 1  thereby bridging the gap of column 1; in control engineering  methods and libraries with computer codes are available to handle all problems arising. once the states at one point in time are known  the corresponding time derivatives are delivered by these models. 
recursive estimation techniques developed since the 
1ies exploit this knowledge by making state predictions over one cycle disregarding perturbations; then  the measurement models arc applied yielding predicted measurements. in the 1-d approach  these are communicated to the image processing stage in order to improve image evaluation efficiency  arrow 1 from field  1  to  1  in table 1 on the object level  and arrow 1 from  1  to  1  on the feature extraction level . a comparizon with the actually measured features then yields the prediction errors used for state update. 
   in order to better understand what is going to happen on a larger scale  these predictions may be repeated several to many times in a very fast in advance simulation assuming likely control inputs  for stereotypical maneuvers like lane changes in road vehicle guidance  a finite sequence of 'feedforward' control inputs is known to have a longer term state 
transition effect. these are represented in field  1  of table 1 and by arrow 1; section 1 below will deal with these problems. 
   for the compensation of perturbation effects  direct state feedback well known from control engineering is used. with linear systems theory  eigenvalues and damping characteristics for state transition of the closed loop system can be specified  field  1  and row 1 in table 1 . this is knowledge also linking differential representations to integral ones; low frequency and high frequency components may be handled separately in the time or in the frequency domain  laplace-transform  as usual in aero-space engineering. this is left open and indicated by the empty row and column in table 1. 
   the various feed-forward and feedback control laws which may be used in superimposed modes constitute behavioral capabilities of the autonomous vehicle. if a sufficiently rich set of these modes is available  and if the system is able to recognize situations when to activate these behavioral capabilities with which parameters for achieving mission goals  the capability for autonomous performance of entire missions is given. this is represented by field  n n   lower right  and will be discussed in sections 1 to 1. essentially  mission performance requires proper sequencing of behavioral capabilities in the task context; with corresponding symbolic representations on the higher  more abstract system levels  an elegant symbiosis of control engineering and al-methods can thus be realized. 
1 task domains 
though the approach is very general and has been adapted to other task domains also  only road and air vehicle guidance will be discussed here. 
1 	road vehicles 
the most well structured environments for autonomous vehicles are freeways with limited access  high speed vehicles only  and strict regulations for construction parameters like lane widths  maximum curvatures and slopes  on- and off-ramps  no same level crossings. for this reason  even though speed driven may be high  usually  freeway driving has been selected as the first task domain for autonomous vehicle guidance by our group in 1. 
   six perceptual and behavioral capabilities are sufficient for navigation and mission performance on freeways: 1. lane recognition and lane following with adequate speed  1. obstacle recognition and proper reaction  e.g. transition into convoy driving or stopping; 1. recognition of neighboring lanes  their availability for lane change  and lane changing performance; 1. reading and obeying traffic signs  1. reading and interpreting navigation information including proper lane selection  and 1. handling entries and exits. 
	dickmanns 	1 
   on well kept freeways it is usually not necessary to check surface structure or to watch for humans or animals entering from the side. none-the-less  safe reaction to unexpected events must be required for a mature autonomous system. 
   on normal state roads the variability of road parameters and of traffic participants is much larger; especially  same level crossings and oncoming traffic increase relative speed between objects  thereby increasing hazard potential even though traveling speed may be limited to a much lower level. bicyclists and pedestrians as well as many kinds of animals are normal traffic participants. in addition  lane width may be less in the average  and surface state may well be poor on lower order roads  e.g. potholes  especially in the transition zone to the shoulders. 
   in urban traffic  things may be even worse with respect to crowdedness and crossing of subjects. these latter mentioned environments are considered to be not yet amenable to autonomous driving because of scene complexity and computing performance required. 
   however  driving on minor roadways with little traffic  even without macadam or concrete sealing  has been attacked for research purposes in the past  and may soon be performed safely with the increasing computing power becoming available now. if it is known that the ground is going to support the vehicle  even cross country driving can 
be done including obstacle avoidance. however  if compared to human capabilities in these situations  there is still a long way to go until autonomous systems can compete. 
1 	air vehicles 
as compared to ground vehicles with 1  full 1 degrees of freedom are available for trajectory shaping  here. in addition  due to air turbulence and winds  the perturbation environment may be much harder than on roads. for this reason  inertial sensing is considered mandatory in this task domain  in addition  visual navigation guidelines like lanes on roads are not available once the aircraft is airborne at higher altitudes. microwave electronic guidelines have been established instead. 
   vision allows the pilot or an autonomous aircraft to navigate relative to certain landmarks; the most typical task is the landing approach to a prepared runway for fixed wing aircraft  or to the small landing site usually marked by the large letter h for a helicopter. these tasks have been selected for first demonstrations of the capabilities of seeing aircraft. contrary to other electronic landing aids like ils or mls  machine vision also allows to detect obstacles on the runway and to react in a proper manner. 
   for flights close to the earth surface  terrain formations may be recognized as well as buildings and power lines  thus  obstacle avoidance in nap-of-the-earth flights is a natural extension of this technique for unmanned air vehicles  both with fixed wings and for helicopters. for the 
1 	invited speakers 
latter  the capability of recognizing structures or objects on the ground and of hovering in a fixed position relative to these objects despite perturbations  will improve rescue capabilities and delivery performance. 
   motion control for fixed wing aircraft and for helicopters is quite different from each other; by the use of proper dynamical models and control laws it has been shown that the 1-d approach allows to turn each craft into an autonomous agent capable of fully automatic mission performance. this will be discussed in section 1. 
1 	the sensory systems 
the extremely high data rates of image sequences are both an advantage  with respect to versatility in acquiring new 
information on both environment and on other objects/subjects  and a disadvantage  with respect to computing power needed and delay time incurred until the information has been extracted from the data . for this reason it makes sense to also rely on conventional sensors in addition  since they deliver information on specific output variables with minimal time delay. 
1 	conventional sensors 
for ground vehicles  odometers  speedometers as well as sensors for positions and angles of subparts like actuators and pointing devices are commonplace. for aircraft  pressure measurement devices yield information on speed and altitude flown; here  inertial sensors like accelerometers  angular rate- and vertical as well as directional gyros arc standard. evaluating this information in conjunction with vision alleviates image sequence processing considerably. based on the experience gained in air vehicle applications  the inexpensive inertial sensors like accelerometers and angular rate sensors have been adopted for road vehicles too  because of the beneficial and complementary effects relative to vision. part of this has already been discussed in section 1 and will be detailed below. 
1 	vision sensors 
because of the large viewing ranges required  a single camera as vision sensor is by no means sufficient for practical purposes. in the past  bifocal camera arrangements  see fig.l  with a wide angle  about 1뫢  and a tele camera  about 1뫢 aperture  mounted fix relative to each other on a two-axis platform for viewing direction control have been used  dickmanns  1a ; in future systems  trinocular camera arrangements with a wide simultaneous field of view    1뫢  from two divergently mounted wide angle cameras and a 1-chip color ccd-camera will be used  dickmanns  1b . for high-speed driving on german autobahnen  even a fourth camera with a relatively strong tele-lens will be added allowing lane recognition at several hundred meters distance. 

   all these data are evaluated 1 times per second  the standard european video rate. 

figure 1: binocular camera arrangement of vamp 
1 	global positioning system  gps-  sensor 
for landmark navigation in connection with maps a gpsreceiver has been integrated into one system in order to have sufficiently good initial conditions for landmark detection. even though only the least accurate c/a code is being used  in connection with inertial sensing and map interpretation good accuracies can be achieved after some time of operation  furst et al  1 ; gps signals are available only once every second. 
1 	spatio-temporal perception: 
the 1-d approach 
since the late 1ies  observer techniques as developed in systems dynamics  luenberger  1  have been used at ubm in the field of motion control by computer vision  meissner  1; meissner and dickmanns  1 . in the early 1ies  h.j. wuensche did a thorough comparison between observer- and kalman filter realizations in recursive estimation applied to vision for the original task of 
balancing an inverted pendulum on an electro-cart by computer vision  wuensche  1 . since then  refined versions of the extended kalman filter  ekf  with numerical stabilization  udut-factorization  square root formulation  and sequential updates after each new measurement have been applied as standard methods to all dynamic vision problems at ubm. 
based on experience gained from 'satellite docking' 
 wuensche  1   road vehicle guidance  and on-board autonomous aircraft landing approaches by machine vision  it was realized in the mid 1ies that the joint use of dynamical models and temporal predictions for several aspects of the overall problem in parallel was the key to achieving a quantum jump in the performance level of autonomous systems based on machine vision. beside state estimation for the physical objects observed and control computation based on these estimated states it was the feedback of knowledge thus gained to the image feature extraction and to the feature aggregation level which allowed for an increase in efficiency of image sequence evaluation of one to two orders of magnitude.  see fig. 1 for a graphical overview.  
   following state prediction  the shape and the measurement models were exploited for determining: 
  viewing direction control by pointing the two-axis plat-form carrying the cameras; 
  locations in the image where information for most easy  non-ambiguous and accurate state estimation could be found  feature selection   
  the orientation of edge features which allowed to reduce the number of search masks and directions for robust yet efficient and precise edge localization  
  the length of the search path as function of the actual measurement uncertainty  
  strategies for efficient feature aggregation guided by the idea of the 'gestalt' of objects  and 
  the jacobian matrices of first order derivatives of feature positions relative to state components in the dynamical models which contain rich information for interpretation of the motion process in a least squares error sense  given the motion constraints  the features measured  and the statistical properties known. 

figure 1: multiple feedback loops on different space scales for efficient scene interpretation and behavior control: control of image acquisition and -processing  lower left corner   1-d 'imagination-space in upper half; motion control  lower right corner . 
dickmanns 

this integral use of 
1. dynamical models for motion of and around the center of gravity taking actual control outputs and time delays into account  
1. spatial  1-d  shape models for specifying visually measurable features  
1. the perspective mapping models  and 
1. prediction error feedback for estimation of the object state in 1-d space and time 
simultaneously and in closed loop form was termed the '1-d approach'. it is far more than a recursive estimation algorithm based on some arbitrary model assumption in some arbitrary subspace or in the image plane. 
   it is estimated from a scan of recent publications in the field that even today most of the papers referring to 'kalman filters' do not take advantage of this integrated use of spatio-temporal models based on physical processes. 
   initially  in our applications just the ego-vehicle has been assumed to be moving on a smooth surface or trajectory  with the cameras fixed to the vehicle body. in the meantime  solutions to rather general scenarios are available with several cameras spatially arranged on a platform which may be pointed by voluntary control relative to the vehicle body. these camera arrangements allow a wide simultaneous field of view  a central area for trinocular  skew  stereo interpretation  and a small area with high image resolution for 'tele'-vision. the vehicle may move in full 1 degrees of freedom; while moving  several other objects may move independently in front of a stationary background. one of these objects may be 'fixated'  tracked  by the pointing device using inertia! and visual feedback signals for keeping the object  almost  centered in the high resolution image. a newly appearing object in the wide field of view may trigger a fast viewing direction change such that this object can be analysed in more detail by one of the tele-cameras; this corresponds to 'saccadic' vision as known from vertebrates and allows very much reduced data rates for a complex sense of vision. it essentially trades the need for time-sliced attention control and sampled-data based scene reconstruction against a data rate reduction of 1 to 1 orders of magnitude as compared to full resolution in the entire simultaneous field of view. 
   the 1-d approach lends itself for this type of vision since both object-orientation and the temporal  'dynamical'  models are available in the system already. this complex system design for dynamic vision has been termed emsvision  from expectation-based  multi-focal and saccadic ; it is actually being implemented with an experimental set of four miniature tv-cameras on a two-axis pointing platform named 'multi-focal active/reactive vehicle eye' marveye pickmanns  1b . 
   in the rest of the paper  major developmental steps in the 1-d approach over the last decade and results achieved will be reviewed. as an introduction  in the next section we 
invited speakers 
summarize the basic assumptions underlying the 1-d approach. 
1 	basic assumptions underlying the 
1-d approach 
it is the explicit goal of this approach to take  as much as possible  advantage of physical and mathematical models of processes happening in the real world. models developed in the natural sciences and in engineering over the last centuries  in simulation technology and in systems engineering  decision and control  over the last decades form the base for computer-internal representations of real-world processes: 
1. the  mesoscopic  world observed happens in 1-d space and time as the independent variables; non-relativistic  newtonian  models are sufficient for describing these processes. 
1. all interactions with the real world happen 'here and now'   at the location of the body carrying special input/ouput devices; especially the locations of the sensors  for signal or data input  and of the actuators  for control output  as well as those body regions with strongest interaction with the world  as for example the wheels of ground vehicles  are of highest importance. 
1. efficient interpretation of sensor signals requires background knowledge about the processes observed and controled  that is both its spatial and temporal characteristics. invariants for process understanding may be abstract model components not graspable at one point in time. similarly  
1. efficient computation of  favorable or optimal  control outputs can only be done taking complete  or partial  process models into account  control theory provides the methods for fast and stable reactions. 
1. wise behavioral decisions require knowledge about the longer-term outcome of special feed-forward or feedback control modes in certain situations and environments; these results are obtained from integration of the dynamical models. this may have been done beforehand and stored appropriately  or may be done on the spot if analytical solutions are available or numerical ones can be derived in a small fraction of real-time as becomes possible now with the increasing processing power available. behaviors are realized by triggering the modes available from point 1 above. 
1. situations are made up of arrangements of objects  other active subjects  and of the own goals pursued; therefore  
1. it is essential to recognize single objects and subjects  their relative state  and for the latter also  if possible  their intentions in order to be able to make meaningful predictions about the future development of a situation  which is needed for successful behavioral decisions . 
1. as the term re-cognition tells  in the usual case it is assumed that objects seen are  at least  generically known already  only their appearance here  in the geometrical range of operation of the senses  and now is new; this allows a fast jump to an object hypothesis when first visual impressions arrive through sets of features. exploiting background knowledge  the model based perception process has to be initiated. free parameters in the generic object models may be determined efficiently by attention control and the use of special algorithms and behaviors. 
1. in order to be able to do step 1 efficiently  knowledge about 'the world' has to be provided in the context of 'task domains' in which likely co-occurrences are represented. in addition  knowledge about discriminating features is essential for correct hypothesis generation  indexing into the object data base . 
1. most efficient object  class  descriptions by invariants is usually done in 1-d space  for shape  and time  for motion constraints or stereotypical motion sequences ; modern microprocessors are sufficiently powerful to compute the visual appearance of an object under given aspect conditions in an image  in a single one  or even in several ones with different mapping parameters in parallel  at runtime. they are even powerful enough to numerically compute the jacobian matrices for sensor/object pairs of features evaluated with respect to ob-
ject state or parameter values; this allows a very flexible general framework for recursive state and parameter estimation. the inversion of perspective projection is thus reduced to a least squares model fit once the recursive process has been started. the underlying assumption here is that local linearizations of the overall process are sufficiently good representations of the nonlinear real process; for high evaluation rates like video frequency  1 or 1 hz  this is usually the case. 
1. in a running interpretation process of a dynamic scene  newly appearing objects will occur in restricted areas of the image such that bottom-up search processes may be confined to these areas. passing cars  for example  always enter the field of view from the side just above the ground; a small class of features allows to detect them reliably. 
1. subjects  i.e. objects with the capability of self induced generation of control actuation  are characterized by typical  sometimes stereotypical  i.e. predictive  motion behavior in certain situations. this may also be used for recognizing them  similar to shape in the spatial domain . 
1. the same object/subject may be represented internally at different scales with various degrees of detail; this allows flexible and efficient use in changing contexts  e.g. as a function of distance or degree of attention . 
1 	structural survey on the 1-d approach 
figure 1 shows the main three activities running in parallel in an advanced version of the 1-d approach: 
1. detection of objects from typical collections of features not yet assigned to some object already tracked  center left  upward arrow ; when these feature collections are stable over several frames  an object hypothesis has to be formed and the new object is added to the list of those regularly tracked  arrow to the right . 
1. tracking of objects and state estimation is shown in the loop to the lower right in figure 1; first  with the control output chosen  a single step prediction is done in 1-d space and time  the 'imagined real world'. this step consists of two components  a  the 'where'- signal path concentrating on progress of motion in both translational and rotational degrees of freedom  and b  the 'what'- signal path dealing with object shape.  in order not to overburden the figure these components are not shown.  
1. learning from observation is done with the same data as for tracking; however  this is not a single step loop but rather a low frequency estimation component concentrating on 'constant' parameters  or it even is an offline component with batch processing of stored data. this is an actual construction site in code development at present which will open up the architecture towards becoming more autonomous in new task domains as experience of the system grows. both dynamical models  for the 'where'-part  and shape models  for the 'what'part shall be learnable. 
another component under development not detailed in figure 1 is situation assessment and behavior decision; this will be discussed in section 1. 
1 	generic 1-d object classes 
the efficiency of the 1-d approach to dynamic vision is achieved by associating background knowledge about classes of objects and their behavioral capabilities with the data input. this knowledge is available in generic form  that is  structural information typical for object classes is fixed while specific parameters in the models have to be adapted to the special case at hand. motion descriptions for the center of gravity  the translational object trajectory in space  and for rotational movements  both of which together form the so-called 'where'-problem  are separated from shape descriptions  called the 'what'-problem. typically  summing and averaging of feature positions is needed to solve the where-problem while differencing feature positions contributes to solving the what-problem. 
motion description 
possibilities for object trajectories are so abundant that they cannot be represented with reasonable effort. however  good models are usually available describing their evolution 
dickmanns 


1-d approach to dynamic machine vision: 
model-based recognition ; analysis through synthesis 
figure 1: survey on the 1-d approach to dynamic machine vision with three major areas of activity: object detection  central arrow upwards   tracking and state estimation  recursive loop in lower right   and learning  loop in center top   the latter two being driven by prediction error feedback. 
over time as a function of the actual state  the control- and the perturbation inputs. these so-called 'dynamical models'  usually  are sets of nonlinear differential equations 
u. v' t   with x as the n-component state vector  u as r-
componcnt control vector and v' as perturbation input. 
   through linearization around a nominal trajectory locally linearized descriptions are obtained which can be integrated analytically to yield the  approximate  local transition matrix description for small cycle times t tivity within the object  be it by pre-programmed outputs or by results obtained from processing of measurement data  we speak of a 'subject'. 
shape and feature description 
with respect to shape  objects and subjects are treated in the same fashion. only rigid objects and objects consisting of several rigid parts linked by joints have been treated; for elastic and plastic modeling see  decarlo and metaxas  1 . since objects may be seen at different ranges the appearance in the image may vary considerably in size. at large ranges the 1-d shape of the object  usually  is of no importance to the observer  and the cross-section seen contains most of the information for tracking. however  this cross-section depends on the angular aspect conditions; therefore  both coarse-to-fine and aspect-dependent modeling of shape is necessary for efficient dynamic vision. this will be discussed briefly for the task of perceiving road vehicles as they appear in normal road traffic. 
   coarse-to-fine shape models in 1-d: seen from behind or from the front at a large distance  any road vehicle may be adequately described by its encasing rectangle; this is convenient since this shape just has two parameters  width b and height h. absolute values of these parameters are of no importance at larger distances; the proper scale may be inferred from other known objects seen  like road or lane 
invited speakers 

width at that distance. trucks  or buses  and cars can easily be distinguished. our experience tells that even the upper limit and thus the height of the object may be omitted without loss of functionality  reflections in this spatially curved region of the car body together with varying environmental conditions may make reliable tracking of the upper body boundary very difficult ; thus  a simple u-shape of unit height  corresponding to about 1 m turned out to be practical  seems to be sufficient until 1 to 1 dozen pixels can be found on a line crossing the object in the image. depending on the focal length used  this corresponds to different absolute distances. 
   fig. 1a shows this shape model. if the object in the image is large enough so that details may be distinguished reliably by feature extraction  a polygonal shape approximation as shown in fig. 1b or even with internal details  fig. 1c  may be chosen; in the latter case  area-based features like the licence plate  the tires or the signal light groups  usually in yellow or reddish color  may allow more robust recognition and tracking. 

figure 1: coarse to fine shape model of a car in rear view: a  encasing rectangle  u-shape ; b  polygonal silhouette  
c  silhouette with internal structure. 
   if the view is from an oblique direction  the depth dimension  length of the vehicle  comes into play. even with viewing conditions slightly off the axis of symmetry of the vehicle observed  the width of the car in the image will start increasing rapidly because of the larger length of the body and due to the sine-effect in mapping. usually  it is impossible to determine the lateral aspect angle  body width and length simultaneously from visual measurements; therefore  switching to the body diagonal as a shape representation has proven to be much more robust and reliable in realworld scenes  schmid  1 . 
   just for tracking and relative state estimation  taking one of the vertical edges of the lower body and the lower bound of the object body has proven to be sufficient in most cases  thomanek  1 ; this  of course  is domain specific knowledge which has to be introduced when specifying the features for measurement in the shape model. 
   in general  modeling of well measurable features for object recognition has to be dependent on the aspect conditions. experience tells that area based features should play an important role in robust object tracking. initially  this has been realized by observing the average grey value on the vehicle-side of edge features detected; with more computing power available  color profiles in certain crosssections yield improved performance. 
   full 1-d models with different degrees of detail similar to the 1-d rear silhouette  different models may also be used for 1-d shape. the one corresponding to fig. 1a is the encasing box with perpendicular surfaces; if these surfaces can be easily distinguished in the image  and their separation line may be measured precisely  good estimates of the overall body dimensions may be obtained from small image sizes already. since space does not allow more details here  the interested reader is referred to  schick and dickmanns  1  schmid 1 . 
1 	image feature extraction 
due to space restrictions  this topic will not be detailed here; the interested reader is referred to  dickmanns and graefe  1  and an upcoming paper  dickmanns et al.  1 . figure 1 shows a survey on the method used. 

figure 1: intelligent control of image feature extraction parameters in the algorithms cronos  for edges  marked with a window lable eij  and triangle'  labeled t  large rectangles with broken lines for efficient object tracking and state estimation in the 1-d approach 
   two types of feature extraction algorithms are used: oriented edge features extracted by ternary mask correlations in horizontal or vertical search paths  a rather old component   and area-based segmentations of 'stripes' of certain widths  arbitrarily oriented in the image plane  a new one . 
   the intelligent control of the parameters of these algorithms is essential for efficient tracking. in the 1-d approach  these parameters are set by predictions from the 
dickmanns 

spatio-temporal representations and application of perspective mapping. from fig.1 it may be seen that a small percentage of image data properly analysed allows to track objects reliably and precisely when used in a tight bottomup and top-down loop traversed frequently  1 hz ; this has to be seen in the context of figure 1. 
1 	state estimation 
the basic approach has been described many times  see 
 wuensche  1; dickmanns  1; dickmanns  1; behringer  1; thomanek  1   and has remained the same for visual relative state estimation over years by now. however  in order to be able to better deal with the general case of scene recognition under  more strongly  perturbed ego-motion  an menially based component has been added  werner et al.  1; werner  1 . 
   this type of state estimation is not new at all if compared to inertial navigation  e.g. for missiles; however  here only very inexpensive accelerometers and angular rate sensors are being used. this is acceptable only because the resulting drift problems are handled by a visual state estimation loop running in parallel  thereby resembling the combined use of  relatively poor  inertial signals from the vestibular apparatus and of visual signals in vertebrate perception. some of these inertial signals may also be used for stabilizing the viewing direction with respect to the stationary environment by direct negative feedback of angular rates to the pointing device carrying the cameras. this feedback actually runs at very high rates in our systems  1 hz  see  schiehlen  1  . 
inertially based ego-state estimation  ibse  the advantage of this new component is three-fold: 1 because of the direct encoding of accelerations along  and rotational speed components around body fixed axes  time delays are negligeable. these components can be integrated numerically to yield predictions of positions. 1. the quantities measured correspond to the forces and moments actually exerted on the vehicle including the effects of perturbations; therefore  they are more valuable than predictions from a theoretical model disregarding perturbations which are unknown  in general. 1. if good models for the eigenbehavior are available  the inertial measurements allow to estimate parameters in perturbation models  thereby leading to deeper understanding of environmental effects. 
dynamic vision 
with respect to ego-state recognition  vision now has reduced but still essential functionality. it has to stabilize longterm interpretation relative to the stationary environment  and it has to yield information on the environment  like position and orientation relative to the road and road curvature in vehicle guidance  not measurable inertially. with respect to other vehicles or obstacles  the vision task also is slightly alleviated since the high-frequency viewing 
invited speakers 
direction component is known now; this reduces search range required for feature extraction and leads to higher efficiency of the overall system. 
   these effects can only be achieved using spatiotemporal models and perspective mapping  since these items link inertial measurements to features in the image plane. with different measurement models for all the cameras used  a single object model and its recursive iteration loop may be fed with image data from all cameras relevant. jacobian matrices now exist for each object/sensor pair. 
   the nonlinear measurement equation  1  is linearized around the predicted nominal state xn and the nominal parameter set pn yielding  without the noise term  
 1  
are the jacobian ma-
trices with respect to the state components and the parameters involved. since the first terms to the right hand side of the equality sign are equal by definition  eq.  1  may be used to determine  in a least squares sense from as the prediction error messured  observability given ; 
this is the core of recursive estimation. 
1 	situation assessment 
for each object an estimation loop is set up yielding best estimates for the relative state to the ego-vehicle including all spatial velocity components. for stationary landmarks  the velocity is the negative of ego-speed  of course. since this is known reliably from conventional measurements  the distance to the landmark can be determined even with monocular vision exploiting motion stereo  hock  1; thomanek  1;muller  1 . 
   with all this information available for the surrounding environment and the most essential objects in it  an interpretation process can evaluate the situation in a task context and come up with a conclusion whether to proceed with the behavioral mode running or to switch to a different mode. fast in-advance simulations exploiting dynamical models and alternative stereotypical control inputs yield possible alternatives for the near-term evolution of the situation. by comparing the options or by resorting to precomputed and stored results  these decisions are made. 
1 	generation of behavioral capabilities 
dynamic vision is geared to closed-loop behavior in a task context; the types of behavior of relevance  of course  depend on the special task domain. the general aspect is that behaviors are generated by control output. there are two basically different types of control generation: 
1. triggering the activation of  generically  stored time histories  so-called feed-forward control  by events actually observed  and 

1. gearing actual control to the difference between desired and actual state of relevant systems  so-called feedback control. 
in both cases  actual control parameters may depend on the situation given. a very general method is to combine the two given above  as a third case in the list   which is especially easy in the 1-d approach where dynamical models are already available for the part of motion understanding. 
the general feed-forward control law in generic form is 
where pm may contain averaged state components  like speed . 
   a typical feed-forward control element is the steer control output for lane change: in a generic formulation  for example  the steer rate is set in five phases during the maneuver time the first and the final control phase of duration each  consist of a constant steer rate  say r. in the second and fourth phase of same duration  the amplitude is of opposite sign to the first and last one. in the third phase the steer rate is zero; it may be missing at all  duration zero . the parameters r  have to be selected such that at the lateral offset is just one lane width with vehicle heading the same as before; these parameters  of course  depend on the speed driven. 
   given this idealized control law  the corresponding state component time histories can be computed according to a good dynamical model; the additional time period  at the end is added 
because in real dynamical maneuvers the transition is not completed at the time when the control input ends. in order to counteract disturbances during the maneuver  the differe n c e
모모 may be used in a superimposed state feedback controller to force the real trajectory towards the ideal one. 
the general state feedback control law is 
with k being the r by n gain matrix. the gain coefficients may be set by pole placement or by a riccati design  optimal linear quadratic controller  well known in control engineering  kailath  1 . both methods include knowledge about behavioral characteristics along the time axis: while pole placement specifies the eigenvalues of the closed loop system  the riccati design minimizes weighted integrals of state errors and control inputs. 
   the simultaneous use of dynamical models for both perception and control and for the evaluation process leading to behavior decision makes this approach so efficient. figure 1 shows the closed-loop interactions in the overall system. 
   based on object state estimation  lower left corner  events arc detected  center left  and the overall situation is assessed  upper left . initially  the upper level has to decide 


figure 1: knowledge based real-time control system with three hierarchical levels and time-horizons. 
	dickmanns 	1 

which of the behavioral capabilities available are to be used: feed-forward  feedback  or a superposition of both; lateron  the feedback loops activated are running continuously  lower part in fig. 1 with horizontal texture  without intervention from the upper levels  except for mode changes. certain events also may trigger feed-forward control outputs directly  center right . 
모모since the actual trajectory evolving from this control input may be different from the nominal one expected due to unforseeable perturbations  commanded state time histories are generated in the block 'state prediction'  center of 
fig. 1  upper right central part  and used as reference values for the feedback loop  arrow from top at lower center . in this way  combining feed-forward direct control and actual error feedback  the system will realize the commanded behavior as close as possible and deal with perturbations without the need for replanning on the higher levels. 
   all  that is needed for mission performance of any specific system then is a sufficiently rich set of feed-forward and feedback behavioral capabilities. these have to be activated in the right sequence such that the goals are achieved in the end. for this purpose  the effect of each behavioral capability has to be represented on the upper decision level by global descriptions of their effects: 
1. for feed-forward behaviors with corrective feedback superimposed  case 1 given above  it is sufficient to just represent initial and final conditions including time needed; note that this is a quasi-static description as used in al-methods. this level does not have to worry about real-time dynamics  being taken care off by the lower levels. it just has to know in which situations 
these behavioral capabilities 
gun. the newly available computing power will lead to quick progress on this mission level  now that the general concept has been defined. 
1 	multiple loops in dynamic scene understanding 
the principles discussed above have lead to parallel realizations of multiple loops in the interpretation process both in space and in time; figure 1 has displayed the spatial aspects. in the upper half of the figure  the essential scales for feedback loops are the object level  the local situation level  and the global mission performance level on which behavior decisions for achieving mission goals are being done  see table 1 also . 
   these decisions may be based on both local and extended predictions of the actual situation and on knowledge about behavioral capabilities of the own vehicle and of other subjects in the scene. the multiple loops used in our system in the time domain are displayed in figure 1; they range from the millisecond scale for inertial viewing direction control to several hours for ground and flight vehicles on the mission scale encompassing sequences of maneuvers and feedback behavioral modes. 
   the outermost two loops labeled 'quasi-static' are closed  up to now  mainly by human operators and software developers. they are being tackled now for automation on the system structure developed; it is felt that a unified approach encompassing systems dynamics  control engineering  computer simulation and animation techniques as well as methods from ai has become feasible. 

may be activated with which parameter set. 
1. for feedback behaviors it is sufficient to know when this mode may be used; these reflex-like fast reactions may run over unlimited periods of time if not interrupted by some special event. a typical example is lane following in road vehicle guidance; the integral of speed then is the distance traveled  irrespective of the curvatures of the road. these values are given in information systems for planning  like maps or tables  and can be used for checking mission progress on the upper level. 
performing more complex missions on this basis has just be-
invited speakers 

figure 1: multiple feedback loops on different time scales in  visual  cognition systems and corresponding representational levels 

1 experimental results 
1 	road vehicles 
the autonomous road vehicle vamp  see figure 1  and its twin vita ii of daimler-benz have shown remarkable performance in normal freeway traffic in france  germany and denmark since 1. vamp has two pairs of bifocal camera sets of focal lengths 1 and 1 mm; one looks to the front  the other one to the rear. with 1 by 1 pixels per image this is sufficient for observing road and traffic up to about 1m in front of and behind the vehicle. with its 1 transputers for image processing it has been able in 1 to recognize road curvature  lane width  number of lanes  type of lane markings  its own position and attitude relative to the lane and to the driveway  and the relative state of up to ten other vehicles including their velocity components  five in each hemisphere. at the final demonstration of the eureka-project prometheus near paris  vamp has demonstrated its capabilities of free lane driving and convoy driving at speeds up to 1 km/h in normally dense threelane traffic  dickmanns et al.  1   lane changing for passing and even the decision whether lane changes were safely possible have been done autonomously  kujawski  1 . the human safety pilot just had to check the validity of the decision and to give a go-ahead input. 

figure 1: the autonomous vehicle vamp of ubm 
   in the meantime  transputers had been replaced by powerpcs mpc 1 with an order of magnitude more computing power. a long range trip over about 1 km to a project meeting in odense  denmark in 1 has been performed in which about 1% of the distance could be traveled fully automatically  in both longitudinal and lateral degrees of freedom. maximum speed on a free stretch in the northern german plain was 1 km/h. 
   since only black-and-white video signals have been evaluated with edge feature extraction algorithms  construction sites with yellow markings on top of the white ones could not be handled; also  passing vehicles cutting into the own lane very near by posed problems because they could not be picked up early enough due to lack of simultaneous field of view  and because monocular range estimation took too long to converge to a stable interpretation. for these reasons  the system is now being improved with a wide field of view from two divergently oriented wide angle cameras with a central region of overlap for stereo interpretation; additionally  a high resolution  1-chip  color camera also covers the central part of the stereo field-ofview. this allows for trinocular stereo and area-based object recognition. 
   dual-pentiumpro processors now provide the processing power for tens of thousands of mask evaluations with cronos per video cycle and processor. 
   vamors  the 1-ton van in operation since 1 which has demonstrated quite a few 'firsts' in autonomous road driving  has seen the sequence of microprocessors from intel 1  1  via transputers and powerpcs back to general purpose intel pentium and pentiumpro. in addition to early high-speed driving on freeways  dickmanns and zapp  1  it has demonstrated its capability of driving on state and on minor unsealed roads at speeds up to 1 km/h  1 ; it is able to recognize hilly terrain and to estimate vertical road curvature in addition to the horizontal one  dickmanns and mysliwetz  1 . 
   recognizing cross-roads of unknown width and angular orientation has been demonstrated as well as turning off onto these roads  even with tight curves requiring an initial maneuver to the opposite direction of the curve  muller  1; dickmanns and muller  1 . these capabilities will also be considerably improved by the new camera arrangement with a wide simultaneous field of view and area based color image processing. 
   performing entire missions based on digital maps has been started  hock  1  and is alleviated now by a gpsreceiver in combination with inertial state estimation recently introduced  muller  1; werner  1 . the vehicles vamors and vamp together have accumulated a record of about 1 km in fully autonomous driving on many types of roadways. 
1 	air vehicles 
after the feasibility of autonomous control in all six degrees of freedom by dynamic machine vision had been demonstrated for the case of straight-in  unperturbed landing approaches in hardware-in-the-loop simulations  eberl  1   a second effort including inertial sensing and both wind and gust disturbances led to first flight tests in 1  schell  1 . because of the safety regulations  the autonomous vision system was not allowed to control the aircraft  a twin turbo-prop of about 1-ton weight  near the ground; the human pilot did the flying but the vision system determined all 1 state components relative to the runway for distances below 1m from runway threshold. 
   the next step was to introduce bifocal vision with a mild and a stronger tele lens in connection with the new transputer system in the early 1ies; 1  in another set of 
	dickmanns 	1 

flight experiments with the same aircraft of the university of brunswick it was proved that visual range could be doubled  essentially  but more computing power would be needed for robust tracking and initialization. the powerpc satisfied these requirements; it is now possible to detect large obstacles on the runway sufficiently early for safe reactions  furst et al.  1 . 
   the most demanding guidance and control task performed up to now in hardware-in-the-loop real-time simulations is helicopter flight near the ground including landmark navigation. the capability of performing a small scale mission starting at one end of the airport of brunswick  flying along a selected sequence of waypoints on the airport and in the vicinity  road forks   returning to the airport from the other side and slowing down for landing at a helicopter 'h' at the other end has been demonstrated  werner et al.  1; werner  1   see figure 1 . 
   in connection with this demanding task  a complete software package has been developed containing separate inertial and visual state estimation components  integration of gps signals and data fusion in the context of mission performance. in addition  provisions have been made to integrate coarse-scale image data from a synthetic aperture imaging radar system under development elsewhere. the combined use of all-weather radar images and highresolution optical or infrared images is considered an optimal solution for future helicopter guidance systems. the capability of interpreting these data streams by an intelligent on-board computer system will unload the pilot from a very difficult task in a situation where he is stressed to the limit already. 
1 	technical beings'  
there is an ongoing discussion as to what technical beings may be like and what the best architecture for realizing these agents might be  brooks and flynn  1; steels  1 ; subsumption architeaure and neural nets have been proposed as roads leading to these type of creatures. 
   looking at the results achieved with the 1-d approach to dynamic vision  it does not seem unreasonable to expect that quite a few problems to be encountered in complex scenarios with the other approaches may be avoided taking this route which builds upon long term results in the natural sciences and engineering. 

1a: tracking of ' crossing 1' 

1b: tracking of taxiways  frame and heli h during final approach 
figure 1: landmark navigation for helicopters has been demonstrated in hardware-in-theloop  real-time simulations for a small mission near the airport of brunswick 
invited speakers    it has the advantage of having a clear notion of space and time  of objects  subjects and processes  and of the spatio-temporal representational structure necessary to handle multiple independent objects and subjects with own intentions  goals and control capabilities. 

   in  d.dickmanns  1  a corresponding representational framework has been given which allows to handle even complex systems with minimal additional effort on the methodical side; knowledge specific to the task domain may be entered through corresponding data structures. computing power available in the near future will be sufficient to solve rather complex real-time  real-world problems. a corresponding architecture for road vehicle guidance is discussed in  maurer and dickmanns  1 . 
   as compared to the other approaches pursued  the pledge is to take advantage of the state of the art in engineering and simulation technology; introducing goal functions for these autonomous systems and providing them with background knowledge of how to achieve these goals  or how to learn to achieve them will be essential. the argument sometimes heard that these systems will be 'closed' as opposed to neural-net-based ones is not intelligible from this point of view. 
1 conclusions 
the 1-d approach to dynamic machine vision developed along the lines layed out by cybernetics and conventional engineering long time ago does seem to satisfy all the expectations it shares with 'artificial intelligence'- and 'neural net'-approaches. complex perception and control processes like ground vehicle guidance under diverse conditions and in rather complex scenes have been demonstrated as well as maneuver- and mission-control in full six degrees of freedom. the representational tools of computer graphics and -simulation have been complemented for dealing with the inverse problem of computer vision. 
   computing power is arriving now for handling realword problems in real-time. lack of robustness encountered up to now due to black-and-white as-well-as edge-based image understanding can now be complemented by areabased representations including color and texture  both very demanding with respect to processing power. 
   taking advantage of well suited methods in competing approaches and combining the best of every field in a unified overall approach will be the most promising way to go. the good old stuff should not be discarded too early. 
literature 
 behringer  1  r. behringer: visuelle erkennung und interpretation des fahrspurverlaufes durch rechnersehen fur ein autonomes strabenfahrzeug. phd thesis  unibwm  lrt  1. 
 brooks and ftynn 1  r.a. brooks and a.m. flynn: robot beings. ieee/rsj international workshop on intelligent robots and systems  tsukuba  japan  sept. 
1  pp 1. 
pecarlo and metaxas  1  d. decarlo and d. metaxas: 
the integration of optical flow and deformable models with applications to human face shape and motion estimation. ieee computer society conference on computer vision and pattern recognition  san francisco  ca  june 1  pp 1. 
 d.dickmanns  1  dirk dickmanns: rahmensystem fur visuelle wahrnehmung veriinderlicher szenen durch computer. phd thesis  unibwm  inf  1. 
 dickmanns  1  ed. dickmanns: 1-d-dynamic scene analysis with integral spatio-temporal models. 1th int. symposium on robotics research  santa cruz  1. 
 dickmanns and zapp  1  e.d. dickmanns and a. zapp: autonomous high speed road vehicle guidance by computer vision. 1th ifac world congress munich  preprint vol. 1  1  pp 1. 
 dickmanns and graefe 1  e.d. dickmanns  v. graefe: 
a  dynamic monocular machine vision. machine vision and applications  springer international  vol. 1  1  pp 1. b  applications of dynamic monocular machine vision   ibid   1  pp 1. 
 dickmanns  1  e.d. dickmanns: machine perception exploiting high-level spatio-temporal models. 
agard lecture series 1 'machine perception'  hampton  va  munich  madrid  sept./oct. 1. 
 dickmanns and mysliwetz  1  e.d. dickmanns and b. mysliwetz: recursive 1-d road and relative ego-state 
recognition. ieee-transactions pami  vol. 1  no. 1  
special issue on 'interpretation of 1-d scenes'  feb 1  pp 1 
 dickmanns et al.  1  e.d. dickmanns  r. behringer  d. dickmanns  t. hildebrandt  m. maurer  f. thomanek  j. schiehlen: the seeing passenger car 'vamors-p'. in intelligent vehicles symposium '1  paris  oct. 1  pp 1. 
 dickmanns  1a  e.d. dickmanns: performance improvements for autonomous road vehicles. int. conference on intelligent autonomous systems  ias-1   karlsruhe  1. 
 dickmanns 1b  e.d. dickmanns: road vehicle eyes for high precision navigation. in linkwitz et al.  eds : high precision navigation. diimmler verlag  bonn  1  pp. 1. 
 dickmanns and muller  1  e.d. dickmanns and n. muller: scene recognition and navigation capabilities for lane changes and turns in vision-based vehicle guidance. control engineering practice  1nd ifac 
conf. on intelligent autonomous vehicles-1  helsinki 1. 
	dickmanns 	1 

 dickmanns  et al.  1  e.d. dickmanns  s. furst  a. schubert  d. dickmanns: intelligently controlled feature extraction in dynamic scenes. technical report unibwm/lrt/we-1/fb/1-l  1. 
 eberl  1  g. eberl: automatischer landeanflug durch rechnersehen. phd thesis  unibwm  lrt  1. 
 fritz  1  h. fritz: model-based neural distance control for autonomous road vehicles. proc. intelligent vehicles '1 symposium  tokyo  1  pp 1. 
 ftirst et al.  1  s. ftirst  s. werner  d. dickmanns  and e.d. dickmanns: landmark navigation and autonomous landing approach with obstacle detection for aircraft. aerosense '1  conference 1  orlando fl  april 1  1. 
 hock  1  c. hock: wissensbasierte fahrzeugfuhrung mit landmarken fur autonome roboter. phd thesis  unibwm  lrt  1. 
 kailath  1  t. kailath: linear systems. prentice-hall  inc.  englewood cliffs  n.j.  1. 
 klass  1  p.j. klass: darpa envisions new generation of machine intelligence. aviation week & space technology  april 1  pp 1. 
 kujawski  1  c. kujawski: deciding the behaviour of an autonomous mobile road vehicle. 1nd ifac conference on intelligent autonomous vehicles  helsinki  june 1. 
 luenberger  1  d.g. luenberger: observing the state of a linear system. ieee trans on mil electronics 1  1  pp 1. 
 maurer and dickmanns  1  m. maurer and e.d. dickmanns: an advanced control architecture for autonomous vehicles. aerosense '1  conference 1  orlando fl  april 1  1. 
 mecklenburg et al.  1  k. mecklenburg  t. hrycej  u. franke and h. fritz: neural control of autonomous 
vehicles. proc. ieee vehicular technology conference '1  denver  1. 
 meissner  1  h.g. meissner: steuerung dynamischer systeme aufgrund bildhafter informationen. phd thesis  unibwm  lrt  1. 
 meissner and dickmanns  1  h.g. meissner and e.d. dickmanns: control of an unstable plant by computer vision. in t.s. huang  ed : image sequence processing and dynamic scene analysis. springer-verlag  berlin  1  pp 1. 
 muller  1  muller n.: autonomes manovrieren und navigieren mit einem sehenden strabenfahrzeug. phd thesis  unibwm  lrt  1. 
 pomerleau  1  d.a. pomerleau: alvinn: an autonomous land vehicle in neural network. in d.s. 
invited speakers 
touretzky  ed.  advances in neural information processing systems 1. morgan kaufmann  1. 
 pomerleau 1  d.a. pomerleau: neural network perception for mobile robot guidance. phd thesis  cmu  cmu-cs-1   febr 1. 
 schmid  1  m. schmid: 1-d-erkennung von fahrzeugen in echtzeit aus monokularen bildfolgen. phd thesis  unibwm  lrt  1. 
 schick and dickmanns 1  j. schick and e.d. dickmanns: simultaneous estimation of 1-d shape and motion of objects by computer vision. ieee workshop on visual motion  princeton  n.j.  1. 
 schell  1  f.r. schell: bordautonomer automatischer landeanflug aufgrund bildhafter und inertialer mebdatenauswertung. phd thesis  unibwm  lrt  1. 
 schiehlen  1  j. schiehlen: kameraplattformen fur aktiv sehende fahrzeuge. phd thesis  unibwm  lrt  1. 
 steels  1  l. steels: the biology and technology of intelligent autonomous agents. nato-advanced 
study institute  lvano  italy  march 1  1. 
 thomanek  1  f. thomanek f.: visuelle erkennung und zustandsschatzung von mehreren strabenfahrzeugen zur autonomen fahrzeugfuhrung. phd thesis  unibwm  lrt  1. 
 tsugawa et al  1  s. tsugawa  t. yatabe  t. hirose  s. matsumoto: an automobile with artificial intelligence. proceedings 1th ijcai  tokyo  1  pp 1. 
 ulmer  1  b. ulmer: vita ii - active collision avoidance in real traffic. in intelligent vehicles symposium '1  paris  oct. 1. 
 werner et al.  1  s. werner  s. ftirst  d. dickmanns  and e.d. dickmanns: a vision-based multi-sensor machine perception system for autonomous aircraft landing approach. enhanced and synthetic vision  aerosense '1  orlando  fl  april 1. 
 werner  1  s. werner: maschinelle wahrnehmung fur den bordautonomen automatischen hubschauberflug. phd thesis  unibwm  lrt  1. 
 wuensche  1  h.-j. wuensche: verbesserte regelung eines dynamischen systems durch auswertung redundanter sichtinformation unter berucksichtigung der einflusse verschiedener zustandsschatzer und abtastzeiten. report hsbw/lrt/we 1a/ib/1  1. 
 wuensche  1  h.-j. wuensche: detection and control of mobile robot motion by real-time computer vision. in n. marquino  ed : advances in intelligent robotics systems. proceedings of the spie  vol. 1  1  pp 1. 






1 

1 

1 

1 

1 

1 

1 

1 

1 

1 

1 

1 

1 

1 

1 







1 

1 

1 







1 



1 

1 

1 

1 

