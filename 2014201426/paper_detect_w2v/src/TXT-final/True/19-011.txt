 
this paper examines lisp from the point of view of parallel computation. it attempts to identify exactly where the potential for parallel execution really exists in lisp and what constructs are useful in realizing that potential. case studies of three attempts at augmenting lisp with parallel constructs are examined and critiqued. 
1. parallelism in lisp 
there are two main approaches to executing lisp in parallel. 
one is to use existing code and clever compiling methods to parallelize the execution of the code  1  1  1 . this approach is very attractive because it allows the use of already existing code without modification  and it relieves the programmer from the significant conceptual overhead of parallelism. this approach  known as the  dusty deck  approach  suffers from a simple problem: it is very hard to do. this is particularly true in a language such as lisp that shows a much less well defined flow of control than languages such as fortran where such techniques have been applied relatively successfully . a result of this is that  given the current compiler techniques  the amount of parallelism that can be achieved is limited. 
the other approach to the problem is the addition of so-called parallel constructs to lisp. the idea is to allow the programmer to help the compiler out  by specifying the parallelism using special  added language constructs. this  depending on the constructs used  places a significant additional conceptual burden on the programmer. the degree of the burden depends directly on the level or the elegance and simplicity of the constructs used. the higher the level of the constructs  the lighter conceptual burden on the programmer. to put it another way  the more the constructs are able to hide and protect the programmer from the problems inherent in the parallel execution of a language with side effects such as lisp  the better. this approach suffers from the additional problem that existing code may not be executed as is  but rather must be rewritten using these added constructs in order to take advantage of any parallelism. finally  there is the problem of defining a set of constructs that fulfills the goals of placing the minimal conceptual overhead on the programmer while providing a complete set  in the sense that all the parallelism in any given problem may be suitably expressed using only this set. 
in this paper  we focus on the second of the two approaches  because it is in this area that most recent progress has been made. we study in depth three attempts to define a useful set of parallel constructs for lisp and discuss exactly where the opportunities for parallelism in lisp really seem to lie. the three attempts are very interesting  in that two arc very similar in their approach but very different in the level of their constructs  and the third takes a very different approach. we do not study the so called  pure lisp  approaches to parallelizing lisp since these are applicative approaches and do not present many of the more complex problems presented by a lisp with side-effects  1  1 . 
the first two attempts concentrate on what we call control parallelism. control parallelism is viewed here as a mediumor course-grained parallelism on the order of a function call in 
lisp or a procedure call in a traditional  procedure-oriented language. a good example of this type of parallelism is the parallel evaluation of all the arguments to a function in lisp  or the remote procedure call or fork of a process in some procedural language. notice that within each parallel block of computation  there may be encapsulated a significant amount of sequential computation. 
the third attempt exploits what we call data parallelism. data parallelism corresponds closely to the vector parallelism in numerical programming. the basic idea is that  given a large set of data objects on which to apply a given function  that function may be applied to all the data objects in parallel as long as there are no dependencies between the data. here  rather than distributing a number of tasks or function calls between a set of processors  one distributes a set of data and then invokes the same function in all processors. 
these two forms of parallelism have been described as the 
mimd  multiple instruction stream  multiple data stream  and smd  single instruction stream  multiple data stream  approaches   these terms are generally used in classifying parallel architectures based on the type of computation for which they are particularly suited. generally  it is felt that the finer the grain of an architecture  i.e. the simpler and the more numerous the processors  the more simd in nature  and conversely  the larger the grain  i.e. the larger and fewer the processors  the more mimd the architecture. the terms are also frequently used to distinguish between distributed- and shared-memory machines  but it is important to remember that they actually refer to particular models of computation rather than any given particular architectural characteristics. 
the case studies described in this paper deal exclusively with one or the other of these two forms of parallelism. interestingly  the compiler  or dusty deck approaches that we have seen  1  1   also seem to deal exclusively with one or the other of the two forms of parallelism. some work has been done in the functional programming area in combining the two forms of parallelism  and the need to do so has been recognized by parallel lisp designers   but to date very little work has been done in this area. this is surprising given that the distinction between the two forms is  in reality  quite weak. this is especially true in a language such as lisp  where there is a continuum between what might be considered code and what might be considered data. to see this more clearly  let us take the case of the parallel evaluation of a function's arguments. we have seen this is generally considered a form of control parallelism. assuming each argument involves the evaluation of a function  for example: 
 foo  bar a   car b   dar c   
we generally view this as each argument being bundled up with its environment and sent off to some processor to be evaluated. what about the case where the same function is applied to each argument  
 foo  f a   f b   f c   
this is generally viewed as an occasion for exploiting data parallelism. the arguments  the data in this case  are distributed to a number of processors and then the common function is called on each. in the case of moving from a data to a control point of view  consider a vector  each element of which resides in its own processor. invoking a given function on each member of the vector involves first distributing one member of the vector to each processor  this step is often ignored in descriptions of simd execution   and then  broadcasting the code for that function to each processor  or if the code is already stored in the processor  broadcasting the function pointer and a command to begin execution. in the control model  instead of broadcasting the function to all the processors at one time  one must distribute the function along with a particular element of the vector to each processor and immediately begin execution. this could just as well be viewed as first distributing the code and the corresponding data element to each processor and then broadcasting the instruction to eval that form in each processor. the synchronization is different in the two models  but the actual steps in the execution may be viewed as being the same. 
1. case studies 
we will return to the subject of the distinction between control and data parallelism later  when we discuss where the opportunities for parallelism in lisp actually lie. first  we present three case studies of the approaches already mentioned. the current efforts concentrate on adding some additional constructs to a dialect of lisp. they are  therefore  extensions to lisp rather than completely new parallel languages based on lisp. in the text we refer to them as languages  but the meaning should be taken as: lisp and the additional constructs used to express parallelism. in the case studies new constructs are indicated by italics and normal lisp constructs are indicated by a typewriter font 
1. multilisp 
the first extended lisp language we present is multilisp 
  1  1  which is based on scheme   a dialect of lisp which treats functions as first class objects  unlike common lisp   but is lexically scoped  like common lisp. in this paper  we do not distinguish lisp based languages by the particular dialect of lisp on which they are based  but rather by the constructs that have been added to the language and the effect  if any  that the base dialect has on these constructs. 
multilisp is notable both for the elegance and the economy of the constructs through which it introduces parallelism. in fact  a single construct  the future  is used for the expression of all parallelism in the language. a future is basically a promise or an iou for a particular computation. in creating a future  the programmer is implicitly stating two things: one is that it is okay to proceed with the computation before the value of the future is calculated  but that the value will have been calculated or will be calculated at the time it is needed. the other is that the computation of the future is independent of all other computation occurring between the time of creating and before its use  and thus may be carried out at any time  in no particular order  with the rest of the computation or other futures that may have been created. the danger here  of course  is any side effects caused by the future must not depend on their ordering in relation to the rest of these computations. it is the responsibility of the programmer to ensure that these side effects do not cause any  unfortunate interactions . it is this responsibility that places additional conceptual overhead on the programmer. the overhead is reduced by having only one basic construct to deal with  but should not be underestimated. this overhead may be further 
reduced by what halstead describes as a data abstraction and modular programming discipline. an example of the use of a 
future is: 
 setq cc  cons  future  foo a   
 future  bar b     
here we build a cons cell  both the car and cdr of which are futures  representing respectively the future or promised evaluation of  foo a  and  bar b . this cons will return immediately and be assigned to the atom cc. if we later pass cc to the function printcons below  
 defun printcons  conscell  
 print  car conscell   
 print  cdr conscell    
there are four possibilities: 
1. both futures will have already been evaluated  in which case the futures will have been coerced into their actual values and the computation will proceed just as if it was dealing with a regular cons cell  
1.  foo a  has not yet been evaluated in which case printcons will have to wait for it to be evaluated before proceeding. 
1.  bar b has not yet been evaluated in which case printcons will have to wait for it to be evaluated before proceeding. 
1. both  foo a  and  bar b  have not yet been evaluated in which case printcons must wait for the evaluation of the futures before continuing. 
multilisp has an additional construct known as a delay or delayed future that corresponds to a regular future  except that the evaluation of the future is delayed until the value is required. this additional construct is necessary in order to represent infinite data structures and nonterminating computation. for example  suppose we wished to represent the fist of all prime numbers greater than some prime number n. we could do this using a delay as follows: 
 defun primes n  
 cons n  delay  primes  next-prime n      
	van biema 	1 
multilisp allows the computation of non-strict functions  functions whose arguments may not terminate  through the use of both the delay and of the future. however  in the case of futures  significant amounts of resources may be lost in every such computation  and ultimately storage will be exhausted for any non-finite data structure. by using delays  one only computes what is needed. aw futures may be removed from a program without changing the resources used by the program  but the same is not true for delays  since removing a delay may cause the computation of infinite data structures. 
delays thus represent a form of lazy evaluation  whereas the future represents a reduction in the ordering constraints of a computation. eager evaluation strategies are also possible  in which computations are begun that are not necessarily needed at all  but are computed anyway  just in case they are needed. multilisp does not provide such eager constructs or any construct that allows a computation to be halted prematurely. constructs  which allow computations to begin  but are later able to halt them  are useful in freeing the computational resources of an eager computation  once it has been determined that its result is not needed. an alternate technique is to allow such a process to run until it can be determined mat the result being returned is no longer needed at which time it may be garbage collected by the system. 
multilisp includes one additional construct which is the pcall. 
pcall provides for the simultaneous evaluation of the arguments to a function  but does not continue the evaluation of the function itself until all the arguments have fmished evaluating. notice how this differs from a function call in which all the arguments are futures. pcall thus provides a much more limited form of parallelism than futures  but is useful a midway point between the completely undefined flow of control between futures and the complete order of sequential execution. pcall may of course be simulated by a function call  all of whose arguments are futures  provided the first act of the function is to access the values of all its arguments. multilisp provides a primitive identity operator touch which causes a future to be evaluated and which is in fact used to implement the pcall construct. 
the implementation of multilisp calls for a shared-memory multiprocessor  and two implementations are underway 
  1 . each processor maintains its own queue of pending futures  and a processor that has no current task may access another processor pending queue to find a future to execute. an unfair scheduling algorithm is necessary to ensure that constant computational progress is made and the system does not deadlock. the scheduling strategy has been chosen so that a saturated system behaves like a group of processors executing sequentially  i.e. as if all future calls had been removed from the code. once a future has been evaluated  it is coerced to its return value by changing a flag bit stored among the tag bits of the lisp item that represents it. this causes one extra level of indirection  one pointer traversal  in references to values that are the result of future calculations. these indirect references are removed by the garbage collector. 
1. qlisp 
the additional constructs in qlisp  are quite similar in semantics to those of multilisp  but very different in form. there are a much larger number of constructs in qlisp  although the increase in the expressive power of the language is not great. there are two primary constructs that allow the expression of parallelism. they are qlet and qlambda. 
qlet does much what one might expect it performs the bindings of a let in parallel. qlet takes an extra predicate as an argument along with its usual binding pairs. when this extra predicate evaluates to n i l   qlet behaves exactly like let. when the predicate evaluates to the atom eager  the qlet spawns a process to evaluate each of its arguments and continues the execution of the following computation. finally  if the predicate evaluates to neither n i l nor eager the qlet spawns processes for each of its arguments as before  but waits for all of them to finish evaluating before continuing with the subsequent computation. these last semantics for qlet closely resemble those of pcall in multilisp and may be easily used to mimic its semantics exactly  by placing the function call within the qlet that assigns the value of the functions arguments to temporaries . further  a qlet where the predicate evaluates to eager may be used to simulate a function call where all the arguments were passed as futures: 
 qlet 'eager 
  x  foo a    y  bar b    z  car c    
 f x y 1   
qlambda takes the same additional predicate as qlet and forms a closure in the same way as its namesake lambda. if the predicate evaluates to n i l   then qlambda behaves exactly as a lambda does. if the predicate evaluates to eager  the process representing the closure is spawned as soon as the closure is created. if the predicate evaluates to something other than n i l or eager  the closure is run as a separate process when it is applied. when a process closure defined by a qlambda is applied in a non value requiring position  such as in the middle rather than at the end of a prog  it is spawned  and the subsequent computation continues. upon return of the spawned process  its return value is discarded. if a process closure is spawned in a value requiring position the spawning process waits for the return value. in addition  two operators are supplied to alter this behavior. they are wait and no-wait with the obvious semantics. the constructs that have been defined up to this point give the language the same semantic power as multilisp's/wrwre mechanism. 
qlisp deals with an issue not handled in multilisp  which is what happens if a spawned process throws itself out  that is  what happens if a spawned process throws to a catch outside its scope  when a catch returns a value in qlisp  all the processes that were spawned in the scope that catch are immediately killed. the additional construct qcatch behaves slightly differently. if the qcatch returns normally  i.e. it is not thrown to   it waits for all the processes spawned below it to complete before returning its value. only if it is thrown to does it kill its subprocesses. in addition  qlisp defines the semantics of an unwind-protect form over spawned processes which ensures the evaluation of a cleanup form upon a nonlocal exit these additional constructs allow the programmer the power to begin and later kill processes  and therefore give him the power to perform the type of eager evaluations not available in multilisp. 
qlisp has more of a process-oriented flavor to it than 
multilisp  and  although its constructs have similar power to those of multilisp they appear to be on a much lower level. a similar statement may be made for the c-lisp language . 
in qlisp  processes are scheduled on the least busy processor at the time of their creation. unlike multilisp  more than one process is run on a single processor and processes are time swapped in a round robin fashion. the predicates of qlet and qlambda allow for dynamic tuning of the number of processes created at run-time. this is another feature that multilisp does not have  but one that is also better left to the runtime system rather than the programmer. 
1. connection machine lisp 
connection machine lisp  unlike the previous two languages  introduces parallelism in the form of data rather than control. the basic parallel data structure in connection machine lisp is a xapping  a distributed mapping . a xapping is a set of ordered pairs. the first element of each pair is a domain element of the map  and the second is the corresponding range element. the mapping representing the square of the integers 1  1  1 would be denoted in connection machine lisp as: 
{ 1- 1- 1- 1 } 
if the domain and range elements of all the pairs of the xapping are the same  i.e. an identity map   this is represented as: 
{ 1 1 } - { 1- 1- 1- 1 } 
and is called a xet. finally  a xapping in which all of the domain elements are successive integers is known as a xector and is represented as: 
  john torn andy   = 
{ l- john 1- tom 1- andy } 
connection machine lisp also allows the definition of infinite xappings. there are three ways to define an infinite xapping. constant xapping takes all domain elements  or indices as they are also called in connection machine lisp  and maps them into a constant. this is denoted: 
{ - v   
where v is the value all indices are mapped into. the universal xapping may also be defined. it is written { -  } and maps all lisp objects into themselves. finally  there is the concept of lazy xappings which yield their values only on demand. for example the xapping that maps any number to its squarte root may be defined by: 
{ - sqrt } 
and  xref {. sqrt} 1  would return 1. notice that xappings are a type of common lisp sequence and many of the common sequence operators are available and may be meaningfully applied to them. 
connection machine lisp defines two main operators that can be applied to xappings. the a operator is the apply-to-all elements of a xapping operator. it takes a function and applies it to all of the elements of the xapping in parallel. if the function to be applied is n-ary  it takes n xappings as arguments and is applied in parallel to the n elements of each xapping sharing a common index. if the correct number of arguments to the function are not available  that index is omitted from the result element. for example: 
 acona {a- l b- 1 c- 1 d- 1 e- 1} 
{b- 1 c- 1 e- 1   -  
 b-  1  c-  1  e-  1 } 
notice that the domain of the result is the intersection of the domains of the function and argument xappings. the a operator is the main source of parallelism in common machine lisp. 
the other main operator is the p or reduction operator. it takes a xapping and a binary function and reduces the xapping to a value by applying the binary function to all the elements of the xapping in some order. since the order in which the function is applied to the xapping is undefined  the functions used are generally limited to being associative and commutative. for example: 
¡¡¡¡ p+  i 1}  always returns 1  but 
      p- {1 1}  may return 1 -1  1  1 or -1. a non-commutative or nonassociative function may be useful on occasion however; for example  p applied to the function  lambda  x y  y  will return some arbitrary element of the xapping to which it is applied. this particular function has been found to be so useful in connection machine lisp that it has been made into a regular operator called choice. the p operator has a second form in which it may serve as a generalized communication operator. when the p operator is passed a binary function and two xappings  the semantics are that the operator returns a new xapping whose indices are specified by the value of its first argument and whose values are specified by the values of the second argument. if more than one pair with the same index would appear in the resulting xapping  which is  of course  not allowed   the xector of values or these pairs is combined using the binary function supplied with the p operator. for example: 
 pmax 
'{john- old tom~ young phil- dead 
joe- young al- 1} 
'{john- 1 tom- 1 phil- 1 joe- ll}  -  
{old- 1 young- 1 dead- 1} 
in this example  we are using a database of xappings about people to generate some new knowledge. given a qualitative measure of some people's age  old  young  and dead  and their actual age  we generate a value for the qualitative measures. notice that when two indices collide  the results are combined by max  our heuristic being that it is best to represent an age group by its oldest member  not a terribly good heuristic in the general case . the interprocessor communication aspect of the p operator becomes clearer if one considers that all the information for one person  one index  is stored in one processor. in order to generate our new xapping  we transfer all the information about each age group to a new processor and do the necessary calculation there. in the above example  the information from philip and george is transferred to the processor with label  young  and combined with the max operator there. 
the parallelism in connection machine lisp may be compared with the parallelism of the pcall construct of multilisp. as pointed out by guy steele  the distinction between the two is due to the mimd nature of pcall and the simd nature of the a operator in connection machine lisp. 
to be more specific  although in the pcall all the arguments are evaluated in parallel  their synchronous execution is not assured. in fact  in both multilisp and qlisp the proposed implementations would almost guarantee that the evaluation would occur asynchronously. in connection machine lisp  when a function is applied to a xapping  the function is 
	van biema 	1 
executed synchronously by all processors. in the case of connection machine lisp  the control has been centralized  whereas in the case of multilisp  it is distributed. this centralization of control definitely reduces the conceptual overhead placed on the programmer  as well as reducing the computational overhead by requiring only a single call to eval rather than many calls occurring in different processors. the price for this reduced overhead is that the opportunity for exploiting the control parallelism that exists in many problems is lost steele comments on this  suggesting that some of the lost control parallelism may be reintroduced by allowing the application of xectors of functions. for example  the following construct in connection machine lisp: 
 otfuncall ' sin cos tan   x y z   
is equivalent to the multilisp construct: 
 pcall #'xector  sin x   cos y   tan z   
as of yet  this aspect of the language has not been developed. 
1. discussion 
in the languages  presented above we have seen two different methods of providing parallelism in lisp. in one case  there is a process style of parallelism where code and data are bundled together and sent off to a processor to be executed. in the other  there is a distributed data structure to which sections of code are sent to be executed synchronously. the major question that remains is whether these two methods of exploiting parallelism can be merged in some useful way  or is there a different model that can encompass both methods. before exploring this question further it is interesting to examine lisp itself in order to see where the opportunities for parallelism actually lie. 
the obvious place to apply control parallelism in lisp is in its binding constructs. we have seen examples of this both in the pcall of multilisp and the qlet of qlisp. in addition to this form of parallelism  any function application may be executed in parallel with any other provided that there are no  unfortunate interactions  between them. we have seen this in the futures of multilisp and the qlambda of qlisp. a different approach often taken in traditional block-structured languages is to have a parallel block  or a parallel prog in the case of lisp  in which all function calls may be evaluated in parallel. 
an interesting approach that has been taken along these lines is to treat each function application as a nested transaction and attempt to execute all applications in parallel  redoing the application when a conflict is detected . a hardware version of this is also being investigated . yet another very interesting approach to control parallelism is that of making environments first class objects which may be created and evaluted in in parallel . 
conditionals may also be executed in parallel. in particular  and and or are natural places for expressing eager evaluation. in a parallel and or or  one might imagine all of the arguments being spawned in parallel and then killed as soon as one of them returns with a false or a true value respectively. this  of course  results in very different semantics for these special forms  which must be made clear to the programmer. conditionals and case statements may  of course  also be executed in parallel in an eager fashion. in addition to evaluating their antecedents in parallel  if more parallelism is desired  the evaluation of the consequents may be commenced before the evaluation of the predicates has terminated  and it has been determined which value will actually be used. these eager evaluation methods bring with them a number of problems. computations that used to halt may no longer do so  side effects may have to be undone  and the scheduling mechanism must ensure that some infinite computation does not use up all of the resources. 
in order to understand the potentials for data parallelism in lisp  we must look at both the data structures themselves and the control structures used to traverse them. the data structures that provide the potential for parallelism fall under the type known as sequences in common lisp. they are lists  vectors and sets. sets are implemented as lists in common lisp  but need not be in a parallel implementation. the control structures that operate on these data structures in a way that may be exploited are iterative constructs  mapping constructs and recursion. the parallelism available from sequences and iterative constructs is much the same as the parallelism that has been exploited in numerical processing . the flow of control in lisp  as has already been mentioned  is generally more complex then that in numerical programs  complicating the compile time analysis. mapping functions  on the other hand  are easily parallelized. since a mapping construct applies the function passed to it to every element of a list  it can be modeled after the a construct in connection machine lisp. 
recursion in the simplest case reduces to iteration  tail recursion  and the same comments that were made above apply. notice also that in the general case the structure of recursion is very similar to that of iteration. there is the body of the recursion and the the recursion step  just as in iteration there is the body of the iteration and the iteration step. this similarity is taken advantage of by some compilers . recursion is also frequently used in the same way as the mapping constructs to apply a function to a sequence. what distinguishes recursion is that it may also be applied to more complex data structures  such as tree structured data. in traversing these more complex data structures  the parallelism available is often dependent on the actual structure of the data. for example  much more parallelism is available in a balanced rather than an unbalanced tree . this type of parallelism is generally exploited as control rather than data parallelism  but mere is no reason that this must be so. the only thing that is necessary to enable a data point of view is the distribution of the tree structured data. such distribution of tree structures may  in fact  be accomplished through the use of nested xappings in connection machine lisp. finally  there are some problems that are recursive in nature and do not lend themselves to any iterative or parallel solution; the tower of hanoi is the classic example. 
by examining lisp itself  we have seen exactly where the opportunities for parallelism are and we can judge the extent to which each of the languages studied is successful in allowing its expression. one thing that is immediately clear is that none of the languages allows for the expression of control and data parallelism within a single coherent model of parallel execution. it is quite possible that no single such model exists  but it should be the goal of future efforts to provide at least for the convenient expression of both forms of parallelism within a single language. 
