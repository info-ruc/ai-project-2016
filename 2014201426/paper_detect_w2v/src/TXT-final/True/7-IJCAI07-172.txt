
a long-lived agent continually faces new tasks in its environment. such an agent may be able to use knowledge learned in solving earlier tasks to produce candidate policies for its current task. there may  however  be multiple reasonable policies suggested by prior experience  and the agent must choose between them potentially without any a priori knowledge about their applicability to its current situation. we present an  experts  algorithm for efficiently choosing amongst candidate policies in solving an unknown markov decision process task. we conclude with the results of experiments on two domains in which we generate candidate policies from solutions to related tasks and use our experts algorithm to choose amongst them.
1 introduction
an agent in a sufficiently complex environment will likely face tasks related to those it has already solved. given a good policy for a related task  the agent could determine a reasonable policy for its current task by mapping its current situation to an analogous one in the task it knows about  and then taking the action it would take in that situation. there may  however  be many reasonable ways for the agent to apply its experience to its new situation and  without any knowledge about the new problem  there may be no way to evaluate them a priori.
모in particular  we represent the agent's sequential decision making problem as a markov decision process  mdp . we assume it has learned an optimal policy for one mdp  m  and now faces a new  unknown mdp  m. the agent has a group of candidate policies for m which are generated from mappings from the states in m to those in m along with the agent's policy in m. following terminology used often in supervised learning settings  we can think of these policies as  experts  that advise the agent on what to do. the agent  then  must mediate these experts in order to leverage their knowledge in learning a solution to its new task.
모the agent could simply ignore the expert advice and learn the new task from scratch. ideally  however  the experts would provide significant savings in learning time. therefore we desire an algorithm with a sample complexity dependent on the number of experts  rather than the size of the problem. in order to enforce this restriction  the algorithm we present makes no use of state observations or the actions being taken by the experts  even if this information is available.
모what can we expect fromsuch an agent  let 뷇b denote the  best  expert  the one that has the largest expected asymptotic average reward. one objective could be to have the agent's actual return at every time step be near the asymptotic return of 뷇b. this is clearly unreasonable because even if the agent knew the identity of 뷇b to begin with  any mediator would need time close to the  unknown  mixing time of 뷇b to achieve that return. intuitively  the mixing time of a policy is the amount of time it takes to thereafter guarantee return close to its asymptotic return. thus we need a more reasonable objective. in this paper  we provide a mediator algorithm that  in time polynomial in t  accomplishes an actual return close to the asymptotic return of the best expert that has mixing time at most t. thus  as the mediator is given more time to run it competes favorably with a larger subset of experts.
1 relationship to existing work
the idea of using state mappings to known mdps to generate knowledge about other mdps is not entirely novel. for instance  homomorphisms between mdps have been used to generate abstractions of problems  allowing for compact representations that induce policies in the full problem    . in this work  we do not restrict mappings to any special class  nor do we seek an optimal mapping. rather  we consider that  though an optimal mapping may be difficult  or impossible  to calculate  a set of  reasonable  mappings may be heuristically generated. though there will be no guarantee on the quality of any of these policies  we will use  experts  algorithms to efficiently choose amongst them to find a good  albeit suboptimal  policy quickly.
모there is much work in learning to use the advice of a team of  experts  to perform a task  though traditionally this has been focused on a supervised learning setting   ;  ;   . however  because of its sequential nature  our problem is more clearly related to the multi-armed bandit problem      which has long stood as a canonical example of the  exploration-exploitation  tradeoff in on-line learning. an agent is presented with a slot machine with several arms. each pull of an arm yields some reward  drawn from an unknown  fixed distribution. the agent's goal is to minimize its regret  the difference between the reward it would have gotten by always pulling the best arm and the reward it actually receives . lai and robbins provided an algorithm that  for t pulls  achieves a regret of o logt  as t 뫸     . though the similarity to our setting is clear  these results relied on the fact that each arm has a fixed reward distribution over time  which is not the case when the  arms  are policies on a shared mdp.
모an important generalization of the multi-armed bandit problem removed all statistical assumptions about the sequence of rewards assigned to each arm  allowing an adversary to select the reward distributionof each arm at every time

step    . auer et al. provide a o 뫏t  bound on the regret using their algorithm exp1  even in this adversarial setting. in their analysis  auer et al. assumed the adversary creates an a priori fixed sequence of reward distributions  which is not affected by the actions of the decision maker. since choosing different sequences of experts may result in different dynamics on the underlying mdp  the bounds on exp1 do not apply in our setting. nevertheless  because of the clear similarity of its setting to ours  we will compare our algorithm to exp1 in our empirical work.
모the algorithm we present here is most closely related to a family of algorithms collectively called  exploration exploitation experts methods   eee     . these algorithms select amongst experts in an adversarial setting  in which the environment  chooses  observations bt depending on the agent's past actions a1 a1 ... at 1  giving the agent reward r a t  b t   at each time step. they say an expert e has an achievable 뷉-value 뷃 if there exists a constant c뷉 뫟 1 such that at any step s1 with any possible history hs1 and any number of steps t 
e 
모the eee algorithms achieve a return close to the highest achievable 뷉-value in time polynomial in c뷉.
모our setting is a special case of that considered by de farias and megiddo  as we assume the environment is governed by an mdp  rather than allowing it to depend on the entire history. as such  our algorithm is quite similar to those in the eee family. by considering this special case  however  we can characterize its direct dependence on the mixing times of the policies  rather than on the abstract quantity c뷉. this allows us to formally understand how our algorithm will perform during its entire run-time and also gives us strong intuition for when our algorithm will be most useful.
1 preliminaries
in a markov decision process  mdp  an agent perceives the state of the world  from a finite set s  and decides on an action  from a set a . given that state and action  the world probabilistically transitions to a new state and the agent receives some reward  drawn from a distribution associated with the new state . we will assume that rewards are bounded and non-negative  if the former holds  the latter is easy to obtain by adding the minimum reward to all reward signals .
모a policy 뷇 on an mdp m is a probability distribution over actions  conditioned on state and time. we write 뷇t s a  =
p a|s t   the probability of taking action a in state s at time t. a policy is stationary if. for the remainder  all policies mentioned are assumed to be stationary unless otherwise stated.
모a t-path in m is a sequence p of t states and we will write p뷇m p  to mean the probability of traversing p in m while following policy 뷇. a policy is called ergodic if  as the number of steps approaches infinity  the probability of being in any particular state approaches a fixed limiting value. an mdp m is called unichain if all stationary policies are ergodic. we restrict our attention to unichain mdps.
모let m be an mdp  뷇 be a policy on m  and p be a tpath in m. then the expected undiscounted return along p is
  where ri is the expected return
at state i. the expected undiscounted t-step return from state
i when following policy 뷇 is and the asymptotic expected undiscounted return from state i is um뷇  i  = limt뫸 um뷇  i t . note that for an ergodic policy is independent of i and so we will write
for the asymptotic return. hereafter  we will drop the explicit dependence on the unknown but fixed mdp m.
모in our problem setting  an agent is acting on an unknown mdp. it is provided with a set e of stationary policies  or  experts.  at each step  the agent must choose one of the experts to act on its behalf. it does not perceive the current state or the action taken  but it does receive the reward signal. the goal of the agent is to achieve an undiscounted return close to the expected asymptotic return of the best expert in an efficient amount of time.
모the central problem is that the agent does not know the experts' mixing times. it can never be sure that following an expert for any finite number of steps will provide it with a good estimate of that expert's asymptotic quality. we now present our algorithm  which explicitly addresses this issue.
1 atease: a policy-mediating algorithm
arguments: of experts
initialize: t 뫹 1trusting exploration phase
1. run each expert for texp  polynomial of t  steps  recording their texp-step returns 1. set
1. sort the experts in e by their texp-step returns  u .
suspicious exploitation phase
	then set t 뫹 t + 1 and goto 1.	
1. let e be the expert with the highest texp-step return in e .
1. run e for a batch of texp steps.
1. if return of e for the batch is less than u e  then remove e from e and goto 1.
1. if e has run for ltexp steps then t	t + 1 and goto 1. otherwise goto 1.뫹
table 1: pseudocode for the atease algorithmin this section we present a stylized algorithm that facilitates analysis. in our experiments  we will introduce a few practically-minded alterations. we call our algorithm atease for  alternating trusting exploration and suspicious exploitation.  it proceeds in iterations indexed by t = 1 1 .... each iteration involves a trusting exploration phase followed by a suspicious exploitation phase. in the trusting exploration phase  atease tries every expert for texp steps  where texp is a fixed polynomial of t   regardless of any previous disappointments the expert may have caused and regardless of how poorly it may be doing during that fixed time. in the suspicious exploitationphase  atease ranks the experts according to their performance in the exploration phase. it then tries using the best expert for a constant number of batches  which we shall call l   each texp steps long. if the return of any of those batches is much lower than the expert's return in the exploration phase  atease stops using the expert and proceeds to exploit the next best. this process of elimination is continued until either there are no more experts or until one expert lasts long enough in the exploitation phase without being eliminated. these two phases are repeated with increasing durations in order to allow for the possibility that some experts may have long mixing times but will perform well in the long run. pseudocode is provided in table   .
모note that if we ever reach an iteration which has an associated explorationtime greater than the mixing time of all of the experts  the problem of choosing the best expert is precisely the stochastic multi-armed bandit problem. unfortunately  there is no way for the agent to ever know it has reached this point. thus  each iteration is conceptually like a bandit algorithm trying to choose amongst the unknown set of experts that have mixing times less than the exploration time. experts are rejected during exploitation in order to minimize the effect of experts that have not yet mixed.
모atease will take as input a confidence parameter 붻  an approximation parameter   a bound on the maximum reward rmax  and a set of experts e. we will show that with high probability and for all t  in time polynomial in t the actual return of atease will compare favorably with the expected return of the best policy that mixes in time t.
모rather than use the standard concept of mixing time  we will use the weaker  but more directly applicable notion of expected-return mixing time    1  they also related it to more standard definitions of mixing time .
definition 1. let m be an mdp and let 뷇 be an ergodic policy on m. the -expected-returnmixing time of 뷇  denoted is the smallest t such that for all t 뫟 t maxi |u뷇 i t  
.
definition 1. let e be a set of stationary  ergodic experts on mdp m. then  denotes the set of experts in e whose -expected-return mixing time is at most t and define
t 
.
모so  with appropriate selection of texp and l  in time polynomial in any mixing time  t  atease will achieve return close to  with high probability. we now formalize this claim in theorem 1  which  because of its similarity to de farias and megiddo's results  we present without proof.
theorem 1. for all t  given input parameters  and
rmax  the atease algorithm's actual return will be within  of with probability at least 1 붻 in time polynomial in  and rmax.
모note that if the mixing time of the asymptotically best expert is t   then the actual return of atease will compete with that expert in time polynomial in t . so  if the asymptotically best expert mixes quickly  the performance of atease will compare favorably to that expert quickly even if other experts have much longer mixing times.
모at first glance  theorem    seems to imply that the sample complexity of atease is completely independent of the number of states and actions in the mdp environment. this is not the case  however  because the mixing time of the experts will in general be dependent on the size of the mdp state space. indeed the mixing time of the asymptotically best expert may be exponential in the size of the state space. however  as we have pointed out before  no algorithm can avoid at least running the best experts for its mixing time and the only dependence of atease on the complexity of the mdp is entirely due to this unavoidable dependence on the mixing time.
1 empirical illustrations
in this section  we use two toy problems to study the applicability of the atease algorithm in comparison to other experts algorithms. it has been found that  despite the existence of more sophisticated techniques  a simple -greedy algorithm  which either chooses the arm that looks the best so far or  with probability  chooses an action at random  was difficult to beat in practice    . we therefore use -greedy as our representative of bandit algorithms. we compare -greedy and the previously discussed exp1 algorithm to a slightly more practical version of atease  denoted ateasel  for atease-lite   which contains a few modifications designed to help speed convergence.
		the mixing problem
	time step	time step	1	1
figure 1: results from toy problems. see text for descriptions. results for all three algorithms were averaged over 1 runs.모ateasel differs from atease in how it increases the exploration time  the number of exploitation batches  and how it chooses experts to be eliminated in the exploitation phase. after each full iteration  the exploration time is multiplied by some constant  c  rather than incremented. these larger jumps in exploration time help expand the number of mixing experts more quickly. rather than a large constant  the number of exploitation batches is set equal to the exploration time  reducing the impact of earlier iterations. finally  during the suspicious exploitation phase  experts are abandoned if the performance of any batch falls below the next best expert's estimate minus some constant . this ensures that even if the best expert has not yet mixed  it will continue to be exploited if it has been performing better than the alternatives. for the sake of clarity in these simple experiments  we set c = 1 and so the exploitation phase is not suspicious .
모the first problem  figure     is a standard 1-armed bandit problem. the underlying mdp has one state and two actions  one giving a reward of 1  the other a reward of 1. each expert i 뫍 1 ... 1 chooses the rewarding action with probabil and so expert i has an expected asymptotic return of  and an -expected-return mixing time of 1 for all . the algorithms perform similarly in the beginning  though eventually ateasel is surpassed. this illustrates an aspect of the ateasel algorithm  namely that it continues to explore lowreturn experts for longer and longer periods of time in hopes that they may perform better in the long run. this is necessary in the case that the underlying domain is an mdp with a long mixing time. in this case  however  the result is a much slower convergence time in comparison to the bandit algorithms. the  sawtooth  pattern seen here shows clearly the effects of the alternating exploration and exploitation phases.
모the second problem  figure     is a 1-state  1-action mdp  shown in    . there are two experts. one always choses action c in state 1 and action a in state 1. thus  it has an expected asymptotic return of 1 and mixes very quickly. the other expert always choses action c in state 1 and in state 1 choses b with probability 1 and a with probability 1. the second expert has an expected asymptotic return of 1  but takes longer to mix. this problem highlights the strength of ateasel. neither -greedy nor exp1 are likely to stay with one expert long enough to allow it to mix so they do not receive good estimates of experts 1's quality. in contrast ateasel discovers the second expert quickly and adopts it in every subsequent iteration  which accounts for the superior return seen in   .
모the results of these simple exerimentsare intended to highlight the strengths and weaknesses of atease. in particular we have seen that atease is not effective at solving the stochastic bandit problemin comparison to algorithms specifically designed for that purpose  but when the mixing time of the experts is unknown  it may significantly outperform algorithms that do not take mixing time into account.

figure 1: the robocup soccer keepaway domain  a  and the delivery domain  b . see text for descriptions.
1 applications to transfer learning
we now demonstratethe utility of experts algorithms in transfer settings. as described in the introduction  we imagine an agent that applies its knowledge from one task to another via some mapping from the original state-space to the new one. such a mapping  combined with a policy for the old problem induces a policy on the new state space. because the agent may not be able to identify the optimal mapping  it may be advised by multiple  experts  which provide different state mappings. the problem of automatically discovering a small set of  reasonable  mappings is a deep one  and well outside the scope of this paper. in our experiments the mappings are heuristically created by hand.
모in this section  we consider two transfer learning problems. in the first  the agent is presented with a task more complex than the one it has learned. its mappings will therefore represent different ways to discard some state information  in order to make use of its knowledge about a simpler space. in the second  we imagine an agent that loses the use of some of its sensors. this agent's mappings must be educated guesses about how to add state information so as to obtain advice from a policy that depends on richer observations.
1 robocup soccer keepaway
 a 	x 1	robocup experts	 b 	x 1	ateasel and sarsa	 c 	x 1	붼 greedy
	 1	 1	 1
1 1 1 1 1 1 training time  hours  training time  hours  training time  hours 
figure 1: results from keepaway. figure  a  shows the performance of a typical set of experts. in  b   ateasel  
c = 1   is shown in solid lines  and sarsa     in dashed lines. in  c  we see -greedy  for the first experiment  we used a modified version of stone's robocup keepaway testbed    . this domain simulates two teams of robots: the keepers and the takers  see figure    . the keepers attempt to keep a ball from the takers for as long as possible. though this is intended to be a multiagent learning problem  we considered a simpler problem by fixing all but one agent's policy as the provided hand-coded policy. the other modification made was to the reward signal. as originallyposed  the reward for any action was the number of steps until the agent next recieved the ball. in this case the average reward will always be 1. instead  we used a reward signal in which the agent recieved no reward for any action and at the end of an episode  recieved a reward of -1  incidentally  we found that reinforcment learning agents learned faster with this reward signal .
모following     we used sarsa 1  with 1d tile-coding and a linear function approximator to train agents in 1 keepaway for 1 episodes and then asked if we could use the resulting policies to do well in 1 keepaway. we generated 1 experts  each mapped to 1 keepaway by ignoring a keeper using a different criterion  such as closest  furthest   most open   etc. . a typical spectrum of the performance of the 1 experts in 1 keepaway is shown in figure   .
모in figure    we see the performance of 1 representative runs of ateasel compared to 1 representative runs of linear sarsa learning 1 keepaway from scratch. in this domain  the best expert has the longest mixing time. as such  it is no surprise that ateasel does not approach the performance of the best expert in the amount of time shown. it is  however  in all cases able to quickly avoid the  bad  experts. also note that  unless the optimal policy is among the experts provided to ateasel  it will never achieve optimal return. it is therefore expected that the learning algorithm will eventually surpass ateasel. however  the learner spends a significant amount of time performing poorly. it is this transient period of poor performance that transfer learning attempts to avoid and ateasel appears to side-step it effectively.
모we note  however  that because of the episodic nature of this domain  the return of each episode is an unbiased estimate of an expert's expected return. therefore each expert's -mixing time is one episode. thus  by thinking of time in terms of episodes  the problem can be expressed as a stochastic bandit problem. as such  we compare ateasel to -greedy  where each  pull  chooses an expert for a full episode. figure    shows 1 representative runs of -greedy. as we might expect from our toy examples  -greedy seems to perform better  on the whole  than ateasel. however  unlike in the toy experiment  ateasel does perform competitively with -greedy  and also provides theoretical performance guarantees that -greedy cannot.
delivery domain results
q1h learningateasel붼 greedyq1
q1exp1q1 1 1 1
1
1
	1	1	1
time steps	1 x 1
figure 1: results from experiments on the delivery domain  averaged over 1 runs   comparing ateasel  and c = 1  to h-learning  뷈 = 1 and 붸 = .1   exp1.p.1  붻 = .1   and -greedy   . the label qxhy represents the expert that assumes queue 1 contains job x and after pick up from queue 1  assumes the agent is holding job y.
1 the delivery domain
in the delivery domain      a robot has to deliver jobs from two queues to two conveyor belts  figure    . queue 1 produces only jobs of type 1  which are destined for conveyor belt 1 and which provide reward of 1 when delivered. queue 1 producesjobs of type 1 and 1 with equal probability. jobs of type 1 are destined for conveyor belt 1 and provide a reward of 1 when delivered. an obstacle  open circle  moves up and down with equal probability. the agent incurs a penalty of -1 if it collides with the obstacle. the world is fully observable  with 1 sensors  and the agent has 1 actions available to it: do nothing  switch lanes  move up  move down  pick up  and drop off. the pick up action is only available when the agent has no job and is at a queue. similarly  the drop off action is only available at the appropriateconveyorbelt when the agent holds a job.
모following tadepalli  we trained agents using h-learning  an average reward reinforcement learning algorithm  with 뷈 = 1 and 붸 = .1 . we then asked if those policies could still be used if the agents were to lose the 1 sensors that indicate which jobs are held in the queues and which job the agent is holding. in this new problem  we can imagine  experts  that fill in the missing 1 sensors and ask the original policy what it would do. since queue 1 always produces the same job  we will let all experts assume that queue 1 is holding job type 1  which leaves two sensors to fill in. each missing sensor can take on two values  job 1 or job 1  and so we will have four experts total. each expert will assume queue 1 either contains job 1 at all times or job 1 at all times. we allow the agents to be slightly more sophisticated regarding the remaining sensor  what job the agent is carrying  by providing them two pieces of historical information. the first tells the agent at which conveyor belt  if any  it has tried and failed to drop off its job. thus  if the agent has attempted a drop off and failed  it knows exactly what job it is carrying. it is when the agent has not yet attempted a drop off that it must make a guess. it does this with the help of the second historical feature: the queue from which the agent picked up a job it is holding. every expert assumes that when picking up a job from queue 1  that it is of type 1. each expert then has an assumption of what job type it is carrying when it picks up from queue 1  which may be different than what job it assumes is contained in queue 1 .
모figure    shows the performance of the experts  ateasel -greedy  exp1.p.1  see     for details   and h-learning from scratch. for fairness of comparison the h-learning algorithm was provided with the same historical information used by the experts. we see that learning from scratch eventually surpasses ateasel  but ateasel performs well quickly. exp1 and -greedy both do worse on average because they are unlikely to try a better expert for long enough for it to demonstrate its quality.
1 conclusion
we presented an algorithm for mediating experts that simultaneously competes favorably with all experts in a number of steps polynomial in the mixing time of each expert. we performed experiments in two transfer learning contexts  in which experts were policies induced by mappings from the state space of a new problem to the state space of an already known problem. we found that experts algorithms were effective in avoiding the transient period of poor performance experienced by uninformed learners. we found that in episodic domains  since the mixing time of the experts is known  standard experts algorithms such as -greedy were most effective. in non-episodic domains  however  it is likely that mixing times would be unknown and variable. in these cases  an algorithm that specifically takes mixing time into account  such as atease may significantly outperform algorithms that do not.
1 acknowledgements
erik talvitie was supported by a nsf funded stiet fellowship from the university of michigan. satinder singh was funded by nsf grant ccf-1  and by a grant from darpa's ipto program. any opinions  findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf or darpa.
