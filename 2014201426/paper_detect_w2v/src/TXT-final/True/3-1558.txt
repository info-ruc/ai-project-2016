
usual numerical learning methods are primarily concerned with finding a good numerical fit to data and often make predictions that do not correspond to qualitative laws in the domain of modelling or expert intuition. in contrast  the idea of q1 learning is to induce qualitative constraints from training data  and use the constraints to guide numerical regression. the resulting numerical predictions are consistent with a learned qualitative model which is beneficial in terms of explanation of phenomena in the modelled domain  and can also improve numerical accuracy. this paper proposes a method for combining the learning of qualitative constraints with an arbitrary numerical learner and explores the accuracy and explanation benefits of learning monotonic qualitative constraints in a number of domains. we show that q1 learning can correct for errors caused by the bias of the learning algorithm and discuss the potentials of similar hierarchical learning schemes.
1 introduction
learning understandable models is one of the main goals of machine learning  but has been recently overshadowed by methods that mainly concentrate on classification or regression accuracy. less effort has been devoted to improving the explanation strength of machine learning methods. induced models are often too complex and overly detailed to provide an understandable explanation of phenomena in a modelled domain. this is particularly notable with methods that achieve excellent accuracy by constructing ensembles of classifiers  overviewin  dietterich  1  . another problem  illustrated and discussed in  suc몭 et al.  1   is that state-ofthe-art numerical machine learning methods often make predictions that a knowledgeable user finds obviously incorrect - not so much in numerical  but in qualitative terms. such qualitative errors of numerical predictors are undesirable particularly because they make numerical results difficult to interpret. the underlying mechanism in the domain is usually best explained in qualitative terms. however  this is obscured by qualitative errors in numerical predictions.
모qualitatively faithful quantitative learning  called q1 learning for short  was proposed  suc몭 et al.  1  to rectify the qualitative problems of numerical learning. q1 learning combines qualitative and numerical learning to give numerical predictions that both fit the data well and are consistent with an induced qualitative model. the qualitative consistency is beneficial in terms of explanation of phenomena in a modelled domain. quite surprisingly  a case study with q1 learning shows that induced qualitative constraints can also improve numerical accuracy. this paper extends the previous work in several directions.
모one contribution of this paper is a q1 learning scheme that combines learning of monotonic qualitative constraints with an arbitrary numerical learner and enables us to study accuracy benefits of the induced constraints. qualitative learning has been previously used in a number of applications that are mainly tied to dynamic systems and control. in these applications  advantages in terms of explanation and in terms of the control performance of the induced qualitative models were observed. since these models define only constraints on a class variable  a direct assessment of their accuracy benefits was previously not possible. the second contribution is an empirical evaluation in a number of domains and a demonstration that such regression  guided by induced qualitative constraints  often increases numerical accuracy.
모we analyze the reasons for these accuracy improvements and show that q1 learning corrects for errors caused by the bias of a learner. in this respect q1 learning is similar to ensembles of classifiers  in particular approaches that combine classifiers constructed by different learning algorithms 
e.g. combining instance and model-based learning  quinlan  1  or stacking and its variations  wolpert  1; gama and brazdil  1; todorovski and dz몭eroski  1 . an important distinction of q1 learning is the  qualitative  consistency of models at different levels of abstraction. we discuss advantages of similar hierarchical learning schemes and we demonstrate that explanation improvements do not necessarily come at the price of lower accuracy.
모in section 1 we describe the q1 learning scheme proposed in this paper. the elements and the details of q1 learning are described in section 1. then we give experimental results in various domains and study accuracy improvements using bias-variance decomposition. section 1 discusses results and benefits of q1 learning  and gives directions for future work.
1 q1 learning with quin and qfilter
the idea of q1 learning is to combine qualitative and numerical learning to find a regression function that fits the data well and is consistent with an induced qualitative model. in this paper  q1 learning consists of two stages:
1. program quin 1  described in section 1  induces a qualitative tree from numerical training examples. qualitative trees are similar to decision trees  but have monotonic qualitative constraints in their leaves.
1. algorithm qfilter  described in section 1  uses the induced qualitative tree  plus training examples  plus class predictions of an arbitrary numerical learner  to alter the predictions to respect the induced qualitative tree. qfilter is an optimization procedure that finds the minimal quadratic changes in class values that achieve consistency with the qualitative tree. in our experiments  the numerical learners  also called base-learners  are regression trees  model trees and locally weighted regression.
모in this paper  a q1 learner consists of a qualitative constraints learner and a numerical base-learner  and can be denoted by q1 qual-learner  base-learner . we abbreviate the common case q1 quin  base-learner  simply as q1baselearner. this learning scheme is particulary interesting because base-learner's predictions are changed optimally in the sense of squared error. therefore  the differences between base-learner's predictions and q1 predictions come just from induced qualitative constraints.
1 elements of q1 learning
1 monotonic qualitative constraints  qualitative trees and quin
monotonic qualitative constraints  mqcs  are a kind of monotonicity constraints that are widely used in the field of qualitative reasoning and are a generalization of monotonic function constraint used in qsim  kuipers  1 . a simple example of an mqc is: y = m+ x . this says that y is monotonically increasing in its dependance on x  i.e. whenever x increases  y also increases. in general  mqcs can have more than one argument. for example  z = m+   x y   says that z is monotonically increasing in x and monotonically decreasing in y . if both x and y increase  then according to this constraint  z may increase  decrease or stay unchanged. in such a case  an mqc cannot make an unambiguous prediction of the qualitative change in z.
모qualitative trees are similar to decision trees  but have monotonicqualitative constraints in the leaves. figure 1 gives an example of a simple qualitative tree. this qualitative tree is a qualitative model of the function y = lsin 붯   where 1 뫞 붯 뫞 뷇 and l   1. it describes how y qualitatively depends on attributes 붯 and l. the tree partitions the attribute space into two regions that correspond to the two leaves of
id 1 

figure 1: a qualitative tree induced from examples for the function y = lsin 붯   where 1 뫞 붯 뫞 뷇 and l   1. the right leaf  which applies when 붯   뷇/1  says that y is monotonically decreasing in 붯 and monotonically increasing in l.

figure 1: achieving consistency with mqc c = m+ a : class values ci  denoted by circles  are changed into ci + di  denoted by crosses  by minimizing the sum of squared changes di. the arrows denote the class changes di.
the tree. note that a simple qualitative tree can describe a relatively complicated nonlinear function. such qualitative trees are induced from numerical data by learning program quin  suc  1몭	 .
모quin constructs a qualitative tree in a top-down greedy fashion  similar to decision or regression tree learning algorithms. the split selection criterion is based on the minimum description length principle and takes into account the encoding complexity of the subtrees  the consistency of the mqcs in the subtrees with the corresponding data  and the  ambiguity  of the mqcs with respect to the data  the more unambiguous qualitative predictions the mqc can make  the better .
1 qfilter algorithm
qfilter handles each leaf of a qualitative tree separately. it first splits the examples according to the qualitative tree and then changes class values to achieve consistency with mqcs in corresponding leaves.
모let us first observe a simple example in figure 1. we have eight examples  ai ci   i=1 ... 1. class c has values ci and attribute a has values ai=i. the examples are not consistent with the given mqc c =m+ a   because the mqc requires that ci+1   ci which is violated at i = 1 and i = 1.
모to achieve consistency with c = m+ a   class values should be changed into ci + di  where the unknown parameter di denotes the change in the i-th class value. class changes di are constrained by mqc-imposed inequalities: ci+1 + di+1   ci + di where i = 1 ... 1. these inequalities can be formulated in matrix notation as ad   b  where d is a vector of unknown parameters di  vector b has elements bi = ci   ci+i  and matrix a has elements ai i =  1  ai i+1 = 1 and zeros elsewhere. in general  b and a depend on the mqc-imposed inequalities  which in turn depend on the mqc and on the ordering of attributes' values.
모therefore  finding minimal quadratic changes in class values that achieve consistency with a given mqc can be posed as the quadratic programming optimization problem: find vectord that minimizesthe criterionfunctiondthd such that ad   b. in the above formulation matrix h is the identity matrix. in general h can be changed to differently penalize the changes in class values as described in section 1.
모since the criterion function with diagonal matrix h is a convex function  and because the linear constraints ad   b
모define a convex hull  a local minimum of the criterion function is a globally optimal solution. a more elaborate description of qfilter  defining also the appropriate ordering of class values when more than one attributes are used in an mqc  is given in  suc and bratko  1몭  . in previous work qfilter was used with qualitativetrees derivedmanuallyfromdomain knowledge. here we use it in a different and a more challenging context  where qualitative trees are induced from data.
1 qfilter for q1 learning
qfilter is supplied with a qualitative tree  training examples with their class values and test examples with the baselearner's class predictions. qfilter then adjusts the class values of the training and the test examples to achieve consistency with the qualitative tree.
모one improvement of qfilter is to use the base-learner's confidence estimates in its predictions. in this case  qfilter makes smaller adjustments to the class values with higher confidences at the expense of larger changes of class values that have lower confidences. this is achieved by changing the quadratic programming criterion function. namely  matrix h is changed from the identity to a diagonal matrix with elements hi i = wi. weight wi is computed from the base-learner's confidence estimate in the i-th class value. of course  the computation of weight wi depends on a type and a scale of confidence estimates  but would generally be larger if a numerical predictor is more confident in the i-th class prediction.
based on this idea  we used a heuristic weighting function:
  where ci denotes
base-learner's confidence in the class prediction of the i-th test example  and 뷃 and  denote the mean and the standard deviation of base-learner's confidences over all test examples. therefore  the weight of a test example is between zero and two. the weight of all training examples is set to two. in experiments with locally weighted regression  confidence estimates ci were set to the sizes of confidence intervals. model and regression trees do not provide similar confidence estimates. for this reason  confidence estimates of all test examples were set to one.
1 empirical evaluation
1 experimental details
here we evaluate accuracy benefits of q1 learning with various numerical base-learners. given a set of training exam-

figure 1: a planar two-link  two-joint robot arm. the first link is extendible with length l1 ranging from 1 to 1.
ples  a base-learner is used to predict class values of test examples. the same training examples are used for quin to induce a qualitative tree. the qualitative tree  the training and the test examples with base-learner's class predictions are then used by qfilter to give predictions that are consistent with the qualitative tree. this procedure is then repeated  for example  ten times with ten-fold cross-validation  with different training and test examples.
모in the experiments we compare root relative squared errors  rres for short  of the base-learner and the q1 learner. here  the rre is the root mean squared error normalized by the root mean squared error of average class value. the baselearners in our study are regression and model trees  breiman et al.  1; quinlan  1   and locally weighted regression  atkeson et al.  1 . the first two base-learners were chosen because they are well-established numerical learners that provide a symbolical model. locally weighted regression  lwr for short  does not provide a model explaining a studied domain  but often gives more accurate predictions than model or regression trees. with such base-learners  the explanation benefits of q1 learning are even more obvious. in experiments we used our implementations of these baselearners. our regression and model trees use cost-complexity pruning  breiman et al.  1  and smoothing. lwr uses gaussian weighting function with local optimization to set the kernel size at each prediction point. we also give rres of m1 model trees  quinlan  1 and its weka implementation called m1prime  witten and wang  1  to show that we are not comparing q1 to base-learners that perform poorly. with all learners  default values of their parameters were used.
모we analyze the reasons for the q1 accuracy improvements using bias-variance decomposition  geman et al.  1  and draw some interesting conclusions.
1 robot arm domain
here we describe experiments in the domain of a planar twolink  two-joint robot arm depicted in figure 1. the angle in the shoulder joint is denoted by 붯1 and the angle in the elbow joint is denoted by 붯1. angle 붯1 is between zero and 뷇  while 붯1 is between  뷇/1 and 뷇/1. the first link  i.e. the link from the shoulder to the elbow joint  is extendible with length l1 ranging from 1 to 1. the second link has fixed length 1. y-coordinates of the first and the second link ends are denoted by y1 and y1 respectively. we experimentedwith four learning problems that differ in class variable  y1 or y1  and the attributes used for learning. these learning problems were defined to pose increasingly more difficult problems to q1 learning.

figure 1: comparing rres of base-learners and q1 learners in learning problems a  b  c and d with 1%  1% and 1% noise.table 1: rres of lwr  model  mt  and regression trees  rt  and the corresponding q1 learners with no noise. the last column gives rres of m1prime.
lwrq1lwrmtq1mtrtq1rtm1pra1111111b1111111c1111111d1111111모the easiest learning problem  called problem a  is to predict the y-coordinate of the first link end given its length and angle  i.e. learning y1 = f l1 붯1 . other learning problems require predicting the y-coordinate of the second link end using different attributes. in learning problems b and c we helped the learners with a derived attribute 붯sum = 붯1+붯1  i.e. the deflection of the second link from the horizontal. problem c is to learn y1 = f l1 붯1 붯1 붯sum   while in problem b we also used y1 as an attribute. problem d requires learning y1 = f l1 붯1 붯1 . these problems pose increasingly more difficult problems to q1 learning. the easiest is problem a  because a correct qualitative model is a simple qualitative tree  given also in figure 1. learning problem d is the most difficult for q1  since a correct qualitative model cannot be expressed by a qualitative tree.
모to compare accuracy of different base-learners and q1 learning we generated examples where angles 붯1 and 붯1 and link length l1 were randomly generated from a uniform distribution. we experimented with different percentages of gaussian noise in the class variable. noise percentage p% means that the standard deviation of noise is p 뫄 dc/1  where dc denotes the difference between maximal and minimal class value. we used 1 training examples and measured accuracy on separate test sets of 1 examples without noise. all results are averages on 1 randomly generated training and test sets.
모table 1 gives rres of lwr  model and regression trees and q1 learning that uses these base-learners. for comparison rres of m1prime model trees are also given. results with zero  1%  and 1% noise are given in figure 1. in all of the learning problems and with all noise levels  q1 improves the average rres of all base-learners. these improvements in accuracy depend on the base-learner and the learning problem. generally  the improvements are the greatest with regression trees and the smallest with model trees. it is notable that in general our base-learners are much more accurate than m1prime on these learning problems. although we are here not interested in comparison of different base-learners  it is good to know that with q1 we are not improving the baselearners that perform poorly.
모the significance of the q1 accuracyimprovementsfor each learning problem was tested using the resampled paired t test. four learning problems 뫄 three base-learners 뫄 three noise levels gives 1 comparisons of rre of a base-learner and a corresponding q1 learner. at 1% significance level  the q1 learners are significantly better in 1 comparisons and about the same in only three comparisons. the comparisons where differences in rre are not significant correspond to model trees on learning problem d with all three noise levels.
bias-variance decomposition
to understand the reasons for the q1 accuracy improvements we used bias-variance decomposition  geman et al.  1; domingos  1   which has proved to be a very useful tool for understanding machine learning algorithms. biasvariance decomposition in regression states that the expected squared error of a learner on test example x is the sum of the irreducible noise n x   the bias b x  and the variance v  x . the bias b x  of the learner on an example x is the squared difference between the true value and the mean prediction on x over all possible training sets. here defined bias b x  is in the literature called also squared bias  but we use the notation from  domingos  1  and call it bias. the variance v  x  is the expected value of the squared difference between the true value and the mean prediction on x. the bias measures the systematic error incurred by a learner  and the variance measures the error incurred by its fluctuations around the central tendency in response to different training sets. irreducible noise is the error of the optimal  bayes  model and is in general very difficult to estimate. however  we are here using an artificial learning problem and can therefore measure the bias and variance by directly simulating the definitions.
table 1: description of data sets and average 1-fold cross-validation root relative squared errors  rres  of base-learners and the corresponding q1 learners. the last column gives rres of m1 where this is available  and m1prime otherwise.
data setcasesattributeslwrq1lwrmod.tr.q1mod.tr.reg.tr.q1reg.tr.m1autompg1/1.1.1.1.1.1.1.1autoprice1/1.1.1.1.1.1.1.1housing1/1.1.1.1.1.1.1.1machinecpu1/1.1.1.1.1.1.1.1servo1/1.1.1.1.1.1.1.1craneskill111111111craneskill111111111antisway1/1.1.1.1.1.1.1.1모we used 1 training sets of size 1  generated as in the previous experiment and measured the average bias and average variance on a test set of 1 equidistant data points. we refer to these averages over different training sets and different test examples simply as bias and variance. comparing the base-learnerswe noticed that regression trees have the highest variance. this is in accordance with their complexity - they usually had more than 1 leaves. model trees are smaller and have the smallest variance when no noise  but with increasing noise the variance increases most notably. comparing q1 learners to the corresponding base-learners we noticed that q1 always notably reduces the bias of the base-learners. this happens in all four learning problems  with all three baselearners and different noise levels. for example  in problem d with no noise  q1 reduces bias and variance of lwr from 1 and 1 to 1 and 1  respectively. q1 always notably decreases the variance of regression trees  but sometimes increases the variance of lwr and model trees. the base-learners  used here  have a bias towards linear models. it seems that q1  with less restrictive monotonicity constraints  reduces their bias  but does not considerablyincrease the variance.
모q1 learning combines hypotheses of two different learning algorithms  i.e. a qualitative learner and a numerical learner. in this respect it is similar to ensembles of classifiers and in particular approaches that combine classifiers constructed by different learning algorithms  as for example stacking and its variations. such methods improve accuracy mainly by reducing the error due to the bias of a learner. this is the consequence of combining hypotheses that make uncorrelated errors  dietterich  1 . we believe that such uncorrelated errors lead to bias reductions also in the case of q1 learning. it should be noted that  although q1 predictions are consistent with an induced qualitative model  they combine both the base-learner's predictions  and the qualitative model.
1 uci and dynamic domains
to explore the potentials of learning similar constraints with a wider range of learning problems we describe experiments with eight data sets. the first five are the smallest regression data sets from the uci repository  blake and merz  1  with the majority of continuous attributes. a reason for choosing these data sets is also that quinlan  gives results of m1 and several other regression methods on these data sets  which enables a better comparison of q1 to other methods. these data sets are autompg  autoprice  housing  machinecpu and servo.
the other three data sets are from dynamic domains where
quin has typically been applied so far  suc  1;몭 suc and몭 bratko  1 . it should be noted that in these domains the primary objective was to explain the underlying control skill and to use the induced qualitative models to control a dynamic system. until now  it was not possible to measure their numerical accuracy or compare it to other learning methods.
	 vgain d 1 	servo qualitative tree 

m1: m- + - ctympg stroke peakrpm     m1: m+ + curbwght width  
m1: m+ losses  	 	 	    m1: m+ - curbwght ctympg 
m1: m+ wheelbase   	 	    m1: m- compratio 
m1: m- + + ctympg bore curbwght 
figure 1: qualitative trees induced from data sets servo and autoprice. mqcs in leaves of each qualitative tree give monotonic constraints on the class variable.
data sets craneskill1 and craneskill1 are the logged data of two experienced human operators controlling a crane simulator. such control traces are typically used to reconstruct the underlying operator's control skill. the learning task is to predict the velocity of a crane trolley given the position of the trolley  rope angle and its velocity. data set antisway was used in reverse-engineering an industrial gantry crane controller. this so-called anti-sway crane is used in metallurgical companies to reduce the swing of the load and increase the productivity of transportation of slabs. the learning task is to learn the control force applied to the trolley  given the desired and the current trolley velocity and the position and velocity of the load relative to the trolley.
모in all experiments  qualitative trees were considerably simpler than other induced models. figure 1 gives examples of qualitative trees induced in data sets servo and autoprice. these qualitative trees are considerably simpler than model trees. in servo  m1prime induces a model tree with eleven leaves  with all four attributes appearing in all eleven linear models. in autoprice  m1prime induces a model tree with ten leaves  with at least ten attributes in each linear model.

figure 1: comparison of rres of base-learners and corresponding q1 learners. disjoint columns give rres of m1 model treesten-fold cross-validation results are given table 1.	for where available  and m1prime otherwise.
each data set we give  respectively  the numbers of cases  numbers of continuous and all attributes  rres of baselearners and rres of corresponding q1 learners. the last column gives published rres of m1 model trees  quinlan  1  on uci data sets  and rres of m1prime for others. these results are presented also in figure 1.
모a generalobservationis that q1 improvesrres of all three base-learners in seven out of eight data sets. q1 is worse just in autompg with lwr and model trees. smaller root relative squared errors in the last three data sets do not imply that these learning problems are easier  but are just the consequence of the normalization of rres with large class variances.
모the significance of the q1 accuracy improvements was tested using the 1-fold cross-validated paired t test. eight data sets 뫄 three base-learners gives 1 comparisons of rre of a base-learner and the corresponding q1 learner. at 1% significance level  the q1 learners are significantly better in 1 comparisons and about the same in six comparisons. the differences are not significant in autompg with all three base-learners and with one base-learner in autoprice  housing and craneskill1. q1 is never significantly worse.
1 discussion and conclusions
the goal of this paper is to explore the accuracy and explanation benefits of q1 learning that were observed also in previous work  suc몭 et al.  1 . we extend the empirical evaluation to a number of domains and analyze the results. the proposed q1 learning scheme  makes it possible to combine learning of qualitative constraints with an arbitrary numerical learner. it uses algorithmqfilter  which is particularyinteresting since the base-learner predictions are optimally changed  in the sense of squared error  to be consistent with an induced qualitative tree. therefore  the accuracy improvements of q1 with respect to a base-learner are only due to the induced qualitative trees.
q1 learning as a hierarchical learning scheme q1 learning  as presented in this paper  can be seen as a hierarchical learning scheme  where a learner at a higher level induces a hypothesis hn that guides the learner at a lower level. at higher levels of the scheme more abstract concepts  or more general constraints  are learned. in this way  hn provides inductive bias for learning of hypothesis hn 1. in the case of q1 learning  h1 is a qualitative tree induced by quin and h1 are numerical predictions found by qfilter. similar hierarchical learning schemes were proposed either to improve the generalization of a single learning task  for example stacked generalization  wolpert  1  and cascade generalization  gama and brazdil  1   or to facilitate the learning of several tasks in a hierarchy  stone and veloso  1 . q1 learning has two distinctive properties. first  the hypotheses in the hierarchy are consistent and described at different levels of abstraction. this is important for the explanation of the phenomena in the modelled domain. second  the hypotheses are learned in the general-to-specific order  which can reduce the search space  and can consequently improve also the generalization properties.
explanation and accuracy benefits of q1 learning q1 predictions are consistent with a qualitative model that provides an explanation at a higher level of abstraction. in this respect q1 learning is different than other methods for combining classifiers. qualitative consistency enables q1 to improve numerical accuracy by combining hypotheses induced by different learners  but retain a simple explanation.
모benefits of qualitative learning in terms of providing simple and understandable explanations in various domains  including craneskill1  craneskill1 and antisway are discussed in  suc  1;몭 suc and bratko  1몭  . for example  qualitative trees induced in craneskill1 and craneskill1 have only a few leaves and were  because of their simplicity  preferred over the previous approaches for skill reconstruction  which typically learned regression or model trees with more than twenty leaves. although the induced qualitative trees are simple  they reveal some surprising and nontrivial aspect of the human skill. simple and understandable models were induced also in other domains studied in this paper. for example  in robot arm domain  problems a  b and c  quin usually induced qualitative trees that are very close to the correct qualitative models even with high percentage of noise. simple qualitativetrees were inducedalso fromuci data sets  see figure 1 .
모in the presented experiments  q1 typically improved accuracy of the three base-learners. we compared root relative squared errors  but similar improvements were observed also with mean absolute errors. we experimented also with k-nearest-neighbor algorithm as a base-learner. it generally performed worse than the other three base-learners and the accuracy improvements of q1 learning were even more obvious.
모bias-variance decomposition in the robot arm domain shows that the accuracy improvements stem mainly from correcting for errors caused by the bias of a base-learner. experiments in the previous section suggest that combining  monotonic regularities  inductive bias with inductive bias of other learners is beneficial in a wide range of domains  also in domains when this might be less expected. as noted in section 1  q1 learning is similar to ensembles of classifiers  in particular approaches that combine classifiers constructed by different learning algorithms. although  the accuracy improvements of q1 are not comparable to those achieved by ensembles of classifiers  q1 has advantages in terms of explanation.
limitations and future work
limitations of q1 learning presented in this paper are mainly tied to the current implementation of quin. because of its complexity it is difficult to apply it to very large data sets. incorporating sampling techniques with qualitative learning might be beneficial. another idea for future work is to assess and use the quality of the induced mqcs in leaves  for example on a separate set of examples. the error of a q1 learner could be used to prune a qualitative tree. this might be useful also in data sets such as autompg  where q1 otherwise has problems. by considering only leaves where mqcs significantly improves a base-learner  a qualitative tree could be transformed into a set of qualitative association rules.
모an interesting direction for future work is to explore the possibilities of the proposed hierarchical learning scheme with several layers of constrains  describing hypotheses at different levels of abstraction. experimental results with q1 learning suggest that such scheme can be used to combine classifiers and improve accuracy  and  at the same time provide an understandable explanation of the phenomena in a modelled domain.
acknowledgements
the work reported in this paper was supported by national ict australia and the slovenian ministry of education  science and sport. national ict australia is founded by the australian government's backing australia's ability initiative  in part through the australian research council.
