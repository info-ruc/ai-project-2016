 
in most research on concept formation within machine learning and cognitive psychology  the features from which concepts are built are assumed to be provided as elementary vocabulary. in this paper  we argue that this is an unnecessarily limited paradigm within which to examine concept formation. based on evidence from psychology and machine learning  we contend that a principled account of the origin of features can only be given with a grounded model of concept formation  i.e.  with a model that incorporates direct access to the world via sensors and manipulators. we discuss the domain of process control as a suitable framework for research into such models  and present a first approach to the problem of developing elementary vocabularies from perceptual sensor data. 
1 	introduction 
the goal of human concept formation is to arrive at a 
conceptual system that partitions the encountered ob-
jects and events in a way that enables us to effectively deal with our environment. as such  the task of concept formation can be split up into two subtasks  concept aggregation and concept characterization. concept ag-
gregation is the decision about which entities are to be grouped together into a concept  concept characterization means finding an intensional description of the proposed concept based on its extension. the latter task is often called concept learning  from examples . 
   consequently  what separates concept formation from concept learning is the difficult question of deciding which objects to aggregate  i.e.  how to carve up the world into different concepts. most operational models of concept formation  most notably conceptual clustering systems  kolodner  1; lebowitz  1; fisher  1; gennari et a/.  1  base their answer to this question on an assumption that was nicely formulated by rosch and her colleagues  1; 1 : the  correlated feature  
   *this work was partially supported by the european community under esprit contract no. p1  machine learning toolbox . 
1 	learning and knowledge acquisition 
view assumes that features in the world occur as clusters  and that the best concepts are those that maximize intra-concept correlations  and minimize inter-concept correlations  i.e. reflect the presumed cluster structure of the world. there is a significant amount of empirical evidence that people detect and use feature correlations  medin et a/.  1; younger and cohen  1   and the existing clustering systems based on the correlated feature principle and extensions thereof1 have been very successful  both in modeling psychological data  fisher  1   and in solving application problems. 
   none of the existing concept formation models  however  examine the fundamental question of where the building blocks of correlations  namely features  come from. invariably  features are assumed to be provided as elementary vocabulary. 
   in this paper  we argue that this is an unnecessarily limited paradigm within which to examine concept formation  and present possible approaches to the problem. we contend that a principled account of the origin of features can only be given with a grounded model of concept formation  i.e.  with a model that incorporates direct access to the world via sensors and manipulators. even though we present first steps towards such a grounded model  the primary goal of the paper is to pull together different lines of argumentation within psychology and machine learning in order to establish a solid foundation for research into grounded models. 
   the paper is organized as follows. in section 1  we pose the problem of features in more detail  and discuss proposed solutions from the fields of machine learning and neural networks. in section 1  we consider the possibility that features are simply  hard-wired  into the human perceptual system as innate capabilities. based on those discussions  we then propose a framework in which the feature formation problem can be examined  section 1   and present first steps towards a model of grounded concept formation  section 1 . related work is discussed in section 1. section 1 contains our conclusions. 
   'see  wrobel  1  for a more detailed discussion of various concept formation approaches. 
1 	the problem of features 
in the introduction  we have already identified the central assumption that underlies most concept formation research: the assumption that features are elementary units of description that are given and can be used as building blocks of concepts. this assumption can be seen as an instance of a basic axiom of the information processing paradigm of cognitive science and artificial intelligence  in that it assumes that there is information in the world that an intelligent agent can absorb. without repeating the philosophical discussion about this point  cf.  wittgenstein  1    suffice it to say that this axiom is very hotly debated  and that it is being contrasted by the constructivist view that there is no information in the world  not even data   and that all structure or information is created inside the intelligent observer. in other words: nothing in the world is independent of human observation. 
　even though we do not share the extreme constructivist position  we believe that any concept formation process relies on the filtered perception/interpretation of the world that the observer imposes. returning to features  this means that any concept formation model relying on features must include an account of their creation. otherwise  we would have only replaced the concept formation problem by the equally hard feature formation problem. 
1 	the new-term problem in machine learning 
within machine learning  the problem with features has been recognized as the new-term problem  or the problem of constructive induction  in recent years: considerable effort is required to make sure that the input representation of a learning problem is stated with the  right  features that enable the learning program to find the de-
sired target concept. j.r. quinlan   for example  has reported spending several weeks on designing a representation for learning a chess endgame concept   lost n-ply  . 
   various solutions to the new-term problem have been proposed  many based on the heuristic that a new feature is to be introduced whenever the existing features no longer allow the formulation of a concept description that separates the positive from the negative examples. utgoff's stabb  and schlimmer s stagger  use this strategy for a learning-frornexamples task; the blip system  wrobel  1  has used it within observational learning. while other strategies for performing new-term construction have been used  eg.  muggleton and buntine  1; wirth  1  rouveirol and puget  1    all approaches construct their new terms by defining them in terms of already existing terms. thus  they all share the same fundamental limitation - their new features can never leave the  closure of existing symbols   i.e.  can never distinguish situations or objects that were indistinguishable with the original feature set1. in a sense  they are mere abbreviations that allow more concise concept descriptions. 
1 this is true even for recursive definitions. 
1 	symbol grounding and connectionism 
any true solution to the new feature problem requires access to the  real world  that the symbolic system is trying to model. it is insufficient to define the meaning of symbols by a semantics that is externally ascribed to them  eg. by the system constructor ; instead  their meaning must be intrinsically grounded in the world - this is the problem of grounding meaning  cottrell et a/.  1   or more specifically  of grounding symbols  hamad  to appear . 
　it is important to emphasize that the problem of grounding symbols is different from the symbolic/subsymbolic/nonsymbolic debate that is currently fought out within ai and related disciplines. on first sight  nonsymbolic approaches seem the right answer if symbols are the problem. in distributed connectionist approaches  neural networks  cf.  rumelhart et a/.  1    for example  there are no symbols  i.e.  no single physical tokens with an assignable meaning. instead  information is represented in a network of interconnected nodes by the weights attached to the connections  and by the activation patterns of the nodes. those activations are the result of propagating the activation of a set of input nodes along the weighted connections  modifying them accordingly. meaning cannot be attributed to any single node or weight  only to distributed patterns of weights or activation. thus there are no symbols - and no symbol grounding problem  
　closer inspection reveals that the crucial point is not the absence of symbols  but the role of the input nodes. in many applications of neural networks  the input nodes can be seen as simply encoding a set of  preselected  input features. in those cases  there still is a  grounding  problem without symbols: the function computed by the network is still expressed only in terms of those input features. as an example  we can look at recent research by lee  flowers  and dyer  on building symbols in a connectionist system. they feed encodings of propositions into a connectionist network  and manage to form hidden layer patterns that can be interpreted as symbolic representations of objects. the symbols seen as bit patterns have the desirable property of encoding their own meaning in terms of their relation to other objects  but they are still not grounded in the world - they can be no finer grained than the input propositions they were built from. thus  the important point is not the presence or absence of symbols  but whether the learning system is actually grounded in its environment. 
1 	features as innate structures 
even if the world does not come prepackaged into features  an important issue to consider is whether elementary features are perhaps innate structures that have developed evolutionarily. to answer this question  a large body of research exists within developmental psychology1. even though there is no unanimously ac-
1
　　we have used  among others  the reviews by gibson and spelke  about perception  mandler  about representation  sigel  about concepts  and by oerter and montada . 
	wrobel 	1 
cepted position on some of the issues  and some of the data are contradictory  it is safe to say that in the domain of perception  humans are innately endowed with an important array of capabilities. most notable  for our purposes here  is the ability to perceive the world in terms of objects and events from very early on:  some mechanisms for detecting invariants are present at birth  .. .   .    gibson and spelke  1  p. 1 . 
   under suitable conditions  even a newborn infant will reach for a visible object. at 1 to 1 weeks  infants showed avoidant behavior  retracting their heads and interposing their hands  when confronted with an approaching   looming   object. at one month of age  infants reliably turn their heads towards a target if that target is introduced not too far away from their line of sight. this behavior can be shown both for visual and auditory targets. at the age of 1 months  infants generally swipe at objects  and at 1/1 months  they begin to systematically reach for them. at 1/1 months  infants attend to the rigidity  and at 1 months  to the weight of objects as indicated by their anticipatory muscle tension. these findings demonstrate that even newborns are capable of perceiving objects and events in their environment and reacting to them  and that attributes such as weight and rigidity are being used relatively early. 
   thus  from the outset  there seems to be a precoordinated system of perception and action in humans. this is also reflected in various proposals of representational systems developed eg. by piaget   werner  werner  1; sigel  1   and bruner  bruner  1; oerter and montada  1 : they all include a sensorimotor level as the basis of the representation system  bruner uses the term enactive representation . above the sensorimotor level  we find a perceptual level  where experience is represented by a selective organization of percepts and images  bruner's iconic level   and a symbolic level. 
   nonetheless  it is also clear that the perceptual level is not cast in stone. for certain perceptual categories  within-category differences look much smaller than between-category differences even when they are of the same size physically. instances of this perceptual categorization effect have been observed empirically in color and phoneme perception  harnad  1 . those effects are dependent on the categories available in the native language of a person  and can therefore not be present at birth. furthermore  they can be modified by acquiring new categories. 
   in summary  it seems to reasonable to assume that in humans at least  the input to concept formation processes is at the level of sensing and acting  but not in the form of uninterpreted physical stimuli  but in a form that innately separates out objects and events. later development then improves the precision of this process to the extent that the perceptual system can benefit from additional acquired categories and features. 
1 	a research framework 
a grounded model of concept formation should therefore incorporate some degree of built-in structure on the perceptual level  but also mechanisms to change and/or 
1 	learning and knowledge acquisition 
augment the elementary features that were initially provided. given the emphasis that psychological theories place on actions  events  nelson  1   and goal-oriented behavior  barsalou  1   see  wrobel  1  for a more detailed discussion   such a model should be developed in a framework that allows these aspects to be incorporated as well. instead of looking at the concept formation task on isolated objects  it must be examined in the context of an agent that is acting in its enviroment according to a certain set of goals. 
   the obvious choice for such a framework would be to use a robot in a real environment  equipped with vi-
sion  multi-joint manipulators  and the ability to move around. even though this setting is very attractive for machine learning from a theoretical as well as application point of view  see eg. mitchell et. al.'s  proposal   we believe it is unnecessarily complex for studying the issues we are interested in. robot vision and motion control are difficult problems the solution of which is unrelated to the concept and feature formation issues that we want to examine: the discussion of innate capabilities in section 1 showed that at the sensorimotor level  a lot of precoordinated schemata exist that probably cannot be acquired by the kind of general  ontogenetic  mechanisms we are interested in here. 
   if one removes the vision and motion aspects of a robot  one is left with a setting where an agent inspects its environment with a number of simple sensors  has a number of simple effectors  some problem solving goals  and means of assessing its own success or failure. a concrete instantiation of this situation can be found in process control  a domain where ai techniques are beginning 
to be applied  rowan  1 . there  a control system takes multiple-process sensor inputs  analyzes the data  makes decisions about the operating conditions of the process  and adjusts certain control parameters. thus  in this concrete setting  the agents sensors are the process measurement devices  its effectors are the parameters that can be set  eg. valves   and its problem solving goal is to keep the process operating optimally  which can be measured by certain key parameters. 
   w i t h i n this general scenario  we will use the following assumptions: 
  all sensor values are real numbers  and there are m i n i m u m and m a x i m u m values. 
  a l l effectors are binary parameters  i.e.  an action consists of an assignment of values to all effectors. this is also sufficient to model command selection by using the convention that an effector command is executed whenever its assigned value is non-zero. 
  there is an fixed internal feedback function   re-ward center   cf.  whitehead and ballard  1   that computes a goal satisfaction indicator based on the current sensor values. 
is this too simple a scenario  we believe not: 
  there is an environment that provides a continuous flow of input in which  events  can happen  
  the agent has goals and means of achieving those goals  

figure 1: sketch of a grounded concept formation model 
  real-valued sensors are sufficient to model categori-cal perception effects  eg. for color. 
   nonetheless  the models developed in this setting and the predictions they make will have to be carefully evaluated with respect to their generality; the fact that human concept formation takes place in a three-dimensional environment may require qualitatively different methods. 
1 towards a model of grounded concept formation 
in this section  we present a preliminary model of a concept-forming agent that could be operating in the above  process-control  framework. 
   the model takes a deliberate top-down approach to the problem of grounding symbols  i.e.  it tries to augment the existing successful approaches to concept formation with a grounding in the world  instead of replacing them by a totally new mechanism. this contrasts with bottom-up approaches that promise one general explanatory mechanism  cf.  schnepf  1; kugler et al.  1    but cannot offer the capabilities of existing symbolic models  yet . 
   as the heart of the agent  we use a hierarchy of probabilistic concepts  smith and medin  1  that has been used in similar form in the clustering systems c o b w e b  fisher  1  or u n i m e m  lebowitz  1 . the concept hierarchy is used not to cluster objects  but situations of the world in order to predict an appropriate reaction in terms of effector values. each situation is described by a nominal  symbolic  value for each of the sensors that is computed from the real-valued  raw  sensor data by a preprocessing element1. 
   the preprocessing elements segment the realnumbered value range of their sensors into a number of disjoint intervals that are each represented by a symbolic value; those values constitute the system's set of features 
1
　　the agent's sensors sample their associated signal according to a certain sampling interval. for simplicity  one may assume the sampling interval as fixed and appropriate to the rate of change in the environment  or one could use an adaptive mechanism that increases  decreases  the sampling rate whenever a certain number of subsequent samples have been different  the same . 
- its elementary symbolic vocabulary. the initial vocabulary could either be a maximally coarse  i.e.  binary segmentation  or a specific pre-determined segmentation that the agent could have inherited through evolutionlike processes. this opens up the interesting possibility of using genetic algorithms  dejong  1  to study the development of elementary feature sets. 
   w i t h o u t specifying the details of classification and action selection  cf.  carlson et a/.  1  or  haider  1  for possible approaches   we now want to briefly sketch the basic idea of a demand-driven feature refinement strategy. whenever the system has classified a situation into its conceptual hierarchy  it executes the action that is associated with the chosen concept  and expects a positive feedback in the next sampling interval. if a negative feedback is observed instead  the system must modify its concept hierarchy by splitting the chosen concept in such a way that the erroneous action is no longer predicted. this corresponds to splitting an overly general rule into several more specific rules. 
   splitting a concept node  however  is possible only if the existing concept is not maximally special yet  i.e.  if the chosen concept is not a completely specified leaf of the hierarchy. otherwise  a refinement of the existing features is necessary: the segmentation rules in the preprocessors are changed so that the segments corresponding to the attribute values of the current situation are split in half. 
   the reverse process  i.e.  a coarsening of the available vocabulary  can be performed by analyzing the use of attribute values in the concept hierarchy  and combining the segments of those neighboring values that represent noninformative distinctions1. 
   based on the above or similar processes of vocabulary refinement and coarsening  the system develops a symbolic vocabulary that is exactly matched to its task. 
1 	related work 
the approach to grounded concept formation presented here contrast with other proposals eg. by harnad  and cottrell et ai  that use connectionist networks to learn associations between pictorial stimuli and symbols. our model cannot at present handle one- or twodimensional sensor input  and cannot model the categorical perception effects that hamad's model exhibits. on the other hand  both connectionist models require hundreds of passes through a training set to learn the required associations  and thus need input that cannot possibly come from an actual environment. furthermore  neither one of the other two models address the question of where the categories or symbols that the network is to learn come from  and do not attempt to embed the grounding process into the context of goal-oriented activity of an agent. 
in its emphasis on a complete model of an agent  the 
1
　　 a distinction is non-informative if the conditional probability of a  predicted  effector value given an attribute value is identical or nearly identical for both values. in a probabilistic concept hierarchy  this can be computed from the attribute probabilities stored with the concepts. 
	wrobel 	1 
model presented here is similar to existing cognitive architectures like s o a r  laird et al  1   none of which address the grounding issue  however. in turn  to be able to concentrate on symbol grounding  our model is much simpler in its treatment of goal-oriented activity and does not tackle the problem of planning at all; important questions with respect to action selection are left open. langley et al.  present a complete model of an intelligent robot with sensing  planning  and action that uses a very similar clustering approach as presented here  but does not specifically address the issue of symbol generation or destruction. 
   the process control research framework is of course not new; a great body of research exists in traditional control theory and also in a i . specifically  our framework is very similar to the reinforcement learning paradigm used eg. in  kaelbling  1  and  whitehead and ballard  1 . the emphasis there  however  is on action selection and not on issues of representation development. 
   finally  it is important to point out the difference between our approach and learning systems that directly use numerical attributes in their concept hierarchies  such as u n i m e m  lebowitz  1   c l a s s i t  gennari et al.  1   and many of the decision tree systems of the i d 1 / c a r t family. while these systems do perform implicit segmentation of numeric attributes locally in the nodes of their hierarchies  they do not acquire an explicit  global symbolic vocabulary. most importantly  those methods store and carry along all of the original sensor data for each example. this does not correspond well with the available psychological evidence  and contrasts sharply with the strongly incremental approach taken here which assumes that most of the original sensor data are discarded right at the preprocessing stage  and are not available later on. the resulting suppression of irrelevant data right at the sensor level might also lead to conceptual structures that are simpler and more efficient than those obtained without a presegmentation stage. 
1 	conclusion 
in this paper  we have argued for a more encompassing framework in which to study concept formation processes. we have detailed why the issue of grounding meaning is important for a full understanding of concept formation  and why it needs to be studied in the context of a complete agent model with goal-oriented activity and true access to the world. 
   we have proposed a simple scenario in which we believe the problem of grounding meaning can be studied. this framework has important limitations that should be emphasized again: it does not include motion of the agent in a three-dimensional world  which may well be a key ingredient of  human  concept formation  and in its present form allows only simple real-valued or nominal sensors. at the same time  it is these very restrictions that allow an examination of the issue of symbol grounding and representation development without having to solve all existing problems of ai first. 
finally  we have sketched a model of an agent that 
1 	learning and knowledge acquisition 
might be able to learn to operate successfully in the simple world we have set up for it. the model is top-down in nature  and builds on existing symbolic concept formation work. it contains a demand-driven technique for introducing truly new symbols based on sensory input  and a complementary mechanism for discarding unnecessary distinctions. it has important shortcomings  among others the inability to deal with higher-dimensional input sensors. 
   in the future  we want to elaborate on those parts of the model that are still sketchy  and test its ability to develop a vocabulary in an actual simple process control task. the complexity of environments this model will be capable of learning will depend to a large degree on the intelligence of the yet unspecified action modifier module. improvements in the scope of the model will have to wait until we fully understand concept formation and the origin of symbols in this simple scenario. 
a c k n o w l e d g e m e n t s 
the author wishes to acknowledge the helpful comments of c. haider  a. horz  c. lischka  dr. k. morik  and u. schnepf that helped shape this paper. 
