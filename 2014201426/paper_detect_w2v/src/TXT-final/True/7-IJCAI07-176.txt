
the training of support vector machines  svm  involves a quadratic programming problem  which is often optimized by a complicated numerical solver. in this paper  we propose a much simpler approach based on multiplicative updates. this idea was first explored in  cristianini et al.  1   but its convergence is sensitive to a learning rate that has to be fixed manually. moreover  the update rule only works for the hard-margin svm  which is known to have poor performance on noisy data. in this paper  we show that the multiplicative update of svm can be formulated as a bregman projection problem  and the learning rate can then be adapted automatically. moreover  because of the connection between boosting and bregman distance  we show that this multiplicative update for svm can be regarded as boosting the  weighted  parzen window classifiers. motivated by the success of boosting  we then consider the use of an adaptive ensemble of the partially trained svms. extensive experiments show that the proposed multiplicative update rule with an adaptive learning rate leads to faster and more stable convergence. moreover  the proposed ensemble has efficient training and comparable or even better accuracy than the best-tuned soft-margin svm.
1	introduction
kernel methods  such as the support vector machines  svms   have been highly successful in many machine learning problems. standard svm training involves a quadratic programming  qp  problem  which is often solved by a complicated numerical solver. moreover while a general-purpose qp solver can be used  they are inefficient on this particular type of qps. consequently  a lot of specialized optimization techniques have been developed. the most popular approach is by using decomposition methods such as sequential minimization optimization  smo   platt  1   which involves sophisticated working set selection strategies  advance caching schemes for the kernel matrix and its gradients  and also heuristics for stepsize prediction.
¡¡a much simpler approach is to use multiplicative updates  which was first explored in  cristianini et al.  1 . however  its convergence is sensitive to a learning rate that has to be fixed manually. when the learning rate is too large  overshooting and oscillations occur; while when it is too small  convergence is slow.
¡¡a second problem with the multiplicative update rule is that it can only work with the hard-margin svm. however  on noisy data  the hard-margin svm has poor performance and the soft-margin svm  which can trade off complexity with training error  is usually preferred. however  the choice of this tradeoff parameter  which can be from zero to infinity  is sometimes critical. various procedures have been proposed for its determination  such as by using cross-validation or some generalization error bounds  evgeniou et al.  1 . however  they are typically very computationally expensive.
¡¡another possibility that avoids parameter tuning completely is by using the ensemble approach.  kim et al.  1  combines multiple svms with different parameters  and obtains improved performance. however  as a lot of svms still have to be trained  this approach is also very expensive.
¡¡in this paper  we first address the learning rate problem by formulating the multiplicative update of svm as a bregman projection problem  collins and schapire  1 . it can then be shown that this learning rate can be adapted automatically based on the data. moreover  because of the connection between boosting and bregman distance  collins and schapire  1; kivinen and warmuth  1   we show that this multiplicative update can be regarded as a boosting algorithm in which the  weighted  parzen window classifiers are the base hypotheses  and each base hypothesis added to the ensemble is a partially trained hard-margin svm. now  in the boosting literature  it is well-known that the whole ensemble performs better than a single base hypothesis. hence  to address the possibly poor performance of the hard-margin svm  we consider using the whole ensemble of partially trained svms for prediction. note that this comes at little extra cost in the multiplicative updating procedure. experimentally this ensemble is observed to have comparable or even better accuracy than the best-tuned soft-margin svm.
¡¡the rest of this paper is organized as follows. the learning rate problem in multiplicative update is addressed in section 1. the connection between this update process and boosting  together with the proposed ensemble of partially trained hard-margin svms  is then discussed in section 1. experimental results are presented in section 1  and the last section gives some concluding remarks.
1	multiplicative updating rule of svm
in this paper  we focus on a particular variant of the hardmargin svm  called ¦Ñ-svm in the sequel  which will be introduced in section 1. we then develop in sections 1 and 1 an iterative multiplicative updating procedure based on the bregman projection  collins and schapire  1; kivinen and warmuth  1 . the convergence of this iterative procedure is shown in section 1.
1	hard-margin ¦Ñ-svm
given a training set  where xi ¡Ê rd and yi ¡Ê r  the ¦Ñ-svm finds the plane in the kernelinduced feature space  with feature map    that separates the two classes with maximum margin:

the corresponding dual is:
	 	 1 
where 1 and 1 are vectors of zero and one respectively  ¦Á = is the vector of lagrange multipliers  and k  =  yiyjk xi xj  . it can be easily shown that
	m	m
	w = x¦Áiyi  xi   and f x  = x¦Áiyik xi x .	 1 
	i=1	i=1
 because of the zero duality gap  the objectives in  1  and  1  are equal at optimality1  i.e. 
	.	 1 
denote
 
where ¦Ñ¡¥ is the lagrangian multiplier for the dual constraint
. moreover  the karush-kuhn-tucker  kkt  conditions lead to:
	 	 1 
where ¡Í denotes that the two vectors are orthogonal  and

on using  1 . moreover  the optimal ¦Ñ¡¥ i.e.  the dual variable
of the dual  is indeed equal to the optimal primal variable ¦Ñ:
¦Ñ¡¥  = ¦Ñ  
as on using  1    1  and  1 .
1	bregman projection
in this section  we assume that we have access to the optimal value of ¦Ñ . we will return to the issue of determining ¦Ñ  in section 1.
¡¡as ¦Á ¡Ê   the mdimensionalprobabilitysimplex  a natural choiceof the bregman distance is the  unnormalized  relative entropy. at the tth iteration  we minimize the bregman distance between the new ¦Á and the current estimate   while requiring ¦Á to satisfy  which is similar to the constraint in  1 . we then have the following entropy projection problem:

¡¡introducing lagrange multipliers for the constraints ¦Ñ  =  and   it will be shown that the remaining constraint ¦Á ¡Ý 1 is inactive and so no lagrange multiplier is needed  and on setting the derivative of the lagrangian
w.r.t. ¦Á to zero  we have ¦Ái = ¦Á it  exp  ¦Çtyift xi
¦Ë  on using  1 .	substituting this back into 
1  we then obtain ¦Ë	=	 lnzt ¦Çt   where zt ¦Çt 	=
  and thus ¦Á it+1  = ¦Á it  exp  ¦Çtyift xi  /zt ¦Çt .  1 
such an update is commonly known as multiplicative update  cristianini et al.  1  or exponentiated gradient  eg   kivinen and warmuth  1   with ¦Çt playing the role of the learning rate. notice that by initializing1 ¦Á1   1  then ¦Át   1 in all subsequent iterations. hence  as mentioned earlier  ¦Á ¡Ý 1 in  1  is an inactive constraint. finally  it can be shown that ¦Çt can be obtained from the dual of  1   as:
	max  ln zt ¦Çt exp ¦Ñ ¦Çt  .	 1 
¦Çt
¡¡as mentioned in section 1  a similar multiplicative updating algorithm for the standard hard-margin svm is also explored in  cristianini et al.  1 . however  the learning rate ¦Ç there is not adaptive and the performance is sensitive to the manual choice of ¦Ç. on the other hand  for our ¦Ñ-svm variant  the value of ¦Çt can be automatically determined from  1 . the improved convergenceproperties of our adaptive scheme will be experimentally demonstrated in section 1.
1	estimating the value of ¦Ñ 
we now return to the question of estimating ¦Ñ . recall that we initialize ¦Á as ¦Á1 = m1. from  1  and  1   ¦Ñ  is equal to the optimal dual objective  which is to be minimized   and so the optimal ¦Á  should yield a smaller objective than that by ¦Á1  which is equal to . moreover  from  1   we have ¦Ñ  ¡Ý 1 as k  1. hence  ¦Ñ  ¡Ê  1 ¦Ñ¡¥1 .
¡¡in the following  we consider using a decreasing sequence of ¦Ñt's such that ¦Ñ¡¥1 ¡Ý ¦Ñt ¡Ý ¦Ñ . consequently  the constraint in  1  has to be replaced by  and the
entropy projection problem becomes

 let. we first consider the feasibility of  1 . proposition 1. for any satisfies
	 	 1 
then there exists an ¦Á ¡Ê pm such that .
proof. since k   we have v for any vector v. put v = ¦Át   ¦Á   then
	.	 1 
from  1   the condition

summing  1  and  1   we have
	 	 1 
and thus ¦Á = ¦Á  satisfies the condition.	
¡¡hence  when  a feasible solution of  1  exists. we set ¦Át+1 as the optimal ¦Á of  1 . notice that  1  differs from  1  only in that the equality constraint in  1  now becomes an inequality constraint. hence  optimization of  1  is almost exactly the same as that in section 1  except that optimization of ¦Çt now has an extra constraint ¦Ç ¡Ý 1.
subsequently  we set  and ¦Ñt+1 ac-
cording to
	.	 1 
from the kkt condition of  1   .
this  together with1 ¦Çt   1  leads to
	.	 1 
using  1   then
	 	 1 
where ¦Øt t+1 is the angle between wt and wt+1. recall that this entropy projection procedure is used to solve the dual in  1 . when  1  starts to converge  the angle ¦Øt t+1 tends to zero  and cos¦Øt t+1 ¡ú 1. as  from
 1   the optimization progress becomes deteriorated. this implies that the current t is too large  and we have overshot and this solution is discarded. this is also the case when  1  is infeasible. instead  we can relax the constraint in  1  by setting to obtain a larger ¦Ñt. this is repeated until ¦Ñ¡¥t+1 ¡Ü ¦Ñ¡¥t. note that we always have ¦Ñ  ¡Ü ¡¤¡¤¡¤ ¡Ü ¦Ñ¡¥t+1 ¡Ü ¦Ñ¡¥t  and ¦Á  is optimal. optimization of  1  then resumes with the new ¦Át+1 and ¦Ñt+1. the whole process iterates until t is smaller than some termination threshold . the complete algorithm1 is shown in algorithm 1.
algorithm 1 training the ¦Ñ-svm.

1: input: s = { xi yi }i=1 ... m  tolerance 1 and .
1: initialize t = 1: ¦Á 1 i	= 1/m for all i = 1 ... m  and
.
1:.
1: set	t = ¡¥t  1 + t .
1: set ¦Çt = argmin¦Ç¡Ý1 zt ¦Ç exp ¦Ñt¦Ç . if ¦Çt   1  then
  and goto step 1 to test whether t becomes too small; else if ¦Çt = 1  then set t = t   1 and terminate.
1: update the lagrangian multipliers ¦Ái's using  1   as   and compute
+1
1: if ¦Ñ¡¥t ¡Ý ¦Ñ¡¥t+1  then and t ¡û t + 1 and goto step 1; else and check whether.
1:   then goto step 1; else set t = t   1 and terminate.
1: output: fsvm x  = ft+1 x .

1	convergence
 in this section  we will design an auxiliary function  which is then used to lower bound the amount that the loss decreases at each iteration  collins and schapire  1 . denote the relative entropy  which is our bregman distance here  by¦Á ¦¤ ¡¤ ¡¤ . from  1   the optimal   solution of  1  is in the intersection of all the hyperplanes defined by the linear constraints  for all t's. using  1    1  and  1   we have
	.	 1 
now  the decrease in loss at two consecutive iterations is:

		 using  1  
		 using  1  
or  for all t 
 ¦¤ ¦Á  ¦Át    ¦¤ ¦Á  ¦Át+1  ¡Ý ¦¤ ¦Át+1 ¦Át  ¡Ô a ¦Át    1  where a ¡¤  is the auxiliary function. as a ¡¤  never increases and is bounded below by zero  as¦Á ¦¤ ¦Á¡¤ ¡¤  ¡Ý 1   the sequence of a  t 's converges to zero. since ¡Ê¦Ápm is compact  by the continuity of a and the uniqueness of    the limit of this sequence of minimizers converges to the global optimum .
moreover  using  1  and  1   we have ¦Át+1 k ¦Át =
. as   then . in the special case where. using the ho¡§lder's inequality  we have .
we then apply the pinsker's inequality1  fedotov et al.  1  and obtain
.
let  and summing up  1  at each step  then
 
and so the number of iterations required is at most t =  for a fixed . this gives a faster finite convergence result than the linear asymptotic convergence of the smo algorithm  lin  1 .
¡¡assuming that  1  is satisfied for all t and following the convergence proof here1  the solution of the ¦Ñ-svm's dual 
¦Át ¡ú ¦Á  in at most t iterations such that ¦Át+1k t+1 ¡ú ¦Ñ . thus  the final solution is the desired ¦Ñ-svm classifier.
1	ensemble of partially trained svms
there is a close resemblance between the update rules of the ¦Ñ-svm with variants of the adaboost algorithm. in particular  we will show in section 1 that the algorithm in section 1  with ¦Ñ  assumed to be known  is similar to the adaboost algorithm  algorithm 1 ; while that in section 1  with ¦Ñ  unknown  is similar to the adaboost ¦Í algorithm  ra¡§tsch and warmuth  1 . inspired by this connection  in section 1  we consider using the output of the boosting ensemble for improved performance. the time complexity will be considered in section 1.

algorithm 1 adaboost  ra¡§tsch and warmuth  1 

1: input: s = { xi yi }i=1 ... m; number of iteration t.
1: initialize: d 1 i	= 1/m for all i = 1 ... m.
1: for t = 1 ... t do
1: train a weak classifier w.r.t. the distributiondi for each pattern xi to obtain a hypothesis ht ¡Ê   1 .
1:	set
	ct = argmin	 	 1 
where.
1:	update distributions di:
	d it+1 	=	d it  exp  ctyiht xi  /nt ct . 1 
1: end for
1: output:.
are identical  with ¦Á it  playing the role of d it   compare  1  and  1    and ¦Çt the role of ct  compare  1  and  1  . this is a consequence of the fact that boosting can also be regarded as entropy projection  kivinen and warmuth  1 . from this boosting perspective  we are effectively using base hypothesis of the form  1   with¦Á ¦Á ¡Ê pm  not necessarily equal to   . this base hypothesis can be regarded as a variant of the parzen window classifier  with the patterns weighted by ¦Á. note that the edge1 of the base hypothesis at the t-th iteration is. thus  the constraint in  1  is the same as requiring that the edge of the base hypothesis w.r.t. the ¦Ái distribution to be ¦Ñ .
¡¡the same correspondence also holds between the ¦Ñ-svm algorithm  algorithm 1  and the adaboost ¦Í algorithm1  with additionally that ¦Ñt in ¦Ñ-svm is analogous to t in adaboost ¦Í. moreover   in the ¦Ñ-svm is similar to ¦Í in adaboost ¦Í in controlling the approximation quality of the maximum margin. note  however  that ¦Í is quite difficult to set in practice. on the other hand  our t can be set adaptively.
¡¡there have been some other well-known connections between svm and boosting  e.g.   ra¡§tsch  1  . however  typically these are interested in relating the combined hypothesis with the svm  and the fact that boosting uses the l1 norm while svm uses the l1 norm. here  on the other hand  our focus is on showing that by using the weighted parzen window classifier as the base hypothesis  then the last hypothesis is indeed a svm.
1	using the whole ensemble for prediction
as mentioned in section 1  the hard-margin svm  which is the same as the last hypothesis  may have poor performance on noisy data. inspired by its connectionwith boosting above  we will use the boosting ensemble's output for prediction:
 
which is a convex combination of all the partially trained svms. note that this takes little extra cost. as evidenced in the boosting literature  the use of such an ensemble can lead to better performance  and this will also be experimentally demonstrated in section 1.
1	computational issue
in  1   each step of the eg update takes o m1  time  which can be excessive on large data sets. this can be alleviated by performing low-rank approximations on the kernel matrix k  fine and scheinberg  1   which takes o mr1  time  r is the rank of k . then  the eg update and computation of ¦Ñ¡¥t both take o mr   instead of o m1   time.
1	experiments
in this section  experiments are performed on five small
 breast cancer  diabetis  german  titanic and waveform 1 and
seven medium-to-large real-world data sets  astro-particles  adult1  adult1  adult1  adult  web1 and web1  table 1 .
table 1: a summary of the data sets used.
data setdim# training patterns# test patternscancer11diabetis11german11titanic11waveform11astro1 1 1adult111adult111adult111adult1 1 1web111web1111	adaptive learning rate
we first demonstrate the advantages of the adaptive learning rate scheme  in section 1 . because of the lack of space  we only report results on breast cancer and waveform. similar behavior is observed on the other data sets. for comparison  we also train the ¦Ñ-svm using the multiplicative updating rule  with fixed ¦Ç  in  cristianini et al.  1 .
¡¡as can be seen from figure 1  the performance of  cristianini et al.  1  is sensitive to the fixed choice of ¦Ç. when ¦Ç is small  e.g.  ¦Ç = 1 or smaller   the objective value decreases gradually. however  when ¦Ç is slightly larger  say  at 1   the estimate will finally over-shoot  as indicated by the vertical lines  and convergencecan no longer be obtained. on the other hand  our proposed adaptivescheme always converges much faster.

figure 1: comparing the eg update  with fixed ¦Ç  with our adaptive scheme.
1	accuracy and speed
next  we show that the proposed ensemble of partially trained hard-margin svms  section 1  has comparable performance as the best-tuned soft-margin svm  c-svm . we use the gaussian kernel  with . each small data set in
table 1 comes with 1 realizations  and the performance is obtained by averaging over all these realizations. however  for each large data set  only one realization is available. the ensemble is compared with the following:
1. svm  best : the best-performing svm among all thec-svms obtained by the regularizationpath algorithm1  hastie et al.  1 ;
1. svm  loo : the c-svm which has the best leave-oneout-errorbound  evgeniouet al.  1  among those obtained by the regularization path algorithm;
1. the parzen window classifier  which is the first hypothesis added to the ensemble;
1. the  hard-margin  ¦Ñ-svm  which is the last hypothesis;
1. the proposed ensemble.
all these are implemented in matlab1 and run on a 1ghz pentium-1 machine with 1gb ram.
table 1: testing errors  in %  on the different data sets.
svmsvmparzen¦Ñ-proposeddata set best  loo windowsvmensemblecancer11111diabetis11111german11111titanic11111waveform11111astro11111adult1.1.1.1.1.1adult1.1.1.1.1.1adult1.1.1.1.1.1web1.1.1.1.1.1web1.1.1.1.1.1¡¡as can be seen from table 1  the ensemble performs much better than the  hard-margin  ¦Ñ-svm. indeed  its accuracy is comparable to or sometimes even better that of svm best . table 1 compares the time1 for obtaining the ensemble with that of running the regularization path algorithm. as can be seen  obtaining the ensemble is much faster  and takes a small number of iterations. to illustrate the performance on large data sets  we experiment on the full adult data set in table 1. the regularization path algorithm cannot be run on this large data. so  we use instead a ¦Í-svm  using libsvm1  for comparison  where ¦Í  unlike the c parameter in the c-svm 

1
¡¡¡¡alternatively  one can find the best-tuned svm by performing a grid search on the soft-margin parameter. however  as each grid point then requires a complete re-training of the svm  the regularization path algorithm is usually more efficient  hastie et al.  1 . 1
¡¡¡¡the inner qp in the regularization path algorithm is solved by using the optimization package mosek  http://www.mosek.com . 1
¡¡¡¡this includes the time for computing the kernel matrix  which can be expensive on large data sets. moreover  time for computing the leave-one-out bound is not included in the timing of svm loo . 1
	version	1.1	 c++	implementation 	from
. http://www.csie.ntu.edu.tw/ cjlin/libsvm/.
table 1: training time  in seconds  and number of iterations.
regularization pathproposed ensembledata settimetime#iterationscancer111diabetis111german111titanic111waveform111astro111adult1.1.1adult1.1.1adult1.1.1web1 111web1 111can only be in  1 . in the experiments  we further restrict ¦Í to  1 1  as the optimization becomes infeasible when ¦Í falls outside this range. moreover  low-rank approximation of the kernel matrix as mentioned in section 1 is used.
¡¡as can be seen from figure 1  training of the single besttuned ¦Í-svm takes 1s  while training the whole ensemble takes 1.1s  which is thus around 1 times slower. however  recall that ours is implemented in matlab while libsvm is in c++. moreover  as shown in figure 1 b   svm's training time is highly dependent on the value of ¦Í. hence  in practice  one has to perform svm training together with ¦Í parameter selection  which can be much more expensive than ensemble training.

figure 1: training of the ensemble vs training of a single ¦Ísvm on the adult data.
1	conclusion
in this paper  we show that the hard-margin svm can be trained by a simple multiplicative updating rule. by casting its training as a bregman projection problem  the optimal learning rate can be adaptively obtained from the data  and a finite convergence result can also be derived. moreover  inspired from a connection between boosting and bregman projection  we propose an ensemble classifier consisting of the partially trained hard-margin svms. though its base hypothesis are hard-margin svms  experimental results show that this ensemble is resistant to overfitting and has comparable performancewith the best-tunedsoft-marginsvm. moreover  training the whole ensemble  which in matlab  is only 1 times slower than that of training the best-tuned svm  in c++ . thus  we can obtain an accurate classifier while avoiding tedious tuning of the soft-margin parameter.
¡¡one problem with the ensemble is that its solution is no longer sparse. in the future  we will investigate the use of reducd set methods to alleviate this.
acknowledgments
this research has been partially supported by the research grants council of the hong kong special administrative region.
