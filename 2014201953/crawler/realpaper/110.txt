Choosing between heuristics and strategies: an enhanced model for

decision-making ∗

Shavit Talman, Rotem Toister, Sarit Kraus

Department of Computer Science, Bar-Ilan University

Ramat-Gan,52900, Israel

{toistet, talmans, sarit}@cs.biu.ac.il

Abstract

Often an agent that has to solve a problem must
choose which heuristic or strategy will help it the
most in achieving its objectives. Sometimes the
agent wishes to obtain additional units of informa-
tion on the possible heuristics and strategies in or-
der to choose between them, but it may be costly.
As a result, the agent’s goal is to acquire enough
units of information in order to make a decision
while incurring minimal cost. We focus on situa-
tions where the agent must decide in advance how
many units it would like to obtain. We present an
algorithm for choosing between two options, and
then formulate three methods for the general case
where there are k > 2 options to choose from. We
investigate the 2-option algorithm and the general
k-option methods effectiveness in two domains: the
3-SAT domain, and the CT computer game. In both
domains we present the experimental performance
of our models. Results will show that applying the
2-option algorithm is beneﬁcial and provides the
agent a substantial gain. In addition, applying the
k-option method in the domains investigated results
in a moderate gain.

Introduction

1
When making decisions, automated agents need to decide
which heuristic function or strategy would best assist them
in achieving their objectives.
In particular, when an agent
possesses several alternatives to tackle a problem, and the
best alternative is unknown in advance, its success depends
In [Azoulay-
on choosing the most beneﬁcial alternative.
Schwartz and Kraus, 2002] a formal model for solving a prob-
lem of choosing between alternatives in e-commerce have
been presented. They formulated the problem in terms of
units of information and agent’s utility, and assumed that the
agent should decide in advance on how many information
units to obtain about each alternative. Moreover, the agent
could not change its decision during the information obtain-
ing process. Then they suggested an algorithm for acquiring
∗This work was supported in part by NSF grant no. IIS-0208608
and by ISF grant no. 1211-04. Sarit Kraus is also afﬁliated with
UMIACS.

an optimal amount of information units, through which the
best alternative could be determined.

In this paper, we generalize their model to suit domains that
involve choosing between heuristics or strategies. Azoulay-
Schwartz and Kraus’ assumptions hold in this research as
well: the agent chooses the best alternative in advance and
is able to hold onto its decision for a long period of time.
This approach is valid in many situations, especially when ac-
quiring information is costly. In these cases, the agent would
like to execute its decision-making process once and apply
the process’s output thereafter. Moreover, once the agent re-
alizes the decision made does not produce satisfactory results
at any given time, it may re-execute the decision-making pro-
cess, and proceed with its new output. Nevertheless, we are
more concerned with the applicative rather than the theoreti-
cal aspect of the model. Indeed, the model suggests many im-
plementation challenges. We present two examples, from the
heuristics domain and from the computer games domain. In
both domains we compare the utility of the automated agent
with and without using our algorithm. Results will show that
an agent that applies our model is more likely to choose the
best option among the group of possible options. Further-
more, it does not waste too many resources during the deci-
sion process, and thus, its overall utility increases.

The paper ﬁrst brieﬂy presents the model constructed by
Azoulay-Schwartz and Kraus.
It then introduces the algo-
rithm for choosing between two options, which was formu-
lated by Azoulay-Schwartz and Kraus, and thereafter the
three generalized methods for cases where there are more
than two options. The ﬁrst of these three methods is a the-
oretical one which was formulated by Azoulay-Schwartz and
Kraus. We generalized their ideas to form the other two meth-
ods, which are much more feasible in real-time environments.
We proceed by describing the experimental design and anal-
ysis. In section 4 we describe the ﬁrst domain we considered
- the 3-SAT domain, and in the following section we describe
the other domain, the CT game domain. In both sections we
present the experiments we conducted and the outcomes of
applying the 2-option algorithm and the k-option generalized
models. In section 6 we compare our work with previous re-
searches. The concluding section discusses the experimental
results and suggests several avenues for future research.

2 Model construction
A risk neutral agent has to choose a heuristic from k heuristics
to solve M problems. After choosing alternative i, the agent
will obtain a value of xi which is unknown in advance. We as-
sume that xi is normally distributed, that is, xi ∼ N(µi, σ2
i ).
In addition, the agent does not know µi, but it has some prior
beliefs about its distribution. We further assume that µi also
follows the normal distribution - µi ∼ N(ζi, τ 2
i ). We assume
that σi, ζi and τi are known and reﬂect the agent’s believes.
Otherwise, if σi is unknown, the agent can use The student
distribution. In addition, if ζi and τi are unknown, we assume
that the agent can estimate their values based on its beliefs as
to the possible values of µi. Furthermore, in future work we
will investigate the effect of eliminating normal distribution
assumptions.

The agent has ni units of information about each alter-
native i, with an average value of xi. For instance, in the
e-commerce example presented by Azoulay-Schwartz and
Kraus, a customer would like to buy an item available from
two suppliers. The customer collected n1 and n2 customer-
impressions from friends or from the web about these suppli-
ers, in order to be able to decide between them. The average
impression was calculated to form x1 and x2.

The agent is able to obtain a combination of comb =
(m1, . . . , mk) additional units about the different alterna-
tives. However, this operation is costly, either in time or in
direct costs. Given 0 < δ ≤ 1, the discount time factor,
the cost model, the list of alternatives and the parameters for
each alternative, the agent decides whether to proceed with
the information it has so far, or to accumulate additional in-
formation units. One could simply use a greedy algorithm in
order to ﬁnd the optimal allocation of additional experiments.
In each step the option which proved to be better so far is cho-
sen, executed and its result is added to the data accumulated.
The greedy algorithm stops when there is no additional sam-
ple that increases the expected utility. However, [Azoulay-
Schwartz and Kraus, 2002] showed that the greedy algorithm
is not optimal. The reason being that there may be situations
where obtaining one unit of information about a particular al-
ternative is not worthwhile since it will not lead to a change
in the decision, but obtaining two and more may be worth-
while. In conclusion, ﬁrst we will present an algorithm for
k=2 heuristics (or strategies) and then generalize it for k > 2.

Algorithm 1 The HSC Algorithm
for mA from 0 to ∞ do

for mB from 0 to ∞ do

calculate Fchange.
calculate Utility.
if U tility(mA − 1, mB − 1) > U tility(mA, mB ) and
U tility(mA − 1, mB − 1) > U tility(mA − 2, mB − 2) then

return (mA − 1, mB − 1).

Add mi experiments to alternative i.
Choose the best alternative according to the new results

2.1 Choosing Between Two Heuristics
Suppose the agent has to decide between alternatives A and
B. Currently, xA > xB. Since the agent is risk neutral, al-
ternative A will be chosen if no additional information is ob-
tained. Firstly, the probability of changing the winning option

(cid:82)

is F change(µA, µB, σA, σB, xA, xB, nA, nB, mA, mB) =
P r(Z > Zα), where Z is a random variable, having the
standard normal distribution and Zα represents the ﬁrst value
where B outperforms A (see [Azoulay-Schwartz and Kraus,
(cid:82)
2002] for more information). Then,
the expected bene-
ﬁts from obtaining additional mA and mB is a function of
Fchange and is denoted by benef its(mA, mB). The agent’s
µB benef its(mA, mB) ·
utility is utility(mA, mB) =
δmA+mB−cost(mA, mB)dµBdµA. The speciﬁcs of the ben-
eﬁts function depend on the cost model. Later we will show
the speciﬁcations for three different cost models. The agent’s
goal is to ﬁnd the pair mA and mB, that yields the highest
utility(mA, mB) value. By considering all possible combi-
nations of information units about A and B, the optimal com-
bination is achieved. Algorithm 1 presents the Heuristics-
Strategies-Choosing (HSC) Algorithm for the two heuristics
(or strategies) case.
Proposition 1. The HSC algorithm stops and returns mA and
mB that maximize the utility function.

µA

Proof. The intuition for the correctness of the algorithm lies
in the form of the utility function. Since it consists of the mul-
tiplication of similar normal distribution functions, its form is
Gaussian as well, with only one maximum point. As soon
as the algorithm ﬁnds that point it stops and returns mA and
mB.

2.2 Choosing Between Multiple Heuristics
There are k > 2 alternatives, and the agent can obtain up
to M-1 units of information about each of them. We suggest
three models:

1. The Statistical model in which we considered the agent
utility function given a combination of information units
as suggested by [Azoulay-Schwartz and Kraus, 2002].
Nevertheless,
in order to ﬁnd a combination of ad-
ditional units for each alternative, one must solve a
quadruple integral. As a result, the model proved to be
inapplicable for our purposes;

2. The Binary Tree model where the alternatives construct
the leaves’ level of the tree. We apply the HSC algo-
rithm for each pair. The winning alternative goes up a
level in the tree. We repeat the procedure until the best
alternative reaches the root. This model deviates from
our initial assumption that all experiments are conducted
prior to decision-making. The binary tree model is an in-
termediate approach between the greedy algorithm and
the HSC algorithm. However, since the comparisons are
done in pairs, a large number of experiments may be ex-
ecuted for alternatives that would not have been consid-
ered at all when regarding all the alternatives together;

3. The Fixed number of experiments model - in which we
distribute a ﬁxed number of experiments denoted by (N)
between the different alternatives using the information
we have so far. For example, suppose we have ﬁve alter-
natives to choose from. Alternative 1 produces the best
results and alternative 5 produces the worst ones. Thus,
it is worthwhile to execute more experiments of alterna-
tive 1 than of alternative 5, as less time will be wasted

on fewer alternatives while accumulating information on
all possible alternatives. Following preliminary experi-
ments, we set N to be four times the number of alterna-
tives. Thus, it is large enough to accommodate exper-
iments of all alternatives and nonetheless, economical
in the number of additional experiments. Consequently,
for the example of ﬁve alternatives N=20, and we sug-
gest that the best alternative be assigned one third of N.
The rest will be assigned two thirds of the remaining
3] = 7; m2 = [13 · 2
experiments: m1 = [20 · 1
3] = 5;
3] = 2 and m5 = [1· 2
m3 = [8· 2
3] = 3; m4 = [3· 2
3] = 1.
This ad-hoc approach will be reﬁned in future work.

3 Experimental design and analysis
Our investigation using the HSC algorithm was conducted
in two domains. The ﬁrst, a classic NP-complete problem,
i.e. the 3-SAT problem. To demonstrate the HSC algorithm’s
vast usage possibilities, it was employed in two different cost
models:

1. Minimal Time (MT) scenario - the agent had to solve M

formulas as quickly as possible;

2. Maximal Formulas (MF) scenario - the agent had T units

of time to solve as many formulas as possible.

We chose three best-known search algorithms as the
agent’s possible heuristics: Greedy-SAT (GSAT), Simulated
Annealing (SA) and GSAT with Random Walk. The agent
had to perform its task by using one of these search algo-
rithms. The second domain was a computer game, the CT
game (for game speciﬁcations see [Grosz et al., 2004]). Here,
the agent’s task was to maximize its game score. To this end,
our agent had seven different strategies to employ against its
opponent agent in the game, and it had to choose the best
strategy against the opponent. The experiments compared
the agent’s achievements without using the HSC algorithm
and its achievements after executing this algorithm. We also
compared these results with methods where a large set of ad-
([Selman et al.,
ditional experiments had been conducted.
1993] for instance conducted an unlimited number of exper-
iments in order to choose the best heuristic). These methods
found the best heuristic more frequently than our algorithm.
Nevertheless, though the decisions made were slightly im-
proved, given our cost model, the approach applied by Sel-
man yielded a huge loss in the number of experiments exe-
cuted. For space reasons, these results are not detailed here.
We ﬁrst executed preliminary runs to prepare an ofﬂine
database. The database comprised vital information, namely,
which option is the best, and moreover, the mean quality
of each option. Our hypothesis was that the execution of
the HSC algorithm will indeed beneﬁt the agent. Moreover,
by executing a small number of additional experiments the
agent’s utility will increase.

4 The 3-SAT domain
We assumed that each Truth-assignment ﬂip takes one unit of
time. Thus, the MT scenario in this domain is a minimum
problem: the agent must solve M formulas within a minimal
number of ﬂips. Without loss of generality, when comparing

two possible heuristics, we assumed that the better heuris-
tic after nA experiments is heuristic A. As a result, the agent
would have chosen heuristic A for the M formulas. In that
case, it would have taken the agent the average number of
ﬂips multiplied by the number of formulas, M · µA, to solve
M formulas. After the HSC algorithm is executed, the total
number of ﬂips will be assembled from,

1. The number of ﬂips in the mA + mB extra experiments
2. The number of ﬂips to solve the remaining M − mA −

yielded by the HSC algorithm;

mB formulas in case heuristic B prevails;

3. The number of ﬂips to solve the remaining formulas in

case heuristic A still leads.

(cid:82)

(cid:82)

µA

µA

µB

µB

the

this

(cid:82)
(cid:82)

(cid:82)
(cid:82)

function

utility
=

δ in this case is 1,
since the agent solves the re-
quired formulas in the additional experiments.
Ac-
domain
cordingly,
in
is
·
utility(mA, mB)
benef its(mA, mB)
δmA+mB − cost(mA, mB)dµBdµA =
M · µA −
F change(M −mA−mB)·µB−(1−F change)(M −mA−
mB)·µA−(mAµA+mBµB)dµBdµA =
(M−mA−
mB)· F change· (µA− µB)− mB ·(µB − µA)dµBdµA. The
agent searches for mA and mB that maximize this equation.
The MF scenario in the domain is a maximum problem: the
agent must solve as many formulas as possible within T ﬂips.
Considering heuristic A is the better heuristic after nA exper-
iments, the agent would have solved T /µA 3-SAT formulas.
However, if it would have used the HSC algorithm, the total
number of 3-SAT formulas would have been the summation
of:

µB

µA

1. The number of formulas solved during the extra experi-

2. The number of formulas to be solved using heuristic B,

ments yielded by the HSC algorithm, mA + mB;
if it prevails, F change · [T /µB];
changed, (1 − F change) · [T /µA].

3. The number of formulas to be solved if heuristic A is not

µA

µA

µA

µB

µB

µB

µB

(cid:82)

(cid:82)

(cid:82)

(cid:82)

dµBdµA =

− T−µB mB

µA
T
µA
mAµA+mB µB

− F change · mAµA+mB µB

mA + mB + F change · T

Here, δ is 1 as well, and cost(mA, mB) consists of the
number of ﬂips lost in each case, namely cost(mA, mB) =
F change · [(mAµA + mBµB)/µB] + (1 − F change) ·
[(mAµA + mBµB)/µA]. Lastly, the HSC algorithm will
yield mA and mB that will maximize utility(mA, mB) =
+ (1 − F change) ·
− (1 − F change) ·
+F change·
mB − µB mB
+ mA − mB]dµBdµA.

[ T−µAmA
4.1 Implementation issues
We constructed 305 different 3-SAT formulas, each consist-
ing of 100 different variables and 430 clauses. The formulas
were tested in advance for the existence of a valid truth as-
signment. The GSAT algorithm restarted with a new random
truth assignment after 5,000 ﬂips, and the total number of
restarts was set at 18. The temperature of the SA algorithm
was set at 2%, and the experiment was stopped after 100,000
unsuccessful ﬂips. The Random Walk with GSAT was im-
plemented using three different probabilities of Walk: 50%,

µB

µA

Heuristic

GSAT

Simulated
Annealing

Flips #

26.2

18.3

Random

Random

Random

50%
5.1

60%
4.5

80%
16.0

Table 1: Ofﬂine results of number of ﬂips (in thousands)

Figure 2: 3-SAT MT scenario inﬂuence of M

2-heuristic experimental results

4.2
According to the results presented in table 1 we compared (1)
Random 60% to Random 50%; (2) SA to GSAT; (3) Random
80% to SA. Other pairs, although feasible, would not effec-
tively demonstrate the HSC algorithm’s performance: the dif-
ference between their means is too large for the algorithm to
improve. From empirical results we know that the algorithm
will not advise on further experiments in those cases, and sim-
ply yield the better heuristic according to the information the
agent has. Thus, it will not endure any loss for these other
pairs. The ofﬂine results revealed that the best heuristic in
each pair was Random 60%, SA and Random 80%, respec-
tively. The total number of experiments in each scenario was
120, 40 experiments per pair.

In the MT scenario, when the agent did not use the HSC al-
gorithm, it always chose the heuristic with the better 5-game
average. In contrast, when the agent applied the HSC algo-
rithm, it mostly executed more experiments for both heuris-
tics, and then either changed its mind or continued with the
better 5-game heuristic. On average, only 12 additional ex-
periments were executed for each heuristic. Figure 1 summa-
rizes the percentage of experiments in which the agent chose
the best heuristic, with and without the HSC algorithm across
the three heuristic pairs. As we expected, the algorithm im-
proved the agent’s decision-making, and directed it to the best
heuristic in 80% of the experiments. Moreover, without the
HSC algorithm, the agent would have chosen the best heuris-
tics only in 61% of the experiments. This improvement re-
sulted in a gain in the number of ﬂips: the average number
of ﬂips for 300 3-SAT formulas without the algorithm (4.20
million) was signiﬁcantly higher than with the HSC algorithm
(4.06 million)(Wilcoxon pv=0.07).

To show the inﬂuence of M on the average gain, we exe-
cuted 120 additional experiments, with 12 different values for
M. Figure 2 summarizes the average gain per formula solved
for M varying between 100 and 3000. As expected, the aver-
age gain is in linear relation to the size of M, since the HSC
algorithm poses a greater potential beneﬁt as the number of
formulas increases.

The agent’s task in the MF scenario was to solve as many
formulas as possible within 200,000 ﬂips. Here, on average
nine additional experiments were executed for each heuris-
tic. Figure 1 presents the percentage of experiments in which
the agent chose the best heuristic, with and without using the
HSC algorithm (note that the percent of without the HSC al-
gorithm is the same as in the MT scenario). In this scenario

Figure 1: 3-SAT % of choosing the best heuristic without the
HSC algorithm, with the algorithm in the MT and with the
algorithm in the MF scenarios

60% and 80% (The three different heuristics of this algorithm
will be denoted by Random 50%, 60% and 80%). In each ex-
periment the maximal number of ﬂips was set at 15,000 and
the number of restarts was set at ﬁve. We established all the
algorithms’ parameters such that, on the one hand, they will
have a reasonable potential to solve any formula, but on the
other, their execution time will remain low.

The preliminary experiments executed each of the ﬁve
heuristics on the 305 3-SAT formulas. Table 1 presents the
average number of ﬂips each heuristics obtained. The param-
eters in the equations of section 2 were estimated using the
data accumulated during the preliminary experiments stage,
since this data comprises our whole population. Thus, the a-
priori parameters ζA and ζB were estimated with the mean of
all the ofﬂine results of all heuristics (55.2), and τA and τB
were estimated by their standard deviation (22.14). In addi-
tion, σA and σB were estimated by heuristic A and heuristic
B standard deviations, respectively. A sensitivity analysis of
these parameters determined that the results were not sensi-
tive to changes in the parameters. In the analyses we varied
the values of σA and σB, of ζA and τA and of ζB and τB
and re-executed the experiments. Except for several extreme
situations we obtained similar results for the speciﬁc values
described above.

Furthermore, in each experiment nA and nB were set to
ﬁve, M to 300, and T to 200,000. That is, in each experiment
of both scenarios, ﬁve formulas chosen randomly from the
305 formulas mentioned above, were solved ﬁrst, and were
used as the agent’s preliminary units of information. Then, it
had to decide with which heuristic to proceed solving the re-
maining 300 formulas. We allowed the agent only ﬁve units
of prior information since this is often the case in the real
world - agents need to base their decision on only few ob-
servations due to uncertainty in their environment or due to a
cost associated with the information.

Greedy

SA

Random

Random

Random

Without

With

1
0

2
0

80%
2
1

50%
16
9

60%
19
30

Greedy

SA

Random

Random

Random

Without

With

1
0

2
0

80%
2
0

50%
16
11

60%
19
29

Table 2: MT Scenario chosen heuristic distribution, with and
without applying the HSC algorithm: (top) The binary tree
results; (bottom) The ﬁxed number of experiments results

5 The CT domain
We investigated the HSC algorithm in a two-player negoti-
ation game that uses the CT game [Grosz et al., 2004]. In
this game each player has a goal placed on the game board
and certain resources to help it reach the goal. The players
can exchange resources, and at the end of the game are as-
signed a score that corresponds to their performance through-
out the game. During the game the agents may negotiate
on resources, commit to resource exchanges and execute ex-
changes. However, the commitments made by an agent are
not enforceable, and it might decide to back down on a com-
mitment or even deceive an opponent agent by committing to
an exchange it does not intend to keep.

In [Talman et al., 2005], an automated agent able to play
repeated CT games was developed. The agent characterizes
itself and its opponent in terms of cooperation and reliabil-
ity. The cooperation trait measures the willingness of an
agent to share its resources with others, whereas the relia-
bility trait measures the agent’s willingness to keep its com-
mitments in the game. Accordingly, the agent is capable of
employing seven strategies, differing in the level of cooper-
ation and reliability the strategy dictates. Each strategy is
suitable to a different type of an opponent, but the optimal
matching scheme is unknown. For example, it may prove
beneﬁcial to play a low-reliability strategy against a highly-
cooperative opponent by deceiving it. On the other hand, per-
haps a more logical strategy against such an opponent will
be a high-cooperation strategy which promotes reciprocity in
the game, and may beneﬁt the agent in the long run. Thus, in
order to maximize the agent’s score in the game, it must de-
termine which strategy best-suits each opponent type. Each
strategy trait can be low (L), medium (M) or high (H), and a
strategy will be referred to by its cooperation-reliability level,
such as a low-cooperation medium-reliability strategy (or LM
strategy). Therefore, the seven possible strategies in the game
are LL, LM, LH, MM, MH, HM and HH. The remaining two
strategies, ML and HL are not applicable as an agent apply-
ing low reliability strategy has by deﬁnition a low cooperation
level - when an agent almost never keeps it commitments, it
is not willing to share its resources.

We expect that our suggested model will assist the agent in
its decision-making, which will result in higher score in the
game. Following ni games of each strategy the agent exe-
cutes the HSC algorithm and determines which strategy best-

Figure 3: 3-SAT MF scenario inﬂuence of T

as well, the agent succeeded in choosing the best heuristic
more frequently by using the algorithm (83% vs 61%). Con-
sequently, it managed to solve six more formulas on aver-
age: 212 formulas with the HSC algorithm in contrast to 206
without it. To demonstrate the inﬂuence of T, we executed
an additional 70 experiments for seven different values of T
varying from 2,500 to 100,000. As the average gain is depen-
dent on T (the larger the T the larger the gain), we normalized
the gain by calculating the average gain per 2,500 ﬂips. Thus,
when T was set to 5,000 the average gain was divided by 2,
and when is was set to 100,000 the average gain was divided
by 40. Figure 3 summarizes the average gain per 2,500 ﬂips.
It supports our hypothesis that the gain of using the HSC al-
gorithms increases as T increases. It seems that the increase is
very steep at the beginning, but when T > 25, 000 it becomes
moderate.

4.3 k-heuristic experimental results

We tested the generalized k-heuristic model using the binary
tree algorithm and the ﬁxed number of experiments algo-
rithm. The binary tree was built according to the pairs we
described in section 4.2. That is, the binary tree’s leaves were
Random 50% and Random 60%, then GSAT and SA and ﬁ-
nally Random 80%. The ﬁxed number of experiments proce-
dure was executed according to the description presented in
2.2. We repeated the procedures 40 times for both algorithms.
Table 2 summarizes the number of times (of the forty ex-
periments) the agent chose each heuristic with and without
each algorithm for the MT scenario. As expected, both algo-
rithms improved the agent’s decision-making, in the binary
tree algorithm by 27.5% and in the ﬁxed number of experi-
ments algorithm by 25%. Nevertheless, the gain in the agent’s
utility was not very impressive in the binary tree case: on
average 1,500 ﬂips were lost per experiment due to the exe-
cution of heuristics that although seemed promising in their
pair, were in fact time consuming in the overall aspect. On
the other hand, in the ﬁxed number of experiments case 1,522
ﬂips were gained per experiment (the average total number of
ﬂips per experiment was 4,800). The number of additional ex-
periments were 14 for each heuristic on average. Due to the
disappointing results of the binary tree method, for the MF
scenario we executed only the ﬁxed number of experiments
method. Here, on average 1.6 formulas more were solved per
experiment when applying the algorithm.

Strategy
Avg. Score

LL
80.9

LM LH MM MH HM HH
85.7
69.2

68.6

69.4

69.5

69

Table 3: Average score of each strategy against an LH oppo-
nent in the CT domain

µA

µB

(cid:82)

(cid:82)

suits each opponent type. The model adjusted to the CT game
is similar to the 3-SAT model except for the introduction of
the time discount factor (δ). Each game the agent plays with
a non-optimal strategy results in a lower score. Hence, each
experiment added by the HSC algorithm brings about a time
cost of δ. In contrast, cost(mA, mB) = 0 in this domain: the
execution of mA + mB additional games does not incur an
additional cost associated with mA and mB. In conclusion,
for two strategies A and B (with A as the current winning
strategy) having a mean score of µA and µB, respectively,
the agent searches for mA and mB that maximize the utility
F change(µB − µA) · δmA+mB dµBdµA.
function:
In addition, we deﬁne the agent’s gain from applying the
HSC algorithm as follows:
let ScoreA be the score of the
strategy the agent chose before the algorithm and ScoreB
be the score of the strategy it chose after using the algo-
rithm. Then, its total gain from the algorithm is obtained by
(ScoreB − ScoreA) · δmA+mB .
5.1
Implementation issues
For our study, we constructed 300 game scenarios, which var-
ied the game boards, the resources each player was assigned
and the dependency relationship between the two players. For
evaluation purposes the HSC algorithm was executed to es-
tablish which strategy best suits an LH type opponent (best
matches for the other opponent types are carried out in a sim-
ilar manner). To this end, we ﬁrst executed all 300 games,
in which the agent played all seven strategies against an LH
type opponent. The results served as the CT domain ofﬂine
database as shown in table 3. Interestingly, the LM strategy
produced the best results. In addition, we set δ at 0.90, to
allow a certain amount of exploration. We later varied δ and
investigated its inﬂuence on the algorithm’s performance.

5.2 2-heuristic experimental results
In light of the results in table 3, we compared strategies: (1)
LL and LH; (2) LM and MH; (3) MM and HM. Other pairs
can be compared in a similar manner as long as the difference
between them is not too diverse for the HSC algorithm to in-
troduce an improvement nor a loss. Each comparison was
executed 40 times, totaling 120 experiments. In each experi-
ment, our agent played ﬁve 2-player CT games against an LH
opponent applying each strategy. Then, the agent executed
the HSC algorithm, which yielded the additional number of
games of each strategy the agent needs to play. On average
only three games of each strategy were additionally executed.
Figure 4 summarizes the percentage of experiments in
which the agent chose the best strategy, with and without us-
ing the HSC algorithm across the pairs of the three strategies.
By using the HSC algorithm, the agent was able to apply the
better strategy in 72% of the games, whereas without the HSC
algorithm it would have applied the better strategy in only
60% of the games. Nevertheless, as we anticipated, when the

Figure 4: CT % of choosing the best strategy with and without
the HSC algorithm

Figure 5: CT % of correct re-evaluations for different δs

difference between the two alternatives was negligible as in
the MM vs HM case, the algorithm did not improve the per-
formance. This is because the two strategies are almost iden-
tical. Thus, applying the algorithm resulted in no gain since it
rarely decided to perform additional experiments. However,
no loss was incurred as well (the average number of addi-
tional experiments was 3 for each alternative). Accordingly,
the agent’s gain was on average 0.8. This gain is somewhat
low because it is multiplied by δmA+mB .

Naturally, the time discount factor has a great inﬂuence on
the HSC algorithm’s performance. We envisioned that if we
increase δ the HSC algorithm will shift more frequently to-
wards the better strategy. To test our hypothesis, we ran-
domly selected ten experiments from the experiments de-
scribed above, across the three pairs, in which the worse strat-
egy would have been chosen by the agent according to the
preliminary experiments. We then re-evaluated the HSC algo-
rithm’s performance in those games with δ varying between
0.10 and 0.98. Figure 5 presents the percentage of experi-
ments the HSC algorithm pointed out the better strategy, for
each possible δ.

5.3 k-heuristic experimental results
We tested the generalized k-heuristic model using the binary
tree algorithm and the ﬁxed number of experiments algo-
rithm. In the binary tree, the pairs were established according
to the settings described in 5.2: LL vs LH, followed by MM
vs HM, then LM vs MH, and ﬁnally we added the HH strat-
egy. Then, we repeated the HSC algorithm bottom up. The
ﬁxed number of experiments procedure was executed accord-
ing to the description in 2.2. We repeated the procedures 40
times for both algorithms.

The results in this domain were similar to those of the 3-
SAT domain: both algorithms improved the agent’s decision-
making while keeping the number of additional experiments
to a minimum (on average, 11 additional games per strategy)
- the binary tree algorithm by 30% and the ﬁxed number of
experiments algorithm by 27.5%. The agent’s gain was 0.015
in both cases. Again, this ﬁgure seems low, but any positive
ﬁgure indicates a gain in performance due to the algorithm.

6 Related work
Our problem is different from the k-armed bandit problem
[Tins, 1989] since once the agent decides which alternative to
use it cannot change its decision. In the k-armed bandit prob-
lem, an item is chosen repeatedly and the decision of which
item to select may change over time. Another approach is the
statistical one. In [Pizarro et al., 2000] the statistical ANOVA
was employed for choosing the best model for a neural net-
work. The procedure enabled them to isolate a subset of
models whose mean error was the smallest, and subsequently
the simplest model was chosen (Occams razor criteria). The
main difference between that work and ours is that they were
not interested in minimizing the number of additional experi-
ments since no cost was involved. The same approach can be
found in heuristics-related works. [Selman et al., 1993], for
instance, conducted a large number of experiments in order
to identify the best heuristic. We suggest a method that will
minimize the number of experiments to be executed, while
providing the best option in a high percentage of the cases. In
another work [Tseng and Gmytrasiewicz, 1999] developed an
information-gathering system that uses the information value
to guide the process. However, they assume that the number
of possible answers for a query in the gathering process is ﬁ-
nite while we consider a continuous set of possible answers.
Moreover, they consider a myopic sequential procedure for
the information gathering process. Thus, their solution is not
optimal: they only consider the nearest step of information
gathering, assuming that in each step the agent can decide
about the next information to be obtained. [Grass and Zil-
berstein, 2000] developed a decision theoretic approach that
uses an explicit representation of the user’s decision model
in order to plan and execute information-gathering actions.
However, their system is based on information sources that
return perfect information about the asked query.

In the world of strategies, researchers also have been more
concerned with ﬁnding the most beneﬁcial strategy in a given
domain rather than formulating a general model. For exam-
ple, in [Carlsson and Johansson, 1998] three types of strate-
gies - generous, even-matched and greedy - were investigated
as concepts for analyzing games. They granted the participat-
ing agents a strategy and examined their performances in two
types of games. Their aim was to determine which kind of
strategy was preferable and in which environments.

7 Conclusions and future work
In this paper, we generalized a model developed in [Azoulay-
Schwartz and Kraus, 2002], which considered the problem of
choosing between alternatives in e-commerce. We have pre-
sented an adjusted model which was designed to improve au-

tomated agents’ decision-making. We evaluated this model
in two different domains - an NP-complete problem and a
computer-game framework. In each domain we executed a
large number of experiments for several aspects of the do-
main. We have shown that agents that apply the HSC algo-
rithm have improved their decision making by at least 20%,
while executing a minimal number of additional experiments.
Moreover, we have demonstrated how to adjust the HSC al-
gorithm to suit these different domains and to perform well
when solving both minimum and maximum problems.

Our future goal is to generalize the ﬁxed number of exper-
iments method. In this work we established its parameters
empirically, and although it produced satisfactory results, we
are interested in investigating its feasibility in additional do-
mains. We also intend to examine the normal distribution
assumption sensitivity. We assume that when the information
units’ distribution and the a-priori distribution are symmet-
ric with one maximum, our assumptions hold. Nevertheless
further analysis is required to ascertain this assumption.

References
[Azoulay-Schwartz and Kraus, 2002] R. Azoulay-Schwartz
and S. Kraus. Acquiring an optimal amount of informa-
tion for choosing from alternatives. In Proceedings of the
6th International Workshop on Cooperative Information
Agents VI, pages 123–137, 2002.

[Carlsson and Johansson, 1998] B. Carlsson and S. Johans-
son. Generous and greedy strategies. In Proceedings of
Complex Systems, 1998.

[Grass and Zilberstein, 2000] J. Grass and S. Zilberstein. A
value-driven system for autonomous information gather-
ing. J. of Intel. Information Systems, 14:5–27, 2000.

[Grosz et al., 2004] B. Grosz, S. Kraus, S. Talman, B. Stos-
sel, and M. Havlin. The inﬂuence of social dependen-
cies on decision-making: Initial investigations with a new
game. In Proceedings of AAMAS, pages 782–789, 2004.

[Pizarro et al., 2000] J. Pizarro, E. Guerrero,

and P.L.
Galindo. A statistical model selection strategy applied to
neural networks. In ESANN ’2000, pages 55–60, 2000.

[Selman et al., 1993] B. Selman, H. A. Kautz, and B. Cohen.
Local search strategies for satisﬁability testing. In the 2nd
DIMACS Challenge on Cliques, Coloring and Satis., 1993.
[Talman et al., 2005] S. Talman, K. Gal, M. Hadad, and
S. Kraus. Adapting to agents’ personalities in negotiations.
In Proceedings of AAMAS, 2005.

[Tins, 1989] J. C. Tins. Multi-armed Bandit Allocation In-

dices. John Wiley & Sons, 1989.

[Tseng and Gmytrasiewicz, 1999] C. Tseng and P. J. Gmy-
trasiewicz. Time sensitive sequential myopic information
gathering. In HICSS-32, 1999.

