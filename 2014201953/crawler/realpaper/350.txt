8 

Session No.  1 Scene Analysis I Robot  Project Papers 

SCENE ANALYSIS  BASED ON  IMPERFECT EDGE DATA* 

Rutgers  University,  New  Brunswick,  N.  J. 

Gilbert  Falk 

ABSTRACT 

The  paper  describes  a  heuristic  scene  des(cid:173)

cription  system.  This  system  accepts  as  input  a 
scene  represented  as  a  line  drawing.  Based  on  a 
set  of  known  object  models,  the  program  attempts 
to  determine  the  identity  and  location  of  each 
object  viewed.  The  most  significant  feature  of 
the  system  is  i ts  a b i l i ty  to  deal  with  imperfect 
input  data.  This  a b i l i ty  appears  essential  in 
light  of  our  current  stock  of  preprocessing  tech(cid:173)
niques  and  the  variation  that  is  possible  in  real 
world  data. 

INTRODUCTION 

A  hand-eye  system  is  a  problem  solving  sys(cid:173)

tem  with  an  eye  (camera)  for  input  and  a  hand 
(manipulator)  for  output.  Such  a  system  must 
have  at  least  1)  a  set  of  scene  analysis  (per(cid:173)
ception)  programs  which  interpret  the  real  world 
in  a  meaningful  way,  2)  a  set  of  manipulation 
programs  which  control  movement  of  the  hand  in 
3-space,  and  3)  an  executive  (problem  solver, 
strategy)  program  which  directs  the  perceptual 
and  motor  processes  toward  a  desired  goal.  This 
paper  concentrates  on  the  problems  of  machine 
perception. 
In  particular,  we  shall  describe 
some  programs  that  were  developed  as  part  of  a 
hand-eye  system  (4)  at  the  Stanford  A r t i f i c i al 
Intelligence  Laboratory.  We  shall  also  contrast 
our  approach  with  previous  work  (6,8)  and  i n d i(cid:173)
cate  some  directions  for  future  work. 

Computer  perception  can  be  thought  of  as  a 
large  data  reduction  problem.  A  matrix  of  d i g i(cid:173)
tized  intensity  values  is  read  into  memory  by 
means  of  some  imaging  device.  The  goal  of  analy(cid:173)
sis  is  a  concise  description  of  the  scene  viewed. 
The  description  or  interpretation  should  corres(cid:173)
pond  approximately  to  the  description  that  a  per(cid:173)
son  would  give  when  presented  with  the  same  scene. 
It  should  contain  at  least  the  identity  and  lo(cid:173)
cation  of  each  object.  The  techniques  which  we 
shall  describe  generate  such  interpretations 
based  on  a  single  perspective  projection. 

Any  system  capable  of  interpreting  i ts  input 

must  in  some  sense  be  model-based.  The  models 
provide  the  difference  between  the  input  informa(cid:173)
tion  and  the  information  contained  in  the  inter(cid:173)
pretation.  People  seldom  need  to  see  the  back 

*This  research  was  supported  in  part  by  The 
Advanced  Research  Projects  Agency  of  the  Depart(cid:173)
ment  of  Defence  under  Contract  SD-183  while  the 
author  was  at  Stanford  University. 

side  of  an  object  which  they  recognize  before 
"knowing"  how  it  looks  from  behind.  From  a  pure(cid:173)
ly  mathematical  point  of  view,  some  sort  of  model 
is  required  to  produce  a  three-dimensional  des(cid:173)
cription  of  the  world  based  on  a  single  two-di(cid:173)
mensional  image. 

Consider  a  set  of  models  (see  Fig.  1)  which 

are  acceptable  as  input  to  a  predictor.  This 
means  there  exists  an  algorithm,  the  predictor, 
which  takes  the  set  of  models  as  input  and  is 
capable  of  generating  any  possible  scene  com(cid:173)
prised  of  instances  of  these  models  as  output. 
The  particular  scene  is  specified  by  a  f i n i te 
set  of  parameters  (variables).  These  parameters 
are  the  names  and  locations  of  the  objects  pre(cid:173)
sent  in  the  scene. 
In  this  framework  the  pro(cid:173)
cess  of  interpreting  a  scene  can  be  viewed  as 
the  process  of  finding  values  for  the  variables 
such  that  the  scene  generated  by  the  predicator 
matches  the  input  scene. 

The  existence  of  a  predicator  suggests  the 

following  "hypothesis  and  test"  appraoch  to  scene 
analysis.  Based  on  the  input  data,  the  models, 
and  a  set  of  heuristics,  the  parameter  values 
comprising  the  interpretation  are  determined. 
These  parameters  are  then  used  to  generate  a  pre(cid:173)
diction  of  the  input.  The  original  input  and  the 
predicted  input  are  f i n a l ly  compared  and  the  i n(cid:173)
terpretation  is  accepted  if  the  two  match.  Sig(cid:173)
nificant  discrepancies  between  the  original  i n(cid:173)
put  and  the  prediction  are  used  to  suggest  a  new 
interpretation.  Such  a  scheme  can  be  thought  of 
as  a  combination  model-driven  (analysis  by  syn(cid:173)
thesis)  and  data-driven  approach. 

A  SIMPLE  VISION  SYSTEM 

To  be  more  concrete,  let  us  consider  the 

(simplified)  analysis  of  a  scene.  Analysis  con(cid:173)
sists  of  transforming  and  abstracting  the  infor(cid:173)
mation  in  the  input.  The  picture  in  Figure  2a 
displays  a  scene  to  be  analyzed  as  seen  through 
the  TV  camera  monitor.  This  image  is  read  into 
memory  and  stored  as  a  333x256  matrix  of  inten(cid:173)
sity  values.  Each  element  in  the  array  repre(cid:173)
sents  the  digitized  brightness  (0-15)  at  a  point 
in  the  f i e ld  of  view.  An  edge-detector  program 
transforms  Figure  2a  into  a  set  of  edge  points 
shown  in  Figure  2b  by  applying  a  local  gradient 
operator  and  thresholding  at  every  point  in  the 
image.  Edge  points  appear  where  there  is  a  sig(cid:173)
nificant  intensity  gradient.  The  f i n al  stage 
of  preprocessing  transforms  Figure  2b  into  a 
line  drawing  shown  in  Figure  2c.  This  is  accom(cid:173)
plished  by  f i t t i ng  straight  lines  to  the  edge 
points,  extending  these  lines  to  form  corners, 
and  identifying  closed  regions  and  the  background. 
The  scene  analysis  system  described  in  the  remain(cid:173)
der  of  this  paper  is  designed  to  accept  such  a 
line  drawing  as  input.  We  do  not  discuss  the 
problems  of  preprocessing.  Nevertheless,  we  are 

Session No.  1 Scene Analysis I Robot Project Papers 

9 

10 

Session No.  1  Scene Analysis I Robot Project Papers 

concerned  with  the  quality  of  output  that  a  pro(cid:173)
cessor  is 

likely  to  produce. 

It  is  assumed  that  the  set  of  objects  is 

completely  specified.  The  scenes  are  required 
to  consist  only  of  the  solids  shown  in  Fig.  1. 
This  particular  set  of  solids  was  chosen  for  the 
following  reasons: 

1)  There  are  enough  different  RPPs  to 

build  interesting  structures(with 
the  hand)• 

2)  Not  a ll  the  parallelepipeds  are  rec(cid:173)

tangular  (e.g.  the  RHOMBOID). 

3)  Not  a ll  of  the  solids  are  parellel-

epipeds  (e.g.  the  wedges). 

4)  Not  a ll  of  the  solids  are  convex 

(the  LBEAM). 

Complete  structural  descriptions  referred  to  as 
prototypes  (models)  exist  for  these  solids.  We 
refer  to  a  real  world  object  as  an  instance  of 
prototype. 

Our  fixed  size  prototypes  are  somewhat  less 

general  than  the  ones  described  by  Roberts  (8). 
Whereas  his  "cube"  model  represented  the  class 
of  a ll  parallelepipeds,  we  need  a  separate  model 
for  each  physically  different  solid.  The  dis(cid:173)
crete  size  r e s t r i c t i o n,  however,  provides  an  ad(cid:173)
ditional  set  of  constraints  which  can  be  applied 
to  resolve  ambiguities. 

Analysis  of  the 

line  drawing  proceeds  in 

several  stages.  We  assume  that  the  camera  has 
i n i t i a l ly  been  calibrated.  This  means  simply 
that  given  any  point  in  the  image,  we  can  find 
the  ray  in  3-space  corresponding  to  i t.  The  line 
drawing  in  Figure  2c  is  f i r st  segmented  into 
pieces  corresponding  to  individual  bodies  (Fig. 
2d). 
In  cases  where  a  body  can  be  (partially) 
completed,  the  program  does  this  (Figure  2e). 
order  to  identify  and  locate  the  corresponding 
objects  in  space,  features  inferred  from  the  pro(cid:173)
jections  of  the  individual  bodies  are  matched 
against  the  stored  prototypes.  To  check  that  the 
resulting  interpretation  of  the  scene  is  consis(cid:173)
tent  with  the  original  data,  the  identities  and 
locations  of  a ll  the  objects  are  used  to  generate 
a  predicted  line  drawing.  Figure  2f  shows  this 
prediction.  Finally,  the  prediction  and  the  or(cid:173)
iginal  line  drawings  are  compared. 
Figure  2g,  the  two  are  approximately  the  same, 
the  program  assumes  that  it  has  correctly  inter(cid:173)
preted  the  scene. 
to  produce  an  alternate  description  of  the  scene. 

If  not,  the  system  must  try 

I f,  as  in 

In 

In  practice,  the  analysis  is  considerably 

In  the  above  example,  the  line 

more  d i f f i c u l t.  Acutal  edges  are  not  seen  due  to 
poor  lighting. 
V11-V12  was  found  only  by  chance.  Often.extran(cid:173)
eous  lines  result  from  shadows  and  n o i s e in  the 
video  system.  There  is  also  the  inherent  ambi(cid:173)
guity  in  inferring  three-dimensional 

information 

from  a  single  view.  These  issues  receive  par(cid:173)
ticular  attention  in  the  rest  of  this  paper. 

INTERPRETATIONS  OF  IMPERFECT  LINE  DRAWINGS 

As  indicated  in  the  above  example,  the  pro(cid:173)

cess  of  interpreting  a  line  drawing  can  be 
thought  to  consist  of  five  stages: 
segmentation, 
completion,  recognition  (body  identification  and 
location),  prediction,  and  verification  (compari(cid:173)
son  and  decision). 
In  this  section  we  describe 
techniques  and  d i f f i c u l t i es  that  arise  in  imple(cid:173)
menting  each  of  them. 

SEGMENTATION 

Guzman  (6)  has  described  some  heuristic  pro(cid:173)

If  our  preprocessors  were  able  to 

cedures  for  segmenting  an  ideal  line  drawing  i n(cid:173)
to  bodies. 
produce  ideal  line  drawings,  we  could  simply  i n(cid:173)
corporate  Guzman's  program,  SEE,  as  part  of  our 
scene  analysis  system.  Experience  indicates, 
however, 
line  drawings.  Edges  are  often  missed  due  to 
poor  lighting  and  spurious  lines  can  result  from 
random  noise  in  the  video  system  and from  shadows. 

that  it  is  unreasonable  to  expect  such 

The  problems  of  missing  and  extra  lines  can 

to  some  extent  be  traded  for  one  another.  Par(cid:173)
ameters  in  the  preprocessors  can  often  be  set  so 
that  a ll  the  actual  edges  appear  in  the  line 
drawing.  Such  settings,  however,  generally  cause 
extraneous  lines  to  appear  as  well.  On  the  other 
hand,  if  one  sets  the  preprocessor  parameters  so 
that  a ll  noise  lines  w i ll  be  rejected,  some  of 
the  true  edges  are  also  rejected. 
In  practice 
(neglecting  shadows)  one  must  choose  between  an(cid:173)
alyzing  an  incomplete  line  drawing  and  one  con(cid:173)
taining  spurious  lines.  The  former  seems  to  be 
the  more  tractable.  We  w i ll  assume  in  the  sequel 
that  no  shadow  lines  are  present.  Techniques  for 
dealing  with  shadows  are  described  in  (3,7).  The 
analysis  of  scenes  with  both  missing  edges  and 
shadow  lines  remains  a  problem  that  needs  work. 

Given  an  incomplete  line  drawing,  a  reason(cid:173)
able  approach  is  to  f i r st  call  upon  a  heuristic 
program  referred  to  as  a  line  proposer.  The 
line  proposer  has  the  job  of  guessing  lines  that 
are  missing  from  the  line  drawing.  Such  an  ap(cid:173)
proach  has  been  investigated  by  the MIT  vision 
group  (2)  and  Grape  at  Stanford  (5). 
If  the  re(cid:173)
sulting  line  drawing  were  guaranteed  to  be  com(cid:173)
plete,  a  segmentation  procedure  like  Guzman's 
could  then  be  applied  to  i t.  The  procedure  which 
we  shall  sketch  below,  on  the  other  hand,  is  a 
complementary  approach;  that  i s,  it  tries  to  seg(cid:173)
ment  the  original  incomplete  line  drawing  into 
bodies. 

Consider  the  line  drawing  shown  in  Fig.  3. 

What  has  happened  here  is  that  the  lighting 
caused  the  triangular  face  of  the  wedge  and  the 
right  vertical  face  of  the  cube  to  be  identified 
as  the  single  region  R3.  Any  segmentation  pro-

Session No.  1 Scene Analysis I Robot Project Papers 

11 

region  partitions  are  used  to  generate  descrip(cid:173)
tions  of  each  ( p a r t i t i a l l y)  visible  body.  These 
descriptions  are  the  input  to  the  body  comple(cid:173)
tion  routines. 

COMPLETION 

Before  recognition  is  attempted,  the  com(cid:173)
pletion  routines  try  to  complete  partial  line 
drawings  of  single  objects  (see  Figure  4).  A  de(cid:173)
cision  was  made  to  be  very  conservative  about 
doing  t h i s.  Partial  projections  are  fixed  up 
only  where  it  is  quite  clear  that  no  mistake  w i ll 
result.  Although  the  recognition  procedure  does 
not  demand  that  the  projection  it  is  presented 
be  complete,  i ts  chances  of  success  are  better 
when  there  is  no  missing  data. 

cedure  which  attempts  to  partition  the  scene  by 
region  is  clearly  doomed  to  failure.  Our  seg(cid:173)
mentation  proceudre,  SEGMENT,  f i r st  segments  the 
scene  into  bodies  by  line.  This  is  accomplished 
by  combining  bits  of  local  evidence  accumulated 
at  the  vertices  in  a  way  similar  to  Guzman.  The 
line  "partition"  is  subsequently  used  by  a  re(cid:173)
gion  assignment  procedures  which  can  detect  and 
split  regions  such  as  R3.  The  resulting  set  of 
regions  is  then  partitioned  by  body. 

To  indicate  roughly  how  the  line  partition 

is  generated,  we  again  refer  to  Figure  3.  Con(cid:173)
sider  the  "Y"  shaped  vertex  at  PI.  This  is  taken 
to  indicate  that  lines  L1,  L2,  and  L3  should  be 
part  of  the  same  body.  Similarly,  the 
shaped  vertex  at  P2  is  taken  to  indicate  that lines 
L1,L4,and  L5  should  be  assumed  to  be  part  of  the 
same  body,Since  these  two  triples  share  the  com(cid:173)
mon  line  L I , it  is  inferred  that  the  five  lines 
L I,  L2,  L3,  L4,  and  L5  belong  to  the  same  body. 
The  "T"  shaped  vertex  at  P4  is  taken  to  indicate 
that  L6  and  L7,  the  collinear  segments  of  the 
"T",  belong  to  one  body  and  the  remaining  line, 
L8  (probably)  belongs  to  a  different  body.  An 
"L"  vertex  is  one  where  two  lines  meet  (e.g.  P3 
and  P5).  A  case  like  P3  where  the  background 
region  is  associated  with  an  angle  greater  than 
180  degrees  is  taken  to  imply  that  the  two  lines 
should  be  identified  as  parts  of  the  same  body. 
In  a  case  like  P5  where  the  background  is  associ(cid:173)
ated  with  the  smaller  angle,  no  such  i d e n t i f i(cid:173)
cation  is  made.  There  heuristics  usually  parti(cid:173)
tion  the  large  majority  of  lines  correctly.  There 
are,  however,  many  additional  ones  that  we  have 
not  mentioned  which  deal  with  special  situations. 

As  indicated  previously,  the  line  partition 

is  used  to  generate  a  partition  on  the  regions 
according  to  body. 
If  a ll  the  bounding  lines 
of  a  REGION  X  belong  to  BODY  Y  then  REGION  X  is 
assigned  to  BODY  Y. 
lines  of  REGION  X  belong  to  BODY  Y  and  some  be(cid:173)
long  to  BODY  Z,  then  we  must  determine  whether 
the  region  should  be  assigned  to  only  one  of  the 
bodies  or  should  be  split  (as  R3  in  Figure  3)and 
part  assigned  to  each  body.  We  shall  not  go  into 
the  details  of  this  procedure  here.  The  line  and 

If  some  of  the  bounding 

Figure  4 

There  are  three  completion  procedures:JOIN, 

ADDCORNER,  and  ADDLINE.  JOIN  handles  cases  sim(cid:173)
i l ar  to  that  of  Figure  4a. 
It  looks  for  a  face 
F,  which  2  hanging  collinear  lines  indicate  is 
incomplete. 
L2)  by  a  single  line  between  PI  and  P2. 

It  replaces  these  two  lines  (LI  and 

ADDCORNER handles  cases  such  as  Figure  4b. 

It  looks  for  a  face,  F,  that  is  incomplete  due 
to  two  hanging  lines  which  can  be  extended  to 
form  a  corner. 

It  completes  the  face  by  extend-

12 

Session No.  1 Scene Analysis I Robot Project  Papers 

ing  these  lines  to  the  new  corner  and  fixing  up 
the  rest  of  the  body  description  accordingly.  One 
must  be  careful,  however,  not  to  attempt  to  ex(cid:173)
tend  nearly  parallel 

lines. 

ADDLINE  looks  for  evidence  that  an  entire 

line  has  been  missed.  Although  there  are  no 
dangling  lines  in  Figure  4c,  something  is  defin(cid:173)
i t e ly  missing.  ADDLINE  adds  a  line  between 
points  PI  and  P2  based  on  the  pair  of  "L"  type 
vertices  there  with  parallel  sides.  Face  F  is 
p l it  into  2  faces. 

RECOGNITION 

Recognition  consists  of  identifying  the  ob(cid:173)
ject  as  an  instance  of  a  prototype  and  locating 
the  object  in  space. 

The  identification  procedure  tries  to  name 

a  given  object  by  extracting  features  from  i ts 
line  drawing  projection,  P,  and  matching  these 
against  the  properties  of  the  3-D  prototypes. 
If  we  define  PROTOTYPES  to  be  the  set  containing 
a ll  existing  prototypes,  we  can  represent  this 
process  schematically  as: 

on  support  hypothesis,  we  shall  assume  that  ob(cid:173)
jects  not  supported  by  the  table  are  resting  on 
the  top  horizontal  faces  of  other  objects. 

In  order  to  apply  support  hypothesis  one 

must  determine  not  only  the  base  points  of  each 
object,  but  also  binary  relations  of  the  form 
SUPPORTER  (BODY  X,  BODY  Y)  for  a ll  X  and  Y  such 
that  BODY  Y  supports  BODY  X  on  i ts  top  face.  The 
technique  which  we  use  to  do  this  is  quite  simi(cid:173)
lar  to  the  one  described  by  Winston  (13).  A  dif(cid:173)
ference  in  approach,  however,  is  that  we  use  a 
partial  3-D  scene  description  as  well  as  informa 
tion  contained  in  the  image  to  resolve  support 
ambiguities.  This  point  w i ll  be  made  clear  in 
one  of  the  examples  which  follow. 

After  deciding  to  which  model  a  particular 
object  in  the  scene  corresponds,  the  recognition 
location  in 
procedure  proceeds  to  determine  i ts 
space.  The  most  obvious  way  to  specify  this  is 
to  find  the  location  of  the  center  of  mass  of 
the  object  and  the  angles  that  3  known  orthogon(cid:173)
al  axes  in  the  body  make  with  the  axes  of  the 
table  coordinate  system.  The  techniques  des(cid:173)
cribed  below  provide  a  convenient  way  to  compute 
and  record  these  six  numbers. 

Feature  F1  is  extracted  from  the  projection  and 
those  prototypes  which  might  possibly  have  this 
feature  are  put  in  S1 
A  fixed 
number,  n - 1,  of  other  features  are  similarly  used 
in  an  attempt  to  further  restrict  the  possible 
interpretations  of  projection  P.  Hopefully,  for 
is  a  singleton  and  the  object 
Si 
some 
is  identified. 
identification  procedure  can  only  say  that  the  ob(cid:173)
ject  is  one  of  the  prototypes  in  Sn. 

If  this  is  not  the  case,  the 

i, 

Consider  a  prototype  with  i ts  center  of 

mass  at  the  origin  of  the  table  system  and  an 
arbitrary  orientation.  Now  imagine  an  instance 
of  this  prototype  translated  and  rotated  to  some 
other  location  in  space. 
vertices  of  the  unmoved  prototype  in  homogeneous 
coordinates  (1)  as  Pj  =  (Xj,  Yj,Zi,1)Tand  those 
of  the  instance  as  Pj' =  (Xj',  Yj • ,  Zj ' ,  1)T  then 
we can write Pi ' = T Pi where 

If  we  represent  the 

The  set  of  features  consist  of  geometric 

properties  such  as  face  shape  (triangular,  square. 
hexagonal),  angle,  and  edge  lengths.  A  problem 
arises,  however,  from  the  fact  that  the  projec(cid:173)
tion  features  are  two-dimensional  whereas  the 
prototype  features  are  three-dimensional.  A  sol(cid:173)
ution  to  this  problem  is  to  infer  3-D  features 
from  the  line  drawing  and  then  match  these  a-
gainst  the  prototypes.  A  number  of  heuristic 
techniques  based  on  the  notion  of  support  hypo(cid:173)
thesis  are  used  to  do  this.  Roughly  speaking, 
support  hypothesis  says  that  if  one  knows  a  plane 
(the  support  plane)  in  which  a  given  image  point 
l i e s,  then  this  plus  the  ray  to  the  point  (known 
since  the  camera  was  i n i t i a l ly  calibrated)  deter(cid:173)
mines  the  3-space  location  of  the  point  precise(cid:173)
ly.  Although  stereo  ranging  (10)  and  focus 
ranging  (11)  provide  a  more  direct  way  to  locate 
a  given  image  point  in  space,  these  procedures 
can  be  quite  clostly  to  apply.  We  currently  plan 
to  resort  to  these  techniques  only  when  the  heur(cid:173)
i s t ic  ones  f a i l.  A  thorough  investigation  of  how 
and  when  binocular  cues  should  be  u t i l i z ed  re(cid:173)
mains  to  be  done.  To  simplify  the  analysis  based 

The  upper  l e ft  hand  corner  of  T  is  a  3x3  rota(cid:173)
tion  matrix,  R,  and  the  f i r st  3  components  of 
the  fourth  column  describe  the  translation  of 
the  center  of  mass  of  the  instance  in  the  table 
system.  By  finding  the  location  and orientation 
of  an  object  we  shall  mean  finding  the  matrix 
T  which  moves  the  identified  prototype  to  the 
position  of  the  instance. 

There  are  six  parameters  needed  to  specify 
T;DX,  DY,  DZ,  and  the  three  angles  implicit  in 
R.  Knowing  the  3-space  locations  of  three  cor(cid:173)
responding  points  on  the  object  and  model  w i ll 
permit  T  to  be  computed.  We  shall  not  go  into 
the  actual  procedure  used  to  set  up  these  cor(cid:173)
respondences . 

PREDICTION 

After  having  identified  and  located  each  of 

the  objects  in  the  scene,  the  predictor  is  in  a 

Session No.  1 Scene Analysis I Robot Project Papers 

13 

position  to  say  how  the  described  scene  would  ap(cid:173)
pear  in  the  image.  This  problem  has  been  inves(cid:173)
tigated  rather  extensively  in  recent  years  by 
workers  in  the  area  of  computer  graphics.  A  ma(cid:173)
jor  goal  of  their  research  has  been  to  find  ef(cid:173)
ficient  algorithms 
"hidden-line  (surface)  problem"  (9,12).  For  the 
extremely  complex  objects  and  scenes  with  which 
they  deal,  an  efficient  algorithm  is  a  necessity. 
Our  vision  system,  on  the  other  hand,  is  current(cid:173)
ly  not  able  to  analyze  scenes  of  such  complexity 
and  is  not  as  s t r i c t ly  constrained  by  the  need 
for  efficiency. 

for  solving  the  so-called 

The  algorithm  currently  used  by  the  predict(cid:173)

or  is  a  modified  brute  force  approach.  First, 
the  predictor  generates  a  prediction  with  none  of 
the  hidden  lines  removed.  Next,  lines  on  the 
"back  faces"  of  individual  objects  are  deleted 
from  the  prediction.  Back  faces  can  be  deter(cid:173)
mined  by  comparing  the  face  normals  with  the  line 
of  sight  vector.  Finally,  the  remaining  hidden 
lines  are  removed  by  computing  a ll  the  line  i n(cid:173)
tersections  in  the  image  plane  and  determining 
which  of  the  resulting  line  segments  are  behind 
It  is  this  last  step  which  is 
visible  faces. 
time  consuming  and  a  number  of  heuristics  are 
used  to  speed  up  the  process.  The  predictor  is 
totally  independent  of  the  rest  of  the  scene  des(cid:173)
cription  system  and  could,  therefore,  easily  be 
replaced  by  one  employing  a  more  efficient  algo(cid:173)
rithm. 

VERIFICATION 

Verification  is  the  process  of  comparing  the 

failed  in  i ts 

predicted  line  drawing  with  the  original  input 
and  making  a  decision  based  on  the  result  of  this 
comparison.  There  are  two  distinct  interpreta(cid:173)
tions  for  "original  input"  here.  First,  the  pre(cid:173)
dicted  line  drawing  should  (approximately)  match 
the  original  line  drawing  from  which  it  was  de(cid:173)
rived.  This  can  be  confirmed  by  checking  that 
for  each  line  of  the  prediction  there  is  a  cor(cid:173)
responding  line  in  the  original  line  drawing. 
such  a  correspondence  cannot 
f i er  concludes  that  either  it  has 
interpretation  of  the  scene  or  the  original  line 
drawing  was  incomplete,  i . e .,  some  lines  were 
missing.  Quite  often  such  missing  lines  can  be 
detected  by  a  s t a t i s t i c al  line  verification  op(cid:173)
erator  (11)  applied  to  the  intensity  data  in  the 
v i c i n i ty  of  the  predicted  edge.  While  such  an 
operator  is  too  costly  to  apply  everywhere  in 
it  is  reasonable 
the  original 
to  apply  selectively  at  this  stage  of  the  analy(cid:173)
sis. 

If 
set  up,  the  veri(cid:173)

intensity  matrix, 

If  a ll  of  the  predicted  lines  match  lines  in 

the  original  line  drawing,  the  verifier  accepts 
the  interpretation  of  the  scene.  For  any  pre(cid:173)
dicted  line  that  cannot  be  matched,  the  statis(cid:173)
t i c al  verification  operator  is  called  to  check 
for  the  edge  (line)  in  the  original  TV  data. 
If 
for  a  particular  body  no  more  than  N  of  i ts  pre(cid:173)
dicted  edges  remain  unconfirmed  (N  is  currently 

set  equal  to  3),  the  verifier  assumes  that  the 
If  the  pre(cid:173)
body  has  been  recognized  correctly. 
diction  of  a  body  does  not  match  the  input  data, 
the  v e r i f i er  assumes  that  the  body  has  been  rec(cid:173)
ognized  incorrectly  and  asks  the  recognizer  to 
try  again  for  this  body. 

This  procedure  handles  the  case  where  the 
recognizer  is  unable  to  identify  an  object  un(cid:173)
iquely.  Unfortunately,  the  scene  analysis  pro(cid:173)
grams  at  present  are  unable  to  revise  decisions 
made  during  the  segmentation  and  completion 
stages.  Such  a  capability  is  certainly  one 
which  would  be  desirable  to  add  in  the  future. 
The  examples  which  follow  indicate  how  the  sys(cid:173)
tem  performed  on  some  real  scenes  and  w i ll  hope(cid:173)
f u l ly  clarify  some  of  the  points  made  above. 

SCENE  I 

The  scene  of  Figure  5a  consists  of  an 

RPP122  with  a  cube  in  front  of  i t.  The  intensi(cid:173)
ty  distribution  was  such  that  in  two  cases  re(cid:173)
gions  that  actually  correspond  to  2  faces  result. 
Three  visible  edges  are  not  present  in  the  line 
drawing.  Note  that  P8  and  P14  are  "L"  vertices 
of  a  type  we  have  not  mentioned  previously,  i.e. 
neither  associated  region  is  the  background.  To 
avoid  making  a  segmentation  error  at  points  such 
as  P8,  we  have  chosen  not  to  infer  anything  from 
such  "L"  vertices.  Because  of  this  decision,  we 
see  in  Figure  5b  that  the  line  P8-P14  is  not  pre(cid:173)
sent  in  the  segmented  line  drawing.  Figure  5c 
shows  the  situation  after  completion  of  the  i n(cid:173)
dividual  bodies  has  been  attempted.  For  the 
CUBE,  lines  P9-P10  and  P14-P15  have  been  exten(cid:173)
ded  to  the  corner  NEWP1  by  ADDCORNER.  Applica(cid:173)
tion  of  ADDCORNER  twice  to  the  RPP122  results  in 
NEWP2  and  a  completed  line  drawing  for  the  body. 
Since  neither  body  is  found  to  support  the  other, 
they  both  must  be  supported  by  the  table. 
Iden(cid:173)
t i f i c a t i on  of  points  P l l,  P9,  P12,  PI,  P7  and 
NEWP2  as  table  (base)  points  allows  recognition 
using  3-D  information  inferred  from  the  line 
drawing.  Figure  5d  shows  the  predicted  line 
drawing.  As  is  clear  from  Figure  5e,  the  pre(cid:173)
diction  and  original  input  match  quite  well  ex(cid:173)
cept  for  the  three  missing  edges.  Most  probably 
these  could  be  found  in  the  TV  data  by  the  v e r i(cid:173)
fication  operator.  Even  if  they  could  not, 
enough  of  the  lines  match  so  that  the  hypothe(cid:173)
sized  scene  description  is  acceptable. 

SCENE 

II 

In  contrast  to  the  previous  scene  where 

both  blocks  rested  on  the  table,  this  scene  (Fi(cid:173)
gure  6a)  illustrates  the  complications  arising 
from  other  forms  of  support.  As  in  the  previous 
example,  there  are  two  vertices  whose  type  was 
not  considered  in  our  abbreviated  discussion  of 
the  segmentation  procedure.  The  "Y"  type  v e r t i(cid:173)
ces  at  PI3  and  P5  are  not  used  to  infer  any  line 
groupings  since  one  of  the  3  regions neeting  thene 
is  the  background.  Figure  6b  illustrates  the 
situation  after  segmentation  and  Figure  6c  shows 

14 

Session No.  1 Scene Analysis I Robot Project Papers 

Session No.  1 Scene Analysis I Robot Project Papers 

15 

the  partial  body  projections  that  result  after 
completion. 

REFERENCES 

Recognition  proceeds  in  a  bottom-to-top  fa(cid:173)

It  then  tries  to  re(cid:173)

shion.  To  begin  with  the  recognizer  might  de(cid:173)
cide  to  identify  either  the  RPP114  or  the  RPP122 
which  are  both  resting  on  the  table.  Assume  that 
it  recognizes  the  RPP114. 
cognize  the  RPP124  above  the  RPP114  but  this 
fails  since 
it  has  not  yet  recognized  the  RPP-
112.  This  must  be  done  in  order  to  determine 
which  object  the  RPP124  is  actually  resting  on. 
The  next  body  processed,  therefore,  must  be  the 
RPP122  resting  on  the  table.  After  it  is  identi(cid:173)
fied  and  located  in  3-space,  its  top  face  can  be 
used  as  the  support  plane  for  the  RPP112  which  is 
processed  next.  Now  knowing  the  positions  of  the 
RPP114  and  the  RPP112,  the  system  can  determine 
the  true  support  plane  of  the  RPP124.  Finally, 
this  last  body  is  recognized. 

The  prediction  is  shown  in  Figure  6d  and 
with  the  original 
line  drawing  superimposed  in 
Figure  6e.  Again  the  two  match  and  the  inter(cid:173)
pretation  of  the  scene  is  accepted. 

CONCLUSION 

We  have  sketched  the  operation  of  a  large 
heuristic  program  capable  of  interpreting  line 
drawings  as  a  three-dimensional  scene.  We  have 
tested  the  program  on  approximately  30  different 
line  drawings  containing  between  one  and  six  ob(cid:173)
jects.  Failures  arose  only  where  a  body  was  too 
occluded  by  another,  many  links  were  missed  by 
the  preprocessors,  or  one  body  was  leaning  on 
another.  Such  scenes  are  often  confusing  to  hu(cid:173)
mans  as  well.  The  only  previous  scene  description 
system  comparable  to  ours  is  the  one  described 
by  Roberts.  Guzman's  recognition  programs  did 
not  operate  on  real  data  nor  were  they  concerned 
with  the  problem  of  locating  an  object  in  space. 
Our  scene  description  techniques  are  able  to  deal 
with  more  complex  scenes  than  was  the  Roberts  sys(cid:173)
tem.  Objects  can  be  supported  by  one  another  as 
well  as  by  the  table.  The  most  distinctive  fea(cid:173)
ture  of  the  system,  however,  is  i ts  ability  to 
interpret  a  scene  based  on  imperfect  data.  Tn 
addition  to  tolerating  inaccuracies  in  i ts  input, 
it  can  analyze  degemerate  views  of  objects, 
objects  which  appear  partially  occluded,  and  line 
drawings  in  which  edges  are  totally  missing.  The 
potential  to  cope  with  these  situations  is  a  con(cid:173)
sequence  of  the  basic  organization  we  employed. 
The  system  uses  i ts  line  drawing  input  and  a  set 
of  models  to  suggest  and  test  hypotheses.  We  be(cid:173)
lieve  that  this  approach  to  machine  perception 
w i ll  prove  to  be  a  f r u i t f ul  one. 

ACKNOWLEDGEMENTS 

I  wish  to  thank  my  co-workers  at  The  Stan(cid:173)
Intelligence  Project  for  sugges(cid:173)

ford  A r t i f i c i al 
tions  and  subroutines  supplied.  Particular 
thanks  to  J.  A.  Feldman  for  guidance  and  encour(cid:173)
agement  during  the  course  of  this  work. 

1.  Ahuja,  D.V.  and  Coons,  "Geometry  for  Construc(cid:173)

tion  and  Display','  IBM  Systems  Journal,  Vol.7 
Nos.  3 

4,  pp.  188-206,  1968, 

2.  Binford,  T.,  "A  Visual  Preprocessor,"  Inter(cid:173)

nal  Report,  Project  MAC,  Massachusetts  I n s t i(cid:173)
tute  of  Technology,  Cambridge,  Mass.  Apr.1970. 

3.  Falk,  G.,''Computer  interpretation  of  imperfect 
line  data  as  a  three-dimensional  scene,"  Com(cid:173)
puter  Science  Departmental  Report,  No.  CS180, 
Stanford  University,  Stanford,  Calif.,August 
1970. 

4.  Feldman,  J.A.,  Feldman,  G.M.,  Falk,  G.,  Grape, 

G.,  Pearlman,  J .,  Sobel,  I .,  Tenenbaum,  J.M., 
The  Stanford  hand-eye  project.  Proceedings  of 
the  First  International  Joint  Conference  on 
A r t i f i c i al  Intelligence,  pp.  521-526,May  1969. 

5.  Grape,  G.R.,  "On  predicting  and  verifying  mis(cid:173)

sing  elements  in  line-drawings,  based  on 
brightness  discontinuity  information  from 
their  i n i t i al  TV-images,"  Course  Project 
for 
CS225  and  CS382,  Stanford  University,  Stan(cid:173)
ford,  Calif.,  March  1970. 

6.  Guzman,  A./'Computer  recognition  of  three-

dimensional  objects  in  a  visual  scene,"  Ph.D. 
Thesis,  Project  MAC  TR-59,  Massachusetts  In(cid:173)
stitute  of  Techology,  Cambridge,  Mass. 
December  1968. 

7.  Orban,  R.,"Removing  shadows  in  a  scene,"  Pro(cid:173)

ject  MAC,  A r t i f i c i al  Intelligence  Memo  No.192, 
Massachusetts  Institute  of  Technology,  Cam(cid:173)
bridge,  Mass.,  April  1970. 

8.  Roberts,  L.G.,  "Machine  perception  of  three-

dimensional  solids,"  Technical  Report  No.315, 
Lincoln  Laboratory,  M.I.T.,  May  1963.  Also 
in  Optical  and  Electro-Optical  Information 
Processing,  Tippett,  J.T.  et  a l. 

9.  Ronuey,  G.  W. ,  "Computer  assisted  assembly 
and  rendering  of  solids,"  Rome  Air  Develop(cid:173)
ment  Center  Technical  Report  69-365,  Univer(cid:173)
sity  of  Utah,  September  1969. 

10.  Sobel,  I .,  "Camera  models  and  machine  percep(cid:173)
t i o n ,"  Ph.D.  Thesis,  A r t i f i c i al  Intelligence 
Memo  No.  121,  Stanford  University,Stanford, 
Calif.,  May  1970. 

11.  Tenenbaum,  J.M.,  "Accommodation  in  computer 

vision,"  Ph.D.Thesis,  Computer  Science  Depart(cid:173)
ment  Report  No.  CS182,  Stanford  University, 
Stanford,  C a l i f .,  October  1970. 

12.  Warnock,  J.E.,"A  hidden  line  algorithm  for 

halftone  picture  representation,"  University 
of  Utah  Computer  Science  Technical  Report 
4-15,  May  1968. 

