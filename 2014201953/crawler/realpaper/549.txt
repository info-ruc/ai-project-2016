GRAEL: an agent-based evolutionary computing approach for natural language 

grammar development 

CNTS - Language Technology Group 

UIA - University of Antwerp 

Guy De Pauw 

Antwerp - Belgium 
guy.depauw@ua.ac.be 

Abstract 

This  paper  describes  an  agent-based  evolution(cid:173)
ary computing technique  called  GRAEL  (Grammar 
Evolution),  that  is  able  to  perform  different natu(cid:173)
ral  language  grammar  optimization  and  induction 
tasks.  Two  different  instantiations  of the  GRAEL-
environment are described in this paper:  in GRAEL-
1  large  annotated  corpora  are  used  to  bootstrap 
grammatical  structure  in  a  society  of agents,  who 
engage in a series of communicative attempts, dur(cid:173)
ing  which  they  redistribute  grammatical  informa(cid:173)
tion to reflect  useful  statistics  for the  task  of pars(cid:173)
ing. 
In  GRAEL-2,  agents  are  allowed  to  mutate 
grammatical information, effectively implementing 
grammar  rule  discovery  in  a  practical  context.  A 
combination  of both  GRAEL-1  and  GRAEL-2  can 
be shown to provide an  interesting all-round opti(cid:173)
mization for corpus-induced grammars. 

Introduction 

1 
Evolutionary  computing  has  seen  many  interesting  applica(cid:173)
tions on a broad range of research domains over the years.  Its 
ability  to  overcome  the  problem  of local  maxima  in  finding 
a solution to a particular problem,  by recombining and mu(cid:173)
tating individuals in a society of possible solutions, has made 
it an attractive technique  for problems involving large, com(cid:173)
plicated and  non-linearly  divisible  search  spaces.  The  evo(cid:173)
lutionary  computing  paradigm  has  however  always  seemed 
reluctant to deal  with natural  language syntax,  probably be(cid:173)
cause it is essentially a recursive, non-propositional  system, 
dealing with complex issues such as long-distance dependen(cid:173)
cies and constraints.  This has made it difficult to incorporate 
it in typically propositional evolutionary systems such as ge(cid:173)
netic algorithms. 

A limited amount of GA-related syntactic research has fo(cid:173)
cused  on  linguistic  data  [Smith  and  Witten,  1996;  Wyard, 
1991; Antonisse, 1991; Araujo, 2002], but none of these sys(cid:173)
tems are suited to a generic (treebank) grammar optimization 
task,  mainly  because  the  grammatical  formalism  and  evo(cid:173)
lutionary  processes  underlying  these  systems  are  designed 
to  fit  a particular task,  such as  information retrieval  [Losee, 
1995].  Other  work  on  syntax  in  the  evolutionary  comput(cid:173)
ing  paradigm  has  either  been  involved  in  studying  the  ori-

gins  of grammar  in  a  computational  context  [Batali,  2002; 
Kirby, 2001J or the co-ordinated co-evolution of grammatical 
principles  [Briscoe,  1998].  Yet  so  far,  little  or no  progress 
has been achieved in evaluating evolutionary computing as a 
tool  for the  induction  or optimization  of data-driven parsing 
techniques. 

The  GRAEL1  environment  provides  a  suitable  framework 
for  the  induction  and  optimization  of any  type  of grammar 
for  natural  language  in  an  evolutionary  setting. 
In  this  pa(cid:173)
per we hope to provide an overview of GRAEL as a grammar 
optimization  and  induction  technique.  We  will  first  outline 
the  basic  architecture  of the  GRAEL  environment  in  Section 
2 on the basis of a toy example.  Next, we introduce GRAEL-
1 (Section  3) as a grammar optimization technique that can 
enhance  corpus-induce  grammars.  By  adding  an element of 
mutation in GRAEL-2 we implement a method to extend the 
coverage of a corpus-induced grammar. We will also describe 
a combination of both GRAEL-1  and GRAEL-2 which can be 
shown to provide an  interesting all-round optimization tech(cid:173)
nique for corpus-induced grammars. 

2  GRAEL - Grammar Evolution 
A  typical  GRAEL  society  contains  a  population  of agents  in 
a virtual  environment.  Each of these agents holds a number 
of structures  that  allows  it  to  produce  sentences  as  well  as 
induce a probabilistic grammar to analyze other agents'  sen(cid:173)
tences.  During an extended series of error-driven inter-agent 
interactions,  these  grammars  are  updated  over time.  While 
the evolutionary computing approach of GRAEL is able to de(cid:173)
fine the quality of the grammars that are developed over time, 
the  agent-based  aspect  of GRAEL  ensures  that  the  grammar 
optimization  is  grounded  in  the  practical  task  of parsing  it-
self.  From an  engineering point of view,  GRAEL provides a 
general  framework for grammar optimization and  induction, 
but  from  a  more  theoretical  point of view,  GRAEL  can  also 
help  us  to  understand the  dynamics  of grammar emergence 
and evolution over time. 

In the data-driven GRAEL experiments described in this pa(cid:173)
per,  the  grammatical  knowledge  of the agents  in  the  society 
is bootstrapped by using an annotated natural  language cor(cid:173)
pus [Marcus et  al.,  1993].  At the onset of such a data-driven 

'GRAmmar EvoLution 

NATURAL  LANGUAGE 

823 

be parsed, or two other agents in the GRAEL society will be 
randomly selected to play a language game. 

These  interactions,  which  introduce  a  concept  of  error-
driven knowledge sharing, extend the agents' grammars fast, 
so that the datasets can grow very large  in a  short period of 
time.  A generation-based GRAEL  society can be used to al(cid:173)
low  the  society  to  purge  itself of bad  agents  and  build  new 
generations  of good parser agents,  who  contain  a  fortuitous 
distribution of grammatical knowledge. This involves the use 
of fitness functions that can distinguish good agents from bad 
ones.  For a full  overview of all  the evolutionary parameters 
in the GRAEL environment, many of which have a significant 
impact  on  processing,  we  would  like  to  refer to  [Dc  Pauw, 
2002]. In this paper, we will describe the most relevant subset 
of experiments that allows us to evaluate  GRAEL as a gram(cid:173)
mar induction and optimization technique. 

3  GRAEL-1:  Probabilistic Grammar 

Optimization 

Historically, most syntactic parsers for natural  language have 
made  use  of hand-written  grammars,  consisting  of a  labo(cid:173)
riously  crafted  set  of grammar  rules.  But  in  recent  years, 
a  lot  of  research  efforts  employ  annotated  corpora  to  au(cid:173)
tomatically  induce  grammars  [Bod,  1998;  Collins,  1999; 
De  Pauw,  2000].  Yet,  data-analysis  of  the  output  gener(cid:173)
ated  by  these  parsers  still  brings  to  light  fundamental  limi(cid:173)
tations to these  corpus-based methods  [Klein  and Manning, 
2001]. Even though generally providing a much broader cov(cid:173)
erage  than  hand-built  grammars,  corpus-induced  grammars 
may  still  not  hold  enough  grammatical  information  to  pro(cid:173)
vide  parses  for  a  large  number  of sentences,  as  some  rules 
that are needed to generate the correct tree-structures are not 
induced from the original corpus (cf.  Section 4). 

But  even  if  there  were  such  a  thing  as  a  full-coverage 
corpus-induced grammar, performance would still be limited 
by  the  probabilistic  weights  attributed  to  its  rules.  A  typi(cid:173)
cal  data-driven parser provides a huge collection of possible 
parses  for any given sentence.  Fortunately, we  can  also  in(cid:173)
duce useful statistics from the annotated corpus that provides 
a way to order these parse forests to express a preference for 
a particular parse.  These statistics go a long way in provid(cid:173)
ing well ordered parse forests, but in many other cases, it can 
be observed that the ranking of the parse forest is sometimes 
counter-intuitive in that correct constructs are often overtaken 
by obviously erroneous, but highly frequent structures. 

This  can  easily  be  explained  by  the  inherent  nature  of 
corpus-based  grammars:  the  initial  probabilistic  values  at(cid:173)
tached to the grammar rules induced from the annotated data 
are equal to their relative frequency in the corpus.  It might be 
the case however that, even though they are directly induced 
from the annotated corpus, the probabilities of these rules are 
not suited to the disambiguation task as yet.  It may therefore 
be useful to have the parser use  the grammar to practice the 
task of parsing and adjust the probabilistic  weights of partic(cid:173)
ular structures according to these test cases.  We then need to 
consider the initial grammar as basic raw material  in need of 
optimization,  as  it is merely a reflection of the original data 
set and is  not yet optimized for the task  of parsing (unseen) 

GRAEL society, the syntactic structures of a treebank are ran(cid:173)
domly distributed over the agents, so that each agent holds a 
number of tree-structures in memory. These structures enable 
them to generate sentences, as well as provide grammars that 
allow them to analyze other agents' sentences. 

The  actual  interaction  between  agents  is  implemented  in 
language games:  an agent (agl) presents a  sentence to  an(cid:173)
other agent (ag2).  If ag2 is able to correctly parse agl's sen(cid:173)
tence, the communication is successful.  If on the other hand, 
ag2  is  lacking  the  proper grammatical  information to  parse 
the sentence correctly, agl  shares the necessary information 
for ag2 to arrive at the proper solution. 

Figure  1  displays a toy example of such a language game. 
In this example, a "treebank" of two structures has been dis(cid:173)
tributed over a society of two agents.  The two agents engage 
in  a  language game,  in  which  agl  presents  the  sentence  "1 
offered some bear hugs" to ag2  for parsing.  At this point in 
time, ag2's grammar does not contain the proper grammatical 
information  to  interpret  this  sentence  the  way  agl  intended 
and  so  ag2  will return an  incorrect parse,  even though  it  is 
consistent with its own grammar. 

Consequently,  agl  will  try  and  help  ag2  out  by  reveal(cid:173)
ing the minimal correct substructure of the correct parse that 
should enable ag2 to arrive at a better solution. This informa(cid:173)
tion  is incorporated in  ag2's grammar, who will try to parse 
the sentence again with the updated knowledge.  When ag2 
is able to provide the correct analysis (or is not able to after 
a certain number of attempts) either agl's next sentence will 

824 

NATURAL  LANGUAGE 

sentences itself. 

Typical  methods  of  probabilistic  grammar  optimization 
include,  among  others,  bagging  and  boosting  [Henderson 
and  Brill,  2000;  Collins,  2000],  re-estimation  of the  con(cid:173)
stituents probabilities [Goodman,  1998; Charniak, 2000] and 
including  extra  information  sources  [Belz,  2001;  Collins, 
1999].  But we propose an agent-based evolutionary comput(cid:173)
ing method to resolve this issue.  Grammar optimization  us(cid:173)
ing a GRAHL environment is in this vein related to the afore(cid:173)
mentioned bagging approach to grammar optimization, albeit 
with  some  notable  differences.  By  distributing  the  knowl(cid:173)
edge  over  a  group  of agents  and  having  them  interact  with 
each  other,  we  basically  create  a  multiple-route  model  for 
probabilistic  grammar optimization.  Grammatical  structures 
extracted from the training corpus,  will  be present  in differ(cid:173)
ent  quantities  and  variations throughout the  GRAEL  society. 
While the agents interact with each other and in effect prac(cid:173)
tice the task on each other's grammar, a varied range of prob(cid:173)
abilistic  grammars are  optimized  in  a  situation  that  directly 
relates  to  the  task  at  hand.  The  evolutionary  aspects  of the 
system make sure that, while marginally useful grammatical 
information is down-toned, common constructs arc enforced, 
providing a better balanced model for statistical parsing. 

The way GRAEL accomplishes a re-distribution of the orig(cid:173)
inal probabilistic values is by using the default GRAEL archi(cid:173)
tecture described in Section 2. This type of error-driven learn(cid:173)
ing makes sure that mistakes are being dealt with by transfer(cid:173)
ring difficult grammatical constructs, thereby increasing their 
probabilistic value in  the other agent's grammar.  This prob(cid:173)
abilistic adjustment will  be taken  into account during subse(cid:173)
quent parsing attempts by this agent, hopefully triggering the 
correct grammatical structure in the future. 

3.1  Experimental  Setup 
The  overall  setup of the  GRAEL  experiments  is  displayed  in 
Figure  2.  Baseline  accuracy is  measured  by  directly  induc(cid:173)
ing a grammar from the training set to power Parser 1, which 
disambiguates the  test set.  This grammar takes on the  form 
of a  PMPG  as outlined in  [De  Pauw,  2000]  (cf.  infra).  The 
same  training  set  is  also  randomly  and  equally  distributed 
over a number of agents in the GRAEL society, who will con(cid:173)
sequently  engage  in  a number of language games.  At  some 
point, established by the halting procedure (cf.  infra), the so(cid:173)
ciety is halted and the fittest agent is selected from the society. 
This  agent effectively constitutes  a  redistributed  and proba(cid:173)
bilistically optimized grammar, which can be  used to power 
Parser 2. GRAEL-1 accuracy is achieved by having this parser 
disambiguate the same test set. 

We  used  two  data  sets  from  the  Penn  Treebank  [Marcus 
et al.9  1993].  The main batch of experiments was conducted 
on the small,  homogeneous ATlS-corpus,  which  consists  of 
a  collection  of annotated  sentences  recorded  by  a  spoken-
dialogue  system.  The  larger  Wall  Street  Journal  Corpus 
(henceforth  WSJ),  a  collection  of annotated  newspaper arti(cid:173)
cles, was used to test the system on a larger scale corpus. The 
common  division  between  training  set  (Section  02-21)  and 
test  set  (Section  23)  was  used.  Semantically  oriented flags 
and numeric flags indicating internal relations were removed 
to allow for more streamlined syntactic processing. 

Figure 2:  Comparing parsers:  baseline parser and GRAEL 

For syntactic  processing,  the  agents  use  the  parsing  sys(cid:173)
tem  PMPG  described  in  [De  Pauw,  2000],  which  integrates 
a  CKY  parser  [Chappelier  and  Rajman,  1998]  and  a  parse 
forest ranking scheme that employs probabilistic information 
as  well  as  a  memory-based  operator  to  maximize  for  each 
parse  the  number of nodes  that  can be  retrieved  from  mem(cid:173)
ory.  A  PMPG  takes  the  form  of a  simple  PCFG-type  gram(cid:173)
mar, enriched with numerical  indices that encode contextual 
information previously observed in a treebank. This memory-
based instantiation of Data-Oriented Parsing [Bod,  1998] en(cid:173)
sures that larger syntactic structures are used as the basis for 
parsing, with a minimal loss of computational efficiency over 
regular  PCFGs.  The  PMPG  approach  can  therefore  be  con(cid:173)
sidered to introduce a psycho-linguistically relevant memory-
based operator in the parsing process. 

The full  experimental  run varied society sizes,  generation 
methods, fitness functions and halting procedures [De Pauw, 
2002].  The subset of experiments described in this paper em(cid:173)
ployed the sexual procreation method to introduce new gen(cid:173)
erations by  combining the  grammars of two  fit  agents  in  the 
society to create new generations of parser agents. The fitness 
of an agent is defined by recording a weighted average of the 
F-score during inter-agent communication (also see Figure 3) 
and the F-score of the agent's parser on a held-out validation 
set.  This  information  was  also  used  to  try  and halt the  so(cid:173)
ciety  at a global  maximum  and  select  the  fittest  agent  from 
the  society.  For computational  reasons,  the  experiments on 
the WSJ-corpus were limited to two different population sizes 
and used an approximation of GRAEL that can deal with large 
datasets in a reasonable amount of time.  The test set was not 
used  in  any  way  during  actual  GRAEL-processing  in  agree(cid:173)
ment with blind-testing procedures. 

NATURAL  LANGUAGE 

825 

Table 1: Baseline vs GRAEL-1 results 

3.2  Results 
Table  1  displays the  exact match  accuracy and  F-scores  for 
the baseline model, a standard PMPG parser using a grammar 
induced from the training set (cf.  Figure 2).  It also displays 
scores  of the  GRAEL  system  for  different  population  sizes. 
We  notice  a significant  gain  for all  GRAEL  models over the 
baseline model on the ATIS corpus, but increasing population 
size over 20 agents seems to decrease exact match accuracy 
on the ATIS  corpus.  Likewise,  the small  society of 5  agents 
achieves only a  very limited  improvement over the baseline 
method.  Data analysis showed that the best moment to halt 
the society and select the  fittest  agent from the society,  is a 
relatively  brief period  right  before  actual  convergence  sets 
and  grammars throughout the  society  are  starting to resem(cid:173)
ble each other more closely. The size of the the society seems 
to  be  the  determining  factor controlling  the  duration  of this 
period. 

Preliminary tests on a subset of the WSJ corpus had shown 
that  society  sizes  of 20  agents  and  less  to  be  unsuitable  for 
a large-scale corpus, again ending up in a harmful premature 
convergence. The gain achieved by the GRAEL society is less 
spectacular than on the ATIS corpus, but it is still statistically 
significant.  Larger society  sizes  and  full  GRAEL processing 
on the WSJ corpus should achieve a more significant gain. 

The  experiments  do  show  however,  that  GRAEL-1  is  in(cid:173)
deed  an  interesting method  for probabilistic grammar redis(cid:173)
tribution and optimization.  Data analysis shows that many of 
the  counter-intuitive parse  forest  orderings that  were  appar(cid:173)
ent  in  the baseline  model,  are being resolved after GRAEL-
1  processing.  It  is  also  interesting  to point out that we  are 
achieving an error reduction rate of more than 26% over the 
baseline  method,  without  introducing  any  new grammatical 
information in the society, but solely by redistributing what is 
already there.  These experimental results indicate that anno(cid:173)
tated data can indeed be considered as raw material that can 
be optimized for the practical use of parsing unseen data. 

[De Pauw,  2002]  also  describes experiments that directly 
compare GRAEL to the similar methods of bagging and boost(cid:173)
ing  [Henderson  and  Brill,  2000],  which  are  summarized in 
the bottom two lines of Table  1.  Bagging and boosting were 
shown  to  obtain  significantly  lower  accuracy  figures  on  al(cid:173)
most all  accounts.  Only the  F-score  for the bagging experi(cid:173)
ment exceeded that of the optimal  GRAEL  configuration, but 
this  can  be  attributed  to  the  fact  that  an  approximation  of 
GRAEL  was  used  for  full  processing  on  the  extensive  WSJ 

dataset. 

4  GRAEL-2:  Grammar Rule Discovery 
The functionality of GRAEL-1 can be extended by only apply(cid:173)
ing minor alterations to the  GRAEL  system.  With  GRAEL-2 
we wish to provide a grammar rule discovery method which 
can  deal  with  the  problem  of grammar sparseness.  Hand(cid:173)
written and corpus-induced grammars alike have to deal with 
the fundamental issue of coverage.  [Collins,  1999] for exam(cid:173)
ple reports that  when using sections  2-21  of the  wsj-corpus 
as a training set and section 23 as a test set, 17.1% of the sen(cid:173)
tences in the test set require a rule not seen in the training set. 
Even for a large corpus such as the WSJ, sparse grammar is 
indeed a serious accuracy bottleneck. 

It  would  therefore  be  useful  to  have  a  method  that  can 
take  a corpus-induced  grammar and  extend  it by  generating 
new rules.  But  doing  so  in  a  blind  manner,  would  provide 
huge, over-generating grammars, containing many nonsensi(cid:173)
cal rules.  The GRAEL-2 system described in this section, in(cid:173)
volves a distribute d approach to this type of grammar rule dis(cid:173)
covery. The original (sparse) grammar is distributed among a 
group of agents,  who  can randomly  mutate the  grammatical 
structures they hold.  The new grammatical  information they 
create is tried and tested by interacting with each other.  The 
neo-darwinist  aspect  of this  evolutionary  system  will  make 
sure  that any useful  mutated grammatical  information  is re(cid:173)
tained throughout the population,  while noise is  filtered  out 
over time.  This method provides a way to create new gram(cid:173)
matical structures previously unavailable in the corpus, while 
at the same time evaluating them in a practical context, with(cid:173)
out the need for an external information source. 

To accomplish this, we need to implement some minor al(cid:173)
terations to the GRAEL-1  system. The most important adjust(cid:173)
ment occurs during the language games. We refer back to the 
toy  example  of the  language  game  in  Figure  1  to  the  point 
where agl  suggests the minimal correct substructure to ag2. 
In GRAEL-1 this step introduced a form of error-driven learn(cid:173)
ing, making sure that the probabilistic value of this grammati(cid:173)
cal structure is increased. The functionality of GRAEL-2 how(cid:173)
ever is different:  in this step, we assume that there is a noisy 
channel between agl and ag2 which may cause ag2 to misun(cid:173)
derstand agl's structure.  Small mutations on different levels 
of the substructure may occur, such as the deletion, addition 
and replacement of nodes.  This effectively introduces previ(cid:173)
ously unseen grammatical data in the GRAEL society, which 

826 

NATURAL  LANGUAGE 

Table 2:  Baseline vs GRAEL-1  vs GRAEL-2  VS GRAEL2+1 
Results 

will consequently be optimized over time. 

Preliminary experiments however showed that this does not 
work as such, since the newly created structures were largely 
being ignored in favor of the gold-standard corpus structures. 
We therefore implemented another alteration to the GRAEL-1 
system. Instead of just presenting the tree-structure originally 
assigned by the training set, we now require agl to parse the 
string-only sentence using the grammar acquired during lan(cid:173)
guage  games,  replacing  the  tree-structure  from  the  training 
set,  with  a possibly different tree-structure that  incorporates 
some of the mutated information.  This alteration makes sure 
that  the  mutated  grammatical  structures  are  actively  being 
used,  so that their usefulness as grammatical constructs can 
be measured in a practical context. 

Experimental  Setup and Results 
The setup for the GRAEL-2 experiments is the same as for the 
GRAEL-1  experiments (cf.  Figure 2).  To test the grammar-
rule  discovery capabilities of GRAEL-2  we  have  compiled  a 
special worst-case scenario test set for the ATIS corpus, con(cid:173)
sisting of the 97 sentences in the ATIS that require a grammar 
rule  that  cannot  be  induced  from  the  training  set.  For  the 
WSJ-experimentthe normal test set was used.  A 20-agent and 
a  100-agent society were  respectively used for the ATIS  and 
WSJ experiments. 

The baseline  and GRAEL-1  methods  for the  ATIS  experi(cid:173)
ments  trivially  have  an  exact match  accuracy  of 0%,  which 
also has a negative effect on the F-score (Table 2).  GRAEL-2 
is indeed able to improve on this significantly, proving that it 
is indeed an effective grammar rule discovery method.  Data-
analysis  shows however that  it has  lost the beneficial  proba(cid:173)
bilistic optimization effect of GRAEL-1. 

We therefore performed another experiment,  in which we 
turned the GRAEL-2 society into a GRAEL-1  society after the 
former's  halting  point. 
In  other  words:  we  take  a  society 
of agents  using  mutated  information  and  consequently  ap(cid:173)
ply GRAEL-1 's probabilistic redistribution properties on these 
grammars.  Figure 3 shows the course of the GRAEL2+1  ex(cid:173)
periment.  In this figure we see the F-scores recorded during 
inter-agent  communicative attempts.  After an  almost  linear 
increase during GRAEL-2 processing, the society is halted af(cid:173)
ter an extended period of convergence.  Next, GRAEL-1  pro(cid:173)
cessing resumes, which negatively affects F-scores for a brief 
period of time, until the society reconverges. Even though the 
F-scores do not seem to improve over those observed before 
the transition to GRAEL-1, Table 2 shows that the fittest agent 
selected  from the society after the transition performs better 
on the held-out test set. The results on the wsj-corpus are also 

Figure 3:  GRAEL2+1  Experiment - F-scores during language 
games 

interesting  in this respect.  GRAEL-1  outperforms GRAEL-2 
on this data set,  but the combination of the two seems to be 
quite beneficial for parsing accuracy. 

Evaluating a grammar rule discovery method poses an em(cid:173)
pirical  problem  in  that  it  can  never be  clear  what  grammar 
rules are missing until we actually need them. The test set we 
compiled to perform the  GRAEL-2  ATis-experiments goes  a 
long way in providing a touchstone to see how well GRAEL-
2 performs as a supervised grammar induction method.  And 
results  indicate  it performs  quite  well:  mutated  information 
becomes  available  that  is  able  to  create  parses  for  difficult 
constructions,  while the  number  of structures that  constitute 
noise  is  limited  and  is  attributed  a  small  enough portion  of 
the probability mass as not to stand in the way of actual use(cid:173)
ful mutated structures. 

5  Concluding Remarks 
This paper has presented one of the  first  research efforts that 
introduces agent-based evolutionary computing as a machine 
learning  method  for  data-driven  grammar  optimization  and 
induction.  In recent years, many researchers have employed 
ensemble methods to overcome any negative bias their train(cid:173)
ing  data  might  impose  on  their  classifiers.  It  is  indeed  im(cid:173)
portant  to  view  (annotated)  data,  not  as  an  optimally  dis(cid:173)
tributed set of examples but as raw material that needs to be 
pre-processed  before  it  can  be  used  by  a  machine  learning 
classifier.  The  bagging and  boosting approach for instance, 
tries  to create  resamplings of the  original  data,  to overcome 
the local  maxima the data might restrict the classifier to, but 
we believe  GRAEL  adds an  extra dimension  to the task:  by 
splicing the data and incorporating it in a society of commu(cid:173)
nicating agents, we allow for the parallel development of sev(cid:173)
eral  grammars at once,  enhanced in  a practical context that 
mirrors the goal itself: parsing unseen data. 

We  described  two  instantiations  of the  GRAEL  environ(cid:173)
ment. The basic GRAEL-1 system aims to provide a beneficial 
re-distribution of the probability mass of a probabilistic gram(cid:173)
mar.  By  using a  form  of error-driven  learning in the  course 
of language  games  between  agents,  probabilistic  values  are 
adjusted in a practical context.  This optimizes the grammars 
for the  task  of parsing  data,  rather  than  reflecting  the  prob(cid:173)
ability  mass  of the  initial  data  set.  This  method  favorably 
compares to established grammar optimization methods  like 
bagging and boosting. 

By  adding  an  element  of  mutation  to  the  concept  of 

NATURAL  LANGUAGE 

827 

GRABL-1  we  were  able  to  extend  its  functionality  and  ex(cid:173)
periment on GRAF.L-2 as a grammar rule discovery method. 
The  results showed that  GRAEL-2  could produce a broader-
coverage grammar, but that GRAHL-1 's ability to optimize the 
distribution  of the probability mass  of a grammar was  coun(cid:173)
teracted.  A  grammar obtained  from  a  GRAiiL-2  society  is 
therefore unsuited to be directly applied to parsing.  But com(cid:173)
bining it again with a GRAEL-1  society however, goes a long 
way  in resolving this issue,  providing a grammar that has a 
broader coverage, as well as a better tuned probability mass 
distribution over the structures contained therein.  We there(cid:173)
fore  believe  the  combination  of GRAEL-2  and  GRAEL-1  to 
be an interesting optimization toolkit for any given grammar, 
and  corpus-induced  grammars  in  particular. 
It  can  achieve 
a significant optimization over the baseline method,  without 
using an external  information source, simply on the basis of 
knowledge transfer and mutation in a practical and evolution(cid:173)
ary context. 

Acknowledgments 
The  research  described  in  this  paper  was  financed  by  the 
FWO (Fund for Scientific Research). 

References 
[Antonisse,  1991]  H.  James  Antonisse.  A  grammar-based 
genetic  algorithm. 
In  Gregory  J.  E.  Rawlings,  editor, 
Foundations  of  genetic  algorithms,  pages  193-204.  Mor(cid:173)
gan Kaufmann, San Mateo, 1991. 

[Araujo, 2002]  Lourdes  Araujo.  A  parallel  evolutionary 
algorithm  for  stochastic  natural  language  parsing. 
In 
Proceedings  of The  Seventh  International  Conference  on 
Parallel  Problem  Solving  From  Nature,  pages  700-709, 
Granada, Spain, 2002. 

[Batali, 2002]  J.  Batali.  The negotiation and  acquisition of 
recursive grammars as a result of competition among ex(cid:173)
In  Ted  Briscoe,  editor,  Linguistic  Evolution 
emplars. 
through  Language  Acquisition:  Formal  and  Computa(cid:173)
tional  Models,  chapter  5.  Cambridge  University  Press, 
2002. 

[Belz, 2001]  A.  Belz.  Optimisation of corpus-derived prob(cid:173)
abilistic  grammars.  In  Proceedings of Corpus Linguistics 
2001, pages 46-57, Lancaster University, UK, 2001. 

[Bod,  1998]  R.  Bod.  Beyond  Grammarâ€”An  Experience-
Based Theory of Language. Cambridge University Press, 
Cambridge, England, 1998. 

[Briscoe,  1998]  E.  Briscoe.  Language  as  a complex adap(cid:173)
tive system:  co-evolution of language and of the language 
In  Proceedings  of the  8th  Meeting of 
acquisition  device. 
Comp.  Linguistics  in  the  Netherlands,  pages  pp.  3-40., 
Amsterdam, 1998. Rodopi. 

[Chappelier and Rajman,  1998]  J.-C. Chappelier and M. Ra-
jman.  A generalized cyk algorithm  for parsing stochastic 
cfg.  In Proceedings of Tabulation  in  Parsing and Deduc(cid:173)
tion (TAPD '98), pages  133-137, Paris (FRANCE),  1998. 

[Charniak, 2000]  Eugene  Charniak.  A  maximum-entropy-
inspired parser. In Proceedings of NAACL'00, pages 132-
139, Seattle, USA, 2000. 

[Collins,  1999]  M.Collins.  Head-driven  Statistical Models 
for  Natural Language Parsing.  PhD  thesis,  University  of 
Pennsylvania, Pennsylvania, USA,  1999. 

[Collins, 2000]  Michael  Collins.  Discriminative  reranking 
In  Proc.  17th  Interna(cid:173)
for  natural  language  parsing. 
tional  Conf  on  Machine  Learning,  pages  175-182.  Mor-
gan Kaufmann, San Francisco, CA, 2000. 

[De Pauw, 2000]  G.  De Pauw.  Aspects of pattern-matching 
In  Proceedings of the  18th  International Confer(cid:173)
in  dop. 
ence on  Computational Linguistics, pages 236-242, 2000. 
[De Pauw, 2002] G. De Pauw. An Agent-Based Evolutionary 
Computing Approach  to  Memory-Based Syntactic Parsing 
of  Natural  Language.  PhD  thesis,  University  of  Antwerp, 
Antwerp, Belgium, 2002. 

[Goodman,  1998]  J.  Goodman.  Parsing  Inside-Out.  PhD 

thesis, Harvard University, Massachusetts, USA, 1998. 

[Henderson and Brill, 2000]  J. Henderson and E. Brill.  Bag(cid:173)
ging and boosting a treebank parser. In Proceedings of the 
1st Meeting of the North American Chapter of the Associ(cid:173)
ation/or  Computational Linguistics  (NAACL-2000),  pages 
34-41,2000. 

[Kirby, 2001]  S.  Kirby.  Spontaneous evolution of linguistic 
structure:  an iterated learning model  of the emergence of 
regularity  and  irregularity.  IEEE  Transactions  on  Evolu(cid:173)
tionary Computation, 5(2): 102-110, 2001. 

[Klein and Manning, 2001]  Dan  Klein  and  Christopher  D. 
Manning.  Parsing  with  treebank  grammars:  Empirical 
bounds, theoretical  models,  and the structure of the penn 
treebank.  In Prodeedings ofACL-EACL 2001, pages  330-
337,2001. 

[Losee,  1995]  R.M. Losee.  Learning syntactic rules and tags 
with genetic algorithms for information retrieval and filter(cid:173)
ing:  An empirical basis for grammatical rules.  Information 
Processing and Management, 32(2): 185-197,  1995. 

[Marcus et al,  1993]  M.  P.  Marcus,  B.  Santorini,  and  M.A. 
Marcinkiewicz.  Building  a  large  annotated  corpus  of 
the  penn  treebank.  Computational  linguistics, 
english: 
19:313-330,  1993.  Reprinted  in  Susan  Armstrong,  ed. 
1994,  Using large corpora,  Cambridge,  MA:  MIT  Press, 
273-290. 

[Smith and Witten,  1996]  T.  C.  Smith  and  I.  H.  Witten. 
Learning  language  using  genetic  algorithms. 
In  Stefan 
Wermter, Ellen Riloff, and Gabriele Scheler, editors, Con-
nectionist,  Statistical,  and Symbolic Approaches to Learn(cid:173)
ing for  Natural  Language  Processing,  volume  1040  of 
LNAI, pages  132-145. Springer Verlag, Berlin, 1996. 

[Wyard,  1991]  P.  Wyard.  Context-free  grammar  induction 
using  genetic  algorithms.  In  R.  Belew and  L.B.  Booker, 
editors,  Proceedings  of the  Fourth  International  Confer(cid:173)
ence on  Genetic Algorithms,  pages  514-518,  San Mateo, 
1991. ICG A, Morgan Kaufmann. 

828 

NATURAL  LANGUAGE 

