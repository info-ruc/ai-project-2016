A Continuation Method for Nash Equilibria in Structured Games 

Ben Blum 

Stanford University 
bblum@stanford.edu 

Christian R. Shelton 
Stanford University 

cshelton@cs.stanford.edu 

Daphne Koller 

Stanford University 

koller@cs.stanford.edu 

Abstract 

We describe algorithms for computing Nash equilibria in 
structured game representations,  including  both  graphi(cid:173)
cal games and multi-agent influence diagrams (MAIDs). 
The  algorithms  are derived  from  a continuation  method 
for normal-form and extensive-form games due to Govin-
dan  and  Wilson;  they  follow  a  trajectory  through  the 
space of perturbed games and their equilibria.  Our algo(cid:173)
rithms  exploit  game  structure  through  fast  computation 
of the Jacobian  of the game's  payoff function.  They  are 
guaranteed  to  find  at  least  one  equilibrium  of the  game 
and may find more.  Our approach provides the first exact 
algorithm for computing an exact equilibrium in graphi(cid:173)
cal games with arbitrary topology, and the first algorithm 
to exploit fine-grain structural  properties of MAIDs.  We 
present experimental results for our algorithms.  The run(cid:173)
ning time for our graphical game algorithm is similar to, 
and  often  better  than,  the  running  time  of previous  ap(cid:173)
proximate  algorithms.  Our algorithm for MAIDs can ef(cid:173)
fectively solve games that arc much larger than those that 
could be solved using previous methods. 

Introduction 

1 
Game theory is a mathematical framework that describes in(cid:173)
teractions between  multiple  rational  agents  and  allows  for 
reasoning about their outcomes.  However,  the complexity 
of standard game descriptions grows exponentially with the 
number of agents involved. For many multi-agent situations, 
this  blowup  presents  a  serious  problem.  Recent  work  in 
artificial intelligence [La Mura,  2000; Kearns et  al,  2001; 
Koller and Milch, 2001 ] proposes the use of structured game 
representations that utilize a notion of locality of interaction; 
these representations allow a wide range of complex games 
to be represented compactly. 

In this paper we consider the task of computing Nash equi(cid:173)
libria for structured games. A Nash equilibrium is a strategy 
profile such that it is no agent's interest to deviate unilaterally. 
A naive approach to finding Nash equilibria is to convert the 
structured game into a standard game representation, and ap(cid:173)
ply a standard game-theoretic solution algorithm iMcKelvey 
and McLennan, 1996].  This approach is, in general, infea-
sible for all but the simplest games.  We would like an algo(cid:173)
rithm that exploits the structure in these game representations 
for efficient computation. 

In this paper, we describe a set of algorithms that use con-
tinuation methods for solving structured games. These algo(cid:173)
rithms  follow a trajectory of equilibria of perturbed games 
until an equilibrium of the original game is found. Our algo(cid:173)
rithms are based on the recent work of Govindan and Wil(cid:173)
son  [2002; 2003a; 2003b] (GW hereafter), which apply to 
standard game representations (normal-form and extensive-
form).  We show how the structure of the games can be ex(cid:173)
ploited to perform the key computational step of the algo(cid:173)
rithms of GW. 

Our methods address both graphical games [Kearns et ai, 
2001] and multi-agent influence diagrams (MAIDs) [Koller 
and Milch, 2001].  We present the first algorithm for finding 
exact equilibria in graphical games of arbitrary structure. We 
also provide the first algorithm that can take advantage of the 
fine-grained structure of MAIDs,  and therefore can  handle 
MAIDs which are significantly outside the scope of previous 
methods. We provide experimental results demonstrating the 
efficiency of our approach relative to previous methods. 

2  Game Representations and Equilibria 
2.1  Game Theory 
We begin by briefly reviewing concepts from game theory 
used in this paper, referring to Owen [1995] for a good intro(cid:173)
duction.  A game defines an interaction between a set N of 
agents.  Each agent n  e  N has a set of available strategies 
, where a strategy determines the agent's behavior in the 
game.  The precise definition of the set 
depends on the 
game representation, as we discuss below.  A strategy pro-
file a defines a strategy 
Given a 
strategy profile  , the game defines a payoff 
for each 
to denote the strategy profiles of 
agent 
the agents 
to refer to 
the set of all strategy profiles of agents in  

We use 
N  -  _ .  Similarly,  we use 

for each 

A solution to a game is a prescription of a strategy profile 
for the agents.  The agent's goal is to maximize its payoff. 
Thus, a basic desideratum for a solution profile is stability 
— it should not be in any agent's interests to deviate from 
it. More precisely, the fundamental notion of a Nash equilib(cid:173)
rium [Nash,  1951] is defined to be a strategy profile a such 
that, for all 
for all 
other strategies 
' ~~ . Thus, if an agent knew that the oth(cid:173)
ers were playing according to an equilibrium profile, it would 
have no incentive to deviate. 

MULTIAGENT SYSTEMS 

757 

An e-equilibrium is a strategy profile such that no agent can 
improve  its  expected  payoff by  more than  by  unilaterally 
changing its strategy.  Unfortunately, finding an  -equilibrium 
is not necessarily a step toward  finding  an exact equilibrium: 
the fact that 
-equilibrium does not guarantee the ex(cid:173)
istence of an exact equilibrium in the neighborhood of a. 

is an 

Normal-Form  Games 
A  general-sum  normal-form  game  defines  a  simultaneous-
move multiagent scenario in which each agent independently 
selects an action and then receives a payoff that depends on 
the actions selected  by all  of the agents.  More precisely,  let 
G be a normal-form game with a set N of agents.  Each agent 
n 
N  has  a  discrete  action  set  Sn  and  a  payoff array  Gn 
with entries for every action profile  in   

Equilibrium strategies often require that agents randomize 
their  choice  of action.  A  mixed strategy 
is a probability 
distribution over Sn.  The set 
, is the set of all mixed strate(cid:173)
gies.  The support of a mixed strategy is the set of actions in 
Sn  that have non-zero probability.  A  strategy 
for agent n 
is said to be a pure strategy if it has only a single action in its 
support.  The set 
is thus an  m-vector,  where 
A  mixed  strategy  profile 
m  = 
Every game is guaranteed to have at least 
one mixed-strategy equilibrium, and the number of equilibria 
may be exponential in the number of agents. 

of mixed  strategy  profiles  is 

Extensive-Form  Games 
An  extensive-form  game  is  represented  by  a  tree  in  which 
each node represents a choice either of an agent or of nature. 
Each of nature's choice nodes is associated with a probability 
of 
distribution over its outgoing branches.  Each  leaf 
the  tree  is  associated  with  a  vector of payoffs 
,  where 
.  The choices 
of the agents and nature dictate which path of the tree is  fol(cid:173)
lowed and therefore the payoffs to the agents. 

denotes  the  payoff to  agent 

at  leaf 

The decision nodes belonging to each agent are partitioned 
into  information sets,  where each  information set  is  a set of 
states  among which the  agent cannot distinguish.  Thus,  an 
agent's strategy must take the same action at all nodes in the 
same information set. We define an agent history 
for a 
node y in the tree and an agent n to be a sequence containing 
the information sets traversed in the path from the root to y, 
and the action selected  at each  one.  Thus,  two nodes have 
the same agent-n history  if the paths used to reach them are 
indistinguishable to n.  (The paths  may differ in  other ways, 
such  as  nature's decisions or the  decisions  of other agents.) 
We make the common assumption of perfect recall:  an agent 
does  not  forget information known  nor the choices  made at 
previous decisions.  More precisely,  if two nodes y, y'  are in 
the same information set for agent 

then 

 

We  need  a  representation  of a  strategy  for  an  extensive-
form game; unlike the case of normal-form games, there are 
several  quite  different choices.  For our purposes,  the  most 
appropriate  representation  is  the  sequence form  [Koller  and 
Megiddo, 1992; von Stengel, 1996]. Here, the strategy an for 
an agent n is represented as a vector of real values of size hn, 
for a  leaf z  in  the  tree. 
one  for each  distinct  history 
The number 
, is the product of 
the probabilities controlled by agent n along the history  

, abbreviated 

From this representation, we can easily derive the probability 
of taking an action at a particular information set. 

, where 

The set of sequence form strategies for agent n is therefore 
is  at  most the  number of leaves 

a subset  of 
in  the  tree.  The  set  of legal  sequence  form  strategies 
for  agent  n  is  defined  by  a  set  of linear constraints  on  vec-
.  The  set  of sequence  form  strategy  profiles  is 
tors  in 
The payoff to agent n  in  an 
then defined as 
extensive-form game can be shown to be 

(1) 

Thus, the payoffs arc a sum, over the leaves in the tree, of the 
payoff at a leaf times the product of the sequence form param(cid:173)
eters for that leaf.1  Importantly, this expression has a similar 
multi-linear form to the payoff in a normal-form game, but us(cid:173)
ing sequence form strategies rather than mixed strategies.  In 
an extensive form game satisfying perfect recall,  any mixed 
strategy equilibrium can be represented using  an essentially 
equivalent sequence form strategy profile. 

2.2  Structured  Representations 
Graphical  Games 
The size of the payoff arrays required to describe a normal-
form  game  grows  exponentially  with  the  number of agents. 
Kearns  et al.  12001]  introduced  the  framework ol\ graphical 
games, which provide a more structured representation based 
on probabilistic graphical  models.  Graphical  games capture 
local  structure  in  multi-agent  interactions,  allowing  a  com(cid:173)
pact representation for scenarios where each agent's payoff is 
only affected by a small  subset of other agents.  Examples of 
interactions where this structure occurs include agents that in(cid:173)
teract along organization hierarchies and agents that interact 
according to geographic proximity. 

A  graphical  game  is  described  like  a  normal-form game. 
The  basic  representation  (slightly  generalized)  is  a  directed 
graph with one node for each agent.  An edge from agent n1 
to agent n in the graph indicates that agent n's payoffs depend 
on the action of agent n'.  More precisely, we define Fam n  to 
be the set of agents consisting of n itself and its parents in the 
graph. The agent's payoff 
is an array indexed only by the 
actions  of the  agents  in  Famn.  Thus,  the  description  of the 
game is exponential  in the in-degree of the graph and  not in 
the total number of agents.  In this case, we use 
to refer to strategy profiles of the agent in  Famn   
Multi-agent Influence  Diagrams 
The  description  length  of extensive-form  games  often  also 
grows  exponentially  with  the  number  of  agents. 
In  many 
situations  this  large  tree  can  be  represented  more  com(cid:173)
pactly.  Multi-agent influence diagrams  (MAIDs)  [Koller  and 
Milch,  2001]  allow  a structured representation of games  in(cid:173)
volving  time  and  information  by  extending  influence  dia(cid:173)
grams [Howard and Matheson, 1984] to the multi-agent case. 
A  MAID  is  represented  as  a  directed  acyclic  graph  over 
nodes of three types:  chance,  decision,  and  utility.  (Utility 

1 For notational simplicity, Gn{z) includes nature's probabilities. 

758 

MULTIAGENT SYSTEMS 

nodes are assumed to have no children.) Chance nodes repre(cid:173)
sent  nature's  actions,  and  a  conditional probability  distribu-
tion (CPD) is associated with each such node, describing the 
distribution over outcomes of that variable conditioned on the 
values of its parents.  Each decision node is associated with 
a single agent.  The parents of a decision node represent the 
variables  whose  values  are  known  to  the  agent  when  mak(cid:173)
ing that decision.  Thus,  an agent's decision rule for a node 
can specify a different strategy for each assignment of values 
to the node's parents.  In effect, each such assignment corre(cid:173)
sponds to an information set. A randomizing decision rule for 
a decision node is simply a CPD: a distribution over its values 
for each instantiation of its parents. 

Each utility  node  is  associated  with  an  agent,  and repre(cid:173)
sents  a  component  in  that  agent's  payoff.  The  utility  node 
takes  real  values  as  a  deterministic  function  of its  parents. 
Thus, that component of the agent's payoff depends only on a 
subset of the variables in the MAID. The agent's overall util(cid:173)
ity  is  the  sum  of the  utilities  obtained  at  its  different  utility 
nodes.  It is easy to show that a MAID defines an extensive-
form game.  If we use CPDs and decision rules that are tree-
structured,  then  the  MAID  representation  is  no  larger  than 
the corresponding extensive-form representation, and is expo(cid:173)
nentially smaller in many cases. Note that a graphical game is 
simply a MAID where each agent has a single decision node 
and a single utility node,  and  where the parents of an agent 
n's  utility node are the decision nodes for the agents in Famn. 
3  Continuation  Methods 
We  begin  with  a  high-level  overview of continuation  meth(cid:173)
ods, referring the reader to [Watson, 2000] for a more detailed 
discussion.  Continuation methods work by solving a simpler 
perturbed problem and then tracing the solution as the mag(cid:173)
nitude of the perturbation decreases, converging to a solution 
to the original problem. 

More precisely,  let 

be a scalar parameterizing a contin(cid:173)
uum of perturbed problems. When  = 0, the perturbed prob(cid:173)
lem is the original one; when 
=  1,  the perturbed problem 
is one for which the solution  is  known.  Let w  represent the 
vector of real values of the solution.  For any perturbed prob(cid:173)
lem defined by  (cid:173)  we characterize  solutions by  the equation 
=  0,  where  F  is  a  real-valued  vector function  (so 
that  0  is  a  vector of zeros).  The  function  F  is  such  that  if 
=  0 holds then w  is a solution to the problem per(cid:173)

turbed by 

. 

satisfying 

The continuation method traces solutions along the mani(cid:173)
=  0.  Specif(cid:173)
fold of solution pairs 
ically,  if we  have  a  solution  pair 
we  would  like  to 
trace that solution to adjacent solutions.  Differential changes 
to  w  and  must  cancel  out  so  that  F  remains  equal  to  0. 
Thus, locally, changes dw  and dX along the path must obey 
which is equivalent to 

the matrix equation 

If the  matrix 

has a null-space of rank 1 every(cid:173)
where, the curve is uniquely defined.  If properly constructed, 
the curve starting at 
=  0,  at 

=  1  is guaranteed to cross 

(2) 

which point the corresponding value of w is a solution to the 
original problem. A continuation method begins at the known 
solution  for  =  1  .  The null-space of the Jacobian 
at a 
current solution 
defines a direction, along which the 
solution  is moved by a small amount.  The process then re(cid:173)
peats, tracing the curve until  =  0.  The cost of each step in 
this computation, given the Jacobian,  is cubic in the size of 
the Jacobian, due to the required matrix operations. 

Continuation  methods  involve  the  tracing  of a  dynamical 
system through the continuous variation of the  parameter 
. 
For computational purposes, discrete steps must be taken. As 
a  result,  error  inevitably  accumulates  as  the  path  is  traced. 
One can use several techniques to reduce the error, which we 
do not describe for lack of space.  Unfortunately, these tech(cid:173)
niques can potentially send the algorithm into a cycle, and in 
practice they occasionally do.  If the algorithm cycles, random 
restarts and a decrease in step size can improve convergence. 
4  Continuation Methods for Games 
We  now  review  the  work  of Kohlberg  and  Mertens  [1986] 
and GW on applying the continuation method to the task of 
finding equilibria  in  games.  These  algorithms  form  the ba(cid:173)
sis  for  our  extension  to  structured  games,  described  in  the 
next section.  The continuation method perturbs the game by 
adding  A  times  a  fixed  bonus  to  each  agent's  payoffs,  such 
that an agent's bonus depends only on its own actions.  If the 
bonuses are large enough (and unique), the bonuses dominate 
the original game structure, and the agents need not consider 
their opponents' plays.  Thus,  for  =  1, the perturbed game 
has only one equilibrium: each agent plays the action with the 
largest bonus. We then use the continuation method to follow 
a path in the space  of  and equilibrium profiles  for the re(cid:173)
sulting perturbed game, decreasing 
until it is zero;  at this 
point, the corresponding strategy profile is an equilibrium of 
the original game. We now make this intuition more precise. 
4.1  Normal Form Games 
In  order to apply Eq.  (2),  we  need to characterize the equi(cid:173)
libria of perturbed games  as  the  zeros  of a function  F.  We 
first define an auxiliary function measuring the benefit of de(cid:173)
viating from a given strategy profile.  Specifically, 
is a 
vector payoff function  of the  payoff to  agent n  for deviating 
from the mixed strategy profile a by playing each action s: 

We  now  define  a  retraction  operator  R  : 

to 
be an operator that maps arbitrary  ra-vectors  w  to the point 
of  mixed  strategies  which  is  nearest  to  w 
in  the  space 
in  Euclidean  distance.  As  outlined  in  the  structure  theo(cid:173)
rem  of  Kohlberg  and  Mertens  [1986],  an  equilibrium 
is 
by  the  retraction  operator  R: 
recoverable  from 
In  fact,  this condition  is  a full  charac(cid:173)
terization of equilibria.  Thus,  we can define an equilibrium 
as a solution to the equation 
Conversely, 
we  have  the equivalent 
if 
condition that 
Thus, we can search 
for a  point 
which  satisfies  this equality,  in  which 
case 

is guaranteed to be an equilibrium. 

and 

_ 

MULTIAGENT SYSTEMS 

759 

We now define the game perturbation and associated con(cid:173)
tinuation method.  For a target game G with  payoff function 
V,  we create an easily soluble perturbed game by adding an 
rn-vector  6  of unique  bonuses  that  agents  receive  for  play(cid:173)
ing  certain  actions,  independently  of  what  all  other  agents 
do.  In perturbing the game G by b, we arrive at a new game 

in  which  for each 

,  and  for any  t   
If we  make  6  sufficiently 
has  a  unique equilibrium,  in  which each 

large, then 
agent plays the pure strategy s for which 

is maximal. 

Now,  let  V  be the  payoff function  for the target game G. 
is also perturbed from 

The  induced payoff function 

. Thus, the form of our continuation equation is: 

We have that 
game 
equilibrium  of 

(3) 
is the payoff function for the perturbed 
is an 
the game is unperturbed, so 

is  zero if and only  if 

is an equilibrium of G. 

The expensive step in the continuation method is the cal(cid:173)
culation  of the Jacobian 
required for the computation 
that maintains the constraint of Eq.  (2).  Here,  we  have that 
identity ma(cid:173)
trix.  The hard part is the calculation  of 
For pure strate(cid:173)
,  the value at location  (s1,  s2) 
gies 
in 
is equal  to the expected payoff to agent  n1  when 
it plays the pure strategy  s1,  agent 
plays the pure strategy 
.s2, and all other agents act according to the strategy profile a: 

, where I is the 

and 

Computing  Eq.  (4)  requires  a  number  of  multiplications 
which  is  exponential  in  the game  size;  the  sum  is  over the 
exponentially large space  

4.2  Extensive F o rm Games 
The same method applies to extensive-form games, using the 
sequence form parameterization of strategies.  We  first  need 
to define the bonus vector, and the retraction operator which 
allows us to characterize equilibria. 

The bonus vector in  the  extensive-form adds  a bonus for 
each  sequence  of each  agent.  GW  show  that  a  sufficiently 
large bonus guarantees a unique equilibrium for the perturbed 
game.  The retraction operator R takes  a general  vector and 
projects it onto the valid region of sequence forms.  As all of 
the constraints are linear, the projection amounts to a decom(cid:173)
posable quadratic program (QP) that can be  solved quickly. 
We employ standard QP methods. The Jacobian of the retrac(cid:173)
tion is easily computable from the set of active constraints. 

The  solution  for  the  sequence  form  is  now  surprisingly 
similar  to  that  of the  normal-form.  The  key  property of the 
sequence form strategy representation is that the payoff func(cid:173)
tion  is a multi-linear function of the extensive-form parame(cid:173)
ters, as shown in Eq. (1).  The elements of the Jacobian 
also have the same general structure. In particular the element 
corresponding to sequence  s1  for agent  n1  and  sequence s2 
for agent  n2  is 

Figure 1:  (a) An abstract diagram of the path.  The horizontal axis 
represents  and the vertical axis represents the space of strategy 
profiles  (actually  multidimensional).  The algorithm  starts on  the 
right at 
=  0 at 
point  1, where it has found an equilibrium of the original game.  It 
can continue to trace the path and find the equilibria labeled 2 and 3. 
(b) Two-stage road building MAID tor three agents. 

=1  and  follows  the  dynamical  system until 

to be the empty set (and hence 

where 
is  the set of leaves  that are consistent with  the 
sequences  s1  (for agent  n1)  and  s2  (for agent n2)-  We  take 
if  s1  and  s2 
are incompatible. Eq. (5) is precisely analogous to Eq. (4) for 
normal-form games.  We have a sum, over outcomes, of the 
utility of the outcome multiplied by the strategy probabilities 
for all other agents.  Note that  this sum is over the  leaves of 
the tree, which may be exponential in the number of agents. 
Zero-probability actions in extensive-form games give rise 
to an  additional  subtlety.  Such  actions  induce a probability 
of zero for entire  trajectories  in  the  tree,  possibly  leading to 
equilibria based on unrealizable threats and other undesirable 
phenomena.  For us, they can also lead to bifurcations in the 
continuation  path,  preventing  convergence.  Thus,  we  con(cid:173)
strain all sequence form parameters to be greater than or equal 
to e for some small e. This constraint ensures that the contin(cid:173)
uation path is a 1-manifold. The algorithm thus finds an equi(cid:173)
librium to a perturbed game, where agents have a small prob(cid:173)
ability of choosing an unintended action.  As c tends to zero, 
these  equilibria  converge to perfect  equilibria  of the  original 
game  [Owen,  1995],  a  (nonempty)  subset  of all  equilibria. 
For e  small  enough,  continuity  implies  that  there  is  always 
an  exact  perfect equilibrium  in  the  vicinity  of the  perturbed 
equilibrium, which can easily be found using local search. 

4.3  Path Properties 
In  the  case  of normal-form games,  the  structure theorem  of 
Kohlberg and Mertens  [1986]  implies  that,  with  probability 
one over all choices for 6, the path of the algorithm is a one-
manifold without boundary. GW provide an analogous struc(cid:173)
ture theorem that guarantees the same property for extensive-
form games.  Figure 1 (a) shows an abstract representation of 
the path  followed by the continuation method.  The equilib(cid:173)
is unique, so the one-manifold can(cid:173)
rium for large positive 
not  double back  to  the  side  of 
Furthermore,  the 
perturbed games along the path can have only a finite number 
of discrete equilibria, so the path cannot travel back and forth 
indefinitely.  Therefore,  it must cross the 
=  0  hyperplane 
at least once, yielding an equilibrium.  In fact, the path may 
cross multiple times, yielding many equilibria in a single run. 
As the path must eventually continue to the 
side, it 
will find an odd number of equilibria when run to completion. 
In  both normal-form and extensive-form games,  the path 
is piece-wise polynomial, with each piece corresponding to a 

= 

760 

MULTIAGENT SYSTEMS 

different support set of the strategy profile.  These pieces are 
called support cells.  The path  is  not  smooth  at cell  bound(cid:173)
aries,  due  to  discontinuities  in  the  Jacobian  of  the  retrac(cid:173)
tion operator, and hence in 
when the support changes. 
Thus, in following the path, care must be taken to step up to 
these boundaries exactly. 

In the case of two agents, the path is piece-wise linear and, 
rather than taking steps, the algorithm can jump from "elbow" 
to "elbow" along this path. When this algorithm is applied to 
a two-agent game and a particular bonus vector is used,  the 
steps from support cell to support cell that the algorithm takes 
are exactly equal to the pivots of the Lemke-Howson solution 
algorithm [Lemke and Howson,  1964]  for two-agent games, 
and  the  two  algorithms  find  precisely  the  same  set  of solu(cid:173)
tions. Thus, the continuation method is a strict generalization 
of the Lemke-Howson algorithm that allows different pertur(cid:173)
bation rays and games of more than two agents. 

Iterated  Polymatrix  Approximation 

4.4 
Because perturbed games may themselves have an exponen(cid:173)
tial  number of equilibria,  and  the  path  may  wind  back  and 
forth through any number of them, the continuation algorithm 
can take a while to trace its way back to a solution to the orig(cid:173)
inal game. We can speed up the algorithm using an initializa(cid:173)
tion  procedure  based  on  the  iterated polymatrix  approxima(cid:173)
tion  (IPA) algorithm of GW.  A polymatrix game  is a normal-
form game where the payoffs to a agent n are equal to the sum 
of the payoffs from a set of two-agent games, each involving 
n and another agent. Polymatrix games can be solved quickly 
using the Lemke-Howson algorithm [1964]. 

Given a normal-form game G and a strategy profile c  we 
can  construct a polymatrix game Pa  whose Jacobian at o  is 
the same as the Jacobian of G's payoff function V  at a.  The 
game Pa  is  a  linearized  approximation  to  G  around cr,  and 
can be computed efficiently from the Jacobian of G.  GW pro(cid:173)
vide an iterative algorithm that, in each step, takes a profile o 
and  improves it using the solution of the polymatrix approx(cid:173)
imation  Pa.  This  algorithm  is  not  guaranteed  to  converge, 
but in practice, it quickly moves "near" a good solution.  We 
then construct a perturbed game close to the original game for 
which this approximate equilibrium is an exact equilibrium. 
The continuation method is then run from this starting point 
to find an exact equilibrium of the original game. 

5  Exploiting  Structure 
As mentioned above, the calculation of 
at each step of the 
algorithm consumes  most of the  time.  Both  in  normal-form 
and  (in  the  worst case)  in extensive-form games,  it requires 
time  that  is  exponential  in  the  number of agents.  However, 
as we show in this section, when using a structured represen(cid:173)
tation  such  as  a graphical  game or a MAID,  we can  effec(cid:173)
tively exploit the  structure  of the  game  to  drastically  reduce 
the computational time required. 
5.1  Graphical Games 
Consider  the  computation  of  the  normal-form  Jacobian  in 
Eq.  (4).  The key insight is that the choice of strategy for an 
agent outside the family of  n1  does not affect 
.  This ob(cid:173)
servation allows us to compute the  n1  entries in the Jacobian 

locally, considering only n1 's family. More precisely, we can 
consider two cases.  If n2 

Famn1,  then 

Letting  /  be  the maximal  family  size,  and d the maximal 
number of actions per agent, we have that the computation of 
the Jacobian requires time  
5.2  MAIDs 
The Jacobian for MAIDs 
To  find  equilibria  in  MAIDs,  we  extend  the  sequence  form 
continuation method of Section 4.2.  As above, our key task is 
computing the Jacobian of Eq. (5). The Jacobian has an entry 
for each pair of sequences in  the game (one for each agent). 
We therefore begin  by  noting that the sequence form repre(cid:173)
sentation for MAIDs with perfect recall  is no larger than the 
agent's  decision rules  for that  MAID.  Due  to  perfect recall, 
the  last  decision  node  (in  topological  order)  must  have  in(cid:173)
coming edges from all of its previous actions and all parents 
of previous  actions.  Moreover,  it  must  have  an  information 
set for any  distinct agent history.  Thus, the agent's decision 
rule for that  final  decision has the same size as the sequence 
form.  Hence,  the dimension m  of the 
Jacobian ma(cid:173)
trix  is  linear in  the  size  of the MAID (where size,  as  usual, 
includes the size of the parameterization). 

We  next  turn  to  the  computation  of the  Jacobian  entries. 

Eq. (5) can be rewritten as 

(8) 

A  leaf node  z  in  the  extensive-form game  is  simply  an  as(cid:173)
signment x  to all  of the variables  in  the MAID,  and 
is  n1's  utility  given  x.  The  sequence  probability 
is 
the  product  of  the  probabilities  for  the  decisions  of  agent 
n  in  the  assignment  x.  Thus,  Eq.  (8)  is  an  expectation  of 
. The expectation is over the distribu(cid:173)
tion defined  by the Bayesian  network 
whose structure is 
the same as the MAID, and where the agents' decision nodes 
have CPDs determined by  

The  agent's  utility 

is  the  sum of its  utility  nodes. 
Due to linearity of expectation, we can perform the computa(cid:173)
tion separately for each of the agent's utility nodes, and then 
simply add up the separate contributions.  Thus,  we assume 
from  here on,  without loss  of generality,  that  n 1  has only  a 
single utility node U. 

The value  of 

depends only on the values 
of the set of nodes £>„,  consisting of n1;'s decision nodes and 

MULTIAGENT SYSTEMS 

761 

, and 

their parents. Thus, instead of computing the probabilities for 
all assignments to all variables, we need only to compute the 
marginal joint distribution  over 
From this 
distribution, we can compute the expectation in Eq. (5). 
Using Bayesian Network Inference 
Our  analysis  above  reduces  the  required  computations  sig(cid:173)
nificantly.  We  need only compute one joint distribution for 
every pair of agents  ri1,  n 2.  This joint distribution is the one 
defined by  the Bayesian  network 
.  Naively, this compu(cid:173)
tation  requires  that  we  execute  Bayesian  network  inference 
times:  once for each ordered pair of agents n1, n-2. For-
tunately, we can exploit the structure of the MAID to perform 
this computation much more efficiently. 

The  basis  for  our  method  is  the  clique  tree  algorithm 
of Lauritzen  and  Spiegelhalter  [1998].  A  clique  tree  for  a 
Bayesian network 
is a data structure defined over an undi(cid:173)
rected tree over a set of nodes C. Each node 
is a subset 
of the nodes in B called a clique. The clique tree satisfies cer(cid:173)
tain  important  properties.  It  must  be family preserving:  for 
each node X  in B,  there exists a clique 
such  that X 
and its parents are a subset of Ci. It also satisfies a separation 
requirement:  if C2 blocks the path from C1 to C 3, then, in the 
distribution defined by  we have that the variables in C\  are 
conditionally independent of those in C3  given those in C2. 
Each clique maintains a data structure,  called a potential, 
which  is  an  unnormalized distribution  over the  variables  in 
Ci.  The size of the potential for Ci  is therefore exponential in 
. The clique tree inference algorithm proceeds by passing 
messages  from one clique  to  another in  the tree.  The  mes(cid:173)
sages are used to update the potential in the receiving clique. 
After  a  process  in  which  messages  have  been  sent  in  both 
directions over each  edge  in  the tree,  the  tree  is  said  to  be 
calibrated; at this point, the potential of every clique  C i  con(cid:173)
tains  precisely  the joint  distribution  over the  variables  in  Ci 
according to B. 

We can use the clique tree algorithm to perform inference 
over Ba.  Now,  consider the  final  decision  node for agent ni. 
Due to the perfect recall assumption, all of  nis  previous de-
cisions and all of their parents are also parents of this decision 
node. The family preservation property therefore implies that 
Dni  is fully contained in some clique.  Thus, the expectation 
of Eq.  (8)  requires  the  computation  of the joint  distribution 
over three cliques in the tree:  the one containing (7, the one 
containing  Dni,  and  the  one  containing  Dn2.  We  need  to 
compute this joint distribution for every pair of agents n 1, n 2. 
The  first  key  insight  is  that  we  can  reduce  this  problem 
to  one  of  computing  the joint  marginal  distribution  for  all 
pairs  of  cliques  in  the  tree.  Assume  we  have  computed 
PB(CI,CJ)  for  every  pair  of  cliques  C i,  Cj.  Now,  consider 
any  triple  of cliques  C1,  C2, C3-  There  are  two  cases:  ei(cid:173)
ther  one  of these  cliques  is  on  the  path  between  the  other 
two,  or not.  In  the  first  case,  assume  without  loss  of gen(cid:173)
erality that C2  is on the path from C\  to C3.  In this case, by 
the  separation  requirement,  we  have  that  PB(C1,  C2, C3)  = 
PB(C1C2)PB{C2,C3)/PB(C2)- 
In  the  second  case,  there 
exists a unique clique C* which blocks the paths between any 
pair of these cliques.  Again, by the separation property, C* 
renders  these  cliques  conditionally  independent,  so  we  can 
use a similar method to compute P B { C 1 , C2, C3). 

Thus,  we  have reduced the  problem to one of computing 
the marginals over all pairs of cliques in a calibrated clique-
tree.  We can use dynamic programming to execute this pro(cid:173)
cess efficiently.  We construct a table that contains  PB(Ci,  Cj) 
for each  pair of cliques  Ci,Cj.  We  construct the  table  in  or(cid:173)
der of length of the path from Ci  to Cj.  The base case is when 
Ci and Cj  are adjacent in the tree.  In this case, we have that 
The probability 
expressions in the numerator are simply the clique potentials 
in the calibrated tree.  The denominator can  be obtained by 
marginalizing  either of the  two  cliques.  For  cliques  C i  and 
Cj  that are not adjacent,  we  let  Ck  be  the  node  adjacent to 
Cj  on  the  path  from  Ci  to  Cj.  The  clique  Ck  is  one  step 
closer to  Ci,  so, by construction, we have already computed 
P(Ci ,C k). We can now apply the separation property again: 

Let  be the  number of cliques  in  the  tree,  and  d be  size 
of the  largest  clique  (the  number of entries  in  its  potential). 
The cost of calibrating the clique tree  for 
.  The 
cost of computing Eq.  (9) for all pairs of cliques is 
Finally, the cost of computing the  S i,Sj  entry of the Jacobian 
is  0(d4). 
In  games  where  interactions  between  the  agents 
are highly structured, the size d of the largest clique can be a 
constant even  as  the  number of agents  grows.  In  this  case, 
the  complexity  grows  only  quadratically  in  the  number  of 
cliques, and hence also in the number of agents. 

is 

6  Results 
6.1  Graphical  Games 
We compared two versions of our algorithm:  cont,  the sim(cid:173)
ple  continuation  method,  and 
the  continuation 
method with the IPA initialization.  We compared our results 
to  the published results  of the  the  algorithm of Vickrey  and 
Roller  12002]  (VK  hereafter).  The  VK  method  only  returns 
e-equilibria,  but  their  approach  is  the  only  one  that  applies 
to graphical  games whose  interaction  structure is  not a  (bi-
connected)  tree  and  for  which  timing  results  are  available. 
The VK paper contains several different algorithms. We com(cid:173)
pared against the algorithm which had the smallest approxi(cid:173)
mation error for a given problem. 

Following VK,  our algorithms were run  on two classes of 
games,  of varying  size.  The Road  game,  denoting a  situa(cid:173)
tion  where  agents  must build in  land plots  along a road,  is 
played on  a 2-by-L grid;  each  agent has three actions,  and 
its payoffs depend only on the actions of its (grid) neighbors. 
Following  VK,  we constructed a game  where the payoff for 
an agent is simply the sum of payoffs of games played sepa(cid:173)
rately with its neighbors, and where each such subgame has 
the payoff structure of rock-paper-scissors.  This game  is,  in 
fact, a polymatrix game, and hence is very easy to solve us(cid:173)
ing  our  methods.  We  also  experimented with  a  ring  graph 
with three actions per agent and random payoffs. 

For each class of games, we chose a set of game sizes to run 
on.  For each, we selected, randomly in cases where the pay(cid:173)
offs were random, a set of at least ten (and up to one hundred 
for MAIDs) test games to solve.  We then solved each game 

762 

MULTIAGENT  SYSTEMS 

Figure 2: Results for graphical games: (a) Running time for road game with rock-paper-scissors payoffs. Results for ring game with random 
payoffs: (b) running time; (c) number of iterations of cont; (d) average time per iteration of cont. 

with a different random perturbation vector, 6, and recorded 
the time and number of iterations necessary to reach the first 
equilibrium.  We  then  averaged  over test  cases.  The  error 
bars show the variance due to the choice of perturbation vec(cid:173)
tor and,  for random games, the choice of game.  For smaller 
games, the algorithms always converged to an equilibrium. In 
about 40% of the larger games (more than 20 agents), the al(cid:173)
gorithms did not converge on the first trial; in these cases, we 
restarted the same game with a different random perturbation 
vector.  On average, about 2 restarts were sufficient for these 
difficult games.  In a  few  large graphical games (e.g.  9%  of 
games with 45 agents), IPA did not converge after 10 restarts; 
in these cases we did not record results for IPA+COnt.  In the 
other restarted cases, we recorded the time for the converging 
run. Our results are shown in Figures 2(a,b). 

In all cases, our algorithm found an equilibrium with error 
at most 
,  essentially  machine  precision.  In  the  Road 
games, we compared against the times for VK using their hill 
climbing method which found  -equilibria with error of 10 - 4. 
In these games, the cont method is more efficient for smaller 
games, but then becomes more costly.  Due to the polymatrix 
nature of this game, the 
cont solves it immediately with 
the Lemke-Howson algorithm, and is therefore significantly 
less expensive than  

In  the  random-payoff ring games, 

had an equilibrium 
error of about 0.01  using their cost minimization method with 
a grid discretization of 
Here, our algorithms are more ef(cid:173)
for smaller games (up to 20-30 agents), with 
ficient than 
performing  considerably  better  than  cont.  How(cid:173)
ever,  the running time of our algorithms grows more rapidly 
than that of 
so that for larger games, they become imprac(cid:173)
tical.  Nevertheless, our algorithms performed well  in games 
with up to  50  agents and  3  actions per agent,  games which 
were previously intractable for exact algorithms. 

Here,  we  also  plotted  the  number  of iterations  and  time 
per iteration for cont in Figures 2(c,d).  The number of itera(cid:173)
tions varies based both on the game and perturbation ray cho(cid:173)
sen. However, the time per iteration is almost exactly cubic as 
predicted.  We note that, when 
is used, the continuation 
method converges almost immediately (within a second). 

6.2  MAIDs 
Roller  and  Milch  [2001]  define  a  relevance graph  over the 
where there is an edge from Di  to 
decision nodes in a 
D2  if the  decision  rule  at  D\  impacts  the choice of decision 
rule at D2-  They show that the task of finding equilibria for a 
MAID can be decomposed, in that only decision nodes in the 

same  strongly  connected component  in  the  relevance  graph 
must be considered together.  However, their approach is un(cid:173)
able to deal with structure within a strongly connected com(cid:173)
ponent, and they resorted to converting the game to extensive 
form, and using a standard equilibrium solver.  Our approach 
addresses  the  complementary  problem,  dealing  specifically 
with this finer-grained structure. Thus, we focused our exper(cid:173)
iments on MAIDs with cyclic relevance graphs. 

We ran our algorithms on two classes of games, with vary(cid:173)
ing sizes.  The first, a simple chain, alternates between deci(cid:173)
sion and chance nodes with each decision node belonging to 
a different agent. Each agent has two utility nodes, each con(cid:173)
nected to its decision node and to a neighbor's (except for the 
end agents who have one utility node for their single  neigh(cid:173)
bor).  All  probability tables and payoff matrices are random. 
Our second example is shown in Figure  1(b).  It is an exten(cid:173)
sion of the graphical road game from above. Each agent must 
submit plans simultaneously 
for the type of building 
(home or store) that they will  build along a road.  However, 
building 
proceeds  from left  to right and so  before 
committing to a build,  an agent can see a noisy estimate of 
the plans of the agent to its  left 
.  Agents would pre(cid:173)
fer to be the first one to start a new type of building (i.e., be 
different than their left neighbors, but the same as their right 
neighbors). They also take a penalty if their building and plan 
decisions differ.  Carefully chosen  payoffs ensure non-trivial 
mixed strategies. 

Figures  3(a,b)  show  the  running  times  for  computing  an 
equilibrium  as  the  number  of  agents  is  increased  for  both 
types of games.  We compared our results  to those achieved 
by converting the game to extensive-form and running 
bit, a standard equilibrium computation package.  Our timing 
results for Gambit do not include the time for the conversion 
to extensive-form. 

Figures  3(c,d)  show  the  number of iterations  and  running 
time  per  iteration  for  the  case  of the  two-stage  road  game. 
The running time per iteration is once again well fit by a cu(cid:173)
bic. The variance is mainly due to the execution of the retrac(cid:173)
tion operator whose running time depends on the number of 
strategies in the support. 

7  Discussion and Conclusions 
In  the  last  few  years,  several  papers  have  addressed  the  is(cid:173)
sue of  finding  equilibria in  structured  games.  For graphical 
games, the exact algorithms proposed so far apply only to the 
very restricted class of games where the interaction structure 

MULTIAGENT SYSTEMS 

763 

Figure  3:  Results for MAIDs:  (a) Running times for the chain MAID.  Results for two-stage Road MAID:  (b) running time;  (c)  number of 
iterations; (d) time per iteration. 

is  an  undirected  tree,  and  where  each  has  only  two  possible 
actions  [Kearns  et ai,  2001;  Littman  et  al,  2002]. 

There  have  been  several  algorithms  proposed  for the  com(cid:173)
putation  of  e-equilibria  in  general  graphical  games,  most  of 
which  (implicitly  or  explicitly)  define  an  equilibrium  as  a 
set  of  constraints  over  a  discretized  space  of  mixed  strate(cid:173)
gies,  and  then  use  some  constraint  solving  method:  Kearns 
et  al  [2001]  use  a  tree-propagation  algorithm;  Vickrey, and 
Koller  [2002]  use  variable  elimination  methods  (VK1);  and 
Ortiz  and  Kearns  [2003]  use  arc-consi;.tency constraint prop(cid:173)
agation  followed  by  search.  Vickrey  and  Koller  [2002]  also 
propose  a  gradient  ascent  algorithm  (VK2).  The  running 
times  of KLS  and  V K 1  both  depend  on  the  tree-width  of the 
graph, whereas the  running times of our algorithm,  V K 2,  and 
OK  depend  on  the  degree of the  graph.  However,  these  latter 
three  algorithms all  require multiple  iterations and  no bounds 
are  currently  known  on  the  number of iterations  required. 

For  M A I D s,  Koller  and  Milch  [2001]  ( K M)  define  a  no(cid:173)

tion  of  independence  between  agents'  decision,  and  provide 
an  algorithm that can  decompose the problem based on fairly 
coarse  independence  structure.  Our  algorithm  is  able  to  ex(cid:173)
ploit a  much  finer-grained structure,  resolving an  open prob(cid:173)
lem  left  by  K M.  La  Mura  [2000]  ( L M)  proposes  a  continu(cid:173)
ation  method  for  finding  one  or  all  equilibria  in  a  G  net,  a 
representation  which  is  very  similar  to  M A I D s.  This  pro(cid:173)
posal  only  exploits  a  very  limited  set  of  structural  proper(cid:173)
ties  (a  strict  subset  of  KM  and  of  our  algorithm).  The  pro(cid:173)
posal was  also never implemented, and  several issues regard(cid:173)
ing  non-converging paths  seem  unresolved. 

We  have presented an  algorithm for computing exact equi(cid:173)
libria  in  structured  games.  Our  algorithm  is  based  on  the 
methods  of GW,  but  shows  how  the  key  computational  steps 
in  their approach can  be  performed much  more efficiently  by 
exploiting  the  game  structure.  Our  method  allows  us  to  pro(cid:173)
vide the first exact algorithm for general graphical games, and 
the  first  algorithm  that  takes  full  advantage  of  the  indepen(cid:173)
dence structure of a M A I D.  Our methods can  find  exact equi(cid:173)
libria  in  games  with  large  numbers  of  agents,  games  which 
were  previously  intractable  for exact  methods. 
Acknowledgments. 
This  work  was  supported  by  ONR 
M U RI  Grant  N00014-00-1-0637,  and  by  Air  Force  contract 
F30602-00-2-0598  under DARPA's  TASK  program. 
References 
[Govindan and Wilson, 2002]  S.  Govindan and R. Wilson.  Struc(cid:173)
ture theorems  for game  trees.  Proc.  Natl Academy of Sciences, 
99(13):9077-9080, 2002. 

[Govindan and Wilson, 2003a]  S. Govindan and R. Wilson.  Com(cid:173)
puting  Nash  equilibria  by  iterated  polymatrix  approximation. 
J.  Economic Dynamics and Control, 2003.  to appear. 

[Govindan and Wilson, 2003b]  S.  Govindan  and  R.  Wilson.  A 
global  Newton  method  to compute Nash equilibria.  /  Economic 
Theory, 2003.  to appear. 

[Howard and Matheson,  1984]  R.  A.  Howard  and J.  E.  Matheson. 
Influence diagrams.  In  Readings  on  the Principles and Applica(cid:173)
tions  of Decision  Analysis,  volume  2,  pages  719-762.  Strategic 
Decision Group, 1984.  article dated 1981. 

[Kearns et  al,  2001]  M.  Kearns,  M.  L.  Littman,  and  S.  Singh. 

Graphical models for game theory.  In Proc.  UAI, 2001. 

iKohlbcrg and Mertens, 1986]  E.  Kohlberg  and  J.-F. Mertens.  On 
the  strategic  stability  of equilibria.  Econometrica,  54(5): 1003-
1038, September 1986. 

I Koller and Megiddo,  1992]  D.  Koller and  N.  Megiddo.  The conv 
plexity of two-person zero-sum games in extensive form.  Games 
and Economic Bahavior, 4:528-552,  1992. 

[Kollcr and Milch, 2001J  D.  Kollcr and  B.  Milch.  Multi-agent  in(cid:173)
In  Proc. 

fluence diagrams  for representing  and  solving  games. 
IJCAI, pages  1027-1034, 2001. 

[La Mura, 2000]  P. La Mura.  Game networks.  In Proc.  UAI, pages 

335-342, 2000. 

lLauritzen and  Spiegelhalter,  1998]  S.  L.  Lauritzen  and  D.  J. 
Spiegelha  Iter.  Local  computations  with  probabilities  on  graph(cid:173)
ical  structures  and their application  to expert  systems.  J  Royal 
Statistical Society, B 50(2): 157-224,  1998. 

[Lemke and Howson,  1964]  C.  E.  Lemke  and  J.  T.  Howson,  Jr. 
Equilibrium  points  in  bimatrix  games.  J.  Society  of Applied 
Mathematics,  12(2):413-423, June  1964. 

[Littman et al, 2002]  M.  L. Littman, M.  Kearns,  and S.  Singh.  An 
efficient  exact  algorithm  for  singly  connected  graphical  games. 
In NIPS-14, volume 2, pages 817-823, 2002. 

[McKelvey and McLennan,  1996]  R. D. McKelvey and A. McLen(cid:173)
nan.  Computation of equilibria in finite games.  In Handbook of 
Computational Economics, vol.  1, pages 87-142. Elsevier,  1996. 
[Nash,  1951]  J.  Nash.  Non-cooperative  games.  The  Annals  of 

Mathematics, 52(2):286-295, September  1951. 

[Ortiz and Kearns, 20031  L.  E.  Ortiz and  M.  Kearns.  Nash propa(cid:173)
gation for loopy graphical games.  In NIPS-15, 2003.  to appear. 
[Owen,  1995]  G. Owen. Game Theory. Academic Press, UK, 1995. 
[Vickrey and Koller, 2002]  D.  Vickrey  and D.  Koller.  Multi-agent 

algorithms for solving graphical  games.  In Proc. AAAI, 2002. 

[von Stengel,  19961  B.  von  Stengel.  Efficient computation  of be(cid:173)

havior strategies.  Games and Economic Behavior,  14,  1996. 

[Watson, 2000]  L.  T.  Watson. 

Theory  of  globally  convergent 
probability-one  homotopies  for  nonlinear  programming.  SIAM 
J.  on Optimization,  ll(3):76I-780, 2000. 

764 

MULTIAGENT SYSTEMS 

