Understanding the Power of Clause Learning 

Paul Beanie* 

Henry Kautz* 

Ashish Sabharwal* 

Computer Science and Engineering 

University of Washington 
Seattle, WA 98195-2350 

{beame,kautz,ashish}@cs.Washington.edu 

Abstract 

Efficient  implementations  of DPLL  with  the  addi(cid:173)
tion of clause learning are the fastest complete sat(cid:173)
isfiability solvers and can handle many significant 
real-world problems, such as verification, planning, 
and design.  Despite its importance, little is known 
of the ultimate strengths and limitations of the tech(cid:173)
nique.  This paper presents the first precise charac(cid:173)
terization of clause learning as a proof system, and 
begins the task  of understanding  its power.  In par(cid:173)
ticular, we show that clause learning using any non-
redundant scheme and unlimited restarts is equiva(cid:173)
lent to general resolution.  We also show that with(cid:173)
out restarts but with a new learning scheme, clause 
learning  can  provide  exponentially  smaller proofs 
than regular resolution, which itself is known to be 
much stronger than ordinary DPLL. 

Introduction 

1 
In  recent  years  the  task  of deciding  whether a  CNF  propo-
sitional logic formula is satisfiable has gone from a problem 
of theoretical interest to a practical approach for solving real-
world problems.  SAT procedures are now a standard tool for 
hardware  verification,  including  verification  of super-scalar 
processors [Velev and Bryant, 2001; Biere et  al,  1999]. Open 
problems in group theory have been encoded and solved using 
satisfiability [Zhang and Hsiang,  1994].  Other applications of 
SAT include circuit diagnosis and experiment design  [Konuk 
and Larrabee,  1993; Gomes et  al,  1998b]. 

The most surprising aspect of such relatively recent practi(cid:173)
cal progress is that the best complete satisfiability testing al(cid:173)
gorithms remain variants of the DPLL procedure  [Davis and 
Putnam,  1960; Davis et al., 1962] for backtrack search in the 
space  of partial  truth  assignments.  The  main  improvements 
to  DPLL  have  been  better branch  selection  heuristics  (e.g., 
[Li and Anbulagan,  1997]), and extensions such as random(cid:173)
ized restarts  [Gomes et al,  1998a]  and clause learning.  One 
can argue that clause  learning has been the most significant 
of these in scaling DPLL to realistic problems. 

Clause  learning  grew  out  of work  in  AI  on  explanation-
based  learning  (EBL),  which  sought  to  improve  the  perfor-

* Research supported by NSF Grant ITR-0219468 

mance  of backtrack  search  algorithms  by  generating  expla(cid:173)
nations  for  failure  (backtrack)  points,  and  then  adding  the 
explanations as new constraints on the original problem  [de 
Kleer  and  Williams,  1987;  Stallman  and  Sussman,  1977; 
Genesereth,  1984; Davis,  1984].  For general constraint sat(cid:173)
isfaction  problems  the  explanations  are  called  "conflicts" 
or  "no  goods";  in  the  case  of  Boolean  CNF  satisfiability, 
the  technique  becomes  clause  learning.  A  series  of  re-
searchers  [Bayardo  Jr.  and  Schrag,  1997;  Marques-Silva 
and  Sakallah,  1996;  Zhang,  1997;  Moskewicz  et al,  2001; 
Zhang  et  al,  2001]  showed  that  clause  learning  can  be  ef(cid:173)
ficiently  implemented  and  used  to  solve  hard  problems  that 
cannot be approached by any other technique. 

Despite its importance there has been little work on formal 
properties  of clause  learning,  with the goal  of understanding 
its fundamental strengths and limitations.  A likely reason for 
such  inattention  is  that  clause  learning  is  a  rather  complex 
rule  of inference -  in  fact,  as  we  describe  below,  a complex 
family  of rules  of inference.  A  contribution  of this  paper  is 
that we provide a precise specification of clause learning. 

Another problem in characterizing clause learning is defin(cid:173)
ing  a  formal  notion  of the  strength  or  power  of a  reason(cid:173)
ing  method.  This  paper  uses  the  notion  of proof complex(cid:173)
ity  [Cook  and  Reckhow,  1977],  which  compares  inference 
systems in terms of the sizes of the shortest proofs they sanc(cid:173)
tion.  A  family  of formulas  C  provides  an  exponential sep(cid:173)
aration  between  systems S1 and  52  if the  shortest proofs  of 
formulas in C in system S1 are exponentially smaller than the 
corresponding shortest proofs in S2.  From this basic proposi-
tional proof complexity point of view,  only  families of unsat-
isfiable formulas are of interest, because only proofs of unsat-
isfiability  can  be  large;  minimum  proofs  of satisfiability are 
linear  in  the  number  of variables  of the  formula.  Neverthe(cid:173)
less, Achlioptas et al [2001] have shown how negative proof 
complexity  results  for unsatisfiable  formulas  can be  used to 
derive  time  lower  bounds  for  specific  inference  algorithms 
running on satisfiable formulas as well. 

Proof  complexity  does  not  capture  everything  we  intu(cid:173)
itively  mean by the power of a reasoning system,  because  it 
says nothing about how difficult  it is to  find  shortest proofs. 
However,  it  is  a good  notion with  which  to  begin  our anal(cid:173)
ysis,  because  the  size  of proofs  provides  a  lower-bound  on 
the running time of any implementation of the system.  In the 
systems we consider, a branching function, which determines 

1194 

SATISFIABILITY 

which  variable  to  split  upon  or  which  pair  of clauses  to  re(cid:173)
solve,  guides the search.  A negative proof complexity result 
for a  system  tells  us  that  a  family  of formulas  is  intractable 
even with a perfect branching function;  likewise,  a positive 
result gives us hope of finding a branching function. 

A basic result in proof complexity is that general resolution 
is exponentially stronger than the DPLL procedure  [Bonet et 
al., 2000; Ben-Sasson et al, 2000].  This is because the trace 
of DPLL  running  on  an  unsatisfiable  formula  can  be  con(cid:173)
verted  to  a  tree-like  resolution  proof of the  same  size,  and 
tree-like proofs must sometimes be exponentially larger than 
the  DAG-like  proofs  generated  by  general  resolution.  Al(cid:173)
though resolution can yield shorter proofs,  in practice DPLL 
is better because it provides a more efficient way to search for 
proofs.  The weakness of the tree-resolution proofs that DPLL 
finds is  that they  do  not reuse  derived  clauses.  The conflict 
clauses  found  when  DPLL  is  augmented  by clause  learning 
correspond to reuse of derived clauses  in  the associated res(cid:173)
olution  proofs  and thus  to  more  general  forms  of resolution 
proofs.  An  intuition  behind  the  results  in  this  paper  is  that 
the addition of clause learning moves DPLL closer to general 
resolution while retaining its practical efficiency. 

It has been previously observed that clause learning can be 
viewed  as  adding  resolvents  to  a  tree-like  proof [Marques-
Silva,  1998].  However,  this  paper provides  the  first  mathe(cid:173)
matical  proof  that  clause  learning  is  exponentially  stronger 
than tree-like resolution.  Further, we provide a family of for(cid:173)
mulas that exponentially separates clause learning from  reg(cid:173)
ular resolution, a system that is known to be intermediate in 
strength between general and tree resolution.  The proof uses 
a new clause learning scheme called FirstNewCut that we in(cid:173)
troduce.  We also  show that  combining clause  learning  with 
restarts is as strong as general resolution. 

Although this paper focuses on basic proof complexity, we 
briefly  indicate  how the  understanding  we  gain  through this 
kind of analysis may lead to practical applications. For exam(cid:173)
ple, our proofs describe an improvement to the clause learn(cid:173)
ing  rules previously  suggested  in the  literature,  and suggest 
an approach to leveraging the structure of a problem encoded 
as a CNF formula in order to create a branching heuristic that 
takes  the  greatest  advantage  of clause  learning.  As  an  ex(cid:173)
ample, we apply these ideas to certain natural satisfiable and 
unsatisfiable formulas where we obtain significant speed-ups 
over existing methods. 

2  Preliminaries 

of clauses,  where  each 
A  CNF  formula  F  is  an  AND 
of literals,  and  a  literal  is  a  variable  or 
clause is an OR 
It is natural to think of F as a set of clauses 
its negation 
and each clause as a set of literals.  A clause that is a subset 
of another is called its subclause. 

Let  p be  a  partial  assignment  to  the  variables  of F.  The 
restricted  formula 
is obtained from F by replacing vari(cid:173)
ables in  with their assigned values.  F is said to be simplified 
if all clauses with at least one TRUE literal are deleted, all oc(cid:173)
currences of FALSE literals are removed from clauses, and the 
resulting formula, if different, is simplified recursively. 

2.1  The DPLL Procedure 
The  basic  idea  of  the  Davis-Putnam-Logemann-Loveland 
(DPLL)  procedure  [Davis  and  Putnam,  1960;  Davis  et  al., 
1962]  for testing satisfiability of CNF  formulas  is  to branch 
on variables, setting them to TRUE or FALSE, until either an 
initial clause is violated (i.e. has all literals set to FALSE) or all 
variables have been set.  In the former case,  we backtrack to 
the last branching decision whose other branch has not been 
tried yet, reverse the decision, and proceed recursively.  In the 
latter,  we terminate with a satisfying assignment.  If all pos(cid:173)
sible  branches  have  been  unsuccessfully  tried,  the  formula 
is  declared  unsatisfiable.  To  increase  efficiency, pure  liter(cid:173)
als (those whose negation does not appear) and unit clauses 
(those with only one unset literal) are immediately set to true. 
Definition  1.  A branching sequence for a CNF formula F is 
of literals  of F,  possibly  with 
a sequence 
repetitions.  DPLL on  F branches according to a  if it always 
picks the next variable v to branch on in the literal order given 
by o, skips it if v is currently assigned a value, and branches 
further by setting the chosen literal to FALSE otherwise. 

In this paper, we will use the term DPLL to denote the ba(cid:173)
sic branching and backtracking procedure described above.  It 
will, for instance, not include extensions such as learning con(cid:173)
flict  clauses or restarting, but will allow intelligent branching 
heuristics. Note that this is in contrast with the occasional use 
of the term DPLL to encompass practically all branching and 
backtracking approaches, including those involving learning. 

2.2  Resolution 
Resolution is a simple proof system that can be used to prove 
unsatisfiability  of CNF  formulas.  The  resolution  rule  states 
a n d we  can derive clause 
that given clauses 
(A 
A  resolution  derivation  of  C 
from a CNF formula F is a sequence 
C)  where  each  clause  CI  is  either  a  clause  of F  (an  initial 
clause)  or derived by applying the resolution  rule to 
and 
is s, the number 

(a derived clause).  The size of 

B)  by  resolving  on 

of clauses occurring in it.  We will assume that each 
in  r  is  used  to  derive  at  least  one  other  clause 
Any  derivation  of the  empty  clause  A  from  F,  also  called  a 
refutation  or proof of F,  shows  that  F  is  unsatisfiable. 

Despite  its  simplicity,  resolution  is  not  efficiently  imple-
mentable  due  to  the  difficulty  of  finding  good  choices  of 
clauses to resolve;  natural  choices typically  yield huge  stor(cid:173)
age requirements.  Various restrictions on the structure of res(cid:173)
olution proofs  lead to  less powerful  but easier to  implement 
variants such as tree-like, regular, linear and positive resolu(cid:173)
tion.  Tree-like resolution uses non-empty derived clauses ex(cid:173)
actly once in the proof and is equivalent to an optimal DPLL 
procedure.  Regular resolution allows any  variable to  be re(cid:173)
solved upon at most once along any "path" in the proof from 
an initial clause to A. Both these variants are sound and com(cid:173)
plete but  differ in  efficiency - regular resolution  is known to 
be  exponentially  stronger than  tree-like  [Bonet et al.,  2000; 
Ben-Sasson et al., 2000], and general resolution is exponen(cid:173)
tially stronger than regular [Alekhnovich et al., 2002]. 
Definition  2.  A  resolution  derivation 
is 
trivial  iff all  variables  resolved  upon  are  distinct  and  each 

SATISFIABILITY 

1195 

is either an initial clause or is derived by resolving 
an initial clause. 

A  trivial  derivation  is  tree-like  as  well  as  regular.  More(cid:173)
over, the condition that each derived clause 
in its 
derivation  makes it linear.  As we will  see, trivial  derivations 
correspond to conflicts in clause learning algorithms. 

use 

3  Clause Learning 
Clause  learning proceeds by following the normal branching 
process of DPLL until there is a "conflict" after unit propaga(cid:173)
tion.  If this conflict occurs without any branches, the formula 
is  declared  unsatisfiable.  Otherwise,  the  "conflict  graph"  is 
analyzed and the "cause" of the conflict is learned in the form 
of a  "conflict  clause."  We  now  backtrack  and  continue  as 
in ordinary DPLL, treating the  learned clause just like initial 
ones.  A  clause  is  said  to  be  known  at  a  stage  if it  is  either 
an  initial  clause  or has  already  been  learned.  The  learning 
process is expected to save us from redoing the same compu(cid:173)
tation when we later have an assignment that causes conflict 
due in part to the same reason. 

If a  given  CNF  formula  F  is  unsatisfiable,  clause  learn(cid:173)
ing  terminates  with  a  conflict  without  any  branches.  Since 
all  clauses used  in  this conflict themselves  follow directly or 
indirectly  from  F,  this  failure of clause  learning  in  finding  a 
satisfying assignment constitutes  a logical  proof of unsatisfi-
ability  of F.  Our  bounds  compare  the  size  of such  a  proof 
with the size of a (possibly restricted)  resolution proof of un-
satisfiability  of F. 

Variations  of  such  conflict  driven  learning  [Bayardo  Jr. 
and Schrag, 1997; Marques-Silva and Sakallah, 1996; Zhang, 
1997;  Moskewicz  et  al.,  2001;  Zhang  et  al.,  2001]  include 
different  ways  of choosing  the  clause  to  learn  and  possibly 
allowing  multiple  clauses  to  be  learned  from  a  single  con(cid:173)
flict.  Although  many  such  algorithms  have  been  proposed 
and demonstrated to be  empirically successful,  a theoretical 
discussion  of the  underlying  concepts  and  structures  needed 
for our bounds is lacking.  The rest of this section focuses on 
this formal  framework. 
Definition  3.  A  clause  learning proof-K  of an  unsatisfiable 
CNF  formula  F  under  scheme  S  and  induced  by  branching 
sequence  a  is  the  result  of applying  DPLL  with  unit propa(cid:173)
gation on  F, branching according to a,  and using scheme S 
to  learn  conflict clauses  such  that at the  end  of this process, 
there is a conflict without any branches. The size of the proof, 
size(Tt)%  is  |er|. 

All  clause  learning  algorithms  discussed  in  this  paper are 
based  on  unit propagation,  which  is  the  process  of repeat(cid:173)
edly  applying  the  unit clause  rule  followed  by  formula  sim(cid:173)
plification until  no clause with exactly one unassigned literal 
remains.  In this context,  it is convenient to work with resid(cid:173)
ual  formulas at different stages of DPLL.  Let p be the partial 
assignment at some stage of DPLL on formula F.  The resid(cid:173)
ual  formula  at  this  stage  is  obtained  by  simplifying  F\p  and 
applying unit propagation. 

When  using  unit  propagation,  variables  assigned  values 
through the actual branching process are called decision vari(cid:173)
ables and those assigned values as a result of unit propagation 

are  called  implied  variables.  Decision  and  implied  literals 
are  analogously  defined.  Upon  backtracking,  the  last  deci(cid:173)
sion variable no longer remains a decision variable and might 
instead become an implied variable depending on the clauses 
learned  so  far.  The  decision  level  of a  decision  variable  x 
is one more than the number of current decision variables at 
the time of branching on x. The decision level of an implied 
variable  is  the  maximum  of the  decision  levels  of decision 
variables  used to  imply  it.  The  decision  level at any  step  of 
the underlying DPLL procedure is the maximum of the deci(cid:173)
sion levels of all current decision variables. 

Implication  G r a ph  and  Conflicts 

3.1 
Unit  propagation  can  be  naturally  associated  with  an  impli(cid:173)
cation  graph  that  captures  all  possible  ways  of deriving  all 
implied literals from decision literals. 
Definition  4.  The  implication  graph  G  at  a  given  stage  of 
DPLL is a directed acyclic graph with edges labeled with sets 
of clauses.  It is constructed as follows: 

1.  Create a node for each decision literal, labeled with that 
literal.  These will be the indegree zero root nodes of G. 

3.  Add  to  G  a  special  node  A.  For  any  variable  x  which 
occurs both positively and negatively in G\ add directed 
edges from  and 

to 

 

Since  all  node  labels  in  G  are  distinct,  we  identify  nodes 
with the literals labeling them.  Any variable x occurring both 
positively and negatively in G  is a conflict  variable, and x as 
arc  conflict  literals.  G  contains  a  conflict  if it  has 
well as 
at  least  one  conflict variable.  DPLL  at  a  given  stage  has  a 
conflict  if the  implication graph at that stage contains a con(cid:173)
flict.  A  conflict  can  equivalently  be  thought  of as  occurring 
when the residual formula contains the empty clause A. 

By definition, an implication graph may contain many con(cid:173)
flict  variables and several  ways of deriving any  single literal. 
To better understand and analyze a conflict,  we work  with a 
subgraph,  called  the  conflict graph  (see  Figure  1),  that  cap(cid:173)
tures only one among possibly many ways of reaching a con(cid:173)
flict  from  the  decision  variables.  The  choice  of the  conflict 
graph  is  part  of the  strategy  of the  solver. 
It  can  also  be 
thought of as giving power to clause learning by adding non-
determinism. 
Definition  5.  A  conflict graph  H  is  any  subgraph  of an  im(cid:173)
plication graph with the following properties: 

1.  H contains A and exactly one conflict variable. 
2.  All nodes in H have a path to A. 
3.  Every  node 

than  A  either  corre(cid:173)
sponds  to  a  decision  literal  or  has  precisely  the  nodes 

in  H  other 

I 

as predecessors where  

is a known clause. 

1196 

SATISFIABILITY 

Consider the implication graph at a stage where there is a 
conflict and fix a conflict graph contained in that implication 
graph.  Pick any cut in the conflict graph that has all decision 
variables on one side, called the reason side, and A as well as 
at least one conflict literal  on the other side, called the con(cid:173)
flict side.  All nodes on the reason side that have at least one 
edge  going  to the conflict side  form a cause of the  conflict. 
The negations of the corresponding literals forms the conflict 
clause associated with this cut. 
Proposition  1.  Every conflict clause corresponds to a cut in 
a conflict graph that separates decision variables from A and 
a  conflict  literal. 

Proof  Let  S  denote  the  set  containing  the  negations  of the 
literals of a given conflict clause C and pred(S)  be the set of 
all predecessors of these literals in the underlying implication 
graph.  Let T denote the set containing all literals obtained by 
unit propagation  after setting literals  in  S to TRUE.  Since C 
is a conflict clause, T must contain a conflict literal. Consider 
the subgraph GS,T of the implication graph induced by A and 
the  literals  in 
but having no edges going 
from preid(S)  to T.  Fix any conflict graph that is a subgraph 
of G S,T The cut  in this conflict graph with T as the conflict 
side has C as the conflict clause.  

If there  is  a  trivial  resolution  derivation  of a 
Proposition  2. 
clause C from a set of clauses F, then setting all literals ofC 
to FALSE leads to a conflict. 

Proposition  3.  Any  conflict  clause  can  be  derived  from 
known  clauses  using a  trivial resolution  derivation. 

Proof.  In  the  light  of Proposition  1,  assume  that  we  have 
a  conflict  clause  associated  with  a  cut 
in  a  fixed  conflict 
denote  the  set  of variables  on  the 
graph.  Let 
conflict  side  of o,  but  including  the  conflict  variable  only  if 
it occurs both positively  and  negatively  on the  conflict side. 

3.2  Different  Learning  Schemes 
Different cuts separating decision variables from A and a con(cid:173)
flict literal correspond to different learning schemes (see Fig(cid:173)
ure 1. One can also create learning schemes based on cuts not 
involving conflict literals at all  [Zhang et al, 2001], but their 
effectiveness is not clear.  These will not be considered here. 
It  is  insightful  to think of the non-deterministic scheme as 
the most general learning scheme.  Here we pick the cut non-
deterministically,  choosing,  whenever  possible,  one  whose 
associated  clause  is  not  already  known.  Since  we  can  re(cid:173)
peatedly branch on the same last variable, non-deterministic 
learning  subsumes  learning  multiple  clauses  from  a  single 
conflict as long as the sets of nodes on the reason side of the 
corresponding  cuts  form  a  (set-wise)  decreasing  sequence. 
For simplicity, we will assume that only one clause is learned 
from any conflict. 

In  practice,  however,  we  employ  deterministic  schemes. 
The decision scheme  [Zhang et  al,  2001], for example, uses 
the  cut  whose  reason  side  comprises  all  decision  variables, 
re 1 - s at [Bayardo Jr. and Schrag, 1997] uses the cut whose 
conflict side consists of all implied variables at the current de(cid:173)
cision level.  This  scheme allows the  conflict clause to  have 
exactly one variable from the current decision level, causing 
an automatic  flip  in its assignment upon backtracking. 

This nice  flipping  property  holds in general  for all  unique 
implication  points  (UIPs) 
[Marques-Silva  and  Sakallah, 
1996]. A U1P of an implication graph is a node at the current 
decision  level  d  such  that  any  path  from  the  decision  vari(cid:173)
able at level d to the conflict variable as well as its negation 

SATISFIABILITY 

1197 

must go through  it.  Intuitively,  it is  a single reason  at level 
d that causes  the  conflict.  Whereas  r e l - s at  uses  the  de(cid:173)
cision  variable  as  the  obvious  U1P,  GRASP  [Marques-Silva 
and  Sakallah,  1996]  and  zChaf f  [Moskewicz et  al,  2001] 
use FirstUIP, the one that is "closest" to the conflict variable. 
GRASP also learns multiple clauses when faced with a con(cid:173)
flict.  This  makes  it  typically  require  fewer  branching  steps 
but possibly  slower because  of the  time  lost  in  learning  and 
unit propagation. 

The  concept of UIP  can be  generalized to  decision  levels 
other than  the  current  one.  The  IUIP  scheme  corresponds 
to  learning the  FirstUIP clause of the current decision  level, 
the 2UIP scheme to learning the FirstUIP clauses of both the 
current level and the one before, and so on. Zhang et al [2001 ] 
present a comparison of all these and other learning schemes 
and  conclude  that  1UIP  is  quite  robust  and  outperforms  all 
other schemes they consider on most of the benchmarks. 

Figure 1: A conflict graph depicting various learning schemes 

The  FirstNewCut Scheme 
We  propose  a  new  learning  scheme  called  FirstNewCut 
whose  ease  of analysis  helps  us  demonstrate  the  power  of 
clause learning.  We would like to point out that we use this 
scheme here only to prove our theoretical bounds.  Its effec(cid:173)
tiveness on other formulas has not been studied yet. 

The  key  idea  behind  FirstNewCut  is  to  make  the  con(cid:173)
flict clause  as  relevant to  the  current conflict as possible  by 
choosing a cut close to the conflict literals.  This is what the 
FirstUIP  scheme  also  tries  to  achieve  in  a  slightly  different 
manner.  For the  following definitions,  fix  a cut in a conflict 
graph  and  let  S  be  the  set of nodes  on  the  reason  side  that 
have an edge to some node on the conflict side (5 is the rea(cid:173)
son  side frontier  of the  cut).  Let 
be  the  conflict  clause 
associated with this cut. 
Definition  6.  Minimization  of conflict clause  Cs  is  the  pro(cid:173)
cess of repeatedly identifying, if one exists, a node 
S, all 
of whose predecessors are also in S, moving it to the conflict 
side, and removing it from S. 
Definition  7.  FirstNewCut scheme:  Start with  a  cut whose 
conflict side consists of A  and a conflict literal.  If necessary, 
repeat  the  following  until  the  associated  conflict clause,  af(cid:173)
ter minimization,  is not already known:  pick a node on the 
conflict side,  pull  all  its predecessors  except  those that cor(cid:173)
respond to decision  variables  into the conflict side.  Finally, 
learn the resulting new minimized conflict clause. 

This  scheme  starts with  the cut that is  closest to the  con(cid:173)
flict  literals and iteratively moves it back toward the decision 
variables until a new associated conflict clause is found.  This 
backward search always halts because the cut with  all deci(cid:173)
sion variables on the reason side is certainly a new cut.  Note 
that there are potentially several ways of choosing a literal to 
move the cut back,  leading to different conflict clauses.  The 
FirstNewCut  scheme,  by  definition,  always  learns  a  clause 
not already known.  This motivates the following: 
Definition  8.  A  clause  learning  scheme  is  non-redundant if 
on a conflict, it always learns a clause not already known. 

Fast Backtracking  and  Restarts 

3.3 
Most clause learning algorithms use fast backtracking where 
one  uses  the  conflict  graph  to  backtrack  not  only  the  last 
branching decision but also all other recent decisions that did 
not contribute to the conflict [Stallman and Sussman,  1977]. 
This adds power to clause  learning because the current con(cid:173)
flict might use clauses learned earlier as a result of branching 
on the apparently redundant variables.  Hence, fast backtrack(cid:173)
ing in general cannot be replaced by a "good'* branching se(cid:173)
quence  that  does  not  produce  redundant  branches.  For  the 
same reason,  fast backtracking cannot either be replaced by 
simply learning the decision scheme clause. However, the re(cid:173)
sults  we  present  in this paper  are  independent  of whether or 
not fast backtracking is used. 

Restarts  allow  clause  learning  algorithms  to  arbitrarily 
restart  their  branching  process.  All  clauses  learned  so  far 
are  however  retained  and  now  treated  as  additional  initial 
clauses  [Baptista  and  Silva,  2000].  As  we  will  show,  un(cid:173)
limited  restarts  can  make  clause  learning  very  powerful  at 
the cost of adding non-determinism.  Unless otherwise stated, 
clause learning proofs will be assumed to allow no restarts. 

4  Clause Learning and General Resolution 
Lemma  1.  Let  F  be  a  CNF formula  over  n  variables. 
If 
F  has  a  general  resolution proof of size  s,  then  it  also  has 
a  clause  learning proof of size  at  most  ns  using  any  non-
redundant learning scheme and at most s restarts. 

and 

If  contains a derived clause 

Proof 
A) be a general resolu(cid:173)
tion  proof of F  where  each 
is  either an  initial  clause  or 
derived by resolving two clauses 
occur(cid:173)
ring earlier in 
whose strict 
subclause C[ can be derived by resolving two previously oc(cid:173)
do  trivial 
curring clauses,  then  we  can replace 
with 
simplifications on further derivations that used 
and obtain 
of F  of size  at  most  s.  Doing  this  repeat(cid:173)
another  proof 
edly will remove all such redundant clauses and leave us with 
a simplified proof no  larger in  size.  Hence  we  will  assume 
without loss of generality  that  has no such clause. 

A clause learning proof of F can be constructed by choos(cid:173)
in order, learning each of them, and 
ing derived clauses  of 
restarting.  Suppose every clause 
is already known 
and we are at decision level zero.  This is trivially true when 
there  are  two 

are  initial  clauses. 

known clauses  and 
In this case we have a conflict from the known clauses at de(cid:173)
cision  level  zero  and  our clause  learning proof is  complete. 

whose resolution generates 

If 

1198 

SATISFIABILITY 

and 

Otherwise, let 
and assume without 
loss of generality that  CP  is derived by resolving two known 
where  both  A  and  B  are 
clauses 
subclauses  of Cp.  Our  clause  learning  proof will  choose  to 
to FALSE.  This will fal(cid:173)
branch on and set all  of 
sify both  A  and  B  and thus  imply  both  x and 
after  unit 
propagation,  resulting in a conflict.  It is easy to see that the 
only new clause that can be learned from this conflict is 
Accordingly, we will use any non-redundant learning scheme, 
learn Cv  and restart to get back to decision  level  zero.  Due 
to  the  non-redundancy  of  CP  assumed  earlier  in  the  proof, 
there  is  no  premature  conflict  until  all  of 
have 
been branched upon.  This makes sure that our clause learn(cid:173)
ing proof proceeds as described above and we learn precisely 
the clause  

We learn at most .s clauses and each learning stage requires 
branching on at most n variables and exactly one restart. This 
gives the desired bounds on the size of the constructed clause 
learning proof and the number of restarts it uses.  

Lemma  2.  Let  F  be  a  CNF formula  over  n  variables. 
If 
F  has  a  clause  learning proof of size  s  using  any  learning 
scheme and any number of restarts, then F also has a general 
resolution proof of size at most ns. 

learns,  which  includes  the  empty  clause 

Proof  Given  a  clause  learning  proof 
of  F,  a  resolution 
proof can be constructed by sequentially deriving all clauses 
that 
From 
Proposition  3,  all  these  derivations  are trivial  and hence re(cid:173)
quire at most n steps each.  Consequently, the size of the re(cid:173)
sulting clause learning proof is at most ns.  Note that since we 
derive  clauses  of 
do not change 
the construction.  

individually,  restarts  in 

Combining Lemmas 1 and 2, we immediately get 

Theorem 1.  Clause learning with any non-redundant scheme 
and unlimited restarts  is  equivalent to general resolution. 

Note that this theorem strengthens the result from [Baptista 
and Silva, 2000] that clause learning together with restarts is 
complete.  Our theorem makes the stronger claim that clause 
learning with restarts can find proofs that are as short as those 
of general resolution. 

5  Clause Learning and Regular Resolution 
Here  we  prove  that  clause  learning  even  without  restarts  is 
exponentially  stronger  than  regular  resolution  on  some  for(cid:173)
mulas. We do this by first introducing a way of extending any 
CNF formula based on a given resolution proof of it. We then 
show  that  if a  formula  exponentially  separates  general  res(cid:173)
olution  from  regular  resolution,  its  extension  exponentially 
separates clause learning from regular resolution. Finally, we 
cite specific formulas called 
5.1  The Proof Trace Extension 

that satisfy this property. 

clauses of F together with a trace clause 
clause C  S and each literal 

C. 

for each 

We  first  show  that  if a  formula  has  a  short  general  reso(cid:173)
lution  refutation,  then  the  corresponding  proof trace  exten(cid:173)
sion  has a  short  clause  learning proof.  Intuitively,  the new 
trace variables allow us to handle every resolution step of the 
original proof individually,  effectively  letting us restart after 
learning each derived clause. 
Lemma 3. Let F be any CNF formula and  be a resolution 
)  has  a  clause  learning proof 
refutation  of it.  Then  PT(F, 
of size less than 
using the FirstNewCut scheme and 
no restarts. 

Lemma  4.  Let  F  be  a  CNF  formula  over  n  variables  that 
has  a polynomial  (in  n)  size  general  resolution proof n  but 
requires  exponential  size  regular  resolution  proofs.  Then 
PT(F, 
)  has  a polynomial size  clause  learning proof using 
the FirstNewCut scheme and no restarts,  but requires expo-
nential size  regular  resolution proofs. 

Proof  Lemma  3  immediately  implies  that 
polynomial size FirstNewCut clause learning proof. 

has a 

has a regular resolution refutation 

For  the  other  part,  we  use  a  simple  reduction  argument. 
of 
Suppose 
size  s.  Consider the  restriction 
that sets every trace vari(cid:173)
keeps  original  clauses  of F 
able  of this  formula  to TRUE, 
intact and trivially  satisfies  all  trace  clauses,  thereby  reduc(cid:173)
to  precisely  F.  Recall 
ing  the  initial  clauses  of 
that  regularity  of resolution  proofs  is  preserved  under  arbi(cid:173)
trary restrictions.  Consequently,  applying 
gives us a 
regular  resolution  refutation  of F  of size  at  most  5.  By  the 

to 

SATISFIABILITY 

1199 

assumption in the Lemma, s must be exponential in n.  Note 
itself is  of size  polynomial  in  n  because  of 
that 
Hence  s  is  also  exponential  in  the  size  of 
our choice  of 

itself.  

The 

5.2  The GTr; Formulas 
We use the proof trace extension of an explicit family of un-
satisfiable  CNF  formulas called 
to obtain an exponen(cid:173)
tial separation between regular resolution and clause learning. 
Note that in place  of 
we could also have used any other 
formulas satisfying the conditions on F in Lemma 4, such as 
the modified pebbling formulas of Alekhnovich et al [2002]. 
formulas  are  based  on  the  ordering  principle 
must 

that any partial  linear ordering on the set 
have a maximal element.  The original formulas, called 
were  first  considered  by  Krishnamurthy  L1985]  and  later 
used  by  Bonet  and  Galesi  [1999]  to  show  the  optimality  of 
the  size-width  relationship  of resolution  proofs.  Recently, 
Alekhnovich  et al  [2002]  used 
to show an exponential 
separation between general and regular resolution.  We refer 
the reader to this paper for exact specification of the 
for(cid:173)
mulas.  For our bound, we only need the following result: 
Lemma 5 (I Alekhnovich et al., 20021). 
has a polyno(cid:173)
mial size general  resolution  refutation  but  requires  exponen-
tial size  regular  resolution proofs. 

Let 

be  the  polynomial  size  resolution  refutation  of 
GT'n  described in [Alekhnovich et al., 2002].  It follows from 
Lemmas 4 and 5 that  PT 
has a polynomial size 
clause  learning proof using the  FirstNewCut  scheme  but re(cid:173)
quires exponential size regular resolution proofs.  Hence, 
Theorem  2.  There  exist  CNF formulas  on  which  clause 
learning using the  FirstNewCut scheme and no  restarts pro(cid:173)
vides  exponentially smaller proofs  than  regular resolution. 

6  Experimental Results 
Table  1  reports  the  performance  of  variants  of  z  C h a ff  on 
grid pebbling formulas. We conducted experiments on a 1600 
MHz Linux machine with memory limit set to 512MB.  Base 
code  of 
was extended to allow a (partial) branching 
sequence  to  be  specified  as  part  of the  input.  We  used  the 
more  popular  1UIP  learning  scheme  of 
instead  of 
FirstNewCut because  for these formulas,  both schemes pro(cid:173)
vide small proofs. 

Pebbling formulas based on pebbling graphs are known to 
be  hard  for  tree-like  resolution  but  easy  for  regular  resolu(cid:173)
tion [Ben-Sasson and Wigderson,  1999].  We refer the reader 
to this paper for exact specification  of the  formulas.  For our 
experiments,  we  worked  with  a  uniform,  grid  like  version, 
where  every  non-leaf node  has  precisely  two  predecessors. 
For  a  k  layer  graph,  the  corresponding  formula  has  0(k2) 
variables and clauses.  These formulas are minimally unsatis-
fiable. We also  used a satisfiable version  obtained by delet(cid:173)
ing a randomly chosen clause.  The branching sequence was 
generated based on depth first traversal of the underlying peb(cid:173)
bling graph. Results are reported for z C h a ff with no learn(cid:173)
ing or specified branching sequence  (DPLL),  with specified 
branching sequence only, with clause learning only (original 
z C h a f f ), and both. 

7  Discussion and Open Problems 
This paper has begun the  task  of formally understanding the 
power  of clause  learning  from  a  proof complexity  perspec(cid:173)
tive. We have seen that clause learning can be more powerful 
than  even  regular resolution,  and  that  learning  with  restarts 
yields general resolution. 

In practice,  a solver must employ good  branching heuris(cid:173)
tics as well as implement a powerful proof system. Our result 
that  modified  pebbling  formulas  have  small  clause  learning 
proofs depends critically upon the solver choosing a branch(cid:173)
ing sequence that solves the formula in "bottom up" fashion, 
so that the  learned clauses have maximal reuse.  As we de(cid:173)
scribe  in  a  subsequent  paper  [Sabharwal  et  al.,  2003],  this 
branching sequence can be efficiently generated by a combi(cid:173)
nation of breadth-first and depth-first travcrsals of the original 
pebbling graph even for more general classes of pebbling for(cid:173)
mulas.  As shown in Table  1, one needs both clause learning 
as well as a good branching sequence to efficiently solve large 
problem instances. 

Of course, pebbling graphs, which correspond to problems 
involving  precedence  of  tasks,  represent  a  narrow  domain 
of applicability.  However,  logic  encodings  of many  kinds 
of real-world problems,  such as planning graphs  [Kautz and 
Selman,  1996], exhibit layered structure not unlike pebbling 
graphs.  An  important direction  of our current  research  is to 
generate  branching  sequences  that  allow  clause  learning  to 
work  well  on  the  general  classes  of structures  that  arise  in 
encodings  of particular problem  domains. 

Different learning schemes are likely to vary in their effec(cid:173)
tiveness when used on formulas from different domains.  We 
introduced FirstNewCut as a new scheme and used it in this 
paper to derive our theoretical results.  How well  it performs 
on  encodings  of real-world problems  is  still  open.  It would 
be interesting to know if there is a class of practical problems 

1200 

SATISFIABILITY 

on which FirstNewCut works better than other schemes. 

This  paper  inspires  but  leaves  open  several  interesting 
questions  of proof  complexity.  We  have  shown  that  with 
arbitrary  restarts,  clause  learning  is  as  powerful  as  general 
resolution.  However, judging  when  to  restart  and  deciding 
what  branching  sequence  to  use  after  restarting  adds  more 
non-determinism  to  the  process,  making  it  harder  for  prac(cid:173)
tical  implementations.  Can  clause  learning  with  no  or  lim(cid:173)
ited restarts also simulate general resolution efficiently?  We 
showed  that  there  are  formulas  on  which  clause  learning  is 
much more efficient than regular resolution.  In general,  can 
every small regular refutation be converted into a small clause 
learning proof?  Or are regular resolution and clause learning 
incomparable? 

References 
[Achlioptas et al., 2001]  D.  Achlioptas,  P.  Beame,  and 
M.  Molloy.  A sharp threshold in proof complexity.  In 33rd 
STOC, 337-346, 2001. 
[Alekhnovich et al, 2002]  M.  Alekhnovich,  J.  Johannsen, 
T.  Pitassi, and A.  Urquhart.  An exponential  separation be(cid:173)
tween regular and general resolution.  In 34th STOC\ 448 
456, 2002. 
[Baptista and Silva, 2000]  L. Baptista and J. P. M. Silva.  Us(cid:173)
ing randomization and learning to solve hard real-world in(cid:173)
stances of satisfiability.  In  Prin.  and Prac.  of Const.  Prog., 
489-494, 2000. 
[Bayardo Jr. and Schrag,  1997]  R.  J.  Bayardo  Jr.  and  R.  C. 
Schrag.  Using  CST  look-back  techniques  to  solve  real-
world SAT instances.  In  Nth AAA1, 203-208,  1997. 
[Ben-Sasson and Wigderson,  1999]  E.  Ben-Sasson 
and 
A.  Wigderson.  Short  proofs are  narrow  -  resolution made 
simple.  In 31st STOQ 517-526,  1999. 
LBen-Sasson et al.,2000] E.  Ben-Sasson,  R.  Impagliazzo, 
and A.  Wigderson.  Near-optimal separation of treelike and 
general resolution.  Tech. Rep. TR00-005, Elec. Colloq. in 
Comput. Compl., http://www.eccc.uni-trier.de/eccc/, 2000. 
[Bieve etal,  1999]  A.  Biere,  A.  Cimatti,  E.  M.  Clarke, 
M. Fujita, and Y. Zhu. Symbolic model checking using SAT 
procedures instead of BDDs.  In 36th DAC, 317-320,  1999. 
[Bonet and Galesi,  1999]  M.  L.  Bonet  and  N.  Galesi.  A 
study  of proof search  algorithms  for  resolution  and  poly(cid:173)
nomial calculus.  In 40th FOCS, 422-432,  1999. 
[Bonet et al, 2000]  M.  L.  Bonet,  J.  L.  Esteban,  N.  Galesi, 
and J. Johansen, On the relative complexity of resolution re(cid:173)
finements and cutting planes proof systems, SIAM J. Corn-
put., 30 (2000), 1462-1484. 
[Cook and Reckhow,  1977]  S.  A.  Cook  and  R.  A.  Reck-
how,  The relative efficiency of propositional proof systems, 
J. Symb. Logic, 44 (1977), 36-50. 
[Davis and Putnam,  1960]  M. Davis and H. Putnam, A com(cid:173)
puting  procedure  for  quantification  theory,  CACM,  7 
(1960), 201-215. 
[Davis et al,  1962]  M.  Davis,  G.  Logemann,  and  D.  Love-
land,  A machine  program  for theorem proving,  CACM,  5 
(1962), 394-397. 

[Davis,  1984]  R. Davis, Diagnostic reasoning based on struc(cid:173)
ture and behavior, J. AI, 24 (1984), 347-410. 
[de Klecr and Williams,  1987]  J.  de  Klecr  and  B.  C. 
Williams,  Diagnosing  multiple  faults,  J.  AI,  32  (1987), 
97-130. 
[Genesereth,  1984]  R.  Genesereth,  The  use  of  design  de(cid:173)
scriptions  in automated diagnosis,  J.  AI,  24 (1984),  411-
436. 
[Gomes etal,  1998a]  C.  P.  Gomes,  B.  Selman,  and 
H.  Kautz.  Boosting combinatorial  search through random(cid:173)
ization.  In 15th  AAAI  431â€”437,  1998. 
[Gomes et al,  1998b]  C. P. Gomes, B. Selman, K. McAloon, 
and  C.  Tretkoff.  Randomization  in  backtrack  search:  Ex(cid:173)
ploiting  heavy-tailed  profiles  for  solving  hard  scheduling 
problems.  In 4th Int.  Conf. Art. Intel.  Planning Syst.,  1998. 
[Kautz and Selman,  1996]  11.  A.  Kautz  and  B.  Selman. 
Pushing  the  envelope:  Planning,  propositional  logic,  and 
stochastic search.  In 13th AAAI,  1194-1201,  1996. 
[Konuk and Larrabee,  1993]  H. Konuk and T. Larrabee.  Ex(cid:173)
plorations  of sequential  ATPG  using  boolean  satisfiability. 
In 11th VLSI Test Symposium, 85-90, 1993. 
[Krishnamurthy,  1985]  B.  Krishnamurthy,  Short  proofs  for 
tricky formulas, Acta Inf., 22 (1985), 253-274. 
[Li and Anbulagan,  1997]  C.  M.  Li and Anbulagan.  Heuris(cid:173)
tics based on unit propagation for satisfiability problems.  In 
UCAI(l),  366-371,  1997. 
[Marques-Silva and Sakallah, 1996]  J. P. Marques-Silva and 
K.  A.  Sakallah.  Grasp - a new search algorithm  for satisfi(cid:173)
ability.  In ICCAD, 220-227,  1996. 
LMarques-Silva,  1998]  J.  Marques-Silva.  An  overview  of 
backtrack search  satisfiability algorithms.  In 5th Symp. on 
Al and Math.,  1998. 
[Moskewicz et al, 2001]  M.  W.  Moskewicz,  C.  F.  Madigan, 
Y.  Zhao,  L.  Zhang,  and  S.  Malik.  Chaff:  Engineering an 
efficient SAT solver.  In 38th DAC, 530-535, 2001. 
[Sabharwal et al, 2003]  A.  Sabharwal,  P.  Beame,  and 
H. Kautz.  Using problem structure for efficient clause learn(cid:173)
ing.  To appear in SAT, 2003. 
[Stallman and Sussman, 1977]  R.  Stallman  and  G.  J.  Suss-
man,  Forward  reasoning  and  dependency-directed  back(cid:173)
tracking  in  a  system  for  computer-aided  circuit  analysis, 
J. AI, 9 (1977), 135-196. 
[Velev and Bryant, 2001]  M. Velev and R.  Bryant.  Effective 
use  of boolean  satisfiability  procedures  in  the  formal  veri(cid:173)
In  38th 
fication  of superscalar  and  vliw  microprocessors. 
DAC, 226-231,  2001. 
[Zhang and Hsiang, 1994]  H. Zhang and J. Hsiang.  Solving 
open  quasigroup  problems  by  propositional  reasoning. 
In 
Proceedings of the International Computer Symp., 1994. 
[Zhang etal, 2001]  L.  Zhang,  C.  F.  Madigan,  M.  H. 
Moskewicz,  and  S.  Malik.  Efficient  conflict  driven  learn(cid:173)
ing  in  a boolean  satisfiability  solver.  In ICCAD,  279-285, 
2001. 
[Zhang,  1997]  H.  Zhang.  SATO:  An  efficient propositional 
prover.  In Int.  Conf on Auto.  Deduc,  LNAI, vol.  1249,272-
275, 1997. 

SATISFIABILITY 

1201 

