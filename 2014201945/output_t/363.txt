                                    recursive random fields                                      daniel lowd    pedro domingos                             department science engineering                         university washington seattle wa  usa                                    lowdpedrodcswashingtonedu                          abstract                            example mln formula rx ‚àß sx                                                        treat worlds violate rx sx proba      formula Ô¨Årstorder logic viewed  ble worlds violate mln acts      tree logical connective each node soft conjunction groundings rx sx simply      knowledge base viewed tree    appear distinct formulas mlns convert knowledge                                            root conjunction markov logic richardson    base cnf performing learning inference                             domingos  makes conjunction prob   possible disjunction rx ‚à® sx distinction      abilistic universal quantiÔ¨Åers directly satisfying rx sx satisfying      rest tree remains purely log just universally quantiÔ¨Åed formula effectively      ical causes asymmetry treatment   conjunction groundings existentially      conjunctions disjunctions universal quantiÔ¨Åed formula disjunction leads      existential quantiÔ¨Åers propose   quantiÔ¨Åers handled differently      come allowing features markov logic                                                          asymmetry avoided ‚Äúsoftening‚Äù disjunction      networks mlns nested mlns                                                        existential quantiÔ¨Åcation way markov      representation recursive random Ô¨Åelds rrfs                                                        logic softens conjunction universal quantiÔ¨Åcation      rrfs represent Ô¨Årstorder distributions                                                        result representation mlns nested mlns      exponentially compactly mlns                                                        features recursive markov logic networks      form inference rrfs using mcmc icm                                                        recursive random Ô¨Åelds rrfs short      weight learning using form backpropa      gation weight learning rrfs power      rrfs desirable properties including abil                                                        ity represent distributions like noisy dnf rules ex      ful structure learning mlns applied                    Ô¨Årstorder knowledge bases provides Ô¨Çex ceptions ofall quantiÔ¨Åers compactly      ible form theory revision evaluate rrfs mlns rrfs allow Ô¨Çexibilty revising Ô¨Årst      problem probabilistic integrity constraints order theories maximize data likelihood standard methods      databases obtain promising results          inference markov random Ô¨Åelds easily extended                                                        rrfs weight learning carried efÔ¨Åciently using                                                        variant backpropagation algorithm    introduction                                         rrf theory revision viewed Ô¨Årstorder prob  recent years seen development increasingly pow abilistic analog kbann algorithm initializes  erful combinations relational probabilistic representa neural network propositional theory uses  tions inference learning algorithms propagation improve Ô¨Åt data towell shavlik  general representations date markov  propositional rrf predicates zero  logic attaches weights Ô¨Årstorder formulas arity differs multilayer perceptron output  views templates features markov random Ô¨Åelds joint probability inputs regression  richardson domingos  markov logic variable probabilistic version condi  language choice applications uniÔ¨Åca tional probability propositional rrfs alternative  tion logic probability incomplete boltzmann machines nested features playing role  treats toplevel conjunction universal quanti hidden variables nested features determinis  Ô¨Åers knowledge base probabilistic principle tic functions inputs learning does require em  logical combination viewed limiting case inference does require marginalizing variables  underlying probability distribution markov logic remainder paper organized follows  disjunctions existential quantiÔ¨Åers remain deterministic begin discussing mlns limitations  symmetry conjunctions disjunctions troduce rrfs inference learning algorithms  universal existential quantiÔ¨Åers lost ex compare mlns present preliminary  cept inÔ¨Åniteweight limit                    experimental results                                                    ijcai                                                       markov logic networks                              different standard markov random Ô¨Åeld fea  markov logic network mln consists set Ô¨Årstorder tures built subfeatures arbitrary                                                        number levels speciÔ¨Åcally each feature  formulas weights wifi serve template                                                                constructing markov random Ô¨Åeld each feature    base‚éõ case ‚éû  markov random Ô¨Åeld grounding formulas                                                                                          ‚éù            ‚é†  joint probability distribution         fix     exp      wijfjx    recursive case                                                                  zi                                                                                                                                                                                          exp                       recursive case summation features                                                      referenced ‚Äúparent‚Äù feature fi child feature fj                                                        appear parent feature rrf  ni number true groundings ith formula                                                        viewed directed acyclic graph features  given assignment normalization constant                                                        attribute values leaves probability  probabilities worlds sum richardson                                                        conÔ¨Åguration given root note probabilistic  domingos  show Ô¨Ånite domains gen                                                        graphical model represented rrf undirected  eralizes Ô¨Årstorder logic markov random Ô¨Åelds                                                          overall distribution simply recursive feature    way think markov logic network                                                        write probability distribution follows  ‚Äúsoftening‚Äù deterministic knowledge base Ô¨Årstorder                                                                                 knowledge base represented conjunction                          groundings formulas mlns relax conjunc normalization root feature  tion allowing worlds violate formulas assigning perfeature normalization constants zi absorbed  pergrounding penalty each violated formula worlds corresponding feature weights wki parent fea  violate formulas possible likely tures fk user free choose convenient  violate fewer way inconsistent knowl normalization normalization  edge bases useful                         easy show generalizes markov random    mlns soften conjunctions universals Ô¨Åelds conjunctive disjunctive features each fi ap  disjunctions existentials remain deterministic just proximates conjunction weights wij large  mlns probability world decreases gradually limit fi  iff conjunct true fi  number false groundings universally quantiÔ¨Åed represent disjuncts using large negative weights  mula probability increase gradually negative weight parent feature fk wki nega  number true groundings existentially quantiÔ¨Åed tive weight wki turns conjunction disjunction just  mula rrfs accomplish                           negation does morgan‚Äôs laws                                                        conjunction disjunction represent    recursive random fields                            mofn concepts complex distributions                                                        different features different weights  recursive random Ô¨Åeld rrf loglinear model note features small absolute weights little  each feature observable random variable effect instead using heuristics search  output recursive random Ô¨Åeld build intu determine attributes appear feature  ition Ô¨Årst propositional case generalize include predicates let weight learning sort  interesting relational case concrete example attributes relevant feature sim  given section  illustrated figure  ilar learning neural network initializing small                                                        random values network represent logical    propositional rrfs                               formula need commit speciÔ¨Åc structure  primary goal solving relational problems rrfs ahead time attractive alternative tradi  interesting propositional domains propo tional inductive methods used learning mrf features  sitional rrfs extend markov random Ô¨Åelds boltzmann  rrf seen type multilayer neural net  machines way multilayer perceptrons extend work node function exponential  singlelayer ones extension simple principle sigmoidal network trained maximize joint like  allows rrfs compactly represent important concepts lihood unlike multilayer perceptrons ran  mofn allows rrfs learn features dom variables inputs outputs rrfs  weight learning effective current variables inputs output joint probability  featuresearch methods markov random Ô¨Åelds       ways rrf resembles boltzmann machine    probability distribution represented propositional greater Ô¨Çexibility multiple layers learnable  rrf follows                                    ing variant backpropagation algorithm rrfs                                                                                               hidden variables sum nodes network                                        exp      wifix              deterministic values making inference efÔ¨Åcient                                                                              relational rrfs  normalization constant ensure prob relational case relations arbitrary number  abilities possible states sum  makes objects place Ô¨Åxed number variables allow                                                    ijcai                                                     parameter tying different groundings use parame interesting belief richardson domingos  terized features parfeatures represent parameter  people tend smoke friends omit  tuple vector g size depends arity ted simplicity demonstrate represent  parfeature note g vector logical variables beliefs Ô¨Årst converting Ô¨Årstorder logic  arguments predicates opposed random boolean converting rrf  variables ground atoms represent state world represent Ô¨Årst belief ‚Äúsmoking causes  use subscripts distinguish parfeatures dif cer‚Äù Ô¨Årstorder logic universally quantiÔ¨Åed implica                                                     ‚àÄ      ‚áí  ferent parameterizations ig  ig   represent tion smg cag implication rewritten  different groundings ith parfeature           disjunction ¬¨smg ‚à® cag morgan‚Äôs laws    each rrf parfeature deÔ¨Åned ways   equivalent ¬¨smg ‚àß¬¨cag rep                                                        resented rrf feature                ig rigi    gik  base case                  ‚éõ                  ‚éû                                                                                                            exp  smg  cag                 ‚éù                  ‚é†                                      fix    exp      wij      recursive case                           jgg                                                                                             positive  negative feature                           g                                                        weight negative shown general  base case straightforward simply represents rrf features model conjunction disjunction  truth value ground relation speciÔ¨Åed cnf knowledge base represented rrf sim  grounding each possible combination param ilar approach works second belief ‚Äúfriends people  eters arguments parfeature recursive case sums friends‚Äù  weighted values child parfeatures each parameter Ô¨Årst beliefs handled markov logic  gi child parfeature parameter parent fea networks key advantage recursive random Ô¨Åelds  ture gi ‚àà g parameter child feature summed representing complex formulas belief ‚Äúev                                                      eryone friend smokes‚Äù naturally rep  does appear parent feature gi ‚àà                                                        resented nested quantiÔ¨Åers ‚àÄg‚àÉhfrg ‚àß smh  g parameters analogous parameters appear                                                        best represented rrf feature references sec  body head horn clause just sums                                                      ondary feature  child features act conjunctions summations                                                                                                                     parameters act universal quantiÔ¨Åers markov logic se               mantics fact generalized quantiÔ¨Åers represent   fgx    exp     wfghx                                                                           mofall concepts just simple feature sums repre                     sent mofn concepts                                                                                       relational version recursive random Ô¨Åeld gh exp frg  smh  fore deÔ¨Åned follows                                                                                                        note rrfs feature represent distribution                    xfx                      number smoking friends each person depend                                                        ing assigned weights it‚Äôs possible  set ground relations ra sa                                                        smoking friend people  assignment truth values ground relations                                                        three rrf actually learn  root recursive parfeature root                                                        distribution data  parameters recursive parfeature normal                                                          belief problematic mln  ized constant ensure valid probability distri                                                        mln purely logical there‚Äôs change prob  bution propositional case zi‚Äôs                                                        ability number smoking friends number  absorbed weights parent features                                                        exceeds secondly mlns represent belief ef  normalized convenient way                                                        Ô¨Åciently mln existential quantiÔ¨Åer converted    relational rrf converted propositional                                                        large disjunction  rrf grounding parfeatures expanding summa  tions each distinct grounding parfeature dis frg ‚àß sma ‚à® frg ‚àß smb ‚à®¬∑¬∑¬∑  tinct feature shared weights                                                         objects database disjunction    rrf example                                       conjunctions mln convert                                                                                      clarify ideas let example knowledge base dnf cnf form leading  cnf clauses each  richardson domingos  domain consists grounding rule  three predicates smokesg smoker cancerg features deÔ¨Åne   joint distribution follows                                                                        cancer friendsg friend  exp      wf                                                                                   abbreviate predicates smg cag frg hre                                                                                                        ghi  gh   spectively                                                                                                                                                        wish represent three beliefs smoking causes                    cer ii friends friends friends transitivity friend figure  diagrams Ô¨Årstorder knowledge base contain  ship iii friend smokes ing beliefs corresponding rrf                                                    ijcai                                                                                                                                                                                                                                                                                                                                                           ghi                                                                                                                                                    fg                                                                                                                                               ghi ghi                                                                                                                                                                                                                                                   gh                                                                                                                                                   smg   cag  frgh frhi frgi       ¬¨smg cag ¬¨frgh ¬¨frhi frgi                                         frgh smh                                      frgh smh    figure  comparison Ô¨Årstorder logic rrf structures rrf structure closely mirrors Ô¨Årstorder logic  connectives quantiÔ¨Åers replaced weighted sums      inference                                          Ô¨Årst term evaluated chain rule  rrfs generalize mlns turn generalize Ô¨Ånite          ‚àÇ logf  ‚àÇ logf ‚àÇfi                                                                                Ô¨Årstorder logic markov random Ô¨Åelds exact inference            ‚àÇwij        ‚àÇfi   ‚àÇwij  intractable instead use mcmc inference particu                                                        deÔ¨Ånition fi including normalization zi  lar gibbs sampling straightforward sample each                                  unknown ground predicate turn conditioned          ‚àÇfi              ‚àÇzi                                                                            fi fj ‚àí  ground predicates conditional probability particular      ‚àÇwij           zi ‚àÇwij  ground predicate easily computed evaluating                                                          repeated applications chain rule  relative probabilities predicate true                                                        ‚àÇ logf‚àÇfi term sum derivatives  false                                                        paths through network fi given path    speed signiÔ¨Åcantly caching feature sums                                                        feature graph ffafkfi derivative  predicate updated notiÔ¨Åes parents                                                        path takes form fwafawbfb ¬∑¬∑¬∑wkfkwiwe  change necessary values recomputed                                                        efÔ¨Åciently compute sum paths caching    current implementation map inference uses                                                        perfeature partials ‚àÇf‚àÇfi analogous backpropagation  erated conditional modes icm besag  simple                                                          second term ‚àÇ logz‚àÇwij expected value  method Ô¨Ånding mode distribution starting                                                                                             Ô¨Årst term evaluated possible inputs   random conÔ¨Åguration icm sets each variable turn                                                        complete partial derivative  likely value conditioned variables                                                                                                                                              procedure continues singlevariable change fur ‚àÇ log ‚àÇ logfx   ‚àÇ logfx                                                                                    ‚àí ex  ther improve probability icm easy implement fast ‚àÇwij         ‚àÇwij              ‚àÇwij  run guaranteed converge unfortunately  guarantee converging likely overall conÔ¨Ågura individual components evaluated  tion possible improvements include random restarts simu computing expectation typically intractable  lated annealing                                 approximated using gibbs sampling efÔ¨Åcient    use icm Ô¨Ånd initial state gibbs sam alternative used richardson domingos                                                                                           ‚àó            pler starting mode signiÔ¨Åcantly reduce burn instead optimize pseudolikelihood   besag                                                                              n  time achieve better predictions sooner                  ‚àó                                                                 xx      xt  xtmbxxt    learning                                                                                                                            mb                                       given particular rrf structure initial set weights state markov blanket  learn weights using novel variant backpropagation data pseudolikelihood consistent estimator  algorithm traditional backpropagation goal little known formal properties  efÔ¨Åciently compute derivative loss function form poorly long chains inference required  respect each weight model case loss worked quite test domain  function error predicting output variables expression gradient pseudolog  joint log likelihood variables likelihood propositional rrf follows                                                                               consider partition function root feature zfor ‚àÇ ‚àó                                                                log                   ¬¨x mb    computations extract term                                                                                           ‚àÇwi  use refer unnormalized feature value                                                                                                                                              ‚àÇ      begin discussing simpler propositional case                   ‚àÇ log  log xt¬¨xt                                                                       √ó          ‚àí  abbreviate  arguments derivative                     ‚àÇwi          ‚àÇwi  log likelihood respect weight wij consists  terms                                                compute iterating query predicates    ‚àÇ log  ‚àÇ logfz  ‚àÇ logf  ‚àÇ logz     toggling each turn computing relative likeli                                     ‚àí       ‚àÇwij         ‚àÇwij         ‚àÇwij       ‚àÇwij        hood unnormalized likelihood gradient permuted                                                    ijcai                                                     state note compute gradient unnormalized  experiments probabilistic integrity  log likelihood subroutine computing gradient constraints  pseudologlikelihood longer need  approximate intractable normalization term    integrity constraints statements Ô¨Årstorder logic                                                                                                   learn relational rrf use domain instanti used detect repair database errors abiteboul et                                                                  ate propositional rrf tied weights number al   logical statements work errors  features number children feature critical increasingly impractical noisy  pend number objects domain instead databases arise integration mul  weight attached single feature attached tiple databases information extraction web  set groundings parfeature partial derivative want make constraints probabilistic sta  respect weight sum partial deriva tistically infer types errors sources date                                                                                                      tives respect each instantiation shared weight little work problem andrit                                                        sos et al  ilyas et al  early approaches                                                        natural domain mlns rrfs use    rrfs vs mlns                                      Ô¨Årstorder formulas construct probability distributions  rrfs mlns subsume probabilistic models   worlds databases  Ô¨Årstorder logic Ô¨Ånite domains trained gen common types integrity constraints  eratively discriminatively using gradient descent inclusion constraints functional dependencies inclusion  optimize log likelihood pseudolikelihood constraints form  optimizing log likelihood normalization constant        ‚àÄx‚àÉyrx ‚áí ‚àÉzsx  approximated using probable explanation  mcmc                                              example company   relation    mln converted relational rrf trans ‚Äúprojectlead‚Äùx charge project  lating each clause equivalent parfeature sufÔ¨Å relation ‚Äúmanagerof‚Äùx manages employee  ciently large weights parfeature approximates hard constraint says project leader manages  junction disjunction children worker course employees manage  weights sufÔ¨Åciently distinct parfeature employees lead project  different value each conÔ¨Åguration children evaluate mlns rrfs inclusion constraints  allows rrfs compactly represent distributions generated domains consisting  people  projects  require exponential number clauses mln   probability  person project leader leads    rrf converted mln Ô¨Çattening coleads each project probability  each  model typically require exponential number project leads manages employee probability   clauses mln intractable learning additional managing relationships generated prob  inference rrfs better modeling soft ability vary   actual  disjunction existential quantiÔ¨Åcation nested formulas leadership management relationships unobserved    addition ‚Äúsofter‚Äù mln clause rrf noisy versions corrupted probabil  parfeature represent different mln clauses simply ity vary    adjusting weights makes rrf weight learning converted constraint formula mln  powerful mln structure learning rrf rrf described previous sections added im                                                      plications projectleadx ‚áí projectleadx    recursive parfeatures root represent                           mln structure clauses managerofx ‚áí managerof predi  distributions nclause mln represent  cates primes observed ones    leads new alternatives structure learning mlns rrfs worked better given directions  theory revision domain little background knowl integrity constraint project leaders manage people  edge available rrf initialized small ran managers lead projects learned weights optimize  dom weights converge good statistical model pseudologlikelihood models results shown  potentially better mln structure learning figure  each data point average  traintest  constructs clauses predicate time set pairs rrfs show consistent advantage mlns  adjust weights evaluate candidate clause    cause better represent fact employee    background knowledge available begin manages people probably project leader  initializing rrf background theory just employee manages people  mlns addition known dependen    second type integrity constraints functional depen  cies add dependencies parfeatures dencies form  predicates small weights weight learning ‚àÄx  ‚àÉz  rx   ‚àß rx   ‚áí   learn large weights relevant dependencies negligi                                      ble weights irrelevant dependencies analagous functional dependency each determines unique  kbann does using neural networks tow equivalence set ys example suppose com  ell shavlik  contrast mlns theory pany table parts suppliers represented  revision through discrete search                     lation suppliertaxid companyname parttype sup                                                    ijcai                                                     
