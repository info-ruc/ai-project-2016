                generalizing bias term support vector machines                            wenye li    kwongsak leung       kinhong lee                             department science engineering                                   chinese university hong kong                                  wyli ksleung khleecsecuhkeduhk                          abstract                          ï¬nd solution regularized minimization problem      based study generalized form rep                                                                         m      resenter theorem speciï¬c trick construct                                             ing kernels generic learning model proposed           min        yifxi  Î³ fk                                                                        fâˆˆhk      applied support vector machines algo                         rithm obtained naturally generalizes      bias term svm unlike solution standard                                                        hk Î³ expression represents tradeoff      svm  consists linear expansion ker                                                        empirical error calculated loss func      nel functions bias term generalized algo                                                        tion  â€smoothnessâ€ solution represented      rithm maps predeï¬ned features hilbert space                                                        norm reproducing kernel hilbert space rkhs      takes special consideration                                                                                               leaving space unregularized seek  induced kernel Î³ called      ing solution space empirical evaluations regularizer svm partially derived frame                                                                                                    â‰¡      conï¬rmed effectiveness general work choice hinge loss function yf                                                        max  âˆ’          ization classiï¬cation tasks                            yf      incompatibility comes                                                        bias term makes combined model  gener                                                        ally hk    introduction                                                          paper follow regularization point view  support vector machines svm shown based study generalized regularizer leaves  form machine learning applications based hypothesis space unregularized speciï¬c trick  vapnikâ€™s seminal work statistical learning theoryvap constructing kernels propose new learning scheme  nik  algorithm starts ideas separating                                                      allows user predeï¬ne features uses fea  hyperplanes margins given data xi yii                                                      tures kernel expansions derive solution hy  xi âˆˆr    yi âˆˆ   âˆ’ searches lin pothesis space instead considering kernel expansions  ear hyperplane separates positive negative sam existing kernelbased methods idea applied  ples largest margin distance hyper svm obtain algorithm naturally generalizes  plane nearest data point nonseparable case bias term svm generalized term linearly com  soft margin method used choose hyperplane bines predeï¬ned features instead constant  splits examples cleanly possible using kernel                                                      different empirical results existence  kx    positive deï¬nite function extend bias term does make signiï¬cance practicerifkin  algorithm nonlinear case optimal hyperplane  show term trivial  simple representation linear expansion generalized appropriate choices term improve  kernel functions constant                    applicability algorithm                        m                    âˆ—                                     paper organized follows section                         cikxi                                                                       vestigate generalized regularizer regularized learning                                                                               framework study associated solution applied  each ci real number called bias term svm section  introducing kernel construc    case nonseparability meanings mar tion trick predeï¬ned features mapped  gin motivation lost girosi  evgeniou et al  rkhs clarify mathematical details deriv  poggio smale  suggest different view svm ing svm new algorithm proposed section  based  based framework championed poggio   previous discussions empirical results demonstrated  researcherspoggio girosi implic section  finally section  presents discussions  itly treats learning approximation problem tries conclusions                                                    ijcai                                                       generalized regularized learning                   equation holds âˆˆhk  speciï¬cally letting                                                            Â·  suppose hk direct sum subspaces hk  hâŠ• kx  gives                    Â·Â·Â·                 â‰¤                               m        span  Ï•   Ï• spanned                         âˆ—                                                                           pf          Î±iyikxi  linearly independent features consider generalized                  Î³  regularized learning minimizing                                                             m                                    reproducing property kernels           min                                         yif     Î³  pf                                                  fâˆˆhk                                                                                                                                     âˆ—     âˆ—      âˆ—                                                                        âˆ’ pf         Î±iyikxi                                                                                     pf orthogonal projection                              Î³                 Î³  called generalized regularizer âˆ—       âˆ—                            âˆ—  model applied svm consider hinge loss âˆ’ pf  orthogonal projection hand                                                                                 function mentioned introducing slake variables Î¾i represented Î»pÏ•psowehave  corresponding empirical error point xi problem                                                                           m                                                                     âˆ—                                                                                    Î»pÏ•p      cikxi                         min       Î¾i  Î³ pfpf                                                     fâˆˆh                                                                                                      âˆˆr            satisfying                                            Î»p     ci  Î³ Î±iyi                                                          derived minimizer  satisï¬es reproduction                  yif xi â‰¥    âˆ’ Î¾i                                                                                                 property suppose xi yi comes model                           â‰¥                                                                        Î¾i                               perfectly linearly related Ï• Â·Â·Â·Ï• desirable  Â· Â·k denotes inner product hk wederivethe solution independent features ev  dual quadratic program using technique lagrange ident result  property satisï¬ed parameters  multipliers                                          Â·Â·Â·cm  zero makes regularizer                 m                                      equal zero shown experiments                                                       property effect stabilizing results dif                 Î¾i  Î³ pfpfk                                                               ferent choices kernels regularization parameters prac                m                      m              tically              âˆ’    Î±i yif xi âˆ’ Î¾i âˆ’   Î¶iÎ¾i                                                   mapping predeï¬ned features rkhs                                           want minimize respect Î¾ maxi decomposing hypothesis space hk studying gen  mize wrt Î±i Î¶i subject constraints primal eralized regularizer proposed generalized regular  problem nonnegativity constraints Î±i Î¶itaking ized learning model derived associated solution  derivative wrt Î¾i setting zero  consists linear expansion kernel functions prede                 âˆ‚l                                    ï¬ned features section introduce kernel                        âˆ’ Î±i âˆ’ Î¶i                              âˆ‚Î¾i                                  struction trick maps kernel functions predeï¬ned                                                        features simultaneously hilbert space  substituting                                                      show mathematical details deriving svm gener                                                      alized bias term        Î³ pfpfk âˆ’     Î±iyif xi   Î±i                                                  âˆ—                                              kernel construction trick  suppose minimizer  âˆˆhk let      âˆ—           âˆˆr        âˆˆh                        given features Ï• Â·Â·Â·Ï• strictly positive deï¬nite      Î´g Î´       wehave                      Î¦                      âˆ—           âˆ—                     function  let consider following reproducing kernel             Î³ pf  Î´pgpf   Î´pgk                  m                    m                                                                     âˆ—                                                                âˆ’    Î±iyi  Î´gxi    Î±i                                    Ï•p    Ï•p                                                                                      taking derivative wrt Î´wehave                                                    m      âˆ‚l          âˆ—         Î³  pf   Î´pgpg  âˆ’     Î±iyig xi                                                   âˆ‚Î´                                                                                                                              âˆ‚l                                                            âˆ—                                                    Î¦x           Ï•p Ï•q yÎ¦xp xq  minimizer âˆ‚Î´ Î´   problem                                                                                   m                                                                        âˆ—                                               âˆ’      xÎ¦x   âˆ’      yÎ¦x           Î³ pf pgk âˆ’     Î±iyig xi                          Ï•p               Ï•q                                                                                                                                          ijcai                                                                                                                                    Â·Â·Â·                                                    Ëœ     Ëœ     Ëœ                               Ï•   Ï•  deï¬nes linear transformation prede Î»  Î» Â·Â·Â· Î»  Î±    Î±     Â·Â·Â·Î±m   ï¬ned features Ï• Â·Â·Â·Ï• wrt Â·Â·Â·x                                                              Â·Â·Â·                          Â·Â·Â·                                    âˆ’             Î±         Î±   Î±    Î±         Î±   Î±m        Ï•      Ï•  Â·Â·Â·  Ï• x      Ï•                   Â·Â·Â·                  Â·Â·Â·                                                                   diag  y         diag  y  ym       Â·Â·Â·          Â·Â·Â·         Â·Â·Â·          Â·Â·Â·                                        m                                                          Ï•p xi           Ï•p xi              Ï•       Ï• Â·Â·Â·  Ï• x      Ï•                     pi               pi                                                    vector ones appropriate size taking derivative                                                             Ëœ  satisï¬es                                         wrt Î» obtain                           â‰¤  â‰¤             Ï•  xp                                             eyÎ±   eyÎ±                                        â‰¤   â‰¤                                                            trick studied light wayne  pro                                                                                          vide alternative basis radial basis functions ï¬rst      Î³Ëœc hËœc âˆ’ Î± yhËœc  Î±          used fast rbf interpolation algorithmbeatson et al   sketch properties peripheral concerns taking derivative wrt Ëœc setting zero  cludes                                                                       Î³hËœc âˆ’ hyÎ±                          kxp  Ï•p                                                                                                                                                    strictly positive deï¬nite invertible                Ï•pÏ•q                                                                                                                                           yÎ±                                                                          Ëœc                                             hxp                                                   Î³                                                hxi Ï•p                                                                       substituting                                                 hxi hxj                                                                                                              â‰¤ â‰¤ and   â‰¤ â‰¤ use           Î± hyÎ±  âˆ’    Î± hyÎ±     Î±                                                               Î³                Î³  ful property matrix xi xj                                                                                      ij                      strictly positive deï¬nite used following                                                                                       âˆ’       Î± hyÎ±  âˆ’ Î³  Î±  computations                                                    Î³       property  predeï¬ned features                                                        problem  Ï• Â·Â·Â·Ï• explicitly mapped hk                               subspace  span Ï• Â·Â·Â·Ï•span Ï• Â·Â·Â·Ï•by                                                                                                                        Â·Â·Â·                                              min  Î± hyÎ±   âˆ’ Î³ Î±           property  Ï• Ï•  forms orthonormal basis            Î±                                                           satisfying    computation                                                                                                                 â‰¤   â‰¤                     using kernel deï¬ned   properties   Î±   mand      Î±      Î±            minimizer  rewritten                                                     ï¬rst box constraint comes  nonnegativ                                                                                              âˆ—                                         ity Î± requiring  â‰¤ Î±i â‰¤  â‰¤ â‰¤ second                     Î»pÏ•p      cikxi                                                                                                                                                                              equality constraint comes  hy strictly                    Â·Â·Â·                                positive deï¬nite matrix problem standard                              m                      quadratic programqp comparing svmâ€™s qpvapnik                        Ëœ                                                         Î»pÏ•p       cËœihxi                new problem requires  equality constraints                           i                    stead equality constraint burden        Ëœ     Ëœ                                         computations actually major computations  Î» Â·Â·Â· Î» cËœ Â·Â·Â· cËœm parameters solving qp come box constraint equality  termined furthermore orthogonal property                                                        constraints affect computations                                                                                                  Ëœ  Ï•p wehave                              solution Ëœc obtained Î±  Î» comes                          m                            linear program                      âˆ—                            Ëœi                                                                                      m                         i                                                                                                                              min       Î¾i                                                                                            Î»Ëœ  property                                                                         âˆ—    âˆ—   Ëœct hËœc                 pf  pf                            satisfying       Ëœc Ëœ    Â·Â·Â· Ëœ                                     â›                      â      c    cm  substituting  ob              m  tain matrix representation                             â     Ëœ          Ëœ    â    â‰¥    âˆ’                                                            yi     Î»pÏ•p         cjhxj             Î¾i                   Ëœ        Ëœ                             j    Î³Ëœc hËœc âˆ’ Î± ye Î» âˆ’ Î± Î»  hËœc   Î±                                                                                     Î¾i  â‰¥                                                       ijcai                                                       generalized algorithm    algorithm  based discussions algorithm general  izes bias term svm proposed follows                            start data xi yii     â‰¤ predeï¬ned linearly independent features                                               Ï• Â·Â·Â·Ï• data deï¬ne Ï• Â·Â·Â·Ï• according      equation     choose symmetric strictly positive deï¬nite function                                                    Î¦x Î¦x   continuous Ã—r       hx according equation                rd â†’r                                   figure  image classiï¬cation accuracies using coil dataset    deï¬ne                                        different number training images predeï¬ned                              m                      features gaussian kernel Î¦ used svm                     Ëœ                                 gb Î¦  used svm kernel regularization             Î»pÏ•p       cËœihxi                             i                    parameters selected cross validation oneversusone                                                        strategy used multiclassiï¬cation       solution cËœ Â·Â·Â· cËœm comes      quadratic program  equation       Ëœ      Ëœ                                             experiments      Î» Â·Â·Â· Î» obtained solving linear program                                                         evaluate effectiveness brought generalized bias    virtual samples                                  term groups empirical results reported ï¬rst  new algorithm needs construction orthonormal group experiments focused reproduction property                         Â·Â·Â·                             Â·Â·Â·     generalized bias term used columbia university im  basis Ï•   Ï•  predeï¬ned features Ï•   Ï•                                wrt samples linear transformation al age library coil dataset image classiï¬cation                                                                          Â·Â·Â·  gorithms search linear hyperplanes separate dataset  images    evenly distributed                                                          data invariant linear transformations pro classes preprocessing each image represented                                                             Ã—     vide invariance consider alternative approach       dimensional vector each compo                                                                                        avoids transformation                            nent having value    indicating gray                              Â·Â·Â·                   level each pixel experiments performed each    introduce  virtual points   v satisfying                                                       experiment used ï¬rst     images respectively                            â‰¤  â‰¤                each class training rest testing            Ï•p xvq                                                   â‰¤   â‰¤                 using recently developed nonlinear dimensionality reduc                                                        tion technqiuestenenbaum et al  roweis saul                                               label yvq each virtual point vq deï¬ned  images actually form manifold  study following problem                          lower dimension original   dimen                                                        sions prior knowledge features Ï• Â·Â·Â·Ï•                      min  lv                                      fâˆˆhk                              predeï¬ned follows                                                                                                                                              nn  xi âˆ© trainp                                                                 Ï•p xi                                                                                                                     lv            âˆ’ yvq xvq                             â‰¤  â‰¤   â‰¤  â‰¤                                                                             nn     deï¬nes three                                                    nearest neighbors geodesic distance xandtrainp                   m                                                      set training images class label geodesic dis                      âˆ’ yif xi  Î³ pf          tance computed graph shortest path algorithm                                                                                             tenenbaum et al  deï¬ning features way                      m                                implicitly used knn classiï¬er generalized bias term                                                                      âˆ’ yif xi  Î³ pfk    similar ideas generalized combining different ap                                                proaches knn                                                          figure  compares classiï¬cation accuracies svm    easily seen Ï• Â·Â·Â·Ï• formed                                                        knn new algorithm denoted svmgb  thonormal basis wrt virtual points                                                      sightful study results ï¬nd  predeï¬ned  need compute Ï•  Â·Â·Â·Ï• new regularized                                                      features dominated learned svmgb solution  minimization problem  equivalent original                                                        reproduction property performance   hinge loss introduction virtual points                                                        algorithm similar knn shows better performance  does change scope classiï¬cation problem                                                        standard svm dataset  having avoided linear transformation construct  thonormal basis                                         httpwwwcscolumbiaeducavesoftware                                                    ijcai                                                                                                           gb better results svm bow representation                                                        experiments gives belief embedding                                                        plsa features improves accuracy                                                             discussion conclusion                                                        idea regularized learning traced modern                                                        regularization theorytikhonov arsenin  morozov                                                         states existence regularizer helps                                                        provide unique stable solution illposed problems                                                        paper considered usage generalized regular                                                        izer kernelbased learning straightforward reason                                                        hypothesis space left unregularized new  figure  image classiï¬cation accuracies ï¬xing regularizer wellaccepted fact ï¬nite dimen  groups kernel regularization parameterslogscale sional problems wellposedbertero et al                                                         vito et al  need regularization                                                        case learning problems deï¬ned subspace    reproduction property helps lessen sensitiv spanned number predeï¬ned features similar idea  ity algorithm choices kernels regular explored spline smoothingwahba  leaves  ization parameter Î³ illustrate effect experi polynomial features unregularized kernelbased learn                        Ã—  ment conducted using    training images instead ing similar work semiparametric svm  using cross validation parameter selection groups modelsmola et al  combines set unreg  kernel regularization parameters ï¬xed figure  ularized basis functions svm knowl  picts results generalized bias term makes svmgb edge algorithms applications investigated  stable variations kernel regularization pa machine learning tasks uniï¬ed rkhs regulariza  rameters property especially useful concerning tion viewpoint general way  practical situations arbitrariness exists designing new regularizer speciï¬c trick construct  kernels applications                        ing kernels predeï¬ned features explicitly mapped    study performance new algorithm hilbert space taken special consideration during  second group experiments conducted text catego                                                       learning differentiates work standard  rization tasks using newsgroups dataset  dataset col kernelbased approaches consider kernel expansions  lects usenet postings newsgroups each group constant learning projecting predeï¬ned fea                  messages experimented tures rkhs idea conditionally positive def  major subsets ï¬rst subset ï¬ve groups comp inite functionmicchelli  lurking background  second groups rec groups sci goes discussion paper  groups talk                          applying idea svm developed new                                          dataset removed  words algorithm regarded having generalized  highest mutual information class variable rain bias term svm domain speciï¬c knowledge pre  bow packagemccallum  each document repre deï¬ning features generalization makes bias term  sented bagofwords bow treats individual words trivial conï¬rmed experimental results                                       features linear kernel coupled predeï¬ned fea correct choices features help improve accuracy          Â·Â·Â·  tures Ï• Ï• obtained probabilistic la make algorithm stable terms sensitivity  tent semantic analysis plsa hofmann  used selection kernels regularization parameters  input svmgb experiments carried dif               Ëœ   ferent number        training documents each acknowledgments  subset experiment consisted runs average  accuracy reported each run data separated research partially supported rgc earmarked  xvalprep utility accompanied package grant hong kong sar    comparison svm tested linear kernel rgc research grant direct allocation chinese uni  commonly used method text categorizations versity hong kong  completeness bow plsa representations  documents experimented seen ï¬gure  references  svm plsa representation performs                                                        beatson et al  rk beatson wa light  number training samples small approach loses                                                           billings fast solution radial basis function  advantage training set increases moderate                                                           interpolation equations domain decomposition methods  size svmgb reports best performance                                                           siam sci comput â€“    training documents shown svm                                                        bertero et al  bertero mol er pike    httpwwwcscmueduËœtextlearningdatasetshtml     linear inverse problems discrete data ii stability    httpwwwrulequestcompersonalcrtargz       regularisation inverse probl â€“                                                     ijcai                                                     
