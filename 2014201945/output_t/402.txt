                                robust planning lrtdp                                    olivier buffet   douglas aberdeen                                         national ict australia                                      australian national university                    olivierbuffetdouglasaberdeennictacomau                          abstract                          ponent chooses possible models reduce long                                                        term utility      stochastic shortest path problems ssps sub    principal aim develop efÔ¨Åcient planner      class markov decision problems mdps     common subclass mdps policies      efÔ¨Åciently dealt using realtime dynamic  guaranteed eventually terminate goal state stochastic      programming rtdp mdp models     shortest path ssp problems greedy realtime dy      uncertain obtained through statistics guessing namic programming algorithm rtdp barto et al       usual approach robust planning searching  particularly suitable ssps Ô¨Ånds good policies quickly      best policy worst model   does require complete exploration state space      paper shows rtdp robust          paper shows rtdp robust sec      common case transition probabilities tion  present ssps rtdp robustness sec      known lie given interval                  explains rtdp turned robust algorithm                                                        finally experiments presented analyse behavior    introduction                                       algorithm discussion conclusion  decisiontheoretic planning markov decision problems  bertsekas tsitsiklis  major  background  probabilistic model domain available number  stochastic shortest path problems  algorithms make possible Ô¨Ånd plans policies optimiz stochastic shortest path problem bertsekas tsitsik  ing expected longterm utility optimal policy                                                        lis  deÔ¨Åned tuple hs ci  vergence results depend assumption proba scribes control problem Ô¨Ånite set states  bilistic model domain accurate                                                        ‚àà starting state ‚äÜ set    unfortunately large number mdp models based goal states Ô¨Ånite set possible actions actions  uncertain probabilities rewards rely statistical control transitions state state accord  models physical natural systems plant control ing system‚Äôs probabilistic dynamics described  animal behavior analysis statistical models                                                                                      transition function deÔ¨Åned   rst   times based simulations mathematical                                                         st   aim optimize performance  models observations real human expertise measure based cost function  √ó √ó ‚Üí    working uncertain models Ô¨Årst requires answering ssps assume goal state reachable state  closely related questions ‚Äì model uncer optimal policy stuck  tainty ‚Äì use resulting model existing work looping subset states algorithm solving ssp  shows uncertainty represented set Ô¨Ånd policy maps states probability distributions                                               possible models each assigned model probability munos actions œÄ  ‚Üí Œ†a optimizes longterm          simplest example set possible mod cost deÔ¨Åned expected sum costs goal state                                    els assumed equally probable bagnell et al  paper consider ssps planning pur                         nilim ghaoui   construct possibly poses knowledge tuple deÔ¨Åning problem  inÔ¨Ånite set models represent model uncertainty al                                                        hs ci framework wellknown stochas  lowing each probability single model lie interval tic dynamic programming algorithms value iteration                                   givan et al  hosaka et al               vi make possible Ô¨Ånd deterministic policy cor    uncertain probabilities investigated resource responds minimal expected longterm cost ‚àó value                                allocation problems munos  ‚Äî investigating efÔ¨Åcient iteration works computing function ‚àós gives  exploration strehl littman  state aggregation  givan et al  ‚Äî policy robustness bagnell et al work presented details buffet aberdeen    hosaka et al  nilim ghaoui  fo model certain make usual assump                                                                              cus later considering twoplayer game op tion cs  es cs expected sum costs optimal policies simplistic approach computes average model  unique solution Ô¨Åxed point bellman equation   probable model uses standard ssp                                                      optimization methods approaches guarantee     js     min     cs  js   longterm cost policy true model differs               a‚ààa                   s‚ààs                                 chosen optimization                                                          follow approach described bagnell et al   updating formula leads asymptotic conver                                                        consists Ô¨Ånding policy behaves  gence ‚àó convenience introduce qvalue                                                   worst possible model amounts considering  qs      cs                ‚ààs                                       player zerosum game player‚Äôs gain opponent‚Äôs    ssps easily viewed shortest path problems                                                        loss player chooses policy actions ‚Äúdis  choosing path probabilistically leads ex                                                        turber‚Äù opponent simultaneously chooses policy mod  pected destination represent useful sub set                                                        els simultaneous game optimal policies  mdps essentially Ô¨Ånitehorizon mdps                                                        stochastic results maxminlike algorithm  discount factor                                                                         max    min  jœÄmœÄa    rtdp                                                           œÄm‚ààŒ†m  œÄa‚ààŒ†a  trial based realtime dynamic programming rtdp ssp game value iteration does converge Ô¨Åxed  troduced barto et al  uses fact ssp solution patek bertsekas   costs positive additional assumption possible optimistic considering  trial reach goal state probability  players collaborate endure costs  zero initialization longterm cost function turns max min previous formula second  qvalues monotonically increase during iterative case equivalent classical ssp decision consists  computation                                          choosing action local model    idea rtdp algorithm  follow paths  start state greedily choosing actions locality ‚Äî maxmin algorithm particu  lowest longterm cost updating qs states larly expensive implement restricting search  encountered words action chosen deterministic policy models requires computing  expected lead lowest future costs iterative optimal longterm cost function each model pick  computations show action better  ing worst model optimal policy associated                                                        simpler process used com  algorithm  rtdp algorithm ssps                   pute looking simultaneously worst model                                                        requires hypothesis nextstate distributions ¬∑    rtdpsstate      repeat                                              independent stateaction pair      rtdptrials                                      assumption does hold makes     termination condition                   things easier opponent larger set                                                        models choose consequence conserva    rtdptrialsstate                                  tive producing robust policies    ¬¨goals                                     assume independence stateaction level      greedyactions                                state level equivalent situation      jsqvalues                                 second player makes decision depending      picknextstates                            current state Ô¨Årst player‚Äôs action situation    end                                           amounts sequential game previous players                                                        known player players act                                                        deterministic way loss efÔ¨Åciency    rtdp advantage quickly avoiding plans lead result assumption worst model  high costs exploration looks mainly promis chosen locally updated given stateaction  ing subset state space follows paths pair seen algorithm  worst local model  simulating system‚Äôs dynamics rare transitions                                                        ms  change qvalues evolve previous updates  rarely taken account use simulation makes reachable states‚Äô longterm costs change relative  possible good policies early expense dering changing outcomes considered worst  slow convergence bad update frequency key contribution paper show rtdp  rare transitions                                     robust allowing planning large                                                        uncertain domains retaining worst best case behaviour    robust value iteration                           guarantees  pessimism optimism ‚Äî   turn problem  taking model‚Äôs uncertainty account looking  robust rtdp  ‚Äúbest‚Äù policy possibly inÔ¨Ånite set alternative  models denoted                                      consider intervalbased uncertain                                                        ssps  known interval    assume trial based rtdp implementation rminss rmaxss figure  example wealgorithm  robust value iteration ssp       worst outcome requires Ô¨Årstly sorting reachable                                                                                                          initialize                                 states decreasing order values cs  js ‚â•                                                                                            repeat                                            cs  js ‚â• ¬∑ ¬∑ ¬∑ cs sk  jsk worst     state                              distribution giving highest probability                                                                                               action                           Ô¨Årst state sk pointed            ‚Üê max                        givan et al  equivalent Ô¨Ånding highest             max            ms ‚ààms                                                 index ‚àà                        js                     s‚ààs ms                 ms       end                                                       r‚àí              ji ‚Üê mina‚ààa  qmaxs                                       max     min                                                                          pi       pi  ‚â§       end                                                               ir    converges                                                        resulting transition probabilities                                                                                  max  discuss pessimistic approach optimistic leading                   pi                                                                       rsi       min                    similar results                                                             pi                                                                                                                                                                                                                       rsr     ‚àí       rsi                                                                                                                     iir                                                                                             pk    min                                                        using precomputed bound bmin   pi  alg                                             gives complete implementation insertion sort algo                                                        rithm chosen list usually ordered pre                                  vious updates                                                                                                                               algorithm  worst model stateaction pair        certain ssp           uncertain ssp                                                          worstmodels state action                                                                        figure  views ssp depending     ¬∑ ¬∑ ¬∑  sk  reachablestatessa  model uncertainty taken account costs parenthesis sortr  uncertain ssp action prefered quickly   bound  bmin                                                                        min    max  reaches goal                                     bound ‚àí pi   pi                                                                                 min   max                                                            bound ‚Üê  bound ‚àí pi   pi                                                            rs  ‚Üê pmax    given stateaction pair list                                                                 ‚Üê    ¬∑ ¬∑ ¬∑  sk reachable states each reachable state                   min  max                              end  si ‚àà ii  pi  pi  possible models   ones comply interval constraints en rs  ‚Üê  ‚àí bound ‚àí pmin                                                               suring             fig  illustrates three ‚àà        reachable states                                                  min                                                            rsi ‚Üê pi                                                                                            end                                                          return r¬∑         max                                                             summarise robust vi intervalbased ssp                                                        sists applying normal value iteration transition    min                                                 probabilities updated through alg    ps                                                         need single worst model compute cor                                                                                                                                                              responding qvalue reachable states si                                                                                                                                         value cs sijsi sr                                                                                                                            set states sr inÔ¨Ånite number equiva  figure  triangle probability simplex representing lent worst local models model differing  possible probability distributions three different                                                      probability mass distributed equally bad states  comes rsi   si vertex left triangle                                                        sr worst local model  trapezium showing interval constraint  right triangle shows possible models intersection  three interval constraints                           worst global models ‚Äî   contrary vi rtdp does                                                        necessarily visit complete statespace barto                                                        et al  introduces notion relevant states                                                        extend uncertain case state relevant  worst local models ‚Äî  maximisation step compute  qs alg  giving highest probability httpenwikipediaorgwikiinsertion  sortthere exists start state model ‚àà optimal transformation ensures  additional maxm‚ààm  policy œÄ model reached update formula alg  line  does break  state controller uses œÄ              quirement jt increasing nonoverestimating    notion important equally worst lo   relevant states visited inÔ¨Ånitely guaran  cal models given stateaction pair forbid different teeing complete optimal policy  states models state rel  rtdpsspg robust rtdp differ rtdp  evant rtdp Ô¨Ånd    sspg assumes optimal opponent robust case  optimal policy just relevant states worst wish plan opponents choose  global model does policy apply wrong model robust rtdp rtdpsspg fol  possible states apply reachable states                                                            lowing model md ‚Äî simulation ‚Äî allows  model optimal policies relevant states states visited rtdpsspg optimal opponents  covering relevant states worst model used                                                            forbid md described sec  state  dating qvalues does necessarily cover relevant states reachable model excluded  depends model used choose state search redeÔ¨Åned expanded set rel  simulate system‚Äôs dynamics                  evant states added robustness relevant states    avoid missing relevant states each local model used visited inÔ¨Ånitely  simulation ensure reachable states  visited seen fig  set possible local mod  els stateaction pair ndimensional convex poly                                                          use model simulation ensures  tope model inside polytope excluding bound                                                                         policy cover states opponent lead  ary adequate si ensures                                                       assumes worst model apply  sis      exists global model md used ef  fectively simulate system‚Äôs dynamics missing  experiments  potentially reachable state                          labelled rtdp bonet geffner  modiÔ¨Åed ver                                                        sion rtdp robust similar way    robust trialbased rtdp                        experiments conducted illustrate behavior robust  robust rtdp differs original rtdp   lrtdp end compared bagnell et al‚Äôs robust                                                        value iteration lrtdp cases conver    ‚Ä¢ each time algorithm updating state‚Äôs evaluation              ‚àí      opponent looking worst local model gence criteria    lrtdp vi stop      serves compute qvalues                   maximum change state longterm cost                                                        iteration ‚àí    ‚Ä¢ exploration purposes algorithm follows dynam      ics consider possible transitions  heart      using model                                                       Ô¨Årst experiment compare nonrobust optimal pol    ‚Ä¢ ‚Äúrelevant‚Äù states states reachable follow icy robust small example fig ta      ing optimal policy possible model  ble  shows theoretical expected longterm costs each    adapt context convergence theorem policy normal probable model   barto et al  corresponding proof dis pessimistic optimistic ones robust policy largely  cussing mainly changes                           better pessimistic case  proposition  uncertain undiscounted stochastic short  est path problems robust trialbased rtdp initial table  theoretical evaluation robust nonrobust poli  state each trial restricted set start states converges cies various models match empirical evaluation                        ‚àó  probability set relevant states         normal   pessimistic optimistic  controller‚Äôs policy converges optimal policy pos nonrobust                 sibly nonstationary set relevant states robust                    conditions theorem  barto et al   proof proof outline                                                          mountaincar    shown sec  robustness achieved consid      ering stochastic shortest path game sspg par use mountaincar problem deÔ¨Åned sutton      ticular sequential sspg bert barto  starting valley car      sekas tsitsiklis  establish value iteration momentum reach mountain                                                        fig  dynamics described moun      qlearning algorithms provided states visited            Ô¨Ånitely asynchronous value iteration tain car software employed objective      used solve sequential sspgs avisspg        minimize number time steps reach goal    adapt proof barto et al  transform httpwwwcsualbertacaÀúsutton¬∑ ¬∑ ¬∑      avisspg rtdp algorithm rtdpsspg    mountaincarmountaincarhtml                                                                                       longterm cost function                                       goal                                                  example path                 road reaction                                  vxv                          acceleration                                                                                                                                                                                                                                                                                                                                                                gravity                                                                                                              ‚àí          position                                                                                                                                                                                                                                       figure  mountaincar problem                                                                                                                                                                                                                                                         value iteration    continuous statespace discretized  √ó  grid                                                                                         longterm cost function  corresponding uncertain model transitions ob                                example path  tained sampling  transitions each stateaction    vxv  pair each transition computed intervals                                                                    true model lies  conÔ¨Ådence                                                                                                                                                                                                                 results ‚Äî  preliminary remark simulating path generally      shows car oscillating times leaving valley                                                                     main explanations  speed gathering     just sufÔ¨Åcient reach summit                                                                                                    discretized model accurate applying                                  policy obtained true mathematical model instead                                                                                                                                                                                      discretized better           robust value iteration    fig  shows longterm cost function obtained using    value iteration lrtdp robust counterparts                           longterm cost function                                                                                             example path  mountaincar problem axes car‚Äôs position vxv    speed axis expected cost goal    surface example path start state goal state    follows greedy policy average model             general shape surface obtained                                                                         unexplored parts statespace            lrtdp robust lrtdp expected vertical scales        larger robust cases reÔ¨Çects fact        reaching goal timeconsuming pes                                      simistic model interpreted av                                                                                                                erage time goal graphs show small uncer                                                                                                   tainty lead longer policies times multi lrtdp  plied                                                                                          longterm cost function    executing different algorithms eval                                  example path  uation current greedy policy  ‚àó       vxv  nstates                                                                  value updates result appears        fig  axis expected cost goal                                                                   start state subÔ¨Ågures lrtdpbased algo          rithms obtain good policies quickly slow conver                                                                    gence times vi √ó  updates lrtdp √ó            rvi √ó  rlrtdp √ó                                                                                                                                          experiments buffet aberdeen                                          Ô¨Årm results example showing lrtdp                                                                                                                  faster convergence vi illustrat                                                                                                  ing approach temporal planning problem     robust lrtdp    discussion conclusion                          figure  longterm cost functions mountaincar                                                        problem each case example path generated based  simple extension work suggested hosaka et al                                                        likely model   Ô¨Ånd set worst models
