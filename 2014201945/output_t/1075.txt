                       feature    selection   based     shapley    value                                     shay  cohen    eytan  ruppin                                       school   sciences                                     telaviv university telaviv  israel                                 Â                                       cshayruppin      Â¡ posttauacil                                                 gideon   dror                                      department   science                        academic   college telaviv yaffo telaviv  israel                                          gideonmtaacil                              abstract                          containing iid sampled instances Â¢Â¤Â£Â¥Â§Â© form                                                                                                                              Â¥ available train validation test represent          present study contributionselection al ing training set validation set test set respectively                                                                                                              gorithm csa novel algorithm feature se                                                                                                                                          Â¥Â¥ Â¢Â©                                                        given induction algorithm set          lection algorithm based multi     stands classiï¬er constructed training set      perturbation shapley analysis framework  ing induction algorithm input variables nar      relies game theory estimate usefulness                                                            rowed ones  Â¢Â© labels each                                                                                                               algorithm iteratively estimates usefulness                                     abdc                                                                                  Â¤ Â© Â¤                                                            stance form Â¢Â¥                features selects accordingly using                                                        value domain Â§  task feature selection      forward selection backward elimination empir                                                        choose subset  input variables maxi      ical comparison existing feature mize performance classiï¬er test set      selection methods shows backward elimi  follows shall focus optimizing accuracy classi      nation variant csa leads accurate  ï¬er easily optimize performance      classiï¬cation results array datasets    measure area roc curve balanced error                                                        rate    introduction                                         rest paper organized follows section                                                         troduces necessary background game theory  feature selection refers problem selecting input vari detailed description csa algorithm section  provides  ables called features relevant predicting empirical comparison csa feature  target value each instance dataset feature selection selection methods accompanied analysis results  potential beneï¬ts defying curse dimension section  discusses empirical results provides  ality enhance prediction performance reducing mea insights success backward elimination version  surement storage requirements reducing training csa algorithm  prediction times paper focuses ï¬rst issue  selecting input variables attempt maximize perfor  mance classiï¬er previously unseen data          classiï¬cation  coalitional game    paper suggest recast problem feature cooperative game theory introduces concept â€œcoali  selection context coalitional games notion tional gamesâ€ set players associated  game theory perspective yields iterative algorithm payoff real function denotes beneï¬t achieved      feature selection contributionselection algorithm different subcoalitions game formally coalitional                                                                                                                                                                                                  ei     Â¥jjkÂ¥      csa intent optimizing performance classiï¬er game deï¬ned pair Â¢efÂ¥ghÂ©                                                                                       ne  unseen data algorithm combines ï¬lter set players gÂ¢lmÂ©   real number      wrapper approaches unlike ï¬lter methods features associating worth coalition   game theory  reranked each step using classiï¬er black pursues question representing contribution each  box ranking based shapley value shapley player game constructing value function   known concept game theory estimate assigns realvalue each player values correspond  importance each feature task hand speciï¬ contribution players achieving high payoff  cally taking account interactions features contribution value calculation based shapley    paper use following notations value shapley  intuitive example potential      distribution dataset instances drawn rep use shapley value provided academic set                                     Â£   Â¢Â¥Â¥Â©  resented variables Â¢Â¤Â£Â¦Â¥Â¨Â§ Â©            ting assume professor running lab      represents input variables vector features Â§ decided distribute yearly bonus      represents discrete target value class Â£  three sets students fair manner reï¬‚ects actual contribution each student academic success lab case shapley value feature measures  during year students form spontaneous â€œcoalitionsâ€ tribution combined performance measure just  groups students each group works publishes sum corresponding shapley values linearity      paper summarizing work coalitions shapley value consequence property                                                                                                           assembled professor paper gets rank payoff function multiplied real number                                                                                               Â§         Â§                                                                                                                                                                                                           Â¢ghÂ©   Â¢ghÂ©  impact factor composing â€œpayoff functionâ€ based shapley values scaled    annual data studentsâ€™ coalitions associ words multiplying performance measure  ated payoffs shapley value provides fair efï¬cient constant does change ranking features vital  way distribute bonus each individual student accord property scheme ranks features â€™impor  ing contribution year                tanceâ€™        shapley value deï¬ned follows let marginal                                          Â¡Â        importance player  coalition         estimating features contribution  using                                                     Â¢                                             msa                                                gÂ¢lÂ¤Â£  Â©Â¦Â¥ gÂ¢lmÂ©                  Â¢Â¤mÂ©m                                                                                   calculation shapley value requires summing  shapley value deï¬ned payoff       possible subsets players impractical                                                               case keinan et al  presented unbiased esti                                                             Â¢                    Â§                                       mator shapley value uniformly sampling permu                         Â¢ghÂ©                                    Â¢ Â©Â©                                   Â¨Â¢Â¤                                                                                                             Â©Â¨                                                           tations  estimator considers large                                                                                         small features sets calculate contribution values                                                                                              Â¢ Â©     set permutations  feature selection algorithm use shapley value heuris      set players appearing  th player permutation tically estimate contribution value feature        shapley value player weighted mean task feature selection realistic cases    marginal value averaged possible subsets players sume size  signiï¬cant interactions fea      transforming game theory concepts arena tures smaller number features    feature selection attempts estimating limit calculating contribution value    tribution each feature generating classiï¬er players mutations sampled set players                                                           ing bound permutation size notice ï¬l            mapped features dataset payoff                                                                                                                                                                   Â¢Â¤ Â©  represented realvalued function  measures ter methods equivalent using inter      performance classiï¬er generated using set fea actions taken account feature selection using ran                                                                                                                                                                                                   tures   finally usage shapley value feature dom forests breiman  equivalent       selection justiï¬ed axiomatic qualities bounded estimated contribution value                                                                                                                                                                                                                                                             Â¢      axiom  normalization pareto optimality game                                                                                                                                                                                          Â¨Â¢Â¤ Â¨Â¢ Â©Â©                                                                      Â¨Â¢g Â©                            Â§                                                                                 Â¡                                     Â¢eÂ¥g Â©                                Â¢g Â©m  Â¢eÂ¦Â©            holds                                                                                                                                                                              context feature selection axiom implies  set sampled permutations subsets    performance dataset divided fully size   usage bounded sets coupled method  different features                                   shapley value estimation yields efï¬cient ro                                                        bust way estimate contribution feature task      axiom  permutation invariance symmetry  classiï¬cation detailed discussion msa frame                                                Â§                                                            Â¢eÂ¥g Â©                                                                Â©               permutation      holds  Â¢g      work theoretical background keinan et al       Â§                  Â¢ Â©                                                                    contributionselection algorithm  axiom implies value altered arbitrarily  renaming reordering features                  contributionselection algorithm csa described                                                        figure  iterative nature adopt      axiom  preservation carrier dummyproperty                                                                                           forward selection backward elimination approach                            Â¢Â¤Â£  Â©m  Â¢Â¤mÂ©          game Â¢efÂ¥ghÂ©                             Â§                                        ing subroutine contribution ranks each feature accord                                       Â¢ghÂ©m       holds                                       ing contribution value selects  features                                                        highest contribution values forward selection using  axiom implies dummy feature does inï¬‚u                   ence classiï¬erâ€™s performance receives contribu subroutine selection  eliminates  features  tion value                                          lowest contribution values backward elimination using                                                        elimination repeats phases calculating contri      axiom  additivity aggregation games bution values remaining features given                                Â§           Â§      Â§          Â©    Â¢eÂ¥ Â©          Â¢gÂ¡  Â©   Â¢g Â© Â¨Â¢ Â©      Â¢eÂ¥g     holds                         selected eliminated selecting eliminating new             Â©jÂ¢Â¤mÂ©  Â¢Â¤mÂ© Â¢lmÂ©  Â¢g                                             features contribution values candidate features    axiom applies combination different payoffs alternatively selection subroutine use forward selec      based set features classiï¬cation task tion technique instead   features added ascending order  example accuracy area roc contribution values long classiï¬erâ€™s performance  curve false positive rate false negative rate improves                                                                                            Â¢                                        Â¥  Â¥   contributionselectionalgorithm Â                           classes  features train size test size                                                             reuters                                        Â¢Â¡Â¢Â£Â¥Â¤   Â¦                                                                        reuters                                              Â§Â Â©Â¨  Â¢Â¡Â¢Â£Â¥Â¤          each                                        arrhythmia                                                             Â¢Â¡Â¢Â£Â¥Â¤               contribution                       internet ads                                                Â¢                                      dexter                                                                                                                                                                            Â¢          arcene                                             Â¢Â¡Â¢Â£Â¥Â¤    Â¢Â¡Â¢Â£Â¥Â¤  Â¡Â£                           selection                                                     goto                                                  table  description datasets used                    return selected                              higher  likely features redundant                                                                                                                                                                                              figure  contributionselection algorithm forward se tribution selected  minimizes                                                              redundancy dependencies features increasing  accel                                            lection version  input set features contribution                                                       erates algorithmâ€™s convergence algorithmâ€™s halting      value threshold maximal permutation size calculating       Â¢      contribution values  number features selected each criterion depends  designates tradeoff  phase contribution routine calculates contribution value number selected features performance        feature  pay function described section classiï¬er validation set forward selection                                                                           Â¢                                                                                                             selection routine selects features highest contribu version choosing   means csa selects features    tion values exceed   backward elimination version long exists feature likely improve clas      selection subroutine replaced elimination subroutine siï¬erâ€™s performance selects smaller sets features Â¢                                                                            Â¢  eliminates  features each phase halting criterion increased increasing opposite effect size  changed accordingly                                                        ï¬nal set features intuitive halting crite                                                        rion stop performance gain achieved  exceed contribution threshold Â¢ forward selection restrictive csaâ€™s halting criterion enables se  fall contribution threshold Â¢ backward elimina lection features proved useful later stages veriï¬ed  tion                                                empirically datasets    algorithm speciï¬cation contri  bution subroutine generalization ï¬lter methods  results  main idea algorithm contribution  data benchmark algorithms  subroutine unlike common ï¬lter methods returns contri  bution value each feature according assistance im test csa empirically ran number experiments  proving classiï¬erâ€™s performance generated using seven realworld datasets number features rang  speciï¬c induction algorithm conjunction ing   table  reuters dataset  features using notation section  assuming reuters dataset constructed following koller  optimizes accuracy level classiï¬er contribution sahami  using reuters document collection  subroutine forward selection calculates contribution arrhythmia database uci repository perkins                                                            et al   internet advertisements database                                           Â¢Â¤ Â©  values using following payoff function                                                             uci repository blake merz  collected               Â£  Â¢Â¡Â¢Â£Â¥Â¤                                                         research identifying advertisements web pages                                                          Â¢ Â©    generate classiï¬er  training set train dexter text categorization dataset arcene cancer                                                            dataset nips  workshop feature selec                                   Â¢Â©    evaluate     examples validation set                                                        tion guyon  microarray colon cancer      validation                                                            dataset alon et al                                                             return accuracy level deï¬ned  gÂ¢lmÂ©                                                                                        principle csa work induction algorithm                                                                                                                                                                                                             Â¥                  Â¥                                        computational constraints focused                                                                                                    fast induction algorithms algorithms ef            Â¦    case   end case handled return ï¬ciently combined csa experimented naive  ing number instances largest class divided bayes nn each datasets measured  total number instances classiï¬er selects training set accuracy each classiï¬er using tenfold cross  frequent class backward elimination quite sim validation set features each dataset sub  ilar payoff calculated sampling permutations sequent work used induction algorithm   gave  set features left each phase elimination highest cross validation accuracy detailed table     maximal permutation size  important role different feature selection schemes com  ciding contribution values different features pared datasets described          selected way ensures different com                                                                                binations features interact inspected induction algorithm performing feature      impact demonstrated section                       selection serve baseline                                                                                                                                                                                                                                                                                                                                                                         number selected features  selection sub regularized linear svm using       package  routine controls redundancies selected features joachims  datasets                                                                                                                                                                      Â¤   dataset           fwd   bwd                      reuters dataset feature selection using ran   reuters     nb                                dom  forests did best yielding accuracy level    reuters     nb                                 features far csa   arrhythmia                                  backward elimination version   features   internet ads nn                               koller sahami  example report   dexter                                    markov blanket algorithm yields approximately  se   arcene                                  lected features accuracy levels                                            dataset                                                            table  parameters classiï¬er used csa al reuters dataset csa backward elimination      gorithm each dataset Â  induction algorithm used did best yielding accuracy level   fea      csa nb naive bayes  number features selected tures comparison koller sahami       forward selection each phase  number features elim port markov blanket algorithm yields approx    inated backward elimination each phase  permutation imately  selected features accuracy levels    size Â¡ number permutations sampled estimate   dataset  contribution values explanation hyperparameters  arrhythmia dataset dataset considered  chosen text                                         difï¬cult csa backward elimination did                                                                best yielding accuracy level   features                                                                                                        Â¨Â§                                                            forward selection higher depth value    did      classes split binary classiï¬cation prob                                                            better wrapper implying consider      lems                                                            features concomitantly perform good feature se         filtering using mutual information classiï¬cation lection dataset comparison grafting al               ing   binned continuous domains estimate   gorithm perkins et al  yields accuracy level          mutual information                                   approximately  dataset                                                               filtering using pearson correlation coefï¬cient internet ads dataset algorithms did approx      classiï¬cation using                                 imately leading accuracy levels                                                               csa slightly outperforming oth     random  forests feature selection breiman                                                                ers interestingly wrapper algorithm did      classiï¬cation using                                  select feature ï¬rst phase nn algorithm       feature selection using forward selection wrapper neighbors classes distance      simple wrapper greedily selects feature   each feature checked leading arbitrary selection      improves classiï¬erâ€™s validation performance classes classiï¬erâ€™s performance                                                               constant through phase yielding zero contri                                                      equivalent forward selection csa                                                                                        bution values selecting higher depth      classiï¬cation using performing feature selection levels simple nn algorithm boosted          forward selection csa parameters described perform classiï¬ers svm                                                                                               Â¤      table  parameters  chosen dexter dataset dexter dataset used      expected number times each feature sam algorithm  decision trees process      pled higher  contribution value threshold                                   Â¢                              feature selection linear svm perform                                        stopping selection    termination fea    actual prediction features selected      ture selection ï¬xed choosing contribution value                    Â¢                                             did satisfying accuracy lev                       threshold      hyperparameter selection                                Â¢                                 els feature selection algorithms                      Â¤          formed                                impractical use svm csa large datasets                            classiï¬cation using performing feature selection overcome difference classiï¬ers          backward elimination csa parameters   forming feature selection classiï¬er used                                         Â¤      scribed table  parameters  chosen  actual classiï¬cation added optimization phase      expected number times each fea forward selection algorithm stopped          ture sampled higher  contribution value phase tenfold crossvalidation performed                                           Â¢                                                      threshold stopping elimination    hy     dataset similar way used opti                                                 Â¤      perparameter selection performed   mize ï¬lter methods simple mutual information fea      Â¢                                                     ture selection performed best followed closely                                                            contributionselection algorithm backward elimi    avoid overï¬tting validation set used calcu nation version random forests implies      lating payoff csa used mfold cross validation dexter contribution single features signiï¬cantly                     Â£Â¢ Â¡ Â¤Â¢Â¤ Â¦Â¥  instead single      set                            outweigh contribution feature combinations    feature selection classiï¬cation results          task classiï¬cation forward selection algo                                                            rithm did linear svm feature selec  table  summarizes classiï¬ersâ€™ performance test tion signiï¬cantly lower number features  set number features selected each ex  arcene dataset just case dexter  periments accuracy levels fraction correctly use process feature selection lin  classiï¬ed test set instances                             ear svm perform actual prediction features                                                         dataset      wrapper      fwd        bwd                                                                                                         arrhythmia slpâˆ’   reuters                                                     dexter slpâˆ’   reuters                           âˆ’   arrhythmia                 internet ads                              âˆ’   dexter                   arcene                                 âˆ’                                                                             âˆ’     fs   svm       corr        mi         rf               log  frequency                               âˆ’                                                         âˆ’                                                                                 âˆ’                             âˆ’   âˆ’   âˆ’    âˆ’   âˆ’    âˆ’   âˆ’                                                    log cv                                                                        figure  powerlaw distribution contribution values log  table  comparison accuracy levels number features se log plot distribution contribution values absolute value  lected different datasets upper table wrapper fwdbwd ï¬rst phase arrhythmia dexter prior making  csa forward selectionbackward elimination parameters feature selection demonstrates power law behavior corre  table  table fs feature selection svm sponding plots datasets show identical powerlaw char  linear svm feature selection corr feature selection using acteristics different slopes eliminated  pearson correlation mi feature selection using mutual informa sake clarity  tion rf feature selection using random forests accuracy levels  calculated counting number misclassiï¬ed instances  given percentages number features selected given backward elimination gradual  brackets                                             stable increase contribution values eliminated                                                        features peaks graph contribution values      selected csa backward elimination obtained figure demonstrate contribution values change      better performance rest algorithms csa iterates  case selection single fea     dataset csa backward elimination ture considerably increased contribution value      feature selection using mutual information yielded feature pointing intricate dependencies features      best results poor performance csa forward figures   assist explaining      selection explained poverty data com ward elimination usually outperforms feature selec      paring number features algorithm selected tion methods including forward selection high      ï¬rst phases features explain train dimensionality datasets feature assists pre      ing data coincidence avoided selecting fea diction merely coincidence selected ac      tures truly contribution task classiï¬cation count truly informative features forward selection      phenomenon explained portrait section  penalized severely case signiï¬                                                        features chosen backward    summary   datasets csa backward                                                        elimination maintains signiï¬cant features  elimination achieved best results cases csa                                                        non eliminated set feature truly enhances classi  achieved second best result                                                        ï¬erâ€™s generalization validation set    closer inspection results               eliminated leads stable gen                                                        eralization behavior backward elimination test set  msa intent capturing correctly contribution                                                        through algorithmâ€™s progress figure   elements task enables examine distribution  contribution values features figure  depicts  loglog plot distribution contribution values  final notes  ï¬rst phase arrhythmia dexter prior making contributionselection algorithm presented paper  feature selection distribution follows scalefree views task feature selection context coali  power law implying large contribution values abso tional games uses wrapperlike technique combined  lute value rare small ones quite common novel ranking method based shapley  justifying quantitatively need feature selection tribution values features classiï¬cation accuracy  datasets observed possess similar power csa works iterative manner each time selecting  law characteristic                                   new features eliminating taking account    behavior algorithm through process fea features selected eliminated far  ture selectionelimination displayed figure  csa similarly wrapper algorithms restricted  forward selection algorithm identiï¬es signiï¬cant features selection induction algorithm used evaluating fea  ï¬rst phases sharp decrease contri tures sets time limitations problem  bution values features selected following phases reduced parallelizing advantage shared                                                        
