 real boosting      la carte   application boosting oblique decision trees              claudia henry             richard nock                   frank nielsen               antillesguyane  ceregmia                       sony   cs labs              schoelcher martinique france                tokyo  japan            chenryrnockmartiniqueunivagfr                 nielsencslsonycojp                          abstract                          simple assumption corresponding boosting algorithm                                                        using recent result nock nielsen      past years boosting major algorithm uses realvalued weak hypotheses does      Ô¨Åeld machine learning classiÔ¨Åcation  face repetitive minimization exponential loss faced      paper brings contributions theory algo previous real boosting algorithms friedman et al       rithms Ô¨Årst unify wellknown topdown   schapire singer  classiÔ¨Åer oblique                                            cision tree induction algorithm kearns decision tree algorithm obtained three key features                                               mansour   discrete adaboost freund      fast simple Ô¨Årst provable boosting algorithm                             schapire   versions    fully exploits class important prob      higherlevel boosting algorithm used   lem inducing oblique decision trees longstanding      basic building block devise simple prov tory algorithms available far time      able boosting algorithms complex classiÔ¨Åers  suming complex handle formally breiman et al       provide example Ô¨Årst boosting al     cant¬¥upaz kamath  heath et al  murthy et      gorithm oblique decision trees algorithm  al  shawetaylor cristianini  follow      turns simpler faster signiÔ¨Å ing section presents deÔ¨Ånitions followed section      cantly accurate previous approaches    theoretical results section discusses                                                        gives boosting algorithm oblique decision trees    introduction                                       sections discuss experiments conclude sake  loosely speaking boosting algorithm repeatedly trains clarity proofs postponed appendix  moderately accurate learners gets weak hy  potheses combines output strong classiÔ¨Åer  deÔ¨Ånitions problem statement  boosts accuracy arbitrary high levels kearns boldfaced variables represent vectors  valiant  pioneering paper schapire  unless stated sets represented calligraphic  proved existence boosting algorithms uppercase alphabets  unless explicitely stated  drew roots popular representative ad enumerated following lowercase xi    aboost builds linear combination predictions    vector sets xi     sets  weak hypotheses freund schapire  consider following supervised learning setting let  paper kearns mansour  proved denote domain observations dimension nsuchasrn  popular topdown decision tree induction algorithms   suppose set  examples                                    boosting algorithms disguise breiman et al   cardinal notation deÔ¨Åned  si  xiyi‚àà  quinlan  kinds algorithms outwardly x√ó‚àí      discrete dis  different each adaboost repeatedly modi tribution known ‚Äú‚Äù called positive class  Ô¨Åes weights training examples directly minimizes ‚Äú‚Äù negative class primary objective related  aconvexexponential loss topdown decision tree induction building accurate classiÔ¨Åer hypothesis  x‚Üír  algorithms modify weights examples goodness Ô¨Åt evaluated quan  minimize expectation concave socalled permissible tities  function kearns mansour                                                                           explains starting point paper quite Œµh  ew      signhxy    surprising result prove kinds algo                        ‚àí                                                                    Œµexph ew      exp yh         rithms fact algorithm induction                                                                             ‚â•      ‚àí  formed different classiÔ¨Åer ‚Äúgraphs‚Äù apparent signaiff      ceptual differences algorithms stem different indicator variable expectation   structural properties classiÔ¨Åers suggests generic ventional empirical risk  upperbound expo  induction scheme gives classiÔ¨Åers meet nential loss schapire singer  interested                                                    ijcai                                                        input                           input                  input                                                     ht ‚Üê trees wt                       let  ‚àà leavesht‚àí containing let  ‚àà leavesht‚àí containing          ‚Üê                    ‚àí            Œ±t    arg minŒ±‚ààr ewt exp yŒ±ht          examples classes       examples classes                                ht ‚Üê  stumps            ht ‚Üê stumplss            wti ‚Üê wti exp‚àíŒ±tyihtxizt    ‚Üê  ht                         ‚Üê ht     adaboost  trees                        topdown  dt cart       topdown  odt oc sadt    figure  abstraction  core greedy procedures popular induction algorithms text references details    stagewise greedy Ô¨Åtting scratch large figure  gives abstract view pop  number classes classiÔ¨Åers popular induction algo ular induction algorithms core procedure  rithms core procedure follows scheme including induces large classiÔ¨Åer adaboost trees freund  decision trees dt breiman et al  quinlan  schapire  schapire singer cquin  branching programs bp mansour mcallester  lan cartbreiman et al ocmurthy et  linear separators ls freund schapire  al  sadt heath et al  Ô¨Årst algorithm  nock nielsen  virtually procedures share induction odt dates mideighties  commonpoint components used grow descendant cart breiman et al   classiÔ¨Åers dts adaboost duction algorithms dt odt integrate pruning stage  trees internal node splits dts bps akin interested greedy induction scheme  decision stumps                                wl various forms induces decision tree    given Ô¨Åxed class classiÔ¨Åers Ô¨Åt sample distribution repeatedly modiÔ¨Åed  problem statement weak learner wl adaboost induces decision tree single internal  assumed available queried sam node stump subset reaches leaf  noted                                       ple discrete distribution  assump s distribution modiÔ¨Åed cart  tion existence called weak learning assump replaces ordinary axisparallel stump linear  tion wla returns polynomial time classiÔ¨Åer separator stump called stump sake simplicity  following assumed Œµh w ‚â§ oc sadt choice split dt odt   ‚àí Œ≥ Œ≥     schapire singer  lies repetitive minimization ‚Äúimpurity‚Äù criterion  given   possible build polynomial   expectation leaves current tree  time Œµh ‚â§   having queried socalled permissible function kearns mansour  times wl classiÔ¨Åers hh  ht forsomet     provided additional assumptions generaliza                                                                                                                                  tion abilities section ‚Äúdiscussion‚Äù algorithm                            s                                                           Œµimph  fe‚àºleavesh                   called boosting algorithm freund schapire                                   ws  kearns valiant                                                                                                                                                                                                                                               ws total weight      unifying boosting properties                       weight restricted positive class expec  sake clarity plug subscript tation weight leaf  ws  permissible func                                                      tion   ‚Üí   concave symmetric  h element ls classiÔ¨Åer ht ht                     ‚àà                                     ex    Œ±tht whereŒ±t    leveraging coefÔ¨Åcient amples permissible functions fzz ‚àí  interpreted conÔ¨Ådence ht element dt gini index breiman et al  fz‚àíz log ‚àí  ‚àí  rooted directed tree each internal node supports log ‚àí entropy quinlan  log base  single boolean test observation variable each                                                                                 ‚àí                           leaf labeled real classiÔ¨Åcation obser fz optimal choice kearns                                                        mansour  remark Œµimph wf ‚â•  vation proceeds following root path             ‚àí             tests satisÔ¨Åes reaches leaf gives class Œµimph  min Œµh                                                         missible  minimizing  minimizing  figure  left presents example dt                                                               empirical risk reason  vv boolean description variables finally oblique  cision trees odt generalizes dt linear combinations better choice focusing directly empirical risk                                                                                                      variables authorized internal nodes allows comes concavity permissible function kearns                                                                          splits oblique instead just axisparallel breiman et mansour   seemingly different  al                                            each adaboost dt induction algorithms                                                        cartfamily independently proven boosting al    sake simplicity polynomial time means polynomial gorithms freund schapire  kearns mansour  relevant parameters paper       schapire singer  sofarnosuchformal                                                    ijcai                                                                                                                                                      input                                                                                                                                                                                                                       compute                                                                                      ht ‚Üê wlst  wt                                                                 m          ‚àí                                                           ‚àí                                                                                       Œºtyiht iht  ‚ààs                                                                                    ‚àí       si                                                                    ‚Üêw    √ó        Œºt                                            Œ±h‚àí                        ti   ti                     ‚ààss                          Œ±h Œ±h                                     si                              Œ±h Œ±h ‚àí                                                                       ‚Üê                      ‚àí                                                              Œ±t    ht  ln  Œºt Œºt  figure  dt  leaves  internal nodes left genericgreedy  equivalent representation Ô¨Åts right                                                          figure  generic greedy induction ht  compu  boosting algorithm proof exist odt tation st depends decision graph ht  advantage structure combining oblique stumps    decision trees objective let shift                                                                                            we extend weight notation let wt sp   general standpoint problem address following                                                                                            st            ‚ààpand  question kind classiÔ¨Åers used solve  si‚ààsp       si‚ààst                                                           ‚â§   ‚â§                                    ‚àà  consider following assumption represents     deÔ¨Åne ht maxsi‚ààst  ht                                    ht                                                   kind local linear classiÔ¨Åer                          maximal absolute value ht tafternock                                                        nielsen  deÔ¨Åne notions marginstheÔ¨Årstis      assumption ‚àÄt denote ht  hh  ht                                                         normalized margin ht      set outputs wl exists function                                                                                            maps observation nonempty subset    Œº                    ‚àà ‚àí                 x‚Üí     ht ‚àÖ  ‚àÄx ‚ààx                                        ti         ght                      ght                wtst                                                                          si‚ààst      quired computable polynomially size      ht  describes classiÔ¨Åcation function ht help deÔ¨Ånition figure  presents generic      following way                                greedy induction ht  remark ht linear                                                       separator adaboostr boosting algorithm nock          ht              Œ±thtx  ‚àÄx ‚ààx         nielsen  second margin deÔ¨Ånition mar                        ‚àà                                                x                        ht ght                          gin ht example  nock nielsen                                                                ŒΩt xy    tanhyht ‚àà ‚àí     means words classiÔ¨Åcation ht obtained                                      summing outputs elements  classi margin comes mind relationships  Ô¨Åers satisfy decisition trees decision lists linear boosting logistic prediction ht  logpry   separators                                      xpry  ‚àíx friedman et al  case                                                                       x                    ‚àí  deÔ¨Ånition  decision graph ht isanoriented  ŒΩt  ypry         pry                                                        ‚àí   graph ht   eanarchtht  ‚ààeiff tt      expectation normalized margin             ‚ààx                 ‚àà                                         ‚àí  exists      htht  ght   ht   bayesian prediction    called gentle lo                                                                                           tt   ght                            gistic approximation friedman et al   following                                                        nock nielsen wedeÔ¨Ånethemargin error ht  remark acyclic maps each observation                                                      ‚àÄŒ∏ ‚àà ‚àí   path gwealsodeÔ¨Ånep   representing                                  set paths mapped ght                    ŒΩwht Œ∏    ew ŒΩt s‚â§Œ∏                    ‚äÜh     ‚àÉx ‚ààx                                            ‚â§                           ght             Œµht       ŒΩwht  minimizing                                                                                                                                                    ŒΩwht  minimize Œµht     simplest case obtained                                                      following theorem shows weak  ht  ‚àÄx ‚ààx ht  linear separator asinglepath                                                        conditions show ŒΩw minimized   examples classes classiÔ¨Åers different                          Œ∏                                                     values Œ∏ ‚àà ‚àí  Œ∏   linear separators Ô¨Åt include dt odt  case tree weak hypotheses fact theorem  ‚â•  queries wl classiÔ¨Åer ob                                                        tained  satisÔ¨Åes  stant predictions tree nodes figure  right displays            way represent dt meet remark                                                                                                         ‚â§  Œ∏    √ó                  ‚àí     exists possible equivalent transformations ŒΩw                sp         Œº                                                                   Œ∏    ‚àí Œ∏                     dt way induce ht shall favor single                   p‚ààp        ht‚ààp  follows deÔ¨Åne types subsets                                                        Œ∏ ‚àà ‚àí   ‚ààpand   ‚â§ ‚â§                                                         proof appendix read theorem  consider follow                x   ‚ààs                               iyi      ght          ing wla realvalued weak hypotheses borrowed                ‚à™                      p‚ààpht‚ààp                       nock nielsen                                                     ijcai                                                                 ‚â•                  ‚àÄ ‚â•                                   ‚àí                            wla  Œºt   Œ≥forsomeŒ≥                    arg minr ew exp yht  figure  matches                                                        exactly discrete adaboost freund schapire   wla theorem     states ŒΩw    ‚â§                                            Œ∏         decision trees linear separators ex   exp‚àí min ‚ààp pŒ≥ constant Œ∏    Œ∏                          Œ∏                      tremal classiÔ¨Åers respect finally  constant ‚àà ‚àí  words provided                                                                                                 linear separator restriction weak hypotheses  duction performed paths roughly                                                           specializes adaboostr nock nielsen   equal size breadthÔ¨Årst induction decision  graph margin error guaranteed decrease exponen  boosting algorithms  tially fast Ô¨Åts treeshaped decision graphs  consider following assumption                    original boosting setting examples drawn                             ‚ààh                         dependently according unknown Ô¨Åxed distribu      assumption each ht constant ii tion  goal minimize true risk      rooted tree                                                        Œµht high probability basically wish  assumption basically restricts ht tree arity Œµht ‚â§  probability ‚â•  ‚àí Œ¥ sampling  kind classiÔ¨Åer used split freund schapire  kearns valiant                                       ‚àí  internal nodes  use notation index‚Äô sufÔ¨Åcient conditions polynomial time induction  weight class ‚Äú‚Äù ‚Äú‚Äù respectively          algorithm satisfy constraint return ht                                                        Œµh    ii prove structural parameters  theorem  suppose hold                                                                      class classiÔ¨Åers ht belongs satisfy particular                                                       bounds freund schapire  shawetaylor cris                               wst                ht       ln ‚àí                 tianini  theorem  prove holds                                                                                                                              st                   fairly general conditions algorithm figure                                                         provided wla holds example Œ≥log  ‚àÄx ‚ààxand     leaf furthermore                                                                                                         Œ≥  simpliÔ¨Åes                                         erations ls  dt                                                                ‚â§                                                      Œµht      theorem  fixing mini wi easily                                                        yields bound ls adaboost dis                                                               √ó     st   ‚àí   st       crete real schapire singer  dt   Œµexpht        wst                                                    wst      wst                                                          ht leaf                             improves exponent constant kearns man                                                       sour  finally ii immediate follows                                   ‚àí                  Œµimpht             mild assumptions wl simple matter fact                                                        ii hold inducing odt  proof appendix                                                          recursive boosting oblique decision    discussion                                              trees    kearns mansour‚Äôs algorithm adaboost         preceeding subsections suggest used                                                              build ht  core procedure wl  similarity   fz ‚àí example comes theorem  adaboost  trees                                                                                                 immediate quite surprising shows identity pruning figure  equivalent growing lin                                                                                       tween convex loss expectation concave loss ear separators wl growing decision tree  coincidence theorem  shows wl returns constant weak hypothesis  surprising result choice weak hypotheses general case obtain recursivecomposite design                                                        ‚Äúmaster‚Äù gviag recursion  does impact ht  aandb                                                        end reach wl afford exhaustive search  hold way modify ht through deci  sion graph choice splits tree simple classiÔ¨Åers axisparallel stumps constant classi                                                                                                       simple way choose Ô¨Åers instead calling howeverg  thing popular ls boosting algorithms friedman used build decision graph ht inthesame  et al  schapire singer  repeatedly mini recursive fashion consider example class odt  mize exponential loss  theorem  internal nodes‚Äô splits local classiÔ¨Åers ls decide  amounts minimization impurity criterion  path based sign output equivalently                     ‚àí                                  class suggest build tree  fz      zthisisexactly dt induc                                                              linear separators internal nodes ht  tion algorithm proposed kearns mansour   meets representation optimal bound               use   split leaf  linear separator uses ordi                                                        nary decision stumps tree shape    hand  ht  linear separator  inÔ¨Çuence decision graph induc boostodt odt induction algorithm turns  tion  single path    brings boosting algorithm takes advantage                                                   odt structure time assume  way modify ht  through choice  weak hypotheses suppose each weak hypothe  wla  level deeper stumps linear  sis output restricted set classes ‚àí  separators splits oblique decision tree  case Œ±t  ln ‚àí Œµht wtŒµht wt  theorem  boostodt boosting algorithm                                                    ijcai                                                                                             xd              xd             xd         domain               boostodt       oc   adaboost                                                                                                                                     adultstrech                                                                         breastcancer                                                                    breastcancerw                                                                        bupa                                                       colic                                                                             colicorig                                                                             credita                        figure  margin distributions boostodt creditg                        domain xd  left  center  class diabetes                       noise right stairshaped bold plain curves theoret hepatitis             ical margins logistic model text        ionosphere                                                                                 krvskp                                                                                   labor                           proof omitted lack space builds ledeven                   orem  plus lemma  mansour mcallester  ledeven                  structural arguments freund schapire  monks                           shawetaylor cristianini  proof emphasizes monks                          relative importance sizes suppose each linear sep monks                                                                            mushroom                            arator contains number weak hypotheses  tree complete internal nodes having parity                                                                          sick                                                                          log    Œ©Œ≥  log             sonar                                                                                  vot                               Œµht  ‚â§  experimental xd                               standpoint suggests build trees  yellowsmall                                                                               wins                         experiments                                         wins                      performed comparisons testbed  domains  classes uci table  results  domains each domain bold  repository ml databases blake et al  comparisons faces indicate lowest errors each row ‚Äúwins  performed tenfold stratiÔ¨Åed crossvalidation  z‚Äù bold faces denote number times  oc adaboost weak learner corresponding algorithm column best three                                 ‚àà         pruned boostodt ran     weak   columns boostodtt    oc adaboostc  hypotheses linear separators decision stumps ‚àà  furthermore numbers parenthe   make fair comparisons ran adaboost  ses each row number signiÔ¨Åcant wins student  total number boosting iterations brings paired ttest   boostodt vs oc boostodt vs  fair comparisons observation classiÔ¨Åed  nodes adaboostc oc vs boostodt adaboostc vs  including leaves boostodt  unpruned trees ad boostodt left right  aboost looking results boostodt proven  faster oc practice times                                                         margin error curves domain xd variable class  faster oc‚Äôs time complexity onm log murthy                                                                                           et al  guarantee result boos noise nock nielsen  description                                                        domain averaged test folds nock nielsen  todt‚Äôs onm wla guarantee                                                              reach empirical consistency case complexity reduc  themargin curve obtained compared                                                                                               tions order magnitude respect logistic prediction friedman et al    svm based induction odt shawetaylor cristianini computed exactly approximation logistic model   table  summarizes results obtained rejec boostodt quite remarkable margin curves  tion probabilities ranging  display single stairshape theoretical logistic model                                                        domain xd  additional class noise uni   hypothesis boostodt does perform  better sign tests comparing runs boostodt formly distributed odt leaves  opponents display better performances  conÔ¨Årmed student paired ttests conclusion  tell simulated domains boostodt performs main contribution paper show  better domain gets harder case formal boosting reach using uniÔ¨Åed algo  monks domains ledeven domains boostodt   rithm wide variety formalisms restricted  beaten opponents ledeven beats popular included paper decision lists  ledeven ledeven  irrelevant variables rivest  simple rules nock      looking simulated domains drilled contribution quite surprising show boosting al  results boostodt using  plotted figure gorithm follows immediately complex combinations                                                    ijcai                                                     
