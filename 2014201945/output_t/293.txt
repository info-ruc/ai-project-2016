             learning discontinuities switching local models                  âˆ—                                   marc toussaint    sethu vijayakumar                                institute perception action behavior                               school informatics university edinburgh                      kingâ€™s buildings mayï¬eld road edinburgh eh jz uk                            mtoussaiinfedacuk sethuvijayakumaredacuk      introduction                                       model problem predicting par                                                        ticular model Ï† responsible given input solved  locally weighted learning techniques particular lwpr                                                                higher level explained section formal  vijayakumar et al   successfully used notations assume mixture model  highdimensional regression problems robustness                                                                             efï¬cient online versions crucial robotic domains                                 instance inverse model articulated dynamic robot yx   âˆ’  plocix yi     learned realtime models map high                   dimensional state joint angles velocities                       âˆ’ Ï†                                                                   yi  âˆš   exp  âˆ’              sired change state required motor signals torques              Ï€ Ïƒ            Ïƒ    typically mappings assumed smooth                                                              uy  real world scenarios interesting cases  uniform distribution accounting                                                                                    functions truly discontinuous ground noise outliers hidden variable spec  examples include contacts objects particular ifying particular model generates datum  ground parts body â€œjoint limitsâ€ aim localized models impose locality constraint al                                                                                      fact interesting interactions environment man ready level follows let denote mean input                                                                             Ï†  ifest through discontinuities sensorimotor center model trained given                                                                 data                                                 input  th model eligible does                                                        exist jth model center â€œbetweenâ€    paper show discontinuous switching                                                                                                 precisely  tween local regression models learned general  topic switching models discussed plocix   â‡â‡’ âˆƒj  hx âˆ’ cj ci âˆ’ cji     context state space models ghahramani hin hÂ· Â·i scalar product input space  ton  pavlovic et al  multiple inverse mod        eligible eligible    els wolpert kawato  generally question             cj          cj  particular model receives responsibilities given                               input modeled hidden variable generative              mixture model case assume responsibil       ci              ci  ity index predicted input robot state plocix uniform eligible iâ€™s given  inferring model corresponds classifying current family models infer posterior  input domain regions each submodel         responsibility index given datum using bayes    robotic domains local learning crucial pre rule                                                                                vent interference allow online adaptation techniques    iy  yi plocix  propose model responsibility index               self composition local classiï¬ers multiple pairwise clas calculating map assignment Ë†i allows associate ev  siï¬ers concatenated construct complete model ery training datum likely model using  form productofsigmoids capable learning sufï¬cient statistics each local model Ï†i updated fur  complex sharply bounded domains each local model ther data labeled â€œyet unmodeledâ€  lieu typical gaussian kernels                 inferred generated uy used generate                                                        new family member following heuristic compare    learning family models                        ransac random datum  selected                                                        modeled data closest neighbors wrt eu                            given training data xk ykk inputs xk outputs clidean input distance chosen initial training data  yk ï¬rst level goal algorithm learn family new model random poisson number  models Ï†  Ï†n  datum explained mean  input dimensionality finally mod                                                        els receive map responsibilities     âˆ—the ï¬rst author acknowledges support german research experiments discarded iterative process  foundation dfg emmy noether fellowship  repeated new models generatedfigure  kernels represented product sig  moids      general scheme family learning realized  type models Ï†i experiments  choose Ï†i linear functions learned partial  squares pls regression pls involving intermediate  lowerdimensional projection proven efï¬cient  highdimensional problems vijayakumar et al     products sigmoids switching  second level algorithm goal learn pre  dictive model ix latent responsibility index figure  test function Ïƒ  precise uninformed prior plocix given learned switching model  iterations  data easy decide models â€œpo training data points blended switching model                                                                tentially neighboredâ€â€”namely exists data yx  Î²ix Ï†ix compared lwpr family er  models eligibleâ€”based centers ror cf sec  classiï¬cation error  runs ran  each pair ij neighbored models learn sigmoidal dom test functions Ïƒ   function Ïˆijx Ïˆij â‰¡  âˆ’ Ïˆji product bold line average curves  sigmoids submodel deï¬nes coefï¬cient Î²ix  associate submodel given input                                                           discussion                                          Î²ix        Ïˆijx  Ïˆijx                      presented model addresses problem handling                                  expâˆ’Ï†                                            ij         discontinuities naturally arise sensorimotor data                                                        during interaction structured environment model  normalizes Î² indicated represent                                                      extends earlier local learning approaches ways  sigmoids Ïˆ scalar function Ï†  fig  illustrates           ij                    ij                     responsibility region associated each local model  kind kernels represented products sigmoids                                                        learned product sigmoids ver    sigmoids Ïˆ meant represent likelihood                 ij                                     satile boundary shape compared typical gaussian kernels  model responsible input                                                        problems associated initialization kernel shapes  conditioned responsible product                                                        widths heuristic choice ad hoc number sub  combination comparable voting map la       Ë†                                                models circumvented robust incremental allocation  beling introduced previous section used new models consistently used pls  train sigmoids experiments consider Ï†ij underlying regression machinery general model allows  linear functions learned pls          utilize efï¬cient single model learner represent                                                        local models Ï†i classiï¬er functions Ï†ij future    experiments                                        work particular investigate nonlinear learners  tested algorithm piecewise linear discontinuous local models boundary classiï¬ers  test functions test function  parameters  dimension number linear pieces com references                            Ïƒ  posed output noise  localities slopes ghahramani hinton  ghahramani ge  boundaries linear pieces sampled randomly fig hinton variational learning switching statespace  ab display learning results example com models neural computation â€“   parison lwpr fig cd display error curves  dimensional test functions large input pavlovic et al  vladimir pavlovic james rehg  main âˆ’  family error mse best  john maccormick learning switching linear mod                                                           els human motion nips pages â€“   ï¬tting eligible model Ï†Ë†i averaged independent test  data set classiï¬cation error counts prod vijayakumar et al  sethu vijayakumar aaron  uct sigmoids correctly predicts Ï†Ë†i best ï¬tting dâ€™souza tomohiro shibata jorg conradt ste                                       Ë†  model given input argmaxiÎ²i  exper fan schaal statistical learning humanoid robots  iments ï¬nd algorithm reliably generates fam autonomous robot â€“                                              ily optimal family error noise level Ïƒ   wolpert kawato  dm wolpert kawato   dimensions displayed classiï¬cation error multiple paired forward inverse models motor  rapidly converges zero  dimensions classi trol neural networks â€“   ï¬cation error converges  results  homepagesinfedacukmtoussaiprojectsijcai
