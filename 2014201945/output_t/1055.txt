                        learning    payoff   functions     inï¬nite   games                    yevgeniy  vorobeychik   michael  wellman    satinder  singh                                           university michigan                                      artiï¬cial intelligence laboratory                                               beal avenue                                      ann arbor  mi    usa                                 yvorobey  wellman  baveja    umichedu                                                                                     abstract                          identify game restrictive game                                                        limited data entailing generalization observed       consider class games realvalued    stances approximating payoff functions using supervised      strategies payoff information available learning regression methods allows deal contin      form data given sample strategy  uous agent strategy sets providing payoff arbitrary      proï¬les solving games respect strategy proï¬le doing adopt functional forms      derlying strategy space requires generalizing sistent prior knowledge game ad      data complete payofffunction representa mit biases forms facilitating subsequent game analysis      tion address payofffunction learning stan equilibrium calculation      dard regression problem provision captur paper present ï¬rst investigation approxi      ing known structure symmetry multiagent  mating payoff functions employing regression lowdegree      environment measure learning performance     polynomials explore example games      consider relative utility prescribed strate complete information realvalued actions      gies accuracy payoff functions standard ï¬rstprice sealed bid auction players      se demonstrate approach evalu    symmetric value distributions solution game      ate effectiveness examples twoplayer wellknown krishna  availability analytical      version ï¬rstprice sealedbid auction form proves useful benchmarking learning approach      known analytical form ï¬veplayer market  second example ï¬veplayer marketbased schedul      based scheduling game known solution   ing game reeves et al  time slots allocated                                                        simultaneous ascending auctions milgrom     introduction                                       game known solution previous work iden  gametheoretic analysis typically begins complete tiï¬ed equilibria discretized subsets strategy space  scription strategic interactions game  sider prior question determining game actually  preliminaries  given database game experience direct  speciï¬cation possible target learning applied  notation  games shoham et al  agents avail generic normal form game formally expressed  able actions outcomes deterministic game âˆ†si  uis  refers set players                                                                       identiï¬ed through systematic exploration instance  number players si set strategies                                                                ask agents play each strategy proï¬le en available player set âˆ†si simplex                                                                         âˆˆ  tire joint strategy set record payoffs each mixed strategies si finally uis  sm                                                                                             Ã—Â· Â· Â·Ã—  â†’   joint action space small limited nondeterminism payoff function player players jointly  handled sampling coordinating exploration play      sm each sj sj com                                                                                          âˆˆ  joint set does pose difï¬cult issues brafman tennenholtz mon assume von neumannmorgenstern utility allow  example address carefully case common ing agent iâ€™s payoff particular mixed strategy pro  stochastic games brafman tennenholtz  ï¬le uiÏƒ  sÏƒs Ïƒmsmuis                                                                             âˆˆ       Â· Â· Â·  general problem maintaining equilib Ïƒj  sj   mixed strategy player assign                                                                â†’          rium learning algorithms brafman tennenholtz ing probability each pure strategy sj sj                                                  probabilities agentâ€™s strategy setâˆˆadd     difï¬culties posed intractably large inï¬ Ïƒj âˆ†sj   nite strategy sets make problem tractable itâˆˆwill convenient refer strategy pure  reducing number proï¬les agents allowed mixed player separately remaining  play comes cost transforming game players accommodate use denote joint  different game entirely instead seek strategy players player iâˆ’  nash equilibrium                                 approximation quality directly terms distance  paper concerned oneshot normalform tween uË† terms strategies dictated  games players make decisions simultaneously uË† evaluated respect appeal  accrue payoffs game ends singleshot notion approximate nash equilibrium  nature preclude learning experience deï¬nition  strategy proï¬le Ïƒ  Ïƒ     Ïƒm constitutes  fact repeated episodes allowed long actions nash equilibrium game âˆ†s                                                                                                  affect future opportunities condition future strategies Ïƒi âˆ†si uiÏƒi Ïƒ   uiÏƒi Ïƒ  game payoff data obtained observations  âˆˆ      âˆˆ              âˆ’      â‰¥       âˆ’                                                           propose using  deï¬nition mea  agents playing game simulations hypo                                                        sure approximation error uË† employ evaluat  thetical runs game cases learning                                                        ing learning methods known com  relevant despite fact game played                                                        pute  straightforward manner let denote iâ€™s best                                                                                      iâˆ—                                                        response correspondence deï¬ned siâˆ—Ïƒ      faced oneshot game agent ideally play                                 âˆ’           âˆˆ                                                        arg maxs uisi Ïƒ  clarity exposition  best strategy given played agents         âˆ’                                                         siâˆ—Ïƒ singlevalued let ÏƒË† solution nash  conï¬guration agents play strategies best âˆ’                                                        equilibrium game âˆ†s   uË†  ÏƒË†   sponses constitutes nash equilibrium                                                                                      nash equilibrium true game âˆ†s                                                                                                    deï¬nition  strategy proï¬le      sm constitutes   maxi uisiâˆ—ÏƒË† ÏƒË† uiÏƒË†i ÏƒË†                                                                 âˆˆ         âˆ’    âˆ’  âˆ’        âˆ’  purestrategy nash equilibrium game si  uis  general unknown amenable                                                  si si uisi uisi   analysis developed method estimating              âˆˆ     âˆˆ          âˆ’   â‰¥       âˆ’  similar deï¬nition applies mixed strategies al data  lowed                                                  remainder report focus special case                                                        general problem action sets realvalued  deï¬nition  strategy proï¬le Ïƒ  Ïƒ     Ïƒm tervals si    restrict attention sym  stitutes mixedstrategy nash equilibrium game    metric games limit number variables  âˆ†si  uis  Ïƒi    âˆ†si   payofffunction hypotheses using form aggrega                              âˆˆ         âˆˆ                                     uiÏƒi Ïƒ uiÏƒi Ïƒ                             tion agentsâ€™ actions assumption symmetry        âˆ’   â‰¥        âˆ’    study devote particular attention games allows adopt convention remainder                                                        paper payoff usi agent playing si  exhibit symmetry respect payoffs                                 âˆ’  deï¬nition  game âˆ†si  uis  symmetric                                                      polynomial  regression    si  sj uisi  uj sj    âˆ€    âˆˆ                            âˆ’            âˆ’      class models consider nthdegree separable  si  sj                       âˆ’    âˆ’                            polynomials  symmetric games relatively compact descriptions                                                                                                                                      usi Ï†s  ansi    asi  present associated computational advantages given         âˆ’                                                                              Â· Â· Â·                     symmetric game focus subclass symmet               bnÏ†     bÏ†s    ric equilibria arguably natural kreps                âˆ’    Â· Â· Â·     âˆ’                                                        Ï†s represents aggregation strategies  avoid need coordinate roles fairly general   âˆ’  settings symmetric games possess symmetric equilibria played agents twoplayer games Ï†  nash                                          simply identity function refer polynomials                                                        form  separable lack terms combining si                                                        consider models terms example    payoff  function  approximation                    theâˆ’ nonseparable quadratic    problem  deï¬nition                                                                                                                                  usi Ï†s  asi  asi  bÏ†  given set data points each describing         âˆ’                       âˆ’                                                                                    bÏ†s  csiÏ†s   instance agents played strategy proï¬le realized                     âˆ’         âˆ’  value      vm deterministic games complete note   coincide case     information simply incomplete information  experiments described employ simpler  stochastic outcomes random variable speciï¬cally                                                        version nonseparable quadratic takes     independent draw distribution function advantage quadratic form ana  expected value                                  lytically solve nash equilibrium given general non    payoff function approximation task select func separable quadratic  necessary ï¬rstorder condition  tion uË† candidate set minimizing measure                                                        interior solution si   cÏ†s ia  deviation true payofuf function true                     âˆ’          âˆ’                                                        reduces si   aa separable case  function unknown course base selection          âˆ’                                                        nonseparable case additive aggregation Ï†sums   evidence provided given data points                                                      âˆ’    goal approximating payoff functions typically restrictions inherent approach  predicting payoffs assessing strate course recognize tradeoffs complexity hy  gic behavior assessing results measure pothesis space generalization performance  ji sj  derive explicit ï¬rstorder condition  strategy aggregation       symmetric equilibrium si  aa          noted consider payoff functions                        âˆ’             âˆ’    purestrategy equilibrium necessarily exist dimensional strategy proï¬les form usi                                                                                                     âˆ’  separable polynomial model guaranteed ex fsi Ï†s long Ï†s invariant different                                                                âˆ’                âˆ’  ist nonseparable case learned quadratic permutations strategies payoff func  concave experiments follow learned tion symmetric actual payofâˆ’f functions  nonseparable quadratic does pure nash equilib example games known symmetric constrain  rium generate arbitrary symmetric pure proï¬le Ï†s preserve symmetry underlying game                                                               âˆ’  approximate nash equilibrium                           experiments compared three variants Ï†s                                                                                                       âˆ’    difï¬culty arises polynomial degree compact simple sum Ï†sums sec                                                                                                   âˆ’  higher three nash equilibrium ond ordered pair Ï†sum Ï†ss Ï†sss                                                                                                     âˆ’  case select equilibrium arbitrarily          sj   variant Ï†identity  sim                                                          ji                               âˆ’       âˆ’                                                        ply  takes strategies direct unaggregated form    local regression                                 enforcep symmetry requirement case sort  addition polynomial models explored learning using strategies                                                                       âˆ’  local regression methods locally weighted average  locally weighted quadratic regression atkeson et al   firstprice sealedbid auction  unlike modelbased methods polynomial regression                                                        standard ï¬rstprice sealedbid fpsb auction game  local methods attempt infer model coefï¬cients                                                        krishna  agents private valuations good  data instead methods weigh training data points                                                        sale simultaneously choose bid price representing  distance query point estimate answerâ€”in                                                        offer purchase good bidder naming high  case payoff strategy proï¬le pointâ€”using                                                        est price gets good pays offered price  function weighted data set used gaussian weight                                                     agents receive pay classic setup ï¬rst ana  function  eâˆ’  distance training lyzed vickrey  agents identical valuation dis  data point query point weight tributions uniform   distributions com  assigned training point                      mon knowledge unique bayesian nash equilibrium    case locally weighted average simply                                                                            game agent bid mâˆ’ xi xi iâ€™s valua  weighted average payoffs training data points tion good  payoff arbitrary strategy proï¬le locally weighted note strategies game generally games  quadratic regression hand ï¬ts quadratic incomplete information bi      func  gression weighted data set each query point tions agentâ€™s private informationâ†’we consider                                                        stricted case bid functions constrained form    support vector machine  regression               bixi  kixi ki     constraint transforms  category learning methods used support action space realâˆˆ interval corresponding choice  vector machines svms details regarding learn parameter ki easily restricted strategy  ing method refer interested reader vapnik  space includes known equilibrium game                                                                    experiments used svm light package joachims si  ki  mâˆ’ equilibrium   opensource implementation svm clas restricted game agents constrained strategies  siï¬cation regression algorithms                  given form                                                           focus special case   corre    finding mixed  strategy equilibria               sponding equilibrium    twoplayer  case polynomial regression able ï¬nd ei fpsb derive closedform description  ther analytic simple robust numeric methods com actual expected payoff function  puting pure nash equilibria local regression svm                                                                                         learning fortunate access                                                                                     âˆ’    âˆ’                                                                                        closedform description function learning     ï£±                                                                                             â‰¥  furthermore interested mixed strategy ap        ï£²ï£´   âˆ’              proximate equilibria polynomial models solution            methods yield pure strategy equilibria                 availabilityï£³ï£´ known solutions example fa     particular learned model amenable cilitates analysis learning approach results  closedform solution approximate learned game summarized figure  each methods classes  ï¬nite strategy grid ï¬nd mixedstrategy equi functional forms measured average  varying train  librium resulting ï¬nite game using generalpurpose ing set sizes instance evaluate performance  ï¬nitegame solver employed replicator dynamics fu separable quadratic approximation training size  denberg levine  searches symmetric independently draw strategies     sn  uniformly  mixed equilibrium using iterative evolutionary algorithm   corresponding training set comprises   treat result ï¬xed number iterations points si sj usi sj         approximate nash equilibrium learned game     given  ï¬nd best separableâˆˆ  quadratic ï¬t uË† tothese points ï¬nd nash equilibrium corresponding uË† indicated inferior learning performance displayed  calculate  strategy proï¬le game  nash equilibrium respect actual payoff func results game provide optimistic view  tion repeat process  times averaging results regression expected perform compared  strategy draws obtain each value plotted figure  discretization game quite easy learning                                                        underlying payoff function captured lower                                                    degree model experimental setup eliminated                              sample best               issue noisy payoff observations employing ac                              separable quadratic                         nonâˆ’separable quadratic   tual expected payoffs selected strategies                              rd degree poly                              th degree poly                                                       marketbased    scheduling  game      Îµ                                                        second game investigate presents signiï¬cantly                                                   difï¬cult learning challenge ï¬veplayer symmetric        average                                           game analytic characterization theoreti                                                    cally known solution game hinges incomplete infor                                                        mation training data available simulator                                                   samples underlying distribution                                                          game based marketbased scheduling scenario                                                       reeves et al  agents bid simultaneous auc                                                             number strategies training set  tions timeindexed resources necessary perform                                                        given jobs agents private information job  figure  epsilon versus number training strategy points lengths values completing jobs various dead  different functional forms                       lines note space strategies quite complex                                                        dependent multidimensional private information                                                        preferences price histories time slots                                                        fpsb example transform policy space                                           actual function      real interval constraining strategies parametrized                                   learned quadratic    form particular start simple myopic policyâ€”                                                        straightforward bidding milgrom  modify                                                    scalar parameter called â€œsunk awarenessâ€ denoted                                                        controls agentâ€™s tendency stick slots                                                                 currently winning details motivation                                                     sunk awareness inessential current study                                                        note   optimal setting involves        payoffss                                         tradeoffs generallyâˆˆ dependent agentsâ€™ behavior                                                      investigate learning game collected data                                                        strategy proï¬les discrete set values                                                                 accounting symmetry representsâˆˆ                                                         distinctstrategy proï¬les evaluation purposes                                                    treat sample averages each discrete proï¬le true                                                                                   expected payoffs grid                                                           previous empirical study game reeves  figure  learned actual payoff function et al  estimated payoff function dis  agent plays  learned function separable crete grid proï¬les assembled strategies  quadratic particular sample                 computing approximate nash                                                        equilibrium using replicator dynamics    seconddegree polynomial forms generated training set based data  tried quite game   quadratic strategies  samples proï¬le regressed  regression outperforms model labeled â€œsample bestâ€ quadratic forms calculated empirical  values  payoff function approximated discrete respect entire data set computing maxi  training set directly derived equilibrium model mum beneï¬t deviation data emp     simply nash equilibrium discrete strategies maxi maxsi si uisi sË† uisË†  si  training set ï¬rst success quadratic model strateâˆˆgy set âˆˆplayer representedâˆ’ âˆ’ data set  surprising actual payoff function  game symmetric maximum players  piecewise differentiable point discontinu dropped agent strategy sets identical  ity figure  appears quite results presented table  nash  smooth approximated quadratic polynomial equilibria learned functions quite close pro  higherdegree polynomials apparently overï¬t data duced replicator dynamics  values quite bit                                                              âˆ’       separable quadratic  lower  grid point determined    post hoc running proï¬le simulations                                                                                 Ï†sum                                                             agents playing  agent deviates                      Ï†sumsum squares  strategies                                                                                       Ï†identity                                                                               sample best symmetric            method          equilibrium si                                      sample best           separable quadratic                     Îµ     nonseparable quadratic                          replicator dynamics                                                             average                                                                table  values  symmetric purestrategy equilib  ria games deï¬ned different payoff function approxima  tion methods quadratic models trained proï¬les   conï¬ned strategies                                                                                                                                                                                                      comprehensive trial collected  million ad     number strategies training set  ditional samples proï¬le ran learning algorithms   training sets each uniformly randomly selected  discrete grid         each training set included figure  effectiveness learning separable quadratic                                                      model different forms Ï†s  proï¬les generated ï¬ve                         âˆ’  agent strategies grid case pro                                                                âˆ’      nonâˆ’separable quadratic  ï¬le does typically appear complete data   set developed method estimating  pure sym                      Ï†sum                                                             metric approximate equilibria symmetric games based                      Ï†sumsum squares  mixture neighbor strategies appear test set               Ï†identity  let designate pure symmetric equilibrium strategy                  sample best symmetric  approximated game sË† ï¬rst determine closest                     sample best    neighbors sË† symmetric strategy set represented Îµ  data let neighbors denoted   deï¬ne mixed strategy Î± support                                                          average    probability playing computed based relative   distance sË† neighbors Î±   sË†    note symmetry allows compactâˆ’  representationâˆ’   âˆ’  payoff function agents choice   strategies deï¬ne usi payoff  symmetric player playing strategy si                                      âˆˆ                                                          agents play strategy  agents each independently                          âˆ’                                         number strategies training set  choose play probability Î± proba  bility exactly choose given                                                       figure  effectiveness learning nonseparable quadratic                                              model different forms Ï†s          prÎ±     âˆ’    Î±   Î±  âˆ’ âˆ’                                         âˆ’                               âˆ’                            approximate  mixed strategy Î±     figure  regression separable                                                     quadratic produces considerably better approximate equi       âˆ’  max     prÎ± usi Î±us  Î±us  librium size training set relatively small  si                   âˆ’          âˆ’   âˆ’   âˆˆ                                                figure  shows nonseparable quadratic performs                                                      similarly results appear relatively insensitive    using method estimating  complete data gree aggregation applied representation  set compared results polynomial regression agentsâ€™ strategies  method simply selects training set pure polynomial regression methods employed yield                                         strategy proï¬le smallest value  refer purestrategy nash equilibria evaluated  method â€œsample bestâ€ differentiating methods generally produce mixedstrategy equilibria  case consider symmetric pure proï¬les la local regression learning methods svm gaussian  beled â€œsample best symmetricâ€ pure proï¬les la                                                       radial basis kernel direct estimation using training  beled â€œsample best allâ€                           data discussed computed mixed strategy equi    interesting observe figures   libria applying replicator dynamics discrete approxima                                                                                         strict search best pure strategy proï¬le symmetric proï¬les tions learned payoff functions ensure  average better terms  restriction  imposed                                                 case direct estimation training data data
