  exploiting    informative     priors   bayesian    classiÔ¨Åcation       regression    trees                                   nicos angelopoulos     james  cussens                                      department   science                                             university york                                     heslington york  yo   dd  uk                                     nicosjccsyorkacuk                        abstract                          paper assumes familiarity basic properties                                                        models      general method deÔ¨Åning informative pri        method fully bayesian using version      ors statistical models presented applied metropolishastings mcmc algorithm approximately      speciÔ¨Åcally space classiÔ¨Åcation regres sample posterior distributions space pos      sion trees bayesian approach learning  sible crt models eliminating priori unin      models data taken metropolis   teresting trees example containing sparsely popu      hastings algorithm used approximately    lated leaves large space key feature      sample posterior using proposal approach user declare structural prior      distributions closely tied prior acceptance knowledge restrict andor bias prior distribution      probabilities easily computable marginal  space      likelihood ratios prior used ap rest paper structured follows section       proach empirically tested varying data describes language deÔ¨Åning priors section       ii prior iii proposal distribution scribes speciÔ¨Åc priors used bayesian      comparison related work given            inference crt models section  presents version                                                        metropolishastings mh algorithm gives    introduction                                       vergence result special case section  gives repre  key feature bayesian approach statistical inference sentative sample results experiments paper  prior knowledge relevant information distinct concludes comparison related work section   data incorporated learning process followed pointers future work section   mathematically rigorous conceptually clear manner  assumption space possible statistical models  deÔ¨Åning priors stochastic logic  deÔ¨Åned prior knowledge expressed prior distri programs  bution space distribution updated approach deÔ¨Åning priors related line research  data using bayes theorem produce posterior distribu independently initiated chipman et al   tion posterior distribution end result learning denison et al  basic idea specify prior  used make predictions future data distribution space models stochastic program  suitably summarised data analyst insight closedform expression  domain    bayesian framework compellingly simple instead specifying closedform expression  complexities applying real data analysis tree prior pt  specify pt  implicitly  difÔ¨Åculties computational example treegenerating stochastic process each realization  highdimensional spaces posterior complex process simply considered ran  report directly features distribution mean dom draw prior chipman et al   variance extracted‚Äîrequiring general difÔ¨Åcult convenient language deÔ¨Åning stochastic pro  integration problems progressively allevi grams stochastic logic programming muggleton   ated increased computing power subsequent use stochastic logic program slp used deÔ¨Åne prior  markov chain monte carlo mcmc techniques approxi given space statistical models twostep process  mately sample posterior                     firstly standard logic program written deÔ¨Ånes    paper existing bayesian method deÔ¨Åning desired model space hypothesis space machine learn  using Ô¨Çexible class prior distributions models ing parlance simply deÔ¨Åne unary predicate cart  developed applied speciÔ¨Åcally classiÔ¨Åcation logic program each instantiation  regression tree crt models crt models makes cartt  true term Ô¨Årstorder logic rep  known machine learning increasingly statistics resents crt model exploit expressivenessof Ô¨Årstorder logic represent crt models whichever     cartleaf  logical representation convenient                 cartsplitlr         point crt models wish exclude   newd    spltsplit  hypothesis space excluded adding ap newd    cartl   newd    cartr  propriate constraints logic program example sup  pose user faced classiÔ¨Åcation task knows spltx spltx spltx  wishes assume distribution classes  stant rectangular region ‚Äòbox‚Äô attribute figure  prior distribution crt models  space follows box contained leaf probability splitting leaf depends depth  ‚Äòtrue‚Äô tree trees leaf containing box  need considered general user wish declare  boxes effected constraints tree fig  displays proof tree proves  experiments declaring facts given fig  cartxxleafleafleaf true  altering deÔ¨Ånition cart check boxes leaf tree model space figs   use  determined declarations split tree abbreviated representation crt models split val                                                        ues omitted each proof tree simply consists rules  constclassprobsboxnamevarminmax              facts used proof ‚Äòbolted together‚Äô equality  constclassprobsboxxinf                 straints initial query appears equality constraint  constclassprobsboxxinf                  root node conceptually slps execute probabilis                                                        tically growing proof tree backtracking constraints  constclassprobsboxxinf                  insoluble  constclassprobsboxxinf                  sum priors deÔ¨Åned sampling algorithm                                                        logical framework sampling algorithm                                                        course implemented programming language  figure  declaring rectangular regions dis                                                        involves progressively building data structure represent  tribution classes constant                                                        ing crt model making choices ii checking    stochastic element slp easily constraints violated iii backtracking  stood Ô¨Årst considering logic programming lan motivation logic programming implementa  guage prolog searches logical consequences logic tion procedures ii iii builtin logic  program given query cartt ‚ÄúÔ¨Ånd convenient language declaring constraints  cartt true‚Äù prolog search suitable ts boxes  deterministic depthÔ¨Årst search choice distributions directly sampled  rules each lead proof prolog mcmc distributions effectively repre  chooses whichever rule occurs Ô¨Årst logic program sented deÔ¨Åning direct sampling algorithm  rule leads deadend prolog backtracks tries statistical model viewed end result  second rule                           generative process generative description    basic slp idea simple probability labels motivated bayes nets generated adding arcs  tached rules logic program converting gelopoulos cussens  crt models split  slp search proofs choose ting leaves addition backtracking extends appli  rules rule chosen probabilistically according prob cability generative view  ability labels paper chosen rule does lead                                                                           cartt  proof backtracking try untried                   rules revisiting recent choicepoints Ô¨Årst renor      cartsplitlr      cartr  malising probabilities necessary approach          cartl                                                                      spltsplit                         cartleaf  introduced cussens  backtrackable sampling            cartsplitlr    query cartt asked dis spltx  tribution possible instantiations each rep                                                         spltsplit      cartl      cartr  resents crt model provides prior distribution                             model space                                        spltx      cartleaf      cartleaf    paper extended representational power  slps probability labels computed ‚Äòon figure       proof   tree     proves  Ô¨Çy‚Äô slp rules form expprob  vars  cartxxleafleafleaf    rule  each point rule possible rule  use required vars ground list  expprob  probability computable vars fig   priors bayesian crt  example slp probability growing tree  splitting leaf depends depth leaf   section actual priors used    slps deÔ¨Åne distribution proofs understand experiments described section  fixing notation  distribution useful represent each proof proof note given crt model maps example describedby set predictor variables     xp distri probabilistic  bution response variable case classiÔ¨Åcation  trees known decision trees discrete Ô¨Ånite  using metropolishastings algorithm  case regression trees continuous                                                        metropolishastings mh algorithm mcmc algo  crt  implementations throw away information given                                distribution simply return majority class mean rithm deÔ¨Ånes markov chain  transition                                                                                        value appropriate just design decision fun kernel kt   follows                                                                                      damentally distribution given   produced generating ‚àº qt ti proposal dis                                                                                             follows each tree deÔ¨Ånes conditional probabil tribution acceptance probability Œ±t  used  ity distribution yt conditional distribution cide accept proposed ti  prob  likelihood function following version bayes theo ability Œ±t proposed accepted ti  ti  rem ty  txp yt xp yx lhs gives probability  ‚àí Œ±t setting acceptance prob  desired posterior distribution trees note ability like  prior tx prior conditional pre                                                                                                         qtt   dictor variables observed values data  Œ±t   min                         used deÔ¨Åne sensible priors                                            tx qtt       importantly allows rule priori trees      leaves contain examples stop chain  converge sampling  user allowing trees sparsely populated leaves ruling sired posterior weak conditions  trees normal research area ensures starting point chain difÔ¨Å                                                        cult bayes theorem qttp tx   space possible tree structures reasonable Ô¨Ånite    size experiments trees leaves rqt qt tp tx function rq   examples permitted following denison et al                                                                                                                   yt   predictors used deÔ¨Åne prior biassed   Œ±t   min rqt                splits data desired                     yt   investigate desirability option    approach choose proposal distributions    experiments decided use permit simpliÔ¨Åcation acceptance probability                                                                             growtree  prior essentially used value rqt  easily computable each pair  chipman et al  attempting use slp model trees  crucial point choice ensures  externally provided prior provides test represen Œ±t easily computable computing  tational Ô¨Çexibility slps unable pass prior probability given crt model opposed  test extended slps variable probabil merely sampling prior prohibitively expen  ity labels using growtree prior allows better sive contrasts chipman et al  ‚Äúspec  comparison others‚Äô results                      iÔ¨Åcations allow straightforward evaluation pt    growtree    prior grows crt tree starting t‚Äù required computation prior prob  single leaf node repeatedly splits each leaf abilities expensive backtracking                               ‚àíŒ≤  node Œ∑ probability Œ±  dŒ∑  dŒ∑ depth tracking slps similar stochasticcontext free grammars  node Œ∑ Œ± Œ≤ prior parameters set user scfgs‚Äîparticularly probability labels Ô¨Åxed  control size trees unsplit nodes leaves computed ‚Äòon Ô¨Çy‚Äô‚Äîand scfg dynamic programming  tree node split splitting rule split algorithms presumably used compute prior prob  chosen uniformly abbreviated fragment slp abilities ruling backtracking greatly restricts  expresses prior given fig                straints user declare allow                                                        tracking  spltspltspltlfdab                                                                     trees explicitly deÔ¨Åne distribution                                                              classes leaves tree likelihoods marginal   nwsplit       expdb                                                            likelihoods‚Äîwe integrate possible class distribu   nwsplitspltlfdab                                                        tions integration automatically prevents overÔ¨Åtting   nwsplitspltlf      dab                                                        trees  training set accuracy generally  spltspltspltlfdablf                                                                  high marginal likelihood actually integration            lf   leafd                                                        use exactly approach chipman et al                                                         denison et al   each leaf uniform  figure  growtree  prior fragment deÔ¨Åned slp dirichlet distribution parameters set  distribu                                                        tion possible distributions classes dirichlet    experimented edittree assumption allows closed form marginal likelihood  priors ‚Äòinitial‚Äô crt model supplied yt  tree example tree produced standard proposals best understood Ô¨Årst recalling  greedy algorithm manually entered user sampling crt trees slp prior effected sam  tree probabilistically ‚Äòedited‚Äô growing prun pling proofs prior explained section   ing changing splitting rule number edits each proof associated proof tree shownin fig  each proposal considered works prun els means proposals snip current crt  ing proof tree corresponding current crt model tree node regrow tree leaving  point regrowing proof tree rest crt tree unscathed makes easier  pruning point using sampling mechanism associated markov chain head direction high likelihood  prior proposals differ choose trees subtrees highlikelihood tend altered rarely  ‚Äòsnip‚Äô proof tree subtree proposed replacements lowlikelihood subtrees  prune point resampled proposals equivalent accepted relatively  proposing prior tx special case  discussed shortly instead each prune point corre  experimental results  sponds proposing tree according tx ‚àà tk experiments used three datasets wis  tk set trees differ current tree consin breast cancer data bcw kyphosis dataset  subtree prune point             pima dataset bcw originally donated    overly simple proposal corresponds indepen uci depository wolberg mangasarian   dent metropolishastings algorithm prunes away used chipman et al  dataset comes  current model completely new model sampled rpart package building manipulating  prior independently current model crts dataset uci dataset denison et al  proposal denoted following mcmc conver  used extensive bayesian crt analysis following  gence result let ¬µi distribution trees produced chipman et al  simply deleted  datapoints                                                ÀÜ  ith iteration independent mh algorithm let bcw contain missing values cases  crt  tree maximal marginal likelihood     machine learning task binary classiÔ¨Åcation using integer                                                    ¬µi ‚àí ¬∑x ‚â§  ‚àí tÀÜxp tÀÜx  valued predictors‚Äî predictors case bcw                                                          splits binary splitting   ¬∑  denotes total variation distance threshold  datapoints bcw   distributions exponentially fast convergence   true posterior ¬∑x using independent mh al performed large number experiments  gorithm catch tÀÜxp tÀÜx ratio analysed results ways small  tween tÀÜ‚Äôs prior posterior probability tiny rea representative sample reported data  sonably sized dataset experiments presented software reproducing experiments  independent mh algorithm performs poorly conver httpwwwcsyorkacuk‚àºnicos  gence result derived doob  swareslpsmcmcms used sicstus prolog  run  given rosenthal  apply doob‚Äôs result ning linux machines each ghz pro                                                    necessary produce upper bound mint kq  cessor mb ram computation log  kq transition kernel associatedp likelihood ratio uses function called prolog run  fact show proof omitted sum exactly ning chain  iterations takes  minutes  tÀÜxp tÀÜx establishing  upper bound  discrete Ô¨Ånite space exactly half size  comparison standard crt algorithm  bound general independent mh algorithm established contrasts bayesian crt standard  mengersen tweedie                      greedy algorithm building single crt model    somewhat better performing proposal denoted q‚àín explored existing bayesian crt litera  cycles through proposals qk        user ture chipman et al  denison et al    supplied qk proposal prunes proof bayesian method represents posterior uncertainty better                                             tree kth choice point rq‚àín  ‚â°  makes thorough exploration model space  acceptance probability simply likelihood ratio cost computation bayesian  best performing proposal denoted quc makes approach does terms predictive accuracy depends  uniform choice prune point available course prior useful prior knowl                                                                      current tree quc rquc   dtdt  dt edge help incorporated prior  depth tree                                      distribution facilitating central motivation    recent innovation greatly increased efÔ¨Åciency tree maximal marginal likelihood maximum  space explored previous work posteriori map model uniform prior assumed  ing slps mcmc  prune points pro trees mapunif trees mcmc run produces  posals jumped organised proof tree approximation mapunif tree maximum  sequence branch relevant sldtree fact es marginal likelihood tree mcmc sample  sentially previous proposal mechanism standard greedy algorithm closely connected search  closely modelled prolog backtracking mapunif tree buntine  interesting compare  tracked choice point crt building work marginal likelihood trees produced using greedy al  choice point thrown away backtrack gorithm mapunif tree approximations  point proof tree stochastically mcmc sample  built parts proof tree built chrono trees rpart datasets bcw using  logically later translating speciÔ¨Åcally crt mod default settings rounded marginal loglikelihoods                                                                                                                                         ‚Äôtrndabbcwdataiksllhoods‚Äô                                                                     density                                                                                         ‚àí    ‚àí                                         bandwidth                                                          figure  loglikelihood trajectory priorgrowtree  figure  distribution differences estimated class pos Œ≤  databcw mcmcq‚àí  terior probabilities mcmc runs differing                                                                                  random seed                                                       ‚Äôtrrnabeditdataiksllhoods‚Äô ‚Äôtrrnabbcwdataiksllhoods‚Äô                                                             sizes   respectively unsurprisingly                                                                                     trees produced mcmc runs higher marginal log                                                                                 likelihoods values closer mapunif tree   bcw resp Ô¨Ånd tree  resp  leaves  loglikelihood  resp                                                                                                          reproducibility results                                                        figure  loglikelihood trajectories cases  aim using mcmc approximate poste                                                        databcw    mcmcquc    sequential     lhs  rior distribution important inferences drawn                                                                           prioredittree rhs priorgrowtree Œ≤    reasonably long realisation markov chain ro  bust changing random seeds determine evo  lution chain using original sequencebased pro  inÔ¨Çuence priors                                                                                    posals strong evidence case fig  compares loglikelihood trajectories using edit  example plots loglikelihood iteration number tree prior growtree prior respectively  loglikelihood trajectories quite different ex parameters equal distinct horizontal  periments identical random seeds                                                                          line   edittree       trajectory    clear  evidence      used                                                 edittree   prior pulling markov chain    test new prooftree based proposals performed initial tree note figs  contain initial low  following experiment reported loglikelihoods corresponding chain‚Äôs initial crt  mcmc runs  iterations performed model truncated space reasons  using growtree  prior Œ±   Œ≤   using denison et al  examined successful   pima dataset differing random seeds chains Ô¨Ånding crt trees high marginal like  used each remaining  pima dataset examples lihood using pima dataset maximum marginal  each mcmc samples used approximate loglikelihood  highest  test hy  posterior probability example class label  pothesis high likelihood crt models did  simply getting each tree mcmc high posterior probability priors  sample make class prediction example based unlikely visited started chains log  majority class appropriate leaf setting class likelihood crt model denison et al fig  show  posterior probability equal relative frequency class results rhs trajectory corresponds prior   predictions clearly mcmc samples perfect stronger bias smaller trees cases  representations true posterior difference es prior pulls away highlikelihood crt models  timates class  posterior zero each exam rapidly stronger prior  ple naturally achieved distribution goal including prior knowledge improve  differences probability estimates shown fig  highly decision making uncertainty classifying test ex  concentrated zero provides evidence ap amples performed experiments known ‚Äòtrue‚Äô  proach produces values close true posterior tree used generate synthetic train test data  different runs                                       included boxes section  consistent true                                                        tree constraints growtree prior produced mcmc    using local jumps                                samples using synthetic training data measured pre  local jumping tree space helps prevent getting stuck dictive accuracy synthetic test data each   loglikelihood trajectory using q‚àí shown fig  shows test examples classiÔ¨Åed probable class  big jumps q‚àí proposes rarely accepted estimated mcmc sample repeated   chain remains stuck model long periods random seeds box constraints testset accuracies  problem pronounced independent    box constraints  mh  algorithmis used compare Ô¨Ågures given fig  testset accuracy exactly   quc used                                each random seed results indicate accurate prior
