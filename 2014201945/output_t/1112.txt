          ensembles partially trained svms multiplicative updates                                       ivor tsang      james kwok                             department science engineering                       hong kong university science technology hong kong                                         ivorjameskcseusthk                          abstract                            simpler approach use multiplicative updates                                                        ï¬rst explored cristianini et al      training support vector machines svm convergence sensitive learning rate      volves quadratic programming problem  ï¬xed manually learning rate large      optimized complicated numerical solver shooting oscillations occur small      paper propose simpler approach convergence slow      based multiplicative updates idea ï¬rst second problem multiplicative update rule      explored cristianini et al  work hardmargin svm      vergence sensitive learning rate noisy data hardmargin svm poor performance      ï¬xed manually update rule  softmargin svm trade complexity      works hardmargin svm known     training error usually preferred choice      poor performance noisy data pa tradeoff parameter zero inï¬n      show multiplicative update svm ity critical various procedures pro      formulated bregman projection prob   posed determination using crossvalidation      lem learning rate adapted au generalization error bounds evgeniou et al       tomatically connection  typically computationally expensive      boosting bregman distance show      possibility avoids parameter tuning com      multiplicative update svm pletely using ensemble approach kim et al       garded boosting weighted parzen window   combines multiple svms different parameters ob      classiï¬ers motivated success boosting tains improved performance lot svms      consider use adaptive ensemble  trained approach expensive      partially trained svms extensive experi                                                          paper ï¬rst address learning rate problem      ments show proposed multiplicative update                                                        formulating multiplicative update svm bregman      rule adaptive learning rate leads faster                                                        projection problem collins schapire       stable convergence pro                                                        shown learning rate adapted automatically      posed ensemble efï¬cient training compa                                                        based data connection      rable better accuracy besttuned                                                        tween boosting bregman distance collins schapire      softmargin svm                                                         kivinen warmuth  show mul                                                        tiplicative update regarded boosting algorithm                                                        weighted parzen window classiï¬ers base    introduction                                       hypotheses each base hypothesis added ensemble  kernel methods support vector machines   partially trained hardmargin svm boosting  svms highly successful machine learn literature wellknown ensemble performs  ing problems standard svm training involves quadratic better single base hypothesis address  programming qp problem solved com possibly poor performance hardmargin svm  plicated numerical solver generalpurpose sider using ensemble partially trained svms  qp solver used inefï¬cient particu prediction note comes little extra cost mul  lar type qps consequently lot specialized optimiza tiplicative updating procedure experimentally ensemble  tion techniques developed popular ap observed comparable better accuracy  proach using decomposition methods sequential besttuned softmargin svm  minimization optimization smo platt whichin  rest paper organized follows learn  volves sophisticated working set selection strategies advance ing rate problem multiplicative update addressed sec  caching schemes kernel matrix gradients tion  connection update process  heuristics stepsize prediction              boosting proposed ensemble partially                                                    ijcai                                                    trained hardmargin svms discussed section   bregman projection  experimental results presented section  section assume access optimal  section gives concluding remarks                value Ïâˆ— return issue determining Ïâˆ—                                                        section                                                                                                multiplicative updating rule svm                  Î±  âˆˆ pm   Î±  âˆˆ    Î± â‰¥  Î±                                                         dimensional probability simplex natural choice breg  paper focus particular variant hard man distance unnormalized relative entropy  margin svm called Ïsvm sequel th iteration minimize bregman distance  introduced section  develop sections                                                                                           new Î± current estimate Î±t Î± Î±m                                                                               âˆ‚uÎ± Ïâˆ—   iterative multiplicative updating procedure based  Î±          Î±         Î± kËœ Î± âˆ’ Ïâˆ—  bregman projection collins schapire  requiring satisfy   âˆ‚Î±                 kivinen warmuth  convergence similar constraint                                                         following entropy projection problem  erative procedure shown section                                                                  xm â€œ     Î±            â€                                                            min      Î± ln  âˆ’ Î±   Î±t  Ïâˆ—  Î±kËœ Î±     hardmargin   Ïsvm                                                                                                                              Î±âˆˆpm           Î±                                                                    given training set xiyiiwherexi âˆˆ yi âˆˆ                                     Ïsvm ï¬nds plane fxw   Ï•x kernel   introducing lagrange multipliers constraints Ïâˆ—   induced feature space feature map Ï• separates Î±kËœ Î± Î±                               Ïw                             shown remaining  classes maximum margin                      constraint Î± â‰¥  inactive lagrange multiplier                                                      needed setting derivative the lagrangian   max Ï âˆ’w      yiw Ï•xi â‰¥ Ï          wÏ                                                          Î±i            âˆ—     Ëœ                                                                  Î±i ln âˆ’Î±i Î±i âˆ’Î·t Ï âˆ’ Î± kÎ±t   âˆ’Î»Î±  âˆ’                                                                  Î±i  corresponding dual                                                                                                                   wrt Î± zero Î±i  Î±i expâˆ’Î·tyiftxi                                                        Î»                                           Î±              Î±kËœ Î±    Î± â‰¥   Î±                      using   substituting               minÎ±                                                     Î»     âˆ’    Î·           Î·                                                         obtain       ln  twhere                                                            Î±t    âˆ’Î·     vectors zero respectively Î±  exp                                                Ëœ  Î±Î±m vector lagrange multipliers                                                                        Î±        Î±      âˆ’Î·tyift xi zt Î·t        yiyjkxi xj  easily shown                        exp                        xm                    xm                     update commonly known multiplicative           Î±iyiÏ•xi fx   Î±iyikxi   date cristianini et al  exponentiated gradient                                                kivinen warmuth  Î·t playing role                                                        learning rate notice initializing Î±   zero duality gap objectives   Î±                                                               subsequent iterations mentioned  equal optimality                        earlier Î± â‰¥   inactive constraint finally                      âˆ—    âˆ—   âˆ—                       shown Î·t obtained dual                      Ï  Î±   kËœ Î±                                                                                                         âˆ—                                                                     max  âˆ’ lnztÎ·texpÏ Î·t          denote                                                              Î·t                         Ëœ                    uÎ± ÏÂ¯ Î± kÎ±  âˆ’ ÏÂ¯Î±  âˆ’                mentioned section  similar multiplicative updat                                                                              ing algorithm standard hardmargin svm ex       Ï  Â¯ lagrangian multiplier dual constraint plored cristianini et al  learning rate  Î±       karushkuhntucker kkt condi Î· adaptive performance sensitive  tions lead                                                       Î·                        Ï                                                       manual choice  hand svm vari                       âˆ‚u Î± Ïâˆ—                        ant value Î·t automatically determined                â‰¤ Î±âˆ—âŠ¥       Â¯    â‰¥                           âˆ‚Î±                        improved convergence properties adaptive scheme                                 Î±âˆ—                     experimentally demonstrated section   âŠ¥ denotes vectors orthogonal                                                          estimating value Ïâˆ—   âˆ‚uÎ± ÏÂ¯                                                                                   âˆ—            kËœ Î± âˆ’ ÏÂ¯ fx  âˆ’ ÏyÂ¯ fx  âˆ’ ÏÂ¯ return question estimating Ï  recall     âˆ‚Î±                                                                                âˆ—                                                        initialize Î± Î±     Ï equal                                 Ï                      optimal dual objective minimized  using  optimal Â¯ dual variable    âˆ—  dual equal optimal primal variable Ï optimal Î± yield smaller objective                                                                                    Ëœ                                                        Î± equal ÏÂ¯                          Ïâˆ—   Ïâˆ—                                 âˆ—                     âˆ—                        Â¯                              Ï â‰¥    Ï âˆˆ  ÏÂ¯                                                          following consider using decreasing sequence    Ïâˆ—   Ïâˆ—Î±âˆ—   Î±âˆ—kËœ Î±âˆ—  Ïâˆ—                                                   âˆ—  Â¯ Â¯                    using    Ïtâ€™s ÏÂ¯ â‰¥ Ït â‰¥ Ï  consequently constraint                                                 âˆ—                             Î±          variables optimality denoted superscript  paper initialize                                                      ijcai                                                                                    âˆ—      Ëœ   replaced Ït â‰¥ Ï  Î± kÎ±tandthe    algorithm  training Ïsvm  entropy projection problem                      input  xiyiim tolerance           xm â€œ                  â€                                                             Î±i                       min      Î±  ln    âˆ’ Î±  Î±       Ï â‰¥ Î± kËœ Î±          initialize  Î±i mand                                      Î±âˆˆpm           Î±                                                Ëœ                                                      ÏÂ¯  Î±kÎ±             Ëœ                                                             ft xi  let ÏÂ¯t â‰¡ Î±tkÎ±t ï¬rst consider feasibility   use  compute                                                            set Ït Â¯Ït  t  proposition   t  ifÏÂ¯t satisï¬es                                                          set Î·t argminÎ·â‰¥  ztÎ·expÏtÎ·ifÎ·t                        âˆ’ t   âˆ—                   Ï       â‰¥ Ï                            t  t goto step  test t                   Â¯t                                                                                  small Î·t thensett  âˆ’  terminate                                   ÏÂ¯t     Ëœ                  Î± âˆˆ pm              â‰¥  Î± kÎ±t            update lagrangian multipliers Î±iâ€™s using   exists    t                                                                   Î±t   Î±t    âˆ’Î·    Î·              Ëœ               Ëœ                                   exp    compute  proof  wehavev  kv  â‰¥  vector               Ëœ                âˆ—                                           ÏÂ¯t  Î±tkÎ±t   Î±t âˆ’ Î±                                                          ÏÂ¯t â‰¥ ÏÂ¯tthent  t â† goto           Î± kËœ Î± â‰¥ Î±âˆ—kËœ Î± âˆ’ Î±âˆ—kËœ Î±âˆ—                                                 step  t  t check t    condition                                 t â‰¥  goto step  set  âˆ’          âˆ’ t    âˆ—        Ëœ    âˆ’ t   âˆ— Ëœ âˆ—             terminate      ÏÂ¯t     â‰¥  Ï   â‡’   Î±tkÎ±t       â‰¥  Î±  kÎ±                                   t                    t                      output svm         summing                     ÏÂ¯t     âˆ— Ëœ                       â‰¥ Î±  kÎ±t                      convergence                 t               âˆ—                                        section design auxiliary function  Î±  Î± satisï¬es condition                                                        used lower bound loss decreases                 âˆ’t    âˆ—    ÏÂ¯t  â‰¥ Ï  feasible solution  ex each iteration collins schapire  denote rela                                                                                                     Â· Â·            Î±t              Î±                         tive entropy bregman distance Î”   ists set   optimal  notice                      âˆ—  differs  equality constraint   optimal Î± solution  intersec  inequality constraint optimization tion hyperplanes deï¬ned linear constraints                                                               Ëœ   exactly section  Ït â‰¥ Î± kÎ±t tâ€™s using     optimization Î·t extra constraint Î· â‰¥               Ëœ       âˆ— Ëœ                                  Ëœ                                  Î±tkÎ±t  â‰¥ Î±  kÎ±t                   subsequently set ÏÂ¯t  Î±tkÎ±t Ït ac  cording                                            decrease loss consecutive iterations                            ÏÂ¯t                      Ï                                        Î±âˆ— Î±   âˆ’   Î±âˆ— Î±                                                      Î”       Î”                                                                                                                       Ëœ                         kkt condition  Î·tÎ±tkÎ±t âˆ’ Ït              âˆ—                                                                                   Î±i  ln Î±i   âˆ’ ln Î±i   Î±t  Î±t                     Î·t   leads                                               Ëœ                                                           Î±tkÎ±t    Ït                                                                                         âˆ’Î·     Î±âˆ—y   âˆ’    Î·  using                                                    ln using                                                                                    Î±  kËœ Î±                                ÏÂ¯t                        m       cos Ï‰tt  âˆš âˆš                                                            ÏÂ¯t ÏÂ¯t  t   ÏÂ¯t               â‰¥âˆ’Î·t       Î±i   yiftxi âˆ’ ln ztÎ·tusing   Ï‰tt angle wt wt recall                                                                   m                         entropy projection procedure used solve dual            starts converge angle Ï‰tt tends   Î±i     ln Î±i   âˆ’ ln Î±i    using   zero cos Ï‰tt â†’ ast  ifÏÂ¯t  ÏÂ¯t         optimization progress deteriorated  implies current t large overshot  solution discarded case  âˆ—     âˆ—                                                         Î”Î±   Î±t âˆ’ Î”Î±  Î±t â‰¥ Î”Î±t Î±t â‰¡ aÎ±t   infeasible instead relax constraint   setting t â† t obtain larger Ït repeated aÂ· auxiliary function aÂ· increases                                   âˆ—  ÏÂ¯t â‰¤ ÏÂ¯t note Ï â‰¤ Â·Â·Â· â‰¤ ÏÂ¯t â‰¤ ÏÂ¯t bounded zero Î”Â· Â· â‰¥  sequence                Ïâˆ—    Î±âˆ— kËœ Î±âˆ—    Î±âˆ—                                aÎ±tâ€™s converges zero Î± âˆˆ pm compact                 optimal optimization                                    âˆ—                        Î±t    Ït                    continuity uniqueness Î±  limit  resumes new      process                                                  âˆ—             t                                        sequence minimizers converges global optimum Î±   iterates smaller termination threshold                                         complete algorithm shown algorithm         using   Î±t kËœ Î±t                                                                                                                            ÏÂ¯t      Ëœ                      Ëœ     tÏÂ¯t                                                           asÎ±   kÎ±   Â¯ÏtthenÎ±t âˆ’ Î±t kÎ±t         Î·t  process achieve global t                               t                                            âˆ—                                               Ëœ  optimal terminate algorithm Î±t  Î±  special case ftxi âˆˆ âˆ’  kÎ±tâˆ â‰¤                                                                                                      tÏÂ¯t                                                                          Î±t âˆ’ Î±t â‰¥     experiments use   initialize       ing hÂ¨olderâ€™s inequality         t                                                     ijcai                                                                                                                                            apply pinskerâ€™s inequality fedotov et al  identical Î±i playing role di compare   obtain                                             Î·t role ct compare                                                        consequence fact boosting regarded                              t ÏÂ¯t                Î±t Î±t                                                                                      Î”                                      entropy projection kivinen warmuth                                                  boosting perspective effectively using base hy             tÏÂ¯t                                      pothesis form  Î± âˆˆ pm necessarily equal  let Î½ min   summing  each step                                                      Î±âˆ— base hypothesis regarded variant                                  t                    parzen window classiï¬er patterns weighted         âˆ—           âˆ—     Î”Î±   Î± âˆ’ Î”Î±  Î±t  â‰¥     Î”Î±t Î±t       Î±                                                                                                   note edge base hypothesis th itera                                                                                     Ëœ                                                        tion Î±iyi  Î±i yikxi xj   Î± kÎ±t         â‡’     â‰¥    Î±âˆ— Î±   tÎ½                 ln     Î”                                constraint  requiring edge                                                                                                   âˆ—  number iterations required  base hypothesis wrt Î±i distribution Ï    lnm                                                                                              Ï    Î½  ï¬xed  gives faster ï¬nite convergence correspondence holds svm                                                                                               âˆ—           result linear asymptotic convergence smo al algorithm algorithm  adaboostÎ½ algorithm                                                       additionally Ït Ïsvm analogous   gorithm lin                                             âˆ—                                                      adaboostÎ½  Ïsvm similar Î½    assuming  satisï¬ed following    âˆ—  convergence proof solution Ïsvmâ€™s dual adaboostÎ½ controlling approximation quality         âˆ—                                Ëœ            maximum margin note Î½ quite difï¬cult  Î±t â†’ Î±   iterations Î±t kÎ±t  â†’   âˆ—                                                                                     Ï  ï¬nal solution desired Ïsvm classiï¬er set practice hand set adaptively                                                          wellknown connections    ensemble partially trained svms                 tween svm  boosting rÂ¨atsch                                                         typically interested relating combined hypoth  close resemblance update rules esis svm fact boosting uses  Ïsvm  variants adaboost algorithm partic norm svm uses norm hand  ular show section  algorithm sec                âˆ—                                       focus showing using weighted parzen  tion  Ï assumed known similar window classiï¬er base hypothesis hy  adaboost algorithm algorithm  section         âˆ—                                 âˆ—             pothesis svm  Ï unknown similar adaboostÎ½ algorithm  rÂ¨atsch warmuth  inspired connection  using ensemble prediction  section  consider using output boosting mentioned section  hardmargin svm  ensemble improved performance time complexity hypothesis poor performance  considered section                     noisy data inspired connection boosting                                                        use boosting ensembleâ€™s output prediction  algorithm  adaboost rÂ¨atsch warmuth                                                                          t                                                                                Î·t    input  xiyiim number iteration                                                                                                       t                                                                                      Î·r    initialize di                                                                 convex combination partially trained     train weak classiï¬er wrt distribution di each svms note takes little extra cost evidenced       pattern xi obtain hypothesis ht âˆˆ âˆ’   boosting literature use ensemble     set                                              lead better performance experimen                  ct argminntcexp                                  câ‰¥                           tally demonstrated section                                                                          computational issue       ntc    di exp âˆ’cyihtxi                                                                                                   update distributions di                          each step update takes om  time                                              excessive large data sets alleviated           di        di  exp âˆ’ctyihtxi ntct  performing lowrank approximations kernel matrix                                                        fine scheinberg whichtakesomr time    end                    t                                rank update computation Ït                                                                                            Â¯    output                                                                                                          omr instead om  time                                                           experiments    Ïsvm training vs boosting                       section experiments performed ï¬ve small                                                                                                                                     Î±t   Î·                 breast cancer diabetis german titanic waveform  comparing update rules section                                                                                                            Î³     dty   di ct adaboost edge hypothesis                                                                        âˆ—                                                           adaboostÎ½ algorithm   algorithm  replaced        âˆ’t    âˆ—     âˆ—      Ëœ     âˆ—     t       ÏÂ¯t   Ï       Ï  â‰¤ Î±  kÎ±t Ï                 minrt Î³r âˆ’ Î½  Î³t           ht      t                      âˆ’t                       edge   satisï¬es loose kkt condition Ïsvm          httpidaï¬rstfraunhoferdeprojectsbenchbenchmarkshtm                                                    ijcai                                                                                                                                     seven mediumtolarge realworld data sets astroparticles                                                                       Î²     xi âˆ’   xj  each small data set  adult adult adult adult web web table                                                         table  comes  realizations performance                                                        obtained averaging realizations          tableasummaryofthedatasetsused             each large data set realization available                                                          ensemble compared following       data set dim    training patterns  test patterns       cancer                                        svm best bestperforming svm       diabetis                                      csvms obtained regularization path algorithm       german                                       hastie et al        titanic                                    svm loo csvm best leaveone      waveform                                    outerror bound evgeniou et al  ob        astro                                                                                  tained regularization path algorithm       adult                          adult                                parzen window classiï¬er ï¬rst hypothe       adult                                 sis added ensemble        adult                               hardmargin Ïsvm hypothesis        web                            web                                 proposed ensemble                                                        implemented matlab run ghz                                                        pentiumâ€“ machine gb ram    adaptive learning rate  ï¬rst demonstrate advantages adaptive learning table  testing errors  different data sets  rate scheme section  lack space                                                                    svm     svm    parzen    Ï   proposed  report results breast cancer waveform similar                                                          data set best  loo   window  svm    ensemble  behavior observed data sets comparison cancer                 train Ïsvm using multiplicative updating               Î·                                          diabetis                    rule ï¬xed incristianini et al           german                        seen figure  performance cris titanic                   tianini et al  sensitive ï¬xed choice Î·when waveform            Î· small Î·  smaller objective value astro                   creases gradually Î· slightly larger say adult                   estimate ï¬nally overshoot indicated adult               vertical lines convergence longer obtained adult               hand proposed adaptive scheme web                       verges faster                                      web                                                                                                                 seen table  ensemble performs                 Ïsvmadaptive          Ïsvmadaptive better hardmargin Ïsvm accuracy               svmÎ·             svmÎ·                     svmÎ·            svmÎ·                                                      comparable better svmbest                 svmÎ·              svmÎ·                                                                                           svmÎ·              svmÎ·   table  compares time obtaining ensemble                                   objective                objective                running regularization path algorithm                                                            seen obtaining ensemble faster takes small                                                    number iterations illustrate performance large                                                                                            iterations              iterations      data sets experiment adult data set table            cancer              waveform         regularization path algorithm run large  figure  comparing update ï¬xed Î· data use instead Î½svm using libsvmfor  adaptive scheme                                      comparison Î½ unlike parameter csvm                                                          alternatively ï¬nd besttuned svm performing                                                        grid search softmargin parameter each grid    accuracy speed                               point requires complete retraining svm regular  show proposed ensemble partially trained ization path algorithm usually efï¬cient hastie et al   hardmargin svms section  comparable perfor   inner qp regularization path algorithm solved  mance besttuned softmargin svm csvm use using optimization package mosek httpwwwmosekcom                                                           gaussian kernel kxi xjexpâˆ’xi âˆ’ xj Î²   includes time computing kernel matrix                                                        expensive large data sets time computing    astroparticles         downloaded         leaveoneout bound included timing svmloo  httpwwwcsientuedutwâˆ¼cjlinlibsvmtoolsdatasets version  implementation   httpresearchmicrosoftcomusersjplattsmohtml httpwwwcsientuedutw cjlinlibsvm                                                    ijcai                                                    
