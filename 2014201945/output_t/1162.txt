          exploiting known taxonomies learning overlapping concepts                                      lijuan cai   thomas hofmann                                      department science                                  brown university providence ri usa                                          ljcai thcsbrownedu                          abstract                          tween categories perceptron learning svm clas                                                        siï¬cation architecture rest paper organized      realworld classiï¬cation problems involve    follows section  introduces ways representing tax      large numbers overlapping categories ar onomy knowledge derive pairwise similarities      ranged hierarchy taxonomy propose  categories based relative locations taxonomy      incorporate prior knowledge category taxonomy  order tie learning categories second adapt      directly learning architecture present standard  loss function weigh misclassiï¬cation errors      concrete multilabel classiï¬cation methods accordance taxonomy structure section       generalized version perceptron hierarchi mulate hierarchical learning problem terms joint      cal multilabel svm learning method works    large margin problem derive efï¬cient train      arbitrary necessarily singly connected tax ing algorithm section  propose hierarchical percep      onomies applied generally     tron algorithm exploits taxonomies similar fashion      settings categories characterized section  examines related work section  presents exper      tributes relations necessarily imental results show new hierarchical algo      duced taxonomy  experimental results     rithms bring signiï¬cant improvements metrics      wipoalpha collection show hierarchical  clusions future work discussed section       methods bring signiï¬cant performance improve      ment                                                           utilizing known taxonomies    introduction                                         problem setting  realworld classiï¬cation tasks involve large numbers assume patterns documents represented  overlapping categories prominent examples include vectors âˆˆx âŠ†rd mapped higher  international patent classiï¬cation scheme approx  dimensional feature space Ï†x denote set cat  patent groups open directory project approx  egories  category âˆˆyanda  categories web pages gene ontology ap label set âˆˆ py py power set  prox  terms gene products cases taxonomy directed acyclic graph ve nodes  instances assigned category cat vâŠ‡ysuch set terminal nodes equals formally  egories rarely mutually exclusive leads large  âˆˆv v âˆˆv  yv âˆˆ note  scale multilabel classiï¬cation problems categories assume taxonomy singly connected tree  typically organized hierarchies taxonomies com est allow converging nodes cases wants  monly introducing superordinate concepts relating express items belong supercategory  categories â€˜isaâ€™ relationships multiply connected tax terminal categories suggest model  onomies uncommon context             formally adding terminal node each inner node repre    believe taxonomies encode valuable domain   senting â€œmiscellaneousâ€ category avoids problem  knowledge learning methods able capital partial paths  ize particular number training examples multilabel learning aim ï¬nding mapping   individual classes small dealing tens xâ†’py based sample training pairs xiyii  thousands classes potential loss valuable nâŠ†xÃ—py popular approach suggested  information ignoring class hierarchies pointed instance schapire singer  actually  led number approaches employ learn ranking function categories each pattern  different ways exploit hierarchies mccallum et al   xâ†’sqwheresq set permutations ranks   wang et al  dumais chen             order unique subset labels needs    paper present approach systematically address additional question select number  incorporating domain knowledge relationships categories pattern assigned                                                    ijcai                                                       common deï¬ne ranking function implic assigning irrelevant item label set   itly scoring function  xÃ—y â†’r     deï¬ne ancy  â‰¡v âˆˆv   âˆƒy âˆˆ yv âˆˆ  ancynowwe  gxy gxy xy fxy cate quantify loss following manner  gories higher values appear earlier ranking                                                                                              Ë†    âˆ’               âˆ’   âˆ’      ease presentation ignore ties notation gy used yy    sv     sv       sv    clear context                                           vâˆˆancy  vâˆˆancyË†        vâˆˆancy                                                                                                  âˆ©ancyË†     class attributes                                 note nodes symmetric difference ancy     following cai hofmann  suggest use scor ancyË†  contribute loss following sim                                                                                          âˆ’      ing functions linear joint feature repre plify presentation assuming    sentation Î¦ inputs categories xy â‰¡ setting sv âˆ€v âˆˆv gets  yyË†   w Î¦                      weight vector following   ancy    ancyË†  intuitively means colors                                               Î¦  tsochantardis et al  cai hofmann  nodes path node color say  chosen form Î›y âŠ— Ï†xwhereÎ›y                                     Ë†                                                    blue nodes paths nodes color  Î»yÎ»sy âˆˆ refers attribute vector repre                     âŠ—                                  say yellow nodes colors blueyellowgreen  senting categories kronecker product correct blue nodes ones missed  interpret terms stacked vector individual weight               wt      wt                           yellow nodes ones incorrectly selected  vectors   s   leading additive types mistakes contribute loss proportional             xy    Î» yw  x  composition                     general  volume  idea notion class attributes allow general during training loss function difï¬cult deal  ization place similar categories just directly involves sets labels like  training examples belonging category work pairwise contributions involving terms  absence information set  deï¬ne                                                                                             Î»ryÎ´ry  leads xywy x                          yy   ancy  ancy              going translate taxonomy information singly connected taxonomies eq  equivalent  attributes categories idea treat nodes length undirected shortest path connecting nodes  taxonomy properties formally deï¬ne           y                                                                                      suggested wang et al   order relate                         âˆˆ ancy               state following proposition               Î»                                                                    proposition  yyË† âŠ†ysatisfying âŠ† yË†                                                        yË† âŠ†   tv â‰¥  attribute value node simplest                   case tv set constant like  denote ancy ancy    ancyË† â‰¤ ancy   ancË†y    set ancestor nodes taxonomy including                     yâˆˆy âˆ’yË†  notational convenience leads intuitive                yË†âˆˆyË† âˆ’y  decomposition scoring function contributions learning ranking functions translate  nodes paths root node speciï¬c                 terminal node                                                 yg            ancy   ancË†y                                                                            yâˆˆyyË†âˆˆyâˆ’y    loss functions                                                      gygË†y  standard loss function multilabel case use look pair categories incorrect cate  symmetric difference predicted ac gory comes correct category order deï¬ned  tual label set count number correct categories count symmetric difference respective ancestor  missed plus number incorrect categories sets corresponding loss  assigned  yyË†  â‰¡y   yË†     applications actual loss predicted la  hierarchical support vector machines  bel set relative true set category labels depend  multilabel classiï¬cation  relationship categories motivation generalize multiclass svm formulation crammer  consider generic setting routing items based singer  multilabel formulation similar  membership nodes taxonomy instance news elisseff weston  given set correct  routing setting readers signup speciï¬c topics se                                                        categories yi denote complement yÂ¯i  yâˆ’yi  lecting appropriate node terminal following elisseff weston  approximate  node taxonomy category â€œsoccerâ€ separation margin respect ith example  ner node supercategory â€œsportsâ€ note  assume items assigned terminal nodes  taxonomy customers prefer signup  Î³iw â‰¡   min  Î¦xiy âˆ’ Î¦xi yÂ¯ w                                                                         âˆˆ   Â¯âˆˆ Â¯  categories en bloc selecting appropriate supercategory       yiy yi    assume relative signup volume sv â‰¥ formulation aims maximizing margin                             âˆ’   each node costs missing relevant item training set maxww mini Î³iwthisis                                                    ijcai                                                     equivalent minimizing norm weight vector algorithm  hierarchical multilabel svm  constraining functional margins greater                           â‰¥                                                          inputs training data iyi tolerance    equal  generalized softmargin svm formulation  initialize si  âˆ… Î±iyyÂ¯  âˆ€ âˆˆ yi yÂ¯ âˆˆ yÂ¯i  obtained introducing slack variables Î¾iâ€™s  repeat  penalty scaled proportional loss associated       Ë†                                                                    select argmaxi Ïˆi  violation respective category ordering mech          Ë†                 Â¯                                                            select Ë†yyÂ¯  argmaxyâˆˆyË†yÂ¯âˆˆyË† gË† Â¯  anism suggested cf tsochantardis et al                              iyy                                                                               sË†  sË† âˆªË†yyÂ¯Ë†  cai hofmann  putting ideas yields  expand working set                                                                                  Î±Ë†  yyÂ¯ âˆˆ sË†  convex quadratic program qp                         solve qp subspace iyyÂ¯                                                                   reduce working set sË†  sË† âˆ’yyÂ¯Î±Ë†                  n                                                                          iyyÂ¯                                                        ÏˆË† â‰¤   min   w       Î¾i                                          wÎ¾                                             Î¾i                            note minimize upper bound loss   st wÎ´Î¦iyyÂ¯â‰¥ âˆ’         âˆ€i âˆˆ yi yÂ¯ âˆˆ yÂ¯i                          yyÂ¯                        eq  simply assign slack variable Î¾iyyÂ¯                                                        triplet instance positive label negative label      Î¾i â‰¥   âˆ€i                                                         leads dual program similar eq  sec       Î´Î¦  yyÂ¯ â‰¡ Î¦x âˆ’ Î¦x  yÂ¯                                            Î±iyyÂ¯ â‰¤  âˆ€    âˆˆ     âˆˆ Â¯                                           ond set constraints yyÂ¯ yyÂ¯     formulation similar used schapire explore direction  singer  generalizes rankingbased multilabel  svm  formulation elisseff weston  follow  optimization algorithm  ing crammer singer  included bias derived qp large employ  terms categories efforts correctly efï¬cient optimization algorithm inspired smo  dering each pair positivenegative labels use algorithm platt  performs sequence sub  size prediction mechanism elisseff space ascents dual using smallest possible subsets  weston  convert category ranking actual variables coupled remaining variables  multilabel classiï¬cation                            through constraints algorithm successively optimizes    dual qp                           subspaces spanned Î±iyyÂ¯  âˆˆ yi yÂ¯ âˆˆ yÂ¯i              n                                       selected instance additional variable selec  max  Î˜Î±          Î±iyyÂ¯                          tion performed each subspace strategy   Î±                                                                                                            yâˆˆyi                                 known column generation demiriz et al  deï¬ne                    Â¯                  yÂ¯âˆˆyi                                          n                                                                                 wÎ± â‰¡             Î±iyyÂ¯Î´Î¦iyyÂ¯                       âˆ’              Î±iyyÂ¯Î±jrrÂ¯Î´Î¦iyyÂ¯Î´Î¦j rÂ¯                                                                         Â¯                                                                  yâˆˆyiyÂ¯âˆˆyi               ij yâˆˆyi râˆˆyj                  yÂ¯âˆˆyÂ¯ Â¯                     rÂ¯âˆˆyj                                   â‰¡          âˆ’           Î±                                                          giyyÂ¯    yyÂ¯  Î´Î¦iyyÂ¯                                                  Î±iyyÂ¯                                                   st Î±iyyÂ¯ â‰¥ âˆ€i âˆˆ yi yÂ¯ âˆˆ yÂ¯i    â‰¤ âˆ€i                                      yyÂ¯                  li â‰¡ max     max   giyyÂ¯                                             yâˆˆyi                                            Â¯                                 Â¯                                     iyâˆˆyiyÂ¯âˆˆyi                               yÂ¯âˆˆyi                             â§                                                                 âª            Â¯                                                                 â¨min  iyâˆˆyi yÂ¯âˆˆyi giyyÂ¯      Î¶i   note                                                                                                                                 â‰¡       Î±iyyÂ¯                                                                            ui  âª         Î´Î¦iyyÂ¯Î´Î¦jr rÂ¯                                  â©                 Â¯                                                                   min  min iyâˆˆyi yÂ¯âˆˆyi giyyÂ¯   Î¶i                                                                                            Î›y âˆ’  Î›Â¯y Î›r âˆ’ Î›Â¯rÏ†x Ï†x                         Î±iyyÂ¯                                                                                                                                                                            simply replace inner products corre                       Î±iyyÂ¯                                                        Î¶i  âˆ’    âˆˆ   Â¯âˆˆ Â¯     deï¬neÏˆi   â‰¡ li âˆ’ ui  sponding kernel functions straightforward observe           yiy yi yyÂ¯     Î¾ yields upper bound training loss derivation similar cai hofmann                                                                         âˆ€  resulting classiï¬er measured   following sense shown Ïˆi  necessary sufï¬cient  deï¬ne maximum loss                             condition feasible solution optimal                                                        score Ïˆi used selecting subspaces giyyÂ¯ used           xyg  â‰¡      max        yyÂ¯         select new variables expand active set each subspace                      âˆˆ  Â¯âˆˆ Â¯   â‰¥ Â¯                     yy                    resulting algorithm depicted algorithm   maximal loss set examples deï¬ned    details convergence sparseness gen                                                                                                                    n                           eral class algorithms tsochantardis et al                                                                â‰¡         gx                                                                                  hierarchical perceptron                          Î¾Ë†  proposition  denote  Ë†   feasible solution svm competitive generating highquality                    Ë†  qp thenn   Î¾i upper bound empiri classiï¬ers computationally expensive percep                             cal maximal loss xf  Ë†  iyi               tron algorithm rosenblatt  known simplicity                                                    ijcai                                                     algorithm  hierarchical minover perceptron algorithm misclassiï¬cations penalized ancestors miss                                                  relevant patterns include irrelevant ones hloss    inputs training data iyi desired margin    initialize Î±iyyÂ¯  âˆ€ âˆˆ yi yÂ¯ âˆˆ yÂ¯i        punishment occurs node descendents    repeat                                            penalized addition loss function works                                                        arbitrary taxonomy just trees     Ë†i yË† yÂ¯Ë†  argmin âˆˆ Â¯âˆˆ Â¯ wÎ±Î´Î¦iyyÂ¯                       iy yiy yi                        rousu et al  applies maximummargin markov     wÎ±Î´Î¦Ë†Ë†yyÂ¯Ë†                                                      networks taskar et al  hierarchical classiï¬cation       terminate satisfactory solution                                                        taxonomy regarded markov networks                                                            propose simpliï¬ed version hloss decomposes         Î±Ë†   â† Î±Ë†     Ë†yyÂ¯Ë†         iyË†yÂ¯Ë† iyË†yÂ¯Ë†                                contributions edges marginalize exponential     end                                           sized problem polynomial methods learn   maximal number iterations performed  ing occurs taxonomy nodes instead edges view                                                        taxonomy dependency graph â€œisaâ€ relation                                                          cai hofmann  proposes hierarchical svm  speed section propose hierarchical decomposes discriminant functions contributions  ceptron algorithm  using minimumoverlap minover different levels hierarchy way  learning rule krauth mÂ´ezard              work compared cai hofmann     minover perceptron uses instance violates stricted multiclass classiï¬cation deal  desired margin worst update separating hyperplane additional challenge posed overlapping categories  deal instances sequentially truly multilabel problem employ category rank  online fashion using minimum overlap selection rule ing approach proposed schapire singer   effectively speeds convergence yields sparser summary major contributions  formulate  lutions                                              multilabel classiï¬cation global joint learning problem    using taxonomybased class attribute scheme eq  taxonomy information account  exploit  tv  simple update rule step  algorithm  taxonomy directly encoding structure scoring func  decomposed                                tion used rank categories  propose novel taxonomy                                                        based loss function overlapping categories    wv â†  wv   yyÂ¯Ï†xi  âˆ€v  âˆˆ ancy âˆ’ ancÂ¯y     â†   âˆ’             âˆ€    âˆˆ       âˆ’             motivated real applications  derive sparse optimiza             yyÂ¯Ï†   ancÂ¯y ancy      tion algorithm efï¬ciently solve joint svm formula    weight vectors nodes predeces tion compared multiclass classiï¬cation sparseness  sors yÂ¯ updated nodes important constraints  left intact strategy used dekel et al  dual variables  present hierarchical percep  online multiclass classiï¬cation severe loss tron algorithm takes advantage proposed methods  incurred dramatic update step encoding known taxonomies   updates scoring functions classes  question spread impact classes sharing  experiments  affected ancestors                           experimental setup                                                        section compare hierarchical approaches    related work                                       ï¬‚at counterparts wipoalpha data set com  approaches hierarchical classiï¬cation use decision prising patent documents  tree like architecture associating each inner node taxonomyderived attributes employed hierar  taxonomy classiï¬er learns discriminate chical approachesâˆš comparison purpose tv eq   children dumais chen  cesabianchi et al  set  depth depth â‰¡ maxy ancysothat  offers advantages terms modularity local maxy Î›y ï¬‚at hierarchical mod  optimization partial classiï¬ers inner node els experiments hierarchical loss equals half value  able reï¬‚ect global objective               eq  historical reason hierarchical learning em    cesabianchi et al  introduces loss function ploys hierarchical loss ï¬‚at employs âˆ’  called hloss speciï¬cally designed deal loss used linear kernel set  each instance  case partial overlapping paths treestructured tax normalized norm  experiments test  onomies cesabianchi et al  proposed bsvm performance evaluated crossvalidation macro  uses hloss uses decoding scheme averaging folds  explicitly computes bayesoptimal label assignment measures used include oneaccuracy average pre  based hloss certain conditional independence cision ranking loss maximal lossandparent oneaccuracy  sumptions label paths loss function proposed ï¬rst three standard metrics multilabel classiï¬ca  eq  exploits taxonomy different way tion problem schapire singer  elisseff  loss partly convert partial path categories ston  oneaccuracy acc measures empirical  complete path ones loss function inspired real probability topranked label relevant doc  applications like routing subscription taxonomy ument average precision prec measures quality la                                                    ijcai                                                                section cat   doc  cat    acc     prec     xloss    rloss    pacc                                   doc  ï¬‚at   hier  ï¬‚at   hier ï¬‚at    hier ï¬‚at   hier ï¬‚at    hier                                                                                                                                                                                                                                                                                                                                                     table  svm experiments wipoalpha corpus each row categories speciï¬ed level node section results  random fold crossvalidation better performance marked bold face â€œcatdocâ€ refers average number categories  document â€œï¬‚atâ€ ï¬‚at svm â€œhierâ€ hierarchical svm                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            flat prec                                                                                                                                                                                        hier prec                                                                                                  flat rloss                                                                           flat acc               hier rloss                                                                             hier acc                                                           performance    flat pacc performance                                                                                                                                                                 hier pacc                                                                                                                                                                         figure  columns left right depict accuracy sample number category sample number category  ï¬‚at hierarchical perceptron average precision ï¬‚at  hierarchical perceptron                          figure  flat hierarchical svm section data vary                                                       ing training set size small number documents sampled     section  acc   prec    xloss  pacc       each category training purpose learned classiï¬ers              ï¬‚at hier ï¬‚at hier ï¬‚at hier  ï¬‚at hier      tested remaining documents repeated  times                      each sampling number bars depict sample standard deviation                                         ument labeled primary category                      number secondary categories types categories                      used form multilabel corpus performed                      dependent experiments taxonomies  toplevel                      sections                                                                             document parsing performed lemur toolkit   table  svm experiments wipoalpha corpus subsam stop words removed stemming performed word  pling three documents sampled each category counts title claim ï¬elds used document fea                                                        tures table  summarizes svm performance  bel rankings precision calculated each position sections hierarchical svm signiï¬cantly outperforms  positive label occurred labels ranked higher ï¬‚at svm terms  xloss ranking loss parent ac  including predicted relevant precision curacy each individual setting attributed  values averaged obtain average precision rank fact hierarchical approach explicitly op  ing loss rloss measures average fraction positive label timizes upper bound  xloss spe  negative label pairs misordered metrics ciï¬c hierarchical form discriminant function  described details schapire singer  hierarchical svm produces higher classiï¬cation    maximal loss denoted  introduced eq  accuracy average precision gains mod  measured hierarchical loss function eval erate improvement statistically signiï¬cant  uate parent oneaccuracy pacc measures conducted fold crossvalidation section  accuracy categoryâ€™s parent nodes level        paired permutation test achieved level signiï¬cance                                                         accuracy     experiments wipoalpha collection             measures  wipoalpha collection comprises patent documents released figure  depicts performance perceptron algorithm  world intellectual property organization wipo  setting allow perceptron run  classiï¬ed ipc categories ipc level hier convergence takes signiï¬cantly time svm  archy consisting sections classes subclasses groups reaches lower performance observe hierarchical  categories experiments refer main groups ceptron performs better cases  leaves depth hierarchy each doc addition randomly sampled  documents each      wwwwipointibisdatasets                            wwwlemurprojectorg                                                    ijcai                                                     
