                          locality sensitive discriminant analysisâˆ—                deng cai                         xiaofei                        kun zhou        department science        yahoo research labs             microsoft research asia    university illinois urbana champaign hexyahooinccom        kunzhoumicrosoftcom      dengcaicsuiucedu                            jiawei han                                   hujun bao                   department science                 college science                university illinois urbana champaign           zhejiang university                    hanjcsuiucedu                           baocadzjueducn                        abstract                          techniques purpose principal component analy                                                        sis pca linear discriminant analysis lda duda et      linear discriminant analysis lda popular   al       dataanalytic tool studying class relation pca unsupervised method aims project data      ship data points major disadvantage direction maximal variance lda supervised      lda fails discover local geometri searches project axes data points      cal structure data manifold paper different classes far each requiring data      introduce novel linear algorithm discriminant points class close each      analysis called locality sensitive discriminant  spectral methods methods based eigenvalue      analysis lsda sufï¬cient train decomposition covariance matrix pca      ing samples local structure generally im scatter matrices withinclass scatter matrix      portant global structure discriminant analy class scatter matrix lda intrinsically methods      sis discovering local manifold structure try estimate global statistics mean covari      lsda ï¬nds projection maximizes mar   ance fail sufï¬cient number      gin data points different classes samples pca lda effectively      each local area speciï¬cally data points euclidean structure fail discover underlying      mapped subspace nearby points structure data lives close submanifold      label close each ambient space      nearby points different labels far apart ex      periments carried standard face data recently lot geometri      bases show clear improvement results cally motivated approaches data analysis high dimen      ldabased recognition                            sional spaces examples include isoamp tenenbaum et                                                        al  laplacian eigenmap belkin niyogi                                                         locally linear embedding roweis saul    introduction                                       methods shown effective discovering                                                        geometrical structure underlying manifold  practical algorithms supervised machine learning degrade unsupervised nature fail discover dis  performance prediction accuracy faced criminant structure data meantime manifold  features necessary predicting desired based semisupervised learning attracted considerable  important question ï¬elds machine learning tention zhou et al  belkin et al thesemeth  knowledge discovery vision pattern recogni ods make use labeled unlabeled samples la  tion extract small number good features beled samples used discover discriminant structure  common way attempt resolve problem use di unlabeled samples used discover geomet  mensionality reduction techniques popular rical structure large unlabeled      âˆ—                                                   samples available methods outperform traditional      work supported national science supervised learning algorithms support vector ma  foundation nsf iisiis specialized research chines regression belkin et al howeverinsome  fund doctoral program higher education china applications face recognition unlabeled samples   national natural science foundation china available semisupervised learning   opinions ï¬ndings conclusions recom  mendations expressed paper authors methods applied  necessarily reï¬‚ect views funding agencies paper introduce novel supervised dimension                                                    ijcai                                                     ality reduction algorithm called locality sensitive discrim lda recently lot interests graph  inant analysis exploits geometry data mani based linear dimensionality reduction typical algo  fold ï¬rst construct nearest neighbor graph model rithms includes locality preserving projections lpp  local geometrical structure underlying manifold niyogi  local discriminant embedding lde  graph split withinclass graph betweenclass chen et al  marginal fisher analysis mfa yan et  graph using class labels way geometri al  lpp uses graph model geometri  cal discriminant structure data manifold cal structure data lde mfa essentially  accurately characterized graphs using uses graphs model discrim  tion graph laplacian chung  ï¬nd linear inant structure data algorithms  transformation matrix maps data points sub implicitly consider withinclass betweenclass  space linear transformation optimally preserves lo lations equally important reduces ï¬‚exibility  cal neighborhood information discriminant infor algorithms  mation speciï¬cally each local neighborhood margin  data points different classes maximized  locality sensitive discriminant analysis    paper structured follows section  provide                                                        section introduce locality sensitive discrimi  brief review linear discriminant analysis locality                                                        nant analysis algorithm respects discriminant  sensitive discriminant analysis lsda algorithm intro                                                        geometrical structure data begin description  duced section  section  perform                                                        locality sensitive discriminant objective function  lsda reproducing kernel hilbert space rkhs  gives rise kernel lsda experimental results pre  locality sensitive discriminant objective  sented section  finally provide concluding   function dimensionality reduction  remarks section                                                         described previously naturally occurring data                                                        generated structured systems possibly fewer    related works                                      degrees freedom ambient dimension sug  generic problem linear dimensionality reduction                                                      gest consider case data lives  following given set Â·Â·Â·  xm  ï¬nd transfor                                 nÃ—d                    close submanifold ambient space hopes  mation matrix Â·Â·Â·  ad âˆˆ maps                                                      estimate geometrical discriminant properties  points set points Â·Â·Â·  ym  nsuch                                                      submanifold random points lying unknown  yi â€œrepresentsâ€ xiwhereyi  xi               submanifold paper consider particular ques    linear discriminant analysis lda seeks directions tion maximizing local margin different classes                                                                                                   efï¬cient discrimination suppose data points given data points Â·Â·Â·  xmâŠ‚r sampled  long classes each point associated label underlying submanifold build near  lxi âˆˆ  Â·Â·Â· objective function lda est neighbor graph model local geometrical struc  follows                                              ture each data point xi ï¬nd nearest                                                                ba                   neighbors edge xi neighbors let                 aopt argmax                                                                   swa                    nxixi    Â·Â·Â·  xi  set nearest neighbors                   c                                   weight matrix deï¬ned follows                                                sb     miÎ¼  âˆ’ Î¼Î¼ âˆ’ Î¼                                                                                                   xi âˆˆ nxj xj âˆˆ nxi                                                          wij                                                         â›                      â                                             c   mi                   â             â            sw          xj âˆ’ Î¼ xj âˆ’ Î¼            nearest neighbor graph weight matrix char                                                acterizes local geometry data manifold                                                        frequently used manifold based learning techniques  Î¼ total sample mean vector mi number                                                      belkin niyogi  tenenbaum et al   samples ith class Î¼ average vector ith                                                      roweis saul  niyogi  class jthsampleintheith class sw                                                      graph fails discover discriminant structure  withinclass scatter matrix sb betweenclass scat                                                        data  ter matrix basis functions lda eigenvectors                                                          order discover geometrical discriminant  following generalized eigenproblem associated                                                        structure data manifold construct graphs  largest eigenvalues                                                        withinclass graph gw betweenclass graph gblet                      sba  Î»swa                     lxi class label xi each data point xithe                                                                                               clearly lda aims preserve global class relationship set  naturally split subsets                                                        nwxj  nwxi contains neighbors sharing  data points fails discover intrinsic            local geometrical structure data manifold label contains neighbors having  real world applications face recognition different labels speciï¬cally                                                                               sufï¬cient training samples case nwxixi lxi lxi  â‰¤ â‰¤  able accurately estimate global structure local                                                                              structure important                             nbxixi  lxi   lxi  â‰¤ â‰¤                                                    ijcai                                                                                                                                 figure  center point ï¬ve neighbors points color shape belong class  withinclass graph connects nearby points label betweenclass graph connects nearby points  different labels locality sensitive discriminant analysis margin different classes maximized    clearly nbxi âˆ© nwxiâˆ…   nbxi âˆª nwxi     simple algebra formulation objective function   nxiletww   wb weight matrices gw reduced                                                                      respectively deï¬ne                                                                                                                       yi âˆ’ yj wwij                    âˆˆ       âˆˆ                                                                     ij       bij                                                                                                                                                                                                               xi âˆ’ xj  wwij                                                                                   xi âˆˆ nwxj  xj âˆˆ nwxi                    ij     wwij                                                                                                                                                                                                                           xidwiixi âˆ’    xiwwijxj  clear  wb  ww nearest neighbor                           ij                                                                                   graph  thought combination withinclass       xdwx    âˆ’ xwwx      graph gw betweenclass graph gb                                                                 consider problem mapping withinclass diagonal matrix entries column  row                                                                                                graph betweenclass graph line connected symmetric sum wii    wij  points gw stay close possible similarly objective function  reduced                                                                        nected points stay distant possible let                                  Â·Â·Â·                                                           yi âˆ’ yj wbij            map reasonable criterion               choosing â€œgoodâ€ map optimize following ob               ij                                                                                           jective functions                                                                                                                                               xi âˆ’ xj  wbij                                                                         ij                 min    yi âˆ’ yj wwij                                                                                                                     ij                                                 xdb âˆ’ wbx                                                                                                                                                      xlbx                   max    yi âˆ’ yj wbij                                 ij                                db diagonal matrix entries  column row                                                        wb symmetric sum wb dbii  wbij  lb   appropriate constraints objective function  âˆ’                      withinclass graph incurs heavy penalty neighboring laplacian matrix                                                          note matrix dw provides natural measure  points xi xj mapped far apart actu                                                        data points dwii large implies class  ally class likewise objective function                        betweenclass graph incurs heavy penalty neighboring taining high density bigger                                                        value dwii â€œimportantâ€ xi  points xi xj mapped close actu  ally belong different classes minimizing  impose constraint follows                                                                                       attempt ensure xi xj close sharing dwy  â‡’    xdwx      label yi yj close maximiz  ing  attempt ensure xi xj close objective function  following                                                                                    different labels far apart learning       min   âˆ’ xwwx                 procedure illustrated figure                                                                                          equivalently    optimal linear embedding                                                                                                                              max  xwwx                   subsection locality sensitive dis              criminant analysis algorithm solves objective objective function  rewritten follows                              functions   suppose projection vector                           yt              Â·Â·Â·     Ã—                            max  xlbx                                       matrix                                                                         ijcai                                                     finally optimization problem reduces ï¬nding    let Î¦ denote data matrix rkhs                                                                                    arg max      Î±lb âˆ’   Î±ww                      Î¦Ï†xÏ†x  Â·Â·Â· Ï†xm                        xdw                                         eigenvector problem rkhs written fol                                                        lows       Î±                        â‰¤  Î± â‰¤                                             suitable constant       projec            Î±l      âˆ’ Î±       Î»     tion vector minimizes  given maximum    Î¦           Î¦      Î¦  wÎ¦         eigenvalue solution generalized eigenvalue problem eigenvector  linear combinations                                                                             Ï†xÏ†x Â·Â·Â· Ï†xm exist coefï¬cients Î±ii         Î±lb âˆ’  Î±ww     Î»xdwx                                                                  Â·Â·Â· msuch                         Â·Â·Â·   let column vector         solutions                     m                                                Î»   equation  ordered according eigenvalues                    Î±iÏ†xiÎ¦Î±  Â·Â·Â·Î»        embedding follows                                                  â†’                                                                                                      Î± Î±Î± Â·Â·Â· Î±m  âˆˆ                    Â·Â·Â·  ad                   following algebraic formulations                                                                                                                                                                                    yi ddimensional vector Ã— matrix   Î¦ Î±lb âˆ’   Î±ww  Î¦   Î»Î¦dwÎ¦                                                                                          note number samples mislessthan                                                                                                         â‡’   Î¦ Î±lb âˆ’   Î±ww  Î¦  Î¦Î±   Î»Î¦dwÎ¦   Î¦Î±  number features rankx â‰¤ conse                                                                                            â‡’        Î±l      âˆ’ Î±      Î±  quently rank  xdwx  â‰¤  rank xÎ±lb  âˆ’                 Î¦ Î¦           Î¦ Î¦  Î±   xt   â‰¤               xd   xt       Î±l                                        fact                       Î»Î¦  Î¦dwÎ¦    Î¦Î±                                                                                     âˆ’ Î±wwx   Ã— matrices implies                                                             â‡’    Î±lb âˆ’  Î±ww   kÎ±Î±  Î»kdwkÎ±Î±       singular case ï¬rst apply principal com  ponent analysis remove components corresponding kernel matrix kij  kxi xjletthe  zero eigenvalues                                     column vectors Î±Î±Î± Â·Â·Â· Î±Î±m solutions equation                                                         test point compute projections    kernel lsda                                        eigenvectors vk according  lsda linear algorithm fail discover          m                  m                                                                                             trinsic geometry data manifold highly nonlinear Â· Ï†x  Î±i Ï†x Â· Ï†xi  Î±i kx xi  section discussion perform lsda                             producing kernel hilbert space rkhs gives rise  Î±k     ith                  Î±k  kernel lsda                                           element vector  original                                                        training points map obtained  kÎ±Î±where    suppose  Â·Â·Â·  xmâˆˆxis training sam    th                                                      element onedimensional representation  ple set consider problem feature space induced  nonlinear mapping                                                   Ï†  xâ†’f                              experimental results  proper chosen Ï† inner product     deï¬ned section investigate use lsda face recog  makes socalled reproducing kernel hilbert nition compare proposed algorithm eigenface  space rkhs speciï¬cally                       pca turk pentland  fisherface lda bel                                                        humeur et al  marginal fisher analysis mfa                  Ï†xÏ†y   kx                                                        yan et al  begin brief discussion  holds  positive semideï¬nite kernel func data preparation  tion popular kernel functions gaussian ker  nel kx yexpâˆ’x    âˆ’  yÏƒ polynomial kernel    data preparation  kx      sigmoid kernel kx     face databases tested ï¬rst yale data  tanh   Î±                                     base second orl database    given set vectors vi âˆˆfi   Â·Â·Â· experiments preprocessing locate faces applied  orthonormal  vi vj   Î´ij  projection Ï†xi âˆˆf original images normalized scale orientation  Â·Â·Â·  vd leads mapping euclidean eyes aligned position  space rd through                                      facial areas cropped ï¬nal image                                                     matching size each cropped image experi      yi    vÏ†xi   vÏ†xi  Â·Â·Â·   vdÏ†xi     ments  Ã—  pixels  gray levels pixel  look vi âˆˆfi      Â·Â·Â· helps   httpcvcyaleeduprojectsyalefaces  yii  Â·Â·Â· preserve local geometrical discrim yalefaceshtml  inant structure data manifold typical scenario httpwwwclcamacukresearchdtg   rn  rÎ¸ dnÎ¸                           attarchivefacesataglancehtml                                                    ijcai                                                     figure  sample face images yale database each subject  face images different lighting  conditions facial expression                         table  recognition accuracy different algorithms yale database                        method       train        train       train       train                        baseline                                 eigenfaces                                       fisherfaces                                        mfa                                              lsda                        each image represented dimensional vec reaches best performance âˆ’  dimen  tor image space preprocessing dif sions property shows lsda does suffer  ferent pattern classiï¬ers applied face recogni problem dimensionality estimation crucial  tion including nearest neighbor turk pentland  problem subspace learning based face recog  bayesian moghaddam  support vector machines nition methods  phillips  paper apply nearest neighbor  classiï¬er simplicity experiments number  face recognition orl database  nearest neighbors taken  parameter Î± orl olivetti research laboratory face database  estimated leave cross validation          used test consists total  face images    short recognition process three steps total  people  samples person images  calculate face subspace training set face im captured different times different variations  ages new face image identiï¬ed projected including expressions open closed eyes smiling non  ddimensional subspace ï¬nally new face image smiling facial details glasses glasses images  identiï¬ed nearest neighbor classiï¬er              taken tolerance tilting rotation                                                        face  degrees  sample images individual    face recognition yale database                displayed figure  each individual     im  yale face database constructed yale center ages randomly selected training rest used  computational vision control contains  grayscale testing  images  individuals images demonstrate variations experimental design each                                                              lighting condition leftlight centerlight rightlight fa given  average results  random splits best  cial expression normal happy sad sleepy surprised result obtained optimal subspace corresponding  wink withwithout glasses figure  shows sam dimensionality each method shown table   ple images individual                           seen lsda algorithm performed best    each individual     images randomly cases fisherface method performed com  selected training samples rest used test paratively lsda size training set increases  ing training set used learn face subspace optimal dimensionality obtained lsda  ing lsda eigenface fisherface methods recog fisherface lower obtained eigenface  nition performed subspaces repeated  discussion  process  times calculate average recognition                                                        experiments standard face databases  rate general recognition rates varies dimen                                                        systematically performed experiments  sion face subspace best performance obtained                                                        vealed number interesting points  algorithms corresponding dimensionality  optimal subspace shown table  baseline  three algorithms lsda mfa fisherface  method simply performed face recognition original performed better optimal face subspace  dimensional image space note upper bound  original image space indicates dimensionality  dimensionality fisherface âˆ’  reduction discover intrinsic structure face  number individuals duda et al                 manifold improve recognition rate    seen algorithm outperformed three  experiments lsda algorithm consistently  methods eigenface method performs worst   outperformed eigenface fisherface mfa meth  cases does obtain improvement baseline ods especially size training set small  method interesting note lsda signiï¬cantly outperformed fisherface  training samples each individual best perfor probably fact fisherface fails accu  mance fisherface longer obtained âˆ’   rately estimate withinclass scatter matrix  dimensional subspace dimensional subspace lsda small number training samples                                                    ijcai                                                     
