                  reinforcement learning pomdps resets               eyal evendar                   sham kakade                    yishay mansour      school science     information science      school science           telaviv university           university pennsylvania           telaviv university         telaviv israel            philadelphia pa              telaviv israel           evendposttauacil          skakadelinccisupennedu          mansourposttauacil                        abstract                          contain asymptotic results general pomdps                                                        guarantee average reward agent near      consider realistic reinforcement learn optimal limit      ing setting agent starts unknown      environment pomdp follow         technical difï¬culty currently      continuous uninterrupted chain experience  general results belief state tracking approximate      access â€œresetsâ€ â€œofï¬‚ineâ€ simula    model showing divergence belief state does      tion provide algorithms general connected eventually occur crudely issue belief state      pomdps obtain near optimal average reward   tracked approximate manner impor      algorithm present convergence rate   tant show approximation quality does contin      depends exponentially certain horizon  ually degrade time â€” agent eventu      time optimal policy dependence  ally loose track belief state inï¬nite horizon      number unobservable states main  course issue mdp current state      building block algorithms implemen  observable boyen koller  address issue      tation approximate reset strategy approximate belief state tracking setting      show exists pomdp   model known perfectly goal compact      ing aspect algorithms use representation belief state note approximate      strategy balancing exploration exploita lief state tracking simpler agent acting      tion                                             ï¬xed ï¬nite horizon bound error                                                        accumulation function horizon                                                          present new algorithms learning pomdps    introduction                                       guarantee agent obtain optimal average  address problem lifelong learning partially ob ward limit furthermore provide ï¬nite time  servable markov decision process pomdp consider vergence rates algorithms expo  general setting agent begins unknown nential dependence certain horizon time optimal  pomdp desires obtain near optimal reward strategy dependence number states  setting agent forced obey dynamics envi pomdp result reminiscent trajectory tree  ronment general permit resets     algorithm kearns et al  similar depen    problem lifelong learning studied dencies assumed access generative  observable mdps kearns singh  provide model allowed simulation pomdp given  algorithm ï¬nite polynomial time guarantees plethora complexity results literature planning  agent obtains near optimal reward unfortunately pomdps lusena et al  feel depen  algorithm applicable challenging dencies best hope general  pomdp setting fact guarantees litera setting  ture learning limit mdps apply pomdps central algorithms implementation ap  reasons essentially partially observabil proximate reset strategy homing strategy idea  ity                                                  reset strategy new literature â€” homing sequences    pomdps problem balancing exploitation used learning deterministic ï¬nite automata  exploration received little attention literature rivest schapire  sequence pro  â€” typically results pomdps planning vided exact resets agent follows homing strat  example sondik  lovejoy hauskrecht egy order approximately reset show   cassandra  existing learning strategy exists ï¬nite convergence  algorithms parr russell  peshkin et al rates depend characteristic time takes approx   assume goal state assume reset button imately reset note existence strat  fact best knowledge literature does egy does imply strategy usefulthe reason agent actions reset  homing strategies  better spent exploring exploiting  turns algorithms use homing strategy clearly having action resets agent des  exploring exploiting fact use homing ignated state useful allow test  strategies inï¬nitely unfortunately detracts compare performance various policies starting  exploiting able show ratio start state general action  time homing strategies used compared time disposal  spent exploiting decreasing sufï¬ciently rapidly instead algorithms utilize approximate reset  near optimal average reward obtained          show exists subtle points                                                        designing reset select actions    preliminaries                                      achieve approximate reset approximate reset  partially observable markov decision process pomdp through use homing strategy                                                        homing agent exploring exploiting sec  deï¬ned ï¬nite set states initial state set  actions set observations output model ond moving ï¬xed state homing strategy  qo rs probability observing hope ï¬xed unknown belief                                                        state shall pomdp  reward performing action state assume                                         âˆˆ   set transitions probabilities  periodic stopping time random variable   sa transition probability implement randomized stopping time introduce ï¬c  forming action deï¬ne rs expected reward titious â€™stayâ€™ actions agent does ac  qÂ·s performing action state tion period mean homing strategy     history   sequence  actions rewards   decides â€™stayâ€™ action time â€”  observations ï¬nite length        possible true pomdp does permit â€™stayâ€™ actions                                                        â€”  agent just ignores â€™stayâ€™ action obtains   rt ot strategy policy  pomdp    deï¬ned mapping  histories  action homing strategy execute  actions deï¬ne  belief state distribu agent taken homing actions  tion states given initial belief state let real â€™stayâ€™ actions agent taken âˆ’ real actions                                                       pomdp stay actions deï¬ne approx  prhb     prr     rt ota      probability observing sequence rewardobservations imate reset strategy     rt ot performing actions       each  strategy Ï€   deï¬ne thorizon ex   deï¬nition   kapproximate reset homing                                            Ï€           strategy belief states  pected reward  belief state rt               pt                                          kheb  âˆ’ hebk  â‰¤   heb expected  tehâˆ¼Ï€  rsi aib  tmarkov strategy  strategy depends observations belief state reached homing actions  optimal tmarkov strategyâ€™s expected return initial real actions taken hb random                        âˆ—                               variable heb  ehbâˆ¼hbhb  lief state deï¬ned rt    assumption make pomdp                                                          deï¬nition states approximately  nected states exists strategy Ï€                                                        reset approximation quality poor  reach positive probability starting                                                        show amplify accuracy approximate hom  make ergodicity assumptions strategies                                                        ing strategy show approximate homing  deï¬nition nonstationary note pomdp discon                                                        strategy exists  nected best statement hope obtain  optimal average reward connected compo                                                        lemma   suppose  approximate reset  nents                                                                                             connectivity implies exists strategy Ï€âˆ—   approximate reset consec  maximizes average reward formally exists utively implements  times furthermore implies     âˆ—                                Ï€âˆ—                exists unique belief state bh hebh    Ï€ limtâ†’âˆ rt exists  does depend denote râˆ— ii bh             âˆ—                Ï€  Ï€  â‰¥ limtâ†’âˆ  sup rt     exist Ï„ â‰¥ Ï„         proof proof standard contraction argument                                                        use induction   claim follows deï¬nition                     âˆ—    Ï€âˆ—                                              âˆ’         âˆ’          âˆ’                   âˆ’ rt  â‰¤                     assume khe  âˆ’   bk â‰¤     let                                                          lâˆ’                 lâˆ’                                                                              qs âˆ’  refer Ï„ horizon time optimal strategy âˆ’                      essentially Ï„ timescale optimal strategy qs â‰¤   arbitrary state  using fact  achieves close average reward    say restart tmarkov strategy Ï€ randomizing stopping times allowing  belief state mean speciï¬cally reset â€™stayâ€™ actions state transition matrix periodic  tory  âˆ… run Ï€ starting state distributed stationary distribution exist states deterministically  according                                       alternate states  linear operator                                                          input    kh  approximate reset strategy      kh  âˆ’                                   âˆ                                                      exploration phase           kh   âˆ’                                                                                                                                       logt Î t                                                                     t            qs âˆ’ qshesk                                                             foreach policy Ï€ Î t                                                                                                                                                                                      qs âˆ’ qshes                              run Ï€ steps                                                                  repeatedly run logt times                           âˆ’ sh âˆ’ sk               end                                                         let vÏ€ average return Ï€                                                                                                                             trials               qs âˆ’ qs                           end                                                           exploitation phase                                                                  âˆ—               Ï€       â‰¤                                                    let Ï€Ë†t  arg maxÏ€âˆˆÎ t                                                                                                                                  kt    current time  ï¬rst term  distributions          t                                                                                                      qs âˆ’ qs   âˆ’      vector hs               time  th exploration phase   constant sum used fact                khs âˆ’ hsk â‰¤                                                            deï¬nition                               âˆ—     show  random walk strategy includ        run Ï€Ë†t steps  ing â€™stayâ€™ actions approximate reset strategy repeatedly run logt times  pomdp including periodic ones prior knowl  end                                                          end  edge better approximate reset strategies  disposal                                                         algorithm  policy search  lemma   pomdps random walk strategy                                                                                             âˆ—  ing â€™stayâ€™ actions constitutes  approximate reset homing sequence run Ï€Ë†t  asymptotically                                   strategy â‰¥                      stop homing nonetheless able show                                                        exists algorithm obtains near optimal reward    proof connectivity assumption states                                                        pomdp ratio time spent exploiting vs  exists strategy reaches pos                                                        homing decreases sufï¬ciently fast  itive probability implies random walk  strategy positive probability moving theorem  exists algorithm  state markov chain irreducible fur connected pomdp obtains optimal average reward  thermore performs â€™stayâ€™ actions markov limit probability   chain aperiodic exists unique stationary dis later provide algorithm better convergence  tribution choose time error rate theorem  start simpler pol  convergence  starting states icy search algorithm establishes theorem  linearity expectation error   belief states steps                              policy search                                                          algorithm  takes input  kh approximate reset    reinforcement learning homing                 strategy random walk strategy   provide algorithms demonstrate  crude reset algorithm works phases interleav  nearoptimal average reward obtained different ing exploration phases exploitation phases let start  rates convergence key success algo describing exploration phases let Î t set  rithms use homing sequences exploration tmarkov strategies estimate value policy  exploitation exploration idea each Ï€ âˆˆ Î t ï¬rst resetting running Ï€  time attempt exploration trajectory steps exploration phase consists obtaining estimate                                                         Ï€  implementing reset strategy â€” information return each policy Ï€ âˆˆ Î t each estimate  approximately grounded respect belief state bh consists average trials followed approximate  recall hebh   bh  idea exploration ï¬nd resets                         âˆ—  good tmarkov strategy Ï€Ë†t bh  during exploitation estimates bias variance variance  goal use tmarkov strategy unfortunately just stochastic nature pomdp bias                         âˆ—  guaranteed Ï€Ë†t performs starting bh fact exactly reset bh   steps each time exploit run log t times t error pa   âˆ—  Ï€Ë†t  run homing sequence close bh rameter tth phase ï¬xed               âˆ—  rerun Ï€Ë†t  gradually increase process lemma  expected belief states approach                                                                log t    problem homing wasting time   t close bh  following lemma  exploiting exploring furthermore use shows accurate estimates obtained                                                                                            previous time spent mdp time plus  lemma   phase   logt Î t each                                                      time spent exploration phase  reset consists using homing sequence log  times                                                      factor accounts case time lies  policies Ï€ âˆˆ Î t estimated thorizon reward   Ï€           Ï€        Ï€                               exploration phase immediately   satisï¬es rt bh  âˆ’  â‰¤ t probability greater   âˆ’                                              bound average reward obtained exploita                                                    tion phase let show taverage reward                                                                     âˆ—                     âˆ—    proof let deal bias expected belief    Ï€Ë†t         âˆ—         Ï€Ë†t                                                        policy used rt  satisï¬es rt bh  âˆ’ rt bh  â‰¤ t  states results using homing sequence log t                                                                      probability  âˆ’   lemma  each policy                                  log t                                    times satisfy kb âˆ’ bh â‰¤   â‰¤ t                  Ï€         Ï€                                                        Ï€ âˆˆ Î t rt bh  âˆ’  â‰¤ t probability  straightforward bh belief states                âˆ—                                                         âˆ’      Ï€Ë†t toptimal probabil  kb âˆ’  â‰¤  strategy Ï€ rÏ€b âˆ’       Î tt                                                       âˆ’                             Ï€Ë†âˆ—  rÏ€b   â‰¤  let belief states time bt ity  observed average return                                                                              râˆ—b    bt  result following Ï€ starting exploitation period close probability                                                      âˆ’   observed average return exploitation   respectively linearity expectation follows                                                                                      Ï€Ë†âˆ—                                                   Ï€        good used ï¬nd    kb âˆ’ bh â‰¤  directly implies rt âˆ’                                             Ï€  rt bh  â‰¤                                           average return during exploitation phase                                                                                           âˆ—    variance hoeffding bound choice observed average return Ï€Ë†t   imply average return each policy t close set each exploitation steps number steps  expectation expectation initial state kh logt resets exploitation pe  policy trajectory probability  âˆ’  riod change average reward fraction                                                   âˆ—                                                        during exploitation algorithm uses policy Ï€Ë†t kh logt  highest return exploration phase  previous lemma close policy largest  model based algorithm  turn note guaranteed large return                                                        previous algorithm simplest way demonstrate  executing Ï€Ë†âˆ— steps like                                                    theorem  inefï¬cient testing  exploit longer period time key                                                        tmarkov policies â€” doubly exponential  reset log  times each time run Ï€Ë†âˆ—                                                    polices provide efï¬cient model based  resets close t close bh  unfortunately means                                         âˆ—              algorithm resembles algorithms given kearns  spend kh log t steps each run Ï€Ë†                                                        et al  mcallester singh  exponen  average return kh log t                                                       tial horizon time dependence  like kh log t fraction time spend number states pomdp  resetting note fraction large desire state convergence rate terms Ï„   t small thought guarantee ac horizon time optimal policy section   curate resets                                       terms homing time recall time exists    exploit reset run exploita                                                                           pomdp using random walk policy  tion phase long time overall  average reward comparable average reward theorem  exists algorithm  exploitation phase                              connected pomdp probability greater  âˆ’ Î´  lemma   time phase average reward achieves average reward  close optimal  time  time satisï¬es  pt â‰¥ râˆ—b   âˆ’    average reward number steps pomdp                                            polynomial aok logÎ´ exponential  o    log                      âˆ’                                            probability    Ï„ furthermore computational runtime algorithm    proving lemma let state corollary polynomial ao logÎ´ exponential Ï„  theorem  follows                                                          provide algorithm page explo                                pt          âˆ—  corollary  let t  ri â‰¥ rt bh  âˆ’ ration phase algorithm builds approximate model    kh logt                                          transition probabilities history occurred        probability  âˆ’                                                         starting bh  tth phase builds model    importantly note loss term goes  goes inï¬n respect set tlength histories denote  ity furthermore large phase know   âˆ—                                          âˆ—         ht exploitation phase uses best markov strat  rt bh  approach optimal average reward   âˆ—                                                    egy respect model use homing strategies   independent starting theorem  follows similar previous algorithm  essentially home inï¬nitely ra let  ao note lt â‰¥  ex  tio time spent homing time spent using tstep                                                                log                    ploration phase algorithm takes actions uniformly ran  exploitation policies going  goes  dom steps resets running homing strategy    proof  let show average reward                  âˆ—                                              ri â‰¥ rt bh  t average number histories length exponential  reward obtained tth exploitation phase number tmarkov polices exponential number tlength                                set time exploitation phase t times greater histories                                                      prË† oh    input   kh  approximate reset strategy     let  Â·                                                                                                                         prha ob  prË† ha ob        âˆ                                                âˆ’                                                                                           prhb prË† prË† hb prË†             logt ht                                                                      t                                                                                  t                        times                                              prha ob  lt prha ob                                                          â‰¤                            âˆ’                         run random  steps                                               t                                                              prË†   prhb âˆ’      prhb                       loglt                                                          run            steps                                   t                          t                                                                            lt            prha ob lt         end                                                                                                                                                               Ë†               t                       t       âˆˆ ht âˆˆ âˆˆ                         pra  prhb âˆ’ lt prhbprhb âˆ’ lt             Ë†          proh                                               Ë†              t                            â‰¤                 prha ob â‰¥ lt                          lt                            Ë†              Ë†             prhaob             proh  Ë†   Ë†                                                                                              prhbpra               ï¬rst inequality holds probability  âˆ’                                                                                                       t           end                                           inequality used fact prhb â‰¥ lt        end                                                exploitation policy using dynamic pro                âˆ—      Ë†       compute Ï€Ë†t using prob                gramming model note pomdp equiva                      current time                       lent mdp histories states exploita               t                                                                                        âˆ—                                                       tion phase algorithm uses best tmarkov policy Ï€Ë†t               time  th exploration phase   respect approximate model interleaving                                                      kh  logt homing steps       times          run Ï€Ë†âˆ— steps                                                                    âˆ—                                                      lemma   phase exploitation policy Ï€Ë†t  satisï¬es                                                                     âˆ—          run kh logt steps                   âˆ—         Ï€Ë†t                       t                                                        rt bh  âˆ’ rt bh  â‰¤ tt  lt   t  lt        end                                                                      end                                                 probability  âˆ’               algorithm  model based                    proof sketch observe ignoring histories                                                                                              Ë†         t                                                        view nodes tree prhb â‰¤ lt                                                         return optimal strategy empirical model                                                                     t  t   logl t times times using decreased lt  fact true                                                                                    t    t  empirical frequencies trajectories algorithm history probability bounded lt  lt  return                Ë†                                       each node bounded total number nodes  forms estimates proh bh  just empirical          probability observing conditioned history followed bounded  prove return opti                                                                                                       taking action histories unlikely mal policy empirical model loses t  lt   empirical estimates bad shall tree approximation nodes                                                        histories using backward induction show pol  need accurate estimates prË† oh bh                                                             âˆ—                           histories let ha history followed icy Ï€Ë†t return t  lt  comparison                                                true optimal value starting âˆ’  length                                                        tories base case leaves tlength histories                                                        holds reward encoded through obser                                                                                                                                                                        vations t  lt  ï¬rst error  lemma   phase    logt ht                                   t                   imperfect reset second marginal distribu                                                 each reset consists using homing sequence logl t                                                                          tion error bounded lt lemma  assume                 Ë†                     t  times  prhb âˆ’ prhb â‰¤ lt   induction assumption holds âˆ’  sources                                           t  ha âˆˆ ht prha ob â‰¥ lt  error ï¬rst current estimation error   Ë†                                               marginal distribution immediate reward  proh âˆ’ proh â‰¤ lt  probability                                                       bounded   second errors   âˆ’                                                       lt                                                                                                                                                          previous levels bounded âˆ’  t  lt                                                        induction assumption summing terms completes    proof ï¬rst note probability  âˆ’  induction step                                                          Ë†                    t  history âˆˆ ht prhb âˆ’ prhb â‰¤ lt similarly subsection  exploit long  using hoeffding bound error  proh âˆ’ overall average reward essentially average                                                        ward exploitation period                                                        lemma   time phase average reward    use algorithm approximate homing strategy                      pt          âˆ—                                                        time  time satisï¬es ri â‰¥ rt bh  âˆ’  simply random policy reset  exploration use policy algorithm ott  lt  tkh logt probability                                                              slightly simplify                                     âˆ’ 
