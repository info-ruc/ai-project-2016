                     generalization error linear neural networks                                 empirical bayes approach                               shinichi nakajima   â€ â€¡ sumio watanabe     â€                                       â€  tokyo institute technology            mailbox  nagatsuda midoriku yokohama kanagawa  japan                           nakajimascspititechacjp swatanabpititechacjp                                            â€¡ nikon corporation                        oazamiizugahara kumagaya saitama  japan                      abstract                          neglected proposed hinton van camp                                                         mackay  attias  ghahramani beal       known unidentiï¬able models  paper consider alternative      bayes estimation advantage generaliza subspace bayes sb approach sb approach      tion performance maximum likelihood esti  empirical bayes eb approach parame      mation accurate approximation    ters model regarded hyperparameters      posterior distribution requires huge computational gard parameters layer hyperparameters      costs paper consider empirical bayes analytically calculate marginal likelihood three      approach parameters   layer models consequently      garded hyperparameters sub   ï¬nd hyperparameter value maximizing marginal like      space bayes approach theoretically analyze   lihood computational costs sb approach      generalization error threelayer linear neu posterior distribution approximation      ral networks show subspace bayes ap   mcmc methods ï¬rst paper prove three      proach asymptotically equivalent positive layer linear neural networks sb approach equivalent      jamesstein type shrinkage estimation positivepart jamesstein js type shrinkage estimation      haves similarly bayes estimation typical james stein  clarify generalization      cases                                            error considering delicate situations important                                                        situations model selection problems statistical tests                                                        kullbackleibler divergence true distribution    introduction                                       singularities comparable inverse num                                                                              unidentiï¬able parametric models neural networks ber training samples conclude sb approach  mixture models wide range applica provides good performance bayes estimation typ  tions models singularities parameter space ical cases  conventional learning theory regular sta section  neural networks linear neural networks  tistical models does hold recently generalization brieï¬‚y introduced framework bayes estima  formance unidentiï¬able models theoreti tion eb approach sb approach  cally clariï¬ed maximum likelihood ml estimation described section  signiï¬cance singularities  asymptotically equivalent maximum poste generalization performance importance analysis  rior map estimation generalization error linear neu delicate situations explained section  sb solu  ral networks proved greater reg tion generalization error derived section  dis  ular models dimension parameter space cussions conclusions follow section  section   model redundant learn true distri respectively  bution fukumizu  hand bayes  estimation generalization error neural networks lin  linear neural networks  ear neural networks mixture models proved âˆˆ rm                         âˆˆ rn                                                       let        input column vector    output  regular models watanabe        aoyagi watanabe  yamazaki watanabe  vector parameter vector neural network model                                                        described parametric family maps fÂ·    bayes posterior distribution seldom rm â†’ rn                               exactly realized furthermore markov chain monte carlo           threelayer neural network hidden  mcmc methods used approximation pos just derived variational bayes solution linear  terior distribution require huge computational costs neural networks clariï¬ed generalization error training  alternative variational bayes approach corre error nakajima watanabe  lation parameters parameters clariï¬ed training error  correlation parameters hidden variables nakajima watanabe aunits deï¬ned                                                                                                                                 h                                                                                  qyx               fx   bhÏˆ ahx                      yx                 dxdy                                                                          log yx nyn                                                                                                     ahbh âˆˆ  Ã—    summa  rizes parameters ÏˆÂ· activation function kullbackleibler kl divergence predictive dis  usually bounded nondecreasing antisymmetric nonlin tribution true distribution Â·qxny denotes  ear function like tanhÂ·andt denotes transpose expectation value sets training samples  matrix vector assume output observed                                                         empirical bayes approach  noise subject nn Ïƒ wherendÂµ Î£ denotes  ddimensional normal distribution average vector Âµ         subspace bayes approach  covariance matrix Î£andid denotes dÃ—d identity matrix little information prior distribution  conditional distribution given        eb approach originally proposed cope                                                                            y âˆ’            introduce hyperparameters prior distribution    yx                 âˆ’                     example use prior distribution depends                 exp                                    Ï€Ïƒ                 Ïƒ                 hyperparameter Ï„                                                                                                paper focus linear neural networks ac                           w  tivation function linear simplest multilayer models Ï†w           exp  âˆ’                                                                                        Ï€Ï„         Ï„   linear neural network model lnn deï¬ned                                                                                                                                       Ï„                   fx bbax                   marginal likelihood eq depends inaneb                                                        approach Ï„ estimated maximizing marginal likeli                        aah  Ã— input parameter ma hood slightly different way efron morris   trix bbh  Ã— output parameter akaike  kass steffey  extending idea  matrix transform â†’ tabtâˆ’ does introduce hyperparameters model dis  change map nonsingular Ã— matrix  tribution sb approach eb approach  parameterization eq trivial redundancy ac parameters model regarded hy  cordingly essential dimension parameter space perparameters following sections analyze ver  given                                              sions sb approach ï¬rst regard output                                                       parameter matrix map eq hyperparame                  hm    âˆ’                                                       ter marginalize likelihood input parameter  assume â‰¤ â‰¤ paper       space mip regard input pa                                                        rameter matrix instead hyperparameter    framework learning methods                      marginalize output parameter space mop    bayes estimation                                                       unidentiï¬ability singularities  let   xxn  yyn arbitrary  training samples independently identically taken say parametric model unidentiï¬able map  true distribution qx yqxqyx marginal parameter probability distribution  ditional likelihood model pyx given  toone neural network model eq unidentiï¬able                                                       cause model independent ah bh orvice                                                                              versa continuous points denoting distribution         zy      Ï†w      pyixiwdw                                                        called singularities fisher information                                                        matrix degenerates true model  Ï†w prior distribution posterior distribu                                                        singularities asymptotically affect prediction  tion given                                                        conventional learning theory regular                             n                        Ï†w      pyixiw            models holds hand true model          pwxnyn                                                     zy nxn                  singularities signiï¬cantly affect generalization                                                        formance follows ml estimation extent  predictive distribution deï¬ned average set points denoting true distribution increases  model posterior distribution follows                                                       neighborhoods ï¬‚exibility imitating noises                                                accelerates overï¬tting bayes es     pyx      pyx wpwx dw       timation large entropy singularities increases                                                        weights distributions near true  generalization error criterion generalization perfor suppresses overï¬tting lnns property appears  mance deï¬ned                                   acceleration overï¬tting selection largest singu                                                    lar value components random matrix sb ap              gngx     qxny                                                               proaches lnns property appears jamesstein    linear neural network model toy useful model type shrinkage shown following sections  known reducedrank regression model applications suppression overï¬tting accompanies insensitivity  reinsel velu                              true components small amplitude tradeoffwhich ignored asymptotic analysis proof given appendix  consider situations true model dis independence makes  tinctly singularities paper posterior distribution localized following lemma holds  consider delicate situations kl divergence  true distribution singularities comparable lemma  predictive distribution sb approaches  inverse number training samples nâˆ’which written follows                                                                                  âˆ’  important situations model selection problems              statistical tests ï¬nite number samples follow pyx  Ï€ vË†                                                                                             ing reasons ï¬rst naturally exist true compo            âˆ’                                 nâˆ’                                    vË†                   âˆ’  nents amplitude comparable    Â· exp âˆ’yâˆ’vË† bË†axË†   yâˆ’vË† bË†axË†        smallest largest model selected secondly              selected model involves components essen                                                                           âˆ’  tially affects generalization performance            vË†      Â·denotes determinant                                                        matrix    theoretical analysis                                                        proof predictive distribution written follows    subspace bayes solution                                                                                                                Ï„            pyx pyx abpaxny nb    distinguish hyperparameter                                    parameter example pyx wÏ„ assume                        âˆ— âˆ—                                                              âˆ qyx exp  bË†aË† âˆ’            variance noise known equal unity                                 paxny nb  conditional distribution lnn mip version sb  approach given                                  Â·p denotes expectation value distribution                                                                    âˆ— âˆ—       âˆ’                                 y âˆ’ bax             bË†aË† âˆ’      sb approaches    yx ab                âˆ’                               Ï€ exp                       expand predictive distribution follows                                                                               use following prior distribution                yx nyn âˆ yx      yt bË†aË† âˆ’ bâˆ—aâˆ—                                                                                                                                                                                                                 tr                                                   Ï†a            exp  âˆ’                                            vv        âˆ’                  Ï€hm                                                                                                                                                         paxny nb                                 pyx ba    Ï†b  note similarly prepare                          âˆš  mop version assume true conditional  nbË†aË†âˆ’ bâˆ—aâˆ—x ndimensional vector              yx aâˆ—bâˆ—        bâˆ—aâˆ—  distribution             true map  order calculating expectation value expanding           hâˆ—  â‰¤                âˆ—  rank        wedenoteby      true value logarithm eq arrive lemma  qed  parameter hat estimator parameter comparing eq ml estimator              Ë† Ë†  example bh simplicity assume                                                                                                    âˆ’  input vector orthonormalized xx qxdx  im           bË†aË†           Ï‰  Ï‰  rq                                                                        mle    bh bh                consequently central limit theorem leads following  equations                                        baldi hornik  ï¬nd sb estimator                                   âˆ’                 âˆ’            each component asymptotically equivalent positive       qx           xx   im                             i                                js type shrinkage estimator virtue             âˆ’         âˆ—  âˆ—      âˆ’    rx        yx            lemma  substitute model sb estimator  qxn     Ã—   symmetric matrix    predictive distribution asymptotically insigniï¬  rxnyn  Ã—  matrix abbreviate impact generalization performance  qxn  qandrxnyn                         conclude sb approach asymptotically equivalent    let Î³h hth largest singular value matrix shrinkage estimation note variance prior  rqâˆ’  Ï‰                                             distribution eq asymptotically effect pre           ah corresponding right singular vector  Ï‰                                                     diction generalization performance far   bh corresponding left singular vector ï¬nd                     âˆ—                         âˆ’     positive ï¬nite constant thedegreeofshrink  eq Î³h hâ‰¤     order                                                          age remember modify theorems  combining eq                                                        paper ml estimation letting      Ï‰  rqÏ    Ï‰    nâˆ’       hâˆ—  hâ‰¤        bh       bh                          âˆ’âˆ  Ïâˆˆ       âˆ  arbitrary constant  generalization error  sb estimator deï¬ned expectation value sb using singular value decomposition true map  posterior distribution given following theorem bâˆ—aâˆ— transform arbitrary aâˆ— bâˆ— change  theorem  let   mip version  map matrix orthogonal row vectors                                  mop version lh maxl nÎ³h sb estimator matrix orthogonal column vectors respec  map lnn given                         tively accordingly assume orthogonalities                             âˆ’         âˆ’      âˆ’           loss generality lemma  implies    bË†aË†       âˆ’  lÏ‰b  Ï‰  rq                              bh                        kl divergence eq set training samples isgiven                                                large scale approximation                                                          âˆ— âˆ—                             similar fashion analysis ml estimation                   b   âˆ’ bË†aË†x     gxnyn                          onâˆ’       fukumizu  second term eq analyt                                                                qx               ically calculated large scale limit hand                                                          âˆ—                  h                                      inï¬nity order deï¬ne following                         xnyn     nâˆ’                                   âˆ—         âˆ—                                                  scalars Î±   âˆ’h  âˆ’h    Î²                                                          âˆ’  hâˆ—n  âˆ’ hâˆ—andÎº    lm    lm  âˆ’  hâˆ—                                                                                                                                                 let  random matrix subject wn                   âˆ— âˆ—t     âˆ—  âˆ—t                                             âˆ’  ghx      tr bhah âˆ’ Ë†bhaË†h bhah âˆ’ Ë†bhaË†h  uun   eigenvalues  measure                                                       empirical distribution eigenvalues deï¬ned                                          Â·  contribution th component tr  denotes       âˆ’                                                           du     Î´     Î´    Â·Â·Â·  Î´     trace matrix denote wdm Î£ Î›                                 dimensional wishart distribution degrees freedom Î´                                    Î£                       Î›                      denotes dirac measure  large scale  scale matrix  noncentrality matrix  abbreviate limit measure eq  converges  wdm Î£ central wishart distribution                         theorem  generalization error lnn sb             âˆ’ umum âˆ’                                                         pudu                     Î¸um uum   du   approaches asymptotically expanded                           Ï€Î±u                                                                     âˆš                   âˆš                         âˆ’      âˆ’                                                                          gnÎ»n                           um     Î± âˆ’  um     Î±    watcher                                                         calculating moments eq obtain fol  coefï¬cient leading term called general lowing theorem  ization coefï¬cient paper given                                                        theorem   generalization coefï¬cient lnn   Î» hâˆ—m     âˆ’ hâˆ—                                                      large scale limit given                  hâˆ’hâˆ—                                                                                                           âˆ—        âˆ—                                                               âˆ—            âˆ—    âˆ’  âˆ’                        Î¸Î³h    âˆ’   Î³h          Î»    âˆ’                                         Î³h                                                      Ï€Î±                                          qÎ³                                                                                                                                                                                                   jstâˆ’ ÎºjstÎº  jst âˆ’                                                      Î¸Â· indicator function event Î³h  hth largest eigenvalue random matrix subject                 âˆ—                                                         wnâˆ’hâˆ—   âˆ’  inâˆ’hâˆ—  Â·qÎ³ denotes ex                          âˆ’                                                         jsÎ±âˆ’s     âˆ’ cos     pectation value distribution eigenvalues           âˆš                                                             jsâˆ’    Î±   âˆ’ Î±cosâˆ’   proof according theorem  difference                              âˆš  sb ml estimators true component posi                            âˆ’  Î±  Î±s Î±                              âˆ’                                           âˆ’  âˆ’ Î±cos        âˆš           tive singular value order  furthermore gen                            Î±s   Î±  Î±  eralization error ml estimator component js âˆ’  regular models identiï¬abil ï£±  âˆš                          âˆš                                                               âˆš                               Î±Î±sÎ±                                                            ï£´    Î± âˆš  âˆ’s   âˆ’    âˆ’s  Î±    âˆ’    âˆš  ity eq obtain ï¬rst term eq ï£²    Î±sÎ±   cos    âˆ’Î± cos  Î±s Î±Î±  contribution ï¬rst hâˆ— components                                                                                         Î±         hand ï¬nd eq theorem  redun ï£´                                                            ï£³     âˆ’s âˆ’   âˆ’               Î±  dant component identifying rqâˆ’ affects sb         cos                                        nâˆ’                                                         âˆš                 estimator order   does affect                             âˆ’                                                      st max   Îº âˆ’   Î± Î±  Ï€Î±Î²  generalization coefï¬cient say general âˆ’  diagonalized matrix Ã— matrix singular Â· denotes inverse function js                        â„¦ uâ„¦          â„¦      â„¦  value decomposed       awhere           delicate  Ã—  Ã— orthogonal matrices respectively        situations  let general diagonalized matrix randd ordinary asymptotic analysis considers situations  âˆ’ hâˆ— Ã— âˆ’  hâˆ— matrix created removing ï¬rst amplitude each component true model  hâˆ— columns rows ï¬rst hâˆ— diagonal zero distinctlypositive theorem  holds  elements correspond positive true singular value situations mentioned paragraph  components d consists noises d section  important consider delicate situations                                                                    âˆ—  âˆ—  general diagonalized matrix nâˆ’rwherer true map bâˆša tiny nonnegligible singular values         âˆ—           âˆ—                                                 âˆ—  âˆ’   Ã— âˆ’   random matrix elements  nÎ³h âˆ theorem  holds situa                                         t                                                         âˆ’  independently subject sothatr subject tions replacing second term eq                    âˆ—                                               âˆ—        âˆ—  âˆ’     âˆ—   nâˆ’h            nâˆ’h   redundant components  regard number ofâˆšdistinctlypositive true singu         âˆ’                                                            âˆ—âˆ’  imitate    using theorem  eq lar values Î³h  loss general  obtain second term eq contribution ity assume bâˆ—aâˆ— nonnegative general diagonal  âˆ’ hâˆ— components complete proof matrix diagonal elements arranged nonincreasing  theorem  qed                                   order let râˆ— true submatrix created removing                                                                                               sbmip                                               sbmip                                 sbmop                                            sbmop                                     ml                                                 ml                                      bayes                                                regular                                  regular                                                                                                                                                                                                       lambda                                         lambda                                                                                                                                                                                                                                                                                                                                         sqrtn gamma              figure  generalization error                   figure  delicate true components        hâˆ—                     bâˆ—aâˆ—        d                    ï¬rst   columns rows      deï¬nedin                                       sbmip  proof theorem  general diagonalized matrix                        sbmop   âˆ’                                    t                                     bayes      wherer   random matrix                                 mlregular                         âˆ—          âˆ— âˆ—                               âˆ—  âˆ’     âˆ— nr  subject nâˆ’h          nâˆ’h                   obtain following theorem                             theorem  generalization coefï¬cient lnn    general situations true map bâˆ—aâˆ— delicate   lambda                             âˆš   âˆ—                              singular values   nÎ³h  âˆ given                                                                                      h         hâˆ’hâˆ—                           âˆ—            âˆ—        âˆ—                                                            Î»h    âˆ’h        nÎ³h        Î¸Î³h                            sqrtn gamma                         hhâˆ—                                  figure  singleoutput lnn                                                                     âˆš      âˆ’      Î³âˆ’  âˆ’      Î³Ï‰t nrâˆ—Ï‰      inconsistent proved superior         Î³          Î³  bh       ah                                           qr   ity bayes estimation learning method                                                        use true prior distribution suspicion cleared       Î³ Ï‰    Ï‰         ah  bh th largest singular value consideration delicate situations following  r corresponding right singular vector corre using theorem  numerically calculate sb  sponding left singular vector respectively Â·qr  ml generalization error delicate situa  denotes expectation value distribution  tions true distribution near singularities fig                                                        ure  shows coefï¬cients lnn input    discussions                                         output   hidden units assump                                                        tion true map consists hâˆ— distinctlypositive    comparison ml estimation                component three delicate components singular val               bayes estimation                                                        ues identical each theâˆš null com                                                                                            âˆ—        âˆ—    âˆ—  figure  shows generalization coefï¬cients lnn ponent horizontal axis indicates nÎ³ whereÎ³h  Î³   input  output  hidden units     bayes generalization error deli                                         âˆ—  horizontal axis indicates true rank  verti cate situations previously clariï¬ed watanabe amari  cal axis indicates coefï¬cients normalized param  unfortunately singleoutput lnns  eter dimension given eq lines correspond   figure  shows coefï¬cients  generalization coefï¬cients sb approaches clariï¬ed solnn input units assumption  paper ml estimation clariï¬ed fuku hâˆ— true singular value component  mizu  bayes estimation clariï¬ed aoyagi indicated horizontal axis delicateweseeinfig  watanabe  regular models respec sb approaches property similar bayes  tively results fig  calculated large estimation suppression overï¬tting entropy  scale approximation using theorem  singularities delicate situations  numerically calculated creating samples subject mip worse bayes estimation shows consis  wishart distribution using theorem  tency superiority bayes estimation  results coincide each clude typical cases suppression singularities  hardly distinguish fig  sb mip comparable stronger  approaches provide good performance bayes esti                                                             mation mip greater general solnn regarded regular model view point                                                                                                                                                                                         â†’ âˆˆ ÃŠ  ization coefï¬cient bayes estimation arbitrary hâˆ— ml estimation transform   makes                                                        model linear identiï¬able ml general    regular models normalized generalization coefï¬cient ization error identical regular models  equal leads penalty term akaikeâ€™s solnn property unidentiï¬able models view point  information criterion akaike                  bayesian learning methods shown fig 
