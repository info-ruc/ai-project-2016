                Performance Analysis of Online Anticipatory Algorithms
                     for Large Multistage Stochastic Integer Programs

                               Luc Mercier and Pascal Van Hentenryck
                                           Brown University
                                     {mercier,pvh}@cs.brown.edu


                    Abstract                          is not pertinent for the group but all group members must be
                                                      allocated to the same hotel). Customers must be immediately
    Despite signiÔ¨Åcant algorithmic advances in recent notiÔ¨Åed of acceptance or rejection of their requests, and ac-
    years, Ô¨Ånding optimal policies for large-scale, mul- cepted requests must be satisÔ¨Åed. Accepted requests must
    tistage stochastic combinatorial optimization prob- also be assigned to a speciÔ¨Åc resource at reservation time and
    lems remains far beyond the reach of existing meth- this choice cannot be reconsidered. The goal is to maximize
    ods. This paper studies a complementary approach, the proÔ¨Åt of the served requests which come from different
    online anticipatory algorithms, that make decisions types with different characteristics and arrival frequencies.
    at each step by solving the anticipatory relaxation Online multiple vehicle routing with time windows [Bent
    for a polynomial number of scenarios. Online an-  and Van Hentenryck, 2003] captures an important class of ap-
    ticipatory algorithms have exhibited surprisingly plications arising in transportation and distribution systems.
    good results on a variety of applications and this In these problems, a Ô¨Çeet of vehicles serve clients which are
    paper aims at understanding their success. In par- located in many different locations and place request for ser-
    ticular, the paper derives sufÔ¨Åcient conditions under vice in real-time in speciÔ¨Åc time windows. Clients must be
    which online anticipatory algorithms achieve good immediately notiÔ¨Åed of acceptance or rejection of their re-
    expected utility and studies the various types of er- quests, and all accepted requests must be satisÔ¨Åed. Routing
    rors arising in the algorithms including the antic- decisions however can be delayed if necessary. The goal is to
    ipativity and sampling errors. The sampling error maximize the number of satisÔ¨Åed requests.
    is shown to be negligible with a logarithmic num-
                                                        All these problems share several characteristics. First, they
    ber of scenarios. The anticipativity error is harder
                                                      can be modeled as multistage integer stochastic programs.
    to bound and is shown to be low, both theoretically
                                                      Second, the number of stages is large. In packet schedul-
    and experimentally, for the existing applications.
                                                      ing, time is discrete by nature, and experiments were made
                                                      with 200,000 stages. For the two other applications, time
1  Introduction                                       is continuous but a reasonable discretization of time would
Online stochastic algorithms for solving large multistage require 200 stages. Third, the set of feasible decisions at
stochastic integer programs have attracted increasing interest each stage is Ô¨Ånite. Finally, these applications require fast
in recent years. They are motivated by applications in which decision-making. These characteristics prohibit the use of a
different types of requests arrive dynamically, and it is the priori methods for (Partially Observable) Markov Decision
role of the algorithm to decide which requests to serve and Processes and for Stochastic Programs. Indeed, [Chang et al.,
how. Unlike traditional online algorithms, these applications 2000] and [Benoist et al., 2001] have shown that (PO)MDPs
assume that the uncertainty is stochastic and that distributions do not scale on these applications. Moreover, successful algo-
of the requests are given.                            rithms for 2-stage stochastic optimization, such as the Sample
  Consider the packet scheduling problem from [Chang et Average Approximation method, are shown to require a num-
al., 2000]. A router receives a set of packets at each time step ber of samples exponential in the number of stages [Shapiro,
and must choose which packet to serve. Packets can be served 2006], precluding their use on these applications.
only for a limited time and they are characterized by a value. Interestingly, high-quality solutions to these applications
The goal is to maximize the values of the served packets. The have been obtained by online algorithms that relax the non-
packet distributions are speciÔ¨Åed by Markov models whose anticipativity constraints in the stochastic programs. These
states specify arrival frequencies for the packet type. online anticipatory algorithms make decisions online at a
  Online reservation systems [Benoist et al., 2001] are an- time t in three steps:
other such application. Customers place requests in real time 1. sample the distribution to obtain scenarios of the future;
for some service at a Ô¨Åxed date. The resources are modeled by
a multiknapsack constraint. (Think of tour operators request- 2. optimize each scenario for each possible decision;
ing rooms in hotels for a group: the choice of a speciÔ¨Åc hotel 3. select the best decision over all scenarios.

                                                IJCAI-07
                                                  1979It is clear that this strategy is necessarily suboptimal, even
                                                       Function MakeDecision(st,   Œ≥t)
with many scenarios. However, experimental results have                          1    m        i
                                                        Use Œ≥t to compute scenarios Œæ ...Œæ where Œæ1..t = Œæ1..t
been surprisingly good, especially with the Regret algorithm     ‚ààX
[Bent and Van Hentenryck, 2004; Hentenryck et al., 2006] foreach x   (st) do
                                                                ‚Üê  1   m  O       i
which is an efÔ¨Åcient way of implementing step 2. Our goal  g(x)    m   i=1  (st,x,Œæ )
                                                           ‚Üê
in this paper is to demystify these results by providing a the- xt argmaxx‚ààX (st) g(x)
oretical analysis of these algorithms. Section 2 describes the
model and the algorithm. Section 3 analyses the performance
of the online anticipatory algorithm and isolates two funda- 3 Analysis of the Anticipatory Algorithm
mental sources of error: a sampling error and a quantity called We compare the performance of the anticipatory algorithm
the global anticipatory gap which is inherent to the problem. with the ofÔ¨Çine, a posteriori solution in the expected sense, as
Section 4 shows how to bound the anticipatory gap theoreti- is typically done in online algorithms [Borodin and El-Yaniv,
cally and experimentally. Section 5 analyzes the effect of ap- 1998]. In other words, for the decision process x produced
proximating the optimization problem. Section 6 compares by the anticipatory algorithm, we bound EVC ‚àí E [f (x,Œæ)],
the anticipatory gap to the expected value of perfect informa- which we call the expected global loss (EGL).
tion. Section 7 presents directions for future research.
                                                      3.1  Local and Global Losses
2  Model and Algorithm                                We Ô¨Årst show that the EGL is the sum of the expected losses
We consider Ô¨Ånite stochastic integer programs of the form of the stages.
                                          
                                                      DeÔ¨Ånition 1 Let st be a state. The expected local loss of
   Q =maxE         max   E  ... max   f(x, Œæ)  ,      decision x ‚ààX(st) is deÔ¨Åned as
       x1‚ààX (s1)  x2‚ààX (s2)   xT ‚ààX (sT )
                                                              Œî(st,x)=E   [O(st,Œæ) ‚àíO(st,x,Œæ) |st ] .
where Œæ is a stochastic process, with Œæt being the observation
                                                      Note that conditioning on a state st does not provide any
at time t, (with Œæ1 being deterministic), st =(x1..t‚àí1,Œæ1..t)
                  X                                   information on Œ≥t: when reading an expression of the
is the state at time t, maps states to non-empty subsets of E   |
a Ô¨Ånite set X of decisions (so the max‚Äôs are well-deÔ¨Åned), form [... st], keep in mind that there is uncertainty on
                                                      Œæt+1,...,ŒæT and on Œ≥t,...,Œ≥T .
and f is the utility function bounded by Fmax. We denote
respectively x and Œæ the vectors x1..T and Œæ1..T .    Lemma 1 (Global Loss = Sum of Local Losses) For any
  A decision process is a stochastic process x such that ‚àÄt : decision process x,
xt ‚ààX(st). We can assume that the computation of each xt                          T
requires exactly one random variable Œ≥t. These variables are   EVC  ‚àí E [f (x,Œæ)] =  E [Œî(st , xt )]
independent and independent of Œæ.                                                 t=1
  In practice, decisions cannot be made based on future ob-
                                                      Proof. Let Ct be the random variable O(st,xt,Œæ) and At =
servations. A decision process x is non-anticipative if xt
                                                      E [Ct ‚àí f(x, Œæ)].ThenAT =0and, for t<T,
is a deterministic function of Œ≥1..t and Œæ1..t (that is, if x is
                                                                    E    ‚àí   +1     +1 ‚àí
adapted to the Ô¨Åltration Ft = œÉ(Œ≥1..t,Œæ1..t)). We can rewrite  At =   [Ct  Ct   + Ct     f(x, Œæ)]
the stochastic program as                                         = E [Ct ‚àí Ct+1]+At+1
   Q =max{E    [f(x, Œæ)] | x non-anticip. dec. proc.} .           = E [Œî(st+1,xt+1)] + At+1.
A scenario is a realization of the process Œæ.TheofÔ¨Çine prob- The last equality comes from decomposing and re-assembling
                                                                                                 EVC   ‚àí
lem is the problem a decision maker would face if, in a given amongst all possible values of xt. Finally
                                                      E f x       E C  ‚àí f x      A                    2
state st, the future observations are revealed; we deÔ¨Åne [ ( ,Œæ)] = [ 0   ( ,Œæ)] =  0 .
O                {       |                     }
 (st,xt,Œæ)=max    f(y,Œæ)  y dec. proc.,y1..t = x1..t , 3.2 Decomposition of the Local Loss
   O             {      |                         }
    (st,Œæ)=max    f(y,Œæ) y dec. proc.,y1..t‚àí1 = x1..t‚àí1 We now show that the local loss at a state st consists of a
          =maxO(st,x,Œæ).                              sampling error and the anticipatory gap.
            x‚ààX (st)
                                                      DeÔ¨Ånition 2 The anticipatory gap of a state st is deÔ¨Åned as
Note that these two problems are deterministic.
                                                                    Œîg(st)=   min  Œî(st,x).
  Finally, the expected value of the clairvoyant (EVC )isde-                 x‚ààX (st)
Ô¨Åned as the expected utility of a clairvoyant decision maker, The choice error of x wrt s is deÔ¨Åned as
that is, EVC = E [O(s ,Œæ)]. The problems discussed in the                   t
                   1                                                                ‚àí
introduction all Ô¨Åt in this model: in particular, the utility is  Œîc(st,x)=Œî(st,x)    Œîg(st).
bounded thanks to capacity constraints. The model can also The anticipatory gap is inherent to the problem and indepen-
be generalized to the case in which f(x, Œæ) has Ô¨Ånite Ô¨Årst and dent of the decision process . An equivalent deÔ¨Ånition is
                                                                              x                    
second moments for every x.                                                                       
                                                                                                  
  The anticipatory algorithm studied here is Algorithm   max   E [O(st,x,Œæ)|st] ‚àí E max  O(st,x,Œæ)st .
MakeDecision, parametrized by the number of scenarios   x‚ààX (st)                   x‚ààX (st)
m, whose successive decisions form a non-anticipative pro- This expression shows that this gap can be interpreted as the
cess:                                                 cost of commuting of E and max. We now bound Œîc(st,x).

                                                IJCAI-07
                                                  1980                                                        An important consequence of this theorem is that the sam-
Lemma 2 (Sampling Error) Let xt be computed by the an-
                                                      pling error can be made smaller than some constant a by
ticipatory algorithm using m samples per decision. Let st be       1      2
                                                     choosing m ‚â•  /K log ( /aT |X| F ). [Shapiro, 2006] ar-
a state and x be argmax E [O(st,x,Œæ) |st ] (break ties arbi-                       max
trarily). Then                                        gues that the SAA method does not scale to multistage prob-
                                               
                                             2       lems, because the number of samples to achieve a given accu-
                                   ‚àímŒî   (s ,x)
E [Œî (s ,x ) |s ] ‚â§   Œî (s ,x)exp       c  t     ,    racy grows exponentially with T . The anticipatory algorithm
    c  t  t  t         c  t                 2         only requires m to grow logarithmically with T |X|,which
                 ‚ààX ( )              2œÉ(st,x)
                x    st                               makes it highly scalable. Of course, it only produces high-
where œÉ(st,x) is the standard deviation of O(st,x,Œæ) ‚àí quality decisions when the anticipatory gap is small.
      
O(st,x ,Œæ) given st.
Proof. Here all probabilities and expectations are implicitly 4 Bounding the Global Anticipatory Gap
conditional on st. The left-hand side can be decomposed as This section provides theoretical and experimental results on
                                                     the anticipatory gap, explaining why anticipatory algorithms
       E                          P
         [Œîc(st,xt)] =    Œîc(st,x) (xt = x).          are effective in the applications mentioned in the introduction.
                     x‚ààX (st)
                                                      4.1  Theoretical Proof on Packet Scheduling
Due to the argmax in MakeDecision,theeventxt   = x
                                                                                              GAG
implies ‚àÄx ‚ààX(st),g(x ) ‚â§ g(x). Therefore P (xt = x) ‚â§ We Ô¨Årst show how to compute an upper bound on for a
            
P (g(x) ‚â• g(x )).Sincef    is bounded, O(st,x,Œæ) ‚àí    simpliÔ¨Åed version of the packet scheduling problem. Suppose
                                                                                               1
O(st,x ,Œæ) has a Ô¨Ånite expectation and variance. Now, that there are k types of packets whose values are v <...<
                                                      v  respectively. At each step from 1 to T ‚àí 1, a packet of
                   m                         
        k
                 1                                    type i arrives with probability p . All these random variables
   g(x) ‚àí g(x)=       O(s  ,x,Œæi) ‚àíO(s ,x,Œæi)                                  i
                           t           t              are independent. Each packet has a lifetime of 2, meaning a
                 m  =1
                   i                                  packet received at time t can be scheduled either at time t or
and, by the central limit theorem, this difference is normally at time t +1. The utility is the sum of scheduled packets over
                                     ‚àí
distributed for m large enough, with mean Œîc(st,x) and the T stages. All packets take a single time step to serve. For
        1       2            ‚àºN       2
variance m œÉ(st,x) . Finally,  
 if X (Œº, œÉ ) with Œº<0, convenience, we introduce a packet type 0 with value v0 =0
                        2
                       Œº                              and probability p0 =1. It should be clear that this problem
then P (X ‚â• 0) ‚â§ exp ‚àí  2  (Chernoff bound).     2
                      2œÉ                              satisÔ¨Åes all assumptions above. In particular, the utility is
                                                                ‚â§   ‚â§
3.3  Performance of the Algorithm                     bounded (0  f   Tvk).
                                                        Why is the GAG  small on this problem? We show that
We now assemble the previous results.
                                                      Œîg is rarely high, inducing a small GAG.Forst a state and
DeÔ¨Ånition 3 The Global Anticipatory Gap of the problem is x, y ‚ààX(s ), we say that x dominates y if O(s ,x,Œæ) ‚â•
                    ‚é°                 ‚é§                         t                               t
                                                      O(s ,y,Œæ) almost surely given s . Studying Œî (s ) only re-
                            T                            t                      t           g  t
                    ‚é£                 ‚é¶               quires to focus on non-dominated decisions: there are at most
          GAG   = E   max       Œîg (st ) .
                       x1..T                          two non-dominated decisions for a given state, which consists
                             =
                     xi‚ààX (si) t 1                    of scheduling
Once again, this quantity is inherent to the problem.   ‚Ä¢ the most valuable packet, of type i, received at time t‚àí1
Theorem 1 The expected global loss of the anticipatory al- and not already scheduled; or
gorithm satisÔ¨Åes                                        ‚Ä¢
                                                        the most valuable packet, of type j, received at time t.
                                ‚àí
             EGL  ‚â§ GAG   + O  e Km                   Moreover, if i ‚â• j, then choosing j is dominated, since i is
                                                      more valuable and will be lost if not chosen now. Also, if
where m is the number of samples per decision and
                                                      i<jbut the second most valuable packet received at t is of
                                   2
                           Œîc(st,x)                   type k ‚â• i, then choosing i is dominated. If one of these two
             K  =min               2 .                conditions holds, a decision dominates all the other ones, and
                  st,x‚ààX (st) 2œÉ(st,x)
                  Œîc(st,x)>0                          thus Œîg(st)=0.
Proof. We have                                          Suppose now that st does not satisfy any of them. By the
                                                      dominance property, scenarios can be partitioned into those
                                                 

      T               T                             where scheduling i (resp. j) is the unique ofÔ¨Çine, optimal de-
EGL       E Œî s  x   ‚â§    E  Œî  s     E Œî   s x
    =      [  ( t , t )]    [ g ( t )] + [ c( t , t )] . cision and those on which there is a tie. Introduce the random
      t=1              t=1                            variable y , taking values i, j or ‚ä• in these three respective
        GAG                                                   t
The term     comes from                           cases. We then have
T                T                   T
                                                       Œî(s ,i)=E  [O(s ,Œæ) ‚àíO(s ,i,Œæ) |s ]
   E [Œîg(st)] = E    Œîg(st)  ‚â§ E  max     Œîg(st) ,        t           t         t      t
                                  x1..T
t=1               t=1                  t=1                    = E [O(st,Œæ) ‚àíO(st,i,Œæ) |st,yt = j ] P (yt = j)
and the global sampling error satisÔ¨Åes                and symmetrically
         T
                                                      Œî(st,j)=E  [O(st,Œæ) ‚àíO(st,j,Œæ) |st ]
                                     ‚àíKm
           E [Œîc(st,xt)] ‚â§ 2T |X| Fmaxe   .
                                                 2            = E [O(st,Œæ) ‚àíO(st,j,Œæ) |st,yt = i] P (yt = i) .
        t=1

                                                IJCAI-07
                                                  1981Now, if i is scheduled and the optimal ofÔ¨Çine solution was P (O(st,xt,Œæ)=O(st,Œæ) |st ) the consensus rate.This
to schedule j, then the loss cannot exceed vj ‚àí vi, since the quantity can be estimated easily during the computation of
rest of the optimal ofÔ¨Çine schedule is still feasible. Hence an anticipatory algorithm. It sufÔ¨Åces to count how many sce-
E [O(st,Œæ) ‚àíO(st,i,Œæ) |st,yt = j ] ‚â§ vj ‚àí vi. Moreover, narios make the same decision, i.e.,
for the optimal ofÔ¨Çine schedule to choose j at time t,itis                                      
                                                          1   ‚àà{         } O      i   O        i  
necessary to have a packet of value greater than vi arriving at i 1,...,m   (st,x,Œæ )=   (st,xt,Œæ ) .
            P        ‚â§   ‚àí                    ‚àí          m
t +1and thus (yt = j)  1    k>i qk where qk =1  pk.
Finally, we Ô¨Ånd                                       [Hentenryck et al., 2006] kindly gave us some of these statis-
                                                    tics on online reservation systems: they depict the consensus
                                 
                                                      rate (min/max/avg) as a function of the number of scenarios.
          Œî(st,i) ‚â§ (vj ‚àí vi) 1 ‚àí   qk  .
                                                                                        Max
                                 k>i                                100
                                                                    80                Average
The other case is harder to study, but a trivial upper bound is
                                                                    60
        ‚â§                                                                                Min

Œî(s ,j)   v . Now it remains to bound the expectation of            %  Agreement
   t       i                                                        40
Œîg(st) = min(Œî(st,i), Œî(st,j)) by enumerating the possi-
ble values of i and j and weighting each case with its proba-       20
                                                                     0        m  (#Scenarios / decision)
bility of occurrence. This weight is, in fact, bounded by the         02550          75100
product of the probabilities of the 3 following events:
                                                      On this class of instances, there are 6 possible decisions in
  ‚Ä¢ a packet of type i arrived at time t ‚àí 1;         each state. Therefore, one could expect an average consen-
  ‚Ä¢ the most valuable packet arrived at t is of type j; sus rate of 20%, but it is actually much higher, at about 80%.
                                                      Moreover the maximal ofÔ¨Çine loss of a bad decision can eas-
  ‚Ä¢ no packet of type i ‚â§ k<jarrived at time t.
                                                      ily be bounded in this problem. By Markov inequality, the
Here is a numerical example. Suppose there are 4 types of GAG is low. Similar observations were made in the packet
packets, with the following values and emission probabilities: scheduling problem, where the measured average consensus
            type  0   1     2    3    4               was about 90%, and in the vehicle routing problem, where the
           value01248                                 rate varies among the stages, exhibiting an increasing trend
           prob   1.  .60  .30  .20  .10              from 65 to 100%. This argument, however, would be useless
                                                      when Fmax is high, e.g. problems with high penalty states.
The upper bound on Œîg depending on i and j,is           More generally, the following theorem gives a way to mea-
             1                                        sure the anticipatory gap of a state.
          (j) 2  .496                                                                
                                                      Theorem 2  Let st be a state. DeÔ¨Åne Œîg(st) as
             3   1.00  .560                                                                         
             4   1.00  1.68  .400                          m                           m
                                                        1                     i                     i
                  1234(i)                                       max  O(st,x,Œæ ) ‚àí  max     O(st,x,Œæ )  ,
                                                       m       x‚ààX (st)           x‚ààX (st)
                                                            i=1                         i=1
We Ô¨Ånd that E [Œîg(st)] ‚â§ .125. On the other hand, a simple
lower bound on the EVC is the expectation of the value of the then this is a strongly consistent estimator of Œîg(st), i.e.,
                                                                                          
most valuable packet arriving at each stage multiplied by the                            
number of stages. In this case, that leads to EVC ‚â• 2 25T .                             
                                             .               P    lim  Œîg(st)=Œîg(st)      st =1.
As a result, the ratio of GAG over EVC on this problem is       m‚Üí+‚àû
less than 5.55%. Because this analysis is not tight ‚Äì espe- Proof. Apply the strong law of large numbers to O(s ,x,Œæ)
cially the lower bound of the EVC ‚Äì, the anticipatory algo-                                        t
                                                      for all x and conclude with the Ô¨Åniteness of X (st). 2
rithm is likely to have an even better expected global loss. 
This analysis also hints at why online anticipatory algorithms Œîg can be computed by the online anticipatory algorithm at
are so effective on packet scheduling and why they outper- no additional cost.
form competitive algorithms on these instances.
                                                      5   Approximating the OfÔ¨Çine Problem
4.2  Discussion on Practical Problems                 Theorem 1 explains why the anticipatory algorithm provides
The previous section shows how to bound the GAG on a par- good results when the GAG is small. However, practical im-
ticular problem: study dominance properties between the de- plementations use a variant of Algorithm 1 in which O is re-
cisions, bound the loss of making a non-optimal (in the ofÔ¨Çine placed by a fast approximation O. This is the case for the
sense) decision, and bound the probability of this event. We three applications mentioned earlier which use an approxi-
are currently applying this method on more complex prob- mating technique called Regret [Bent and Van Hentenryck,
lems but proofs quickly become very cumbersome. As an 2004; Hentenryck et al., 2006].TheRegret algorithm can be
alternative, we discuss another, empirical way to argue that seen as a discrete version of sensitivity analysis: instead of
the GAG of a problem is small.
                                                      computing O(st,x,Œæ) for each for each x ‚ààX(st),theidea
  We have emphasized in the theoretical discussion the             ‚àó            O       i
                                                      is to compute x =argmaxx(   (st,x,Œæ  )) Ô¨Årst and
 then to
importance of bounding the probability that the cho-                                           i
                                                      compute a vector of approximations O(st,x,Œæ )
sen decision is not a  posteriori optimal. We  call                                               x‚ààX (st)

                                                IJCAI-07
                                                  1982using x‚àó. Each entry in this vector is derived by approx-
imating the loss of selecting a speciÔ¨Åc x in X (st) instead
of the optimal decision x‚àó. As a result, the Regret algo-
                                           i
rithm ensures (i) O‚â§Oand   (ii)maxx(O(st,x,Œæ  )) =
              i
maxx(O(st,x,Œæ )).See[Bent et al., 2005] for a discussion
on the complexity of computing this approximated vector.
  It is not easy to provide tight theoretical bounds on the
expected global loss for the Regret approximation. We    Figure 1: low EVPI but high Global Anticipatory Gap
thus measured the empirical distribution of O‚àíO on on-
line stochastic reservation systems from [Hentenryck et al.,         
                                                     Proof.    Let x   be  a decision process maximizing
2006]. The difference O‚àíO is zero in 80% of the cases and                                     
                                                      E [f(x, Œæ)|st]. Then for all Œæ, O(st,Œæ) ‚â• f(x ,Œæ), and, as
its mean is very small (around .2 while the typical values of                                  
                                                      E is non-decreasing, E [O(st,Œæ)|st] ‚â• E [f(x ,Œæ)|st)] =
O are in the range [400,500]), although it can occasionally be            Œæ
                                                      œÜ(st). Inversely, let x be a decision process maximizing
large. This intuitively justiÔ¨Åes the quality of Regret, whose      Œæ
                                                      f(x, Œæ) with x     =  x1  ‚àí1.DeÔ¨Ånex    by aggrega-
expected global loss is not signiÔ¨Åcatively different from the      1..t‚àí1     ..t
                                                                                  Œæ
anticipatory algorithm for the same sample size.      tion: x does the same thing as x on the scenario Œæ.Then
                                                           ‚â• E        |    E  O      |                2
  Finally, recall that, on online reservation systems, the œÜ(st) [f(x ,Œæ) st]= [ (st,Œæ) st].
                                                                                             EVPI
consensus rate is very high on average. Let x  =      In two-stage stochastic programming, a low makes
          m          i                                the problem much easier because an optimal decision is also
argmax     =1 O(st,x,Œæ ) and let the consensus rate be
       x  i                                           a good one for each speciÔ¨Åc scenario [Birge and Louveaux,
Œ±. By properties (i) and (ii) of Regret, the approximated
         m                                            1997, ch. 4]. However, this is no longer true in the multistage
‚Äúscore‚Äù     O(s ,x,Œæi) of decision x may only exhibit
         i=1   t                                      case. Consider the three-stage problem depicted in Figure 1.
errors in (1 ‚àí Œ±)m scenarios and hence will be very close to
  Œ±m         i                                       Black dots represent decisions variables x1 and x2. Stochas-
   =1 O(st,x ,Œæ ). Moreover, other decisions have an ap-
  i                                                   tic variables Œæ1 and Œæ2 have no inÔ¨Çuence and are not repre-
proximated score where (almost all) the terms of the sum has
                                                      sented. The white dot represents Œæ3 which take values 0 and
a negative error. Therefore the approximated decision is bi-
                                                      1 with equal probability. Leaves are tagged with their utilities
ased toward consensual decisions and a high consensus rate
                                                      and  is large positive number. The value of the EVPI and
                                O‚àíO                      a
tends to hide the approximation errors of Regret.     the anticipatory gap Œî for each state are the following:
  In summary, a high consensus rate not only makes the AG                g
small but also allows Regret to produce decisions close in         state root state  x1 =0
                                                                                   1
quality to the exact anticipatory algorithm. This does not         Œîg        0      /2(Œµ + a)
mean that a brutal Regret approximation, e.g., assigning zero       Œ∑Œµ1/2(Œµ             + a)
to each non-optimal decision, would be as effective [Bent and On this problem, the EVPI is Œµ: an optimal solution has a
Van Hentenryck, 2004].                                score of Œµ, whatever the scenario. The expected value of the
   GAG           EVPI                                 optimal policy is zero. However, the online anticipatory algo-
6        Versus                                       rithm always chooses x1 =0and thus has an expected utility
This section studies the relationships between the anticipatory of 1/2(Œµ ‚àí a). Therefore anticipatory algorithms may behave
gap and the expected value of perfect information (EVPI ). poorly even with a low EVPI . Moreover, in this case, the
Since these concepts are seemingly close, it is useful to ex- inequality of Theorem 1 is tight when m converges to +‚àû,
plain how they differ and why we chose to introduce the no- since the GAG equals 1/2(Œµ + a).
tion of anticipatory gap.                               The phenomenon comes from the fact that the EVPI of
  Consider the following two maps assigning values to the problem is low although the EVPI of the node (x1 =0)
                                                          ‚àí 1 2  ‚àí      1 2
states: the ofÔ¨Çine value and the online value of state st,re- is Œµ / (Œµ a)= / (Œµ + a) and thus much larger. This
                                                                                                  [
spectively denoted by œÜ(st) and œÄ(st) and deÔ¨Åned by   does not contradict the super-martingale property of Demp-
                                                      ster, 1998] because Dempster considers optimal decision pro-
            {E       |   |           }
 œÜ(st)=max     [f(x, Œæ) st] x dec. proc.              cesses, which is not the case of anticipatory algorithms. As
 œÄ(st)=max{E   [f(x, Œæ)|st] | x non-anticip. dec. proc.} . a result, the expected global loss of the anticipatory algo-
                                                                                      EVPI
             ‚â•                                        rithm cannot be bounded by the root   . The example
Note that œÜ(st) œÄ(st) for all state st. The difference may suggest that the maximum of the EVPIs at each node of
                                                                                    EGL
               Œ∑(st)=œÜ(st)  ‚àí œÄ(st).                  the tree gives an upper bound of the , but this is not true
                                                      either. Figure 2 presents a stochastic program, where ‚ÄòSub‚Äô
                                            EVPI
is the (local) expected value of perfect information ( ). are clones of the problem in Figure 1, with variables indices
The expected value of perfect information of the problem is shifted. On this problem, the optimal solutions to the scenar-
Œ∑(s1), that is, the advantage, in the expected sense, of a clair- ios have an expected expected utility of Œµ, and those of the
voyant over a non-clairvoyant (both with inÔ¨Ånite computa- anticipatory algorithm (with m = ‚àû) have expected utility
tional resources). The next lemma relates the ofÔ¨Çine problem 1 4 ‚àí ;theEGL thus equals 3 4 . By Theorem 1,
                                      E                / ( 3a+Œµ)                    / (a+Œµ)
and œÜ and shows that the operators max and commute for the GAG is not smaller than the EGL (m = ‚àû: no sampling
clairvoyant decision processes.                       error). As a result, the GAG is greater than the maximum of
                                                                                          1
Lemma 3  For any state st, œÜ(st)=E [O(st,Œæ)|st] .     the EVPI over all nodes, which is equal to /2(Œµ + a).

                                                IJCAI-07
                                                  1983