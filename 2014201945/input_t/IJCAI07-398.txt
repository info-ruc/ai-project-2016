              The Value of Observation for Monitoring Dynamic Systems

           Eyal Even-Dar                   Sham M. Kakade                    Yishay Mansourâˆ—
 Computer and Information Science    Toyota Technological Institute     School of Computer Science
     University of Pennsylvania            Chicago, IL, USA                 Tel Aviv University
    Philadelphia, PA 19104 USA               sham@tti-c.org                Tel Aviv, 69978, Israel
      evendar@seas.upenn.edu                                               mansour@cs.tau.ac.il

                    Abstract                          & Kanazawa, 1989] â€”  all of which share the Markov as-
                                                      sumption. Naturally, one would like to provide conditions
    We consider the fundamental problem of monitor-   as to when monitoring is possible when modelling errors are
    ing (i.e. tracking) the belief state in a dynamic sys- present. Such conditions can be made on either the dynam-
    tem, when the model is only approximately correct ics of the system (i.e. the transition between the states) or on
    and when the initial belief state might be unknown. the observability of the system. While the true model of the
    In this general setting where the model is (perhaps transition dynamics usually depends on the application itself,
    only slightly) mis-speciï¬ed, monitoring (and con- the observations often depend on the user, e.g. one might be
    sequently planning) may be impossible as errors   able to obtain better observations by adding more sensors or
    might accumulate over time. We provide a new      just more accurate sensors. In this paper our main interest is
    characterization, the value of observation,which  in quantifying when the observations become useful and how
    allows us to bound the error accumulation.        it effects the monitoring problem.
    The value of observation is a parameter that gov-   Before we deï¬ne our proposed measure, we give an illus-
    erns how much information the observation pro-    trative example of an HMM, where the value of information
    vides. For instance, in Partially Observable MDPs can vary in a parametric manner. Consider an HMM in which
    when it is 1 the POMDP is an MDP while for an     at every state the observation reveals the true state with prob-
    unobservable Markov Decision Process the param-   ability 1 âˆ’  and with probability  gives a random state. This
    eter is 0. Thus, the new parameter characterizes a can be thought of as having a noisy sensor. Intuitively, as
    spectrum from MDPs to unobservable MDPs de-       the parameter  varies from zero to one, the state monitoring
    pending on the amount of information conveyed in  become harder.
    the observations.                                   We introduce a parameter which characterizes how infor-
                                                      mative the observations are in helping to disambiguate what
                                                      the underlying hidden state is. We coin this parameter the
1  Introduction                                       value of observation. Our value of observation criterion tries
Many real world applications require estimation of the un- to quantify that different belief states should have different
                                                                                            L
known state given the past observations. The goal is to main- observation distributions. More formally, the 1 distance be-
tain (i.e. track) a belief state, a distribution over the states; tween any two belief states and their related observation dis-
in many applications this is the ï¬rst step towards even more tributions is maintained up to a multiplicative factor (which
challenging tasks such as learning and planning. Often the is at least the value of observation parameter).
dynamics of the system is not perfectly known but an approx- In this paper we use as an update rule a variant of the
imate model is available. When the model and initial state are Bayesian update. First we perform a Bayesian update given
perfectly known then state monitoring reduces to Bayesian our (inaccurate) model, and then we add some noise (in par-
inference. However, if there is modelling error (e.g. the tran- ticular, we mix the resulting belief state with the uniform dis-
sition model is slightly incorrect), then the belief states in the tribution). Adding noise is crucial to our algorithm as it en-
approximate model may diverge from the true (Bayesian) be- sures the beliefs do not become incorrectly overly conï¬dent,
lief state. The implications of such a divergence might be thus preventing the belief state from adapting fast enough to
dire.                                                 new informative information.
  The most popular dynamic models for monitoring some   Our main results show that if the model is only approxi-
unknown state are the Hidden Markov Model (HMM) [Ra-  mate then our modiï¬ed Bayesian updates guarantee that the
biner & Juang, 1986] and extensions such as Partially Observ- true belief state and our belief state will not diverge â€” assum-
able Markov Decision Process [Puterman, 1994], Kalman Fil- ing the value of observation is not negligible. More speciï¬-
ters [Kalman, 1960] and Dynamic Bayesian Networks [Dean cally, we show that if the initial state is approximately ac-
                                                      curate, then the expected KL-divergence between our belief
  âˆ—Supported in part by a grant from ISF and BSF.     state and the true belief state remains small. We also show

                                                IJCAI-07
                                                  2474that if we have an uninformative initial state (e.g., arbitrary the algorithm; Subsection 3.2 provides the main monitoring
initial belief state) we will converge to a belief state whose theorem; Subsection 3.3 proves the Theorem. In Section 4,
expected KL-divergence from the true belief state is small we show how to extend the results into the dynamic Bayesian
and will remain as such from then on. Finally, we extend our networks.
results to the setting considered in [Boyen & Koller, 1998],
where the goal is to compactly represent the belief state. The 2 Preliminaries
precision and rate of convergence depends on the value of
                                                      An Hidden Markov Model (HMM) is 4-tuple, (S, P, Ob, O),
observation.
                                                      where S is the set of states such that |S| = N, P is the
  One natural setting with an inaccurate model is when the
                                                      transition probability form every state to every state, Ob is
underlying environment is not precisely Markovian. For ex-
                                                      the observations set and O is the observation distribution in
ample, it might be that the transition model is slightly inï¬‚u-
                                                      every state. A belief state b is a distribution over the states
enced by some other extrinsic random variables. Given these
                                                      S such that b(i) is the probability of being at state si.The
extrinsic variables, the true transition model of the environ-
                                                      transition probabilities of the belief states are deï¬ned accord-
ment is only a slightly different model each time step. This
                                                      ing to HMM  transition and observation probability, using a
is a case where we might like to model the environment as
                                                      Bayesian update.
Markovian, even at the cost of introducing some error, due to
                                                        For a belief state b(Â·), the probability of observing o is
the fact that transition model is not entirely Markovian. Our
                                                      O(o|b),where
results apply also to this setting. This is an encouraging re-               
sult, since in many cases the Markovian assumption is more           O(o|b)=    O(o|s)b(s).
of an abstraction of the environment, then the a precise de-                  s
scription.
Related Work. The work most closely related to ours is that After observing an observation o in belief state b(Â·), the up-
of Boyen and Koller (1998), where they considered monitor- dated belief state is:
ing in a Hidden Markov Model. In their setting, the environ-            O       O(o|s)b(s)
                                                                     (Uo b)(s)=
ment is (exactly) known and the agent wants to keep a com-                        O(o|b)
pact factored representation of the belief state (which may
                                                              O
not exactly have a factored form). Their main assumption is where Uo is deï¬ned to be the observation update operator.
that the environment is mixing rapidly, i.e the error contract Also, we deï¬ne the transition update operator T as,
by geometric factor after each time we apply the transition                  
                                                                     P                   
matrix operator. In contrast, we are interested in monitoring      (T b)(s)=     P (s ,s)b(s ).
when we have only an approximate environment model. Both                      s
our work and theirs assume some form of contraction where
                                                        We denote by bt the belief state at time t, where at time 0 it
beliefs tend to move closer to the truth under the Bayesian
                                                      is b . (We will discuss both the case that the initial belief state
updates â€” ours is through an assumption about the value of 0
                                                      is known and the case where it is unknown.) After observing
observation while their is through assumption about the tran-
                                                      observation ot âˆˆ Ob, the inductive computation of the belief
sition matrix. The main advantage of our method is that in
                                                      state for time t +1is:
many applications one can improve the quality of its observa-
                                                                        b      T P U Ob ,
tions, by adding more and better sensors. However, the mix-              t+1 =     ot t
ing assumption used by Boyan and Koller may not be alter-
able. Furthermore, in the ï¬nal Section, we explicitly consider where we ï¬rst update the belief state by the observation up-
                                                                                          o
their assumption in our setting and show how a belief state date operator according to the observation t andthenbythe
can be compactly maintained when both the model is approx- transition update operator. It is straightforward to consider
                                                                                     b
imate and when additional error accumulates from maintain- a different update order. Therefore, t+1 is the distribution
                                                                                      {o ,o ,...o }
ing a compact factored representation.                over states conditioned on observing 0 1  t  and on
                                                                             b
  Particle Filtering [Doucet, 1998] is a different monitoring the initial belief state being 0.
approach, in which one estimates the current belief state by
making a clever sampling, where in the limit one observes the 3 Approximate Monitoring
true belief state. The major drawback with this method is in We are interested in monitoring the belief state in the case
the case of a large variance where it requires many samples. where either our model is inaccurate or we do not have the
A combination of the former two methods was considered by correct initial belief state (or both). Let us assume that an
[Ng et al., 2002].                                    algorithm has access to a transition matrix P and an observa-
                       [                   ]
  Building on the work of Boyen & Koller, 1998 and the tion distribution O, which have error with respect to the true
              [               ]
trajectory tree of Kearns et al., 2002 , McAllester and Singh models. The algorithmâ€™s goal is to accurately estimate the
(1999) provides an approximate planning algorithm. Similar            t                  Ë†b
extensions using our algorithm may be possible.       belief state at time , which we denote by t.
                                                        For notational simplicity, we deï¬ne Eoâˆ¼b = Eoâˆ¼O Â·|b .
  Outline. The outline of the paper is as follows. In Sec-                                           ( )
                                                      When  P and P are clear from the context, we deï¬ne T to
tion 2 we provide notation and deï¬nitions. Section 3 is the              b
main section of the paper and deals with monitoring and is be T P and T to be T P .WhenO and O are clear from the
                                                                               O            Ob
composed from several subsections; Subsection 3.1 describes context, we deï¬ne Uo to be Uo and Uo to be Uo .

                                                IJCAI-07
                                                  2475  Our main interest is the behavior of                3.1  The Belief State Update
                              
                                                      Now we present the belief state update. The naive approach
                  E  KL   b ||Ë†b
                         ( t t)                       is to just use the approximate transition matrix PË† and the ap-
                                                      proximate observation distribution OË†. The problem with this
where the expectation is taken with respect to observation se-
                                                      approach is that the approximate belief state might place neg-
quences {o ,o ,...otâˆ’ } drawn according to the true model,
         0  1      1                                  ligible probability on a possible state and thus a mistake may
   b     Ë†b                         t
and t and t are the belief states at time , with respect to be irreversible.
these observation sequences.                            Consider the following update operator TËœ. For each states
  In order to quantify the accuracy of our state monitoring, s âˆˆ S,
we must assume some accuracy conditions on our approxi-
                                                               (TËœ)(s)=(1âˆ’  U )(TË†)(s)+U Uni(s),
mate model. The KL-distance is the natural error measure.   Uni
The assumptions that we make now on the accuracy of the where   is the uniform distribution. Intuitively, this update
                                                              UËœ
model will later be reï¬‚ected in the quality of the monitoring. operator mixes with the uniform distribution, with weight
                                                      U , and thus always keeps the probability of being in any state
Assumption 3.1 (Accuracy) For a  given HMM   model    bounded away from zero. Unfortunately, the mixture with
(S, P, Ob, O),an(T ,O) accurate model is an HMM     the uniform distribution is an additional source of inaccuracy
(S, P, Ob, O), such that for all states s âˆˆ S,        in the belief state, which our analysis would latter have to
                                                      account for.
                       
            KL(P  (Â·|s)||P (Â·|s)) â‰¤ T                  The belief state update is as follows.
            KL  O Â·|s ||O Â·|s â‰¤     .                                  Ë†b    TËœUË† Ë†b ,
               (  ( )   (  ))      O                                      t+1 =   ot t                (1)
                                                      where Ë†bt is our previous belief state.
  Next we deï¬ne the value of observation parameter.
Deï¬nition 3.1 Given an observation distribution O,letM O 3.2 Monitoring the belief state
be the matrix such that its (o, s) entry is O(o|s).TheValue In this subsection we present our main theorem, which relates
             Î³                       Mx             the accuracy of the belief state to our main parameters: the
of Observation, , is deï¬ned as infx:x1=1 1 and it is
in [0, 1].                                            quality of the approximate model, the value of observation,
                                                      and the weight on the uniform distribution.
  Note that if the value of observation is Î³, then for any two
                                                      Theorem 3.2 At time t let Ë†bt be the belief state updated ac-
belief states b1 and b2,
                                                      cording to equation (1), bt be the true belief state, Zt =
   b âˆ’ b   â‰¥O  Â·|b  âˆ’ O Â·|b   â‰¥ Î³b âˆ’ b   .
     1   2 1      (  1)   (  2) 1      1   2 1        E  KL(bt||Ë†bt) , and Î³ be the value of observation. Then
                                                                                        2
where the ï¬rst inequality follows from simple algebra.                Zt+1 â‰¤ Zt +  âˆ’ Î±Zt ,
  The parameter Î³ plays a critical rule in our analysis. At                     âˆš              Î³2
                                                      where   T   U    N    Î³  O and Î±      2 N .
                Î³            b âˆ’ b     O Â·|b  âˆ’           =    +    log  +3            =  2log
the extreme, when =1we have    1   2 1 =    (  1)                                               U
O Â·|b                                                                 Ë†            
 (  2) 1. Note that this deï¬nition is very similar to def- Furthermore, if b0 âˆ’ b01 â‰¤ Î± then for all times t:
inition of the Dobrushin coefï¬cient, supb ,b P (b , Â·) âˆ’                 
                                     1 2     1                                  log N âˆš
P b , Â·                                                                            U
 ( 2  ) 1 and it is widely used in the ï¬ltering literature           Zt â‰¤     =         2
[Moral, 2004]. We now consider some examples.                               Î±      Î³
     Î³                b
  Let  be 1 and consider 1 having support on one state and Also, for any initial belief states b and Ë†b , and Î´>0,there
b                           b âˆ’ b                                               0      0
 2 on another state. In this case 1 2 1 =2and there-  exists a time Ï„(Î´) â‰¥ 1, such that for any t â‰¥ Ï„(Î´) we have
    O Â·|b  âˆ’ O Â·|b                                                  
fore  (  1)    (  2) 1 =2, which implies that we have                              N âˆš
                                                                                 log U
a different observations from the two states. Since this holds   Zt â‰¤      + Î´ =        2 + Î´
for any two states, it implies that given an observation we can          Î±         Î³
uniquely recover the state. To illustrate the value observation The following corollary now completely speciï¬es the algo-
characterization, in POMDP terminology for Î³ =1we have rithm by providing a choice for U , the weight of the uniform
a fully observable MDP as no observation can appear with distribution.                     
                                                                               Ë†              
positive probability in two states. At the other extreme, for an Corollary 3.3 Assume that b0 âˆ’ b01 â‰¤ Î± and U =
unobservable MDP, we can not have a value of Î³>0 since T                  t
                                                      log N . Then for all times ,
O(Â·|b1) âˆ’ O(Â·|b2)1 =0for any two belief states b1 and b2.                  N  
                                                                                        âˆš
                                                                         6log T
  Recall the example in the Introduction where at every state      Zt â‰¤           T + Î³ O
the observation reveals the true state with probability 1âˆ’ and            Î³
                                                                             
with probability gives a random state. Here, it is straight- Proof: With the choice of U ,wehave:
                 Î³    âˆ’                                    âˆš               âˆš               âˆš
forward to show that is 1 . Hence, as approaches 0,the                    Î³    â‰¤        Î³   .
value of observation approaches 1.                             2 =    4 T +6    O   3   T +     O
  We now show that having a value of Î³ bounded away from And,
                                                                    N       N  log N       N
zero is sufï¬cient to ensure some guarantee on the monitor-       log   =log         â‰¤  2log   ,
ing quality, which improves as Î³ increases. Throughout, the         U         T          T
paper we assume that Î³>0.                             which completes the proof.                       

                                                IJCAI-07
                                                  2476                                                                  
3.3  The Analysis                                       Note that Zt is a positive super-martingale and therefore
                                                                                                  Z
We start by presenting two propositions useful in proving the converges with probability 1 to a random variable .The
                                                                  Z                      
theorem. These are proved later. The ï¬rst provides a bound expectation of cannot be larger than Î± , since whenever
                                                                      
on the error accumulation.                            Z  is larger than Î± its expectation in the next timestep is
                                                                                                    Z â‰¥
Proposition 3.4 (Error Accumulation) For every belief states strictly less than its expected value. Since by deï¬nition t
      Ë†                              Ë†         Ë†     Zt then, regardless the our initial knowledge on the belief
bt and bt and updates bt = TUot bt and bt = TËœUot bt,
                     +1               +1              state, the monitoring will be accurate and results in error less
we have:                                                     
                                                    than   Î± .                                       
              Ë†                 Ë†
 Eot KL(bt   ||bt )   â‰¤  KL(bt||bt)+U  log N + O
           +1   +1                                    Error Accumulation Analysis
                                       
                         âˆ’KL(O(Â·|bt)||O(Â·|Ë†bt))       In this subsection we present a series of lemmas which prove
                                                      Proposition 3.4. The lemmas bound the difference between
  The next proposition lower bounds the last term in Propo- the updates in the approximate and true model.
sition 3.4. This term, which depends in the value of obser- We start by proving the Lemma 3.8 provided at the begin-
vation, enables us to ensure that the two belief states will not ning of the Subsection
diverge.
                                                                                    b     b
Proposition 3.5 (Value of Observation) Let Î³ be the value of Lemma 3.6 For every belief states 1 and 2,
observation, b and b be belief states such that b (s) â‰¥ Î¼                
            1     2                       2                     KL(Tb   ||Tb ) â‰¤ KL(b ||b )+T
for all s.Then                                                         1   2         1  2
                                           
                                                       
                                             2          Proof: Let us deï¬ne the joint distributions p1(s ,s)=
                             1  Î³KL(b1||b2)                                        
    KL  O  Â·|b ||O Â·|b  â‰¥                            P (s, s )b1(s) and p2(s ,s)=PË†(s, s )b2(s). Throughout the
       (  (  1)  (  2))                1
                             2     log Î¼              proof we speciï¬cally denote s to be the (random) â€™ï¬rstâ€™ state,
                                âˆš                     and s to be the (random) â€™nextâ€™ state. By the chain rule for
                            âˆ’3Î³   O + O
                                                      relative entropy, we can write:
  Using these two propositions we can prove our main theo-
                                                                                                   
rem, Theorem 3.2.                                      KL(p1||p2)=KL(b1||b2)+Esâˆ¼b[KL(T        (Â·|s)||T(Â·|s)]
                                       UËœ
  Proof of Theorem 3.2: Due to the fact that mixes with            â‰¤   KL(b1||b2)+T
the uniform distribution, we can take Î¼ = U /N . Combining
Propositions 3.4 and 3.5, and recalling the deï¬nition of ,we where the last line follows by Assumption 3.1.
                                                        Let p (s|s) and p (s|s) denote the distributions of the
obtain that:                                               1          2
                                                      ï¬rst state given the next state, under p and p respectively.
        E    KL  b  ||Ë†b  |o   , ..., o â‰¤                                              1     2
          ot    ( t+1  t+1) tâˆ’1    0                  Again, by the chain rule of conditional probabilities we have,
                                       
                                         2                                        
            KL(bt||Ë†bt)+ âˆ’ Î± KL(bt||Ë†bt)                 KL(p1||p2)=KL(Tb1||Tab2)
                                                                                                  
                                                                          +Esâˆ¼Tb[KL(p1(s|s )||p2(s|s )]
  By taking expectation with respect to {o ,o ,...otâˆ’ },we
                                   0 1       1                        â‰¥   KL  Tb ||Tb ,
have:                                                                        (  1    2)
                                       
                                         2            where the last line follows from the positivity of the relative
       Zt    â‰¤   Zt +  âˆ’ Î±E  KL(bt||Ë†bt)
         +1                                           entropy. Putting these two results together leads to the claim.
                            2                          
             â‰¤   Zt +  âˆ’ Î±Z ,
                            t                           The next lemma bounds the effect of mixing with uniform
where the last line follows since, by convexity,      distribution.
                                  
                     2                    2
                                                      Lemma 3.7  For every belief states b1 and b2
      E    KL(bt||Ë†bt)  â‰¥  E  KL(bt||Ë†bt)  ,

                                                       KL(Tb1||TbËœ 2) â‰¤   (1 âˆ’ U )KL(Tb1||TbË† 2)+U log N
which proves the ï¬rst claim in the theorem.
  We proceed with the case where the initial belief state is Proof: By convexity,
                    Ë†              
good in the sense that b0 âˆ’ b01 â‰¤ Î± .Thenwehave
                                                           KL(Tb   ||TbËœ ) â‰¤  (1 âˆ’ U )KL(Tb ||TbË† )
   Z                                 Z  âˆ’ Î±Z2                    1   2                     1   2
that t is always less than Î± . The function t t+                                KL   Tb ||Uni Â·
             âˆ’   Z Î±                     Z  â‰¤                                 + U    (  1     ( ))
has derivative1 2 t , which is positive when t  Î± .
                                                                        â‰¤   (1 âˆ’ U )KL(Tb ||TbË† )
Since Zt at   is mapped to   ,theneveryZt â‰¤      is                                          1   2
            Î±               Î±                  Î±                                     N,
mapped to a smaller value. Hence the Zt will always remain                    + U log
        
below   Î± .
  We conclude with the subtle case of unknown initial belief where the last line uses the fact that the relative entropy be-
state, and deï¬ne the following random variable        tween any distribution and the uniform one is bounded by
                                                    log N.                                           
                     Z ,Z>        
                    t     t     Î±                     Combining these two lemmas we obtain the following
              Zt =      
                       Î± , Otherwise                  lemma on the transition model.

                                                IJCAI-07
                                                  2477Lemma 3.8  For every belief states b1 and b2,           Proof of Proposition 3.4: Using the deï¬nitions of updates
                                                      and the previous lemmas:
                                                                             
     KL(Tb1||TbËœ 2) â‰¤ KL(b1||b2)+T + U log N
                                                         E       KL  b  ||Ë†b
                                                           otâˆ¼bt    ( t+1  t+1)
  After dealing with transition model, we are left to deal with                         
                                                                                     Ë†
the observation model. We provide an analog lemma with      =  Eotâˆ¼bt KL(TUot  bt||TËœUot bt)
regards to the observation model.                                                    
                                                                                  Ë†
                                                            â‰¤  Eotâˆ¼bt KL(Uot  bt||Uot bt) + T + U log N
Lemma 3.9  For every belief states b1 and b2,
                                                                                          
                                                            â‰¤  KL(bt||Ë†bt)+O âˆ’ KL(O(Â·|bt)||O(Â·|Ë†bt))
    E         KL  U  b ||U b
      oâˆ¼O Â·|b1    ( o 1  o 2)
         (  )                                                  +T + U log N
                                       
        â‰¤ KL(b1||b2)+O  âˆ’  KL(O(Â·|b1)||O(Â·|b2))      where the ï¬rst inequality is by Lemma 3.8 and the second is
                                                      by Lemma 3.9 This completes the proof of Proposition 3.4.
  Proof: First let us ï¬x an observation o.Wehave:                                                      
                                 U b  s
KL  U b ||U b         U  b s      o 1( )             Value of Observation Proposition - The Analysis
   ( o 1   o 2)=         o 1( )log 
                     s            Uob2(s)             The following technical lemma is useful in the proof and it
                                                               L
                                  O o|s b s /O  o|b  relates the 1 norm to the KL divergence.
                        U b   s      (  ) 1( )  (  1)
                =      ( o 1)( )log                 Lemma 3.10  Assume that Ë†b(s) >Î¼for all s and that Î¼< 1 .
                     s             O(o|s)b2(s)/O(o|b2)                                                 2
                                                      Then
                                  b  s      O  o|b
                        U b   s     1( ) âˆ’     (  1)                KL  b||Ë†b â‰¤b âˆ’ Ë†b   1
                =      ( o 1)( )log b s   log                         (   )         1 log Î¼
                     s              2( )     O(o|b2)
                      
                                     O(o|s)             Proof: Let A be the set of states where b is greater than Ë†b,
                    +    (Uob )(s)log
                             1                       i.e. A = {s|b(s) â‰¥ Ë†b(s)}.So
                       s             O(o|s)
                                                                             b s             b s
                                                      KL  b||Ë†b        b s     ( ) â‰¤    b s     ( )
where the last line uses the fact that O o|b and O o|b are ( )=        ( )log           ( )log
                                (  1)     (   2)                              Ë†b(s)            Ë†b(s)
constants (with respect to s).                                       s               sâˆˆA
                                                                                           
  Now let us take expectations. For the ï¬rst term, we have:                          b(s)             b(s)
                                                                =      (b(s) âˆ’ Ë†b(s)) log +    Ë†b(s)log
                                                                                   Ë†b(s)            Ë†b(s)
                         b s                                       sâˆˆA                     sâˆˆA
    E         U  b  s     1( )                                           
     oâˆ¼b1     ( o 1)( )log                                             1
                          b2(s)                                 â‰¤   log     (b(s) âˆ’ Ë†b(s))
            s                                                        Î¼
                                                                       sâˆˆA
                          O(o|s)        b (s)                                                  

             O  o|b             b  s     1                                          b s âˆ’ Ë†b s
        =      (  1)     O  o|b  1( )logb  s                                          ( )   ( )
                           (  1)         2( )                       +    Ë†b(s)log 1+
           o           s                                                                Ë†b s
                          b  s                                       sâˆˆA                ( )
             O  o|s b s     1( )   KL  b ||b                                             
        =      (  ) 1( )log     =     ( 1  2)                          1
                           b (s)                                â‰¤            b s âˆ’ Ë†b s      b s âˆ’ Ë†b s
           s,o              2                                       log Î¼   ( ( )   ( )) +   ( ( )  ( ))
                                                                       sâˆˆA             sâˆˆA
where the last step uses the fact that o O(o|s)=1. Simi-
                                                                    1      1     b âˆ’ Ë†b
larly, for the third term, it straightforward to show that:     =      log Î¼ +1         1
                                                                  2
                          O o|s                      where we have used the concavity of the log function and
     E         U  b  s      (   )                         
      oâˆ¼b1     ( o 1)( )log                                                  1
                                                     that  sâˆˆA(b(s)âˆ’Ë†b(s)) = bâˆ’Ë†b . The claim now follows
             s             O(o|s)                                            2      1
                                                                     Î¼<   1                            
                       O  o|s b s    O  o|s         using the fact that 2 .
               O o|b       (  ) 1( )     (  )           We are now ready to complete the proof. We will make
         =      (  1)      O o|b    log 
            o          s     (  1)     O(o|s)         use of Pinskerâ€™s inequality, which relates the KL divergence
                                                         L                                       p    q
                                                     to the 1 norm. It states that for any two distributions and
         = Esâˆ¼b  KL(O(Â·|s)||O(Â·|s))
                                                                               1         2
                                                                    KL(p||q) â‰¥  (p âˆ’ q1) .          (2)
  For the second term,                                                         2
                                                      Proof of Proposition 3.5: By Pinskerâ€™s inequality and As-
                O  o|b
                  (  1)                              sumption 3.1, we have
    Eoâˆ¼b1  âˆ’ log         = KL(O(Â·|b  )||O(Â·|b ))                           
                O o|b              1      2                                                      âˆš
                  (  2)                                                                  
                                                        O(Â·|s) âˆ’ O(Â·|s)1 â‰¤ 2KL(O(Â·|s)||O(Â·|s)) â‰¤  2O
directly from the deï¬nition of the relative entropy. The lemma for all states s âˆˆ S. Using the triangle inequality,
follows from Assumption 3.1.                     
                                                                                         
  Now we are ready to prove Proposition 3.4.          O(Â·|b)âˆ’O(Â·|Ë†b)1 â‰¤O(Â·|b)âˆ’O(Â·|Ë†b)1+O(Â·|Ë†b)âˆ’O(Â·|Ë†b)1 .

                                                IJCAI-07
                                                  2478