            Using Predictive Representations to Improve Generalization in
                                     Reinforcement Learning
                   Eddie J. Rafols, Mark B. Ring, Richard S. Sutton, Brian Tanner
                                   Department of Computing Science
                                          University of Alberta
                                 Edmonton, Alberta, Canada T6G 2E8
                           {erafols,markring,sutton,btanner}@cs.ualberta.ca

                    Abstract                          weightings from a set of features over the agent‚Äôs past and
                                                      present perceptions. Good generalization occurs when the
    The predictive representations hypothesis holds   features are well chosen and are roughly independent of each
    that particularly good generalization will result other. Predictive representations, too, are generally used in
    from representing the state of the world in terms conjunction with function approximation, where each predic-
    of predictions about possible future experience.  tion is treated as a feature.
    This hypothesis has been a central motivation be-   Yet function approximation is itself a complex issue, re-
    hind recent research in, for example, PSRs and    quiring choices of method, optimization of learning param-
    TD networks. In this paper we present the Ô¨Årst    eters, etc., each of which can inÔ¨Çuence the results in its
    explicit investigation of this hypothesis. We show own way. To avoid such confusion and test the predictive-
    in a reinforcement-learning example (a grid-world representations hypothesis most directly, we have controlled
    navigation task) that a predictive representation in for the possible confounding effects of function approxima-
    tabular form can learn much faster than both the  tion by developing a strictly tabular form of predictive repre-
    tabular explicit-state representation and a tabular sentation. Furthermore, our focus is on how predictive repre-
    history-based method.                             sentations affect generalization, not on how these representa-
                                                      tions are acquired, and so we assume they have already been
1  Introduction                                       acquired by some other process.
A predictive representation is one that describes the world in We illustrate the power of predictive representations in a
terms of predictions about future observations. The predictive grid-world navigation task where the agent‚Äôs action and ob-
representations hypothesis holds that such representations are servation space is chosen to be particularly impoverished,
particularly good for generalization. A good representation is thus imposing a high degree of perceptual ambiguity. With
one that captures regularities of the environment in a form only a small subset of the predictions necessary for a full
useful to the learning agent; and in a reinforcement-learning representation of the environmental state, a reinforcement-
task, something is ‚Äúuseful‚Äù if it increases the agent‚Äôs ability to learning agent can learn quickly and still achieve nearly opti-
receive rewards. Thus, representations generalize well when mal performance.
the regularities they capture allow an agent to learn more ef-
Ô¨Åciently how to increase its cumulative reward.       2   Predictive Representations
  Why do we believe that predictive representations might Several recently introduced methods for modeling dynamical
generalize particularly well? Good generalization tends to systems have been inspired by the predictive-representations
result when similar situations have similar representations. In hypothesis. Among these are predictive state representa-
interactive tasks, two situations are similar when executing tions (PSRs) [Littman et al., 2002] and temporal-difference
an action sequence in one leads to about the same results as networks (TD networks) [Sutton and Tanner, 2005]. Current
executing the same action sequence in the other, regardless of research on predictive representations has focused principally
what the action sequence is. The more similar the results tend on the acquisition of the representation (though their use for
to be, the more similar the situations are, and, therefore, the control is also beginning to be explored [James et al., 2004;
more similar their predictive representations will be. Izadi and Precup, 2003]).
  The predictive representations hypothesis makes an espe- PSRs are based on the concept of a core test‚Äîa sequence
cially broad claim, and as a result it is especially resistant to of actions followed by an observation‚Äîsimilar to the tests of
testing or proof. In particular, questions regarding representa- Rivest and Schapire [1994]. An agent records the outcome
tion and generalization often involve the confounding issues of each core test as either a success or failure depending on
of function approximation and representation acquisition ‚Äî whether the predicted observation matches the actual obser-
issues that we have developed speciÔ¨Åc measures to avoid. In vation. The agent maintains the probability of success for
reinforcement learning, for example, generalization is com- each core test, and this value becomes a feature in the agent‚Äôs
monly achieved through function approximation: by learning representation of the world. Knowledge of the world can beexpressed as a function (generally a linear combination) of say these states are identically predictive for that value of n
these features. Core tests are continually acquired until the and belong to the same identically predictive class (similar to
features form a sufÔ¨Åcient statistic, which means that they cap- the states of Rivest and Schapire‚Äôs Simple-Assignment Au-
ture all relevant, knowable information about the environment tomata [1994], but constrained by n).
and can maintain such information from one time step to the If for a given n an environment has c identically predictive
next. (More will be said about sufÔ¨Åcient statistics below.) classes, we arbitrarily number them 1 through c, and when
  TD  networks consist of two conceptually separate net- an agent visits an environmental state, it observes the number
works: a question network and an answer network. The  for that state‚Äôs identically predictive class. This number is
question network poses questions about possible future ob- therefore the agent‚Äôs predictive representation in tabular form.
servations. The answer network learns to predict answers to As an example, suppose that a = 3, and n = 1, then N =
those questions. The questions are therefore analogous to the 3, and the panel consists of three tests (one for each action).
core tests of PSRs and the answer network is analogous to Each test can result in an observation of 0 or 1, so there are
the function that computes the probabilities of core-test suc- 23 possible panel conÔ¨Ågurations and c = 8 (at most). Every
cesses. However, the units of the question network can make state is aggregated into one of these eight classes, labeled 1
predictions about not just the agent‚Äôs observations, but also through 8, and the agent observes a 1, 2, 3, 4, 5, 6, 7, or 8 in
about the values of other network units, and these predictions each state that it visits.
of predictions allow the TD network to represent many possi- In general, as n increases, both N and c increase, so there
ble core tests in a compact form. Furthermore, these predic- are fewer states per class on average and the agent‚Äôs represen-
tions are generally, but not necessarily, action conditional. tation of its environment becomes more expressive.
  The two approaches just outlined share the common aspi-
                                                        However, not all 2N panel conÔ¨Ågurations are necessarily
ration of representing the world as a set of predictions about
                                                      represented in the environment: clearly, there can never be
future observations. While research into predictive represen-
                                                      more classes c then there are environmental states S, so if
tations is still in its nascent phases, we attempt to make a pre-
                                                      N  > S  then some tests must be equivalent (meaning there
diction of our own about possible future observations. In this
                                                      are no environmental states where the tests yield different
spirit we pose the following question: if a method does indeed
                                                      results). Furthermore, as n increases, N increases exponen-
prove successful at acquiring predictive representations, will
                                                      tially, yet c tends to increase quite slowly in environments
these representations be good at generalization?
                                                      with even a moderate amount of regularity.
3  Tabular Predictive Representations                   Eventually, increasing n no longer increases c, and the
                                                      classes represent a sufÔ¨Åcient statistic. At that point c may still
In order to focus exclusively on prediction as a basis for gen- be less than S if there are environmental states that cannot be
eralization, unclouded by issues involving representation ac- distinguished by any test of any length.
quisition, we assume the agent has already acquired the abil-
ity to make correct predictions. To eliminate issues involving 3.2 Sarsa(0) with Identically Predictive Classes
function approximation, we consider deterministic tasks with
a single, binary observation (though our method can also be All agents in this paper are trained using the reinforcement-
applied to multiple binary observations, and we describe an learning algorithm known as episodic tabular Sarsa(0) [Sut-
extension in the future-work section for accommodating con- ton and Barto, 1998]. In the traditional Markov case‚Äî
tinuous observations or stochastic environments). To further where the agent directly observes the environmental state‚Äî
focus our tests, we choose to look only at predictions that are an action-value function is learned over the space of environ-
contingent upon the agent‚Äôs actions, though this restriction is mental states and actions. In this algorithm, the estimated
not a requirement of predictive representations in general. value Q(s, a) of each experienced state‚Äìaction pair s, a is
                                                      updated based on the immediate reward r and the estimated
3.1  Identically Predictive Classes                   value of the next state‚Äìaction pair; i.e,
We start with a set of binary tests resembling PSR core tests.
Each test is of length n, meaning that it is a sequence of n   ‚àÜQ(s, a) = Œ±[r + Q(s0, a0) ‚àí Q(s, a)],
actions followed by a single observation bit. We construct all
possible tests of length 1 through n and produce a panel of test where Œ± is a learning-rate parameter.
results in each state, one entry per test. Given N binary tests, Episodic tabular Sarsa(0) is implemented over the predic-
the panel may take on 2N distinct conÔ¨Ågurations of outcomes tive state space by mapping environmental states to their cor-
(assuming the environment is deterministic). If the agent has responding identically predictive classes, as described in the
a actions available, then there are at most an distinct tests of previous section. The function C(¬∑) provides this mapping,
length n, and the number of tests of length n or less is: and the resulting classes are then treated by the Sarsa agent
                          n                           as though they were environmental states:
                         X
                 N   =      ai
                                                                                  0   0
                         i=1                             ‚àÜQ(C(s), a) = Œ±[r + Q(C(s ), a ) ‚àí Q(C(s), a)] (1)
                          n+1
                     =   a   ‚àí  1.                    Because no distinction is made between the states within a
If two states cannot be distinguished by any of the N tests, class, the learning that occurs in one environmental state ap-
then the panel of results is identical in both states, and we plies to all states mapped to the same class.                                                      further tests ever reduce the number of classes or change the
                                                      way states have been assigned to classes.
                                                        But an agent whose predictive representation is a sufÔ¨Åcient

                         x                            statistic may still not distinguish all environmental states.
                                                      This idea is illustrated in Figure 1 where there are 68 distinct
                                                      environmental states (four orientations in each grid cell).
                                                        Given an agent that has three actions (Rotate 90‚ó¶ Right,
                                                      Rotate 90‚ó¶ Left, Advance) and that observes only whether
                                                      or not there is a wall directly in front of it, each arm of the
Figure 1: An agent in this world can have one of 4 orienta- cross will produce exactly the same sets of predictions. To the
tions in each of the 17 grid squares. Its observation consists predictive agent, the arms are identical because there is no test
of one ‚Äúnose touch‚Äù bit that senses only whether there is a that can distinguish a state in one arm from the corresponding
wall directly in front of it. It chooses actions from {Rotate states in the other three arms, even with inÔ¨Ånite-length tests.
  ‚ó¶               ‚ó¶
90 Right, Rotate 90 Left, Advance}. There are therefore Therefore, for this environment there are a maximum of 17
68 distinct environmental states, but a predictive representa- identically predictive classes (four orientations in four arm
tion will identify (at most) 17 identically predictive classes. cells, plus only one in the center because all orientations in
                                                      the center are identically predictive).
3.3  Performance and Generalization                     This example also illustrates how an agent‚Äôs learning speed
                                                      can be improved through the use of predictive representa-
In episodic tasks the performance of a reinforcement-learning tions. If the task is to navigate to the cell marked X, an agent
agent can be quantiÔ¨Åed by the total reward it receives per observing identically predictive class numbers will learn an
episode. Given inÔ¨Ånite time, agents that observe the (Markov) optimal policy much faster than an agent observing environ-
state of the environment can achieve optimal performance. mental state, because the predictive-class agent has only a
  In practice, however, learning an optimal policy may take quarter of the situations to learn about and therefore requires
arbitrarily long for an arbitrarily large state space, and these far less experience to learn about all possible situations.
days one can rarely Ô¨Ånd enough time, let alone an inÔ¨Ånite
or even an arbitrary amount of it. It is therefore desirable in
many cases to use methods that speed learning, even if the 4 Tabular History-based Representations
Ô¨Ånal solution is not absolutely optimal.              The obvious competitors to predictive representations are
  We anticipate a trade-off between the agent‚Äôs asymptotic history-based representations. Of these, the Ô¨Åxed-length
performance and its speed of learning. As c increases, the or Markov-k approaches [Ring, 1994; McCallum., 1996;
classes more closely resemble the Markov state of the envi- Mitchell, 2003] most clearly lend themselves to a tabular for-
ronment and the agent‚Äôs asymptotic performance approaches mat. To promote a fair comparison between representations,
optimal. As c decreases, there are fewer distinct cases to we deÔ¨Åne a k-length history to be an observation followed by
learn about and learning speed improves, but there is a risk k action-observation pairs. In tabular format, each possible
that the states comprising a class will disagree about the op- history is uniquely labeled.
timal action for that class. This disagreement can lead to Predictive representations and Ô¨Åxed-length history repre-
sub-optimal action selection, and in extreme cases the con- sentations offer fundamentally different kinds of generaliza-
Ô¨Çict may be catastrophic, precluding the discovery of any tion. SpeciÔ¨Åcally, most environmental states can be reached
reasonable policy. Therefore, a good representation will Ô¨Ånd via multiple different paths, each corresponding to a different
low values of c while minimizing the amount of disagree- history; conversely, a single action sequence (history) may
ment within classes. The predictive-representations hypothe- reach multiple environmental states by starting from different
sis speciÔ¨Åcally holds that the classes formed through predic- states. Therefore, the mapping between environmental states
tive representations will do just that.               and Ô¨Åxed-length history sequences is many to many. But the
                                                      mapping between environmental states and identically pre-
3.4  Predictive Classes and SufÔ¨Åcient Statistics      dictive classes is many to one.
An agent‚Äôs representation of the world is a sufÔ¨Åcient statis- Conceptually, the reason for the difference is that an agent
tic if it cannot be improved through any further experience can take many paths to arrive in a state, but once there, has
with the world. In the case of predictive representations, a only one set of possible futures; and the set of possible futures
sufÔ¨Åcient statistic implies that there are no additional predic- is absolutely Ô¨Åxed for each environmental state. For example,
tions that can add knowledge or improve performance. On if k = 1 the agent has at least two ways of reaching every state
the other hand, adding more predictions to a sufÔ¨Åcient statis- in Figure 1‚Äîrotating right or rotating left. If k = 2 there
tic can never worsen asymptotic performance.          are at least 4 different histories for each state. The number
  In the tabular case, if a sufÔ¨Åcient statistic consists of c of Ô¨Åxed-length history representations that can lead to each
classes, no number of additional tests will increase c. (If environmental state increases exponentially with k, so as k
by adding a test we were to end up with c0 classes, where increases, there are an exponential number of cases that the
c0 > c, then our original representation could not have been agent must learn about. (This problem does not automatically
a sufÔ¨Åcient statistic, since it did not represent the knowledge disappear by using function approximation methods.)
captured by the newest c0 ‚àí c class distinctions.) Nor will any In contrast, the set of possible futures available from each                                                                              Unique     % Environmental
                                                       Agent                Observations       States
                                                       Markov          -       1696            100%
                                                       Predictive      n
                                                                       2          67            3.9%
                                                                       3         185           10.7
                                                                       4         308           17.8
                                                                       5         416           24.1
                                                                       6         497           28.8
                                                                       7         568           32.9

                                    G                  Fixed-History   k
                                                                       2          50            2.9%
                                                                       3         205           11.9
Figure 2: The ‚ÄúofÔ¨Åce layout‚Äù grid-world used for the naviga-           4         790           45.7
tion task. The agent starts an episode in one of the six top           5       2,938          170.0
rooms, and Ô¨Ånishes in the square marked G.                             6      10,660          616.9
                                                       Random         case
state is the same no matter how that state is reached, and as a        1       1,526           90%
result, the number of predictive classes is never more than the        2       1,611           95
number of distinct environmental states. The nature of this            3       1,679           99
mapping makes generalization in the predictive case far more
intuitive, and to some extent the predictive-representations Figure 3: The four representational schemas in their tested in-
hypothesis is based on this clearer intuition.        stantiations and the degree of state aggregation in each case.
                                                      In the predictive case, the ‚Äúunique observations‚Äù column rep-
5  Experiment Design                                  resents the number of identically predictive classes, c. For
                                                      Ô¨Åxed-length history, this column represents the number of
We examined the predictive-representations hypothesis by unique histories that occurred during training.
Ô¨Årst designing a grid-world with a large degree of regularity
(Figure 2). We then tested Sarsa(0) agents with four different
methods of representation on a task in this environment to see label. In the predictive case the agent observed a label cor-
how well each method was able to exploit the environment‚Äôs responding to the identically predictive class (as described in
regularities.                                         Section 3.2) for six different values of n. For each value of n,
  The agent‚Äôs task was to navigate to the goal cell (marked G) states were assigned to classes as described in Section 3.1. In
from any randomly chosen square in one of the top six rooms. the Ô¨Åxed-history case, the agent observed a label correspond-
(The environment was designed to resemble a typical ofÔ¨Åce ing to the k-step history it had just experienced (Section 4)
layout, and the task can be likened to Ô¨Ånding the quickest for Ô¨Åve different values of k. To see whether our results in
route to the staircase.) Representations that generalize well the predictive case were merely due to beneÔ¨Åcial properties
should allow their respective agents to exploit the regularities of state aggregation, we tried randomly assigning states to
in the environment to improve their speed of learning. classes and then training according to Equation 1.
  As in Figure 1, the agent has a one-bit observation and Figure 3 shows the vital stats for each of the representa-
three possible actions. The rewards for the task are +1 for tional methods tested. The amount of state aggregation that
reaching the goal state and ‚àí1 on all other timesteps. All occurs in each method is shown in terms of the ratio of unique
transitions in the environment are deterministic and the envi- observations to environmental states.
ronment has a total of 1696 states, 840 of which are possible
start states. On average, there are 42.2 steps along the op- 6 Results
timal path from start to goal. The task is formulated to be
                                                      Performance  results for the different representational
undiscounted and episodic; thus the agent is transported to a
                                                      schemas given in Figure 3 are graphed in Figure 4, with the
randomly chosen starting position upon reaching the goal.
                                                      exception of the random state aggregation method, which per-
  In every case actions were chosen according to an -greedy
                                                      formed too poorly to be graphed meaningfully.
policy;  was set to 0.1, and Œ± was set to 0.25, which are
                                                        Each point in the graph represents the average number
typical values for Sarsa agents in episodic tasks.
                                                      of steps per episode over the previous 10 episodes. The
  The four representational schemas tested were:
                                                      curves are averaged over 10,000 trials, each trial being 1,000
  ‚Ä¢ Markov                                            episodes. At the end of each trial, the agent‚Äôs action values
  ‚Ä¢ n-depth predictive classes                        are reset, and learning begins from scratch. Over the course
  ‚Ä¢ k-step Ô¨Åxed-length histories                      of 1,000 episodes, the Markov case shows a smooth, steadily
                                                      improving curve, which by the 1, 000th episode is performing
  ‚Ä¢ random state aggregation                          very close to optimal.
  In the Markov case the agent directly observed its current Though Ô¨Åxed-length history representations learn more
environmental state, each state being represented by a unique slowly than the other methods, their learning speeds are    100000


     10000
                                                                                               k = 6
Steps                                                                                     k = 5
                                                                                   k = 4
                                                                          k =3
                                                                 k = 2
      1000
                                      n = 4
                                                                     Environmental
                                                  n = 5                 State
                                                             n = 6                              n = 3
       100
       42.2
           10                                          100                                         1000
                                                    Episodes

Figure 4: A comparison of three representational methods on the task of Figure 2. The representational schemas shown are:
explicit environmental state, history-based representations for k = 2 through 6, and identically predictive classes for n = 3
through 6. The vertical axis shows average steps taken per episode over the past 100 episodes of training. Note that the axes
are logarithmic and the vertical axis intersects the horizontal axis at the optimum.

fastest for small values of k and their Ô¨Ånal results are best for graphs because these agents performed so poorly. With only
large values of k, demonstrating the trade-off between repre- 10% state aggregation (90% of the states not aggregated), ran-
sentational expressiveness and learning speed. The number of dom state aggregation brought about catastrophic failure in
histories increases (exponentially) with k, which negatively over 80% of the agents tested, meaning that they show no
impacts learning speed but positively impacts the Ô¨Ånal results progress in their training. To avoid catastrophic failure in
of learning. Interestingly, on individual trials the curves for 90% of the agents, the amount of state aggregation had to be
the history representation resemble a step function. Appar- 1% or less, meaning that only one state in a hundred shared a
ently, the -greedy action selection chooses poorly for many class with another state.
episodes until the agent suddenly learns some key piece of In contrast, there were no cases of catastrophic failure in
information that dramatically improves performance. As k any of the agents using predictive representations.
grows, the agent takes longer to discover this piece of infor- It might be argued that the predictive representations bene-
mation but then converges to a better solution.       Ô¨Åt unfairly from the preprocessing done to create the classes,
  The results look promising for predictive representations. which the history-based methods did not have, and which an
They allow both speedy learning and convergence to a good actual learning agent may not reasonably be expected to ac-
policy. In general, the results for the identically predictive quire. Or it might be argued that history-based methods could
representations are similar to those for the Ô¨Åxed-history rep- somehow be augmented to combine all ways of reaching a
resentations in that convergence speed decreases and conver- state into equivalence classes as our tabular representations
gence quality increases as n increases. However, in con- do for predictions. But these objections miss two fundamen-
trast to the Ô¨Åxed-history representation, the number of identi- tal aspects of this research and of predictive representations
cally predictive classes increases quite slowly with n and the in general. First, it is not our intention here to prove the supe-
generalization beneÔ¨Åt of the predictive classes is clear. The riority of one particular learning algorithm to any other, but
representation effectively aggregates similar states, allowing only to test out, in advance, whether the research direction
the agent to converge to near-optimal solutions. This result of predictive representations looks promising. (The evidence
closely matches our intuition and expectations. Only in the collected clearly gives reason for optimism.) Second, acqui-
n = 3 case was there no improvement in learning speed, in- sition of a predictive representation is always directed toward
dicating there may be a maximum degree of state aggregation acquisition of a sufÔ¨Åcient statistic. Once an agent acquires
beyond which learning speed is not improved‚Äîalso an ex- the predictions necessary for a sufÔ¨Åcient statistic, then it can
pected and intuitive result from section 3.3. Results in the in principle create the class mappings used here. (In fact,
n = 2 and n = 7 cases match these trends but are not shown those mappings would fall directly out of a TD network rep-
so as to keep the graph at least vaguely readable.    resentation, being merely a subset.) It is not clear what the
  The case of random state aggregation is not shown in the equivalent of a sufÔ¨Åcient statistic might be for history-based