                       Feature    Selection   Based   on  the  Shapley    Value
                                   Shay  Cohen   and  Eytan  Ruppin
                                     School  of Computer  Sciences


                               Tel-Aviv University, Tel-Aviv 69978, Israel
                               Â 


                                cshay,ruppin      Â¡ @post.tau.ac.il

                                             Gideon   Dror
                                    Department  of Computer  Science
                      Academic   College of Tel-Aviv Yaffo, Tel-Aviv, 64044, Israel
                                        gideon@mta.ac.il




                    Abstract                          containing i.i.d. sampled instances of Â¢Â¤Â£Â¥Â§Â© of the form


                                                        $#


                                                          Â¥"! are available: Train, Validation and Test represent-


    We present and study the Contribution-Selection al- ing the training set, validation set and test set respectively.
                                                                                                  #


    gorithm (CSA), a novel algorithm for feature se-                                       )(


                                                                                             Â¥Â¥* +$,-Â¢./Â©
                                                      Given an induction algorithm and a set %'&   ,
    lection. The algorithm is based on the Multi-     stands for a classiï¬er constructed from the training set us-
    perturbation Shapley Analysis, a framework which  ing the induction algorithm, after its input variables were nar-
    relies on game theory to estimate usefulness. The


                                                      rowed down to the ones in S, namely +$, Â¢0./Â© labels each in-
                                                                                                    C


    algorithm iteratively estimates the usefulness of                                     (?>A@B>DC


                                                                            .51Â¤6 7863Â© 9Â¤:<;=%    %
                                                      stance of the form Â¢./1324Â¥ , ,         with
    features and selects them accordingly, using either
                                                      a value in the domain of Â§ . The task of feature selection is
    forward selection or backward elimination. Empir-
                                                      to choose a subset % of the input variables, that would maxi-
    ical comparison with several other existing feature mize the performance of the classiï¬er on the test set. In what
    selection methods shows that the backward elimi-  follows we shall focus on optimizing accuracy of the classi-
    nation variant of CSA leads to the most accurate  ï¬er, although we could as easily optimize other performance
    classiï¬cation results on an array of datasets.    measure such as the area under the ROC curve, balanced error
                                                      rate etc.
1  Introduction                                         The rest of this paper is organized as follows: Section 2 in-
                                                      troduces the necessary background from game theory with a
Feature selection refers to the problem of selecting input vari- detailed description of the CSA algorithm; Section 3 provides
ables, otherwise called features, that are relevant to predicting an empirical comparison of CSA with several other feature
a target value for each instance in a dataset. Feature selection selection methods, accompanied by an analysis of the results;
has several potential beneï¬ts: defying the curse of dimension- Section 4 discusses the empirical results and provides further
ality to enhance the prediction performance, reducing mea- insights to the success of the backward elimination version of
surement and storage requirements and reducing training and the CSA algorithm.
prediction times. This paper focuses on the ï¬rst issue, namely
selecting input variables in an attempt to maximize the perfor-
mance of a classiï¬er on previously unseen data.       2   Classiï¬cation  as a Coalitional Game
  In this paper, we suggest to recast the problem of feature Cooperative game theory introduces the concept of â€œcoali-
selection in the context of coalitional games, a notion from tional gamesâ€, in which a set of players is associated with a
game theory. This perspective yields an iterative algorithm payoff, a real function that denotes the beneï¬t achieved by


for feature selection, the Contribution-Selection algorithm different sub-coalitions in a game. Formally, a coalitional


                                                                                           )(     #


                                                                                      EI    Â¥JJKÂ¥*


(CSA), intent on optimizing the performance of the classiï¬er game is deï¬ned by a pair Â¢0EFÂ¥GHÂ© where is the
                                                                                     %N&=E
on unseen data. The algorithm combines both the ï¬lter and set of all players and G/Â¢L%MÂ© , for every , is a real number


wrapper approaches. However, unlike ï¬lter methods, features associating a worth with the coalition % . Game theory further
are reranked on each step by using the classiï¬er as a black pursues the question of representing the contribution of each
box. The ranking is based on the Shapley value [Shapley, player to the game by constructing a value function, which
1953], a well known concept from game theory, to estimate assigns a real-value to each player. The values correspond to
the importance of each feature for the task at hand, speciï¬- the contribution of the players in achieving a high payoff.
cally taking into account interactions between features. The contribution value calculation is based on the Shapley
  Throughout the paper we use the following notations. The value [Shapley, 1953]. An intuitive example of the potential


distribution from which the dataset instances are drawn is rep- use of the Shapley value can be provided in an academic set-
                                   Â£Â¢Â¥Â¥Â©
resented by two variables Â¢Â¤Â£Â¦Â¥Â¨Â§
Â© , where           ting. Assume that you are a Professor running a lab, and, once


represents the input variables (vector of features), and Â§ and for all, you have decided to distribute the yearly bonus to


represents a discrete target value (class) for Â£ . Three sets your students in a fair manner, that reï¬‚ects the actual con-tribution of each student to the academic success of the lab. case, the Shapley value of a feature, that measures its con-
During the year, the students form spontaneous â€œcoalitionsâ€ tribution to the combined performance measure, is just the
of groups of students, each such group works and publishes sum of the corresponding Shapley values. The linearity of


a paper summarizing its work (these coalitions may also be the Shapley value is a consequence of this property. Namely,
                                                                                                   &


assembled by the Professor). Every paper gets a rank, (e.g., if the payoff function G is multiplied by a real number then


                                                                                         Â§         Â§


                                                                                           1         1


                                                                                           Â¢'&-GHÂ© (& Â¢0GHÂ©
its impact factor), composing its â€œpayoff functionâ€. Based on all Shapley values are scaled by & namely .
this annual data of the studentsâ€™ coalitions and their associ- In other words, multiplying the performance measure by a
ated payoffs, the Shapley value provides a fair and efï¬cient constant does not change the ranking of the features, a vital
way to distribute the bonus to each individual student accord- property for any scheme that ranks features by their â€™impor-
ing to his contribution over the year.                tanceâ€™.


  The Shapley value is deï¬ned as follows. Let the marginal


                              %      9Â¡Â  ;%


importance of player 9 to a coalition , with , be     2.1  Estimating Features Contribution  Using the


                            #


             Â¢                                             MSA


               1


                     G/Â¢L%Â¤Â£ 9 Â©Â¦Â¥ G/Â¢L%MÂ© 
               Â¢Â¤%MÂ©M                          (1)
                                                      The calculation of the Shapley value requires summing over
Then, the Shapley value is deï¬ned by the payoff       all possible subsets of players, which is impractical in our




                      (                               case. [Keinan et al., 2004] have presented an unbiased esti-


                        

                            Â¢


              Â§                                       mator for the Shapley value by uniformly sampling permu-




               1Â¢0GHÂ© 
                                1Â¢ Â©"Â©


                             1Â¨Â¢Â¤%              (2)


                                                                 
                      Â©Â¨


                      *                               tations from . Still, the estimator considers both large and
                        


                                                      small features sets to calculate the contribution values. In our


                                          1


                                  E      % Â¢ Â©
  where  is the set of permutations over , and is the feature selection algorithm, we use the Shapley value heuris-


set of players appearing before the 9 th player in permutation tically to estimate the contribution value of a feature for the


 . The Shapley value of a player is a weighted mean of its task of feature selection. Since in most realistic cases we as-

marginal value, averaged over all possible subsets of players. sume that the size ) of signiï¬cant interactions between fea-

  Transforming these game theory concepts into the arena of tures is much smaller than the number of features, * , we will
feature selection, in which one attempts at estimating the con- limit ourselves to calculating the contribution value from per-

tribution of each feature in generating a classiï¬er, the players mutations sampled from the whole set of players, with ) be-


E                                                     ing a bound on the permutation size. Notice that most ï¬l-




  are mapped to the features of a dataset and the payoff is                               (


                                                                                     ) 


                                 Â¢Â¤% Â©
represented by a real-valued function G , which measures ter methods are equivalent to using where no inter-


the performance of a classiï¬er generated using the set of fea- actions are taken into account. Feature selection using Ran-
                                                                                                  *
                                                                 [            ]              )+*-,
tures % . Finally, the usage of the Shapley value for feature dom Forests Breiman, 2001 is equivalent to . The


selection may be justiï¬ed by its axiomatic qualities: bounded estimated contribution value becomes


                                                                            (


                                                                               

                                                                                   Â¢


Axiom 1 (Normalization or Pareto optimality) For any game          .


                                                                           C  C




                                                                                     1Â¨Â¢Â¤% 1Â¨Â¢ Â©Â©
                                                                    1Â¨Â¢G Â© 


                    Â§


                                                                           Â¡/


                     1




Â¢0EÂ¥"G Â©        
                      Â¢G Â©M G Â¢0EÂ¦Â©


      it holds that                                                            !0


                 


                1
                                                              
In the context of feature selection, this axiom implies that where / is the set of sampled permutations on subsets of

the performance on the dataset is divided fully between the size ) . The usage of bounded sets coupled with the method
different features.                                   for the Shapley value estimation, yields an efï¬cient and ro-
                                                      bust way to estimate the contribution of a feature to the task


Axiom 2 (Permutation invariance or symmetry) For any  of classiï¬cation. For a detailed discussion of the MSA frame-


                                          Â§


                                            1




Â¢0EÂ¥"G Â©                 E
                                              Â©  


      and permutation   on    it holds that  Â¢G      work and its theoretical background see [Keinan et al., 2004].


Â§


 


    Â¢ G Â©


   1
                                                      2.2  The  Contribution-Selection Algorithm
This axiom implies that the value is not altered by arbitrarily
renaming or reordering the features.                  The Contribution-Selection algorithm (CSA), described in
                                                      detail in Figure 1, is iterative in nature, and can either adopt


Axiom 3 (Preservation of carrier or dummy-property) For
                              #


                                                     a forward selection or backward elimination approach. Us-


                      G Â¢Â¤%Â£ 9 Â©M G Â¢Â¤%MÂ©    %'&


any game Â¢0EFÂ¥GHÂ© such that           for every


             Â§                                        ing the subroutine contribution, it ranks each feature accord-


              1


E              Â¢0GHÂ©M

  it holds that                                       ing to its contribution value, and then selects 1 features with
                                                      the highest contribution values with forward selection (using
This axiom implies that a dummy feature that does not inï¬‚u-                 1
ence the classiï¬erâ€™s performance indeed receives a contribu- the sub-routine selection) , or eliminates 2 features with the
tion value 0.                                         lowest contribution values with backward elimination (using
                                                      elimination). It repeats the phases of calculating the contri-


Axiom 4 (Additivity or aggregation) For any two games bution values of the remaining features given those already


                          Â§           Â§      Â§


    Â©    Â¢0EÂ¥ Â©          1Â¢GÂ¡  Â©  1"Â¢G Â©! 1Â¨Â¢ Â©


Â¢0EÂ¥"G and     it holds that                         selected (or eliminated), and selecting (or eliminating) new


     "#  Â©JÂ¢Â¤%MÂ© =G Â¢Â¤%MÂ©$%
Â¢L%MÂ©
where Â¢G                                             features, until the contribution values of all candidate features

This axiom applies to a combination of two different payoffs 1Alternatively, the selection sub-routine can use a forward selec-


based on the same set of features. For a classiï¬cation task tion technique instead of 3 ; features are added in ascending order
these may be, for example, accuracy and area under the ROC of their contribution values, as long as the classiï¬erâ€™s performance
curve or false positive rate and false negative rate. In such improves.

                                                    2

                                Â¢


                                  Â¥ ) Â¥ 1
Contribution-Selection-Algorithm( Â  ; )                Name         Classes  Features Train Size Test Size


                                                       Reuters1        3      1579       145        145


     2Â¢Â¡'2Â¢Â£Â¥Â¤ 2 ) Â¦
 1. 1       :=


                                                       Reuters2        3      1587       164        164


           Â§Â Â©Â¨1  2Â¢Â¡'2Â¢Â£Â¥Â¤ 2 )


 2. for each +;                                       Arrhythmia      2       278       280        140


                        +  1 2Â¢Â¡'2Â¢Â£Â¥Â¤ 2 ) )
    2.1. 
 := contribution( ,   ; )                  Internet Ads    2      1558       2200       800



                Â¢                                      Dexter          2      20000      300        300


       

             


 3. if    




                                       #


                                            Â¢          Arcene          2      10000      100        100


        1 2Â¢Â¡'2Â¢Â£Â¥Â¤ 2 ) 1 2Â¢Â¡'2Â¢Â£Â¥Â¤ 2 )Â¡Â£ 
 1
    3.1.        :=          selection(  ; ,  )        I2000           2      2000        40        22
    3.2. goto 2                                                 Table 1: Description of datasets used.
    else




    3.3. return selected                              higher 1 is, the more likely that features with redundant con-


                                                                                           (
                                                                                        
Figure 1: The Contribution-Selection algorithm in its forward se- tribution will be selected. Although 1 minimizes the



                                                      redundancy dependencies of the features, increasing 1 accel-
                                    


lection version.  is the input set of features, is a contribution
                                                     erates the algorithmâ€™s convergence. The algorithmâ€™s halting


value threshold, is the maximal permutation size for calculating       Â¢


the contribution values, 3 is the number of features selected in each criterion depends on , which designates a trade-off between
phase. The contribution routine calculates the contribution value of the number of selected features, and the performance of the



feature  with the pay off function described in this section. The classiï¬er on the validation set. With the forward selection


                                                                     Â¢


                         3
                                                                          
selection routine selects at most features with highest contribu- version, choosing  means that CSA selects features as

tion values that exceed  . In the backward elimination version, long as there exists a feature that is likely to improve the clas-


the selection sub-routine is replaced with an elimination sub-routine siï¬erâ€™s performance, and selects smaller sets of features as Â¢
                                                                          Â¢
which eliminates  features in each phase and the halting criterion is is increased. Increasing has the opposite effect on the size
changed accordingly.
                                                      of the ï¬nal set of features. The more intuitive halting crite-
                                                      rion, to stop when no further performance gain is achieved, is
exceed a contribution threshold Â¢ with forward selection (or too restrictive, while CSAâ€™s halting criterion enables the se-
fall below a contribution threshold Â¢ with backward elimina- lection of features proved useful at later stages, as veriï¬ed
tion).                                                empirically over several datasets.
  The algorithm, without further speciï¬cation of the contri-
bution sub-routine, is a generalization of ï¬lter methods. How- 3 Results
ever, the main idea of the algorithm is that the contribution 3.1 The Data and Benchmark Algorithms
sub-routine, unlike common ï¬lter methods, returns a contri-
bution value for each feature according to its assistance in im- To test CSA empirically we ran a number of experiments
proving the classiï¬erâ€™s performance, which is generated using on seven real-world datasets with number of features rang-
a speciï¬c induction algorithm, and in conjunction with other ing from 278 to 20,000 (Table 1): the Reuters1 dataset and
features. Using the notation in Section 2 and assuming one the Reuters2 dataset both constructed following [Koller and
optimizes the accuracy level of the classiï¬er, the contribution Sahami, 1996] using the Reuters-21578 document collection;
sub-routine for forward selection calculates the contribution the Arrhythmia database from the UCI repository [Perkins


                                                      et al., 2003] ; the Internet Advertisements database from the


                                     Â¢Â¤% Â©
values using the following payoff function G :


                                                      UCI repository [Blake and Merz, 1998] which was collected


        % Â£ 1 2Â¢Â¡'2Â¢Â£Â¥Â¤ 2 )


 1. % :=                                              for the research of identifying advertisements in web pages,


                      ,


                        Â¢. Â©
 2. Generate a classiï¬er + from the training set, Train the Dexter text categorization dataset and the Arcene cancer


                                                      dataset, both from the NIPS 2003 workshop on feature selec-


             ,
              Â¢0./Â©
 3. Evaluate +    for all examples of the validation set,
                                                      tion [Guyon, 2003] and the I2000 microarray colon cancer
    Validation


                                                      dataset [Alon et al., 1999].
                                                 


 4. Return the accuracy level, deï¬ned  as G/Â¢L%MÂ©


                     


                                                       In principle, CSA can work with any induction algorithm


                $!%'& %'(


                                                      0


        7


          ! #" "   1 1*),+




       
                      /


                   
                %Â¥(


            $-%Â¥&                                       . However, due to computational constraints we focused


               1  1.)
               /


                                                      on fast induction algorithms or algorithms that may be ef-
          /Â¦
  The case %B  is an end case which is handled by return- ï¬ciently combined into CSA. We experimented with Naive
ing the number of instances in the largest class divided by the Bayes, C4.5 and 1NN. For each of the datasets, we measured
total number of instances (a classiï¬er which always selects the training set accuracy of each classiï¬er using ten-fold cross
the most frequent class). Backward elimination is quite sim- validation on the whole set features. For each dataset, all sub-
ilar, and the payoff is calculated by sampling permutations sequent work used the induction algorithm 0 , that gave the
from the set of features left after each phase of elimination. highest cross validation accuracy, as detailed in Table 2.

The maximal permutation size ) has an important role in de- Eight different feature selection schemes were then com-
ciding the contribution values of the different features, and pared on the datasets described above:




should be selected in a way that ensures that different com-                    0
                                                        1
binations of features that interact together are inspected. Its The induction algorithm without performing feature


impact is demonstrated in Section 3.                      selection to serve as a baseline.


                                                                                              & (


                                                                                              17698


                                                        1


                                                                                        %3254
  The number of selected features 1 for the selection sub- Regularized linear SVM using the       package
routine controls the redundancies of the selected features; the [Joachims, 1999]. Datasets that had more than two

                                                    3

                                                        1


                              2         )     Â¤
 Dataset      L     1 (Fwd.)   (Bwd.)                     The  Reuters1 dataset. Feature selection using Ran-
 Reuters1     NB       1        100     20   1500         dom  Forests did best, yielding accuracy level of 100%
 Reuters2     NB       1        100     20   1800         with 30 features. Not too far behind is the CSA in its
 Arrhythmia   C4.5     1         50     20   500          backward elimination version (98.6% with 51 features).
 Internet Ads 1NN      1        100     20   1500         [Koller and Sahami, 1996], for example, report that the
 Dexter       C4.5     50        50     12   3500         Markov Blanket algorithm yields approximately 600 se-
 Arcene       C4.5    100       100     5   10000         lected features with accuracy levels of 95% to 96% on


 I2000        C4.5    100       100     3    2000         this dataset.
                                                        1
Table 2: The parameters and the classiï¬er used with the CSA al- The Reuters2 dataset. CSA with backward elimination


gorithm for each dataset. Â  is in the induction algorithm used with did best, yielding accuracy level of 93% with 109 fea-


CSA (NB being Naive Bayes), 3 is the number of features selected tures. For comparison, [Koller and Sahami, 1996] re-


in forward selection in each phase,  is the number of features elim- port that the Markov Blanket algorithm yields approx-

inated in backward elimination in each phase,  is the permutation imately 600 selected features with accuracy levels of

size and Â¡ is the number of permutations sampled to estimate the 89% to 93% on this dataset.
contribution values. For an explanation how hyperparameters are 1 The Arrhythmia dataset. This dataset is considered to
chosen, see text.                                         be a difï¬cult one. CSA with backward elimination did


                                                          best, yielding an accuracy level of 84% with 21 features.


                                                                                                 Â¨Â§
                                                          Forward selection with higher depth value ( ) ) did
    classes were split into few binary classiï¬cation prob-
                                                          better than wrapper, implying that one should consider
    lems.
                                                          many features concomitantly to perform good feature se-


  1 Filtering using mutual information and classiï¬cation us- lection for this dataset. For comparison, the grafting al-
       0
    ing  . We binned continuous domains to estimate the   gorithm [Perkins et al., 2003] yields an accuracy level


    mutual information.                                   of approximately 75% on this dataset.
                                                        1
  1 Filtering using the Pearson correlation coefï¬cient and The Internet Ads dataset. All the algorithms did approx-
    classiï¬cation using 0 .                               imately the same, leading to accuracy levels between
                                                          94%  and 96% with CSA slightly outperforming the oth-
  1 Random  Forests feature selection [Breiman, 2001] and


                    0                                     ers. Interestingly enough, the wrapper algorithm did not
    classiï¬cation using .                                 select any feature; in the ï¬rst phase, the 1NN algorithm

  1 Feature selection using forward selection wrapper. Since had neighbors from both classes with the same distance
    simple wrapper greedily selects a feature that most   for each feature checked, leading to arbitrary selection
    improves the classiï¬erâ€™s validation performance, it is of one of the classes, and the classiï¬erâ€™s performance


                                           (              was constant through all the phase, yielding zero contri-
                                         


    equivalent to forward selection CSA with ) .


                     0
  1                                                       bution values. However, when selecting the higher depth
    Classiï¬cation using after performing feature selection levels, the simple 1NN algorithm was boosted up to out-


    with forward selection CSA and parameters as described perform classiï¬ers such as SVM.


                                                        1
                               Â¤
    in Table 2. The parameters ) and were chosen such that The Dexter dataset. For the Dexter dataset, we used
    the expected number of times that each feature is sam- algorithm 0 (C4.5 decision trees) only for the process
    pled is higher than 5. The contribution value threshold



                           Â¢                              of feature selection, and Linear SVM to perform the
                                
    for stopping selection was  . termination of fea-    actual prediction on the features selected. This was
    ture selection was ï¬xed by choosing a contribution value



            Â¢                                             done because C4.5 did not give satisfying accuracy lev-
               
    threshold    . No hyperparameter selection was per-



                        Â¢                                 els for any of the feature selection algorithms, and it
                    Â¤


    formed on either ) , or .                             is impractical to use SVM with CSA for large datasets.
                     0
  1 Classiï¬cation using after performing feature selection To overcome the difference between the classiï¬ers per-


    with backward elimination CSA and parameters as de-   forming feature selection and the classiï¬er used for the
                                       Â¤
    scribed in Table 2. The parameters ) and were chosen  actual classiï¬cation, we added an optimization phase
    such that the expected number of times that each fea- for the forward selection algorithm after it stopped. In


    ture is sampled is higher than 5. The contribution value this phase, a ten-fold cross-validation is performed on


                                     Â¢
                                          


    threshold for stopping elimination was  . No hy-     the dataset in a similar way to the one used to opti-
                                               Â¤
    perparameter selection was performed on either ) , or mize ï¬lter methods. The simple mutual information fea-
    Â¢                                                     ture selection performed best, followed closely by the
                                                          Contribution-Selection algorithm in its backward elimi-
  To avoid overï¬tting on the validation set used for calcu- nation version and by Random Forests. This implies that


lating the payoff with CSA, we used m-fold cross validation in Dexter the contribution of single features signiï¬cantly


               Â£Â¢ Â¡9 )Â¤Â¢,Â¤ 9Â¦Â¥4*
instead of a single 2     set.                            outweigh the contribution of feature combinations for
3.2  Feature Selection and Classiï¬cation Results          the task of classiï¬cation. The forward selection algo-
                                                          rithm did as well as Linear SVM without feature selec-
Table 3 summarizes the classiï¬ersâ€™ performance on the test tion, but with a signiï¬cantly lower number of features.
set and the number of features selected in each of the ex- 1 The Arcene dataset. Here, just as in the case of Dexter,
periments. The accuracy levels are the fraction of correctly we use C4.5 for the process of feature selection, and Lin-
classiï¬ed test set instances:                             ear SVM to perform the actual prediction on the features

                                                    4 Dataset      Wrapper      Fwd.        Bwd.                  0
                                                                                     Arrhythmia (slp=âˆ’1.21)
 Reuters1      92.4 (7)  96.5 (10)   98.6 (51)                                       Dexter (slp=âˆ’1.20)
 Reuters2      91.4 (5)  90.1 (14)   93.2 (109)             âˆ’1
 Arrhythmia     70 (5)   74.2 (28)   84.2 (21)
 Internet Ads     -       95.6 (8)   96.1 (158)             âˆ’2
 Dexter        80 (10)   92.6 (100)  93.3 (717)
 Arcene         58 (7)    81 (600)   86 (7200)              âˆ’3
 I2000        86.3 (550) 86.3 (500) 90.9 (1100)
                                                            âˆ’4

 No FS   SVM       Corr.        MI         RF               log  frequency
 84.1    94.4    90.3 (20)   94.4 (20)   100 (30)           âˆ’5
 81.1    91.4    88.4 (20)    90.2 (5)   87.2 (21)
 76.4     80     71.4 (20)    70 (20)     80 (40)           âˆ’6
 94.7    93.5    94.2 (15)   95.75 (70)  95.6 (10)
                                                            âˆ’7
 92.6    92.6   92.6 (1240)  94 (230)   93.3 (800)           âˆ’5   âˆ’4.5   âˆ’4    âˆ’3.5   âˆ’3    âˆ’2.5   âˆ’2
 83       83    83% (6600)   81 (5600)   82 (6000)                             log CV
 86.3    72.7   81.8 (260)  90.9 (1060) 86.3 (100)
                                                      Figure 2: Power-law distribution of contribution values. This log-
Table 3: Comparison of accuracy levels and number of features se- log plot of the distribution of the contribution values (absolute value)
lected in the different datasets. Upper table: Wrapper and Fwd/Bwd in the ï¬rst phase for Arrhythmia and Dexter, prior to making any
(CSA with forward selection/backward elimination with parameters feature selection, demonstrates a power law behavior. The corre-
from Table 2). Bottom table: No FS (no feature selection), SVM sponding plots for the other datasets show identical power-law char-
(linear SVM without feature selection), Corr (feature selection using acteristics (though with different slopes), and were eliminated for
Pearson correlation), MI (feature selection using mutual informa- the sake of clarity.
tion), RF (feature selection using Random Forests). Accuracy levels
are calculated by counting the number of misclassiï¬ed instances and
given in percentages. The number of features selected is given in while with backward elimination, there is a gradual and rather
brackets.                                             stable increase in the contribution values of the eliminated
                                                      features. The peaks in the graph of the contribution values in
    selected. The CSA with backward elimination obtained Figure 3A demonstrate that the contribution values do change
    better performance than the rest of the algorithms. as the CSA iterates . In this case, the selection of a single fea-
  1 The I2000 dataset. CSA with backward elimination and ture considerably increased the contribution value of another
    feature selection using mutual information yielded the feature, pointing at intricate dependencies between features.
    best results. The poor performance of CSA with forward Figures 2 and 3 also assist in explaining why back-
    selection can be explained by the poverty of data com- ward elimination usually outperforms several feature selec-
    paring to the number of features; the algorithm selected tion methods, including forward selection; due to the high
    in the ï¬rst phases features which explain well the train- dimensionality of the datasets, a feature that assists in pre-
    ing data by coincidence, and avoided from selecting fea- diction merely by coincidence, may be selected, on the ac-
    tures that truly contribution to the task of classiï¬cation. count of other truly informative features. Forward selection
    This phenomenon is explained in portrait in Section 3.3. is penalized severely in such case: among the few signiï¬-
                                                      cant features, some will not be chosen. However, backward
  In summary, in 5 out of the 7 datasets, CSA with backward
                                                      elimination always maintains the signiï¬cant features in the
elimination achieved the best results. In all other cases, CSA
                                                      non eliminated set; a feature that truly enhances the classi-
achieved the second best result.
                                                      ï¬erâ€™s generalization will do so for the validation set as well,
3.3  A Closer Inspection of the Results               and will not be eliminated. This leads to a more stable gen-
                                                      eralization behavior for backward elimination on the test set
The MSA, intent on capturing correctly the contribution of
                                                      through the algorithmâ€™s progress (Figure 3).
elements to a task, enables us to examine the distribution of
the contribution values of the features. Figure 2 depicts a
log-log plot of the distribution of the contribution values in 4 Final Notes
the ï¬rst phase for Arrhythmia and Dexter, prior to making The Contribution-Selection algorithm presented in this paper
any feature selection. This distribution follows a scale-free views the task of feature selection in the context of coali-
power law, implying that large contribution values (in abso- tional games. It uses a wrapper-like technique combined with
lute value) are very rare, while small ones are quite common, a novel ranking method which is based on the Shapley con-
justifying quantitatively the need of feature selection. The tribution values of the features to the classiï¬cation accuracy.
other datasets were also observed to possess similar power The CSA works in an iterative manner, each time selecting
law characteristic.                                   new features (or eliminating them) while taking into account
  The behavior of the algorithm through the process of fea- the features that were selected (or eliminated) so far.
ture selection/elimination is displayed in Figure 3; after the CSA, similarly to wrapper algorithms, is restricted in the
forward selection algorithm identiï¬es the signiï¬cant features selection of the induction algorithm used for evaluating fea-
in the ï¬rst few phases, there is a sharp decrease in the contri- tures sets, due to time limitations. This problem can be
bution values of the features selected in the following phases, reduced by parallelizing, an advantage not shared by other

                                                    5