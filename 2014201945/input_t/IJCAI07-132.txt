                  Optimistic Active Learning using Mutual Information

                                    Yuhong Guo and Russ Greiner
                                   Department of Computing Science
                                          University of Alberta
                                 Edmonton, Alberta, Canada T6G 2E8
                           { yuhong     | greiner     }@cs.ualberta.ca

                    Abstract                          pages, then produce a classiÔ¨Åer from these labeled pages. An
                                                      active learner is good if it produces the best classiÔ¨Åer from a
    An ‚Äúactive learning system‚Äù will sequentially de- small number of labeled pages.
    cide which unlabeled instance to label, with the
                                                        This paper presents an effective active learner, ‚ÄúMM+M‚Äù,
    goal of efÔ¨Åciently gathering the information nec- based on two ideas: (1) In general, select the query instance
    essary to produce a good classiÔ¨Åer. Some such sys- that provides the maximum conditional mutual information
    tems greedily select the next instance based only about the labels of the unlabeled instances, given the labeled
    on properties of that instance and the few currently data. There is a subtlety here that we resolve by using an
    labeled points ‚Äî e.g., selecting the one closest to ‚Äúoptimistic guess‚Äù about the query‚Äôs label. (2) It can be help-
    the current classiÔ¨Åcation boundary. Unfortunately, ful to allow the selection criterion to depend on the outcomes
    these approaches ignore the valuable information  of previous selections. In our case, if this optimistic guess is
    contained in the other unlabeled instances, which
                                                      wrong for the selected query, MM+M uses a different strategy
    can help identify a good classiÔ¨Åer much faster. For to identify the next query, then returns to use the maximum
    the previous approaches that do exploit this unla- information strategy for the next selection.1
    beled data, this information is mostly used in a con-
                                                        Section 2 presents related works, to help position, and
    servative way. One common property of the ap-
                                                      motivate, our approach. Section 3 then introduces our spe-
    proaches in the literature is that the active learner
                                                      ciÔ¨Åc MM+M   approach, and Section 4 presents experimental
    sticks to one single query selection criterion in the
                                                      results based on our implementation of these ideas. The web-
    whole process. We propose a system, MM+M,
                                                      page   http://www.cs.ualberta.ca/Àúgreiner/
    that selects the query instance that is able to pro-
                                                      RESEARCH/OptimisticActiveLearning           provides
    vide the maximum conditional mutual information
                                                      more information about our approach, and the experiments.
    about the labels of the unlabeled instances, given
    the labeled data, in an optimistic way. This ap-
    proach implicitly exploits the discriminative parti- 2 Related Work
    tion information contained in the unlabeled data. Many previous researchers have addressed this ‚Äúactive learn-
    Instead of using one selection criterion, MM+M    ing‚Äù task in various different ways. Some of their sys-
    also employs a simple on-line method that changes tems use a very simple heuristic to determine which instance
    its selection rule when it encounters an ‚Äúunexpected to label next: select the most uncertain instance, based on
    label‚Äù. Our empirical results demonstrate that this the classiÔ¨Åer produced using the current set of labeled in-
    new approach works effectively.                   stances. Freund et al. [1997] employed a committee of clas-
                                                      siÔ¨Åers and choose the instance on which the committee mem-
1  Introduction                                       bers disagree. Lewis and Gale [1994] used a probabilistic
                                                      classiÔ¨Åer, over binary classes; here this most-uncertain ap-
There are many situations where unlabeled instances are proach would select the instance whose conditional proba-
plentiful and cheap, but it is expensive to label these in- bility P ( yi =1| xi ) is closest to 0.5. The same princi-
stances. For example, consider the challenge of learning a ple is also used in active learning with support vector ma-
classiÔ¨Åer that can determine which webpages contain job ads. chines [Tong and Koller, 2000; Schohn and Cohn, 2000;
One can easily grab literally billions of webpages at essen- Campbell et al., 2000], where it suggests choosing the in-
tially no cost. However, to produce an effective training set, stance closest to the classiÔ¨Åcation boundary. Tong and
we Ô¨Årst need labels for a sufÔ¨Åcient number of these pages; Koller [2000] analyzed this active learning as a version space
unfortunately, this typically requires paying a person to pro- reduction process, while Schohn and Cohn [2000] used a
duce each such label. It is therefore useful to Ô¨Ånd a small set
of pages that (when labeled) will produce a high quality clas- 1The name ‚ÄúMM+M‚Äù is short for ‚ÄúMCMI[min]+MU‚Äù. The Ô¨Årst
siÔ¨Åer. An ‚Äúactive learning‚Äù system will sequentially select point corresponds to ‚ÄúMCMI‚Äù, the subtlety to ‚Äú[min]‚Äù and the second
the most informative pages to label from a pool of unlabeled point to ‚Äú+MU‚Äù.

                                                IJCAI-07
                                                   823                                                                             ‚ó¶‚ó¶
                                                                            ‚ó¶‚ó¶
heuristic search view. Our MM+M algorithm incorporates                       ‚ó¶ B
                                                                            ‚ó¶             ‚ó¶
this approach as one of its components. While this ‚Äúmost            ¬£ ¬†       ‚ó¶          ¬£ ¬†
                                                                  ‚ó¶ ¬¢ ¬°                  ‚àí¬¢ ¬°
                                                                   ‚ó¶ +
uncertain‚Äù approach often works well, we provide empirical            ‚ó¶                   ‚ó¶
results that the complete MM+M system typically works bet-                     ‚ó¶ A
ter.
                                                               Figure 1: Problem with Optimizing (1)
  These ‚Äúmost uncertain‚Äù approaches decide which instance
to select based only on how that instance relates to the cur-
rent classiÔ¨Åer(s), which in turn is based on only a few labeled tion; i.e., the one we currently know least about:
instances; notice in particular this selection does not depend
on the remaining unlabeled instances. As the goal is produc-       argmax  H( Yi | xi,L) .            (1)
                                                                     i‚ààU
ing a classiÔ¨Åer that has a good generalization performance, it
makes sense to use (at least) the marginal distribution P ( x ) where
over this unlabeled data. This motivates a second class of ap-             
proaches, which use this unlabeled data. Cohn et al. [1996] H( Yi | xi,L)=‚àí   P ( yi | xi,L)logP ( yi | xi,L)
and Zhang and Chen [2002] employed the unlabeled data                      yi
by using the prior density P ( x ) as weights. Roy and Mc-
                                                      represents the conditional entropy of the unknown label Yi
Callum [2001] selected instances to optimize expected gen-
                                                      wrt the instance xi given the labeled data L. For binary
eralization error over the unlabeled data. Others also used
                                                      classes, this measure prefers instances whose conditional dis-
the clustering distribution of the unlabeled instances: Xu et
                                                      tribution P ( y =1| x,L) is closest to 0.5. We refer to this as
al. [2003] proposed a representative sampling approach that
                                                      the ‚ÄúMU-instance‚Äù (for ‚Äúmost uncertain‚Äù).
selected the cluster centers of the instances lying within the
margin of the support vector machine. Nguyen and Smeul- Unfortunately, this MU approach is limited in that its
                                                      assessment of an instance involves only the small set of
ders [2004] presented a mathematical model that explicitly
                                                      currently-labeled instances (that produce the classiÔ¨Åer used
combines clustering and active learning together. McCallum
and Nigam [1998] used an EM approach to integrate the in- in this step) but not the distribution of the other unlabeled in-
                                                      stances. To see why this is problematic, consider Figure 1.
formation from unlabeled data, while Muslea et al. [2002]
                                                      The MU-instance here will be the one nearest the bisector ‚Äî
combined active learning with semi-supervised learning.
                                                      ‚ÄúA‚Äù. Notice, however, this label does not provide as much in-
  Our MM+M    uses an ‚Äúoptimistic‚Äù information theoretic formation about the remaining unlabeled instances as B:As
way to use the unlabeled instances: seek the instance whose B is in the midst of a cluster of points, its value will signiÔ¨Å-
optimistic label (i.e., the ‚Äúbest‚Äù of its possible labels) leads
                                                      cantly increase our conÔ¨Ådence in the labels of the neighbors.
to the maximum mutual information about the labels of the
                                                        Based on this observation, we propose using a mutual in-
remaining unlabeled instances. This approach implicitly ex-
                                                      formation criterion for instance selection: select the instance
ploits the clustering information contained in the unlabeled
                                                      whose label will provide maximum mutual information about
data, in an optimistic way.
                                                      the labels of the remaining unlabeled instances, given the la-
  While this optimistic heuristic typically works, it is oc- beled data:
casionally misled. When this happens, MM+M employs
a different rule to select the next instance to label. This argmax{H( YU | XU ,L) ‚àí H( YU | XU ,L,(xi,yi))} (2)
means our selection process is not as myopic as the other ap- i‚ààU
proaches, as it is based on the outcome of the previous in- As the Ô¨Årst term in (2) does not depend on the instance i se-
stance. Our empirical results show that this component is lected, we can rewrite (2) as:
critical to MM+M‚Äôs success.
                                                                 argmin H( YU | XU ,L,(xi,yi)).       (3)
                                                                   i‚ààU
3TheMM+M         Active Learner
                                                      Assuming we use a parametric probabilistic conditional
We   let x  denote  the  input features of  an  in-   model P ( y | x, Œ∏ ) for the classiÔ¨Åcation task, and use maxi-
stance, y   ‚àà{1, ..., K}    denote the  class label,  mum likelihood for parameter Œ∏ estimation, then the labeled
L  =  {(x1,y1),...,(xn,yn)} denote the set of labeled data L +(xi,yi) will produce a classiÔ¨Åer parameterized by
             U                                        Œ∏
instances and  denote the index set for the unlabeled  L+(xi,yi). This allows us to approximate criterion (3) as:
data {x1, ..., xm}. Here, XU refers to the set of unlabeled          
                                                                        H  Y  |  , Œ∏        ,
instances.                                                    argmin      ( u  xu  L+(xi,yi) )        (4)
  Our approach uses probabilistic classiÔ¨Åers that compute the   i‚ààU   u
                                 y                                              
posterior distribution of the class label , conditioned on the H  Y  |   , Œ∏          H  Y |   , Œ∏
input x; our MM+M implementation uses logistic regression. since here ( U XU )=   u‚ààU   ( u xu   ).There-
                                                      fore, maximizing conditional mutual information corre-
3.1  Using Mutual Information                         sponds minimizing the classiÔ¨Åcation uncertainty, i.e., mini-
                                                      mizing entropy, on unlabeled data set. Minimizing entropy on
As the goal of active learning is to learn the best classiÔ¨Åer the unlabeled data has been showed useful in semi-supervised
with the least number of labeled instances, it may make sense learning [Grandvalet and Bengio, 2005]. A smaller classi-
to select the instance whose label provides the most informa- Ô¨Åcation uncertainty usually indicates a larger classiÔ¨Åcation

                                                IJCAI-07
                                                   824                            ‚ó¶
                         ‚ó¶ ‚ó¶ ‚ó¶ A ‚ó¶                                        Line 1

                ¬£ ¬†                ¬£ ¬†                                      ?          ‚ó¶
                ¬¢ ¬°                ¬¢ ¬°                            ‚ó¶
                +                  +                                                   ‚ó¶      ¬®¬® Line 3
                                                                  ‚ó¶
                                                                     ¬£ ¬†               ‚ó¶   ¬®
                           ‚ó¶ B                                    ‚ó¶  ¬¢ ¬°          ‚ó¶ A    ¬®
                                                                     +               ¬®¬®       
                         ‚ó¶‚ó¶‚ó¶                                                          ‚ó¶ B
                ¬£ ¬†                ¬£ ¬†                                          ¬®¬£¬®¬†              Line 2
                ‚àí¬¢ ¬°               ‚àí¬¢ ¬°                   C ‚ó¶                     ‚àí¬¢ ¬°
                                                                              ¬®        ‚ó¶
                                                           ‚ó¶                ¬®
 Figure 2: Why should (6) use the Optimistic Assignment?                  ¬®            ‚ó¶
                                                           ‚ó¶            ¬®
                                                                     ¬®¬®                ‚ó¶

margin, therefore leading to a better classiÔ¨Åcation perfor- Figure 3: The MCMI[min] criterion can guess wrong. (Lines 1
mance. As this means our criterion is directly related to op- and 2 are not shown, to avoid cluttering the image.)
timizing the classiÔ¨Åcation performance, we can view (4) as
a discriminative information criterion for selecting query in- used in many unsupervised and semi-supervised learning al-
stances.                                              gorithms: instances separated by a wide margin usually be-
  Unfortunately, (4) can not be used directly since yi (the long to different classes. This means minimizing classiÔ¨Å-
true value of xi) is unknown. One obvious approach to cation uncertainty is usually consistent with the underlying
resolve this problem is to use the apparently most-likely clustering partitions.
value of yi based on the conditional model Œ∏L trainedonthe
labeled data L ‚Äî i.e., use                            Example:  To make this concrete, consider Figure 2, where
                                                      each  and ‚äï  is a labeled instance, and the ‚ó¶‚Äôs are unla-
            ‚àó
           yi  =argmaxP     ( y | xi, Œ∏L ) .          beled. Assigning A the label  would mean the data is
                       y                              not linearly separable, which would add a great deal of un-
Alternatively, Roy and McCallum [2001] chose to take the certainty to the remaining unlabeled instances; hence the
                                                              H  Y |   , Œ∏
expectation wrt Yi                                    value of  ( u xu   L+(A,) ) for each unlabeled instance
                                                                                   ‚äï
                                                     xu would be high. By contrast, the label would not change
             P  y | , Œ∏  H  Y |   , Œ∏       .         the support vectors, meaning H( Yu | xu, Œ∏L+(A,‚äï) ) would
   argmin     (   xi  L )  ( u xu   L+(xi,y) )  (5)
      i   y                                           be essentially unchanged; i.e., H( Yu | xu, Œ∏L+(A,‚äï) ) ‚âà
                                                      H( Yu | xu, Œ∏L ). Hence MCMI[min] would use the ‚Äúmore in-
(We will later refer to this as the ‚ÄúMCMI[avg]-instance‚Äù.) This formative‚Äù ‚äï label, if it decides to use A. Now consider B,
corresponds to the ‚ÄúSELF-CONF‚Äù approach in [Baram et al., and observe both ‚äï and  are consistent with some linear
2004]. Notice both of these approaches use the P ( y | xi, Œ∏L ) separator. Notice, however, that while the  label will well
information determined by the labeled data; unfortunately, in specify the label of the points below immediately below it,
the typical active learning situations where there are very few the ‚äï label will not. This is why MCMI[min] will use the 
labeled instances L, the labeled data might lead to a bad clas- label for B.
siÔ¨Åer, therefore the P ( y | xi, Œ∏L ) will not be helpful. This This assignment clearly depends on the other unlabeled in-
concern is veriÔ¨Åed in our empirical study.            stances; e.g., if those 3 instances were just above B,then
  We propose instead using an ‚Äúoptimistic‚Äù strategy, which mutatis mutandis, MCMI[min] would use the ‚äï label. Hence,
takes the y value that minimizes the entropy term. Using this the min part of MCMI[min] is exploiting relevant nuances of
optimistic strategy, our MCMI[min] approach would compute the P ( x ) density over the instances themselves, as it seeks
(4) as                                                the labels with the largest separation margin.
                                                        Given the choice between A versus B, our MCMI[min] cri-
          argmin f( i )                                              B            A, ‚äï      A, 
            i‚ààU                                       terion will prefer , as neither ( ) nor ( ) will sig-
                                                      niÔ¨Åcantly increase the information about the labels for the
      where
                                                     instances (beyond the information from the other current la-
          f  i          H  Yu | u, Œ∏L+(x ,y) .  (6)              B
           (  )=miny      (    x        i  )          bels), while could be very informative ‚Äî especially if its
                      u                               label is .
 Notice this measure will give each candidate xi instance its 3.2 On-Line Adjustment
best possible score, over the set of y labels. For instance, if
the classiÔ¨Åer is seeking a linear separator, and if there is a Potential Problem: There is a potential problem with this
setting y for which the classiÔ¨Åer based on L +(xi,y) nicely approach: at the beginning, when we only have a few la-
separates the unlabeled data with a wide margin, then (6) will beled data, there may be many consistent parameter settings
give the score associated with this large margin. We will re- (read ‚ÄúclassiÔ¨Åers‚Äù); here, any method, including ours, may
                ‚àó
fer to this optimal i =argmini‚ààU f( i ) as the ‚ÄúMCMI[min] well ‚Äúguess‚Äù wrong. To illustrate this, consider seeking a lin-
instance‚Äù.                                            ear separator within the data shown in Figure 3. Which point
  This differs from Roy and McCallum  [2001],which    should be selected next?
chooses y in a supervised way, as our approach can be viewed Recall that MCMI[min] will seek the instance that could
as choosing y in an unsupervised way. This approach is moti- lead to the most certainty over the remaining unlabeled in-
vated by the ‚Äúclustering assumption‚Äù [Sindhwani et al., 2005] stances. If A is labeled , then the resulting set of 3 labeled

                                                IJCAI-07
                                                   825MM+M(  U: indices of unlabeled instances; L: labeled instances ) Section 4 provides empirical evidence that this approach
  Repeat                                              works well.
   For each i ‚àà U, compute
     y i               H  Y | x , Œ∏
      ( ):=argmin y  u  ( u  u  L+(xi,y) )           3.3  Approximating Logistic Regression
     f  i        H  Y | x , Œ∏
      (  ):=    u  ( u  u  L+(xi,y(i)) )
     % ie, score based on this minimum y(i) value     Recall our system uses logistic regression, which in general
       ‚àó                                                                         L   {  i,yi }
   Let i := argmini f( i )                            takes a set of labeled instances = (x ) , and seeking
                                                                                                3
     % instance with optimal MCMI[min] score          the parameters Œò that best Ô¨Åt the logistic function
   Purchase true label wi‚àó for xi‚àó
           ‚àó                                                                T    Œî         1
   Remove i from U;add(xi‚àó ,wi‚àó ) to L.                   P ( y | x )=œÉ(‚àíy x Œò)  =                  .
             ‚àó                                                                             ‚àíy  T
   If wi‚àó = y(i ),then                                                            1+exp(     x  Œò)
         ‚àó
     Let i := argmax   H( Yi | xi,L)
                    i‚ààU                               (We use the obvious variant for multiclass classiÔ¨Åca-
     Purchase true label wi‚àó for xi‚àó
             ‚àó                                            [                   ]
     Remove i from U;add(xi‚àó ,wi‚àó ) to L.             tion Schein and Ungar, 2005 .) We use a regularized version,
                                                                               4
  until bored.                                        seeking the Œò that minimizes
End MM+M                                                          
                                                                                    T       Œª    2
           Figure 4: The MM+M Algorithm                  (Œò)=       log(1 + exp(‚àíyx Œò)) +   Œò  .   (7)
                                                                   i                        2
instances would suggest a vertical dividing line (‚ÄúLine 1‚Äù); There are a number of iterative algorithms for this compu-
in fact, it is easy to see that this leads to the largest mar- tation [Minka, 2003]; we use Newton‚Äôs method: Œònew :=
                                                        old    ‚àí1
gin, which means the highest MCMI[min] score. Our algo- Œò  ‚àí HL  gL   where
rithm will therefore select instance A, anticipating this  la-      
                                                                                   T
bel. Now imagine the (unknown) true class dividing line is   gL =        (1 ‚àí œÉ(‚àíyx  Œò)) y x + ŒªŒò
Line 3, which means our MCMI[min] approach was misled              (x,y)‚ààL
‚Äî that is, the optimistic prediction of A‚Äôs label was different
                                                                                          L
from its true label.                                  is the gradient based on the labeled dataset and
                                                                    
  At this point, given these 3 labeled instances, MCMI[min]     ‚àí        œÉ  T      ‚àí œÉ   T      T  ‚àí Œª
would next select C, anticipating that it would be labeled , HL =        (ŒòLx)(1     (ŒòLx)) xx        I
consistent with a horizontal separating line (Line 2), Unfortu-   (x,y)‚ààL
                     C        ‚äï
nately, this too is wrong: ‚Äôs label is , meaning MCMI[min] is the Hessian. If there are n instances of d dimensions,
again guessed wrong. This example suggests it might be help- this requires O(nd2) time per iteration. To compute each
ful to consider a different selection strategy after MCMI[min] MCMI[min] instance, our MM+M algorithm (Figure 4) will
has been misled.                                      have to compute parameters |U|√óK times, as this requires
                                                                                 i ‚àà  U                K
                                                      computing ŒòL+(xi,y) for each      and each of the
Approach: Fortunately, our algorithm can easily detect this Y
‚Äúguessed wrong‚Äù situation in the immediate next step, by sim- classes i.
ply comparing the actual label for A with its optimistically In order to save computational cost, we use a simple way
predicted label. When they disagree, we propose switching to approximate these parameters: Assuming we have a good
from MCMI[min] to another criterion; here we choose MU. estimate of ŒòL, based on the gL and HL, we can then ap-
This is done in our MM+M algorithm, shown in Figure 4.2 proximate the values of ŒòL+(xi,yi), by starting with that ŒòL
If MM+M guesses correctly (i.e., if the label returned for the value and performing just one update iteration, based on the
selected instance is the one producing the minimum entropy), new values of gL+(xi,yi) and HL+(xi,yi):
then MM+M   continues to use the MCMI[min] approach ‚Äî                             ‚àí1
                                                             ŒòL+(x ,y ) := ŒòL ‚àí H         gL+(x ,y )
i.e., if it is ‚Äúconverging‚Äù to the correct separator, it should    i i            L+(xi,yi)    i i
keep going. Otherwise, it will select a single MU-instance, to Moreover, there are easy ways to approximate
help it locate a more appropriate ‚Äúsplit‚Äù in the space, before ‚àí1                 ‚àí1
                                                      HL+(x  ,y )gL+(xi,yi) based on HL gL.
returning to the MCMI[min] criterion.                       i i
  For our example, once MM+M has seen that it was wrong 4 Experiments
about A (i.e., A was labeled ‚äï rather than the anticipated ),
MM+M   will then select a MU-instance ‚Äî i.e., an instance To investigate the empirical performance of our MM+M al-
near the current boundary (Line 2) ‚Äî which means it will gorithm, we conducted a set of experiments on many UCI
select B.WhenB is labeled , we can come closer to the true datasets [UCI, 2006], comparing MM+M with several other
separator, Line 3. Notice this means we will be conÔ¨Ådent that active learning algorithms. The four primary algorithms we
C‚Äôs label is ‚äï, which means we will no longer need to query considered are:
this instance. In this case, the information produced by the 1. MCMI[min]+MU: current MM+M algorithm
MU-instance is more relevant than the MCMI[min]-instance. 2. MCMI[min]: always use the MCMI[min]-instance

                  2                                      3
  2This requires O(|U| ) time to make each selection: Ô¨Årst iterat- Of course, we include x0 =1within each xi to avoid the need
ing over each i ‚àà U, then for each such i, iterating over each u ‚àà U to explicitly separate out the constant bias term.
when computing the f( i ) score. For larger datasets, would could 4In our experiments, we used Œª =0.01 in binary classes, and
use sampling, for both loops.                         Œª =1for others.

                                                IJCAI-07
                                                   826                         pima                                             breast
    0.8                                                1


   0.75
                                                     0.95

    0.7

                                                      0.9

   0.65

                                                     0.85
   Accuracy
                                                     Accuracy
    0.6
                                      AllData                                          AllData
                                      MCMI[min]+MU    0.8                              MCMI[min]+MU
   0.55                               MCMI[min]                                        MCMI[min]
                                      MCMI[avg]                                        MCMI[avg]
                                      Most Uncertain                                   Most Uncertain
    0.5                                              0.75
      0      20      40      60      80     100        0   10  20  30  40  50  60 70  80  90  100 110
              Number of Labeled Instances                      Number of Labeled Instances
                        vehicle                                            vote

    0.8                                               0.93

   0.75                                               0.92

    0.7                                               0.91

   0.65                                               0.9

    0.6                                               0.89

   0.55                                               0.88
   Accuracy
    0.5                                              Accuracy 0.87

   0.45                               AllData         0.86                              AllData
                                      MCMI[min]+MU                                      MCMI[min]+MU
    0.4                                               0.85
                                      MCMI[min]                                         MCMI[min]
   0.35                               MCMI[avg]       0.84                              MCMI[avg]
                                      Most Uncertain                                    Most Uncertain
                                                      0.83
      0      20      40      60      80     100         0  10  20  30  40  50  60  70  80  90 100 110
              Number of Labeled Instances                       Number of Labeled Instances

                           Figure 5: Comparing active learners on various UCI datasets

 3. MCMI[avg]: differs from MCMI[min] by averaging over instances as the unlabeled pool. Each of the active learners
    the y values (5) rather than taking the minimum (6) then sequentially selects 100 instances7 from the unlabeled
    (note, this is same as the SELF-CONF in [Baram et al., pool to add to the labeled set. Every time a new instance
    2004])                                            is labeled, that active learner then retrains a new classiÔ¨Åer
 4. MU: ‚Äúmost uncertain‚Äù; based on (1)                on the increased labeled set and evaluates its performance
                                                      on the test set. In general, let acci( D, A[m]) be the accu-
  We consider the following 17 UCI datasets (we show  racy of the classiÔ¨Åer learned after the A active learner has
the name, followed  by its number of classes, num-    added m  labels to the D database, on the ith run, and let
ber of instances and the number of attributes): AUS-  acc( D, A[m]) be the average of this accuracy values, over
TRALIAN(2;690;14), BREAST(2;286;9); CLEVE(2;303;13);  the 30 runs, i =1..30.
CRX(2;690;15), DIABETES(2;768;8), FLARE(2;1389;13),
                                                        Our MM+M    (‚ÄúMCMI[min]+MU‚Äù) wins on most datasets.
GERMAN(2;1000;20), GLASS2(2;163;9), HEART(2;270;13),  Figure 5 shows these averaged acc( D, A[m]) results for
HEPATITIS(2;155;20), MOFN(2;10;300),  PIMA(2;768;8),
                                                      4 datasets. The PIMA graph shows that MM+M   can do
VOTE(2;435;15),5 IRIS(3;150;4), LYMPHOGRAPHY(4;148;
                                                      well even when MU  does relatively poorly. The fact that
18) and VEHICLE(4;846;18). Note each of the last 3 datasets
                                                      MCMI[min], which did not include the ‚ÄúMU-correction‚Äù, does
has strictly more than 2 classes.
                                                      comparably to MM+M shows that MM+M did not need this
  For each dataset, we ran 30 trials. For each trial, we Ô¨Årst correction here. Now consider BREAST, and notice that
randomly selected 1/3 of the instances from each class to
                                    /                 MCMI[min] was the worst performer here, while MM+M
serve as the test data. From the remaining 2 3, we randomly was able to match MU‚Äôs performance ‚Äî i.e., here the MU-
picked k labeled instances from each class to form the ini-
             L       k                                correction was essential. (We observed here that MM+M
tial labeled set ,where =2for binary-class databases, used this MU-correction as often as possible ‚Äî i.e., for essen-
   k                          6
and  =1for multiclass databases, and left the remaining tially every other query.) That is, when MCMI[min] is work-
                                                      ing well, MM+M can capitalize on it; but when MCMI[min] is
  5As suggested in [Holte, 1993], we remove the‚ÄúPhysician-free-
freeze‚Äù variable from the VOTE data, as it alone leads to 99% accu- misled, MM+M can then fall back on the alternative MU ap-
racy.                                                 proach. In the third database, VEHICLE, we see that MM+M
  6Active learning is harder for small starting Ls, and our start- in fact does better than both MU and MCMI[min]. (This
ing size is smaller than many other projects; e.g., Schein and Un- dataset also shows that MM+M system can perform well even
gar [2005] began with at least 20 instances, and Nguyen and Smeul-
ders [2004] with 10 instances.                           7We used fewer for the 3 smallest databases.

                                                IJCAI-07
                                                   827