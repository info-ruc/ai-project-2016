                Generalizing the Bias Term of Support Vector Machines

                        Wenye Li   and  Kwong-Sak Leung      and  Kin-Hong Lee
                           Department of Computer Science and Engineering
                                 The Chinese University of Hong Kong
                                {wyli, ksleung, khlee}@cse.cuhk.edu.hk


                    Abstract                          ï¬nd a solution to a regularized minimization problem
    Based on the study of a generalized form of rep-
                                                                       m
    resenter theorem and a speciï¬c trick in construct-               1                        2
    ing kernels, a generic learning model is proposed           min        V (yi,f(xi)) + Î³ fK      (2)
                                                                fâˆˆHK m
    and applied to support vector machines. An algo-                    i=1
    rithm is obtained which naturally generalizes the
    bias term of SVM. Unlike the solution of standard
                                                      in HK for some Î³>0. This expression represents a tradeoff
    SVM  which consists of a linear expansion of ker-
                                                      between the empirical error as calculated by the loss func-
    nel functions and a bias term, the generalized algo-
                                                      tion V , and â€smoothnessâ€ of the solution as represented by
    rithm maps predeï¬ned features onto a Hilbert space
                                                      the norm of f in a reproducing kernel Hilbert space (RKHS)
    as well and takes them into special consideration by
                                                      H                                  2
    leaving part of the space unregularized when seek-  K which is induced by a kernel K. Î³ f K is also called
    ing a solution in the space. Empirical evaluations a regularizer. SVM can be partially derived from this frame-
                                                                                              (    (x)) â‰¡
    have conï¬rmed the effectiveness from the general- work with the choice of a hinge loss function V y,f
                                                      max (1 âˆ’   (x)  0)
    ization in classiï¬cation tasks.                            yf    , . The only incompatibility comes from
                                                      the bias term b, which makes the combined model (1) gener-
                                                      ally not in HK any more.
1  Introduction
                                                        In this paper, we follow the regularization point of view.
Support vector machines (SVM) have been shown to per- Based on the study of a generalized regularizer which leaves
form well in many machine learning applications. Based part of the hypothesis space unregularized and a speciï¬c trick
on Vapnikâ€™s seminal work in statistical learning theory[Vap- in constructing kernels, we propose a new learning scheme
nik, 1998], the algorithm starts with the ideas of separating
                                          m           which allows user to predeï¬ne features, and uses these fea-
hyperplanes and margins. Given the data (xi; yi)i=1 where
       d                                              tures and kernel expansions to derive a solution in the hy-
xi âˆˆR    and yi âˆˆ{+1,   âˆ’1}, one searches for the lin- pothesis space, instead of considering kernel expansions only
ear hyperplane that separates the positive and negative sam- as in existing kernel-based methods. When the idea is applied
ples with the largest margin (the distance from the hyper- to SVM, we obtain an algorithm which naturally generalizes
plane to the nearest data point). In the nonseparable case, the bias term of SVM. The generalized term linearly com-
a soft margin method is used to choose a hyperplane that bines the predeï¬ned features instead of being a constant only.
splits the examples as cleanly as possible. Using a kernel
                                                    Different from the empirical results that the existence of the
Kx (x )=K   (x, x ) (a positive deï¬nite function) to extend bias term does not make much signiï¬cance in practice[Rifkin,
the algorithm to the nonlinear case, an optimal hyperplane is 2002], we will show the term is not trivial any more when it is
found which has a simple representation as a linear expansion generalized, and appropriate choices of the term will improve
of kernel functions and a constant                    the applicability of the algorithm.
                      m
                  âˆ—                                     The paper is organized as follows. In section 2, we in-
                 f =     ciKxi + b              (1)
                                                      vestigate a generalized regularizer in the regularized learning
                      i=1
                                                      framework and study its associated solution when it is applied
where each ci is a real number and b is called a bias term. to SVM. In section 3, after introducing a kernel construc-
  In the case of nonseparability, some meanings of the mar- tion trick, with which predeï¬ned features are mapped onto an
gin motivation are lost. [Girosi, 1998; Evgeniou et al., 2000; RKHS, we further clarify the mathematical details in deriv-
Poggio and Smale, 2003] suggest a different view of SVM ing SVM. A new algorithm is proposed in section 4 based on
based on a framework championed by Poggio and other   the previous discussions. Empirical results are demonstrated
researchers[Poggio and Girosi, 1990a; 1990b] which implic- in section 5. Finally, section 6 presents our discussions and
itly treats learning as an approximation problem and tries to conclusions.

                                                IJCAI-07
                                                   9192  Generalized Regularized Learning                   The equation holds for all g âˆˆHK . Speciï¬cally, letting g =
                                                         (Â·)
Suppose HK is the direct sum of two subspaces: HK = H0âŠ• Kx  gives
H        H   =      (   Â·Â·Â·   )              (â‰¤   )                            m
  1, where 0   span  Ï•1,   ,Ï• is spanned by   m                       âˆ—    1
                                                                     P1f  =        Î±iyiKxi
linearly independent features. We consider the generalized                  2Î³
regularized learning by minimizing                                             i=1
              1 m                                    by the reproducing property of kernels. Or,
         min         (    (x )) +     2
                   V  yi,f  i    Î³  P1f K       (3)                                  m
        fâˆˆHK  m                                                                      
                i=1                                             âˆ—     âˆ—      âˆ—    1
                                                               f  =(f   âˆ’ P1f )+        Î±iyiKxi .
                                            H                                     2
where P1f is the orthogonal projection of f onto 1 and                             Î³ i=1
       2
   1 
Î³ P  f K is called a generalized regularizer. When the âˆ—       âˆ—                            âˆ—
model is applied to SVM, we consider as the hinge loss f âˆ’ P1f  is the orthogonal projection of f onto H0,and
                                  V                                            
function mentioned above. By introducing slake variables Î¾i hence it can be represented as p=1 Î»pÏ•p.Sowehave,
corresponding to the empirical error at point xi, our problem
becomes                                                                          m
                    m                                                âˆ— =         +
                 1                                                 f       Î»pÏ•p      ciKxi           (7)
            min       Î¾i + Î³ P1f,P1f                                   p=1       i=1
           fâˆˆH                       K
               K m i=1
                                                              âˆˆR        =  1
satisfying                                            where Î»p     and ci  2Î³ Î±iyi.
                                                        The derived minimizer in (7) satisï¬es the reproduction
                yif (xi) â‰¥   1 âˆ’ Î¾i                                          m
                                                      property. Suppose (xi; yi) comes from a model that is
                         â‰¥   0                                               i=1
                     Î¾i                               perfectly linearly related to {Ï•1, Â·Â·Â·,Ï•}, it is desirable to
where Â·, Â·K denotes the inner product in HK .Wederivethe get back a solution independent of other features. As an ev-
dual quadratic program by using the technique of Lagrange ident result of (3), the property is satisï¬ed. The parameters
multipliers:                                          c1, Â·Â·Â·,cm in (7) will all be zero, which makes the regularizer
               m                                     in (3) equal to zero. As will be shown in the experiments, this
             1                                        property often has the effect of stabilizing the results from dif-
     L  =         Î¾i + Î³ P1f,P1fK             (4)
            m  i=1                                    ferent choices of kernels and regularization parameters prac-
              m                      m              tically.
            âˆ’    Î±i (yif (xi) âˆ’ 1+Î¾i) âˆ’   Î¶iÎ¾i.
              i=1                     i=1             3   Mapping Predeï¬ned Features onto RKHS
                                        i
We want to minimize L with respect to f and Î¾ and maxi- By decomposing a hypothesis space HK and studying a gen-
mize w.r.t. Î±i and Î¶i, subject to the constraints of the primal eralized regularizer, we have proposed a generalized regular-
problem and nonnegativity constraints on Î±i and Î¶i.Taking ized learning model and derived its associated solution which
derivative w.r.t. Î¾i and setting it to zero, we have  consists of a linear expansion of kernel functions and prede-
               âˆ‚L    1                                ï¬ned features. In this section, we will introduce a kernel con-
                  =    âˆ’ Î±i âˆ’ Î¶i =0.            (5)
               âˆ‚Î¾i   m                                struction trick which maps kernel functions and predeï¬ned
                                                      features simultaneously onto a Hilbert space. Then we will
Substituting (5) into (4), we have
                         m              m             show the mathematical details in deriving SVM with a gener-
                                                    alized bias term.
     L = Î³ P1f,P1fK âˆ’     Î±iyif (xi)+   Î±i.   (6)
                        i=1            i=1
         âˆ—                                            3.1  A Kernel Construction Trick
Suppose f is the minimizer to (6). For any f âˆˆHK ,let
  =  âˆ— +          âˆˆR        âˆˆH                        Given features {Ï•1, Â·Â·Â·,Ï•} and a strictly positive deï¬nite
f   f   Î´g where Î´    and g    K .Wehave                      Î¦
                    âˆ—           âˆ—                     function , let us consider the following reproducing kernel
       L  =   Î³ P1f + Î´P1g,P1f  + Î´P1gK
                m                    m                                          
                         âˆ—                                      (x y)=    (x y)+        (x)  (y)
              âˆ’    Î±iyi (f + Î´g)(xi)+    Î±i                   K   ,     H   ,        Ï•p    Ï•p         (8)
                i=1                   i=1                                         p=1
By taking derivative w.r.t. Î´,wehave                  where
                                 m
    âˆ‚L          âˆ—
       =2Î³  P1f  + Î´P1g,P1g  âˆ’     Î±iyig (xi) .              H (x, y)                               (9)
    âˆ‚Î´                       K
                                  i=1                                    
                               âˆ‚L                                                    
      âˆ—                                                    =Î¦(x,   y)+         Ï•p (x) Ï•q (y)Î¦(xp, xq)
Since f is the minimizer, we have âˆ‚Î´ |Î´=0 =0. Then the
problem becomes                                                         p=1 q=1
                         m                                                       
               âˆ—                                               âˆ’      (x)Î¦(x   y) âˆ’      (y)Î¦(x x )
        2Î³ P1f ,P1gK âˆ’     Î±iyig (xi)=0.                          Ï•p        p,        Ï•q       , q  ,
                          i=1                                    p=1                q=1

                                                IJCAI-07
                                                   920                                                                           
T
{  Â·Â·Â·   }                                                 Ëœ     Ëœ     Ëœ                             T
 Ï•1,   ,Ï•  deï¬nes a linear transformation of the prede- where Î» = Î»1, Â·Â·Â·, Î» , Î±    =(Î±1,     Â·Â·Â·,Î±m) ,
ï¬ned features {Ï•1, Â·Â·Â·,Ï•} w.r.t. {x1, Â·Â·Â·x}:
                                                            =(Â·Â·Â·         )T         =(         Â·Â·Â·   )T
                                âˆ’1             Î±1         Î±1,   ,Î±  ,  Î±2         Î±+1,   ,Î±m   ,
   Ï•1 (x)      Ï•1 (x1)  Â·Â·Â·  Ï•1 (x)      Ï•1 (x)      Y    =       (   Â·Â·Â·  )  Y    =       (    Â·Â·Â·   )
                                                        1     diag y1, ,y ,   2    diag y+1,  ,ym ,
    Â·Â·Â·    =      Â·Â·Â·         Â·Â·Â·          Â·Â·Â·                      ,                  ,m
                                                     E1  =  Ï•p (xi)       , E2 =   Ï•p (xi)         and 1
   Ï• (x)       Ï• (x1) Â·Â·Â·  Ï• (x)      Ï• (x)                     p=1,i=1               p=1,i=+1
                                               (10)   is a vector of ones with appropriate size. Taking derivative
                                                           Ëœ
and satisï¬es                                         w.r.t. Î», we obtain
                     1   1 â‰¤ p = q â‰¤ 
          Ï•  (xp)=                     .       (11)                 E1Y1Î±1  + E2Y2Î±2   = 0.          (20)
            q         0   1 â‰¤ p 
= q â‰¤ 
                                                      Then, we have
  This trick was studied in [Light and Wayne, 1999] to pro-
                                                                        T       T          T
vide an alternative basis for radial basis functions and ï¬rst     L = Î³Ëœc HËœc âˆ’ Î±2 Y2HËœc +1 Î±        (21)
used in a fast RBF interpolation algorithm[Beatson et al.,
2000]. A sketch of properties peripheral to our concerns in- Taking derivative w.r.t. Ëœc and setting it to zero, we have
cludes
                                                                     2Î³HËœc âˆ’ HY2Î±2  = 0.
                     Kxp = Ï•p                  (12)
                         
                                                    H
                         1   p = q                    is strictly positive deï¬nite and hence invertible. So we have
              Ï•p,Ï•q    =                       (13)
                     K     0   p 
= q
                                                                               Y2Î±2
                          =0                                              Ëœc =      .                (22)
                    Hxp                      (14)                             2Î³
                         
                   Hxi ,Ï•p   =0                (15)
                         K                          Substituting it back into (21), we have
                          =   (x  x )
              Hxi ,Hxj K    H   i, j           (16)
                                                                1  T  T           1  T  T           T
where 1 â‰¤ p, q â‰¤ ,and +1  â‰¤ i, j â‰¤ m. Another use-    L  =      Î±2 Y2 HY2Î±2  âˆ’    Î±2 Y2 HY2Î±2  + 1  Î±
                                          m                    4Î³                2Î³
ful property is that the matrix H =(H (xi, xj))  is                                        
                                          i,j=+1                 1   1
strictly positive deï¬nite, which will be used in the following          T  T              T
                                                           =   âˆ’       Î±2 Y2 HY2Î±2  âˆ’ 2Î³1  Î±
computations.                                                    2Î³   2
  By property (12), we can see that predeï¬ned features
                                                      So, the problem becomes
{Ï•1, Â·Â·Â·,Ï•} are explicitly mapped onto HK , which has a
                          
subspace H0 = span (Ï•1, Â·Â·Â·,Ï•)=span (Ï•1, Â·Â·Â·,Ï•).By                  1
                                                                       T  T             T
            {   Â·Â·Â·   }                                           min  Î±2 Y2 HY2Î±2   âˆ’ 2Î³1 Î±         (23)
property (13), Ï•1, ,Ï•  also forms an orthonormal basis            Î±  2
of H0.
                                                      satisfying
3.2  Computation                                                   1
                                                           0 â‰¤   â‰¤         E Y     + E Y     = 0
Using the kernel deï¬ned in (8) and (9) and its properties in   Î±   m,and    1  1Î±1    2  2Î±2    .    (24)
(12) and (14), the minimizer in (7) can be rewritten as:
                             m                       The ï¬rst box constraint comes from (5) and the nonnegativ-
                                                                              1
            âˆ—                                         ity of Î±, requiring 0 â‰¤ Î±i â‰¤ (1 â‰¤ i â‰¤ m). The second
           f   =      Î»pÏ•p +     ciKxi         (17)                             m
                                                                                        T
                   p=1       i=1                      equality constraint comes from (20). Y2 HY2 is a strictly
               =   Â·Â·Â·                                positive deï¬nite matrix, and the problem becomes a standard
                            m                      quadratic program(QP). Comparing with SVMâ€™s QP[Vapnik,
                      Ëœ                                  ]
               =      Î»pÏ•p +      cËœiHxi              1998 , the new problem requires  equality constraints in-
                   p=1       i=+1                    stead of one equality constraint. This seems to burden the
      Ëœ     Ëœ                                         computations. Actually, because the major computations in
where Î»1, Â·Â·Â·, Î», cËœ+1, Â·Â·Â·, cËœm are m parameters to be de- solving the QP come from the box constraint, these equality
termined. Furthermore, from the orthogonal property (15)
                                                     constraints will not affect the computations much.
               x                                                                                  Ëœ
between Ï•p and H i ,wehave                              The solution of Ëœc is obtained after Î± by (22). Then Î» comes
                        m                            from a linear program,
                    âˆ—
                 1   =      Ëœi  x              (18)
                P f         c H  i .                                            m
                       i=+1                                                  1
                                                                          min       Î¾i               (25)
                                                                           Î»Ëœ
By property (16), we have                                                     m i=1
                  âˆ—    âˆ—  = ËœcT HËœc
               P1f  ,P1f  K                           satisfying
     Ëœc =(Ëœ    Â·Â·Â· Ëœ )T                                     â›                      â
where     c+1,   , cm . Substituting it into (6), we ob-              m
tain in a matrix representation                             â     Ëœ   +       Ëœ    â  (x )  â‰¥   1 âˆ’
                                          
               yi     Î»pÏ•p         cjHxj    i          Î¾i,
       T       T     T Ëœ   T      T Ëœ          T              p=1       j=+1
L =  Î³Ëœc HËœc âˆ’ Î±1 Y1E1 Î» âˆ’ Î±2 Y2 E2 Î» + HËœc + 1 Î±
                                               (19)                                    Î¾i  â‰¥   0.

                                                IJCAI-07
                                                   9214  A Generalized Algorithm
4.1  Algorithm
Based on the discussions above, an algorithm which general-
izes the bias term of SVM is proposed as follows:
                       m
 1. Start with data (xi; yi)i=1.
 2. For  (â‰¤ m) predeï¬ned linearly independent features
                                       
    {Ï•1, Â·Â·Â·,Ï•} of the data, deï¬ne {Ï•1, Â·Â·Â·,Ï•} according
    to equation (10).
 3. Choose a symmetric, strictly positive deï¬nite function
                                         d     d
    Î¦x (x )=Î¦(x,  x ) which is continuous on R Ã—R .
    Get Hx according to equation (9).
            : Rd â†’R                                   Figure 1: Image classiï¬cation accuracies using coil20 dataset
 4. Deï¬ne f          by                               with different number of training images. Twenty predeï¬ned
                            m                      features along with a Gaussian kernel Î¦ are used by SVM-
                   Ëœ                                 GB; Î¦  is also used by SVM. The kernel and regularization
        f (x)=    Î»pÏ•p (x)+       cËœiHxi (x) , (26)
               p=1           i=+1                    parameters are selected via cross validation. One-versus-one
                                                      strategy is used for multi-classiï¬cation.
    where  the solution of cËœ+1, Â·Â·Â·, cËœm comes from
    a quadratic program (23) and equation (22), and
    Ëœ      Ëœ                                          5   Experiments
    Î»1, Â·Â·Â·, Î» is obtained by solving a linear program (25).
                                                      To evaluate the effectiveness brought by the generalized bias
4.2  Virtual Samples                                  term, two groups of empirical results are reported. The ï¬rst
The new algorithm needs the construction of an orthonormal group of experiments focused on the reproduction property
             
     {   Â·Â·Â·   }                       {  1 Â·Â·Â·  }   of the generalized bias term. It used Columbia university im-
basis Ï•1,   ,Ï•  from predeï¬ned features Ï• ,  ,Ï•                              1
w.r.t. samples via a linear transformation. However, al- age library (coil-20) dataset for image classiï¬cation. The
                                                               1 440       x  Â·Â·Â· x
gorithms, which search for linear hyperplanes to separate the dataset has , images 1, , 1440 evenly distributed in
                                                      20
data, are often not invariant to linear transformations. To pro- classes. After preprocessing, each image was represented
                                                          32 Ã— 32 = 1024
vide the invariance, we consider an alternative approach that as a       dimensional vector with each compo-
                                                                               0     255
avoids the transformation.                            nent having a value between and    indicating the gray
                          x   Â·Â·Â· x                   level at each pixel. Four experiments were performed. Each
  We introduce  virtual points v1 , , v satisfying
                                                     experiment used the ï¬rst 6, 12, 24 and 48 images respectively
                     1  1 â‰¤ p = q â‰¤                within each class for training and the rest for testing.
          Ï•p xvq  =                     .
                       0  1 â‰¤ p 
= q â‰¤                 Using recently developed nonlinear dimensionality reduc-
                                                      tion technqiues[Tenenbaum et al., 2000; Roweis and Saul,
                             x               0
The label yvq of each virtual point vq is deï¬ned to be .We 2000], we found that these images actually form a manifold
study the following problem:                          with a much lower dimension than the original 1, 024 dimen-
                                                      sions. With the prior knowledge, twenty features Ï•1, Â·Â·Â·,Ï•20
                    min  Lv (f)                (27)
                    fâˆˆHK                              were predeï¬ned as follows:
                                                                     
where                                                                   1   3NN  (xi) âˆ© trainp 
= {}
                                                            Ï•p (xi)=
                                                                        0         otherwise
             1             
 Lv (f)=           1 âˆ’ yvq f xvq                            1 â‰¤  â‰¤ 20 1 â‰¤  â‰¤  1440 3    (x)
             m                   +                    where    p     ,    i       , NN     deï¬nes the three
               q=1                                    nearest neighbors in geodesic distance to x,andtrainp is the
                 m
               1                           2          set of training images with class label p. The geodesic dis-
             +      (1 âˆ’ yif (xi)) + Î³ P1f          tance was computed by the graph shortest path algorithm as
              m                 +          K
                 i=1                                  in [Tenenbaum et al., 2000]. Deï¬ning features in this way, we
                    m                                implicitly used a kNN classiï¬er as a generalized bias term.
                 1                           2
         =     +       (1 âˆ’ yif (xi))+ + Î³ P1fK .   Similar ideas can be generalized to combining different ap-
             m    m i=1                               proaches other than kNN.
                                                        Figure 1 compares classiï¬cation accuracies among SVM,
  It can be easily seen that Ï•1, Â·Â·Â·,Ï• have formed an or-
                                                      kNN, and the new algorithm (denoted by SVM-GB). An in-
thonormal basis of H0 w.r.t. the virtual points and there is no
                                                    sightful study of the results may ï¬nd that the 20 predeï¬ned
need to compute Ï• , Â·Â·Â·,Ï• any more. The new regularized
                1                                    features have dominated the learned SVM-GB solution due to
minimization problem in (27) is equivalent to the original one
                                                      the reproduction property, and made the performance of the
in (3) with a hinge loss. The introduction of the virtual points
                                                      algorithm similar to kNN which shows better performance
does not change the scope of the classiï¬cation problem while
                                                      than standard SVM on this dataset.
having avoided the linear transformation to construct an or-
thonormal basis.                                         1http://www1.cs.columbia.edu/CAVE/software/

                                                IJCAI-07
                                                   922                                                      GB has better results than SVM in BoW representation in all
                                                      experiments, which gives us the belief that the embedding of
                                                      pLSA features improves the accuracy.

                                                      6   Discussion and Conclusion
                                                      The idea of regularized learning can be traced back to modern
                                                      regularization theory[Tikhonov and Arsenin, 1977; Morozov,
                                                      1984], which states that the existence of a regularizer helps to
                                                      provide a unique stable solution to ill-posed problems. In this
                                                      paper, we have considered the usage of a generalized regular-
                                                      izer in kernel-based learning. One straightforward reason that
                                                      part of the hypothesis space is left unregularized in the new
Figure 2: Image classiï¬cation accuracies by ï¬xing four regularizer is due to a well-accepted fact that ï¬nite dimen-
groups of kernel and regularization parameters(log-scale). sional problems are often well-posed[Bertero et al., 1988;
                                                      De Vito et al., 2005] and do not always need regularization,
                                                      as in the case of learning problems deï¬ned in a subspace
  The reproduction property also helps to lessen the sensitiv- spanned by a number of predeï¬ned features. Similar idea
ity of the algorithm to the choices of kernels and the regular- was explored in spline smoothing[Wahba, 1990] that leaves
ization parameter Î³. To illustrate the effect, another experi- polynomial features unregularized. In kernel-based learn-
                      48Ã—20
ment was conducted using    training images. Instead of ing, the most similar to our work is the semiparametric SVM
using cross validation for parameter selection, four groups of model[Smola et al., 1998], which combines a set of unreg-
kernel and regularization parameters were ï¬xed. Figure 2 de- ularized basis functions with SVM. However, to our knowl-
picts the results. The generalized bias term makes SVM-GB edge, few algorithms and applications have been investigated
more stable to the variations of kernel and regularization pa- for machine learning tasks from a uniï¬ed RKHS regulariza-
rameters. This property is especially useful concerning the tion viewpoint in a general way.
practical situations where arbitrariness exists in designing With the new regularizer and a speciï¬c trick in construct-
kernels for some applications.                        ing kernels, predeï¬ned features are explicitly mapped onto
  To further study the performance of the new algorithm, the a Hilbert space and taken into special consideration during
second group of experiments was conducted on text catego-
                                   2                  the learning, which differentiates our work from standard
rization tasks using 20-newsgroups dataset . The dataset col- kernel-based approaches that only consider kernel expansions
lects UseNet postings into twenty newsgroups and each group and a constant in learning. For projecting predeï¬ned fea-
         1 000
has about ,    messages. We experimented with its four tures onto an RKHS, the idea of a conditionally positive def-
major subsets. The ï¬rst subset has ï¬ve groups (comp.*), the inite function[Micchelli, 1986] is lurking in the background,
second four groups (rec.*), the third four groups (sci.*) and which goes beyond the discussion of this paper.
the last four groups (talk.*).                          When applying the idea to SVM, we have developed a new
                                   2 000
  For the dataset, we removed all but the , words with algorithm which can be regarded as having generalized the
highest mutual information with the class variable by rain- bias term of SVM. With domain speciï¬c knowledge in pre-
bow package[McCallum, 1996]. Each document was repre- deï¬ning features, this generalization makes the bias term not
sented by bag-of-words (BoW) which treats individual words trivial any more. As conï¬rmed by the experimental results,
                                   10
as features. A linear kernel coupled with predeï¬ned fea- correct choices of features help to improve the accuracy and
        Â·Â·Â·
tures Ï•1, ,Ï•10, which were obtained by probabilistic la- make the algorithm more stable in terms of sensitivity to the
tent semantic analysis (pLSA) [Hofmann, 1999], was used as selection of kernels and regularization parameters.
input for SVM-GB. Experiments were carried out with dif-
             100Ëœ3 200
ferent number (   ,   ) of training documents for each Acknowledgments
subset. One experiment consisted of ten runs and the average
accuracy was reported. In each run, the data were separated This research was partially supported by RGC Earmarked
by the xval-prep utility accompanied in C4.5 package3. Grant #4173/04E and #4132/05E of Hong Kong SAR and
  As a comparison, SVM was tested with a linear kernel, RGC Research Grant Direct Allocation of the Chinese Uni-
which is a commonly used method for text categorizations. versity of Hong Kong.
For completeness, both BoW and pLSA representations of
documents were experimented. As can been seen in ï¬gure 3, References
although SVM with pLSA representation performs well when
                                                      [Beatson et al., 2000] R.K. Beatson, W.A. Light, and
the number of training samples is small, the approach loses
                                                         S. Billings. Fast solution of the radial basis function
its advantage when the training set increases to a moderate
                                                         interpolation equations: Domain decomposition methods.
size. SVM-GB reports the best performance when we have
                                                         SIAM J. Sci. Comput., 22:1717â€“1740, 2000.
800 training documents or more. It is also shown that SVM-
                                                      [Bertero et al., 1988] M. Bertero, C. De Mol, and E.R. Pike.
  2http://www.cs.cmu.edu/ËœTextLearning/datasets.html     Linear inverse problems with discrete data: II. stability and
  3http://www.rulequest.com/Personal/c4.5r8.tar.gz       regularisation. Inverse Probl., 4(3):573â€“594, 1988.

                                                IJCAI-07
                                                   923