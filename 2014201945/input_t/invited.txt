                        What   kind   of a  graphical   model    is the brain?
                                          Geoffrey  E. Hinton
                              Canadian  Institute for Advanced Research  &
                        Department   of Computer  Science, University of Toronto
                                         10 Kings College  Road
                                       Toronto, Canada  M5S  3G4
                                         hinton@cs.toronto.edu

                    Abstract                          recent work in collaboration with Simon Osindero and Yee-
                                                      Whye  Teh that combines these two tricks in a surprising way
     If neurons are treated as latent variables, our vi- to learn a hybrid generative model that was Ô¨Årst proposed by
     sual systems are non-linear, densely-connected   Yee-Whye  Teh. In this model, the top two layers form an
     graphical models containing billions of variables undirected associative memory. The remaining layers form
     and thousands of billions of parameters. Cur-    a directed acyclic graph that converts the representation in
     rent algorithms would have difÔ¨Åculty learning a  the associative memory into observable variables such as the
     graphical model of this scale. Starting with an  pixels of an image. In addition to working well, this hybrid
     algorithm that has difÔ¨Åculty learning more than  model has some other nice features:
     a few thousand parameters, I describe a series
     of progressively better learning algorithms all of 1. The learning Ô¨Ånds a fairly good model quickly even in
     which are designed to run on neuron-like hard-        deep directed networks with millions of parameters and
     ware. The latest member of this series can learn      many hidden layers. For optimal performance, how-
     deep, multi-layer belief nets quite rapidly. It turns ever, a slower Ô¨Åne-tuning phase is required.
     a generic network with three hidden layers and     2. The learning algorithm builds a full generative model of
     1.7 million connections into a very good genera-      the data which makes it easy to see what the distributed
     tive model of handwritten digits. After learning,     representations in the deeper layers have in mind.
     the model gives classiÔ¨Åcation performance that is
     comparable to the best discriminative methods.     3. The inference required for forming a percept is both fast
                                                           and accurate.
                                                        4. The learning algorithm is unsupervised. For labeled
1  Introduction                                            data, it learns a model that generates both the label and
Our perceptual systems make sense of the visual input using the data.
a neural network that contains about 1013 synapses. There 5. The learning is local: adjustments to a synapse strength
has been much debate about whether our perceptual abilities depend only on the states of the pre-synaptic and post-
should be attributed to a few million generations of blind evo- synaptic neuron.
lution or to a few hundred million seconds of visual experi-
                                                        6. The communication is simple: neurons only need to
ence. Evolutionary search suffers from an information bottle-
                                                           communicate their stochastic binary states.
neck because Ô¨Åtness is a scalar, so my bet is that the main
contribution of evolution was to endow us with a learning Section 2 describes a simple learning algorithm for undi-
algorithm that could make use of high-dimensional gradient rected, densely-connected, networks composed of stochastic
vectors. These vectors provide millions of bits of informa- binary variables some of which are unobserved. Section 3
tion every second thus allowing us to perform a much larger shows how to make this simple algorithm efÔ¨Åcient by re-
search in one lifetime than evolution could perform in our stricting the architecture of the network. Section 4 introduces
entire evolutionary history.                          the idea of variational approximations for learning directed
  So what is this magic learning algorithm? I have been in- graphical models in which correct inference is intractable
volved in attempts to answer this question using undirected and describes the ‚Äúwake-sleep‚Äù algorithm that makes use of a
graphical models [Hinton and Sejnowski, 1986], directed variational approximation in a multi-layer, directed network
graphical models [Hinton et al., 1995], or no graphical model of stochastic binary variables. All of these sections can be
at all [Rumelhart et al., 1986]. These attempts have failed as safely ignored by people already familiar with these ideas.
scientiÔ¨Åc theories of how the brain learns because they sim- Section 5 introduces the novel idea of a ‚Äúcomplementary‚Äù
ply do not work well enough. They have, however, produced prior. Complementary priors seem about as probable as father
two neat tricks, one for learning undirected models and one Christmas because, by deÔ¨Ånition, they exactly cancel the ‚Äúex-
for learning directed models. In this paper, I describe some plaining away‚Äù phenomenon that makes inference difÔ¨Åcult indirected models. Section 5.1 includes a simple example of a
complementary prior and shows the equivalence between re-
stricted Boltzmann machines and inÔ¨Ånite directed networks
with tied weights.
  Section 6 introduces a fast, greedy learning algorithm
for constructing multi-layer directed networks one layer at
a time. Using a variational bound it shows that as each new
layer is added, the overall generative model improves. The
greedy algorithm resembles boosting in its repeated use of the
same ‚Äúweak‚Äù learner, but instead of re-weighting each data-
vector to ensure that the next step learns something new, it
re-represents it. Curiously, the weak learner that is used to
construct deep directed nets is itself an undirected graphical
model.
  Section 7 shows how the weights produced by the efÔ¨Åcient
greedy algorithm can be Ô¨Åne-tuned using the ‚Äúup-down‚Äù al- Figure 1: A Boltzmann machine composed of stochastic
gorithm which is a contrastive version of the wake-sleep al- binary units with symmetric connections. When data is
gorithm.                                              clamped on the visible units, a simple stochastic updating
  Section 8 shows the pattern recognition performance of rule infers a conÔ¨Åguration of states of the hidden units that
a network with three hidden layers and about 1.7 million is a good interpretation of the data. If no data is clamped and
weights on the standard MNIST set of handwritten digits. the same updating rule is used for all of the units, the network
When no knowledge of geometry is provided and there is no generates visible vectors from its model.
special preprocessing, the generalization performance of the
network is 1.25% errors on the ofÔ¨Åcial test set. This beats the
                                                             Œ±Œ≤
1.5% achieved by the best back-propagation nets when they where si is the binary state of unit i in the global binary
are not hand-crafted for this particular application, and it is conÔ¨Åguration Œ±Œ≤. If units are chosen at random and their
quite close to the 1.1% or 1.0% achieved by the best support binary states are updated using the stochastic activation rule
vector machines.                                      in Eq. 1 the Boltzmann machine will eventually converge to
  Finally, section 9 shows what happens in the mind of the a stationary probability distribution in which the probability
model when it is running without being constrained by visual of Ô¨Ånding it in any global state is determined by the energy of
input. The network has a full generative model, so it is easy that state relative to the energies of all the other global states:
to look into its mind ‚Äì we simply generate an image from its
                                                                                     Œ±Œ≤
high-level representations.                                           Œ±Œ≤      exp(‚àíE   )
                                                                     P   =             Œ≥Œ¥             (3)
  Throughout the paper, we will consider nets composed of                     Œ≥Œ¥ exp(‚àíE  )
stochastic binary variables but the ideas can be generalized to
other models in which the log probability of a variable is an If we sum over all conÔ¨ÅgurationsP of the hidden units, we get
additive function of the states of its directly-connected neigh- the probability, at thermal equilibrium, of Ô¨Ånding the visible
bours.                                                units in conÔ¨Åguration Œ±
                                                                               exp(‚àíEŒ±Œ≤)
2  The  Boltzmann   machine   learning  algorithm                    P Œ± =   Œ≤                        (4)
                                                                               exp(‚àíEŒ≥Œ¥)
A Boltzmann machine (Hinton and Sejnowski, 1986) is a net-                 PŒ≥Œ¥
work of stochastic binary units with symmetric connections. A Boltzmann machinePcan be viewed as a generative
It is usually divided into a set of ‚Äúvisible‚Äù units which can model that assigns a probability, via Eq. 4, to each possi-
have data-vectors clamped on them, and a set of hidden units ble binary state vector over its visible units. By changing
that act as latent variables (see Ô¨Ågure 1). Each unit, i, adopts the weights and biases, we can change the probability that
its ‚Äúon‚Äù state with a probability that is a logistic function of the model assigns to each possible visible vector. So we can
the inputs it receives from other units, j:           model a set of training vectors by adjusting the weights and
                              1                       biases to maximize the sum of their log probabilities. A nice
        p(si = 1) =                             (1)   feature of Boltzmann machines is that the maximum likeli-
                   1 + exp(‚àíb  ‚àí    s w  )
                             i     j j ij             hood learning rule for the weights is both simple and local.
where bi is the bias of unit i, and wij is thePweight on the sym- We can learn a locally optimal set of weights by collecting
metric connection between i and j. The weights and biases of two sets of statistics:
a Boltzmann machine deÔ¨Åne an energy function over global
                                                         ‚Ä¢ In the positive phase we clamp a training vector on the
                                                 Œ±
conÔ¨Ågurations (i.e. binary state vectors) of the net. Using visible units and then repeatedly update the hidden units
                                              Œ≤
as an index over conÔ¨Ågurations of the visible units, and for in random order using Eq. 1. Once the distribution
conÔ¨Ågurations of the hidden units:                         over hidden conÔ¨Ågurations has reached ‚Äúthermal‚Äù equi-
           Œ±Œ≤           Œ±Œ≤       Œ±Œ≤ Œ±Œ≤                     librium with the clamped data-vector, we sample the
         E    = ‚àí    bisi ‚àí     si sj wij       (2)
                   i         i<j                           hidden state and record which pairs of units are both
                  X         X     on. By repeating this for the entire training set, we can
                    +
     compute < sisj > , the average correlation between i
     and j when data is clamped on the visible units.
   ‚Ä¢ In the negative phase we let the network run freely with
     the visible units unclamped. Their states are updated in
     just the same way as the hidden units. Once the distri-
     bution over global conÔ¨Ågurations has reached equilib-
     rium, we sample the states of all the units and record
     which pairs are both on. By repeating this many times,
                                                      Figure 2: This depicts a Markov chain that uses alternating
     we can compute <s s >‚àí, the average correlation be-
                     i j                              Gibbs sampling. In one full step of Gibbs sampling, the hid-
     tween i and j when the network is running freely and
                                                      den units in the top layer are all updated in parallel by apply-
     therefore producing samples from its generative model.
                                                      ing Eq. 1 to the inputs received from the the current states
  We  can then follow the gradient of the log probability of of the visible units in the bottom layer, then the visible units
the data by using the simple rule                     are all updated in parallel given the current hidden states. The
                          +          ‚àí                chain is initialized by setting the binary states of the visible
           ‚àÜwij = (<sisj>  ‚àí  <sisj>  )        (5)   units to be the same as a data-vector. The correlations in the
where  is a learning rate. It is very surprising that the learn- activities of a visible and a hidden unit are measured after the
ing rule is so simple because the gradient of the log likeli- Ô¨Årst update of the hidden units and again at the end of the
hood with respect to one weight depends in complicated ways chain. The difference of these two correlations provides the
on all the other weights. In the back-propagation algorithm, learning signal for updating the weight on the connection.
these dependencies are computed explicitly in the backward
pass. In the Boltzmann machine they show up as the differ-
ence between the local correlations in the positive and nega- where the superscript 0 indicates that the correlation is mea-
tive phases.                                          sured at the start of the chain with a data-vector clamped on
  Unfortunately, the simplicity and generality of the Boltz- the visible units and n indicates that the correlation is mea-
mann machine learning algorithm come at a price. It can take sured after n full steps of Gibbs sampling. The angle brack-
a very long time for the network to settle to thermal equi- ets denote expectations over both the choice of data-vector
librium, especially in the negative phase when it is uncon- and the stochastic updating used in the Gibbs sampling. This
strained by data but needs to be highly multi-modal. Also, learning rule does not follow the gradient of the log like-
the gradient used for learning is very noisy because it is the lihood, but it does closely approximate the gradient of an-
difference of two noisy expectations. These problems make other function, contrastive divergence, which is the difference
the general form of the algorithm impractical for large net- of two Kullback-Leibler divergences [Hinton, 2002]. Intu-
works with many hidden units.                         itively, it is not necessary to run the chain to equilibrium in
                                                      order to see how the data distribution is being systematically
3  Restricted  Boltzmann   machines   and             distorted by the model. If we just run the chain for a few steps
   contrastive  divergence  learning                  and then lower the energy of the data and raise the energy of
                                                      whichever conÔ¨Åguration the chain preferred to the data, we
If we are willing to restrict the architecture of a Boltzmann will make the model more likely to generate the data and
machine by not allowing connections between hidden units, less likely to generate the alternatives. An empirical inves-
the positive phase no longer requires any settling. With a tigation of the relationship between the maximum likelihood
data-vector clamped on the visible units, the hidden units are and the contrastive divergence learning rules can be found in
all conditionally independent, so we can apply the update rule [Carreira-Perpinan and Hinton, 2005].
in Eq. 1 to all the units at the same time to get an unbiased
                                                         Contrastive divergence learning in a restricted Boltzmann
sample from the posterior distribution over hidden conÔ¨Ågura-
                                                      machine is efÔ¨Åcient enough to be practical [Mayraz and Hin-
tions. This makes it easy to measure the Ô¨Årst correlation in
                                                      ton, 2001]. Variations that use real-valued units and differ-
Eq. 5.
                                                      ent sampling schemes are described in [Teh et al., 2003] and
  If we also prohibit connections between visible units, we have been quite successful for modeling the formation of to-
can update all of the visible units in parallel given a hidden pographic maps [Welling et al., 2003] and for denoising nat-
conÔ¨Åguration. So the second correlation in Eq. 5 can be found ural images [Roth and Black, 2005]. However, it appears that
by alternating Gibbs sampling as shown in Ô¨Ågure 2. Unfortu- the efÔ¨Åciency has been bought at a high price: It is not pos-
nately we may need to run the alternating Gibbs sampling for sible to have deep, multilayer nets because these take far too
a long time before the Markov chain converges to the equilib- long even to reach conditional equilibrium with a clamped
rium distribution. Fortunately, if we start the Markov chain at data-vector. Also, nets with symmetric connections do not
the data distribution, learning still works well even if we only give a causal model in which the data is explained in terms of
                         [           ]
run the chain for a few steps Hinton, 2002 . This gives an underlying causes.
efÔ¨Åcient learning rule:
                                                         The next section describes a simple learning algorithm for
                           0         n
           ‚àÜwij = (<sisj>  ‚àí <sisj>  )         (6)   an apparently quite different type of network that uses di-rected connections. This learning algorithm also has deÔ¨Å-
ciencies, but it can be combined with contrastive divergence
learning in a surprising way to produce an algorithm that
works much better and is signiÔ¨Åcantly more similar to the
real brain.

4  Variational  learning
Inference in directed graphical models that use non-linear,
distributed representations is difÔ¨Åcult because of a phe-
nomenon called ‚Äúexplaining away‚Äù [Pearl, 1988] which cre-
ates dependencies between hidden variables. This is illus-
trated in Ô¨Ågure 3. Radford Neal [Neal, 1992] showed that it
was possible to use Gibbs sampling to perform inference cor-
rectly in multilayer directed networks composed of the same
type of binary stochastic units as are used in Boltzmann ma- Figure 3: A simple logistic belief net containing two inde-
chines. The communication required is more complicated pendent, rare causes that become highly anti-correlated when
than in a Boltzmann machine because in addition to seeing we observe the house jumping. The bias of ‚àí10 on the earth-
the binary states of its ancestors and descendants, a unit needs quake node means that, in the absence of any observation, this
to see the probability that each of its descendants would be node is e10 times more likely to be off than on. If the earth-
turned on by the current states of all that descendant‚Äôs an- quake node is on and the truck node is off, the jump node has
cestors. However, once we have a sample from the poste- a total input of 0 which means that it has an even chance of
rior distribution over conÔ¨Ågurations of the hidden units, the being on. This is a much better explanation of the observa-
maximum  likelihood learning rule for updating the directed tion that the house jumped than the odds of e‚àí20 which apply
connection from j to i is very simple:                if neither of the hidden causes is active. But it is wasteful to
                                                      turn on both hidden causes to explain the observation because
                ‚àÜwji  = sj(si ‚àí sÀÜi)           (7)
                                                      the odds on them both happening are e‚àí10 √ó e‚àí10 = e‚àí20.
where  is a learning rate and sÀÜi is the probability that i would
be turned on by the current states of all its ancestors. There
is no need for a ‚Äúnegative phase‚Äù because directed models do where yŒ± is a conÔ¨Åguration of the hidden units, Q(yŒ±|x) is
not require the awkward normalizing term that shows up in the probability that the sender will choose to use yŒ± in order
the denominator of Eq. 3. Radford Neal showed that logistic to communicate data-vector x, log p(yŒ±) is the cost of com-
belief nets are somewhat easier to learn than Boltzmann ma- municating yŒ± to a receiver who already has the model and
chines, but the use of Gibbs sampling to get samples from the log p(x|yŒ±) is the cost of communicating x to a receiver who
posterior distribution still makes it very tedious to learn large, has both the model and the hidden conÔ¨Åguration yŒ±. Rich
deep nets.                                            soon discovered that it was better to minimize a different
  Rich Zemel and I realised that it might still be possible function and we eventually understood why.
to learn a belief net that contained a layer of binary stochas- Suppose there are two different hidden conÔ¨Ågurations that
tic hidden units even when the cost of computing the pos- give the same communication cost for a data-vector. The
terior distribution, or sampling from it, was prohibitive. In- sender can Ô¨Çip a coin to decide which one to use. After receiv-
stead of trying to perform maximum likelihood learning, we ing the data-vector, the receiver can Ô¨Ågure out what choices
adopted a coding perspective and attempted to learn a model were available to the sender and he can therefore Ô¨Ågure out
that would minimize the description length of the data [Zemel the value of the random bit produced by the coin. So if there
and Hinton, 1995]. The idea is that the sender and receiver are two equally good hidden conÔ¨Ågurations, the sender can
both have access to the model and instead of communicating communicate one additional bit from a random bit stream by
a data-vector directly, the sender Ô¨Årst communicates a hid- his choice of conÔ¨Åguration. In general, the number of extra
den conÔ¨Åguration of the model. This costs some bits, but it bits is equal to the entropy of the sender‚Äôs distribution across
also gives the receiver a good idea of what data to expect. hidden conÔ¨Ågurations. All of these extra bits can be used to
Given these expectations, the data-vector can be communi- communicate some other bit string, so we need to subtract
cated more cheaply1. So it appears that the expected cost of them from the communication cost of the data-vector:
communicating a data-vector is:
                                                         C(x)   =  ‚àí     Q(yŒ±|x) [log p(yŒ±) + log p(x|yŒ±)]
                                                                      Œ±
   C(x) = ‚àí     Q(yŒ±|x) [log p(yŒ±) + log p(x|yŒ±)] (8)                X
             Œ±
            X                                                      ‚àí   ‚àí    Q(yŒ±|x) log Q(yŒ±|x)       (9)
  1Shannon showed that, using an efÔ¨Åcient block coding scheme,
                                                                          Œ±                   !
the cost of communicating a discrete value to a receiver is asymp-       X
totically equal to the negative log probability of that value under If the sender picks hidden conÔ¨Ågurations from their true
a probability distribution that has already been agreed upon by the posterior distribution, the communication cost is minimized
sender and the receiver.                              and is equal to the negative log probability of the data-vectorunder the model. But if it is hard to sample from the true pos- The wake-sleep algorithm works quite well, but the sleep
terior, the sender can use any other distribution he chooses. phase is not exactly following the gradient of the variational
The communication cost goes up, but it is still perfectly well- bound. As a result, it does the wrong thing when a data-
deÔ¨Åned. The sender could, for example, insist on using a fac- vector can be generated by two quite different hidden conÔ¨Ågu-
torial distribution in which the states of the hidden units are rations: Instead of picking one of these hidden conÔ¨Ågurations
chosen independently. The communication cost will then be and sticking with it, it averages the conÔ¨Ågurations to produce
an upper bound on the negative log probability of the data un- a vague factorial distribution that gives signiÔ¨Åcant probability
der the model and by minimizing the communication cost we to many poor conÔ¨Ågurations.
will either push down the negative log probability of the data
or we will make the bound tighter. The looseness of the bound 5 Complementary priors
is just the Kullback-Liebler divergence between the distribu-
tion used by the sender and the true posterior, P (yŒ±|x). The phenomenon of explaining away makes inference difÔ¨Å-
                                                      cult in directed networks. It is comforting that we can still
                                    Q(y  |x)
   ‚àí log p(x) = C(x) ‚àí   Q(y  |x) log  Œ±       (10)   improve the parameters even when inference is done incor-
                             Œ±      p(y |x)           rectly, but it would be much better to Ô¨Ånd a way of eliminat-
                       Œ±               Œ±
                      X                               ing explaining away altogether, even in models whose hidden
  The use of an approximate posterior distribution to bound variables have highly correlated effects on the visible vari-
log p(x) [Neal and Hinton, 1998] is now a standard way to ables. Most people who use directed graphical models regard
learn belief nets in which inference is intractable [Jordan et this as impossible.
al., 1999].                                              In a logistic belief net with one hidden layer, the prior dis-
                                                      tribution over the hidden variables is factorial because their
4.1  The wake-sleep algorithm                         binary states are chosen independently when the model is
A simple way to make use of variational learning in a multi- used to generate data. The non-independence in the posterior
layer logistic belief net is to use a set of ‚Äúrecognition‚Äù con- distribution is created by the likelihood term coming from the
nections that compute a factorial approximation to the pos- data. Perhaps we could eliminate explaining away in the Ô¨Årst
terior distribution in one layer when given the binary states hidden layer by using extra hidden layers to create a ‚Äúcom-
of the units in the layer below [Hinton et al., 1995]. These plementary‚Äù prior that has exactly the opposite correlations
recognition connections will not, in general, have the same to those in the likelihood term. Then, when the likelihood
values as the corresponding generative connections. Given a term is multiplied by the prior, we will get a posterior that
set of recognition weights, it is easy to update the generative is exactly factorial. This seems pretty implausible, but Ô¨Ågure
weights to follow the gradient of the description cost in Eq. 9. 4 shows a simple example of a logistic belief net with repli-
We use a data-vector to set the states of the visible units and cated weights in which the priors are complementary at every
then we use the recognition connections to compute a prob- hidden layer. This net has some interesting properties.
ability for each unit in the Ô¨Årst hidden layer. Then we use
these probabilities to pick independent binary states for all 5.1 An inÔ¨Ånite directed model with tied weights
the units in that layer. This is repeated for each hidden layer
in turn until we have a sample from the approximate poste- We can generate data from the inÔ¨Ånite directed net by start-
rior. Given this sample the learning rule for the generative, ing with a random conÔ¨Åguration at an inÔ¨Ånitely deep hidden
top-down weights is given by Eq. 7. This is the ‚Äúwake‚Äù phase layer and then performing an ancestral pass all the way down
of the wake-sleep algorithm.                          to the visible variables. Clearly, the distribution that we will
  The ‚Äúcorrect‚Äù way to learn the recognition weights is to get over the visible variables is exactly the same as the distri-
follow the derivative of the cost in Eq. 9. The recognition bution produced by the Markov chain in Ô¨Ågure 2 so the inÔ¨Å-
                                                      nite directed net with tied weights is equivalent to a restricted
weights only affect the Q terms; they do not affect the p terms.       2
However, the derivatives w.r.t the Q terms are messy because Boltzmann machine.
Q comes outside the log. So we make a further approxima- We can sample from the true posterior distribution over all
tion. In the ‚Äúsleep‚Äù phase, we perform an ancestral pass to of the hidden layers of the inÔ¨Ånite directed net by starting with
generate a sample from the generative model. Starting at the a data vector on the visible units and then using the transposed
top layer, we pick binary states for the units from their inde- weight matrices to infer the factorial distributions over each
pendent priors. Then we pick states for the units in each lower hidden layer in turn. At each hidden layer we sample from the
layer using the probabilities computed by applying the gener- factorial posterior before computing the factorial posterior for
ative weights to the states in the layer above. Once we have the layer above. This is exactly the same process as starting a
completed an ancestral pass, we have both a visible vector restricted Boltzmann machine at the data and letting it settle
and its true hidden causes. So we can adjust the recognition to equilibrium. It is also exactly the same as the inference
weights to be better at recovering the hidden causes from the procedure used in the wake-sleep algorithm, but in this net
states of the units in the layer below:               it gives unbiased samples because the complementary prior
                                                      at each layer ensures that the posterior distribution really is
                ‚àÜwij =  si(sj ‚àí sÀÜj)          (11)   factorial.

                                                         2
where sÀÜj is the probability that j would be turned on by the We can interpret any undirected model that uses Gibbs sampling
current states of all its descendants.                as an inÔ¨Ånite directed model with tied weights.