          Ensembles of Partially Trained SVMs with Multiplicative Updates

                                   Ivor W. Tsang      James T. Kwok
                           Department of Computer Science and Engineering
                     Hong Kong University of Science and Technology, Hong Kong
                                       {ivor,jamesk}@cse.ust.hk


                    Abstract                            A much simpler approach is to use multiplicative updates,
                                                      which was ï¬rst explored in [Cristianini et al., 1999].How-
    The training of support vector machines (SVM) in- ever, its convergence is sensitive to a learning rate that has to
    volves a quadratic programming problem, which is  be ï¬xed manually. When the learning rate is too large, over-
    often optimized by a complicated numerical solver. shooting and oscillations occur; while when it is too small,
    In this paper, we propose a much simpler approach convergence is slow.
    based on multiplicative updates. This idea was ï¬rst A second problem with the multiplicative update rule is
    explored in [Cristianini et al., 1999], but its con- that it can only work with the hard-margin SVM. However,
    vergence is sensitive to a learning rate that has to on noisy data, the hard-margin SVM has poor performance
    be ï¬xed manually. Moreover, the update rule only  and the soft-margin SVM, which can trade off complexity
    works for the hard-margin SVM, which is known     with training error, is usually preferred. However, the choice
    to have poor performance on noisy data. In this pa- of this tradeoff parameter, which can be from zero to inï¬n-
    per, we show that the multiplicative update of SVM ity, is sometimes critical. Various procedures have been pro-
    can be formulated as a Bregman projection prob-   posed for its determination, such as by using cross-validation
    lem, and the learning rate can then be adapted au- or some generalization error bounds [Evgeniou et al., 2004].
    tomatically. Moreover, because of the connection  However, they are typically very computationally expensive.
    between boosting and Bregman distance, we show      Another possibility that avoids parameter tuning com-
    that this multiplicative update for SVM can be re- pletely is by using the ensemble approach. [Kim et al., 2003]
    garded as boosting the (weighted) Parzen window   combines multiple SVMs with different parameters, and ob-
    classiï¬ers. Motivated by the success of boosting, tains improved performance. However, as a lot of SVMs still
    we then consider the use of an adaptive ensemble  have to be trained, this approach is also very expensive.
    of the partially trained SVMs. Extensive experi-
                                                        In this paper, we ï¬rst address the learning rate problem by
    ments show that the proposed multiplicative update
                                                      formulating the multiplicative update of SVM as a Bregman
    rule with an adaptive learning rate leads to faster
                                                      projection problem [Collins and Schapire, 2002]. It can then
    and more stable convergence. Moreover, the pro-
                                                      be shown that this learning rate can be adapted automatically
    posed ensemble has efï¬cient training and compa-
                                                      based on the data. Moreover, because of the connection be-
    rable or even better accuracy than the best-tuned
                                                      tween boosting and Bregman distance [Collins and Schapire,
    soft-margin SVM.
                                                      2002; Kivinen and Warmuth, 1999], we show that this mul-
                                                      tiplicative update can be regarded as a boosting algorithm in
                                                      which the (weighted) Parzen window classiï¬ers are the base
1  Introduction                                       hypotheses, and each base hypothesis added to the ensemble
Kernel methods, such as the support vector machines   is a partially trained hard-margin SVM. Now, in the boosting
(SVMs), have been highly successful in many machine learn- literature, it is well-known that the whole ensemble performs
ing problems. Standard SVM training involves a quadratic better than a single base hypothesis. Hence, to address the
programming (QP) problem, which is often solved by a com- possibly poor performance of the hard-margin SVM, we con-
plicated numerical solver. Moreover, while a general-purpose sider using the whole ensemble of partially trained SVMs for
QP solver can be used, they are inefï¬cient on this particu- prediction. Note that this comes at little extra cost in the mul-
lar type of QPs. Consequently, a lot of specialized optimiza- tiplicative updating procedure. Experimentally, this ensemble
tion techniques have been developed. The most popular ap- is observed to have comparable or even better accuracy than
proach is by using decomposition methods such as sequential the best-tuned soft-margin SVM.
minimization optimization (SMO) [Platt, 1999],whichin-  The rest of this paper is organized as follows. The learn-
volves sophisticated working set selection strategies, advance ing rate problem in multiplicative update is addressed in Sec-
caching schemes for the kernel matrix and its gradients, and tion 2. The connection between this update process and
also heuristics for stepsize prediction.              boosting, together with the proposed ensemble of partially

                                                IJCAI-07
                                                  1089trained hard-margin SVMs, is then discussed in Section 3. 2.2 Bregman Projection
Experimental results are presented in Section 4, and the last In this section, we assume that we have access to the optimal
section gives some concluding remarks.                value of Ïâˆ—. We will return to the issue of determining Ïâˆ— in
                                                      Section 2.3.
                                                                             m             
2  Multiplicative Updating Rule of SVM                  As Î±  âˆˆ Pm  = {Î±  âˆˆ R   | Î± â‰¥ 0, Î± 1 =1},them-
                                                      dimensional probability simplex, a natural choice of the Breg-
In this paper, we focus on a particular variant of the hard- man distance is the (unnormalized) relative entropy. At the t-
margin SVM, called Ï-SVM in the sequel, which will be th iteration, we minimize the Bregman distance between the
introduced in Section 2.1. We then develop in Sections 2.2                              (t)     (t) 
                                                      new Î± and the current estimate Î±t =[Î±1 ,...,Î±m ] , while
                                                                           âˆ‚u(Î± ,Ïâˆ—)
and 2.3 an iterative multiplicative updating procedure based  Î±          Î±    t      Î± KËœ Î± âˆ’ Ïâˆ—1
on the Bregman projection [Collins and Schapire, 2002; requiring to satisfy   âˆ‚Î±    =   (    t     )=0,
Kivinen and Warmuth, 1999]. The convergence of this it- which is similar to the constraint in (5). We then have the
                                                      following entropy projection problem:
erative procedure is shown in Section 2.4.
                                                               Xm â€œ     Î±            â€
                                                          min      Î± ln  i âˆ’ Î±  + Î±(t) : Ïâˆ— = Î±KËœ Î± .
2.1  Hard-Margin   Ï-SVM                                            i    (t)   i   i              t    (7)
                                                         Î±âˆˆPm           Î±
                       m               d                       i=1       i
Given a training set {xi,yi}i=1,wherexi âˆˆ R and yi âˆˆ R,
                                 
the Ï-SVM ï¬nds the plane f(x)=w   Ï•(x) in the kernel-   Introducing Lagrange multipliers for the constraints Ïâˆ— =
induced feature space (with feature map Ï•) that separates the Î±KËœ Î± Î±1
                             Ï/w                          t and    =1(it will be shown that the remaining
two classes with maximum margin    :                  constraint Î± â‰¥ 0 is inactive and so no Lagrange multiplier
              2                                      is needed) and on setting the derivative of the Lagrangian
 max 2Ï âˆ’w     : yiw Ï•(xi) â‰¥ Ï, i =1,...,m.   (1)   
  w,Ï                                                   m        Î±i       (t)      âˆ—     Ëœ        
                                                        i=1 Î±i ln (t) âˆ’Î±i +Î±i âˆ’Î·t Ï âˆ’ Î± KÎ±t   âˆ’Î»(Î±  1âˆ’1)
                                                                Î±i
The corresponding dual is:                                                            (t)
                                                      w.r.t. Î± to zero, we have Î±i = Î±i exp(âˆ’Î·tyift(xi)+
                                                      Î»                                           Î±1
            Î±KËœ Î±    Î± â‰¥ 0,  Î±1    ,                 ) on using (3).  Substituting this back into    =
         minÎ±       :             =1            (2)                     Î»     âˆ’   Z  Î·          Z  Î·
                                                      1, we then obtain   =    ln  t( t),where  t( t)=
                                                        m  Î±(t)    âˆ’Î·  y f x
where 0 and 1 are vectors of zero and one respectively, Î± = i=1 i exp( t i t( i)), and thus
                                              Ëœ
[Î±1,...,Î±m] is the vector of Lagrange multipliers, and K =      (t+1)   (t)
                                                              Î±        Î±      âˆ’Î·tyift xi /Zt Î·t .     (8)
[yiyjk(xi, xj )]. It can be easily shown that                   i   =   i exp(       (  ))   ( )
         Xm                    Xm                     Such an update is commonly known as multiplicative up-
     w =    Î±iyiÏ•(xi), and f(x)=   Î±iyik(xi, x). (3)  date [Cristianini et al., 1999] or exponentiated gradient (EG)
         i=1                    i=1                   [Kivinen and Warmuth, 1997], with Î·t playing the role of
                                                      the learning rate. Notice that by initializing2 Î±1 > 0,then
Because of the zero duality gap, the objectives in (1) and (2) Î± > 0
                  1                                     t     in all subsequent iterations. Hence, as mentioned
are equal at optimality , i.e.,                       earlier, Î± â‰¥ 0 in (7) is an inactive constraint. Finally, it can
                    âˆ—    âˆ—   âˆ—                       be shown that Î·t can be obtained from the dual of (7), as:
                   Ï = Î±   KËœ Î± .               (4)
                                                                                       âˆ—
                                                                   max  âˆ’ ln(Zt(Î·t)exp(Ï Î·t)).        (9)
Denote                                                              Î·t
                    1   Ëœ       
           u(Î±, ÏÂ¯)= Î± KÎ±  âˆ’ ÏÂ¯(Î± 1 âˆ’ 1),               As mentioned in Section 1, a similar multiplicative updat-
                    2
                                                      ing algorithm for the standard hard-margin SVM is also ex-
     Ï
where Â¯ is the Lagrangian multiplier for the dual constraint plored in [Cristianini et al., 1999]. However, the learning rate
Î±1
    =1. Moreover, the Karush-Kuhn-Tucker (KKT) condi- Î· there is not adaptive and the performance is sensitive to the
tions lead to:                                                       Î·                        Ï
                                                     manual choice of . On the other hand, for our -SVM vari-
                     âˆ‚u Î±, Ïâˆ—                        ant, the value of Î·t can be automatically determined from (9).
            0 â‰¤ Î±âˆ—âŠ¥     (  Â¯ )   â‰¥ 0,
                        âˆ‚Î±                     (5)   The improved convergence properties of our adaptive scheme
                               Î±âˆ—                     will be experimentally demonstrated in Section 4.1.
where âŠ¥ denotes that the two vectors are orthogonal, and
                                                      2.3  Estimating the Value of Ïâˆ—
 âˆ‚u(Î±, ÏÂ¯)                                                                                   âˆ—
         = KËœ Î± âˆ’ ÏÂ¯1 =[y f(x ) âˆ’ Ï,...,yÂ¯ f(x ) âˆ’ ÏÂ¯], We now return to the question of estimating Ï . Recall that
   âˆ‚Î±                 1   1         m   m       (6)                        1                   âˆ—
                                                      we initialize Î± as Î±1 = m 1. From (2) and (4), Ï is equal to
                               Ï                      the optimal dual objective (which is to be minimized), and so
on using (3). Moreover, the optimal Â¯ (i.e., the dual variable    âˆ—
of the dual) is indeed equal to the optimal primal variable Ï: the optimal Î± should yield a smaller objective than that by
                                                                               1   Ëœ
                                                      Î±1 (which is equal to ÏÂ¯1 = m2 1 K1). Moreover, from (4),
                     Ïâˆ—   Ïâˆ—,                                 âˆ—                     âˆ—
                      Â¯ =                             we have Ï â‰¥ 0 as K  0. Hence, Ï âˆˆ [0, ÏÂ¯1].
                                                        In the following, we consider using a decreasing sequence
  Ïâˆ—   Ïâˆ—Î±âˆ—1   Î±âˆ—KËœ Î±âˆ—  Ïâˆ—                                                   âˆ—
as Â¯ =Â¯       =         =    on using (4), (5) and (6). of Ïtâ€™s such that ÏÂ¯1 â‰¥ Ït â‰¥ Ï . Consequently, the constraint
  1                                             âˆ—        2                     Î±  =  1 1
   Here, variables at optimality are denoted by the superscript . In this paper, we initialize as 1 m .

                                                IJCAI-07
                                                  1090                                âˆ—      Ëœ
in (7) has to be replaced by Ït â‰¥ Ï = Î± KÎ±t,andthe    Algorithm 1 Training the Ï-SVM.
entropy projection problem becomes                     1: Input: S = {(xi,yi)}i=1,...,m, tolerance 1 and .
       Xm â€œ                  â€                                           (1)
                Î±i         (t)           
  min      Î±  ln    âˆ’ Î± + Î±     :  Ï â‰¥ Î± KËœ Î± .        2: Initialize t =1: Î±i =1/m for all i =1,...,m,and
             i   (t)   i   i        t       t  (10)
 Î±âˆˆPm           Î±                                                Ëœ
       i=1       i                                        ÏÂ¯1 = Î±1KÎ±1.
           Ëœ                                                             ft xi
Let ÏÂ¯t â‰¡ Î±tKÎ±t. We ï¬rst consider the feasibility of (10). 3: Use (3) to compute ( ).
                                                       4: Set Ït =Â¯Ït/(1 + t).
Proposition 1. For any 1 >t > 0,ifÏÂ¯t satisï¬es
                                                       5: Set Î·t =argminÎ·â‰¥0  Zt(Î·)exp(ÏtÎ·).IfÎ·t <  0,then
                   1 âˆ’ t   âˆ—
                 Ï       â‰¥ Ï  >  ,                        t = t/2, and goto Step 8 to test whether t becomes too
                 Â¯t            0              (11)
                   1+  t                                  small; else if Î·t =0,thensetT = t âˆ’ 1 and terminate.
                                 ÏÂ¯t     Ëœ
                Î± âˆˆ Pm              â‰¥  Î± KÎ±t           6: Update the Lagrangian multipliers Î±iâ€™s using (8), as
then there exists an    such that 1+t       .
                                                          Î±(t+1)   Î±(t)    âˆ’Î· y f x   /Z  Î·
            Ëœ               Ëœ                             i    =   i exp (  t i t( i))  t( t), and compute
Proof. Since K  0,wehavev  Kv  â‰¥ 0 for any vector v.               Ëœ
              âˆ—                                           ÏÂ¯t+1 = Î±t+1KÎ±t+1.
Put v = Î±t âˆ’ Î± ,then
                                                       7: If ÏÂ¯t â‰¥ ÏÂ¯t+1,thent+1 = t and t â† t +1and goto
         Î± KËœ Î± â‰¥ Î±âˆ—KËœ Î± âˆ’ Î±âˆ—KËœ Î±âˆ—.
          t    t  2       t                    (12)       Step 3; else t = t/2 and check whether t <.
From (4), the condition                                8: If t â‰¥ , then goto Step 4; else set T = t âˆ’ 1 and
      1 âˆ’ t    âˆ—        Ëœ   1 âˆ’ t   âˆ— Ëœ âˆ—             terminate.
    ÏÂ¯t     â‰¥  Ï   â‡’   Î±tKÎ±t       â‰¥  Î±  KÎ±  . (13)              f      x    f    x
      1+t                    1+t                     9: Output: SVM(   )=   T +1( ).
Summing (12) and (13), we have
                 ÏÂ¯t     âˆ— Ëœ
                     â‰¥ Î±  KÎ±t,                 (14)   2.4  Convergence
               1+t
             âˆ—                                        In this section, we will design an auxiliary function, which is
and thus Î± = Î± satisï¬es the condition.
                                                      then used to lower bound the amount that the loss decreases at
               1âˆ’t    âˆ—
  Hence, when ÏÂ¯t 1+ â‰¥ Ï , a feasible solution of (10) ex- each iteration [Collins and Schapire, 2002]. Denote the rela-
                  t                                                                                  Â·, Â·
          Î±t+1              Î±                         tive entropy (which is our Bregman distance here) by Î”( ).
ists. We set   as the optimal of (10). Notice that (10)                     âˆ—
differs from (7) only in that the equality constraint in (7) now From (14), the optimal Î± solution of (2) is in the intersec-
becomes an inequality constraint. Hence, optimization of tion of all the hyperplanes deï¬ned by the linear constraints
                                                             Ëœ
(10) is almost exactly the same as that in Section 2.2, except Ït â‰¥ Î± KÎ±t for all tâ€™s. Using (14), (15) and (16), we have
that optimization of Î·t now has an extra constraint Î· â‰¥ 0.              Ëœ       âˆ— Ëœ
                                Ëœ                                  Î±t+1KÎ±t  â‰¥ Î±  KÎ±t.               (18)
  Subsequently, we set ÏÂ¯t+1 = Î±t+1KÎ±t+1 and Ït+1 ac-
cording to                                            Now, the decrease in loss at two consecutive iterations is:
                          ÏÂ¯t
                    Ï         .                               Î±âˆ—, Î±   âˆ’   Î±âˆ—, Î±
                     t =                      (15)         Î”(      t)  Î”(     t+1)
                        1+   t                               m                    
                                   Ëœ                       
From the KKT condition of (10), Î·t(Î±t+1KÎ±t âˆ’ Ït)=0.              âˆ—     (t+1)     (t)           
                                                        =      Î±i  ln Î±i   âˆ’ ln Î±i   (as Î±t1 = Î±t+11 =1)
                3
This, together with Î·t > 0, leads to                        i=1
                      Ëœ                                        m
                  Î±t+1KÎ±t  =  Ït.              (16)             
                                                            âˆ’Î·     Î±âˆ—y f  x  âˆ’   Z  Î·
Using (3), then                                         =      t    i i t( i)  ln t( t) (using (8))
                                                               i=1
                 Î±  KËœ Î±
                  t+1   t      1     ÏÂ¯t                        m
     cos Ï‰t,t+1 = âˆš âˆš     =              ,     (17)                 (t+1)
                  ÏÂ¯t ÏÂ¯t+1  1+t   ÏÂ¯t+1               â‰¥âˆ’Î·t       Î±i   yift(xi) âˆ’ ln Zt(Î·t)(using (18))
where Ï‰t,t+1 is the angle between wt and wt+1. Recall that      i=1
                                                            m                       
this entropy projection procedure is used to solve the dual in   (t+1)    (t+1)    (t)
(2). When (2) starts to converge, the angle Ï‰t,t+1 tends to =  Î±i     ln Î±i   âˆ’ ln Î±i  ,  (using (8))
zero, and cos Ï‰t,t+1 â†’ 1.Ast > 0,ifÏÂ¯t+1 > ÏÂ¯t, from       i=1
(17), the optimization progress becomes deteriorated. This
implies that the current t is too large, and we have overshot or, for all t,
and this solution is discarded. This is also the case when (10) âˆ—     âˆ—
                                                       Î”(Î±  , Î±t) âˆ’ Î”(Î± , Î±t+1) â‰¥ Î”(Î±t+1, Î±t) â‰¡ A(Î±t), (19)
is infeasible. Instead, we can relax the constraint in (10) by
setting t â† t/2 to obtain a larger Ït. This is repeated until where A(Â·) is the auxiliary function. As A(Â·) never increases
                                 âˆ—
ÏÂ¯t+1 â‰¤ ÏÂ¯t. Note that we always have Ï â‰¤ Â·Â·Â· â‰¤ ÏÂ¯t+1 â‰¤ ÏÂ¯t and is bounded below by zero (as Î”(Â·, Â·) â‰¥ 0), the sequence
          
  Ïâˆ—    Î±âˆ— KËœ Î±âˆ—    Î±âˆ—                                of A(Î±t)â€™s converges to zero. Since Î± âˆˆ Pm is compact, by
as   =          and    is optimal. Optimization of (10)                                   âˆ—
                      Î±t+1    Ït+1                    the continuity of A and the uniqueness of Î± , the limit of this
then resumes with the new  and    . The whole process                                                  âˆ—
           t                                        sequence of minimizers converges to the global optimum Î± .
iterates until is smaller than some termination threshold .                                      
The complete algorithm4 is shown in Algorithm 1.        Moreover, using (15) and (16), we have Î±t+1 KËœ Î±t =
                                                                
                                                       ÏÂ¯t      t Ëœ t                      Ëœ     tÏÂ¯t
  3                                                       .AsÎ±   KÎ±   =Â¯Ït,then|(Î±t âˆ’ Î±t+1) KÎ±t| =    .In
   When Î·t =0, no process can be made as we achieve the global 1+t                               1+t
                                          âˆ—                                               Ëœ
optimal. Hence, we terminate the algorithm and Î±t = Î± . the special case where ft(xi) âˆˆ [âˆ’1, 1], KÎ±tâˆ â‰¤ 1.Us-
  4                                                                                                  tÏÂ¯t
                         =0.005          1 =0.1                                    Î±t+1 âˆ’ Î±t1 â‰¥
   In the experiments, we use   and initialize   .    ing the HÂ¨olderâ€™s inequality, we have         1+t .

                                                IJCAI-07
                                                  1091                                                                       (t)                 (t)
We then apply the Pinskerâ€™s inequality [Fedotov et al., 2001] are identical, with Î±i playing the role of di (compare (8)
and obtain                                            and (21)), and Î·t the role of ct (compare (9) and (20)). This is
                             2 2                      a consequence of the fact that boosting can also be regarded
                            t ÏÂ¯t
              Î±t+1, Î±t >          .                                      [                        ]
           Î”(        )          2                    as entropy projection Kivinen and Warmuth, 1999 .From
                         2(1 + t)                     this boosting perspective, we are effectively using base hy-
           tÏÂ¯t                                      pothesis of the form (3), with Î± âˆˆ Pm (not necessarily equal
Let Î½ =min1+  , and summing up (19) at each step, then
              t                                       to Î±âˆ—). This base hypothesis can be regarded as a variant of
                                T                    the Parzen window classiï¬er, with the patterns weighted by
       âˆ—           âˆ—
   Î”(Î±  , Î±1) âˆ’ Î”(Î± , Î±T +1) â‰¥     Î”(Î±t+1, Î±t),       Î±                 6                        t
                                                        . Note that the edge of the base hypothesis at the -th itera-
                                t=1                           m         m    (t)               Ëœ
                                                      tion is i=1 Î±iyi  j=1 Î±i yik(xi, xj ) = Î± KÎ±t. Thus,
       â‡’    m  â‰¥    Î±âˆ—, Î±   >TÎ½2/     ,
          ln     Î”(      1)          2                the constraint in (7) is the same as requiring that the edge of
                                                                                                 âˆ—
and so the number of iterations required is at most T = the base hypothesis w.r.t. the Î±i distribution to be Ï .
 2lnm                                                                                              Ï
 Î½2  for a ï¬xed . This gives a faster ï¬nite convergence The same correspondence also holds between the -SVM
                                                                                             âˆ—         7
result than the linear asymptotic convergence of the SMO al- algorithm (Algorithm 1) and the AdaBoostÎ½ algorithm ,
       [        ]                                     with additionally that Ït in Ï-SVM is analogous to t in
gorithm Lin, 2001 .                                           âˆ—
                                  t                   AdaBoostÎ½. Moreover,  in the Ï-SVM is similar to Î½ in
  Assuming that (11) is satisï¬ed for all and following the    âˆ—
convergence proof here5, the solution of the Ï-SVMâ€™s dual, AdaBoostÎ½ in controlling the approximation quality of the
       âˆ—                                Ëœ            maximum margin. Note, however, that Î½ is quite difï¬cult to
Î±t â†’ Î±   in at most T iterations such that Î±T +1KÎ±T +1 â†’
 âˆ—                                                                                   
Ï . Thus, the ï¬nal solution is the desired Ï-SVM classiï¬er. set in practice. On the other hand, our t can be set adaptively.
                                                        There have been some other well-known connections be-
3  Ensemble of Partially Trained SVMs                 tween SVM  and boosting (e.g., [RÂ¨atsch, 2001]). However,
                                                      typically these are interested in relating the combined hypoth-
There is a close resemblance between the update rules of the esis with the SVM, and the fact that boosting uses the L1
Ï-SVM  with variants of the AdaBoost algorithm. In partic- norm while SVM uses the L2 norm. Here, on the other hand,
ular, we will show in Section 3.1 that the algorithm in Sec-
              âˆ—                                       our focus is on showing that by using the weighted Parzen
tion 2.2 (with Ï assumed to be known) is similar to the window classiï¬er as the base hypothesis, then the last hy-
AdaBoost algorithm (Algorithm 2); while that in Section 2.3
      âˆ—                                 âˆ—             pothesis is indeed a SVM.
(with Ï unknown) is similar to the AdaBoostÎ½ algorithm
[RÂ¨atsch and Warmuth, 2005]. Inspired by this connection, 3.2 Using the Whole Ensemble for Prediction
in Section 3.2, we consider using the output of the boosting As mentioned in Section 1, the hard-margin SVM (which is
ensemble for improved performance. The time complexity the same as the last hypothesis) may have poor performance
will be considered in Section 3.3.                    on noisy data. Inspired by its connection with boosting above,
                                                      we will use the boosting ensembleâ€™s output for prediction:
Algorithm 2 AdaBoost [RÂ¨atsch and Warmuth, 2005]
                                                                       T
                                                                              Î·t
 1: Input: S = {(xi,yi)}i=1,...,m; Number of iteration T .                         f x ,
                                                                           T       t( )
             (1)                                                                Î·r
 2: Initialize: di =1/m for all i =1,...,m.                            t=1   r=1
 3: for t =1,...,T do                                 which is a convex combination of all the partially trained
 4:  Train a weak classiï¬er w.r.t. the distribution di for each SVMs. Note that this takes little extra cost. As evidenced
     pattern xi to obtain a hypothesis ht âˆˆ [âˆ’1, 1].  in the boosting literature, the use of such an ensemble can
 5:  Set                                              lead to better performance, and this will also be experimen-
                ct =argminNt(c)exp(c),        (20)
                        câ‰¥0                           tally demonstrated in Section 4.
                   
                     m   (t)                          3.3  Computational Issue
     where Nt(c)=    i=1 di exp (âˆ’cyiht(xi)).
                                                                                            2
 6:  Update distributions di:                         In (8), each step of the EG update takes O(m ) time, which
          (t+1)      (t)                              can be excessive on large data sets. This can be alleviated by
         di     =   di  exp (âˆ’ctyiht(xi)) /Nt(ct). (21) performing low-rank approximations on the kernel matrix K
                                                      [Fine and Scheinberg, 2001],whichtakesO(mr2) time (r is
 7: end for
                  T      c                           the rank of K). Then, the EG update and computation of Ït
           f x          P  t  h  x .                                                                   Â¯
 8: Output: ( )=    t=1  T  c  t( )                                               2
                         r=1 r                        both take O(mr), instead of O(m ), time.
                                                      4   Experiments
3.1  Ï-SVM Training vs Boosting                       In this section, experiments are performed on ï¬ve small
                                                                                                    8
                             Î±(t)   Î·                 (breast cancer, diabetis, german, titanic and waveform) and
By comparing the update rules of i and t in Section 2.2                               P
            (t)                                          6                    h   Î³ =   m  d(t)y h (x )
with those of di and ct in AdaBoost, we can see that they The edge of the hypothesis t is t i=1 i i t i .
                                                         7            âˆ—
                                                         In the AdaBoostÎ½ algorithm,   in Algorithm 2 is replaced by
  5    1âˆ’t    âˆ—     âˆ—      Ëœ     âˆ—     2t
     ÏÂ¯t   <Ï       Ï  â‰¤ Î±  KÎ±t <Ï(1 +      )          t =minr=1,...,t Î³r âˆ’ Î½  Î³t           ht
   If  1+t    ,then       t             1âˆ’t which                      ,where  is the edge of .
satisï¬es the loose KKT condition for the Ï-SVM.          8http://ida.ï¬rst.fraunhofer.de/projects/bench/benchmarks.htm

                                                IJCAI-07
                                                  1092                                                                               2
seven medium-to-large real-world data sets (astro-particles, 1 m        1   m    
                                                      Î² =  m   i=1 xi âˆ’ m   j=1 xj . Each small data set in
adult1, adult2, adult3, adult, web1 and web2)9 (Table 1).
                                                      Table 1 comes with 100 realizations, and the performance is
                                                      obtained by averaging over all these realizations. However,
        Table1:Asummaryofthedatasetsused.             for each large data set, only one realization is available.
                                                        The ensemble is compared with the following:
     data set dim   # training patterns # test patterns
     cancer    9         200            77              1. SVM (BEST): The best-performing SVM among all the
     diabetis  8         468            300               C-SVMs obtained by the regularization path algorithm10
     german    20        700            300               [Hastie et al., 2005];
     titanic   3         150           2,051            2. SVM (LOO): The C-SVM which has the best leave-one-
    waveform   21        200           4,600              out-error bound [Evgeniou et al., 2004] among those ob-
      astro    4        3,089          4,000
                                                          tained by the regularization path algorithm;
     adult1   123       1,605         30,956
     adult2   123       2,265         30,296            3. the Parzen window classiï¬er, which is the ï¬rst hypothe-
     adult3   123       3,185         29,376              sis added to the ensemble;
      adult   123       32,561        16,281            4. the (hard-margin) Ï-SVM, which is the last hypothesis;
      web1    300       2,477         47,272
      web2    300       3,470         46,279            5. the proposed ensemble.
                                                      All these are implemented in Matlab11 and run on a 3.2GHz
                                                      Pentiumâ€“4 machine with 1GB RAM.
4.1  Adaptive Learning Rate
We ï¬rst demonstrate the advantages of the adaptive learning Table 2: Testing errors (in %) on the different data sets.
rate scheme (in Section 2.2). Because of the lack of space, we
                                                                  SVM     SVM    Parzen    Ï-   proposed
only report results on breast cancer and waveform. Similar
                                                        data set (BEST)  (LOO)   window  SVM    ensemble
behavior is observed on the other data sets. For comparison, cancer 23.1  26.1    27.4    33.5    28.2
we also train the Ï-SVM using the multiplicative updating
             Î·                                          diabetis  24.8    28.9    33.4    34.5    24.8
rule (with ï¬xed )in[Cristianini et al., 1999].          german    22.6    24.9    29.8    29.3    23.7
  As can be seen from Figure 1, the performance of [Cris- titanic 22.7    25.5    23.3    45.2    22.5
tianini et al., 1999] is sensitive to the ï¬xed choice of Î·.When waveform 11.2 18.8 22.0   11.2    10.0
Î· is small (e.g., Î· =0.01 or smaller), the objective value de- astro 5.8   9.3     7.0    22.1    3.4
creases gradually. However, when Î· is slightly larger (say, adult1 16.3   17.7    24.1    19.4    15.9
at 0.02), the estimate will ï¬nally over-shoot (as indicated by adult2 16.0 17.4   24.0    22.8    15.8
the vertical lines) and convergence can no longer be obtained. adult3 16.1 16.2   24.1    19.7    15.9
On the other hand, our proposed adaptive scheme always con- web1   2.3     2.8     3.0    14.4    2.8
verges much faster.                                      web2      2.1     2.2     3.0    3.4     2.1


    0                        0
  10                        10                          As can be seen from Table 2, the ensemble performs much
               Ï-SVM(adaptive)          Ï-SVM(adaptive) better than the (hard-margin) Ï-SVM. Indeed, its accuracy is
    -1         SVM(Î·=0.005)             SVM(Î·=0.005)
  10
               SVM(Î·=0.01)   -1         SVM(Î·=0.01)
                            10                        comparable to or sometimes even better that of SVM(BEST).
               SVM(Î·=0.02)              SVM(Î·=0.02)
    -2                                                                      12
  10           SVM(Î·=0.04)              SVM(Î·=0.04)   Table 3 compares the time for obtaining the ensemble with
                             -2
  objective                objective 10               that of running the regularization path algorithm. As can be
    -3
  10                                                  seen, obtaining the ensemble is much faster, and takes a small
    -4                       -3                       number of iterations. To illustrate the performance on large
  10 0      1     2      3  10 0     1     2      3
    10    10     10     10   10     10    10     10
            #iterations              #iterations      data sets, we experiment on the full adult data set in Table 1.
         (a) cancer              (b) waveform         The regularization path algorithm cannot be run on this large
Figure 1: Comparing the EG update (with ï¬xed Î·) with our data. So, we use instead a Î½-SVM (using LIBSVM13)for
adaptive scheme.                                      comparison, where Î½ (unlike the C parameter in the C-SVM)
                                                        10Alternatively, one can ï¬nd the best-tuned SVM by performing
                                                      a grid search on the soft-margin parameter. However, as each grid
4.2  Accuracy and Speed                               point then requires a complete re-training of the SVM, the regular-
Next, we show that the proposed ensemble of partially trained ization path algorithm is usually more efï¬cient [Hastie et al., 2005].
hard-margin SVMs (Section 3.2) has comparable perfor-   11The inner QP in the regularization path algorithm is solved by
mance as the best-tuned soft-margin SVM (C-SVM). We use using the optimization package Mosek (http://www.mosek.com).
                                          2             12
the Gaussian kernel k(xi, xj)=exp(âˆ’xi âˆ’ xj /Î²), with   This includes the time for computing the kernel matrix, which
                                                      can be expensive on large data sets. Moreover, time for computing
  9astro-particles    is      downloaded       from   the leave-one-out bound is not included in the timing of SVM(LOO).
http://www.csie.ntu.edu.tw/âˆ¼cjlin/libsvmtools/datasets/, while 13Version 2.8.1 (C++ implementation)  from
others are from http://research.microsoft.com/users/jplatt/smo.html. http://www.csie.ntu.edu.tw/ cjlin/libsvm/.

                                                IJCAI-07
                                                  1093