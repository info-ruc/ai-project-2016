                   Logistic Regression Models for a Fast CBIR Method
                                   Based on Feature Selection
                          R. Ksantini1,D.Ziou1,B.Colin2, and F. Dubeau2
                                        University of Sherbrooke
                                   (1) Computer Science Department
                             {riadh.ksantini, djemel.ziou}@usherbrooke.ca
                                      (2) Mathematic Department
                           {bernard.colin, francois.dubeau}@usherbrooke.ca

                    Abstract                          information about the relative relevances of the featurebase
                                                      feature vectors and because of the noise in these vectors, dis-
    Distance measures like the Euclidean distance have tance measures can fail and irrelevant features might hurt re-
    been the most widely used to measure similarities trieval performance. Probabilistic approaches are a promis-
    between feature vectors in the content-based image ing solution to this CBIR problem, that when compared to
    retrieval (CBIR) systems. However, in these sim-  the standard CBIR methods based on the distance measures,
    ilarity measures no assumption is made about the  can lead to a signiÔ¨Åcant gain in retrieval accuracy. In fact,
    probability distributions and the local relevances these approaches are capable of generating probabilistic sim-
    of the feature vectors. Therefore, irrelevant fea- ilarity measures and highly customized metrics for comput-
    tures might hurt retrieval performance. Probabilis- ing image similarity based on the consideration and distinc-
    tic approaches have proven to be an effective solu- tion of the relative feature vector relevances. As to previ-
    tion to this CBIR problem. In this paper, we use  ous works based on these probabilistic approaches, [Peng et
    a Bayesian logistic regression model, in order to al., 2004] used a binary classiÔ¨Åcation to classify the database
    compute the weights of a pseudo-metric to improve color image feature vectors as relevant or irrelevant, [Cae-
    its discriminatory capacity and then to increase im- nen and Pauwels, 2002] used the classical quadratic logistic
    age retrieval accuracy. The pseudo-metric weights regression model, in order to classify database image feature
    were adjusted by the classical logistic regression vectors as relevant or irrelevant, [Aksoy et al., 2000] used
    model in [Ksantini et al., 2006]. The Bayesian
                                                      weighted L1 and L2 distances, in order to measure the simi-
    logistic regression model was shown to be a sig-  larity degree between two images and [Aksoy and Haralick,
    niÔ¨Åcantly better tool than the classical logistic re- 2001] measure the similarity degree between a query image
    gression one to improve the retrieval performance. and a database image using a likelihood ratio derived from a
    The retrieval method is fast and is based on feature Bayesian classiÔ¨Åer.
    selection. Experimental results are reported on the In this paper, we investigate the effectiveness of a Bayesian
    Zubud and WANG color image databases proposed     logistic regression model based on a variational method, in
       [                  ]
    by  Deselaers et al., 2004 .                      order to adjust the weights of a pseudo-metric used in [Ksan-
                                                      tini et al., 2006], and then to improve its discriminatory ca-
1  Introduction                                       pacity and to increase image retrieval accuracy. This pseudo-
                                                      metric makes use of the compressed and quantized versions
The rapid expansion of the Internet and the wide use of               8
digital data in many real world applications in the Ô¨Åeld of of the Daubechies- wavelet decomposed feature vectors, and
medecine, security, communications, commerce and acad- its weights were adjusted by the classical logistic regression.
emia, increased the need for both efÔ¨Åcient image database We will show that thanks to the variational method, the used
creation and retrieval procedures. For this reason, content- Bayesian logistic regression model is a signiÔ¨Åcantly better
based image retrieval (CBIR) approach was proposed. In this tool than the classical logistic regression model to compute
approach, each image from the database is associated with a the pseudo-metric weights and to improve the querying re-
feature vector capturing certain visual features of the image sults. The retrieval method is fast, efÔ¨Åcient and based on
such as color, texture and shape. Then, a similarity measure feature selection. The evaluation of the retrieval method us-
is used to compare these feature vectors and to Ô¨Ånd similar- ing both models, separately, is performed using precision and
                                                                             [                  ]
ities between images with the assumption that images that scope curves as deÔ¨Åned in KherÔ¨Å and Ziou, 2006 .
                                                        In the next section, we brieÔ¨Çy deÔ¨Åne the pseudo-metric. In
are close to each other in the feature space are also visu-  3
ally similar. Distance measures like the Euclidean distance section , we brieÔ¨Çy describe the pseudo-metric weight com-
have been the most widely used for feature vector compar- putation using the classical logistic regression model, while
ison in the CBIR systems. However, these similarity mea- showing the limitations of this latter and that the Bayesian
sures are only based on the distances between feature vec- logistic regression model is more appropriate for the pseudo-
tors in the feature space. Therefore, because of the lack of metric weight computation. Then, we detail the Bayesian lo-

                                                IJCAI-07
                                                  2790                                                                                                 Àú
gistic regression model. Moreover, we will describe the data the two feature vectors are intended to be similar. X0,i is the
training performed for both models. The feature selection absolute value of the difference between the scaling factors
based image retrieval method and the feature vectors used to of the Daubechies-8 wavelets decomposed, compressed and
                                              4                                                      J‚àí1
represent the database images are presented in section .Fi- quantized versions of the two feature vectors and {Xk,i}k=0
nally, in section 5, we will perform some experiments to val- are the numbers of mismatches between the J resolution level
idate the Bayesian logistic regression model and we will use coefÔ¨Åcients of these latter. We suppose that we have n0 pairs
the precision and scope, in order to show the advantage of the of similar feature vectors and n1 pairs of dissimilar ones.
Bayesian logistic regression model over the classical logistic Thus, the class Œ©0 contains n0 explanatory vectors and their
                                                                                     r  r     n0
regression one, in terms of querying results.         associated binary target variables {Xi ,Si =0}i=1 to repre-
                                                      sent the pairs of the similar feature vectors, and the class Œ©1
2  The pseudo-metric                                  contains n1 explanatory vectors and their associated binary
                                                                    { ir  ir =1}n1
Given a query feature vector Q and a featurebase of |DB| target variables Xj ,Sj j=1 to represent the pairs of
                                       J              the dissimilar feature vectors. The pseudo-metric weights wÀú0
feature vectors Tk (k =1, ..., |DB|) having 2 components
                                                          {  }J‚àí1
each, our aim is to retrieve in the featurebase the most similar and wk k=0 and an intercept v are chosen to optimize the
feature vectors to Q. To achieve this, Q and the |DB| feature following conditional log-likelihood.
vectors are Daubechies-8 wavelets decomposed, compressed                      n0         n1
                                                                                      r           ir
to m coefÔ¨Åcients each and quantized. Then, to measure the L(Àúw0,w0, ..., wJ‚àí1,v)= log(pi )+  log(pj ), (4)
similarity degree between Q and a target feature vector Tk                    i=1         j=1
of the featurebase, we use the one-dimensional version of the
                                                             r     ir
pseudo-metric used in [Ksantini et al., 2006] and given by where pi and pj are the relevance and irrelevance probabili-
the following expression                              ties, respectively, and given by
                           
      =Àú|  Àú[0]‚àí Àú [0]|‚àí            ( Àúc[ ]= Àúc [ ])                            J‚àí1
 Q, Tk    w0 Q    Tk            wbin(i) Qq i  Tkq i ,          r             Àú r          r
                                                              pi  =   F (‚àíwÀú0X0,i ‚àí   wkXk,i ‚àí v),
                         i:QÀúc[i]=0
                           q                                                      k=0
                                                (1)
                                                                                 J‚àí1
where                                                         ir           Àú ir          ir
                                                          pj   =   F (Àúw0X0,j +  wkXk,j +  v),
                                Àúc     Àúc
        Àúc     Àúc         1  if Qq[i]=Tkq[i]                                     k=0
       Qq[i]=Tkq[i]  =                          (2)
                          0  otherwise,                              ex
                                                      where F (x)=  1+ex is the logistic function. For this rea-
Àú        Àú                                    Àúc
Q[0] and Tk[0] are the scaling factors of Q and Tk, Qq[i] son, standard optimization algorithms such as Fisher scoring
    Àúc                                                and gradient ascent algorithms [Clogg et al., 1991], can be
and Tkq[i] represent the i-th coefÔ¨Åcients of their Daubechies-
                                                      invoked. However, in several cases, especially because of
8 wavelets decomposed, compressed to m coefÔ¨Åcients and
                                                      the exponential in the likelihood function or because of the
quantized versions, wÀú0 and the wbin(i)‚Äôs are the weights to
                                  ()                  existence of many zero explanatory vectors, the maximum
compute, and the bucketing function bin groups these lat- likelihood can fail and estimates of the parameters of interest
ters according to the J resolution levels, such as    (weights and intercept) may not be optimal or may not ex-
                                        J
   bin(i)=log2(i)    with   i =1, ..., 2 ‚àí 1. (3)   ist or may be on the boundary of the parameter space. Also,
                                                      as there is complete or quasicomplete separation between Œ©0
                                                          Œ©
3  The weight computation                             and  1, the function L is made arbitrarily large and standard
                                                      optimization algorithms diverge [Krishnapuram et al., 2005].
In order to improve the discriminatory power of the pseudo- Moreover, as Œ©0 and Œ©1 are large and high-dimensional,
                                     J‚àí1
metric, we compute its weights wÀú0 and {wk}k=0 using a clas- these standard optimization algorithms have high computa-
sical logistic regression model and a Bayesian logistic regres- tional complexity and take long time to converge. The Ô¨Årst
sion model, separately. We deÔ¨Åne two classes, the relevance two problems can be solved by smoothing the parameter of
class denoted by Œ©0 and the irrelevance class denoted by Œ©1, interest estimates, assuming a certain prior distribution for
in order to classify the feature vector pairs as similar or dis- the parameters, thereby reducing the parameter space, and
similar. The basic principle of using the Bayesian logistic re- the third problem can be solved by using variational trans-
gression model and the classical logistic regression one is to formations which simplify the computation of the parameter
allow a good linear separation between Œ©0 and Œ©1,andthen of interest estimates [Jaakkola and Jordan, 2000]. This mo-
to compute the weights which represent the local relevances tivates the adoption of a Bayesian logistic regression model
of the pseudo-metric components.                      based on variational methods.
3.1  The classical logistic regression model          3.2  The Bayesian logistic regression model
In this model, each feature vector pair is represented by an ex- In the Bayesian logistic regression framework, there are three
planatory vector and a binary target variable. SpeciÔ¨Åcally, for main components which are a chosen prior distribution over
the i-th feature vector pair, we associate an explanatory vec- the parameters of interest, the likelihood function and the pos-
         Àú                       J
tor Xi =(X0,i,X0,i, ..., XJ‚àí1,i, 1) ‚àà R √ó{1} and a binary terior distribution. These three components are formally com-
target Si which is either 0 or 1, depending on whether or not bined by Bayes‚Äô rule. The posterior distribution contains all

                                                IJCAI-07
                                                  2791                                                                                          
                                                           ‚àù     |   =0    =1      1       1    (  )
the available knowledge about the parameters of interest in  P W  S0    , S1   , i i=0, qi i=0 œÄ W
the model. Among many priors having different distributional
                                                      where
forms, gaussian prior has the advantage of having low com-                         
putational intensity and of smoothing the parameter estimates | =0  =1      1       1   =
                                                      P W  S0    , S1  ,  i i=0, qi i=0
toward a Ô¨Åxed mean and away from unreasonable extremes.          
                                     
However, when the likelihood function is not conjugate of                                         
                                                      
           P    Eq [H ]‚àí P
                                                         1           1    i i  i ‚àí  1   œï( ) E [H2]‚àí2
the gaussian prior, the posterior distribution has no tractable     i=0    2       i=0   i   qi i  i
form and its mean computation involves high-dimensional    F (i) e                                     ,
integration which has high computational cost. According i=0

to [Jaakkola and Jordan, 2000], it‚Äôs possible to use accurate where Eq0 and Eq1 are the expectations with respect to the
                                                                                                   i
variational transformations in order to approximate the like-                                 tanh( 2 )
                                                                  0     1              ( i)=
                                                      distributions q and q , respectively, œï  4i   and
lihood function with a simpler tractable exponential form. In  1
this case, thanks to the conjugacy, with a gaussian prior dis- i i=0 are the variational parameters. Therefore, the ap-
tribution over the parameters of interest combined with the proximation of the posterior distribution is considered as an
likelihood approximation, we obtain a closed gaussian form adjustable lower bound and as a proper Gaussian distribu-
approximation to the posterior distribution. However, as the tion with a posterior mean Œºpost and covariance matrix Œ£post
number of observations is large, the number of variational which are estimated by the following Bayesian update equa-
parameters updated to optimize the posterior distribution ap- tions
proximation is also large, thereby the computational cost is
                                                                              1                
high. In the Bayesian logistic regression model that we pro- ‚àí1         ‚àí1                     t
                                                      (Œ£post)    =(Œ£)     +2      œï(i)Eqi [xi(xi) ] , (5)
pose, we use variational transformations and the Jensen‚Äôs in-                 i=0
equality in order to approximate the likelihood function with            
                           
                                                                                   1      1       
tractable exponential form. The explanatory vectors are not      =Œ£       (Œ£)‚àí1  +      ( ‚àí  )   [ ]
observed but instead are distributed according to two speciÔ¨Åc Œºpost   post      Œº       i   2 Eqi xi  . (6)
distributions. The posterior distribution is also approximated                     i=0
with a gaussian which depends only on two variational pa- The weight and intercept computation algorithm is in two
rameters. The computation of the posterior distribution ap- phases. The Ô¨Årst phase is the initialization of q0, q1 and
proximation mean is fast and has low computational com- the gaussian prior œÄ(W), and the second phase is iterative
plexity. In this model, we denote the random vectors whose and allows the computation of Œ£post and Œºpost through the
                                      { r}n0
realizations represent the explanatory vectors Xi i=1 of the Bayesian update equations (5) and (6), respectively, while
             Œ©                          {  ir}n1
relevance class 0 and the explanatory vectors Xj j=1 of using an EM type algorithm [Jaakkola and  Jordan, 2000],in
                 Œ©        =(Àú                    1)                                        1
the irrelevance class 1,byX0  X0,0, X0,0, ..., XJ‚àí1,0, order to Ô¨Ånd the variational parameters i i=0 at each iter-
          Àú
and X1 =(X0,1, X0,1, ..., XJ‚àí1,1, 1), respectively. We sup- ation to have an optimal approximation to the posterior dis-
                                                      tribution. In the initialization phase, q0 and q1 are chosen to
pose that X0 ‚àº q0(X0) and X1 ‚àº q1(X1),whereq0 and q1
                                                      model Œ©0 and Œ©1, respectively, and because of the absence of
are two chosen distributions. For X0 we associate a binary
                                                      prior knowledge about the weights and the intercept, œÄ(W) is
random variable S0 whose realizations are the target vari-
       r      n0                                      chosen univariate with zero mean and large variances [Con-
ables {Si =0}i=1,andforX1   we associate a binary ran-
                                                      gdon, 2001]. The values of Œºpost components are the desired
dom variable S1 whose realizations are the target variables                                       J‚àí1
  ir     n1                                                                            Àú     {   }
{Sj =1}j=1.WesetS0   equal to 0 for similarity and we set estimates of the pseudo-metric weights w0 and wk k=0 and
S1 equal to 1 for dissimilarity. Parameters of interest (weights the intercept v. Once the parameters of the posterior distrib-
                                                      ution approximation are computed, its magnitude is given by
and intercept) are considered as random variables and are de- 1
noted by the random vector W =(Àúw0,w0, ..., wJ‚àí1,v).We the term i=0 F (i). This latter becomes very close to 1 as
assume that W ‚àº œÄ(W),whereœÄ   is a gaussian prior with Œ©0 and Œ©1 are linearly separated or quasi separated and tends
prior mean Œº and covariance matrix Œ£. Using Bayes‚Äô rule, towards 0 as Œ©0 and Œ©1 become more and more overlapped.
the posterior distribution over W is given by         Analogically, in the classical logistic regression model, the
                                                            2L                                   1
                                                      term e  has almost the same characteristics as i=0 F (i)
P (W|S0 =0, S1 =1)=
ÀÜ                                            Àú        [Caenen and Pauwels, 2002]. These two terms will be used
 P           Q1
                  P (S = i|X = xi, W)qi(X = xi) œÄ(W)  to perform feature selection in the retrieval method.
   x0‚ààŒ©0,x1‚ààŒ©1 i=0   i     i           i          ,
                  P (S0 =0, S1 =1)
                                        t             3.3  Training
where P (Si = i|Xi = xi, W)=F ((2i ‚àí 1)W xi) for each
i ‚àà{0, 1}. Using a variational approximation [Jaakkola and Let us consider a color image database which consists of sev-
Jordan, 2000] and the Jensen‚Äôs inequality, the posterior distri- eral color image sets such that each set contains color images
bution is approximated as follows                     which are perceptually close to each other in terms of object
                                                      shapes and colors. In order to compute the pseudo-metric
P (W|S0 =0, S1 =1)                                    weights and the intercept by the classical logistic regression
                                                                                             Œ©
                          1    1                 model, we have to create the relevance class 0 and the ir-
       P W|S0 =0, S1 =1,  i    , qi    œÄ(W)          relevance class Œ©1. To create Œ©0, we draw all possible pairs
    ‚â•                        i=0     i=0     ,
                  P (S0 =0, S1 =1)                    of feature vectors representing color images belonging to the

                                                IJCAI-07
                                                  2792same database color image sets, and for each pair we com- 4. The similarity degrees between the query color image
pute an explanatory vector and we associate to this latter a and the database color images are represented by a re-
binary target variable equal to 0. Similarly, to create Œ©1,we sulted array TotalScore,suchas,TotalScore[i]=
                                                          N
draw all possible pairs of feature vectors representing color l=1 Œ≥lScorel[i] for each i ‚àà{1, ..., |DB|},where
images belonging to different database color image sets, and  N
                                                          {Œ≥l}l=1 are weightfactors used to down-weight the fea-
for each pair we compute an explanatory vector and we asso-                                           2L
                                                          ture which has low discriminatory power. Œ≥l = e l
ciate to this latter a binary target variable equal to 1.Forthe
                                         Œ©      Œ©         when the weights are computed by the classical logis-
Bayesian logistic regression model, we create the 0 and 1                           =   1    ( l)
with the same way, but instead of associating a binary target tic regression model, and Œ≥l i=0 F i when the
                                                          weights are computed by the Bayesian logistic regres-
variable value to each explanatory vector of Œ©0 and Œ©1,we
                                                          sion model.
associate a binary target variable S0 equal to 0 to all Œ©0 ex-
planatory vectors and we associate a binary target variable S1 5. Organize the database color images in order of increas-
equal to 1 to all Œ©1 explanatory vectors.                 ing resulted similarity degrees of the array TotalScore.
                                                          The most negative resulted similarity degrees corre-
4  Color image retrieval method                           spond to the closest target images to the query image.
                                                          Finally, return to the user the closest target color images
The querying method is in two phases. The Ô¨Årst phase is a
                                                          to the query color image and whose number is denoted
preprocessing phase done once for the entire database con-
                                                          by RI and chosen by the user.
taining |DB| color images. The second phase is the querying
phase.                                                4.3  Used feature vectors
4.1  Color image database preprocessing               In order to describe the luminance, colors and the edges of a
We detail the preprocessing phase done once for all the data- color image, we use luminance histogram and weighted his-
                                                      tograms. The image texture description is performed by kur-
base color images before the querying in a general case by                                 √ó
the following steps.                                  tosis and skewness histograms. Given an M N pixel LAB
                                                      color image, its luminance histogram hL contains the number
 1. Choose N feature vectors for comparison.          of pixels of the luminance L, and can be written as follows
                                    ( ‚àà{1        })
 2. Compute the N  feature vectors Tli l   , ..., N                      M‚àí1 N‚àí1
    for each i-th color image of the database, where i ‚àà           ( )=           (  (  ) ‚àí  )
    {1    |   |}                                                 hL c            Œ¥ IL i, j  c ,       (7)
      , ..., DB .                                                        i=0 j=0
 3. The feature vectors representing the database color
    images are Daubechies-8 wavelets decomposed, com- for each c ‚àà{0, ..., 255},whereIL is the luminance image
                                                                                  0
    pressed to m coefÔ¨Åcients each and quantized.      and Œ¥ is the Kronecker symbol at . The weighted histograms
                                                      are the color histogram constructed after edge region elimina-
 4. Organize the decomposed, compressed and quantized
                                  Œòl      Œòl     =    tion and the multispectral gradient module mean histogram.
    feature  vectors into search arrays + and ‚àí l     The former is given by
    1, ..., N which are used to optimize the pseud-metric
                                                               M‚àí1  N‚àí1                           
    computation process [Ksantini et al., 2006].                   
                                                         h( )=          (  (   ) ‚àí )          (   )
                                  l       l J‚àí1         hk c           Œ¥ Ik i, j  c œá[0,Œ∑] Œªmax i, j , (8)
 5. Adjustment of the metric weights wÀú0 and {wk}k=0 for        i=0 j=0
    each featurebase Tli (i =1, ..., |DB|) representing the
    database color images, where l ‚àà{1, ..., N}.      and the latter is given by
                                                                                  e ( )
4.2  The querying algorithm                                             ¬Øe ( )=  hk c
                                                                        hk c        ( ),              (9)
We detail the querying algorithm in a general case by the fol-                  Np,k c
lowing steps.                                         where Np,k(c) is the number of the edge region pixels and is
 1. Given a query color image, we denote the feature vectors  deÔ¨Åned as
    representing the query image by Ql l =1, ..., N .           M‚àí1 N‚àí1                            
 2. The feature vectors representing the query image are Np,k(c)=        Œ¥(Ik(i, j) ‚àí c)œá]Œ∑,+‚àû[ Œªmax(i, j) ,
    Daubechies-8 wavelets decomposed, compressed to m            i=0 j=0
                                                                                                     (10)
    coefÔ¨Åcients each and quantized.              
                                                      and
 3. The similarity degrees between Ql l =1, ..., N
                                                 =     e ( )=
    and the  database color image feature vectors Tli l hk c
    1       ( =1     |   |)
     , ..., N i , ..., DB  are represented by the arrays M‚àí1 N‚àí1                                  
             =1                      [ ]=       
    Scorel l    , ..., N such that Scorel i Ql,Tli             Œ¥(Ik(i, j) ‚àí c)Œªmax(i, j) œá]Œ∑,+‚àû[ Œªmax(i, j) ,
    for each i ‚àà{1, ..., |DB|}. These arrays are returned by  i=0 j=0
                                l   l
    the procedure Retrieval(Ql, m, Œò+, Œò‚àí) l =1, ..., N ,                                            (11)
    respectively. The procedure Retrieval is used to optimize for each c ‚àà{0, ..., 255} and k = a, b,whereŒªmax represents
    the querying process [Ksantini et al., 2006].     the multispectral gradient module [Ksantini et al., 2006], Œ∑ is

                                                IJCAI-07
                                                  2793a threshold deÔ¨Åned by the mean of the multispectral gradient them follows a truncated poisson distribution at its greatest
modules computed over all image pixels, Ia and Ib are the realization, to have a best Ô¨Åt. Analogically, we make the
images of the chrominances a red/green and b yellow/blue, same choice for (X0,1, ..., XJ‚àí1,1). Also, we assume that the
                                                                     Àú
respectively, and œá is the characteristic function. The multi- random variable X0,0 whose realizations are positive reals,
spectral gradient module mean histogram provides informa- follows a gaussian mixture distribution, which is the same
tion about the overall contrast in the chrominance and the     Àú
                                                      choice for X0,1. Generally, to carry out an evaluation in the
edge region elimination allows the avoidance of overlappings image retrieval Ô¨Åeld, two principal issues are required: the
or noises between the color histogram populations caused by acquisition of ground truth and the deÔ¨Ånition of performance
the edge pixels. The LAB color image kurtosis and skewness criteria. For ground truth, we use human observations. In
histograms are given by                               fact, three external persons participate in the below evalu-
                M‚àí1 N‚àí1                             ation. Concerning performance criteria, we represent the
          Œ∫                Œ∫
         hk (c)=        Œ¥(Ik (i, j) ‚àí c),      (12)   evaluation results by the precision-scope curve Pr = f(RI),
                 i=0 j=0                              where the scope RI is the number of images returned to the
and                                                   user. In each querying performed in the evaluation experi-
                M‚àí1 N‚àí1                             ment, each human subject is asked to give a goodness score to
          s ( )=         ( s(   ) ‚àí )                 each retrieved image. The goodness score is 2 if the retrieved
         hk c           Œ¥ Ik i, j  c ,         (13)                                  1
                 i=0 j=0                              image is almost similar to the query, if the retrieved image
                                                      is fairly similar to the query and 0 if there is no similarity
respectively, for each c ‚àà{0, ..., 255} and k = L, a, b,where
 Œ∫  Œ∫      Œ∫                                          between the retrieved image and the query. The precision
IL, Ia and Ib are the kurtosis images of the luminance L                       =
                                          s   s       is computed as follows: Pr the sum of goodness scores
and the chrominances a and b, respectively, and IL, Ia and                                       =  (   )
 s                                                    for retrieved images/RI. Therefore, the curve Pr f RI
Ib are the skewness images of these latter. They are obtained gives the precision for different values of RI which lie
by local computations of the kurtosis and skewness values at between 1 and 20 when we perform the querying evaluation
the luminance and chrominance image pixels. Then, a linear on the WANG database, and lie between 1 and 5 when we
interpolation is used to represent the kurtosis and skewness perform the querying evaluation on the ZuBuD database.
values between 0 and 255. Since each used feature vector is a When the human subjects perform different queryings in the
histogram having 256 components, we set J equal to 8 in the evaluation experiment, we compute an average precision for
following section.                                    each value of RI, and then we construct the precision-scope
                                                      curve. In our evaluation experiment, each color image of
5  Experimental results                               the WANG and Zubud databases is represented by N =11
                                                                              h   h ¬Øe  ¬Øe  Œ∫   Œ∫  Œ∫   s
                                                      histograms which are hL, ha, hb , ha, hb, hL, ha, hb , hL,
In this section, we will discuss the choices of the distributions s s
q0 and q1, in order to validate the Bayesian logistic regression ha and hb. In order to evaluate the querying in the WANG
model in the image retrieval context. Finally, we will use database, each human subject is asked to formulate a query
the precision and scope as deÔ¨Åned in [KherÔ¨Å and Ziou, from the database and to execute a querying, using weights
2006], to evaluate the querying method using both models computed by the classical logistic regression model, and
separately. The choices of the distributions q0 and q1 and the to give a goodness score to each retrieved image, then to
querying evaluation will be conducted on the WANG and reformulate a query from the database and to execute the
Zubud color image databases proposed by [Deselaers et al., querying, using weights computed by the Bayesian logistic
2004]. The WANG database contains |DB| = 1000 color   regression model, and to give a goodness score to each
images which were selected manually to form 10 sets (e.g. retrieved image. Each human subject performs the querying
Africa, beach, ruins, food) of 100 images each. The Zurich Ô¨Åfty times by choosing a new query from the database
Building Image Database (ZuBuD) contains a training part each time. We repeat this experience for different orders of
of |DB| = 1005 color images and query part of 115 color compression m ‚àà{30, 20, 10}. To evaluate the querying in
images. The training part consists of 201 building image sets, the ZuBuD database, each human subject is asked to follow
where each set contains 5 color images of the same building the preceding steps, while formulating the queries from the
taken from different positions. Before the feature vector database query part. For the WANG and Zubud databases,
extractions, we represent the WANG and Zubud database the resulted precision-scope curves are given in Figure 1
color images in the perceptually uniform LAB color space. for compression orders m ‚àà{30, 20, 10}. The Figure 2
Since from each color image of the Zubud and WANG     illustrates two retrieval examples in the Zubud database
databases we extract N =11histograms which are given  comparing the performances of the regression models for
by (7), (8), (9), (12) and (13) respectively, each database is m =30. In each example the query is located at the top-left
represented by eleven featurebases. The choices of q0 and q1 of the dialog box.
will be separately performed for each featurebase. For each
                        Àú
featurebase, we assume that X0,0 and (X0,0, ..., XJ‚àí1,0) are
                                           Àú
independent. We make the same assumption for X0,1 and
(X0,1, ..., XJ‚àí1,1). Moreover, we suppose that the random
vector (X0,0, ..., XJ‚àí1,0) random variables whose realiza-
tions are positive integers, are independent and each one of

                                                IJCAI-07
                                                  2794