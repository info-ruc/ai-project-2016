                        Learning    Payoff   Functions     in InÔ¨Ånite   Games
                  Yevgeniy  Vorobeychik,   Michael  P. Wellman,   and  Satinder  Singh
                                         University of Michigan
                                    ArtiÔ¨Åcial Intelligence Laboratory
                                           1101  Beal Avenue
                                    Ann Arbor,  MI 48109-2110   USA
                               yvorobey,  wellman,  baveja    @umich.edu
                             {                              }

                    Abstract                          identify the full game (or at least a less restrictive game) from
                                                      limited data, entailing some generalization from observed in-
    We  consider a class of games with real-valued    stances. Approximating payoff functions using supervised
    strategies and payoff information available only in learning (regression) methods allows us to deal with contin-
    the form of data from a given sample of strategy  uous agent strategy sets, providing a payoff for an arbitrary
    proÔ¨Åles. Solving such games with respect to the un- strategy proÔ¨Åle. In so doing, we adopt functional forms con-
    derlying strategy space requires generalizing from sistent with prior knowledge about the game, and also ad-
    the data to a complete payoff-function representa- mit biases toward forms facilitating subsequent game analysis
    tion. We address payoff-function learning as a stan- (e.g., equilibrium calculation).
    dard regression problem, with provision for captur- In this paper, we present our Ô¨Årst investigation of approxi-
    ing known structure (symmetry) in the multiagent  mating payoff functions, employing regression to low-degree
    environment. To measure learning performance,     polynomials. We explore two example games, both with in-
    we consider the relative utility of prescribed strate- complete information and real-valued actions. First is the
    gies, rather than the accuracy of payoff functions standard Ô¨Årst-price sealed bid auction, with two players and
    per se. We demonstrate our approach and evalu-    symmetric value distributions. The solution to this game is
    ate its effectiveness on two examples: a two-player well-known [Krishna, 2002], and its availability in analytical
    version of the Ô¨Årst-price sealed-bid auction (with form proves useful for benchmarking our learning approach.
    known analytical form), and a Ô¨Åve-player market-  Our second example is a Ô¨Åve-player market-based schedul-
    based scheduling game (with no known solution).   ing game [Reeves et al., 2005], where time slots are allocated
                                                      by simultaneous ascending auctions [Milgrom, 2000]. This
1  Introduction                                       game has no known solution, though previous work has iden-
Game-theoretic analysis typically begins with a complete de- tiÔ¨Åed equilibria on discretized subsets of the strategy space.
scription of strategic interactions, that is, the game. We con-
sider the prior question of determining what the game actually 2 Preliminaries
is, given a database of game experience rather than any direct
speciÔ¨Åcation. This is one possible target of learning applied 2.1 Notation
to games [Shoham et al., 2003]. When agents have few avail- A generic normal form game is formally expressed as
able actions and outcomes are deterministic, the game can [I, ‚àÜ(Si) , ui(s) ], where I refers to the set of players and
                                                         {     } {    }
be identiÔ¨Åed through systematic exploration. For instance, m = I is the number of players. Si is the set of strategies
                                                           | |
we can ask the agents to play each strategy proÔ¨Åle in the en- available to player i I, and the set ‚àÜ(Si) is the simplex of
                                                                       ‚àà
tire joint strategy set and record the payoffs for each. If the mixed strategies over Si. Finally, ui(s) : S1 Sm
                                                                                           √ó¬∑ ¬∑ ¬∑√ó  ‚Üí  R
joint action space is small enough, limited nondeterminism is the payoff function of player i when all players jointly
can be handled by sampling. Coordinating exploration of the play s = (s1, . . . , sm), with each sj Sj. As is com-
                                                                                        ‚àà
joint set does pose difÔ¨Åcult issues. Brafman and Tennenholtz, mon, we assume von Neumann-Morgenstern utility, allow-
for example, address these carefully for the case of common- ing an agent i‚Äôs payoff for a particular mixed strategy pro-
interest stochastic games [Brafman and Tennenholtz, 2003], Ô¨Åle to be ui(œÉ) = s S[œÉ1(s1) œÉm(sm)]ui(s), where
                                                                           ‚àà       ¬∑ ¬∑ ¬∑
as well as the general problem of maintaining an equilib- œÉj : Sj [0, 1] is a mixed strategy of player i, assign-
                                                              ‚Üí         P
rium among learning algorithms [Brafman and Tennenholtz, ing a probability to each pure strategy sj Sj such that
2004].                                                all probabilities over the agent‚Äôs strategy set‚ààadd to 1 (i.e.,
  Further difÔ¨Åculties are posed by intractably large (or inÔ¨Å- œÉj ‚àÜ(Sj )).
nite) strategy sets. We can make this problem tractable by It‚ààwill often be convenient to refer to the strategy (pure
reducing the number of proÔ¨Åles that agents are allowed to or mixed) of player i separately from that of the remaining
play, but this comes at the cost of transforming the game of players. To accommodate this, we use s i to denote the joint
interest into a different game entirely. Instead, we seek to strategy of all players other than player i‚àí.2.2  Nash Equilibrium                                 approximation quality not directly in terms of a distance be-
In this paper, we are concerned with one-shot normal-form tween uÀÜ and u, but rather in terms of the strategies dictated
games, in which players make decisions simultaneously and by uÀÜ evaluated with respect to u. For this we appeal to the
accrue payoffs, upon which the game ends. This single-shot notion of approximate Nash equilibrium.
nature may seem to preclude learning from experience, but DeÔ¨Ånition 4 A strategy proÔ¨Åle œÉ = (œÉ1, . . . , œÉm) constitutes
in fact repeated episodes are allowed, as long as actions can- an -Nash equilibrium of game [I, ‚àÜ(S ) , u (s) ] if for
                                                                                    {    i } { i  }
not affect future opportunities, or condition future strategies. every i I, œÉi0 ‚àÜ(Si), ui(œÉi, œÉ i) +  ui(œÉi0, œÉ i).
Game payoff data may also be obtained from observations of  ‚àà      ‚àà              ‚àí      ‚â•       ‚àí
                                                        We  propose using  in the above deÔ¨Ånition as a mea-
other agents playing the game, or from simulations of hypo-
                                                      sure of approximation error of uÀÜ, and employ it in evaluat-
thetical runs of the game. In any of these cases, learning is
                                                      ing our learning methods. When u is known, we can com-
relevant despite the fact that the game is to be played only
                                                      pute  in a straightforward manner. Let s denote i‚Äôs best-
once.                                                                                     i‚àó
                                                      response correspondence, deÔ¨Åned by si‚àó(œÉ i) = x : x
  Faced with a one-shot game, an agent would ideally play                                 ‚àí     {      ‚àà
                                                      arg maxs ui(si, œÉ i) . For clarity of exposition, we take
its best strategy given those played by the other agents. A   i       ‚àí }
                                                      si‚àó(œÉ i) to be single-valued. Let œÉÀÜ be a solution (e.g., a Nash
conÔ¨Åguration where all agents play strategies that are best re- ‚àí
                                                      equilibrium) of game [I, ‚àÜ(S ) , uÀÜ (s) ]. Then œÉÀÜ is an -
sponses to the others constitutes a Nash equilibrium.                           i     i
                                                      Nash equilibrium of the true{ game} [{I, ‚àÜ(}S ) , u (s) ], for
                                                                                      {    i } { i  }
DeÔ¨Ånition 1 A strategy proÔ¨Åle s = (s1, . . . , sm) constitutes  = maxi I [ui(si‚àó(œÉÀÜ i), œÉÀÜ i) ui(œÉÀÜi, œÉÀÜ i)] .
                                                              ‚àà         ‚àí    ‚àí  ‚àí        ‚àí
a (pure-strategy) Nash equilibrium of game [I, Si , ui(s) ] Since in general u will either be unknown or not amenable
                                      {  } {     }
if for every i I, si0 Si, ui(si, s i) ui(si0 , s i).  to this analysis, we developed a method for estimating  from
           ‚àà     ‚àà          ‚àí   ‚â•       ‚àí
A similar deÔ¨Ånition applies when mixed strategies are al- data. We will describe it in some detail below.
lowed.                                                  For the remainder of this report, we focus on a special case
                                                      of the general problem, where action sets are real-valued in-
DeÔ¨Ånition 2 A strategy proÔ¨Åle œÉ = (œÉ1, . . . , œÉm) con- tervals, Si = [0, 1]. Moreover, we restrict attention to sym-
stitutes a mixed-strategy Nash equilibrium of game    metric games and further limit the number of variables in
[I, ‚àÜ(Si) , ui(s) ] if for every i I, œÉi0    ‚àÜ(Si),   payoff-function hypotheses by using some form of aggrega-
  {      } {    }               ‚àà         ‚àà                                   1
ui(œÉi, œÉ i) ui(œÉi0, œÉ i).                             tion of other agents‚Äô actions. The assumption of symmetry
      ‚àí   ‚â•        ‚àí
  In this study we devote particular attention to games that allows us to adopt the convention for the remainder of the
                                                      paper that payoff u(si, s i) is to the agent playing si.
exhibit symmetry with respect to payoffs.                                 ‚àí
DeÔ¨Ånition 3 A game [I, ‚àÜ(Si) , ui(s) ] is symmetric if
                     {     } {     }                  3.2  Polynomial  Regression
 i, j  I, (a) Si = Sj and (b) ui(si, s i) = uj (sj , s j )
‚àÄ    ‚àà                            ‚àí            ‚àí      One class of models we consider are the nth-degree separable
whenever si = sj and s i = s j
                    ‚àí    ‚àí                            polynomials:
Symmetric games have relatively compact descriptions and
                                                                          n
                                                         u(si, œÜ(s i)) = ansi +  + a1si+
may present associated computational advantages. Given a         ‚àí
                                                                          n   ¬∑ ¬∑ ¬∑                   (1)
symmetric game, we may focus on the subclass of symmet-              + bnœÜ (s i) +   + b1œÜ(s  i) + d,
ric equilibria, which are arguably most natural [Kreps, 1990],               ‚àí    ¬∑ ¬∑ ¬∑     ‚àí
                                                      where œÜ(s i) represents some aggregation of the strategies
and avoid the need to coordinate on roles. In fairly general   ‚àí
settings, symmetric games do possess symmetric equilibria played by agents other than i. For two-player games, œÜ is
[Nash, 1951].                                         simply the identity function. We refer to polynomials of the
                                                      form (1) as separable, since they lack terms combining si and
                                                      s i. We also consider models with such terms, for example,
3  Payoff  Function  Approximation                    the‚àí non-separable quadratic:
3.1  Problem  DeÔ¨Ånition                                                       2            2
                                                             u(si, œÜ(s i)) = a2si + a1si + b2œÜ (s i)+
We are given a set of data points (s, v), each describing an         ‚àí                       ‚àí        (2)
                                                                         + b1œÜ(s i) + csiœÜ(s i) + d.
instance where agents played strategy proÔ¨Åle s and realized                     ‚àí         ‚àí
value v = (v1, . . . , vm). For deterministic games of complete Note that (2) and (1) coincide in the case n = 2 and c =
information, v is simply u. With incomplete information or 0. In the experiments described below, we employ a simpler
stochastic outcomes, v is a random variable, more speciÔ¨Åcally
                                                      version of non-separable quadratic that takes b1 = b2 = 0.
an independent draw from a distribution function of s, with One advantage of the quadratic form is that we can ana-
expected value u(s).                                  lytically solve for Nash equilibrium. Given a general non-
  The payoff function approximation task is to select a func- separable quadratic (2), the necessary Ô¨Årst-order condition
tion uÀÜ from a candidate set minimizing some measure of
                                                      for an interior solution is si = (a1 + cœÜ(s i))/2a2. This
deviation from the true payofUf function u. Because the true                     ‚àí          ‚àí
                                                      reduces to si =  a1/2a2 in the separable case. For the
function u is unknown, of course, we must base our selection          ‚àí
                                                      non-separable case with additive aggregation, œÜsum(s i) =
on evidence provided by the given data points.                                                      ‚àí
  Our goal in approximating payoff functions is typically not 1Although none of these restrictions are inherent in the approach,
predicting payoffs themselves, but rather in assessing strate- one must of course recognize the tradeoffs in complexity of the hy-
gic behavior. Therefore, for assessing our results, we measure pothesis space and generalization performance.  j=i sj , we can derive an explicit Ô¨Årst-order condition for 3.6 Strategy Aggregation
   6
symmetric equilibrium: si = a1/(2a2 + (m 1)c).        As  noted above, we consider payoff functions on two-
P                       ‚àí             ‚àí
  While a pure-strategy equilibrium will necessarily exist for dimensional strategy proÔ¨Åles in the form u(si, s i) =
                                                                                                  ‚àí
any separable polynomial model, it is only guaranteed to ex- f(si, œÜ(s i)). As long as œÜ(s i) is invariant under different
                                                              ‚àí                ‚àí
ist in the non-separable case when the learned quadratic is permutations of the same strategies in s i, the payoff func-
concave. In the experiments that follow, when the learned tion is symmetric. Since the actual payof‚àíf functions for our
non-separable quadratic does not have a pure Nash equilib- example games are also known to be symmetric, we constrain
rium, we generate an arbitrary symmetric pure proÔ¨Åle as the that œÜ(s i) preserve the symmetry of the underlying game.
                                                             ‚àí
approximate Nash equilibrium.                           In our experiments, we compared three variants of œÜ(s i).
                                                                                                     ‚àí
  Another difÔ¨Åculty arises when a polynomial of a degree First and most compact is the simple sum, œÜsum(s i). Sec-
                                                                                                 ‚àí
higher than three has more than one Nash equilibrium. In ond is the ordered pair (œÜsum, œÜss), where œÜss(s i) =
                                                              2                                    ‚àí
such a case we select an equilibrium arbitrarily.          (sj ) . The third variant, œÜidentity (s i) = s i, sim-
                                                        j=i                               ‚àí       ‚àí
                                                      ply 6 takes the strategies in their direct, unaggregated form. To
3.3  Local Regression                                 enforceP the symmetry requirement in this last case, we sort
In addition to polynomial models, we explored learning using the strategies in s i.
                                                                     ‚àí
two local regression methods: locally weighted average and
locally weighted quadratic regression [Atkeson et al., 1997]. 4 First-Price Sealed-Bid Auction
Unlike model-based methods such as polynomial regression,
                                                      In the standard Ô¨Årst-price sealed-bid (FPSB) auction game
local methods do not attempt to infer model coefÔ¨Åcients from
                                                      [Krishna, 2002], agents have private valuations for the good
data. Instead, these methods weigh the training data points
                                                      for sale, and simultaneously choose a bid price representing
by distance from the query point and estimate the answer‚Äîin
                                                      their offer to purchase the good. The bidder naming the high-
our case, the payoff at the strategy proÔ¨Åle point‚Äîusing some
                                                      est price gets the good and pays the offered price. Other
function of the weighted data set. We used a Gaussian weight
               d2                                     agents receive and pay nothing. In the classic setup Ô¨Årst ana-
function: w = e‚àí , where d is the distance of the training lyzed by Vickrey [1961], agents have identical valuation dis-
data point from the query point and w is the weight that is tributions, uniform on [0, 1], and these distributions are com-
assigned to that training point.                      mon knowledge. The unique (Bayesian) Nash equilibrium of
  In the case of locally weighted average, we simply take the                   m  1
                                                      this game is for agent i to bid m‚àí xi, where xi is i‚Äôs valua-
weighted average of the payoffs of the training data points as tion for the good.
our payoff at an arbitrary strategy proÔ¨Åle. Locally weighted Note that strategies in this game (and generally for games
quadratic regression, on the other hand, Ô¨Åts a quadratic re- of incomplete information), bi : [0, 1] [0, 1], are func-
gression to the weighted data set for each query point. tions of the agent‚Äôs private information.‚ÜíWe consider a re-
                                                      stricted case, where bid functions are constrained to the form
3.4  Support Vector Machine  Regression               bi(xi) = kixi, ki   [0, 1]. This constraint transforms the
The third category of learning methods we used was Support action space to a real‚àà interval, corresponding to choice of
Vector Machines (SVMs). For details regarding this learn- parameter ki. We can easily see that the restricted strategy
ing method, we refer an interested reader to [Vapnik, 1995]. space includes the known equilibrium of the full game, with
                                                                m 1
In our experiments, we used SVM light package [Joachims, si = ki = m‚àí for all i, which is also an equilibrium of the
1999], which is an open-source implementation of SVM clas- restricted game in which agents are constrained to strategies
siÔ¨Åcation and regression algorithms.                  of the given form.
                                                        We  further focus on the special case m = 2, with corre-
3.5  Finding Mixed  Strategy Equilibria               sponding equilibrium at s1 = s2 = 1/2. For the two-player
In the case of polynomial regression, we were able to Ô¨Ånd ei- FPSB, we can also derive a closed-form description of the
ther analytic or simple and robust numeric methods for com- actual expected payoff function:
puting pure Nash equilibria. With local regression and SVM
                                                                     0.25              if s1 = s2 = 0,
learning we are not so fortunate, as we do not have access           (s 1) (s )2 3(s )2
                                                                      1‚àí  [ 2 ‚àí   1 ]
                                                        u(s1, s2) =           2        if s1 s2,      (3)
to a closed-form description of the function we are learning.     Ô£±        6(s1)
                                                                     s (1 s )              ‚â•
Furthermore, we are often interested in mixed strategy ap-        Ô£≤Ô£¥  1 ‚àí 1            otherwise.
proximate equilibria, and our polynomial models and solution           3s2
methods yield pure strategy equilibria.                 The availabilityÔ£≥Ô£¥ of known solutions for this example fa-
  When  a particular learned model is not amenable to a cilitates analysis of our learning approach. Our results are
closed-form solution, we can approximate the learned game summarized in Figure 1. For each of our methods (classes of
with a Ô¨Ånite strategy grid and Ô¨Ånd a mixed-strategy equi- functional forms), we measured average  for varying train-
librium of the resulting Ô¨Ånite game using a general-purpose ing set sizes. For instance, to evaluate the performance of
Ô¨Ånite-game solver. We employed replicator dynamics [Fu- separable quadratic approximation with training size N, we
denberg and Levine, 1998], which searches for a symmetric independently draw N strategies, s1, . . . , sN , uniformly
mixed equilibrium using an iterative evolutionary algorithm. on [0, 1]. The corresponding training{ set comprises} O(N 2)
We treat the result after a Ô¨Åxed number of iterations as an points: ((si, sj), u(si, sj )), for i, j 1, . . . , N , with u as
approximate Nash equilibrium of the learned game.     given by (3). We Ô¨Ånd the best separable‚àà { quadratic} Ô¨Åt uÀÜ tothese points, and Ô¨Ånd a Nash equilibrium corresponding to uÀÜ. indicated by their inferior learning performance displayed in
We then calculate the least  for which this strategy proÔ¨Åle is this game.
an -Nash equilibrium with respect to the actual payoff func- The results of this game provide an optimistic view of how
tion u. We repeat this process 200 times, averaging the results well regression might be expected to perform compared to
over strategy draws, to obtain each value plotted in Figure 1. discretization. This game is quite easy for learning since
                                                      the underlying payoff function is well captured by our lower-
      0.03                                            degree model. Moreover, our experimental setup eliminated
                            Sample best               the issue of noisy payoff observations, by employing the ac-
                            Separable quadratic
     0.025                  Non‚àíseparable quadratic   tual expected payoffs for selected strategies.
                            3rd degree poly
                            4th degree poly
      0.02                                            5   Market-Based    Scheduling  Game
    Œµ
                                                      The second game we investigate presents a signiÔ¨Åcantly more
     0.015                                            difÔ¨Åcult learning challenge. It is a Ô¨Åve-player symmetric

    Average                                           game, with no analytic characterization, and no (theoreti-
      0.01                                            cally) known solution. The game hinges on incomplete infor-
                                                      mation, and training data is available only from a simulator
     0.005                                            that samples from the underlying distribution.
                                                        The game is based on a market-based scheduling scenario
        0                                             [Reeves et al., 2005], where agents bid in simultaneous auc-
        5            10          15           20
                Number of strategies in training set  tions for time-indexed resources necessary to perform their
                                                      given jobs. Agents have private information about their job
Figure 1: Epsilon versus number of training strategy points lengths, and values for completing their jobs by various dead-
for different functional forms.                       lines. Note that the full space of strategies is quite complex: it
                                                      is dependent on multi-dimensional private information about
                                                      preferences as well as price histories for all the time slots.
                                                      As in the FPSB example, we transform this policy space to
      0.2
                                 Actual function      the real interval by constraining strategies to a parametrized
                                 Learned quadratic    form. In particular, we start from a simple myopic policy‚Äî
                                                      straightforward bidding [Milgrom, 2000], and modify it by
      0.15                                            a scalar parameter (called ‚Äúsunk awareness‚Äù, and denoted by
                                                      k) that controls the agent‚Äôs tendency to stick with slots that
    ,0.5)


     1                                                it is currently winning. Although the details and motivation
      0.1                                             for sunk awareness are inessential to the current study, we
                                                      note that k [0, 1], and that the optimal setting of k involves

    payoffs(s                                         tradeoffs, generally‚àà dependent on other agents‚Äô behavior.
      0.05                                              To investigate learning for this game, we collected data
                                                      for all strategy proÔ¨Åles over the discrete set of values k
                                                       0, 0.05, . . . , 1 . Accounting for symmetry, this represents‚àà
                                                      53,130{ distinct}strategy proÔ¨Åles. For evaluation purposes, we
        0
        0      0.2     0.4     0.6    0.8     1       treat the sample averages for each discrete proÔ¨Åle as the true
                           s
                            1                         expected payoffs on this grid.
                                                        The  previous empirical study of this game by Reeves
Figure 2: Learned and actual payoff function when the et al. [2005] estimated the payoff function over a dis-
other agent plays 0.5. The learned function is the separable crete grid of proÔ¨Åles assembled from the strategies
quadratic, for a particular sample with N = 5.         0.8, 0.85, 0.9, 0.95, 1 , computing an approximate Nash
                                                      {equilibrium using replicator} dynamics. We therefore
  As we can see, both second-degree polynomial forms we generated a training set based on the data for these
tried do quite well on this game. For N < 20, quadratic strategies (300000 samples per proÔ¨Åle), regressed to the
regression outperforms the model labeled ‚Äúsample best‚Äù, in quadratic forms, and calculated empirical  values with
which the payoff function is approximated by the discrete respect to the entire data set by computing the maxi-
training set directly. The derived equilibrium in this model mum beneÔ¨Åt from deviation within the data: emp =

is simply a Nash equilibrium over the discrete strategies in maxi I maxsi Si [ui(si, sÀÜ i) ui(sÀÜ)] , where Si is the
the training set. At Ô¨Årst, the success of the quadratic model strate‚ààgy set of ‚ààplayer i represented‚àí ‚àí within the data set. Since
may be surprising, since the actual payoff function (3) is the game is symmetric, the maximum over the players can be
only piecewise differentiable and has a point of discontinu- dropped, and all the agent strategy sets are identical.
ity. However, as we can see from Figure 2, it appears quite From the results presented in Table 1, we see that the Nash
smooth and well approximated by a quadratic polynomial. equilibria for the learned functions are quite close to that pro-
The higher-degree polynomials apparently overÔ¨Åt the data, as duced by replicator dynamics, but with  values quite a bit                                                              ‚àí3       Separable quadratic
lower. (Since 0.876 is not a grid point, we determined its x 10
 post hoc, by running further proÔ¨Åle simulations with all
                                                                               œÜ:sum
                                                         12
agents playing 0.876, and where one agent deviates to any                      œÜ:(sum,sum squares)
of the strategies in 0, 0.05, . . . , 1 .)
                {           }                                                  œÜ:identity
                                                         11                    Sample best (symmetric)
          Method          Equilibrium si                                      Sample best (all)


     Separable quadratic      0.876       0.0027        Œµ
   Non-separable quadratic    0.876       0.0027         10
    Replicator Dynamics  (0,0.94,0.06,0,0) 0.0238
                                                        Average  
                                                          9
Table 1: Values of  for the symmetric pure-strategy equilib-
ria of games deÔ¨Åned by different payoff function approxima-
tion methods. The quadratic models were trained on proÔ¨Åles 8
conÔ¨Åned to strategies in 0.8,0.85,0.9,0.95,1 .
                    {                }
                                                          7
                                                          5       6       7       8       9      10
  In a more comprehensive trial, we collected 2.2 million ad-     Number of strategies in training set
ditional samples per proÔ¨Åle, and ran our learning algorithms
on 100 training sets, each uniformly randomly selected from
the discrete grid 0, 0.05, . . . , 1 . Each training set included Figure 3: Effectiveness of learning a separable quadratic
              {           }                           model with different forms of œÜ(s i).
proÔ¨Åles generated from between Ô¨Åve and ten of the twenty-                         ‚àí
one agent strategies on the grid. Since in this case the pro-
                                                              ‚àí3      Non‚àíseparable quadratic
Ô¨Åle of interest does not typically appear in the complete data x 10
set, we developed a method for estimating  for pure sym-                      œÜ:sum
                                                         12
metric approximate equilibria in symmetric games based on                      œÜ:(sum,sum squares)
a mixture of neighbor strategies that do appear in the test set.               œÜ:identity
Let us designate the pure symmetric equilibrium strategy of 11                 Sample best (symmetric)
the approximated game by sÀÜ. We Ô¨Årst determine the closest                     Sample best (all)

neighbors to sÀÜ in the symmetric strategy set S represented Œµ
within the data. Let these neighbors be denoted by s0 and 10
s00. We deÔ¨Åne a mixed strategy Œ± over support s0, s00 as

                                         {     }        Average  
the probability of playing s0, computed based on the relative 9
distance of sÀÜ from its neighbors: Œ± = 1 sÀÜ s0 / s0 s00 .
Note that symmetry allows a more compact‚àí | representation‚àí | | ‚àí of|
a payoff function if agents other than i have a choice of only 8
two strategies. Thus, we deÔ¨Åne U(si, j) as the payoff to a
(symmetric) player for playing strategy si S when j other 7
                                   ‚àà                      5       6       7       8       9      10
agents play strategy s0. If m 1 agents each independently
                        ‚àí                                         Number of strategies in training set
choose whether to play s0 with probability Œ±, then the proba-
bility that exactly j will choose s is given by
                           0                          Figure 4: Effectiveness of learning a non-separable quadratic
                    m   1   j      m  1 j             model with different forms of œÜ(s i).
        Pr(Œ±, j) =    ‚àí    Œ± (1  Œ±)  ‚àí ‚àí .                                        ‚àí
                      j        ‚àí
                        
We can thus approximate  of the mixed strategy Œ± by    From  Figure 3 we see that regression to a separable
    m  1                                              quadratic produces a considerably better approximate equi-
     ‚àí
max     Pr(Œ±, j) (U(si, j) Œ±U(s0, j) (1 Œ±)U(s00, j)) . librium when the size of the training set is relatively small.
si S                   ‚àí          ‚àí   ‚àí
 ‚àà  j=0                                               Figure 4 shows that the non-separable quadratic performs
    X                                                 similarly. The results appear relatively insensitive to the de-
  Using this method of estimating  on the complete data gree of aggregation applied to the representation of other
set, we compared results from polynomial regression to the agents‚Äô strategies.
method which simply selects from the training set the pure The polynomial regression methods we employed yield
                                     
strategy proÔ¨Åle with the smallest value of . We refer to pure-strategy Nash equilibria. We further evaluated four
this method as ‚Äúsample best‚Äù, differentiating between the methods that generally produce mixed-strategy equilibria:
case where we only consider symmetric pure proÔ¨Åles (la- two local regression learning methods, SVM with a Gaussian
beled ‚Äúsample best (symmetric)‚Äù) and all pure proÔ¨Åles (la-
                      2                               radial basis kernel, and direct estimation using the training
beled ‚Äúsample best (all)‚Äù).                           data. As discussed above, we computed mixed strategy equi-
  2It is interesting to observe in Figures 3 and 4 that when we re- libria by applying replicator dynamics to discrete approxima-
                                                                                     3
strict the search for a best pure strategy proÔ¨Åle to symmetric proÔ¨Åles, tions of the learned payoff functions. Since we ensure that
we on average do better in terms of  then when this restriction is not
imposed.                                                 3In the case of direct estimation from training data, the data itself