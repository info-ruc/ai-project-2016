                      Question    Classiï¬cation     by  Structure    Induction   Â 
                  Menno   van  Zaanen   and  Luiz Augusto   Pizzato  and Diego  MollaÂ´
                      Division of Information  and Communication   Sciences  (ICS)
                                       Department  of Computing
                                         Macquarie   University
                                          2109  Sydney,  NSW


                                                Australia
                                Â¡


                                 menno,  pizzato, diego Â¢ @ics.mq.edu.au
                    Abstract                                    â€œDESCâ€    (What)(is (caffeine))
                                                                â€œDESCâ€    (What)(is (Teï¬‚on))
    In this article we introduce a new approach (and            â€œLOCâ€     (Where) is (Milan)
    several implementations) to the task of question            â€œLOCâ€     What (are the twin cities)
    classiï¬cation. The approach extracts structural in-
    formation using machine learning techniques and
                                                            Table 1: Example sentences with ABL structure
    the patterns found are used to classify the questions.
    The approach ï¬ts in between the machine learning
    and handcrafting of regular expressions (as it was is a generic unsupervised grammatical inference framework
    done in the past) and combines the best of both:  that learns structure from plain text sentences. The underly-
    classiï¬ers can be generated automatically and the ing idea of ABL is that constituents can be interchanged. To
    output can be investigated and manually optimised give an example, if we exchange the noun phrase the man
    if needed.                                        in the sentence He sees the man with another noun phrase
                                                      a woman, we get another valid sentence: He sees a woman.
1  Introduction                                       This process can be reversed (by aligning sentences) and pos-
                                                      sible constituents, called hypotheses can be found. This is
Before a question answering (QA) system can answer a ques- called the alignment learning phase, one of the three phases
tion, it needs to have an idea what the question is about. of ABL, and the one that we use in this article. Table 1 shows
One of the principal tasks of the question analysis stage of a an example of the structure learned from a toy corpus of 4
QA system is the determination of the expected answer type sentences.
(EAT). Finding the EAT of a question is called question clas-
siï¬cation (or EAT classiï¬cation) [Hermjakob, 2001].     In the training phase, regular expressions associated with
  In this work we introduce a new approach to the prob- the structures found are stored together with the EAT of the
lem of EAT classiï¬cation, based on structural information corresponding questions. Several EATs with their frequen-
that can be extracted from the questions. Re-occurring struc- cies may be stored if a regular expression matches several
tures found in questions, such as â€œHow far . . . â€ may help questions.
ï¬nding the correct EAT (in this case â€œdistanceâ€) for a par- We have experimented with two implementations. The ï¬rst
ticular question. The approach described here automatically implementation, which we call hypo, uses the words in the
ï¬nds these structures during training and uses this informa- hypotheses (i.e. the words between the brackets) to form reg-
tion when classifying new questions.                  ular expressions which are stored together with the EATs of
  Our approach combines the two main methods to question the associated questions. The second implementation, called
classiï¬cation: machine learning and pattern matching. Us- unhypo, uses the words that are left after removing each hy-
ing machine learning, patterns are extracted from the training pothesis. For example, the hypo version uses the ï¬rst ques-
data. These patterns serve as regular expressions during the tion of Table 1 to produce the patterns /What/, /is caffeine/,
classiï¬cation task.                                   and /caffeine/, whereas unhypo produces the patterns /is caf-
  In the next two sections, we will describe two systems that feine/, /What/, and /What is/.
ï¬t into this approach. The ï¬rst one uses a grammatical infer- During the classiï¬cation phase we have experimented with
ence system and the second one is based on tries.     two further approaches that differ on the use of the frequency
                                                      counts of the matched regular expressions. The ï¬rst method
2  Alignment-Based    Learning   Classiï¬er            (called default) increments the counts of the EATs of the
                                                      question by the frequency that is stored with the EATs of the
The structure extraction phase of the ï¬rst system is done by regular expression. The second method (called prior) will
Alignment-Based Learning (ABL) [van Zaanen, 2002]. ABL simply increment the counts by 1. When all regular expres-
  Â£ This work is supported by the Australian Research Council, sions are tried, both methods select the EAT with the highest
ARC Discovery Grant no. DP0450750.                    count. If there several expressions with the same count, thedefault method makes a random choice, whereas the prior                              words   POS
method selects the EAT with the highest overall frequency.    Baseline               0.188   0.188
  Finally, we have experimented with the impact of the parts  ABL   hypo     default 0.516   0.682
of speech (POS) as a simple approach to disambiguate words.                  prior   0.554   0.624
We used Brillâ€™s tagger [Brill, 1992] and the resulting POS in-      unhypo   default 0.652   0.638
formation is simply combined with the plain words. Adjacent                  prior   0.580   0.594
words that have the same POS are combined into one token.     Trie  strict           0.844   0.812
For example the question Who is Federico Fellini? is, af-           ï¬‚ex              0.850   0.794
ter POS tagging, divided into three tokens: (Who, WP), (is,
VBZ) and (Federico Fellini, NNP).                             Table 2: Results on the coarse-grained data
3  Trie Classiï¬er
                                                        The results show that the POS information helps the perfor-

The second system uses a trie structure. A trie Â Â¢Â¡Â¤Â£Â¦Â¥ mance of the ABL method but not of the trie-based method.

is a data structure deï¬ned by a recursive rule Â Â¢Â¡Â§Â£Â¨Â¥
Â©


                                                     Overall, the trie-based approach outperforms the ABL imple-


       Â¥Â Â¡Â§Â£  Â¥Â Â¢Â¡Â¤Â£  Â¥ Â£


 Â Â¡Â§Â£                      .  is a set of sequences mentations. We obtained similar results with the ï¬ne-grained


                                         Â£  


whose elements are taken from the alphabet  . is the data (not shown for reasons of space).


                                                 
                                                
set of strings that contains all strings of Â£ that start with Our best results fall close to the best-performing system in
but stripped of that initial element [ClementÂ´ et al., 1998].
                                                      the bag-of-words and bag-of- 7 -grams versions of [Zhang and
  During the learning phase, all questions are inserted into a Sun Lee, 2003]. Their results ranged from 75.6% (Nearest
trie structure that contains, in addition to the token, the EAT
                                                      Neighbours on words) to 87.4% (SVM on 7 -grams). Given
and frequency information (the number of questions that use the simplicity of our methods and their potential for further
that path in the trie).                               improvement, this is encouraging.
  During the classiï¬cation phase, the trie is traversed in the
usual way. If the new question is a preï¬x of a training ques- 5 Conclusion
tion the traversal is trivial, and the node at the end of the
traversal path indicates the EAT of the question.     The results on the annotated questions of the TREC10 data
  To ï¬nd a path for unseen questions, non-matching nodes show that the approach is feasible and both systems generate
are skipped in a methodical way in what we call the look- acceptable results. We expect that future systems that fall


ahead process. Let us say that question tokens of ques- in the structure induced question classiï¬cation approach (for


                               % Â Â¢Â¡Â¤Â£& "&'()*%Â§Â¥


tion Â©! "#$ match up to :            ,   example, based on other grammatical inference systems) will


                 %  %-,.


          * +& & Â¥
and Â Â¢Â¡Â¤Â£&               does not exist. The look-  result in even better performances.


ahead process then builds a set of sub-tries of the form The automatically learned regular expressions can be in-





                  0 * %1,2Â¥3 0 456
 Â Â¡Â§Â£* '*'+&%/ . The sub-trie with the spected and extended by humans. The systems therefore


highest frequency associated is selected, and the process con- combine the advantages of machine-learning and pattern-
          %-,
tinues with   until all tokens are consumed.          matching methods.


  There are two variations of the above process. In the strict Additionally, we think that the structure found by the sys-
              %1,.
method, 0 and    must have the same POS tag. If the re- tems can be used to ï¬nd the focus of the question.


sulting set of sub-tries is empty, the search process stops and
                               %
the EAT is retrieved from the node  in the trie. In the ï¬‚ex References
method, if an empty set of sub-tries is retrieved, we consider [Brill, 1992] Eric Brill. A simple rule-based part-of-speech


0 as a sequence of question words until we ï¬nd a question tagger. In Proc. ANLP, pages 152â€“155, Trento, Italy, 1992.
             %-,2
word equal to  .
  We also experimented with a set of variations that have no [ClementÂ´ et al., 1998] J. Clement,Â´ P. Flajolet, and B. Vallee.Â´


                                                         The analysis of hybrid trie structures. In Proc. ACM-
                               '%1,2%
POS information. In this case, 0 and must be exactly the
same token. Again we allow for a strict and a ï¬‚ex version. SIAM Symposium on Discrete Algorithms, pages 531â€“539,
                                                         Philadelphia:PA, USA, 1998. SIAM Press.
4  Results                                            [Hermjakob, 2001] Ulf Hermjakob. Parsing and question
                                                         classiï¬cation for question answering. In Proc. ACL/EACL
To compare our systems with current machine-learning meth-
                                                         Workshop on Open-Domain Question Answering, 2001.
ods, we have used the same data as [Zhang and Sun Lee,
2003]. This is a collection of 5,452 training questions and [van Zaanen, 2002] Menno van Zaanen. Bootstrapping
500 test questions. The data have 6 coarse-grained classes Structure into Language: Alignment-Based Learning. PhD
and 50 ï¬ne-grained classes.                              thesis, University of Leeds, Leeds, UK, January 2002.
  Table 2 indicates the precision of the questions classiï¬ed [Zhang and Sun Lee, 2003] Dell Zhang and Wee Sun Lee.
with the coarse-grained classes for all our systems and for a Question classiï¬cation using support vector machines. In
baseline that always selects the most frequent class according Charles Clarke, Gordon Cormack, Jamie Callan, D avid
to the training data. This baseline is, of course, the same for Hawking, and Alan Smeaton, editors, Proc. SIGIR, pages
the plain words and POS tagged data. All our implementa- 26â€“32, New York:NY, US, 2003. ACM Press.
tions performed well above the baseline.