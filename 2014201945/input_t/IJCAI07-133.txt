                      Continuous Time Associative Bandit Problems‚àó
        Andras¬¥  Gyorgy¬® 1 and  Levente Kocsis1   and  Ivett Szabo¬¥ 1,2 and Csaba Szepesvari¬¥ 1,3
               1Computer and Automation Research Institute of the Hungarian Academy
                            of Sciences, Machine Learning Research Group
               gya@szit.bme.hu, kocsis@sztaki.hu, ivett7@gmail.hu, szcsaba@sztaki.hu
            2 Budapest University of Technology and Economics, Department of Stochastics
                       3University of Alberta, Department of Computing Science

                    Abstract                          continuous values, hence the time instants when the decisions
    In this paper we consider an extension of the multi- can take place take continuous values, too. The supercom-
    armed bandit problem. In this generalized setting, puter has a Ô¨Åxed cost of running, whilst the centre‚Äôs income
    the decision maker receives some side information, is based on the quality of solutions delivered. At any given
    performs an action chosen from a Ô¨Ånite set and then time only a single task can be executed on the supercomputer.
    receives a reward. Unlike in the standard bandit  Admittedly, this assumption looks absurd at the Ô¨Årst sight in
    settings, performing an action takes a random pe- the context of our example, however, we think that our results
    riod of time. The environment is assumed to be sta- can be extended to the more general case when the number of
    tionary, stochastic and memoryless. The goal is to algorithms that can run simultaneously is bounded by a con-
    maximize the average reward received in one unit  stant without much trouble. Hence we decided to stick to this
    time, that is, to maximize the average rate of return. simplifying assumption.
    We consider the on-line learning problem where      An allocation rule decides, based on the side information
    the decision maker initially does not know anything available about the task just received, which algorithm to use
    about the environment but must learn about it by  for processing it, the goal being to maximize the return rate.
    trial and error. We propose an ‚Äúupper conÔ¨Ådence   Note that this criterion is different from maximizing the total
    bound‚Äù-style algorithm that exploits the structure reward. In fact, since processing a task takes some time dur-
    of the problem. We show that the regret of this al- ing which no other tasks can be processed, the rate maximiza-
    gorithm relative to the optimal algorithm that has tion problem cannot be solved by selecting the algorithm with
    perfect knowledge about the problem grows at the  the highest expected payoff: Some tasks may look so difÔ¨Åcult
    optimal logarithmic rate in the number of decisions to solve that the best thing could be to drop them, which re-
    and scales polynomially with the parameters of the sults in no payoff, but in exchange the learner does not suffer
    problem.                                          any loss due to not processing other, possibly more rewarding
                                                      tasks. (Note that this would not be possible without the pres-
                                                      ence of side information; in the latter case the problem would
1  Introduction                                       simplify to the usual multi-armed bandit problem where one
Multi-armed bandit problems Ô¨Ånd applications in various needs to Ô¨Ånd the best option with highest reward rate.) This
Ô¨Åelds, such as statistics, control, learning theory or eco- example illustrates that a learner whose aim is to quickly learn
nomics. They became popular with the seminal paper by a good allocation strategy for rate maximization must solve
Robbins [1952] and since then they enjoy perpetual popular- two problems simultaneously: Predicting the long-term val-
ity.                                                  ues of the available algorithms given the information about
  The version of the bandit problem we consider here is mo- the task to be processed and balancing exploration and ex-
tivated by the following example: Imagine that a sequence ploitation so that the loss due to selecting inferior options
of tasks arrive for processing in a computer center that has (i.e., the regret) is kept at minimum. The problem we con-
a single supercomputer. For each of the tasks a number of sider can be thought of as a minimalistic example where the
alternative algorithms can be applied to. Some information learner faces these two problems simultaneously.
about the tasks is available that can be used to predict which Bandit problems in continuous time have been studied ear-
of the algorithms to try. The processing time depends on the lier by a number of authors (see e.g. [Kaspi and Mandel-
task at hand and also on the algorithm selected and may take baum, 1998; Karoui and Karatzas, 1997] and the references
  ‚àóThis research was supported in part by the Ministry of Econ- therein). These earlier results concern the construction of op-
omy and Transport, Hungary (Grant No. GVOP-3.1.1.-2004-05- timal allocation policies (typically in the form of Gittins in-
0145/3.0), the Hungarian National Science Foundation (Grant No. dexes) given some parametric form of the distributions of the
T047193), and by the J¬¥anos Bolyai Research Scholarship of the random variables involved. In contrast, here we consider the
Hungarian Academy of Sciences.                        agnostic case when no particular parametric form is assumed,

                                                IJCAI-07
                                                   830but the environment is supposed to be stationary and stochas- and
tic. The agnostic (or non-parametric) case has been studied             Œ¥i(x)=E  [Œ¥i1(x)]
extensively in the discrete time case. In fact, this problem denote the expected reward and delay, respectively, when op-
was Ô¨Årst considered by Robbins [1952], who introduced a tion i is chosen at the presence of the side-information x.
certainty-equivalence rule with forcing. In the same article The exact protocol of decision making is as follows: De-
Robbins showed that this rule is asymptotically consistent in cision making happens in discrete trials. Let œÑ0 =0and
the sense that the frequency of the time instants when the best let œÑt denote the time of the beginning of the t-th trial. At
arm is selected converges to one almost surely. More recently, the beginning of the tth trial the decision maker receives the
       [    ]
Agrawal 1995  suggested a number of simple sample-mean side information xt. Based on the value of xt and all infor-
based policies and showed that the resulting policies‚Äô regret mation received by the decision maker at prior trials, the de-
after n decisions is O(log n). Since it is known that no al- cision maker must select an option It. Upon executing It,
location rule can achieve regret lower than Cp log n for an
                                                      the decision maker receives a reward rt = rIt,t(xt) and suf-
appropriate (problem dependent) constant Cp [Lai and Rob-
                                                      fers a delay Œ¥t = Œ¥It,t(xt). That is, the next time point
bins, 1985], Agrawals‚Äô policies are unimprovable apart from available when the decision maker can select an option is
constant factors. Lately, Auer et al. [2002] strengthened
                                                      œÑt+1 = œÑt + Œ¥It,t(xt).
these results by suggesting policies that achieve logarithmic The goal of the decision maker is to Ô¨Ånd a good allocation
regret uniformly over time, rather than just asymptotically. policy. Formally, an allocation policy maps possible histories
An added beneÔ¨Åt of their policies is that they are simple to to some index in the set A. The gain (average reward rate)
implement.                                            delivered by an allocation policy u is given by
  We base our algorithm on algorithm UCB1 from [Auer et                           
                                                                                E   n    u
al., 2002] (see also [Agrawal, 1995]). We assume a stationary        u            [t=1 rt ]
                                                                    Œª  = lim sup    n    u ,
memoryless stochastic environment, where the side informa-                n‚Üí‚àû   E [ t=1 Œ¥t ]
tion is an i.i.d. process taking values in a Ô¨Ånite set, the payoff-
                                                            { u}                        { u}
delay sequences are jointly distributed for any of the options where rt is the reward sequence and Œ¥t is the delay se-
and their distribution depends on the side information (the quence experienced when policy u is used. An optimal allo-
precise assumptions will be listed in the next section). Like cation policy is one that maximizes this gain. Note that the
UCB1, our algorithm decides which option to choose based problem as stated is a special case of semi-Markov decision
on sample-means corrected by upper conÔ¨Ådence bounds. In problems [Puterman, 1994]. The theory of semi-Markov de-
our case, however, separate statistics are kept for all option - cision problems furnishes us with the necessary tools to char-
side-information pairs. Our main result shows that the result- acterize optimal allocation policies: Let us deÔ¨Åne the optimal
                                                      gain by
ing policy achieves logarithmic regret uniformly in time and               ‚àó       u
                                                                          Œª  =supŒª  .
hence it is also unimprovable, apart from constant factors.                     u
  The paper is organized as follows: We deÔ¨Åne the problem                                      ‚àó     u
and the proposed algorithm in Section 2. Our main result, a A policy u is said to be optimal if it satisÔ¨Åes Œª = Œª .It
logarithmic regret bound on the algorithm‚Äôs performance is follows from the generic theory that there exist determinis-
presented in Section 3. Conclusions are drawn in Section 4. tic stationary policies that are optimal. An optimal action for
                                                      some x ‚ààXcan be determined by ordering the options by
                                                      their relative values.Therelative value of option i upon ob-
2  The algorithm                                      serving x is the expected reward that can be collected minus
The problem of the previous section is formalized as fol- the expected reward that is not gained during the time it takes
lows: Let K denote the number of options available, and let to collect the reward:
X denote the set of possible values of the side information,          ‚àó                  ‚àó
                                                                     qi (x)=ri(x) ‚àí Œ¥i(x)Œª .
which is assumed to be Ô¨Ånite. Let x1,x2,...,xt bearan-
dom sequence of covariates representing the side information Intuitively it should be clear that a policy that always selects
available at the time of the t-th decision, generated indepen- options with best relative values should optimize the over-
dently from a distribution p supported on X . At each deci- all gain. In fact, it follows from the theory of semi-Markov
sion point the decision maker may select an option It from decision problems that this is indeed the case. A stationary
A    {       }
  =   1,...,K  and receives reward rt = rIt ,t(xt),where deterministic policy u : X‚ÜíAis optimal if and only if it
rit(xt) is the reward the decision maker would have received obeys the constraints
had it chosen option i. Unlike in classical bandit problems the              ‚àó                    ‚àó
                                                           ru(x)(x) ‚àí Œ¥u(x)(x)Œª =max[ri(x) ‚àí Œ¥i(x)Œª ] (1)
collection of the reward takes some random time. When op-                        i‚ààA
tion i is selected and the side information equals x, this time is
                                                      simultaneously for all x ‚ààX.
Œ¥it(x). We assume that for any Ô¨Åxed x and i, (rit(x),Œ¥it(x))
                                                        The (total) regret of an allocation policy is deÔ¨Åned as the
is an i.i.d. sequence, independent of {xt}. We further as-
                                                      loss suffered due to not selecting an optimal option in each
sume that rit(x) ‚àà [rmin,rmax], Œ¥it(x) ‚àà [Œ¥min,Œ¥max] with
                                                      time step. Since we are interested in the expected regret only,
Œ¥min > 0. (We expect that the boundedness assumptions can
                                                      our regret deÔ¨Ånition uses the optimal gain Œª‚àó:
be relaxed to Œ¥it(x) ‚â• 0 and appropriate moment conditions
                                                                              n      n
on Œ¥it(x) and rit(x).) Let                                                         
                                                                            ‚àó
                                                                     Rn = Œª     Œ¥t ‚àí    rt.
                         E
                 ri(x)=   [ri1(x)]                                           t=1     t=1

                                                IJCAI-07
                                                   831The value of the Ô¨Årst term is the maximum reward that could Here ct,s is an appropriate deterministic sequence that is se-
                                                                                                      u
be collected during the time of the Ô¨Årst n decisions. The ex- lected such that simultaneously for all policies u ‚ààU, Œªt is
pected regret thus compares the expected value of the latter                 u
                                                      in the ct,Tu(t)-vicinity of Œª with high probability. This se-
with the expected value of the actual total payoffs received. quence will be explicitly constructed during the proof where
It follows that an allocation policy that minimizes the regret we will also make sure that it depends on known quantities
will optimize the rate of return.                     only. In words, Œªn is the optimal gain that the decision maker
                     X
  When Œ¥it(x)=1,and    has a single element, the problem can guarantee itself with high probability given the data seen
reduces to the classical stochastic bandit problem. Since for so far.
the stochastic bandit problems the regret is lower bounded by                         UCB
                                                        Our proposed allocation policy, {ut }, selects the op-
       , we are seeking policies whose regret grows at most     UCB
O(log n)                                              tions It = ut (xt) by the rule
at a logarithmic rate.                                                                             
                                                          UCB                    ‚àí
  The idea underlying our algorithm is to develop upper  ut   (x) = argmax  rit(x) Œ¥it(x)Œªt +ÀÜct,Ti(x,t) ,
                      ‚àó                                              i‚ààA
estimates of the values qi (x) with appropriate conÔ¨Ådence
bounds. Just like in [Auer et al., 2002], the upper conÔ¨Å- where, similarly to ct,s, cÀÜt,s is an appropriate deterministic
dence estimates are selected to ensure that for any given x sequence that will be chosen later.
with p(x) > 0 all options are ultimately selected inÔ¨Ånitely
often, but at the same time suboptimal options are selected 3Mainresult
increasingly rarely.                                  Our main result is the following bound on the expected regret:
  The algorithm is as follows: Let us consider the t-th deci- Theorem 1 Let the assumptions of the previous section hold
                                 ‚àó                                                                   UCB
sion. If we had a good estimate Œªt of Œª , then for any given on rit,Œ¥it,xt.LetRn be the n-step regret of policy u .
x we could base our decision on the estimates of the relative      ‚â•
       ‚àó                                              Then, for all n 1,
                               it   ‚àí  it   t                                     
values qi (x) of the options given by r (x) Œ¥ (x)Œª .Here                       2
rit(x) denotes the average of rewards during the Ô¨Årst n deci-     ‚àó          2œÄ
                                                      E [Rn] ‚â§  L     2+             K|X | +2K|X | log(n)
sions for those time points when the side information is x and            3(|U| +1)2
option i was selected, and Œ¥it(x) is deÔ¨Åned analogously:                                        

                                                                                      |U|
                       t                                                      a log(n(    +1))
                   1                                               +                     2       ,
                           I                                                        Œîi(x)
     rit(x)=                (Is = i, xs = x) rs,                     i:Œîi>0 x‚ààX
                  i
                T (x, t) s=1                                 ‚àó        ‚àó
                                                      where L  = Œ¥maxŒª ‚àí  rmin,
                         t
                                                                      ‚àó       ‚àó
                   1                                      Œîi(x)=maxqj   (x) ‚àí qi (x) ‚â• 0,i=1,...,K,
     Œ¥it(x)=               I (Is = i, xs = x) Œ¥s,                 j‚ààA
                Ti(x, t) s=1
                                                      and the positive constant a is given by (7) in the proof of the
where Ti(x, t) denotes the number of times option i was theorem.
selected when side information x was present in trials The proof follows similar lines to that of Theorem 1 of [Auer
1, 2,...,t:         t                                et al., 2002], with the main difference being that now we have
                                                                                   ‚àó
           Ti(x, t)=   I (It = i, xt = x) .           to handle the estimation error of Œª . We prove the theorem
                    j=1                               using a series of propositions.
                                                        The Ô¨Årst proposition bounds the expected regret in terms of
  The plan is to combine appropriate upper bounds on ri(x) the number of times when some suboptimal option is chosen:
and lower bounds on Œ¥i(x) based on the respective sample
                                                      Proposition 2 The following bound holds for the expected
averages rit(x), Œ¥it(x) and Ti(x, t), to obtain an upper es-
         ‚àó                                            regret of an arbitrary policy, u =(u1,u2,...):
timate of qi (x). However, in order to have a sample based                                        
                                                                                 n
estimate, we also need an appropriate lower estimate of Œª‚àó.                    
                                                        E     ‚â§          ‚àó   E     I       ‚ààU‚àó
This estimate is deÔ¨Åned as follows:                       [Rn]      p(x)L (x)       (ut(x)     (x)) , (2)
  Let U denote the set of stationary policies: U = {u|u :       x‚ààX             t=1
                               u
X‚ÜíA}.Pickanyu        ‚ààU.LetŒªt    denote the empirical where
                                                                 ‚àó             ‚àó           ‚àó
estimate of the gain of policy u:                              U  (x)={i ‚ààA|qi  (x)=maxqj   (x)}
                  t                                                                  j‚ààA
              u     s=1 I (Is = u(xs)) rs
            Œªt =  t                                  denotes the set of optimal options at x, and
                       I (Is = u(xs)) Œ¥s                           ‚àó                ‚àó
                    s=1                                           L (x)=max(Œ¥j(x)Œª    ‚àí rj (x))
                                                                           j
and let Tu(t) denote the number of times when an option
                                                      is the loss for the worst choice at x.Further,byL‚àó(x) ‚â§ L‚àó,
‚Äòcompatible‚Äô with policy u was selected:                                                          
                     t                                                        n
                                                       E       ‚â§    ‚àó        E     I       ‚ààU‚àó
             Tu(t)=     I (Is = u(xs)) .                 [Rn]      L     p(x)       (ut(x)     (x))
                     s=1                                             x‚ààX       t=1                   
                     ‚àó                                                     n
Then Œªt, the estimate of Œª is deÔ¨Åned by                             ‚àó    E     I       ‚ààU‚àó
                         u                                     =   L             (ut(x)    (x),xt = x) .
              Œªt =max(Œªt  ‚àí ct,T (t)).                               x‚ààX    t=1
                   u‚ààU         u

                                                IJCAI-07
                                                   832                                            ‚àó
Proof.   Let us consider the t-th term, E [Œ¥tŒª ‚àí rt], Proposition 3 Assume that the following conditions are sat-
                                        ‚àó
of the expected regret. We   have E [Œ¥tŒª ‚àí rt]=      isÔ¨Åed:
           ‚àó                                                                   u
  i‚ààA E [(Œ¥tŒª ‚àí rt)I (It = i)] . Using It = ut(xt) and that            u
                                                                     Œª    ‚â•  Œªt ‚àí ct,T (t),           (3)
ut depends only on the past, i.e., if Ft is the sigma algebra of                     u
                                                                       ‚àó       ‚àó
 1  1  1     t  t t      t      Ft‚àí1                                      ‚â§
x ,r ,Œ¥ ,...,x ,r ,Œ¥ then I = i is   measurable, we                  Œª       Œªt + ct,Tu‚àó (t).         (4)
get that
                                                      where the Ô¨Årst condition is meant to hold for all stationary
        ‚àó                                                      ‚ààU
  E [(Œ¥tŒª ‚àí rt)I (It = i)]                            policies u  .Then
       E          ‚àó ‚àí        I                                       ‚àó ‚â•   ‚â•   ‚àó ‚àí
    =    [(Œ¥i,t(xt)Œª  ri,t(xt)) (It = i)]                           Œª    Œªt   Œª   2ct,Tu‚àó (t).        (5)
                    ‚àó                                                                         u
    =  E [E [(Œ¥i,t(xt)Œª ‚àí ri,t(xt))I (It = i) |Ft‚àí1,xt]]                                       ‚àí
                                                      Proof.  Let u be the policy that maximizes Œªt ct,Tu(t).
                             ‚àó                                                              u
    =  E [I (It = i) E [(Œ¥i,t(xt)Œª ‚àí ri,t(xt))|Ft‚àí1,xt]]                
                                                      Since (3) holds for u , we get that Œªt = Œªt ‚àí ct,T  ‚â§
                           ‚àó                                                                       u (t)
    =  E [I (It = i) E [(Œ¥i(xt)Œª ‚àí ri(xt))|Ft‚àí1,xt]]   u    ‚àó
                                                      Œª   ‚â§ Œª , proving the upper bound for Œªt. On the other hand,
                         ‚àó                                                         ‚àó
    =  E [I (It = i)(Œ¥i(xt)Œª ‚àí ri(xt))] .                                      ‚â•    ‚àí
                                                      because of the choice of u , Œªt Œªt ct,Tu‚àó(t) which can be
                                                                             ‚àó ‚àí
Now, using again that ut does not depend on xt,weget  further lower bounded by Œª 2ct,Tu‚àó(t) using (4), proving
                                                                                                        
        ‚àó                                             the lower bound for Œªt.
   E [Œ¥tŒª ‚àí rt]
                                                       The following proposition shows that Œªt is indeed a lower
                                 ‚àó                              ‚àó
     =      E [I (ut(xt)=i)(Œ¥i(xt)Œª ‚àí ri(xt))]        bound for Œª with high probability.
         i‚ààA
                                                    Proposition 4 Let
                       ‚àó                                                 
     =   ‚àí        p(x)qi (x)E [I (ut(x)=i) |xt = x]                                
           i‚ààA x‚ààX                                                         2c1 log(t |U| +1)
                                                                 ct,s =
                       ‚àó                                                           s
     =   ‚àí        p(x)qi (x)E [I (ut(x)=i)] .
           i‚ààA x‚ààX                                    where                                        
                                                                                2   2              2
                                                                    (rmax ‚àí rmin)  rmax(Œ¥max ‚àí Œ¥min)
Then                                                    c1 =2max          2       ,       4           .
                                                                       Œ¥min            Œ¥min
E    ‚àó ‚àí       ‚àí                ‚àó   E I
 [Œ¥tŒª   rt]=         p(x)      qi (x) [ (ut(x)=i)]    Then
                 x‚ààX     i‚ààU ‚àó(x)                                                         
                                                          P        ‚àó ‚àí           P   ‚àó      ‚â§  2
                                ‚àó                              Œªt <Œª    2ct,Tu‚àó (t) + Œª  < Œªt     .
               ‚àí     p(x)      qi (x)E [I (ut(x)=i)] .                                           t
                 x‚ààX     i‚ààU ‚àó(x)                    Proof. According to Proposition 3, if (3) holds for all station-
                                                                                    ‚àó        ‚àó
                                                                                     ‚â•   t ‚â•   ‚àí  t,T ‚àó (t)
                                 ‚àó                    ary policies u and if (4) holds then Œª Œª Œª 2c u   .
Let wt(i|x)=E [I (ut(x)=i)] if i ‚ààU(x) and wt(i|x)=0                     ‚àó                   ‚àó
                                                      Hence, in order Œªt <Œª ‚àí 2ct,T ‚àó (t) or Œªt >Œª to hold, we
                   |        |             |                                      u
otherwise, and let Œºt(i x)= wt(i x)/ j‚ààA wt(j x).Then must have that one of the conditions in Proposition 3 is vio-
   |   ‚â•     |                 |  ‚â§
Œºt(i x)  wt(i x) (since j‚ààA wt(j x)  1), the Ô¨Årst term lated. Using a union bound we get
                                                                                    
of the last expression can be upper bounded by                 ‚àó                 ‚àó
                                                      P  Œªt <Œª  ‚àí 2ct,T ‚àó (t) + P Œª < Œªt
                                                                   u                              
                             ‚àó                                        u                      ‚àó
          vt = ‚àí     p(x)   qi (x)Œºt(i|x).              ‚â§     P   u      ‚àí           P   ‚àó
                                                                 Œª  < Œªt   ct,Tu(t) +   Œª <  Œªt + ct,Tu(t) .
                 x‚ààX      i                                 u

Since Œºt(i|x)=0if i is not optimal, Œºt deÔ¨Ånes an optimal Fix u. By the law of total probability,
(stochastic) policy and hence, Bellman‚Äôs equation gives t                  t                        
                                              v  =             u                       u
                                                      P   u     ‚àí              P   u     ‚àí
0. Therefore,                                            Œª <  Œªt  ct,Tu(t) =      Œª  < Œªt  cts,Tu(t)=s  .
                                                                          s=1
     ‚àó                            ‚àó
E [Œ¥tŒª ‚àí rt] ‚â§‚àí       p(x)       q (x)E [I (ut(x)=i)]
                                  i                   DeÔ¨Åne
                  x‚ààX     i‚ààU ‚àó(x)
                                                             t                        
                         ‚àó                              u         I                  u
             ‚â§      p(x)L (x)       E [I (ut(x)=i)]    rÀÜt =       (Is = u(xs)) rs,r=       p(x)ru(x)(x)
                x‚ààX           i‚ààU ‚àó(x)                        s=1                       x‚ààX
                                                               t
                         ‚àó                ‚àó                                             
             =      p(x)L (x)E [I (ut(x) ‚ààU (x))] .    u                            u
                                                       Œ¥ÀÜt =      I (Is = u(xs)) Œ¥s,Œ¥=      p(x)Œ¥u(x)(x).
                x‚ààX
                                                               s=1                       x‚ààX
Summing up this last expression over t gives the advertised Using elementary algebra, we get that
                                                                               
bound.                                                      u    u
                                                        P  Œª  < Œªt ‚àí cts,Tu(t)=s
  The next statements are used to prove that with high prob-                u      u
                           ‚àó                              ‚â§   P (ctsŒ¥min/2 ‚â§ rÀÜ /s ‚àí r ,Tu(t)=s)
ability Œªt is a good estimate of Œª . Here and in what follows              t                        
                                                 ‚àó
 ‚àó                                         ‚àó    u                 P     2       ‚â§   u ‚àí ÀÜu
u denotes an arbitrary (Ô¨Åxed) optimal policy and Œªt = Œªt .      +   ctsŒ¥min/rmax   Œ¥   Œ¥t /s, Tu(t)=s .

                                                IJCAI-07
                                                   833              u      u
Exploiting that rÀÜt and Œ¥ÀÜt are martingale sequences and re- The expectations of the second two terms will be bounded
                                                                                                     
sorting to a slight variant of the Hoeffding-Azuma bound by Proposition 4. The Ô¨Årst term, multiplied by Zt(s, s ) is
(see, e.g. [Devroye et al., 1996]), we get the bound 2/(|U| + bounded by
  ‚àí2                                                          
1)t . Summing over s and u and by an analogous argument                         ‚àó
            ‚àó                                                 I         ‚àí                  
   P   ‚àó                                             Zt(s, s ) ri,t‚àí1(x) Œ¥i,t‚àí1(x)Œª +ÀÜct‚àí1,s
for   Œª < Œªt + ct,Tu(t) , we get the desired bound.                                                    
                                                                      ‚àó        ‚àó       ‚àó
                                                                   > r   (x) ‚àí Œ¥  (x)(Œª ‚àí 2ct,s)+ÀÜct‚àí1,s .
  Now we are ready to prove the main theorem. In the                  t‚àí1      t‚àí1
proof we put a superscript ‚Äò‚àó‚Äô to any quantity that refers to
                 ‚àó               ‚àó                    When this expression equals one then at least one of the fol-
the optimal policy u . For example, rt (x)=ru‚àó(x),t(x), lowing events hold:
 ‚àó                 ‚àó
Œ¥t (x)=Œ¥u‚àó(x),t(x), T (x, t)=Tu‚àó(x)(x, t),etc.
                                                      At,s,s =
                                         UCB
Proof of Theorem 1. Proposition 2 applied to u shows    ‚àó       ‚àó     ‚àó   ‚àó     ‚àó    ‚àó            
that it sufÔ¨Åces if for any Ô¨Åxed x ‚ààXand suboptimal choice {rt‚àí1(x)‚àíŒ¥t‚àí1(x)Œª ‚â§r (x)‚àíŒ¥ (x)Œª ‚àíct‚àí1,s,Zt(s, s )=1},
     ‚àó
i ‚ààU(x)  we derive an O(log n) upper bound on the ex-  t,s,s
                                               UCB    B     =
pected number of times choice i would be selected by u                  ‚àó             ‚àó             
                                                      {       ‚àí          ‚â•     ‚àí                       }
when the side information is x. That is, we need to show ri,t‚àí1(x) Œ¥i,t‚àí1(x)Œª ri(x) Œ¥i(x)Œª +ÀÜct‚àí1,s ,Zt(s, s )=1 ,
                                                                 ‚àó       ‚àó    ‚àó               ‚àó
                                                       t,s,s {     ‚àí           i   ‚àí  i         t,s }
         n                                             C     =  r (x)   Œ¥ (x)Œª <r(x)    Œ¥ (x)Œª +2ÀÜc    .
                                                                        ‚àó
               UCB                                                     ‚àí
     E      I ut   (x)=i, xt = x  ‚â§  O(log n).  (6)   Here ct‚àí1,s =ÀÜct‚àí1,s 2Œ¥t ct‚àí1,s. Now let us give the choices
        t=1                                           for the conÔ¨Ådence intervals. DeÔ¨Åne
                                                                        
                                                                                       
  Let qit(x)=rit(x) ‚àí Œ¥it(x)Œªt. Using the deÔ¨Ånition of             uts =  log t  |U| +1   /s.
 UCB      UCB
ut   ,ifut   (x)=i    holds then qit(x)+ÀÜct,Ti(x,t) >
 ‚àó                                                                                               ‚àö
qt (x)+ÀÜct,T ‚àó(x,t). Hence, for any integer A(n, x),  We have already deÔ¨Åned cts in Proposition 4: cts = 2c1uts,
                                                      where c1 was deÔ¨Åned there, too. We deÔ¨Åne cÀÜts implicitly,
n                                                                      
       UCB                                            through a deÔ¨Ånition of cts which is deÔ¨Åned so as to keep the
   I  ut  (x)=i   ‚â§ A(n, x)
                                                      probability of At,s,s small: Let
t=1                                                        
       n
                                                                             2  2                2
            UCB                                        a0 =  8max{(rmax  ‚àí rmin) ,rmax(Œ¥max ‚àí Œ¥min)/Œ¥min},
    +    I ut   (x)=i, Ti(x, t ‚àí 1) ‚â• A(n, x),xt = x
                                                                          
      t=1                                                                    2
                                                      cts = a0uts. and a1 = 2Œ¥maxc1.DeÔ¨Åne
             n                                  
  ‚â§             I   UCB             ‚àí    ‚â•                                           2
    A(n, x)+      ut   (x)=i, Ti(x, t 1)   A(n, x) .                     a =(a0 + a1) ,               (7)
             t=1
                                                      and cÀÜts =(a0 +a1)uts. Using these deÔ¨Ånitions we bound the
We write the t-th term in the last sum as follows:    probabilities of the above three events. We start with At,s,s :
                              
   UCB                                                                     ‚àó      ‚àó          
I                   ‚àí   ‚â•                             P   t,s,s ‚â§ P     ‚â§      ‚àí        t
  ut   (x)=i, Ti(x, t 1)   A(n)                         (A    )    (cts/2 r (x)  rt (x),Z (s, s )=1)   
                                                                                ‚àó
                              ‚àó                                   P       ‚àó ‚â§       ‚àí  ‚àó         
  =  I qi,t‚àí1(x)+ÀÜct,T (x,t‚àí1) >qt‚àí1(x)+ÀÜct‚àí1,T ‚àó(x,t‚àí1),       +    cts/(2Œª )  Œ¥t (x) Œ¥ (x),Zt(s, s )=1
                    i                                                                      
                                                                       2                2
                                                               ‚â§ exp  ‚àícts s/(2(rmax ‚àí rmin) )
                  Ti(x, t ‚àí 1) ‚â• A(n)                                                           
                                                                         2      ‚àó 2           2
                                                                   ‚àí              max ‚àí  min
                             ‚àó                                 +exp    cts s/(2(Œª ) (Œ¥    Œ¥   ) )
         I  i,t‚àí1     t‚àí1,s          t‚àí1,s  t
  =        q    (x)+ÀÜc     >qt‚àí1(x)+ÀÜc     Z  (s, s ),                            t
  (s,s)‚ààH(t)                                         Here    we    used   that     s=1 I (It = i, xt = x) rt,
                                                      t
                                                        s=1 I (It = i, xt = x) Œ¥t are martingales for any x, i,
where                                                 and the above-mentioned variant of the Hoeffding-Azuma
                                                                                          
                                                    inequality. Plugging in the deÔ¨Ånition of cts we get that the
   H(t)={(s, s    )|1 ‚â§ s ‚â§ t ‚àí 1,A(t) ‚â§ s ‚â§ t ‚àí 1},                                        ‚àí4        ‚àí2
                                                      probability of event At,s,s is bounded by 2t (|U| +1) .
            I       ‚àí         ‚àó    ‚àí
Zt(s, s )=    (Ti(x, t 1) = s ,T (x, t 1) = s) .      The probability of Bt,s,s can be bounded in the same way
                                                                                        
                                                     and by the same expression since cÀÜts >cts. Therefore
Fix any s, s ‚àà H(t). Using the deÔ¨Ånition of qit(x),
                                                        n    
                      ‚àó
I  qi,t‚àí1(x)+ÀÜct‚àí1,s >qt‚àí1(x)+ÀÜct‚àí1,s                                [P (At,s,s )+P (Bt,s,s )]
       
                                                          t=1 (s,s)‚ààH(t)
   ‚â§  I ri,t‚àí1(x) ‚àí Œ¥i,t‚àí1(x)Œªt‚àí1 +ÀÜct‚àí1,s
                                                                n                             2
                                                                                 4           2œÄ
                  ‚àó        ‚àó                                ‚â§                           ‚â§           .
            >    rt‚àí1(x) ‚àí Œ¥t‚àí1(x)Œªt‚àí1 +ÀÜct‚àí1,s,                             4 |U|    2     |U|    2
                                                               t=1 (s,s)‚ààH(t) t ( +1)   3(   +1)
         ‚àó           ‚àó
           ‚â•  t‚àí1 ‚â•    ‚àí                                                                
        Œª    Œª      Œª    2ct‚àí1,Tu‚àó (t‚àí1)                                                               2
                                                    Moreover, deÔ¨Åne A(t, x)=a log(t( |U| +1))/Œîi(x) .
                  ‚àó                     ‚àó
       I           ‚àí                 I                                                                 
      +   Œªt‚àí1 <Œª    2ct‚àí1,Tu‚àó (t‚àí1) + Œª  < Œªt‚àí1 .    Now, if Ct,s,s holds then one must have Œîi(x) > 2ÀÜct,s ,

                                                IJCAI-07
                                                   834