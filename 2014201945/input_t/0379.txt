                  Bounded Policy Iteration for Decentralized POMDPs

         Daniel S. Bernstein                Eric A. Hansen                  Shlomo Zilberstein
     Dept. of Computer Science        Dept. of CS and Engineering       Dept. of Computer Science
    University of Massachusetts       Mississippi State University      University of Massachusetts
        Amherst, MA 01003             Mississippi State, MS 39762           Amherst, MA 01003
         bern@cs.umass.edu              hansen@cse.msstate.edu             shlomo@cs.umass.edu

                    Abstract                          problems, there are various ways to extend it to the inÔ¨Ånite-
                                                      horizon case. However, in both cases, it suffers from the fact
    We present a bounded policy iteration algorithm for that the memory requirements grow quickly with each itera-
    inÔ¨Ånite-horizon decentralized POMDPs. Policies    tion, and in practice it has only been used to solve very small
    are represented as joint stochastic Ô¨Ånite-state con- problems. It is likely that any optimal algorithm would suf-
    trollers, which consist of a local controller for each fer this problem, as Ô¨Ånite-horizon DEC-POMDPs have been
    agent. We also let a joint controller include a cor- shown to be NEXP-complete, even for just two agents [Bern-
    relation device that allows the agents to correlate stein et al., 2002].
    their behavior without exchanging information dur-  In this paper, we present a memory-bounded dynamic
    ing execution, and show that this leads to improved programming algorithm for inÔ¨Ånite-horizon DEC-POMDPs.
    performance. The algorithm uses a Ô¨Åxed amount     The algorithm uses a stochastic Ô¨Ånite-state controller to rep-
    of memory, and each iteration is guaranteed to pro- resent the joint policy for the agents. A straightforward ap-
    duce a controller with value at least as high as the proach is to use a set of independent local controllers, one for
    previous one for all possible initial state distribu- each agent. We provide an example to illustrate that higher
    tions. For the case of a single agent, the algorithm value can be obtained through the use of shared randomness.
    reduces to Poupart and Boutilier‚Äôs bounded policy As such, we deÔ¨Åne a joint controller to be a set of local con-
    iteration for POMDPs.                             trollers along with a correlation device. The correlation de-
                                                      vice is a Ô¨Ånite-state machine that sends a signal to all of the
                                                      agents on each time step. Its behavior can be determined prior
1  Introduction                                       to execution time, and thus it does not require that the agents
The Markov decision process (MDP) framework has proven exchange information after receiving local observations.
to be useful for solving problems of sequential decision mak- Our algorithm generalizes bounded policy iteration for
ing under uncertainty. For some problems, an agent must POMDPs [Poupart and Boutilier, 2003] to the multi-agent
base its decision on partial information about the system case. On each iteration, a node is chosen from one of the
state. In this case, it is often better to use the more gen- local controllers or the correlation device, and its parameters
eral partially observable Markov decision process (POMDP) are updated through the solution of a linear program. The
framework. Though POMDPs are difÔ¨Åcult to solve in the generalization has the same theoretical guarantees as in the
worst case, much progress has been made in the development POMDP case. Namely, an iteration is guaranteed to produce
of practical dynamic programming algorithms [Smallwood a new controller with value at least as high for every possible
and Sondik, 1973; Cassandra et al., 1997; Hansen, 1998; initial state distribution.
Poupart and Boutilier, 2003; Feng and Zilberstein, 2004]. In our experiments, we applied our algorithm to idealized
  Even more general are problems in which a team of deci- networking and robot navigation problems. Both problems
sion makers, each with its own local observations, must act are too large for exact dynamic programming, but could be
together. Domains in which these types of problems arise handled by our approximation algorithm. We found that the
include networking, multi-robot coordination, e-commerce, addition of a correlation device gives rise to better solutions.
and space exploration systems. To model such problems, we In addition, larger controllers most often lead to better solu-
can use the decentralized partially observable Markov deci- tions.
sion process (DEC-POMDP) framework. Though this model   A number of approximation algorithms have been devel-
has been recognized for decades (see, e.g., [Witsenhausen, oped previously for DEC-POMDPs [Peshkin et al., 2000;
1971]), there has been little work on efÔ¨Åcient algorithms for Nair et al., 2003; Emery-Montemerlo et al., 2004]. How-
it.                                                   ever, the previous algorithms do not guarantee both bounded
  Recently, an exact dynamic programming algorithm was memory usage and monotonic value improvement for all ini-
proposed for DEC-POMDPs  [Hansen et al., 2004]. Though tial state distributions. Furthermore, the use of correlated
the algorithm was presented in the context of Ô¨Ånite-horizon stochastic policies in the DEC-POMDP context is novel. Theimportance of correlation has been recognized in the game
                                                               AB             +R              AA
theory community [Aumann, 1974], but there has been little     BA     AA                      AB
work on algorithms for Ô¨Ånding correlated policies.             BB                             BA
2  Background                                             ‚ÄìR                                       ‚ÄìR
                                                                  s                        s
In this section, we present our formal framework for               1                        2
multi-agent decision making. A decentralized partially-
observable Markov decision process (DEC-POMDP) is a tu-
ple hI, S, {Ai}, {Oi}, P, Ri, where
                                                                                       BB
  ‚Ä¢ I is a Ô¨Ånite set of agents indexed 1, . . . , n                            +R
  ‚Ä¢ S is a Ô¨Ånite set of states

  ‚Ä¢ Ai is a Ô¨Ånite set of actions available to agent i and Figure 1: This Ô¨Ågure shows a DEC-POMDP for which the
                                                      optimal memoryless joint policy requires correlation.
    A~ =  √ói‚ààI Ai is the set of joint actions, where ~a =
    ha1, . . . , ani denotes a joint action

  ‚Ä¢ Oi is a Ô¨Ånite set of observations for agent i and O~ = Taken together, the agents‚Äô controllers determine the con-
                                                                             0
    √ói‚ààI Oi is the set of joint observations, where ~o = ditional distribution P (~a, ~q |~q, ~o). This is denoted an inde-
    ho1, . . . , oni denotes a joint observation      pendent joint controller. In the following subsection, we
                                                      show that independence can be limiting.
  ‚Ä¢ P is a set of Markovian state transition and observation
                        0
    probabilities, where P (s , ~o|s,~a) denotes the probability 3.2 The Utility of Correlation
    that taking joint action ~a in state s results in a transition
    to state s0 and joint observation ~o              The joint controllers described above do not allow the agents
                                                      to correlate their behavior via a shared source of randomness.
            ~
  ‚Ä¢ R : S √ó A ‚Üí < is a reward function                We will use a simple example to illustrate the utility of cor-
  In this paper, we consider the case in which the process relation in partially observable domains where agents have
unfolds over an inÔ¨Ånite sequence of stages. At each stage, all limited memory. This example generalizes the one given in
agents simultaneously select an action, and each receives the [Singh et al., 1994] to illustrate the utility of stochastic poli-
global reward and a local observation. The objective of the cies in single-agent partially observable settings.
agents is to maximize the expected discounted sum of rewards Consider the DEC-POMDP shown in Figure 1. This prob-
received. We denote the discount factor Œ≥ and require that lem has two states, two agents, and two actions per agent (A
0 ‚â§ Œ≥ < 1.                                            and B). The agents each have only one observation, and thus
                                                      cannot distinguish between the two states. For this example,
3  Finite-State Controllers                           we will consider only memoryless policies.
                                                        Suppose that the agents can independently randomize their
Our algorithm uses stochastic Ô¨Ånite-state controllers to rep-
                                                      behavior using distributions P (a1) and P (a2), and consider
resent policies. In this section, we Ô¨Årst deÔ¨Åne a type of con- the policy in which each agent chooses either A or B accord-
troller in which the agents act independently. We then provide ing to a uniform distribution. This yields an expected reward
an example demonstrating the utility of correlation, and show of ‚àí R per time step, which results in an expected long-term
how to extend the deÔ¨Ånition of a joint controller to allow for 2
                                                      reward of  ‚àíR  . It is straightforward to show that no in-
correlation among agents.                                      2(1‚àíŒ≥)
                                                      dependent policy yields higher reward than this one for all
3.1  Local Finite-State Controllers                   states.
In a DEC-POMDP, each agent must select an action based on Next, let us consider the larger class of policies in which
its history of local observations. Finite-state controllers pro- the agents may act in a correlated fashion. In other words, we
                                                      consider all joint distributions P (a1, a2). Consider the policy
vide a way to represent local policies using a Ô¨Ånite amount               1                           1
of memory. The state of the controller is based on the ob- that assigns probability 2 to the pair AA and probability 2 to
servation sequence, and the agent‚Äôs actions are based on the the pair BB. This yields an average reward of 0 at each time
state of its controller. We allow for stochastic transitions step and thus an expected long-term reward of 0. The dif-
and stochastic action selection, as this can help to make ference between the rewards obtained by the independent and
up for limited memory. This type of controller has been correlated policies can be made arbitrarily large by increasing
used previously in the single-agent context [Platzman, 1980; R.
Meuleau et al., 1999; Poupart and Boutilier, 2003].
  Formally, we deÔ¨Åne a local Ô¨Ånite-state controller for agent 3.3 Correlated Joint Controllers
i to be a tuple hQi, œài, Œ∑ii, where Qi is a Ô¨Ånite set of con- In the previous subsection, we established that correlation can
troller nodes, œài : Qi ‚Üí ‚àÜAi is an action selection function, be useful in the face of limited memory. In this subsection, we
and Œ∑i : Qi √ó Ai √ó Oi ‚Üí ‚àÜQi is a transition function. The extend our deÔ¨Ånition of a joint controller to allow for correla-
functions œài and Œ∑i parameterize the conditional distribution tion among the agents. To do this, we introduce an additional
      0
P (ai, qi|qi, oi).                                    Ô¨Ånite-state machine, called a correlation device, that provides                             0
 Variables: , x(c, ai), x(c, ai, oi, qi)
 Objective: Maximize 
 Improvement constraints:
                                 X
      ‚àÄs, q‚àíi, c V (s, ~q, c) +  ‚â§ P (a‚àíi|c, q‚àíi)[x(c, ai)R(s,~a) +
                                  ~a
                                    X              0    0                  0          0     0  0 0
                                 Œ≥       x(c, ai, oi, qi)P (q‚àíi|c, q‚àíi, a‚àíi, o‚àíi)P (s , ~o|s,~a)P (c |c)V (s , ~q , c )]
                                  s0,~o,~q 0,c0
 Probability constraints:
                              X                         X            0
                          ‚àÄc     x(c, ai) = 1, ‚àÄc, ai, oi  x(c, ai, oi, qi) = x(c, ai)
                              a                          0
                               i                        qi
                                                          0            0
                             ‚àÄc, ai x(c, ai) ‚â• 0, ‚àÄc, ai, oi, qi x(c, ai, oi, qi) ‚â• 0

Table 1: The linear program used to Ô¨Ånd new parameters for agent i‚Äôs node qi. The variable x(c, ai) represents P (ai|qi, c), and
                    0                0
the variable x(c, ai, oi, qi) represents P (ai, qi|c, qi, oi).

extra signals to the agents at each time step. The device op- lated joint controller, we can either change the correlation de-
erates independently of the DEC-POMDP process, and thus vice or one of the local controllers. Both improvements can
does not provide the agents with information about the other be done via a bounded backup, which involves solving a lin-
agents‚Äô observations. In fact, the random numbers necessary ear program. Following an improvement, the controller can
for its operation could be determined prior to execution time. be reevaluated through the solution of a set of linear equa-
  Formally, a correlation device is a tuple hC, œài, where C tions. Below, we describe how a bounded backup works, and
is a set of states and œà : C ‚Üí ‚àÜC is a state transition func- prove that it always produces a new controller with value at
tion. At each step, the device undergoes a transition, and each least as high for all initial state distributions.
agent observes its state.
  We must modify the deÔ¨Ånition of a local controller to 4.1 Improving a Local Controller
take the state of the correlation device as input. Now, a We Ô¨Årst describe how to improve a local controller. To do
local controller for agent i is a conditional distribution of this, we choose an agent i, along with a node qi. Then,
              0
the form P (ai, qi|c, qi, oi). The correlation device together we search for new parameters for the conditional distribution
                                                            0
with the local controllers form a joint conditional distribu- P (ai, qi|c, qi, oi).
tion P (c0,~a, ~q 0|c, ~q, ~o). We will refer to this as a correlated The search for new parameters works as follows. We as-
joint controller. Note that a correlated joint controller with sume that the original controller will be used from the second
|C| = 1 is effectively an independent joint controller. step on, and try to replace the parameters for qi with better
  The value function for a correlated joint controller can be ones for just the Ô¨Årst step. In other words, we look for the
computed by solving the following system of linear equa- best parameters satisfying the following inequality:
tions, one for each s ‚àà S, ~q ‚àà Q~ , and c ‚àà C:                        X
                                                         V (s, ~q, c) ‚â§   P (~a|c, ~q)[R(s, a) +
                X
   V (s, ~q, c) =   P (~a|c, ~q)[R(s,~a) +                              ~a
                                                                          X
                 ~a                                                    Œ≥       P (~q 0|c, ~q,~a, ~o)P (s0, ~o|s,~a)
                    X
                Œ≥        P (s0, ~o|s,~a)P (~q 0|c, ~q,~a, ~o)           s0,~o,~q 0,c0
                  s0,~o,~q 0,c0                                                ¬∑ P (c0|c)V (s0, ~q 0, c)]
                         ¬∑ P (c0|c)V (s0, ~q 0, c0)].
                                                      for all s ‚àà S, q‚àíi ‚àà Q‚àíi, and c ‚àà C. Note that the inequality
We sometimes refer to the value of the controller for an initial is always satisÔ¨Åed by the original parameters. However, it is
state distribution. For a distribution Œ¥, this is deÔ¨Åned as often possible to get an improvement.
                                                        Finding new parameters can be done using linear program-
                       X
            V (Œ¥) = max   Œ¥(s)V (s, ~q, c).           ming, as shown in Table 1. We note that this linear program is
                    ~q,c
                        s                             the same as that of Poupart and Boutilier [2003] for POMDPs,
                                                      with the nodes of the other local controllers and correlation
It is assumed that, given an initial state distribution, the con-
                                                      device considered part of the hidden state. Its size is polyno-
troller is started in the joint node which maximizes value from
                                                      mial in the sizes of the DEC-POMDP and the joint controller,
that distribution.
                                                      but exponential in the number of agents.
4  Bounded Policy Iteration                           4.2  Improving the Correlation Device
We now describe our bounded policy iteration algorithm for The procedure for improving the correlation device is very
improving correlated joint controllers. To improve a corre- similar to the procedure for improving a local controller. We Variables: , x(c0)
 Objective: Maximize 
 Improvement constraints:
                               X                      X
        ‚àÄs, ~q V (s, ~q, c) +  ‚â§ P (~a|c, ~q)[R(s,~a) + Œ≥  P (~q 0|c, ~q,~a, ~o)P (s0, ~o|s,~a)x(c0)V (s0, ~q 0, c0)]
                                ~a                   s0,~o,~q 0,c0
 Probability constraints:
                                          X
                                     ‚àÄc0     x(c0) = 1, ‚àÄc0 x(c0) ‚â• 0
                                          c0

Table 2: The linear program used to Ô¨Ånd new parameters for the correlation device node c. The variable x(c0) represents
P (c0|c).

                                                        k+1        k                        k
Ô¨Årst choose a device node c, and consider changing its param- Tn (Vo) ‚â• Tn (Vo). Since Vn = limk‚Üí‚àû Tn (Vo), we have
eters for just the Ô¨Årst step. We look for the best parameters that Vn ‚â• Vo. Thus, the value of the new controller is higher
satisfying the following inequality:                  than that of the original controller for all possible initial state
                 X                                    distributions.
   V (s, ~q, c) ‚â§   P (~a|c, ~q)[R(s, a) +              The argument for changing nodes of the correlation device
                  ~a                                  is almost identical to the one given above. 2
                    X
                 Œ≥      P (~q 0|c, ~q,~a, ~o)P (s0, ~o|s,~a)
                                                      4.4  Local Optima
                  s0,~o,~q 0,c
                                                      Although bounded backups give nondecreasing values for all
                         ¬∑ P (c0|c)V (s0, ~q 0, c0)]
                                                      initial state distributions, convergence to optimality is not
for all s ‚àà S and ~q ‚àà Q~ .                           guaranteed. There are a couple of factors contributing to this.
  As in the previous case, the search for parameters can First is the fact that only one local controller, or the corre-
be formulated as a linear program. This is shown in Table lation device, is improved at once. Thus, it is possible for
2. This linear program is also polynomial in the sizes of the algorithm to get stuck in a suboptimal Nash equilibrium
the DEC-POMDP and joint controller, but exponential in the in which each of the controllers and the correlation device
number of agents.                                     is optimal with the others held Ô¨Åxed. It is an open problem
                                                      whether there is a linear program for updating more than one
4.3  Monotonic Improvement                            controller at a time.
                                                        Of course, a bounded backup does not Ô¨Ånd the optimal pa-
We have the following theorem, which says that performing rameters for one controller with the others held Ô¨Åxed. Thus,
either of the two updates cannot lead to a decrease in value a sequence of such updates may converge to a local optimum
for any initial state distribution.                   without even reaching a Nash equilibrium. For POMDPs,
Theorem 1 Performing a bounded backup on a local con- Poupart and Boutilier [2003] provide a characterization of
troller or the correlation device produces a correlated joint these local optima, and a heuristic for escaping from them.
controller with value at least as high for every initial state This could be applied in our case, but it would not address
distribution.                                         the suboptimal Nash equilibrium problem.

Proof. Consider the case in which some node qi of agent
i‚Äôs local controller is changed. Let Vo be the value function 5 Experiments
for the original controller, and let Vn be the value function We implemented bounded policy iteration and tested it on two
for the new controller. Recall that the new parameters for different problems, an idealized networking scenario and a
      0
P (ai, qi|c, qi, oi) must satisfy the following inequality for all problem of navigating on a grid. Below, we describe our ex-
s ‚àà S, q‚àíi ‚àà Q‚àíi, and c ‚àà C:                          perimental methodology, the speciÔ¨Åcs of the problems, and
                 X                                    our results.
  Vo(s, ~q, c) ‚â§    P (~a|c, ~q)[R(s, a) +
                  ~a                                  5.1  Experimental Setup
                    X
                 Œ≥       P (~q 0|c, ~q,~a, ~o)P (s0, ~o|s,~a) Although our algorithm guarantees nondecreasing value for
                  s0,~o,~q 0,c0                       all initial state distributions, we chose a speciÔ¨Åc distribution
                         ¬∑ P (c0|c)V (s0, ~q 0, c)]   to focus on for each problem. Experiments with different dis-
                                 o                    tributions yielded qualitatively similar results.
Notice that the formula on the right is the Bellman opera- We deÔ¨Åne a trial run of the algorithm as follows. At the
tor for the new controller, applied to the old value function. start of a trial run, a size is chosen for each of the local con-
Denoting this operator Tn, the system of inequalities implies trollers and the correlation device. The action selection and
that TnVo ‚â• Vo. By monotonicity, we have that for all k ‚â• 0, transition functions are initialized to be deterministic, with   10.0                                                  4.5

   9.0                                                   4.0

   8.0
                                                         3.5
   7.0
                                                         3.0
   6.0
                                                         2.5
   5.0                            Independent                                            Independent
                                                        Value
  Value                                                  2.0                             Correlated
   4.0                            Correlated
                                                         1.5
   3.0
                                                         1.0
   2.0

   1.0                                                   0.5

   0.0                                                   0.0
         1     2     3     4     5     6      7                1     2     3     4      5     6     7
                   Size of Local Controllers                             Size of Local Controllers

                         (a)                                                    (b)

Figure 2: Average value per trial run plotted against the size of the local controllers, for (a) the multi-access broadcast channel
problem, and (b) the robot navigation problem. The solid line represents independent controllers (a correlation device with one
node), and the dotted line represents a joint controller including a two-node correlation device.

the outcomes drawn according to a uniform distribution. A the buffer for agent 1 containing a message and the buffer for
step consists of choosing a node uniformly at random from agent 2 being empty.
the correlation device or one of the local controllers, and per-
forming a bounded backup on that node. After 50 steps, the 5.3 Meeting on a Grid
run is considered over. In practice, we found that values usu- In this problem, we have two robots navigating on a two-
ally stabilized within 15 steps.                      by-two grid with no obstacles. Each robot can only sense
  We varied the sizes of the local controllers from 1 to 7 (the whether there are walls to its left or right, and the goal is
agents‚Äô controllers were always the same sizes as each other), for the robots to spend as much time as possible on the same
and we varied the size of the correlation device from 1 to 2. square. The actions are to move up, down, left, or right, or
Thus, the number of joint nodes ranged from 1 to 98. Memory to stay on the same square. When a robot attempts to move
limitations prevented us from using larger controllers. For to an open square, it only goes in the intended direction with
each combination of sizes, we performed 20 trial runs. We probability 0.6, otherwise it either goes in another direction
recorded the highest value obtained across all runs, as well as or stays in the same square. Any move into a wall results in
the average value over all runs.                      staying in the same square. The robots do not interfere with
                                                      each other and cannot sense each other.
5.2  Multi-Access Broadcast Channel                     This problem has 16 states, since each robot can be in any
Our Ô¨Årst domain is an idealized model of control of a multi- of 4 squares at any time. Each robot has 4 observations, since
access broadcast channel [Ooi and Wornell, 1996]. In this it has a bit for sensing a wall to its left or right. The total
problem, nodes need to broadcast messages to each other over number of actions for each agent is 5. The reward is 1 when
a channel. Only one node may broadcast at a time, otherwise the agents share a square, and 0 otherwise, and the discount
a collision occurs. The nodes share the common goal of max- factor is 0.9. The initial state distribution is deterministic,
imizing the throughput of the channel.                placing both robots in the upper left corner of the grid.
  At the start of each time step, each node decides whether or
not to send a message. The nodes receive a reward of 1 when 5.4 Results
a message is successfully broadcast and a reward of 0 other- For each combination of controller sizes, we looked at the
wise. At the end of the time step, each node observes its own best solutions found across all trial runs. The values for these
buffer, and whether the previous step contained a collision, a solutions were the same for all controller sizes except for the
successful broadcast, or nothing attempted.           few smallest.
  The message buffer for each agent has space for only one It was more instructive to compare average values over all
message. If a node is unable to broadcast a message, the mes- trial runs. Figure 2 shows graphs of average values plotted
sage remains in the buffer for the next time step. If a node i against controller size. We found that, for the most part, the
is able to send its message, the probability that its buffer will average value increases when we increase the size of the cor-
Ô¨Åll up on the next step is pi. Our problem has two nodes, relation device from one node to two nodes (essentially mov-
with p1 = 0.9 and p2 = 0.1. There are 4 states, 2 actions ing from independent to correlated).
per agent, and 5 observations per agent. The discount fac- For small controllers, the average value tends to increase
tor is 0.9. The start state distribution is deterministic, with with controller size. However, as the controllers get larger,