                        Dynamics of Temporal Difference Learning

                                         Andreas Wendemuth
                      Otto-von-Guericke-University, 39016 Magdeburg, Germany
                                       Cognitive Systems Group
                           andreas.wendemuth@e-technik.uni-magdeburg.de


Abstract                                              Smart 2004]. We adopt the following notation in the course
                                                      of this paper, following [Dayan and Abbott 2001]:
In behavioural sciences, the problem that a sequence of stim-
                                                        ‚Ä¢
uli is followed by a sequence of rewards r(t) is considered. Stimulus u(t)
The subject is to learn the full sequence of rewards from the ‚Ä¢ Future reward r(t)
stimuli, where the prediction is modelled by the Sutton-Barto ‚Ä¢ Sum of future rewards R(t)
rule. In a sequence of n trials, this prediction rule is learned it- ‚Ä¢
eratively by temporal difference learning. We present a closed Weights w(k)
formula of the prediction of rewards at trial time t within trial ‚Ä¢ Predicted reward v(t)
n. From that formula, we show directly that for n ‚Üí‚àûthe We want to compute, for all trials n of duration T ,andfor
predictions converge to the real rewards. In this approach, a any time of trial t, the predicted reward vn(t). The (ex-
new quality of correlation type Toeplitz matrices is proven. tended) stimulus u(t) is given at times tu,min ...tu,max ,the
We give learning rates which optimally speed up the learning (extended) reward r(t) is presented at times tr,min ...tr,max.
process.                                              Stimulus and reward do not overlap, i.e. tr,min >tu,max.
                                                        The subject is to learn the total remaining reward at time t,
                                                              T‚àít
1  Temporal Difference Learning                       R(t)=       r(t + œÑ) (only after stimulus onset!)
We consider here a mathematical treatment of a problem in     œÑ=0
                                                        The brackets  refer, in general, to stochastic values of
behavioural biology. This problem has been described e.g. in
                                                      R(t), if not exactly the same rewards are given at each trial,
[Dayan and Abbott 2001] as learning to predict a reward. It is
                                                      but when there are Ô¨Çuctuations.
Pavlovian in the sense that classical conditioning is adressed,
                                                        All previous stimuli u(t) are weighted to give a linear pre-
however reward is not immediate, but after a series of stimuli
                                                      diction model [Sutton and Barto 1990]:
there is a latency time, followed by a series of rewards. Us-
                                                                            t
ing a simple linear rule, we will show that the subject is able
                                                                                      ‚àí               (1)
to predict the remaining rewards by that rule, repeating the         v(t)=     w(œÑ)u(t  œÑ)
                                                                            œÑ=0
same stimuli-reward pattern over a series of trials. A review
of learning theory related to these problems can be found in An update rule for the Sutton-Barto-formula can easily be de-
                                                      rived from the mean square error [Dayan and Abbott 2001]:
[Sutton and Barto 1998].                                                                           
                                                                         T‚àít         t              2
  There are recent experimental biological correlates with          2
this mathematical model, e.g. in the activity of primate R(t) ‚àí v(t) =     r(t + œÑ) ‚àí   w(œÑ)u(t‚àíœÑ)
dopamine cells during appetitive conditioning tasks, together            œÑ=0          œÑ=0
with the psychological and pharmacological rationale for                                              (2)
studying these cells. A review was given in [Schultz 1998], using the partial derivative with respect to any weight w(Œ±),
                                                                         2
the connection to temporal difference learning can be found ‚àÇR(t) ‚àí v(t)
in [Montague et al. 1996].
                                                                ‚àÇw(Œ±)
  In this paper, the focus is on two mathematical issues: a) to                              
                                                                   T ‚àít          t
provide a direct constructive proof of convergence by giving                   
                                                              ‚àí              ‚àí            ‚àí        ‚àí
an explicit dependence of the prediction error over trials, b) to = 2 r(t + œÑ)     w(œÑ)u(t  œÑ)  u(t  Œ±)
minimize the learning time by giving a formula for optimal         œÑ=0          œÑ=0
setting of the learning rate. Hence, the paper contributes as Updates are made in discrete steps in the direction of the neg-
well to temporal difference learning as a purely mathematical ative gradient, providing the following rule:
issue, which may be valuable also without reference to be- Œîw(œÑ)=ŒµŒ¥(t)u(t   ‚àí œÑ)                      (3)
                                                                                
havioural biology and reinforcement learning. It can also be          T‚àít
understood in a Dynamic Programming sense, see [Watkins    Œ¥(t)=         r(t + œÑ)  ‚àí v(t)=R(t)‚àív(t)
1989] and later work, e.g. [Gordon 2001] and [Szepesvari and          œÑ=0

                                                IJCAI-07
                                                  1107            Obviously, the total future reward R(t) is not known to the This can be written in matrix notation as follows, with E the
            subject at time t. We can use R(t)=r(t)+R(t +1).Inthis unit matrix:
            formulation, again R(t +1)is not known. The subject can      ‚éõ   n+1            ‚éû
                                                                            v   (tu,max)
            approximate the value by his prediction v(t +1). This will   ‚éú   n+1            ‚éü
                                                                         ‚éú  v   (tu,max +1) ‚éü
            provide the so-called                                        ‚éú  .               ‚éü
              Temporal difference rule at time t:                        ‚éú  .               ‚éü  =(E  ‚àí ŒµGA)  √ó
                                                                         ‚éù  .               ‚é†
                     w             u   ‚àí      ‚àÄ
                   Œî   (œÑ)=ŒµŒ¥(t)     (t  œÑ) ,   œÑ =0...t                     n+1
                                             ‚àí                              v   (tr,max)
                      Œ¥(t)=r(t)+v(t     +1)    v(t)         (4)            ‚éõ                ‚éû       ‚éõ           ‚éû
                                                                               n                       0
            The updates can be made sequentially, after each time step t,     v (tu,max)
                                                                           ‚éú   n            ‚éü       ‚éú  .        ‚éü
            or in parallel, sampling the weight updates for all times in a ‚éú  v (tu,max +1) ‚éü       ‚éú  .        ‚éü
                                                                           ‚éú  .             ‚éü       ‚éú           ‚éü
            given trial, and then updating at the end of the trial. The two √ó ‚éú .           ‚éü +  ŒµG ‚éú  r(tr,min) ‚éü
                                                                           ‚éù  .             ‚é†       ‚éú           ‚éü
            alternatives do not substantially differ in the Ô¨Ånal result after                       ‚éù  .        ‚é†
                                                                               n                       .
            many trials. For computational reasons, we use the parallel       v (tr,max)
            version of the update rule. Other learning schedules may be                                r(tr,max)
            adopted, however we use eq. (4) since this is widely accepted where we shall now use the total time T = tr,max‚àítu,max+1:
            in the community, following [Dayan and Abbott 2001].Let
            us denote the trial number by superscripts n.                      tu,max‚àíy
                                                                        g(y)=         u(k) u(y + k) , ‚àÄ y =0...k  (8)
            2  Dynamics of temporal difference learning                        k=tu,min

            The parallel temporal difference rule eq. (4) was obtained, with any real values u(k) and u(tu,max) =0 ,andT √ó T -
            using an approximation to gradient descent. Therefore, con- matrices
            vergence is not guaranteed and shall be proven in the course
                                                                               ‚éõ            ¬∑¬∑¬∑          0 ‚éû
            of this paper, where a compact notation will be introduced.          g0     g1       gk
                                                                               ‚éú                     .     ‚éü
            We proceed as follows:                                             ‚éú                 ¬∑¬∑¬∑  ..   ‚éü
                                                                               ‚éú g1     g0  g1          gk ‚éü
              ‚Ä¢ Incorporate the rule into the prediction formula, yielding     ‚éú .      .   .    .       . ‚éü
                                                 n                        G =  ‚éú .       .   .    .      . ‚éü  and
                a recursive formula for all predictions v (t) at trial n.      ‚éú .        .   .   .      . ‚éü
                                                                               ‚éù gk     ¬∑¬∑¬∑ g1   g0     g1 ‚é†
              ‚Ä¢ Make this a closed formula vn(t).
                                                                                      ..
              ‚Ä¢                  n   ‚Üí                                           0     . gk ¬∑¬∑¬∑  g1     g0
                Show convergence v (t)  R(t) for large N.                      ‚éõ                      ‚éû
                                                                                     ‚àí
              ‚Ä¢ Choose an optimal learning rate Œµ for maximally fast             1    1
                                                                               ‚éú         ‚àí            ‚éü
                convergence.                                                   ‚éú     1     1          ‚éü
                                      n+1                                      ‚éú         .    .       ‚éü
            We obtain for the predictions v (t) at trial n +1the fol-     A =  ‚éú          ..   ..     ‚éü           (9)
                                                                               ‚éù                      ‚é†
            lowing recursive formula. Summation limits explicitely state                      1   ‚àí1
            situations where stimuli and reward phases may overlap, we                            1
            will restrict to non-overlapping cases later.

              n+1      n           min(t,tu,max) n               where the ‚Äù0‚ÄùinG  refers to the full remaining upper and
             v   (t) ‚àí v (t) t‚â•tu,min        Œîw   (t ‚àí k)u(k)
                   Œµ                                Œµ             lower triangle, respectively, and in A all entries except for
                                     k=tu,min                     the two diagonals are 0. We can further write this in compact
               min(t,tu,max) min(tr,max‚àít+k,tu,max)
                                                 n                notation (vectors have component index t):
             =           u(k)                   Œ¥ (x + t ‚àí k)u(x)
                                                                               n+1               n
                 k=tu,min          x=tu,min                                  v    =(E  ‚àí  ŒµGA)  v  + ŒµGr
               min(tr,max‚àít,tu,max‚àítu,min)
                                      n
             =                       Œ¥ (t + y) g(t, y)            With v0 = 0 one has
                 y=t   ‚àímin(t,t   )
                    u,min     u,max                                                 N
                                                            (5)            vN+1         E ‚àí  GA   n Gr
            where Œ¥n(t)=r(t)+vn(t  +1)‚àí  vn(t) and                              =  Œµ   (    Œµ    )
                                                                                    n=0
                       min(min(t,tu,max),tu,max‚àíy)                                 ‚àí1                (N+1)
                                                                           = Œµ(ŒµGA)   (E ‚àí (E ‚àí ŒµGA)       )Gr
               g(t, y)=                      u(k)u(y + k)   (6)                                              
                                                                               ‚àí1        ‚àí1            (N+1)
                         k=max(tu,min,tu,min‚àíy)                            = A    E  ‚àí G    (E ‚àí ŒµGA)       G  r (10)

            Since we are interested whether the subject will make proper Only if (‚àíGA) is a Hurwitz (stability) matrix (all eigenval-
            predictions after all stimuli have been given, we will restrict ues have negative real part), a suitable Œµ can be chosen such
            our analysis to times t>tu,max.Theng(t, y) will become               (N+1)
                                                                  that (E ‚àí ŒµGA)       will vanish with large N. A similar
            independent of t,giving
                                                                  approach, relying on Hurwitz matrices, was followed in
                             min(tu,max,tu,max‚àíy)                [Sutton 1988], however there the Hurwitz property was not
             g(y)=g(t, y)=                     u(k)u(y + k) (7)   shown immediately on the matrix structure of eq. (9). Our

                            k=max(tu,min,tu,min‚àíy)                aim here is to provide an explicit proof, which will establish

                                                            IJCAI-07
                                                              1108as an interesting result in its own right a general property of hence xTGx can be written as
Toeplitz matrices with correlation entries.               N k            N  k k‚àíj
                                                                    2 2
                                                        ‚àí       u(t) xi +2            u(t) u(t + j) xi xi+j
  We will show the Hurwitz property later, and we will give i=0 t=0         i=0 j=0 t=0
values for Œµ for which convergence is assured and for which it                                       (13)
is optimal. Continuing with the large N-behaviour, we obtain
     N‚Üí‚àû                                              Rearranging the inner sums with
vN+1  ‚àí‚Üí   A‚àí1 r . This holds no matter what the stimulus!
                                                                        k k‚àíj  k k‚àít
With              ‚éõ                  ‚éû
                    11111                                                      =                     (14)
                  ‚éú                  ‚éü                                  j=0 t=0  t=0 j=0
                  ‚éú     1111‚éü
             ‚àí1   ‚éú        .   .   . ‚éü
           A   =  ‚éú         ..  .. . ‚éü                and using vectors u of dimension (k +1)with components
                  ‚éù                . ‚é†
                               11                     ut = u(t) leads to
                                   1                                N  k            N
                                                           T                   2 2         T Àú (i)
we obtain                                                 x  Gx = ‚àí        u(t) xi +2     u X   u    (15)
                    T ‚àít
                                                                    i=0 t=0          i=0
              ‚àû
            v  (t)=     r(t + œÑ)=R(t)                                  Àú (i)
                    œÑ=0                               where the matrices X are (k +1)√ó (k +1)band matrices
                                                                 Àú (i)
which is the desired result. This proves convergence in full with entries Xm,m+n = xixi+n for all m and n ‚â• 0,and0
generality. We need to show that (‚àíGA) is a Hurwitz ma- otherwise. Symmetrizing the matrix products, we can write
                                                                                                  
trix, i.e. that all eigenvalues of that matrix have negative real N       N               N
                                                             T  (i)     T       (i)      T       (i) T
part. Further, we want to give values for Œµ which provide 2 u  XÀú u = u       XÀú   u + u       (XÀú )   u
maximum speed of convergence.                            i=0               i=0              i=0
                                                                                                     (16)
3  Proof of convergence                               The matrix elements of the second term are, for ‚àík ‚â§ n ‚â§ 0:
                                                                  
In previous approaches in the literature, temporal difference N             N           N
                                                              Àú (i) T
(TD) learning was extended to TD(Œª) learning where the pa-   (X  )         =    xjxj‚àín =     xixi+n  (17)
rameter Œª refers to an exponential weighting with recency. i=0      m,m+n    j=0          i=0
Learning and convergence issues were considered e.g. in
[Dayan 1992] and [Dayan and Sejnowski 1994].IntheTD(Œª) where the summation limits in the last result were taken, for
framework, what we consider here is TD(0)-learning where convenience, larger than necessary for covering all cases of
in contrast to the mentioned literature we concentrate on a di- n for which the summand is nonzero (note that n is nonposi-
rect proof of convergence by establishing a general property tive), the extra terms being zero due to the zero padding con-
of Toeplitz matrices with correlation entries.        vention for the xi. As a result, the matrix elements of the
  The proof proceeds as follows: we will show Ô¨Årst that G Ô¨Årst and the second term in eq. (16) have identical format, for
is positive deÔ¨Ånite. Then, we will show that all eigenvalues nonnegative and nonpositive n, respectively.
of (GA) have positive real part. Hence, we will have estab- Making use of eq. (16) and eq. (17), and incorporating the
                                                      Ô¨Årst sum in eq. (15) leads to
lished the Hurwitz property required for convergence.                                  
  In order to see that G is positive deÔ¨Ånite, we consider any                    N
                              T                                      T        T       (i)
nonzero real vector x and show that x Gx is always positive.        x  Gx =  u      XÀÜ    u          (18)
We Ô¨Årst extract from G the diagonal matrix GD and the upper                      i=0
triangular matrix G+.Thenwehave
                                                      where the matrices XÀÜ (i) are band matrices with entries
    xTGx       xTG   x   xTG   x   xTGTx              ÀÜ (i)
            =       D  +      +  +      +             Xm,m+n  =  xixi+n for positive and negative n.
                 T         T         T  T   T
               x  GDx    x  G+x     x G   x             We can now write the sum of matrices in eq. (18) in a dif-
            =          +         +(     +  )                                   (i)
                 T          T                         ferent way. Denote vectors xÀú of dimension (k +1)with
            =  x  GDx  +2x   G+x                                  (i)
                                                      components xÀúm = xi+m, m =0...k, where, as before, it is
               N            N    k
                        2                             understood that xj =0for j<0 and j>N. Hence, these
            =      g(0) xi +2    xi   g(j) xi+j (11)  vectors will have nonzero components only for i = ‚àík...N.
                i=0          i=0   j=1                  Notice that the following covariance matrices are generated
where in order to keep summation limits easy it is understood by these vectors:
                                                                           
that xi =0for i<0 and i>N(‚Äùzero padding‚Äù). There are                (i) (i) T
                                                                   xÀú (xÀú )       = xi+m xi+n        (19)
2k+1 Toeplitz bands, k being the number of stimuli. Shifting                 (m,n)
the time index in eq. (8) from tu,min to 0, g(j) arises from the
k stimuli for u(k) =0 ,u(t)realas                    Then we have for (m =0...k,n=  ‚àím...k‚àí   m):
                                                                        
         k‚àí|j|                                             N                    N+m
                                                                xÀú(i) xÀú(i) T
   g(j)=      u(t) u(t + |j|) , |j| =0...k ,‚àÄ t (12)               (   )       =        xi xi+n‚àím    (20)
          t=0                                              i=‚àík           (m,n)  i=‚àík+m

                                                IJCAI-07
                                                  1109and                                                     Let y be any eigenvector of (GA) and Œª be the corre-
                                                                             GA           y
     N                        N+m                   sponding eigenvalue. Since (  ) is real, and Œª are ei-
          (i) (i) T                                   ther both real or both complex. In the complex case, there
         xÀú (xÀú )          =         xi xi+n   (21)                           ‚àó
                                                      exists a further eigenvector y and corresponding eigenvalue
     i=‚àík                     i=‚àík+m
                    (m,m+n)                           Œª‚àó,where()‚àó denotes complex conjugated and transposed.
                                                                    z   Ay
which establishes that                                  Let us denote =     and write
                                                                 z‚àóGz    y‚àóA‚àóGAy       y‚àóA‚àóy
               N              N                                        =            = Œª              (26)
                                                                                              ‚àó
                  xÀú(i) xÀú(i) T  XÀÜ (i)               Also, with G real and symmetric, we have G = G and
                     (   )  =                  (22)                 ‚àó       ‚àó    ‚àó    ‚àó ‚àó
             i=‚àík             i=0                                  z Gz =(z  Gz)   = Œª y Ay          (27)
                                                      The summation of eqns. 26 and 27 gives
Then we can rewrite eq. (18), using eq. (22):                        ‚àó       ‚àó  ‚àó        ‚àó
                                                                   2z Gz  = y [Œª A +  ŒªA  ] y        (28)
            N                    N
                                                    With G real, symmetric and positive deÔ¨Ånite, we have for any
   xTGx         uTxÀú(i) xÀú(i) T u   |uTxÀú(i)|2                      ‚àó
         =           (   )   =                 (23)   complex z that z Gz is real and positive. This can be used as
           i=‚àík                 i=‚àík                  follows.
                                                        Case 1: y and Œª are both real.
This sum is certainly nonnegative. We will now show that it                A
cannot become zero.                                   Then we have, with real , from eq. (28) immediately
                                                                         ‚àó         T   T
                                                                0 < Re(2z Gz)=Œªy     A   + A  y      (29)
  The proof is by contradiction. Suppose the sum is zero. hence                               
                                                                                     T   T
Then all individual terms must be zero. Start with the Ô¨Årst sign(Re(Œª)) = sign(Œª)=sign(y A + A  y)   (30)
term. We have from the deÔ¨Ånition
                                                        Case 2: y and Œª are both complex.
                     k                               Let us write y = v+iw and Œª = g +ih. Then from eq. (28),
           uTxÀú(‚àík)                                           A
                   =     ujx‚àík+j = ukx0        (24)   with real , we obtain              
                                                                     ‚àó         T    T
                     j=0                                 0  <   Re(2z Gz)=g   v   A   + A  v         (31)
                                                                                                
                                                                     T   T               T    T
This becomes zero, since uk =0 (eq. (12)), only for x0 =0.     +g w   A   + A  w ‚àí 2 h w   A  ‚àí A  v
                                                                                         
Now proceed with the second term:                                    ‚àó          T   T
                                                         0=Im(2z      Gz)=h   v   A   + A  v         (32)
                                                                                                
             k                                                      T   T               T    T
  T  (‚àík+1)                                                     +h w   A   + A  w +2g  w    A  ‚àí A  v
 u xÀú      =     ujx‚àík+1+j = uk‚àí1x0 + ukx1 = ukx1
             j=0                                      Combining eqns. 31 and 32 gives            
                                                               2   2    T   T            T   T
                                               (25)     0 <g(g  + h ) v   A   + A  v + w   A   + A  w
where the previous result x0 =0was used. This becomes                                                (33)
            
zero, since uk =0(eq. (12)), only for x1 =0. Continuing in from which follows with sign(Re( Œª)) = sign(g):  
                                                                          T   T            T   T
this vein, after N steps, we end with the product     sign(Re(Œª)) = sign v  A   + A  v + w   A   + A  w
                    k                                                                               (34)
        T  (‚àík+N)
       u xÀú       =    ujx‚àík+N+j                      The results for both cases, eqns. 30 and 34, allow the
                    j=0                               following sufÔ¨Åcient condition:
                                                      Let G  be a real, symmetric and positive deÔ¨Ånite matrix,
       = u0xN‚àík + ...+ uk‚àí1xN‚àí1  + ukxN  = ukxN                                                 T
                                                      and A be a real square matrix. If the matrix A + A is
where all the previous results were used. Hence xN =0, positive deÔ¨Ånite, then the real parts of the eigenvalues of
which in total means that x = 0. This is a contradiction, (GA) are positive.
since x must be a nonzero vector.
  With this result, we have established that xTGx > 0,  Let us now turn our attention to the speciÔ¨Åc matrix A given
hence we have established:                            in eq. (9). We have‚éõ                     ‚éû
  Symmetric Toeplitz matrices of the structure of G after               2    ‚àí1
                                                                     ‚éú  ‚àí         ‚àí            ‚éü
eq. (9), with correlation entries gj after eq. (12), are positive    ‚éú    12       1           ‚éü
deÔ¨Ånite.                                                     T       ‚éú       .    .   .        ‚éü
                                                           A   + A = ‚éú        ..  ..   ..      ‚éü
                                                                     ‚éù                         ‚é†     (35)
                                                                                  ‚àí12      ‚àí1
  We now turn our attention to showing that all eigenvalues                           ‚àí
of (GA)  have positive real part. We will look at any real                              12
A and any G which is real, symmetric and positive deÔ¨Ånite.
(We will not require the stronger condition that the structure It can be shown that the matrix Q = AT + A is positive def-
of G is after eq. (9), and we say nothing about the entries gj.) inite, by using the previous result of this paper: matrices of
  Under these premises, the proof will Ô¨Årst give a sufÔ¨Åcient the structure of G are positive deÔ¨Ånite. Noting that Q has the
condition for the sign of the real part of the eigenvalues of structure of G, using eq. (12) with k =1and u =(1,-1),im-
(GA). Then we will show that this sign is indeed positive mediately gives the desired result. This completes our proof
for the A at hand.                                    that (‚àíGA)  is a Hurwitz matrix.

                                                IJCAI-07
                                                  11104LearningrateŒµ       for fast convergence             along curves m(Œµ(k)), ensuring the condition that our chosen
The convergence behaviour of the temporal difference rule is k satisÔ¨Åes maxk m(Œµ(k)).
                                                        At Œµ =0,allm(Œµ(k)) =   1 and decreasing with Œµ.The
known from eq. (10). For large number of trials, convergence                                         ‚àó
                                        E ‚àí  GA       slope is given by ‚àí2gk < 0. We start by choosing the k for
speed will be dominated by the eigenvalue of ( Œµ  )          ‚àó
                                                      which k  =argmink   gk, which ensures that the condition
with the largest modulus, for which convergence speed is                                     ‚àó
slowest. We have for the eigenvalues (ev):            maxk  m(Œµ(k)) is satisÔ¨Åed by our choice of k . Our current
                                                      position is Œµc =0. Then we apply the following procedure:
 ev(E ‚àí ŒµGA)=1‚àí     Œµ ev(GA)=1‚àí     Œµ (g + ih) (36)     [START]
                                                                         ‚àó
where the same notation for the eigenvalues of GAas in the Continuing on curve k , we would reach the minimum for
                                                                         ‚àó       gk‚àó
previous section, Œª = g + ih, was used.                                 Œµk‚àó =  2     2               (41)
  Hence, the Ô¨Årst task to do is to compute the eigenvalues of                 gk‚àó + hk‚àó
GA
    which are only dependent on the given stimuli u(k) and if everywhere between our current position Œµc and the pro-
                         GA                                         ‚àó
on the total time T . Note that deÔ¨Ånes the special struc- posed position Œµk‚àó the maximum condition maxk m(Œµ(k)) is
ture of an unsymmetric Toeplitz matrix with 2k +2bands. satisÔ¨Åed for k‚àó. In order to check this, we compute the inter-
It is well known that a closed solution for the eigenvalues of sections of our curve under inspection k‚àó to all other curves
Toeplitz matrices exists only for not more than three bands, k.
they are given for symmetric and asymmetric cases e.g. in After eq. (37), these intersections are given at values Œµk
[Beam and Warming 1993]. Hence only for k =0can a     according to
closed solution been given, which we do in sec. 5. For k>0         2    2 2              2   2  2
                                                          (1 ‚àí Œµk gk) + Œµk hk =(1‚àí Œµk gk‚àó ) + Œµk hk‚àó (42)
numerical values must be obtained, special methods for the
solution, and for the asymptotic structure of the eigenvalues or
                                                                               gk ‚àí gk‚àó
for large T ,aregivenin[Beam and Warming 1993].                     k
                                                                   Œµ =2    2 ‚àí  2    2 ‚àí  2          (43)
  The square modulus m of the eigenvalues is given by                     gk   gk‚àó + hk  hk‚àó
                         2          2    2 2            Now we inspect the intersection which is closest to our
  m(Œµ)=|   ev(E ‚àí ŒµGA)  | =(1‚àí   Œµg)  + Œµ h    (37)
                                                      current position of Œµc,i.e.forwhichkÀÜ =argmink{ Œµk,
We will Ô¨Årst give a choice for Œµ for which convergence is Œµk >Œµc}.
                                                                ‚àó        ‚àó
always assured and which also serves as a good rough value If ŒµkÀÜ >Œµk‚àó ,thenŒµk‚àó after eq. (41) is indeed the solution
for setting Œµ without much computational effort.      (free minimum). [STOP]
                                                                                                 ‚àó
  Note Ô¨Årst that m(Œµ =0)=1for all eigenvalues. Fur-     Else, there is an intersection prior to reaching Œµk‚àó .Ifthe
ther, since (‚àí GA) is Hurwitz, g>0 for all eigenvalues, curve that is intersecting is rising at Œµ(kÀÜ), this means that
and for small Œµ>0, m(Œµ) <  1 for all eigenvalues due to we are done, since we actually have reached the condition
eq. (37). Lastly, for large Œµ, m(Œµ) will increase again beyond of eq. (40). The solution is
all bounds for all eigenvalues, since m(Œµ) ‚àù Œµ2 for large Œµ                                            
                                                                                 gk ‚àí gk‚àó
and all eigenvalues.                                    ÀÜ
                                                      Œµ(k)=mink    Œµ(k)=  2  2 ‚àí  2    2 ‚àí  2  ,Œµ(k) >Œµc
  After this sketch of the general behaviour of m(Œµ),itis                   gk  gk‚àó + hk   hk‚àó
clear that for each eigenvalue k there is a Œµ(k) =0 for which                                       (44)
m(Œµ(k)) = 1 and m(Œµ(k)) is rising with Œµ(k). Computing (bounded minimum). [STOP]
                                                                                                   ÀÜ
these values from eq. (37) for eigenvalues Œªk = gk + ihk Else, if the curve that is intersecting is falling at Œµ(k),we
gives                                                 must continue on that new curve which now satisÔ¨Åes the max-
                           2 gk                       imum condition in eq. (40). To this end, we set Œµc = Œµ(kÀÜ) and
                  Œµ(k)=   2    2               (38)    ‚àó   ÀÜ
                         gk + hk                      k  = k and continue (change in falling curves with maximum
                                                      modulus). [GO TO START]
Hence, m(Œµ) < 1 for 0 <Œµ<mink Œµ(k), and a good choice
ŒµÀú for convergent behaviour is therefore the mean value of the Fig. 1 illustrates the behaviour with free minimum, Ô¨Åg. 2
bounds,                                               with bounded minimum. Both Ô¨Ågures have a change in falling
                            gk                        curves with maximum modulus.
                 ŒµÀú =mink  2    2              (39)     This algorithm terminates after Ô¨Ånitely many steps, either
                          gk + hk
                                                      at the minimum of the curve k‚àó or at an intersection of a
Note again that whenever hk =0 , there is an additional
            ‚àí                                         falling with a rising curve. It is clear that one of the two
eigenvalue gk  ihk for which the minimum in eq. (39) is possibilities exist since all curves eventually rise with large Œµ.
identical and needs not be computed.
                                     ‚àó
  We now turn to Ô¨Ånding the optimal value Œµ for fastest con- 5 One stimulus
vergence. This is given by
             ‚àó                                        We look at the special case where only one stimulus u is
            Œµ  =argminŒµ  maxk m(Œµ(k))          (40)   present, hence k =0. Then, eq. (10) takes a particularly
                                                                        E ‚àí A
This optimization problem has no closed solution. However, simple form, where ( ) is the unit shift matrix:
                                                                         
a simple line search algorithm can be given to Ô¨Ånd the solu-       T                               
                                                          N+1           N          2 N‚àíj    2        j
tion in Ô¨Ånitely many (less than T )steps.              Av    = r ‚àí          (1 ‚àí Œµu )    Œµu  (E ‚àí A)   r
  The general idea is to start at Œµ =0and increase Œµ un-           j=0  j
til we reach Œµ‚àó. In all of this process, we select a k and go                                        (45)

                                                IJCAI-07
                                                  1111