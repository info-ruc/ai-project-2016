# 人工智能课程总结
## 1.图像
### 1.1论文阅读
 首先，阅读了两篇论文，理解了什么是Residual Network
> Deep Residual Learning for Image Recognition 

> Identity Mappings in Deep Residual Networks

![Residual Unit](http://upload-images.jianshu.io/upload_images/4029277-bc47a41b2a4cbb17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/4029277-322a0f210ccbcf95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/4029277-0cc5e333dea6c4ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/4029277-78ff49169cc14ca8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/4029277-e044835c84c6b36f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.2 Caffe的安装与使用
#### 2.2.1 安装
在安装Caffe的时候，对Ubuntu系统还很不熟悉，出现了很多不明白的错误，最后通过百度，都一一解决，几乎所有的错误都是有路径引起的，将Anaconda和Caffe的路径等加入系统变量就可以安装成功。

#### 2.2.2 使用
* 通过caffe自带的几个python的例子初步了解了caffe，在自己电脑上跑了mnist，cifar10等例子。
* 了解caffe的net.prototxt和solver.prototxt，并自己尝试通过caffe的python接口写一个简单的网络结构。
* 在写net.prototxt时对CNN的原理不懂，开始学习CNN，它的两个主要特点是局部感知与权值共享。

### 2.3 ResNet实验

由于自己写的ResNet效果差，于是找到一个ResNet-20的网络进行实验。在cifar10上得到了接近80%的正确率，在BoT上得到的争取率只有60%。在进行ResNet-20的实验时遇到的问题主要有以下几点：
* 在筛选图片时，一开始通过文件管理器操作，由于图片数量大，很容易卡住，于是用python写了个小程序解决的，并做了标签。
* 将图片转成lmdb时，一开始定的大小为256×256，结果电脑运行极其缓慢，然后在转lmdb时，resize=true，size=28*28，电脑才以可以接受的速度运行。

## 2 文本
在开始做文本的时候，一开始不打算用CNN了，于是想尝试一下其它的工具，例如tensorflow，cntk，matconvnet，cntk总是出莫名其妙的错误，matconvnet是基于matlab的caffe，简单直观，但是在定义网络结构的时候也不像caffe的net.prototxt那样简单直观，tensorflow只是稍微了解了一下。

### 2.1 论文阅读
只阅读了 Convolutional Neural Networks for Sentence Classification 这一篇论文。

![Model architecture with two channels for an example sentence](http://upload-images.jianshu.io/upload_images/4029277-cf9b3a6b74b91a04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 将单词转成词向量，word2vec
* CNN

### 2.2 论文真假判别
由于word2vec不易使用，并且词向量太大，每篇文章生成的向量矩阵巨大，难以以接受的时间训练得到结果，于是采取了通过统计词频，用SVM聚类的传统方法。

```
//统计词频，将文章转成向量
import nltk
import os
def wordfreq(filename):    
    file = open(filename, 'r')    
    document = file.read().lower().split("\n")    
    file.close()    

    //分词
    text = ""    
    for line in document: 
        text += line
    text = text.decode("utf8")
    sens = nltk.sent_tokenize(text)
    word = []
    for sen in sens:
        word.extend(nltk.word_tokenize(sen))
    stop = nltk.corpus.stopwords.words("english")
    word = [w for w in word if w.isalpha() and w not in stop]
    
    //得到词频，并降序
    fdict = nltk.FreqDist(word)
    fdict = sorted(fdict.iteritems(), key=lambda d: d[1], reverse=True)
    vec = []

    //取前300 不足补0
    if len(fdict) >= 300:
        for i in range(300):
            vec.append(fdict[i][1])
    else:
        for i in range(len(fdict)):
            vec.append(fdict[i][1])
        for i in range(len(fdict),300): 
            vec.append(0)
    return vec
```
得到文章的统计词频向量后，通过LibSVM进行分类。
```
from svmutil import *
inputs = //所有文章的向量
answers = //对应文章的标记
prob = svm_problem(answers,inputs)
param = svm_parameter()
m = svm_train(prob, param)
svm_save_model("paper.model",m)

//验证
p_labels, p_acc, p_vals = svm_predict(answers_test, inputs_test, m)
```

### 2.3 论文分类
通过TF-IDF计算文章的向量，然后通过knn或kmeans聚类。

```
//将每篇文章去停用词，标点，返回一行
import nltk
def doc2words(filename):
    file = open(filename, 'r')
    document = file.read().lower().split("\n")
    file.close()
    text = ""
    for line in document:
      text += line

    text = text.decode("utf8")
    sens = nltk.sent_tokenize(text)
    word = []
    for sen in sens:
      word.extend(nltk.word_tokenize(sen))

    stop = nltk.corpus.stopwords.words("english")
    word = [w for w in word if w.isalpha() and w not in stop]
    sentence = " ".join(word)
    return sentence
```

```
// 将目录下的所有文章添加到语料库中，一行为一篇文章
def addcorpus(route):
    import os
    filelist = os.listdir(route)
    corpus = []
    i = 0
    for f in filelist:
        if f.endswith(".txt"):
            corpus.append(doc2words(route+f))
        i += 1
        print i

    return corpus
```

```
//通过sklearn 计算每个单词的TF-IDF权重，返回权重矩阵
def corpus2vec(route):
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.feature_extraction.text import CountVectorizer

    corpus = addcorpus(route)
    vectorizer = CountVectorizer()
    transformer = TfidfTransformer()
    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
    weight = tfidf.toarray()
    return weight
```

下一步进行聚类。由于没有带标记的论文，所以还没有分类。


### 2.4 论文分类
在sciencedirect上找了三类论文分别为physics，health，finance，加上原有的ijcai论文，总共四类 ，由于下载论文速度慢，pdf处理慢，因此每一类只选取了100篇作为训练，20篇进行测试。

* 首先将pdf处理成txt
* 然后将所有文章放进一个语料库中
* 通过sklearn的tfidf模型生成每篇文章的向量
* 最后通过sklearn的svm进行分类并验证。

pdf2txt.py
```
import os

def pdftotxt(route):
    os.chdir("D:/Program Files/pdfminer/tools")
    fieldlist = os.listdir(route)
    i = 1
    for field in fieldlist:
        paperlist = os.listdir(route+field)
        for paper in paperlist:
            paper_route = route+field+"/"+paper
            save_route = route+field+"/"+paper[:-4]+".txt "
            command = "python pdf2txt.py -o "+save_route+paper_route
            os.system(command)
            print i
            i += 1

TRAIN_ROUTE = "D:/Documents/2016-2017_autumn_semester/Artificial_intelligence/NLP/PAPERSET/train/"
TEST_ROUTE = "D:/Documents/2016-2017_autumn_semester/Artificial_intelligence/NLP/PAPERSET/test/"

pdftotxt(TRAIN_ROUTE)
pdftotxt(TEST_ROUTE)

```

doc2vec.py
```
import nltk
import os

def doc2words(filename):
    file = open(filename, 'r')
    document = file.read().lower().split("\n")
    file.close()
    text = ""
    for line in document:
      text += line

    text = text.decode("utf8")
    sens = nltk.sent_tokenize(text)
    word = []
    for sen in sens:
      word.extend(nltk.word_tokenize(sen))

    stop = nltk.corpus.stopwords.words("english")
    word = [w for w in word if w.isalpha() and w not in stop]
    sentence = " ".join(word)
    return sentence


def addcorpus(route):
    import os
    filelist = os.listdir(route)
    corpus = []
    i = 0
    for f in filelist:
        if f.endswith(".txt"):
            corpus.append(doc2words(route+f))
        i += 1
        print i

    return corpus


def corpus2vec(corpus):
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.feature_extraction.text import CountVectorizer

    vectorizer = CountVectorizer()
    transformer = TfidfTransformer()
    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
    weight = tfidf.toarray()
    return weight

TRAIN_ROUTE = "D:/Documents/2016-2017_autumn_semester/Artificial_intelligence/NLP/PAPERSET/train/"
TEST_ROUTE = "D:/Documents/2016-2017_autumn_semester/Artificial_intelligence/NLP/PAPERSET/test/"
corpus = []

items = os.listdir(TRAIN_ROUTE)
for item in items:
    corpus.extend(addcorpus(TRAIN_ROUTE+item+"/"))

items = os.listdir(TEST_ROUTE)
for item in items:
    corpus.extend(addcorpus(TEST_ROUTE+item+"/"))

weight = corpus2vec(corpus)
```

通过svm方法进行训练，使用的是sklearn.svm.SVC()
```
answers = []
answers.extend([1]*100)
answers.extend([2]*100)
answers.extend([3]*100)
answers.extend([4]*98)
inputs = weight[0:398]

from sklearn import svm  

clf = svm.SVC()  
clf.fit(inputs, answers)

result = clf.predict(weight[398:])   
print result   
```


![结果展示](http://upload-images.jianshu.io/upload_images/4029277-c434f04a6e536b2d.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


最后正确率只有大概65%，其中第1，2类的分类是准确的，第3类有部分被分到第1类，第4类完全被分到第1类。查看训练集中的第4类的结果，发现训练是就没有将第1类与第4类分开。
